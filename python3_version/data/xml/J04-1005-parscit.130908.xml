<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.6621805">
Squibs and Discussions
The Kappa Statistic: A Second Look
</title>
<author confidence="0.992473">
Barbara Di Eugenio∗ Michael Glass†
</author>
<affiliation confidence="0.952779">
University of Illinois at Chicago Valparaiso University
</affiliation>
<bodyText confidence="0.998830571428571">
In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating
intercoder agreement for tagging tasks. In this squib, we highlight issues that affect κ and that
the community has largely neglected. First, we discuss the assumptions underlying different
computations of the expected agreement component of κ. Second, we discuss how prevalence and
bias affect the κ measure.
In the last few years, coded corpora have acquired an increasing importance in ev-
ery aspect of human-language technology. Tagging for many phenomena, such as
dialogue acts (Carletta et al. 1997; Di Eugenio et al. 2000), requires coders to make
subtle distinctions among categories. The objectivity of these decisions can be as-
sessed by evaluating the reliability of the tagging, namely, whether the coders reach
a satisfying level of agreement when they perform the same coding task. Currently,
the de facto standard for assessing intercoder agreement is the κ coefficient, which
factors out expected agreement (Cohen 1960; Krippendorff 1980). κ had long been
used in content analysis and medicine (e.g., in psychiatry to assess how well stu-
dents’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981).
Carletta (1996) deserves the credit for bringing κ to the attention of computational
linguists.
κ is computed as P(A) − P(E)
1 − P(E) , where P(A) is the observed agreement among the
coders, and P(E) is the expected agreement, that is, P(E) represents the probabil-
ity that the coders agree by chance. The values of κ are constrained to the inter-
val [−1,1]. A κ value of one means perfect agreement, a κ value of zero means
that agreement is equal to chance, and a κ value of negative one means “perfect”
disagreement.
This squib addresses two issues that have been neglected in the computational
linguistics literature. First, there are two main ways of computing P(E), the expected
agreement, according to whether the distribution of proportions over the categories
is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel
and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reflect different
conceptualizations of the problem. We believe the distinction between the two is often
glossed over because in practice the two computations of P(E) produce very similar
outcomes in most cases, especially for the highest values of κ. However, first, we
will show that they can indeed result in different values of κ, that we will call κCo
(Cohen 1960) and κS&amp;C (Siegel and Castellan 1988). These different values can lead
to contradictory conclusions on intercoder agreement. Moreover, the assumption of
</bodyText>
<footnote confidence="0.8562102">
∗ Computer Science, 1120 SEO (M/C 152), 851 South Morgan Street, Chicago, IL 60607. E-mail:
bdieugen@uic.edu.
† Mathematics and Computer Science, 116 Gellerson Hall, Valparaiso, IN 46383. E-mail: michael.glass@
valpo.edu.
© 2004 Association for Computational Linguistics
</footnote>
<note confidence="0.529663">
Computational Linguistics Volume 30, Number 1
</note>
<bodyText confidence="0.999632333333333">
equal distributions over the categories masks the exact source of disagreement among
the coders. Thus, such an assumption is detrimental if such systematic disagreements
are to be used to improve the coding scheme (Wiebe, Bruce, and O’Hara 1999).
Second, κ is affected by skewed distributions of categories (the prevalence prob-
lem) and by the degree to which the coders disagree (the bias problem). That is, for
a fixed P(A), the values of κ vary substantially in the presence of prevalence, bias, or
both.
We will conclude by suggesting that κCo is a better choice than κS&amp;C in those studies
in which the assumption of equal distributions underlying κS&amp;C does not hold: the vast
majority, if not all, of discourse- and dialogue-tagging efforts. However, as κCo suffers
from the bias problem but κS&amp;C does not, κS&amp;C should be reported too, as well as a third
measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993).
</bodyText>
<sectionHeader confidence="0.830962" genericHeader="abstract">
1. The Computation of P(E)
</sectionHeader>
<bodyText confidence="0.947912407407408">
P(E) is the probability of agreement among coders due to chance. The literature de-
scribes two different methods for estimating a probability distribution for random
assignment of categories. In the first, each coder has a personal distribution, based
on that coder’s distribution of categories (Cohen 1960). In the second, there is one
distribution for all coders, derived from the total proportions of categories assigned
by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1
We now illustrate the computation of P(E) according to these two methods. We
will then show that the resulting κCo and κS&amp;C may straddle one of the significant
thresholds used to assess the raw κ values.
The assumptions underlying these two methods are made tangible in the way
the data are visualized, in a contingency table for Cohen, and in what we will call
an agreement table for the others. Consider the following situation. Two coders2
code 150 occurrences of Okay and assign to them one of the two labels Accept or
Ack(nowledgement) (Allen and Core 1997). The two coders label 70 occurrences as Ac-
cept, and another 55 as Ack. They disagree on 25 occurrences, which one coder labels
as Ack, and the other as Accept. In Figure 1, this example is encoded by the top contin-
gency table on the left (labeled Example 1) and the agreement table on the right. The
contingency table directly mirrors our description. The agreement table is an N × m
matrix, where N is the number of items in the data set and m is the number of labels
that can be assigned to each object; in our example, N = 150 and m = 2. Each entry nij
is the number of codings of label j to item i. The agreement table in Figure 1 shows
that occurrences 1 through 70 have been labeled as Accept by both coders, 71 through
125 as Ack by both coders, and 126 to 150 differ in their labels.
1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan’s to produce a
statistic called alpha. Krippendorff computes P(E) (called 1 − De in his terminology) with a
sampling-without-replacement methodology. The computations of P(E) and of 1 − De show that the
difference is negligible:
</bodyText>
<equation confidence="0.941208666666667">
/ n\z
P(E) = Ej I Nk ij I (Siegel and Castellan)
�� 1 − De j Ni nij / (~ N −1 −1) (Krippendorff)
</equation>
<footnote confidence="0.996242">
2 Both κS&amp;C (Scott 1955) and κCo (Cohen 1960) were originally devised for two coders. Each has been
extended to more than two coders, for example, respectively Fleiss (1971) and Bartko and Carpenter
(1976). Thus, without loss of generality, our examples involve two coders.
</footnote>
<page confidence="0.99697">
96
</page>
<figure confidence="0.993569272727273">
Di Eugenio and Glass Kappa: A Second Look
Example 1
Coder 1
Okay1
...
Okay70
Okay71
...
Okay125
Okay126
...
Okay150
Accept Ack
2 0
2 0
0 2
0 2
1 1
1 1
Example 2
Coder 1
Coder 2
Accept Ack
Accept
Ack
70
0
25
55
95
55
70 80 150
Coder 2
Accept Ack
Accept
Ack
70
10
15
55
85
65
80 70 150
165 135
</figure>
<figureCaption confidence="0.991281">
Figure 1
</figureCaption>
<bodyText confidence="0.994070583333333">
Cohen’s contingency tables (left) and Siegel and Castellan’s agreement table (right).
Agreement tables lose information. When the coders disagree, we cannot recon-
struct which coder picked which category. Consider Example 2 in Figure 1. The two
coders still disagree on 25 occurrences of Okay. However, one coder now labels 10
of those as Accept and the remaining 15 as Ack, whereas the other labels the same
10 as Ack and the same 15 as Accept. The agreement table does not change, but the
contingency table does.
Turning now to computing P(E), Figure 2 shows, for Example 1, Cohen’s com-
putation of P(E) on the left, and Siegel and Castellan’s computation on the right. We
include the computations of KCo and KS&amp;C as the last step. For both Cohen and Siegel
and Castellan, P(A) = 125/150 = 0.8333. The observed agreement P(A) is computed
as the proportion of items the coders agree on to the total number of items; N is the
number of items, and k the number of coders (N = 150 and k = 2 in our example).
Both KCo and KS&amp;C are highly significant at the p = 0.5 ∗ 10−5 level (significance is
computed for KCo and KS&amp;C according to the formulas in Cohen [1960] and Siegel and
Castellan [1988], respectively).
The difference between KCo and KS&amp;C in Figure 2 is just under 1%, however, the
results of the two K computations straddle the value 0.67, which for better or worse
has been adopted as a cutoff in computational linguistics. This cutoff is based on the
assessment of K values in Krippendorff (1980), which discounts K &lt; 0.67 and allows
tentative conclusions when 0.67 &lt; K &lt; 0.8 and definite conclusions when K ≥ 0.8.
Krippendorff’s scale has been adopted without question, even though Krippendorff
himself considers it only a plausible standard that has emerged from his and his
colleagues’ work. In fact, Carletta et al. (1997) use words of caution against adopting
Krippendorff’s suggestion as a standard; the first author has also raised the issue of
how to assess K values in Di Eugenio (2000).
If Krippendorff’s scale is supposed to be our standard, the example just worked
out shows that the different computations of P(E) do affect the assessment of inter-
coder agreement. If less-strict scales are adopted, the discrepancies between the two
K computations play a larger role, as they have a larger effect on smaller values of K.
For example, Rietveld and van Hout (1993) consider 0.20 &lt; K &lt; 0.40 as indicating fair
agreement, and 0.40 &lt; K &lt; 0.60 as indicating moderate agreement. Suppose that two
coders are coding 100 occurrences of Okay. The two coders label 40 occurrences as
Accept and 25 as Ack. The remaining 35 are labeled as Ack by one coder and as Accept
by the other (as in Example 6 in Figure 4); KCo = 0.418, but KS&amp;C = 0.27. These two
values are really at odds.
</bodyText>
<page confidence="0.995885">
97
</page>
<table confidence="0.820792666666667">
Computational Linguistics Volume 30, Number 1
Assumption of different distributions among
coders (Cohen)
</table>
<bodyText confidence="0.4974708">
Step 1. For each category j, compute the overall
proportion pj,l of items assigned to j by each coder
l. In a contingency table, each row and column
total divided by N corresponds to one such pro-
portion for the corresponding coder.
</bodyText>
<equation confidence="0.791316692307692">
pAccept,1 = 95/150, pAck,1 = 55/150,
pAccept,2 = 70/150, pAck,2 = 80/150
Step 2. For a given item, the likelihood of both
coders’ independently agreeing on category j by
chance, is pj,1 * pj,2.
pAccept,1 * pAccept,2 = 95/150 * 70/150 = 0.2956
pAck,1 * pAck,2 = 55/150 * 80/150 = 0.1956
Step 3. P(E), the likelihood of coders’ accidentally
assigning the same category to a given item, is
Ej pj,1 * pj,2 = 0.2956 + 0.1956 = 0.4912
Step 4.
reCo= (0.8333 − 0.4912)/(1 − 0.4912) =
.3421/.5088=0.6724
</equation>
<bodyText confidence="0.970847666666667">
Assumption of equal distributions among coders
(Siegel and Castellan)
Step 1. For each category j, compute pj, the overall
proportion of items assigned to j. In an agreement
table, the column totals give the total counts for
each category j, hence:
</bodyText>
<equation confidence="0.834983875">
1
pj = Nk X K nij
pAccept = 165/300 = 0.55, pAck = 135/300 = 0.45
Step 2. For a given item, the likelihood of both
coders’ independently agreeing on category j by
chance is p2 j .
p2 Accept = 0.3025
p2Ack = 0.2025
</equation>
<bodyText confidence="0.496432">
Step 3. P(E), the likelihood of coders’ accidentally
assigning the same category to a given item, is
</bodyText>
<equation confidence="0.4571846">
� 2 0.3025 + 0.2025 = 0.5050
j pj
Step 4.
reS&amp;C= (0.8333 − 0.5050)/(1 − 0.5050) =
.3283/.4950 = 0.6632
</equation>
<figureCaption confidence="0.944564">
Figure 2
</figureCaption>
<bodyText confidence="0.869793">
The computation of P(E) and κ according to Cohen (left) and to Siegel and Castellan (right).
</bodyText>
<sectionHeader confidence="0.824685" genericHeader="keywords">
2. Unpleasant Behaviors of Kappa: Prevalence and Bias
</sectionHeader>
<bodyText confidence="0.999866857142857">
In the computational linguistics literature, r. has been used mostly to validate cod-
ing schemes: Namely, a “good” value of r. means that the coders agree on the cate-
gories and therefore that those categories are “real.” We noted previously that assess-
ing what constitutes a “good” value for r. is problematic in itself and that different
scales have been proposed. The problem is compounded by the following obvious
effect on r. values: If P(A) is kept constant, varying values for P(E) yield vary-
ing values of r.. What can affect P(E) even if P(A) is constant are prevalence and
bias.
The prevalence problem arises because skewing the distribution of categories in
the data increases P(E). The minimum value P(E) = 1/m occurs when the labels are
equally distributed among the m categories (see Example 4 in Figure 3). The maximum
value P(E) = 1 occurs when the labels are all concentrated in a single category. But
for a given value of P(A), the larger the value of P(E), the lower the value of r..
Example 3 and Example 4 in Figure 3 show two coders agreeing on 90 out of 100
occurrences of Okay, that is, P(A) = 0.9. However, r. ranges from −0.048 to 0.80, and
from not significant to significant (the values of r.S&amp;C for Examples 3 and 4 are the
same as the values of r.Co).3 The differences in r. are due to the difference in the relative
prevalence of the two categories Accept and Ack. In Example 3, the distribution is
skewed, as there are 190 Accepts but only 10 Acks across the two coders; in Example 4,
the distribution is even, as there are 100 Accepts and 100 Acks, respectively. These
results do not depend on the size of the sample; that is, they are not due to the fact
</bodyText>
<page confidence="0.759729">
3 We are not including agreement tables for the sake of brevity.
98
</page>
<table confidence="0.993357388888889">
Di Eugenio and Glass Coder Example 4 Coder Look
Example 3 2 Coder 1 Kappa: A Second
Coder 1 Accept Ack 2
Accept Ack
Accept 90 5 95 Accept 45 5 50
Ack 5 0 5 Ack 5 45 50
95 5 100 50 50 100
P(A) = 0.90, P(E) = 0.905
P(A) = 0.90, P(E) = 0.5
κCo = κS&amp;C = −0.048, p = 1 κCo = κS&amp;C = 0.80, p = 0.5 * 10−5
Figure 3
Contingency tables illustrating the prevalence effect on κ.
Example 5 Example 6
Coder 2 Coder 2
Coder 1 Accept Ack Coder 1 Accept Ack
Accept 40 15 55 Accept 40 35 75
Ack 20 25 45 Ack 0 25 25
60 40 100 40 60 100
</table>
<equation confidence="0.778176">
P(A) = 0.65, P(E) = 0.52 P(A) = 0.65, P(E) = 0.45
κCo = 0.27, p = 0.005 κCo = 0.418, p = 0.5 * 10−5
</equation>
<figureCaption confidence="0.816">
Figure 4
</figureCaption>
<bodyText confidence="0.993361875">
Contingency tables illustrating the bias effect on κCo.
Example 3 and Example 4 are small. As the computations of P(A) and P(E) are based
on proportions, the same distributions of categories in a much larger sample, say,
10,000 items, will result in exactly the same κ values. Although this behavior follows
squarely from κ’s definition, it is at odds with using κ to assess a coding scheme.
From both Example 3 and Example 4 we would like to conclude that the two coders
are in substantial agreement, independent of the skewed prevalence of Accept with
respect to Ack in Example 3. The role of prevalence in assessing κ has been subject
to heated discussion in the medical literature (Grove et al. 1981; Berry 1992; Goldman
1992).
The bias problem occurs in κCo but not κS&amp;C. For κCo, P(E) is computed from
each coder’s individual probabilities. Thus, the less two coders agree in their overall
behavior, the fewer chance agreements are expected. But for a given value of P(A),
decreasing P(E) will increase κCo, leading to the paradox that κCo increases as the
coders become less similar, that is, as the marginal totals diverge in the contingency
table. Consider two coders coding the usual 100 occurrences of Okay, according to
the two tables in Figure 4. In Example 5, the proportions of each category are very
similar among coders, at 55 versus 60 Accept, and 45 versus 40 Ack. However, in
Example 6 coder 1 favors Accept much more than coder 2 (75 versus 40 occurrences)
and conversely chooses Ack much less frequently (25 versus 60 occurrences). In both
cases, P(A) is 0.65 and κS&amp;C is stable at 0.27, but κCo goes from 0.27 to 0.418. Our
initial example in Figure 1 is also affected by bias. The distribution in Example 1
yielded κCo = 0.6724 but κS&amp;C = 0.6632. If the bias decreases as in Example 2, κCo
becomes 0.6632, the same as κS&amp;C.
</bodyText>
<sectionHeader confidence="0.978986" genericHeader="introduction">
3. Discussion
</sectionHeader>
<bodyText confidence="0.7590905">
The issue that remains open is which computation of κ to choose. Siegel and
Castellan’s κS&amp;C is not affected by bias, whereas Cohen’s κCo is. However, it is
</bodyText>
<page confidence="0.991615">
99
</page>
<note confidence="0.493056">
Computational Linguistics Volume 30, Number 1
</note>
<bodyText confidence="0.999911392857143">
questionable whether the assumption of equal distributions underlying κS&amp;C is ap-
propriate for coding in discourse and dialogue work. In fact, it appears to us that it
holds in few if any of the published discourse- or dialogue-tagging efforts for which
κ has been computed. It is, for example, appropriate in situations in which item i may
be tagged by different coders than item j (Fleiss 1971). However, κ assessments for
discourse and dialogue tagging are most often performed on the same portion of the
data, which has been annotated by each of a small number of annotators (between
two and four). In fact, in many cases the analysis of systematic disagreements among
annotators on the same portion of the data (i.e., of bias) can be used to improve the
coding scheme (Wiebe, Bruce, and O’Hara 1999).
To use κCo but to guard against bias, Cicchetti and Feinstein (1990) suggest that κCo
be supplemented, for each coding category, by two measures of agreement, positive
and negative, between the coders. This means a total of 2m additional measures, which
we believe are too many to gain a general insight into the meaning of the specific κCo
value. Alternatively, Byrt, Bishop, and Carlin (1993) suggest that intercoder reliability
be reported as three numbers: κCo and two adjustments of κCo, one with bias removed,
the other with prevalence removed. The value of κCo adjusted for bias turns out to
be ... κS&amp;C. Adjusted for prevalence, κCo yields a measure that is equal to 2P(A) − 1.
The results for Example 1 should then be reported as κCo = 0.6724, κS&amp;C = 0.6632,
2P(A)−1 = 0.6666; those for Example 6 as κCo = 0.418, κS&amp;C = 0.27, and 2P(A)−1 = 0.3.
For both Examples 3 and 4, 2P(A) − 1 = 0.8. Collectively, these three numbers appear
to provide a means of better judging the meaning of κ values. Reporting both κ
and 2P(A) − 1 may seem contradictory, as 2P(A) − 1 does not correct for expected
agreement. However, when the distribution of categories is skewed, this highlights
the effect of prevalence. Reporting both κCo and κS&amp;C does not invalidate our previous
discussion, as we believe κCo is more appropriate for discourse- and dialogue-tagging
in the majority of cases, especially when exploiting bias to improve coding (Wiebe,
Bruce, and O’Hara 1999).
</bodyText>
<sectionHeader confidence="0.98754" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9844598">
This work is supported by grant
N00014-00-1-0640 from the Office of Naval
Research. Thanks to Janet Cahn and to the
anonymous reviewers for comments on
earlier drafts.
</bodyText>
<sectionHeader confidence="0.962905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981611767441861">
Allen, James and Mark Core. 1997. DAMSL:
Dialog act markup in several layers;
Coding scheme developed by the
participants at two discourse tagging
workshops, University of Pennsylvania,
March 1996, and Schloß Dagstuhl,
February 1997. Draft.
Bartko, John J. and William T. Carpenter.
1976. On the methods and theory of
reliability. Journal of Nervous and Mental
Disease, 163(5):307–317.
Berry, Charles C. 1992. The κ statistic [letter
to the editor]. Journal of the American
Medical Association, 268(18):2513–2514.
Byrt, Ted, Janet Bishop, and John B. Carlin.
1993. Bias, prevalence, and kappa. Journal
of Clinical Epidemiology, 46(5):423–429.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The Kappa statistic.
Computational Linguistics, 22(2):249–254.
Carletta, Jean, Amy Isard, Stephen Isard,
Jacqueline C. Kowtko, Gwyneth
Doherty-Sneddon, and Anne H.
Anderson. 1997. The reliability of a
dialogue structure coding scheme.
Computational Lingustics, 23(1):13–31.
Cicchetti, Domenic V. and Alvan R.
Feinstein. 1990. High agreement but low
kappa: II. Resolving the paradoxes.
Journal of Clinical Epidemiology,
43(6):551–558.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37–46.
Di Eugenio, Barbara. 2000. On the usage of
Kappa to evaluate agreement on coding
tasks. In LREC2000: Proceedings of the
Second International Conference on Language
Resources and Evaluation, pages 441–444,
Athens.
Di Eugenio, Barbara, Pamela W. Jordan,
Richmond H. Thomason, and Johanna D.
Moore. 2000. The agreement process: An
</reference>
<page confidence="0.844874">
100
</page>
<note confidence="0.574843">
Di Eugenio and Glass Kappa: A Second Look
</note>
<reference confidence="0.999281736842105">
empirical investigation of human-human
computer-mediated collaborative
dialogues. International Journal of Human
Computer Studies, 53(6):1017–1076.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378–382.
Goldman, Ronald L. 1992. The κ statistic
[letter to the editor (in reply)]. Journal of
the American Medical Association,
268(18):2513–2514.
Grove, William M., Nancy C. Andreasen,
Patricia McDonald-Scott, Martin B. Keller,
and Robert W. Shapiro. 1981. Reliability
studies of psychiatric diagnosis: Theory
and practice. Archives of General Psychiatry,
38:408–413.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology. Sage
Publications, Beverly Hills, CA.
Rietveld, Toni and Roeland van Hout. 1993.
Statistical Techniques for the Study of
Language and Language Behaviour. Mouton
de Gruyter, Berlin.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale
coding. Public Opinion Quarterly,
19:127–141.
Siegel, Sidney and N. John Castellan, Jr.
1988. Nonparametric statistics for the
behavioral sciences. McGraw Hill, Boston.
Wiebe, Janyce M., Rebecca F. Bruce, and
Thomas P. O’Hara. 1999. Development
and use of a gold-standard data set for
subjectivity classifications. In ACL99:
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics,
pages 246–253, College Park, MD.
</reference>
<page confidence="0.998592">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.755189">Squibs and Discussions The Kappa Statistic: A Second Look</title>
<author confidence="0.991841">Di_Michael</author>
<affiliation confidence="0.999983">University of Illinois at Chicago Valparaiso University</affiliation>
<abstract confidence="0.976435951807229">In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating agreement for tagging tasks. In this squib, we highlight issues that affect that the community has largely neglected. First, we discuss the assumptions underlying different of the expected agreement component of Second, we discuss how prevalence and affect the In the last few years, coded corpora have acquired an increasing importance in every aspect of human-language technology. Tagging for many phenomena, such as dialogue acts (Carletta et al. 1997; Di Eugenio et al. 2000), requires coders to make subtle distinctions among categories. The objectivity of these decisions can be assessed by evaluating the reliability of the tagging, namely, whether the coders reach a satisfying level of agreement when they perform the same coding task. Currently, de facto standard for assessing intercoder agreement is the which out expected agreement (Cohen 1960; Krippendorff 1980). long been used in content analysis and medicine (e.g., in psychiatry to assess how well students’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981). (1996) deserves the credit for bringing the attention of computational linguists. computed as where the observed agreement among the and the expected agreement, that is, the probabilthat the coders agree by chance. The values of constrained to the inter- A of one means perfect agreement, a of zero means agreement is equal to chance, and a of negative one means “perfect” disagreement. This squib addresses two issues that have been neglected in the computational literature. First, there are two main ways of computing the expected agreement, according to whether the distribution of proportions over the categories is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reflect different conceptualizations of the problem. We believe the distinction between the two is often over because in practice the two computations of very similar in most cases, especially for the highest values of However, first, we show that they can indeed result in different values of that we will call 1960) and and Castellan 1988). These different values can lead to contradictory conclusions on intercoder agreement. Moreover, the assumption of Science, 1120 SEO (M/C 152), 851 South Morgan Street, Chicago, IL 60607. E-mail: bdieugen@uic.edu. and Computer Science, 116 Gellerson Hall, Valparaiso, IN 46383. E-mail: michael.glass@ valpo.edu. Association for Computational Linguistics Computational Linguistics Volume 30, Number 1 equal distributions over the categories masks the exact source of disagreement among the coders. Thus, such an assumption is detrimental if such systematic disagreements are to be used to improve the coding scheme (Wiebe, Bruce, and O’Hara 1999). affected by skewed distributions of categories (the proband by the degree to which the coders disagree (the That is, for fixed the values of substantially in the presence of prevalence, bias, or both. will conclude by suggesting that a better choice than those studies which the assumption of equal distributions underlying not hold: the vast if not all, of discourseand dialogue-tagging efforts. However, as the bias problem but not, be reported too, as well as a third measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993). The Computation of the probability of agreement among coders due to chance. The literature describes two different methods for estimating a probability distribution for random assignment of categories. In the first, each coder has a personal distribution, based on that coder’s distribution of categories (Cohen 1960). In the second, there is one distribution for all coders, derived from the total proportions of categories assigned all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan We now illustrate the computation of P(E) according to these two methods. We then show that the resulting and straddle one of the significant used to assess the raw The assumptions underlying these two methods are made tangible in the way data are visualized, in a table Cohen, and in what we will call table the others. Consider the following situation. Two 150 occurrences of assign to them one of the two labels and Core 1997). The two coders label 70 occurrences as Acand another 55 as They disagree on 25 occurrences, which one coder labels and the other as In Figure 1, this example is encoded by the top contintable on the left (labeled and the agreement table on the right. The table directly mirrors our description. The agreement table is an where the number of items in the data set and the number of labels can be assigned to each object; in our example, and Each entry the number of codings of label item The agreement table in Figure 1 shows occurrences 1 through 70 have been labeled as both coders, 71 through as both coders, and 126 to 150 differ in their labels. 1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan’s to produce a called alpha. Krippendorff computes 1 his terminology) with a methodology. The computations of of 1 show that the difference is negligible: = ijI and Castellan) j (~</abstract>
<note confidence="0.87747355">Both 1955) and 1960) were originally devised for two coders. Each has been extended to more than two coders, for example, respectively Fleiss (1971) and Bartko and Carpenter (1976). Thus, without loss of generality, our examples involve two coders. 96 Di Eugenio and Glass Kappa: A Second Look Example 1 Coder 1 ... ... ... Accept Ack 2 0 2 0 0 2 0 2 1 1 1 1 Example 2 Coder 1 Coder 2 Accept Ack Accept Ack 70 0 25 55 95 55 70 80 150 Coder 2 Accept Ack Accept Ack 70 10 15 55 85 65</note>
<phone confidence="0.7474605">80 70 150 165 135</phone>
<abstract confidence="0.995512182795699">Figure 1 Cohen’s contingency tables (left) and Siegel and Castellan’s agreement table (right). Agreement tables lose information. When the coders disagree, we cannot reconstruct which coder picked which category. Consider Example 2 in Figure 1. The two still disagree on 25 occurrences of However, one coder now labels 10 those as the remaining 15 as whereas the other labels the same as the same 15 as The agreement table does not change, but the contingency table does. now to computing 2 shows, for Example 1, Cohen’s comof the left, and Siegel and Castellan’s computation on the right. We the computations of and the last step. For both Cohen and Siegel Castellan, = The observed agreement computed the proportion of items the coders agree on to the total number of items; the of items, and number of coders and in our example). and highly significant at the level (significance is for and to the formulas in Cohen [1960] and Siegel and Castellan [1988], respectively). difference between and Figure 2 is just under 1%, however, the of the two straddle the value 0.67, which for better or worse has been adopted as a cutoff in computational linguistics. This cutoff is based on the of in Krippendorff (1980), which discounts &lt; and allows conclusions when 0.67 &lt; and definite conclusions when Krippendorff’s scale has been adopted without question, even though Krippendorff himself considers it only a plausible standard that has emerged from his and his colleagues’ work. In fact, Carletta et al. (1997) use words of caution against adopting Krippendorff’s suggestion as a standard; the first author has also raised the issue of to assess in Di Eugenio (2000). If Krippendorff’s scale is supposed to be our standard, the example just worked shows that the different computations of affect the assessment of intercoder agreement. If less-strict scales are adopted, the discrepancies between the two play a larger role, as they have a larger effect on smaller values of example, Rietveld and van Hout (1993) consider K as indicating fair and 0.40 K as indicating moderate agreement. Suppose that two are coding 100 occurrences of The two coders label 40 occurrences as 25 as The remaining 35 are labeled as one coder and as the other (as in Example 6 in Figure 4); but These two values are really at odds. 97 Computational Linguistics Volume 30, Number 1 Assumption of different distributions among coders (Cohen) 1. each category compute the overall items assigned to each coder In a contingency table, each row and column divided by to one such proportion for the corresponding coder. For a given item, the likelihood of both independently agreeing on category is the likelihood of coders’ accidentally assigning the same category to a given item, is Step 4. = .3421/.5088=0.6724 Assumption of equal distributions among coders (Siegel and Castellan) For each category compute the overall of items assigned to In an agreement table, the column totals give the total counts for category hence: 1 For a given item, the likelihood of both independently agreeing on category is the likelihood of coders’ accidentally assigning the same category to a given item, is Step 4. = Figure 2 computation of to Cohen (left) and to Siegel and Castellan (right). 2. Unpleasant Behaviors of Kappa: Prevalence and Bias the computational linguistics literature, been used mostly to validate codschemes: Namely, a “good” value of that the coders agree on the categories and therefore that those categories are “real.” We noted previously that assesswhat constitutes a “good” value for problematic in itself and that different scales have been proposed. The problem is compounded by the following obvious on If kept constant, varying values for varyvalues of What can affect if constant are prevalence and bias. The prevalence problem arises because skewing the distribution of categories in data increases minimum value = when the labels are distributed among the (see Example 4 in Figure 3). The maximum = occurs when the labels are all concentrated in a single category. But a given value of larger the value of lower the value of Example 3 and Example 4 in Figure 3 show two coders agreeing on 90 out of 100 of that is, = However, from to 0.80, and not significant to significant (the values of Examples 3 and 4 are the as the values of differences in due to the difference in the relative of the two categories In Example 3, the distribution is as there are 190 but only 10 across the two coders; in Example 4, distribution is even, as there are 100 and 100 respectively. These results do not depend on the size of the sample; that is, they are not due to the fact 3 We are not including agreement tables for the sake of brevity.</abstract>
<date confidence="0.346077">98</date>
<author confidence="0.765013">Di_Eugenio</author>
<author confidence="0.765013">Glass Coder Example Coder Look</author>
<note confidence="0.8198068">Example 3 Coder 1 2 Coder 1 Kappa: A Second Accept Ack 2 Accept Ack Accept 90 5 95 Accept 45 5 50 Ack 5 0 5 Ack 5 45 50</note>
<phone confidence="0.717879">95 5 100 50 50 100</phone>
<abstract confidence="0.8570322">0.80, Figure 3 tables illustrating the prevalence effect on</abstract>
<note confidence="0.8866812">Example 5 Example 6 Coder 2 Coder 2 Coder 1 Accept Ack Coder 1 Accept Ack Accept 40 15 55 Accept 40 35 75 Ack 20 25 45 Ack 0 25 25</note>
<phone confidence="0.770861">60 40 100 40 60 100</phone>
<abstract confidence="0.987070118644068">0.27, 0.418, Figure 4 tables illustrating the bias effect on 3 and Example 4 are small. As the computations of based on proportions, the same distributions of categories in a much larger sample, say, items, will result in exactly the same Although this behavior follows from definition, it is at odds with using assess a coding scheme. From both Example 3 and Example 4 we would like to conclude that the two coders in substantial agreement, independent of the skewed prevalence of to Example 3. The role of prevalence in assessing been subject to heated discussion in the medical literature (Grove et al. 1981; Berry 1992; Goldman 1992). bias problem occurs in not For computed from each coder’s individual probabilities. Thus, the less two coders agree in their overall the fewer chance agreements are expected. But for a given value of increase leading to the paradox that as the coders become less similar, that is, as the marginal totals diverge in the contingency Consider two coders coding the usual 100 occurrences of according to the two tables in Figure 4. In Example 5, the proportions of each category are very among coders, at 55 versus 60 and 45 versus 40 However, in 6 coder 1 favors more than coder 2 (75 versus 40 occurrences) conversely chooses less frequently (25 versus 60 occurrences). In both 0.65 and stable at 0.27, but from 0.27 to 0.418. Our initial example in Figure 1 is also affected by bias. The distribution in Example 1 0.6724 but 0.6632. If the bias decreases as in Example 2, 0.6632, the same as 3. Discussion issue that remains open is which computation of choose. Siegel and not affected by bias, whereas Cohen’s However, it is 99 Computational Linguistics Volume 30, Number 1 whether the assumption of equal distributions underlying appropriate for coding in discourse and dialogue work. In fact, it appears to us that it holds in few if any of the published discourseor dialogue-tagging efforts for which been computed. It is, for example, appropriate in situations in which item tagged by different coders than item 1971). However, for discourse and dialogue tagging are most often performed on the same portion of the data, which has been annotated by each of a small number of annotators (between two and four). In fact, in many cases the analysis of systematic disagreements among annotators on the same portion of the data (i.e., of bias) can be used to improve the coding scheme (Wiebe, Bruce, and O’Hara 1999). use to guard against bias, Cicchetti and Feinstein (1990) suggest that be supplemented, for each coding category, by two measures of agreement, positive negative, between the coders. This means a total of measures, which believe are too many to gain a general insight into the meaning of the specific value. Alternatively, Byrt, Bishop, and Carlin (1993) suggest that intercoder reliability reported as three numbers: two adjustments of one with bias removed, other with prevalence removed. The value of for bias turns out to Adjusted for prevalence, a measure that is equal to results for Example 1 should then be reported as those for Example 6 as and both Examples 3 and 4, Collectively, these three numbers appear provide a means of better judging the meaning of Reporting both may seem contradictory, as does for expected agreement. However, when the distribution of categories is skewed, this highlights effect of prevalence. Reporting both and not invalidate our previous as we believe more appropriate for discourseand dialogue-tagging in the majority of cases, especially when exploiting bias to improve coding (Wiebe,</abstract>
<note confidence="0.907553">Bruce, and O’Hara 1999). Acknowledgments This work is supported by grant N00014-00-1-0640 from the Office of Naval Research. Thanks to Janet Cahn and to the</note>
<abstract confidence="0.785912">anonymous reviewers for comments on earlier drafts.</abstract>
<note confidence="0.8119292">References Allen, James and Mark Core. 1997. DAMSL: Dialog act markup in several layers; Coding scheme developed by the participants at two discourse tagging</note>
<affiliation confidence="0.980393">workshops, University of Pennsylvania,</affiliation>
<address confidence="0.848765">March 1996, and Schloß Dagstuhl,</address>
<note confidence="0.880665944444445">February 1997. Draft. Bartko, John J. and William T. Carpenter. 1976. On the methods and theory of of Nervous and Mental 163(5):307–317. Charles C. 1992. The [letter the editor]. of the American 268(18):2513–2514. Byrt, Ted, Janet Bishop, and John B. Carlin. Bias, prevalence, and kappa. Clinical 46(5):423–429. Carletta, Jean. 1996. Assessing agreement on classification tasks: The Kappa statistic. 22(2):249–254. Carletta, Jean, Amy Isard, Stephen Isard, Jacqueline C. Kowtko, Gwyneth Doherty-Sneddon, and Anne H. Anderson. 1997. The reliability of a</note>
<abstract confidence="0.827409555555556">dialogue structure coding scheme. 23(1):13–31. Cicchetti, Domenic V. and Alvan R. Feinstein. 1990. High agreement but low kappa: II. Resolving the paradoxes. of Clinical 43(6):551–558. Cohen, Jacob. 1960. A coefficient of for nominal scales.</abstract>
<note confidence="0.917655">Psychological 20:37–46. Di Eugenio, Barbara. 2000. On the usage of Kappa to evaluate agreement on coding In Proceedings of the Second International Conference on Language and pages 441–444, Athens.</note>
<author confidence="0.9497585">Barbara Di_Eugenio</author>
<author confidence="0.9497585">Pamela W Jordan</author>
<author confidence="0.9497585">Richmond H Thomason</author>
<author confidence="0.9497585">D Johanna</author>
<address confidence="0.616808">Moore. 2000. The agreement process: An 100</address>
<author confidence="0.943251">Di_Eugenio</author>
<author confidence="0.943251">Glass Kappa A Second Look</author>
<note confidence="0.479392272727273">empirical investigation of human-human computer-mediated collaborative Journal of Human 53(6):1017–1076. Fleiss, Joseph L. 1971. Measuring nominal scale agreement among many raters. 76(5):378–382. Ronald L. 1992. The to the editor (in reply)]. of American Medical 268(18):2513–2514.</note>
<author confidence="0.834205333333333">Reliability</author>
<affiliation confidence="0.7335715">studies of psychiatric diagnosis: Theory practice. of General</affiliation>
<address confidence="0.495885">38:408–413.</address>
<intro confidence="0.515517">Klaus. 1980. Analysis:</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Mark Core</author>
</authors>
<title>DAMSL: Dialog act markup in several layers; Coding scheme developed by the participants at two discourse tagging workshops,</title>
<date>1997</date>
<publisher>Draft.</publisher>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="5197" citStr="Allen and Core 1997" startWordPosition="829" endWordPosition="832">iss 1971; Krippendorff 1980; Siegel and Castellan 1988).1 We now illustrate the computation of P(E) according to these two methods. We will then show that the resulting κCo and κS&amp;C may straddle one of the significant thresholds used to assess the raw κ values. The assumptions underlying these two methods are made tangible in the way the data are visualized, in a contingency table for Cohen, and in what we will call an agreement table for the others. Consider the following situation. Two coders2 code 150 occurrences of Okay and assign to them one of the two labels Accept or Ack(nowledgement) (Allen and Core 1997). The two coders label 70 occurrences as Accept, and another 55 as Ack. They disagree on 25 occurrences, which one coder labels as Ack, and the other as Accept. In Figure 1, this example is encoded by the top contingency table on the left (labeled Example 1) and the agreement table on the right. The contingency table directly mirrors our description. The agreement table is an N × m matrix, where N is the number of items in the data set and m is the number of labels that can be assigned to each object; in our example, N = 150 and m = 2. Each entry nij is the number of codings of label j to item</context>
</contexts>
<marker>Allen, Core, 1997</marker>
<rawString>Allen, James and Mark Core. 1997. DAMSL: Dialog act markup in several layers; Coding scheme developed by the participants at two discourse tagging workshops, University of Pennsylvania, March 1996, and Schloß Dagstuhl, February 1997. Draft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Bartko</author>
<author>William T Carpenter</author>
</authors>
<title>On the methods and theory of reliability.</title>
<date>1976</date>
<journal>Journal of Nervous and Mental Disease,</journal>
<volume>163</volume>
<issue>5</issue>
<contexts>
<context position="6607" citStr="Bartko and Carpenter (1976)" startWordPosition="1094" endWordPosition="1097">r labels. 1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan’s to produce a statistic called alpha. Krippendorff computes P(E) (called 1 − De in his terminology) with a sampling-without-replacement methodology. The computations of P(E) and of 1 − De show that the difference is negligible: / n\z P(E) = Ej I Nk ij I (Siegel and Castellan) �� 1 − De j Ni nij / (~ N −1 −1) (Krippendorff) 2 Both κS&amp;C (Scott 1955) and κCo (Cohen 1960) were originally devised for two coders. Each has been extended to more than two coders, for example, respectively Fleiss (1971) and Bartko and Carpenter (1976). Thus, without loss of generality, our examples involve two coders. 96 Di Eugenio and Glass Kappa: A Second Look Example 1 Coder 1 Okay1 ... Okay70 Okay71 ... Okay125 Okay126 ... Okay150 Accept Ack 2 0 2 0 0 2 0 2 1 1 1 1 Example 2 Coder 1 Coder 2 Accept Ack Accept Ack 70 0 25 55 95 55 70 80 150 Coder 2 Accept Ack Accept Ack 70 10 15 55 85 65 80 70 150 165 135 Figure 1 Cohen’s contingency tables (left) and Siegel and Castellan’s agreement table (right). Agreement tables lose information. When the coders disagree, we cannot reconstruct which coder picked which category. Consider Example 2 in F</context>
</contexts>
<marker>Bartko, Carpenter, 1976</marker>
<rawString>Bartko, John J. and William T. Carpenter. 1976. On the methods and theory of reliability. Journal of Nervous and Mental Disease, 163(5):307–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles C Berry</author>
</authors>
<title>The κ statistic [letter to the editor].</title>
<date>1992</date>
<journal>Journal of the American Medical Association,</journal>
<volume>268</volume>
<issue>18</issue>
<contexts>
<context position="14520" citStr="Berry 1992" startWordPosition="2563" endWordPosition="2564">tions of P(A) and P(E) are based on proportions, the same distributions of categories in a much larger sample, say, 10,000 items, will result in exactly the same κ values. Although this behavior follows squarely from κ’s definition, it is at odds with using κ to assess a coding scheme. From both Example 3 and Example 4 we would like to conclude that the two coders are in substantial agreement, independent of the skewed prevalence of Accept with respect to Ack in Example 3. The role of prevalence in assessing κ has been subject to heated discussion in the medical literature (Grove et al. 1981; Berry 1992; Goldman 1992). The bias problem occurs in κCo but not κS&amp;C. For κCo, P(E) is computed from each coder’s individual probabilities. Thus, the less two coders agree in their overall behavior, the fewer chance agreements are expected. But for a given value of P(A), decreasing P(E) will increase κCo, leading to the paradox that κCo increases as the coders become less similar, that is, as the marginal totals diverge in the contingency table. Consider two coders coding the usual 100 occurrences of Okay, according to the two tables in Figure 4. In Example 5, the proportions of each category are very</context>
</contexts>
<marker>Berry, 1992</marker>
<rawString>Berry, Charles C. 1992. The κ statistic [letter to the editor]. Journal of the American Medical Association, 268(18):2513–2514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Byrt</author>
<author>Janet Bishop</author>
<author>John B Carlin</author>
</authors>
<title>Bias, prevalence, and kappa.</title>
<date>1993</date>
<journal>Journal of Clinical Epidemiology,</journal>
<volume>46</volume>
<issue>5</issue>
<marker>Byrt, Bishop, Carlin, 1993</marker>
<rawString>Byrt, Ted, Janet Bishop, and John B. Carlin. 1993. Bias, prevalence, and kappa. Journal of Clinical Epidemiology, 46(5):423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The Kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="1403" citStr="Carletta (1996)" startWordPosition="217" endWordPosition="218">coders to make subtle distinctions among categories. The objectivity of these decisions can be assessed by evaluating the reliability of the tagging, namely, whether the coders reach a satisfying level of agreement when they perform the same coding task. Currently, the de facto standard for assessing intercoder agreement is the κ coefficient, which factors out expected agreement (Cohen 1960; Krippendorff 1980). κ had long been used in content analysis and medicine (e.g., in psychiatry to assess how well students’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981). Carletta (1996) deserves the credit for bringing κ to the attention of computational linguists. κ is computed as P(A) − P(E) 1 − P(E) , where P(A) is the observed agreement among the coders, and P(E) is the expected agreement, that is, P(E) represents the probability that the coders agree by chance. The values of κ are constrained to the interval [−1,1]. A κ value of one means perfect agreement, a κ value of zero means that agreement is equal to chance, and a κ value of negative one means “perfect” disagreement. This squib addresses two issues that have been neglected in the computational linguistics literat</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, Jean. 1996. Assessing agreement on classification tasks: The Kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>Amy Isard</author>
<author>Stephen Isard</author>
<author>Jacqueline C Kowtko</author>
<author>Gwyneth Doherty-Sneddon</author>
<author>Anne H Anderson</author>
</authors>
<title>The reliability of a dialogue structure coding scheme.</title>
<date>1997</date>
<journal>Computational Lingustics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="752" citStr="Carletta et al. 1997" startWordPosition="111" endWordPosition="114">raiso University In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating intercoder agreement for tagging tasks. In this squib, we highlight issues that affect κ and that the community has largely neglected. First, we discuss the assumptions underlying different computations of the expected agreement component of κ. Second, we discuss how prevalence and bias affect the κ measure. In the last few years, coded corpora have acquired an increasing importance in every aspect of human-language technology. Tagging for many phenomena, such as dialogue acts (Carletta et al. 1997; Di Eugenio et al. 2000), requires coders to make subtle distinctions among categories. The objectivity of these decisions can be assessed by evaluating the reliability of the tagging, namely, whether the coders reach a satisfying level of agreement when they perform the same coding task. Currently, the de facto standard for assessing intercoder agreement is the κ coefficient, which factors out expected agreement (Cohen 1960; Krippendorff 1980). κ had long been used in content analysis and medicine (e.g., in psychiatry to assess how well students’ diagnoses on a set of test cases agree with e</context>
<context position="8812" citStr="Carletta et al. (1997)" startWordPosition="1499" endWordPosition="1502">difference between KCo and KS&amp;C in Figure 2 is just under 1%, however, the results of the two K computations straddle the value 0.67, which for better or worse has been adopted as a cutoff in computational linguistics. This cutoff is based on the assessment of K values in Krippendorff (1980), which discounts K &lt; 0.67 and allows tentative conclusions when 0.67 &lt; K &lt; 0.8 and definite conclusions when K ≥ 0.8. Krippendorff’s scale has been adopted without question, even though Krippendorff himself considers it only a plausible standard that has emerged from his and his colleagues’ work. In fact, Carletta et al. (1997) use words of caution against adopting Krippendorff’s suggestion as a standard; the first author has also raised the issue of how to assess K values in Di Eugenio (2000). If Krippendorff’s scale is supposed to be our standard, the example just worked out shows that the different computations of P(E) do affect the assessment of intercoder agreement. If less-strict scales are adopted, the discrepancies between the two K computations play a larger role, as they have a larger effect on smaller values of K. For example, Rietveld and van Hout (1993) consider 0.20 &lt; K &lt; 0.40 as indicating fair agreem</context>
</contexts>
<marker>Carletta, Isard, Isard, Kowtko, Doherty-Sneddon, Anderson, 1997</marker>
<rawString>Carletta, Jean, Amy Isard, Stephen Isard, Jacqueline C. Kowtko, Gwyneth Doherty-Sneddon, and Anne H. Anderson. 1997. The reliability of a dialogue structure coding scheme. Computational Lingustics, 23(1):13–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Domenic V Cicchetti</author>
<author>Alvan R Feinstein</author>
</authors>
<title>High agreement but low kappa: II. Resolving the paradoxes.</title>
<date>1990</date>
<journal>Journal of Clinical Epidemiology,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="16746" citStr="Cicchetti and Feinstein (1990)" startWordPosition="2947" endWordPosition="2950">for which κ has been computed. It is, for example, appropriate in situations in which item i may be tagged by different coders than item j (Fleiss 1971). However, κ assessments for discourse and dialogue tagging are most often performed on the same portion of the data, which has been annotated by each of a small number of annotators (between two and four). In fact, in many cases the analysis of systematic disagreements among annotators on the same portion of the data (i.e., of bias) can be used to improve the coding scheme (Wiebe, Bruce, and O’Hara 1999). To use κCo but to guard against bias, Cicchetti and Feinstein (1990) suggest that κCo be supplemented, for each coding category, by two measures of agreement, positive and negative, between the coders. This means a total of 2m additional measures, which we believe are too many to gain a general insight into the meaning of the specific κCo value. Alternatively, Byrt, Bishop, and Carlin (1993) suggest that intercoder reliability be reported as three numbers: κCo and two adjustments of κCo, one with bias removed, the other with prevalence removed. The value of κCo adjusted for bias turns out to be ... κS&amp;C. Adjusted for prevalence, κCo yields a measure that is eq</context>
</contexts>
<marker>Cicchetti, Feinstein, 1990</marker>
<rawString>Cicchetti, Domenic V. and Alvan R. Feinstein. 1990. High agreement but low kappa: II. Resolving the paradoxes. Journal of Clinical Epidemiology, 43(6):551–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="1181" citStr="Cohen 1960" startWordPosition="179" endWordPosition="180">few years, coded corpora have acquired an increasing importance in every aspect of human-language technology. Tagging for many phenomena, such as dialogue acts (Carletta et al. 1997; Di Eugenio et al. 2000), requires coders to make subtle distinctions among categories. The objectivity of these decisions can be assessed by evaluating the reliability of the tagging, namely, whether the coders reach a satisfying level of agreement when they perform the same coding task. Currently, the de facto standard for assessing intercoder agreement is the κ coefficient, which factors out expected agreement (Cohen 1960; Krippendorff 1980). κ had long been used in content analysis and medicine (e.g., in psychiatry to assess how well students’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981). Carletta (1996) deserves the credit for bringing κ to the attention of computational linguists. κ is computed as P(A) − P(E) 1 − P(E) , where P(A) is the observed agreement among the coders, and P(E) is the expected agreement, that is, P(E) represents the probability that the coders agree by chance. The values of κ are constrained to the interval [−1,1]. A κ value of one means perfect agree</context>
<context position="2681" citStr="Cohen 1960" startWordPosition="432" endWordPosition="433"> agreement, according to whether the distribution of proportions over the categories is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reflect different conceptualizations of the problem. We believe the distinction between the two is often glossed over because in practice the two computations of P(E) produce very similar outcomes in most cases, especially for the highest values of κ. However, first, we will show that they can indeed result in different values of κ, that we will call κCo (Cohen 1960) and κS&amp;C (Siegel and Castellan 1988). These different values can lead to contradictory conclusions on intercoder agreement. Moreover, the assumption of ∗ Computer Science, 1120 SEO (M/C 152), 851 South Morgan Street, Chicago, IL 60607. E-mail: bdieugen@uic.edu. † Mathematics and Computer Science, 116 Gellerson Hall, Valparaiso, IN 46383. E-mail: michael.glass@ valpo.edu. © 2004 Association for Computational Linguistics Computational Linguistics Volume 30, Number 1 equal distributions over the categories masks the exact source of disagreement among the coders. Thus, such an assumption is detri</context>
<context position="4430" citStr="Cohen 1960" startWordPosition="703" endWordPosition="704">d: the vast majority, if not all, of discourse- and dialogue-tagging efforts. However, as κCo suffers from the bias problem but κS&amp;C does not, κS&amp;C should be reported too, as well as a third measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993). 1. The Computation of P(E) P(E) is the probability of agreement among coders due to chance. The literature describes two different methods for estimating a probability distribution for random assignment of categories. In the first, each coder has a personal distribution, based on that coder’s distribution of categories (Cohen 1960). In the second, there is one distribution for all coders, derived from the total proportions of categories assigned by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1 We now illustrate the computation of P(E) according to these two methods. We will then show that the resulting κCo and κS&amp;C may straddle one of the significant thresholds used to assess the raw κ values. The assumptions underlying these two methods are made tangible in the way the data are visualized, in a contingency table for Cohen, and in what we will call an agreement table for the others</context>
<context position="6447" citStr="Cohen 1960" startWordPosition="1071" endWordPosition="1072">ws that occurrences 1 through 70 have been labeled as Accept by both coders, 71 through 125 as Ack by both coders, and 126 to 150 differ in their labels. 1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan’s to produce a statistic called alpha. Krippendorff computes P(E) (called 1 − De in his terminology) with a sampling-without-replacement methodology. The computations of P(E) and of 1 − De show that the difference is negligible: / n\z P(E) = Ej I Nk ij I (Siegel and Castellan) �� 1 − De j Ni nij / (~ N −1 −1) (Krippendorff) 2 Both κS&amp;C (Scott 1955) and κCo (Cohen 1960) were originally devised for two coders. Each has been extended to more than two coders, for example, respectively Fleiss (1971) and Bartko and Carpenter (1976). Thus, without loss of generality, our examples involve two coders. 96 Di Eugenio and Glass Kappa: A Second Look Example 1 Coder 1 Okay1 ... Okay70 Okay71 ... Okay125 Okay126 ... Okay150 Accept Ack 2 0 2 0 0 2 0 2 1 1 1 1 Example 2 Coder 1 Coder 2 Accept Ack Accept Ack 70 0 25 55 95 55 70 80 150 Coder 2 Accept Ack Accept Ack 70 10 15 55 85 65 80 70 150 165 135 Figure 1 Cohen’s contingency tables (left) and Siegel and Castellan’s agreem</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, Jacob. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
</authors>
<title>On the usage of Kappa to evaluate agreement on coding tasks.</title>
<date>2000</date>
<booktitle>In LREC2000: Proceedings of the Second International Conference on Language Resources and Evaluation,</booktitle>
<pages>441--444</pages>
<location>Athens.</location>
<marker>Di Eugenio, 2000</marker>
<rawString>Di Eugenio, Barbara. 2000. On the usage of Kappa to evaluate agreement on coding tasks. In LREC2000: Proceedings of the Second International Conference on Language Resources and Evaluation, pages 441–444, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Pamela W Jordan</author>
<author>Richmond H Thomason</author>
<author>Johanna D Moore</author>
</authors>
<title>The agreement process: An empirical investigation of human-human computer-mediated collaborative dialogues.</title>
<date>2000</date>
<journal>International Journal of Human Computer Studies,</journal>
<volume>53</volume>
<issue>6</issue>
<marker>Di Eugenio, Jordan, Thomason, Moore, 2000</marker>
<rawString>Di Eugenio, Barbara, Pamela W. Jordan, Richmond H. Thomason, and Johanna D. Moore. 2000. The agreement process: An empirical investigation of human-human computer-mediated collaborative dialogues. International Journal of Human Computer Studies, 53(6):1017–1076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="2215" citStr="Fleiss 1971" startWordPosition="357" endWordPosition="358">agreement, that is, P(E) represents the probability that the coders agree by chance. The values of κ are constrained to the interval [−1,1]. A κ value of one means perfect agreement, a κ value of zero means that agreement is equal to chance, and a κ value of negative one means “perfect” disagreement. This squib addresses two issues that have been neglected in the computational linguistics literature. First, there are two main ways of computing P(E), the expected agreement, according to whether the distribution of proportions over the categories is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reflect different conceptualizations of the problem. We believe the distinction between the two is often glossed over because in practice the two computations of P(E) produce very similar outcomes in most cases, especially for the highest values of κ. However, first, we will show that they can indeed result in different values of κ, that we will call κCo (Cohen 1960) and κS&amp;C (Siegel and Castellan 1988). These different values can lead to contradictory conclusions on intercoder agreement. Moreover,</context>
<context position="4585" citStr="Fleiss 1971" startWordPosition="728" endWordPosition="729">be reported too, as well as a third measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993). 1. The Computation of P(E) P(E) is the probability of agreement among coders due to chance. The literature describes two different methods for estimating a probability distribution for random assignment of categories. In the first, each coder has a personal distribution, based on that coder’s distribution of categories (Cohen 1960). In the second, there is one distribution for all coders, derived from the total proportions of categories assigned by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1 We now illustrate the computation of P(E) according to these two methods. We will then show that the resulting κCo and κS&amp;C may straddle one of the significant thresholds used to assess the raw κ values. The assumptions underlying these two methods are made tangible in the way the data are visualized, in a contingency table for Cohen, and in what we will call an agreement table for the others. Consider the following situation. Two coders2 code 150 occurrences of Okay and assign to them one of the two labels Accept or Ack(nowledgement) (Allen an</context>
<context position="6575" citStr="Fleiss (1971)" startWordPosition="1091" endWordPosition="1092">150 differ in their labels. 1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan’s to produce a statistic called alpha. Krippendorff computes P(E) (called 1 − De in his terminology) with a sampling-without-replacement methodology. The computations of P(E) and of 1 − De show that the difference is negligible: / n\z P(E) = Ej I Nk ij I (Siegel and Castellan) �� 1 − De j Ni nij / (~ N −1 −1) (Krippendorff) 2 Both κS&amp;C (Scott 1955) and κCo (Cohen 1960) were originally devised for two coders. Each has been extended to more than two coders, for example, respectively Fleiss (1971) and Bartko and Carpenter (1976). Thus, without loss of generality, our examples involve two coders. 96 Di Eugenio and Glass Kappa: A Second Look Example 1 Coder 1 Okay1 ... Okay70 Okay71 ... Okay125 Okay126 ... Okay150 Accept Ack 2 0 2 0 0 2 0 2 1 1 1 1 Example 2 Coder 1 Coder 2 Accept Ack Accept Ack 70 0 25 55 95 55 70 80 150 Coder 2 Accept Ack Accept Ack 70 10 15 55 85 65 80 70 150 165 135 Figure 1 Cohen’s contingency tables (left) and Siegel and Castellan’s agreement table (right). Agreement tables lose information. When the coders disagree, we cannot reconstruct which coder picked which c</context>
<context position="16268" citStr="Fleiss 1971" startWordPosition="2867" endWordPosition="2868">Discussion The issue that remains open is which computation of κ to choose. Siegel and Castellan’s κS&amp;C is not affected by bias, whereas Cohen’s κCo is. However, it is 99 Computational Linguistics Volume 30, Number 1 questionable whether the assumption of equal distributions underlying κS&amp;C is appropriate for coding in discourse and dialogue work. In fact, it appears to us that it holds in few if any of the published discourse- or dialogue-tagging efforts for which κ has been computed. It is, for example, appropriate in situations in which item i may be tagged by different coders than item j (Fleiss 1971). However, κ assessments for discourse and dialogue tagging are most often performed on the same portion of the data, which has been annotated by each of a small number of annotators (between two and four). In fact, in many cases the analysis of systematic disagreements among annotators on the same portion of the data (i.e., of bias) can be used to improve the coding scheme (Wiebe, Bruce, and O’Hara 1999). To use κCo but to guard against bias, Cicchetti and Feinstein (1990) suggest that κCo be supplemented, for each coding category, by two measures of agreement, positive and negative, between </context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Fleiss, Joseph L. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald L Goldman</author>
</authors>
<title>The κ statistic [letter to the editor (in reply)].</title>
<date>1992</date>
<journal>Journal of the American Medical Association,</journal>
<volume>268</volume>
<issue>18</issue>
<contexts>
<context position="14535" citStr="Goldman 1992" startWordPosition="2565" endWordPosition="2566">) and P(E) are based on proportions, the same distributions of categories in a much larger sample, say, 10,000 items, will result in exactly the same κ values. Although this behavior follows squarely from κ’s definition, it is at odds with using κ to assess a coding scheme. From both Example 3 and Example 4 we would like to conclude that the two coders are in substantial agreement, independent of the skewed prevalence of Accept with respect to Ack in Example 3. The role of prevalence in assessing κ has been subject to heated discussion in the medical literature (Grove et al. 1981; Berry 1992; Goldman 1992). The bias problem occurs in κCo but not κS&amp;C. For κCo, P(E) is computed from each coder’s individual probabilities. Thus, the less two coders agree in their overall behavior, the fewer chance agreements are expected. But for a given value of P(A), decreasing P(E) will increase κCo, leading to the paradox that κCo increases as the coders become less similar, that is, as the marginal totals diverge in the contingency table. Consider two coders coding the usual 100 occurrences of Okay, according to the two tables in Figure 4. In Example 5, the proportions of each category are very similar among </context>
</contexts>
<marker>Goldman, 1992</marker>
<rawString>Goldman, Ronald L. 1992. The κ statistic [letter to the editor (in reply)]. Journal of the American Medical Association, 268(18):2513–2514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Grove</author>
<author>Nancy C Andreasen</author>
<author>Patricia McDonald-Scott</author>
<author>Martin B Keller</author>
<author>Robert W Shapiro</author>
</authors>
<title>Reliability studies of psychiatric diagnosis: Theory and practice.</title>
<date>1981</date>
<journal>Archives of General Psychiatry,</journal>
<pages>38--408</pages>
<contexts>
<context position="1386" citStr="Grove et al. 1981" startWordPosition="213" endWordPosition="216">al. 2000), requires coders to make subtle distinctions among categories. The objectivity of these decisions can be assessed by evaluating the reliability of the tagging, namely, whether the coders reach a satisfying level of agreement when they perform the same coding task. Currently, the de facto standard for assessing intercoder agreement is the κ coefficient, which factors out expected agreement (Cohen 1960; Krippendorff 1980). κ had long been used in content analysis and medicine (e.g., in psychiatry to assess how well students’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981). Carletta (1996) deserves the credit for bringing κ to the attention of computational linguists. κ is computed as P(A) − P(E) 1 − P(E) , where P(A) is the observed agreement among the coders, and P(E) is the expected agreement, that is, P(E) represents the probability that the coders agree by chance. The values of κ are constrained to the interval [−1,1]. A κ value of one means perfect agreement, a κ value of zero means that agreement is equal to chance, and a κ value of negative one means “perfect” disagreement. This squib addresses two issues that have been neglected in the computational li</context>
<context position="14508" citStr="Grove et al. 1981" startWordPosition="2559" endWordPosition="2562">all. As the computations of P(A) and P(E) are based on proportions, the same distributions of categories in a much larger sample, say, 10,000 items, will result in exactly the same κ values. Although this behavior follows squarely from κ’s definition, it is at odds with using κ to assess a coding scheme. From both Example 3 and Example 4 we would like to conclude that the two coders are in substantial agreement, independent of the skewed prevalence of Accept with respect to Ack in Example 3. The role of prevalence in assessing κ has been subject to heated discussion in the medical literature (Grove et al. 1981; Berry 1992; Goldman 1992). The bias problem occurs in κCo but not κS&amp;C. For κCo, P(E) is computed from each coder’s individual probabilities. Thus, the less two coders agree in their overall behavior, the fewer chance agreements are expected. But for a given value of P(A), decreasing P(E) will increase κCo, leading to the paradox that κCo increases as the coders become less similar, that is, as the marginal totals diverge in the contingency table. Consider two coders coding the usual 100 occurrences of Okay, according to the two tables in Figure 4. In Example 5, the proportions of each categ</context>
</contexts>
<marker>Grove, Andreasen, McDonald-Scott, Keller, Shapiro, 1981</marker>
<rawString>Grove, William M., Nancy C. Andreasen, Patricia McDonald-Scott, Martin B. Keller, and Robert W. Shapiro. 1981. Reliability studies of psychiatric diagnosis: Theory and practice. Archives of General Psychiatry, 38:408–413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="1201" citStr="Krippendorff 1980" startWordPosition="181" endWordPosition="182">oded corpora have acquired an increasing importance in every aspect of human-language technology. Tagging for many phenomena, such as dialogue acts (Carletta et al. 1997; Di Eugenio et al. 2000), requires coders to make subtle distinctions among categories. The objectivity of these decisions can be assessed by evaluating the reliability of the tagging, namely, whether the coders reach a satisfying level of agreement when they perform the same coding task. Currently, the de facto standard for assessing intercoder agreement is the κ coefficient, which factors out expected agreement (Cohen 1960; Krippendorff 1980). κ had long been used in content analysis and medicine (e.g., in psychiatry to assess how well students’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981). Carletta (1996) deserves the credit for bringing κ to the attention of computational linguists. κ is computed as P(A) − P(E) 1 − P(E) , where P(A) is the observed agreement among the coders, and P(E) is the expected agreement, that is, P(E) represents the probability that the coders agree by chance. The values of κ are constrained to the interval [−1,1]. A κ value of one means perfect agreement, a κ value of z</context>
<context position="4604" citStr="Krippendorff 1980" startWordPosition="730" endWordPosition="731">oo, as well as a third measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993). 1. The Computation of P(E) P(E) is the probability of agreement among coders due to chance. The literature describes two different methods for estimating a probability distribution for random assignment of categories. In the first, each coder has a personal distribution, based on that coder’s distribution of categories (Cohen 1960). In the second, there is one distribution for all coders, derived from the total proportions of categories assigned by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1 We now illustrate the computation of P(E) according to these two methods. We will then show that the resulting κCo and κS&amp;C may straddle one of the significant thresholds used to assess the raw κ values. The assumptions underlying these two methods are made tangible in the way the data are visualized, in a contingency table for Cohen, and in what we will call an agreement table for the others. Consider the following situation. Two coders2 code 150 occurrences of Okay and assign to them one of the two labels Accept or Ack(nowledgement) (Allen and Core 1997). The t</context>
<context position="8482" citStr="Krippendorff (1980)" startWordPosition="1447" endWordPosition="1448">n to the total number of items; N is the number of items, and k the number of coders (N = 150 and k = 2 in our example). Both KCo and KS&amp;C are highly significant at the p = 0.5 ∗ 10−5 level (significance is computed for KCo and KS&amp;C according to the formulas in Cohen [1960] and Siegel and Castellan [1988], respectively). The difference between KCo and KS&amp;C in Figure 2 is just under 1%, however, the results of the two K computations straddle the value 0.67, which for better or worse has been adopted as a cutoff in computational linguistics. This cutoff is based on the assessment of K values in Krippendorff (1980), which discounts K &lt; 0.67 and allows tentative conclusions when 0.67 &lt; K &lt; 0.8 and definite conclusions when K ≥ 0.8. Krippendorff’s scale has been adopted without question, even though Krippendorff himself considers it only a plausible standard that has emerged from his and his colleagues’ work. In fact, Carletta et al. (1997) use words of caution against adopting Krippendorff’s suggestion as a standard; the first author has also raised the issue of how to assess K values in Di Eugenio (2000). If Krippendorff’s scale is supposed to be our standard, the example just worked out shows that the </context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Krippendorff, Klaus. 1980. Content Analysis: An Introduction to Its Methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toni Rietveld</author>
<author>Roeland van Hout</author>
</authors>
<title>Statistical Techniques for the Study of Language and Language Behaviour. Mouton de Gruyter,</title>
<date>1993</date>
<location>Berlin.</location>
<marker>Rietveld, van Hout, 1993</marker>
<rawString>Rietveld, Toni and Roeland van Hout. 1993. Statistical Techniques for the Study of Language and Language Behaviour. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Scott</author>
</authors>
<title>Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly,</title>
<date>1955</date>
<contexts>
<context position="2202" citStr="Scott 1955" startWordPosition="355" endWordPosition="356">he expected agreement, that is, P(E) represents the probability that the coders agree by chance. The values of κ are constrained to the interval [−1,1]. A κ value of one means perfect agreement, a κ value of zero means that agreement is equal to chance, and a κ value of negative one means “perfect” disagreement. This squib addresses two issues that have been neglected in the computational linguistics literature. First, there are two main ways of computing P(E), the expected agreement, according to whether the distribution of proportions over the categories is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reflect different conceptualizations of the problem. We believe the distinction between the two is often glossed over because in practice the two computations of P(E) produce very similar outcomes in most cases, especially for the highest values of κ. However, first, we will show that they can indeed result in different values of κ, that we will call κCo (Cohen 1960) and κS&amp;C (Siegel and Castellan 1988). These different values can lead to contradictory conclusions on intercoder agreeme</context>
<context position="4572" citStr="Scott 1955" startWordPosition="726" endWordPosition="727">κS&amp;C should be reported too, as well as a third measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993). 1. The Computation of P(E) P(E) is the probability of agreement among coders due to chance. The literature describes two different methods for estimating a probability distribution for random assignment of categories. In the first, each coder has a personal distribution, based on that coder’s distribution of categories (Cohen 1960). In the second, there is one distribution for all coders, derived from the total proportions of categories assigned by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1 We now illustrate the computation of P(E) according to these two methods. We will then show that the resulting κCo and κS&amp;C may straddle one of the significant thresholds used to assess the raw κ values. The assumptions underlying these two methods are made tangible in the way the data are visualized, in a contingency table for Cohen, and in what we will call an agreement table for the others. Consider the following situation. Two coders2 code 150 occurrences of Okay and assign to them one of the two labels Accept or Ack(nowledgeme</context>
<context position="6426" citStr="Scott 1955" startWordPosition="1067" endWordPosition="1068">table in Figure 1 shows that occurrences 1 through 70 have been labeled as Accept by both coders, 71 through 125 as Ack by both coders, and 126 to 150 differ in their labels. 1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan’s to produce a statistic called alpha. Krippendorff computes P(E) (called 1 − De in his terminology) with a sampling-without-replacement methodology. The computations of P(E) and of 1 − De show that the difference is negligible: / n\z P(E) = Ej I Nk ij I (Siegel and Castellan) �� 1 − De j Ni nij / (~ N −1 −1) (Krippendorff) 2 Both κS&amp;C (Scott 1955) and κCo (Cohen 1960) were originally devised for two coders. Each has been extended to more than two coders, for example, respectively Fleiss (1971) and Bartko and Carpenter (1976). Thus, without loss of generality, our examples involve two coders. 96 Di Eugenio and Glass Kappa: A Second Look Example 1 Coder 1 Okay1 ... Okay70 Okay71 ... Okay125 Okay126 ... Okay150 Accept Ack 2 0 2 0 0 2 0 2 1 1 1 1 Example 2 Coder 1 Coder 2 Accept Ack Accept Ack 70 0 25 55 95 55 70 80 150 Coder 2 Accept Ack Accept Ack 70 10 15 55 85 65 80 70 150 165 135 Figure 1 Cohen’s contingency tables (left) and Siegel a</context>
</contexts>
<marker>Scott, 1955</marker>
<rawString>Scott, William A. 1955. Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 19:127–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N John Castellan</author>
</authors>
<title>Nonparametric statistics for the behavioral sciences.</title>
<date>1988</date>
<publisher>McGraw Hill,</publisher>
<location>Boston.</location>
<contexts>
<context position="2262" citStr="Siegel and Castellan 1988" startWordPosition="361" endWordPosition="364">nts the probability that the coders agree by chance. The values of κ are constrained to the interval [−1,1]. A κ value of one means perfect agreement, a κ value of zero means that agreement is equal to chance, and a κ value of negative one means “perfect” disagreement. This squib addresses two issues that have been neglected in the computational linguistics literature. First, there are two main ways of computing P(E), the expected agreement, according to whether the distribution of proportions over the categories is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reflect different conceptualizations of the problem. We believe the distinction between the two is often glossed over because in practice the two computations of P(E) produce very similar outcomes in most cases, especially for the highest values of κ. However, first, we will show that they can indeed result in different values of κ, that we will call κCo (Cohen 1960) and κS&amp;C (Siegel and Castellan 1988). These different values can lead to contradictory conclusions on intercoder agreement. Moreover, the assumption of ∗ Computer Science, 1120 SEO</context>
<context position="4632" citStr="Siegel and Castellan 1988" startWordPosition="732" endWordPosition="735">ird measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993). 1. The Computation of P(E) P(E) is the probability of agreement among coders due to chance. The literature describes two different methods for estimating a probability distribution for random assignment of categories. In the first, each coder has a personal distribution, based on that coder’s distribution of categories (Cohen 1960). In the second, there is one distribution for all coders, derived from the total proportions of categories assigned by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1 We now illustrate the computation of P(E) according to these two methods. We will then show that the resulting κCo and κS&amp;C may straddle one of the significant thresholds used to assess the raw κ values. The assumptions underlying these two methods are made tangible in the way the data are visualized, in a contingency table for Cohen, and in what we will call an agreement table for the others. Consider the following situation. Two coders2 code 150 occurrences of Okay and assign to them one of the two labels Accept or Ack(nowledgement) (Allen and Core 1997). The two coders label 70 occurrenc</context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Siegel, Sidney and N. John Castellan, Jr. 1988. Nonparametric statistics for the behavioral sciences. McGraw Hill, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Rebecca F Bruce</author>
<author>Thomas P O’Hara</author>
</authors>
<title>Development and use of a gold-standard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In ACL99: Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>246--253</pages>
<location>College Park, MD.</location>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>Wiebe, Janyce M., Rebecca F. Bruce, and Thomas P. O’Hara. 1999. Development and use of a gold-standard data set for subjectivity classifications. In ACL99: Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 246–253, College Park, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>