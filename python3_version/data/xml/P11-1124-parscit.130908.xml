<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.99909">
Consistent Translation using Discriminative Learning:
A Translation Memory-inspired Approach*
</title>
<author confidence="0.960891">
Yanjun Ma† Yifan He‡ Andy Way‡ Josef van Genabith‡† Baidu Inc., Beijing, China
</author>
<affiliation confidence="0.868568666666667">
yma@baidu.com
‡Centre for Next Generation Localisation
School of Computing, Dublin City University
</affiliation>
<email confidence="0.990258">
{yhe,away,josef}@computing.dcu.ie
</email>
<sectionHeader confidence="0.984453" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999742631578947">
We present a discriminative learning method
to improve the consistency of translations in
phrase-based Statistical Machine Translation
(SMT) systems. Our method is inspired by
Translation Memory (TM) systems which are
widely used by human translators in industrial
settings. We constrain the translation of an in-
put sentence using the most similar ‘transla-
tion example’ retrieved from the TM. Differ-
ently from previous research which used sim-
ple fuzzy match thresholds, these constraints
are imposed using discriminative learning to
optimise the translation performance. We ob-
serve that using this method can benefit the
SMT system by not only producing consis-
tent translations, but also improved translation
outputs. We report a 0.9 point improvement
in terms of BLEU score on English–Chinese
technical documents.
</bodyText>
<sectionHeader confidence="0.99252" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983053255319149">
Translation consistency is an important factor
for large-scale translation, especially for domain-
specific translations in an industrial environment.
For example, in the translation of technical docu-
ments, lexical as well as structural consistency is es-
sential to produce a fluent target-language sentence.
Moreover, even in the case of translation errors, con-
sistency in the errors (e.g. repetitive error patterns)
are easier to diagnose and subsequently correct by
translators.
*This work was done while the first author was in the Cen-
tre for Next Generation Localisation at Dublin City University.
In phrase-based SMT, translation models and lan-
guage models are automatically learned and/or gen-
eralised from the training data, and a translation is
produced by maximising a weighted combination of
these models. Given that global contextual informa-
tion is not normally incorporated, and that training
data is usually noisy in nature, there is no guaran-
tee that an SMT system can produce translations in
a consistent manner.
On the other hand, TM systems – widely used by
translators in industrial environments for enterprise
localisation by translators – can shed some light on
mitigating this limitation. TM systems can assist
translators by retrieving and displaying previously
translated similar ‘example’ sentences (displayed as
source-target pairs, widely called ‘fuzzy matches’ in
the localisation industry (Sikes, 2007)). In TM sys-
tems, fuzzy matches are retrieved by calculating the
similarity or the so-called ‘fuzzy match score’ (rang-
ing from 0 to 1 with 0 indicating no matches and 1
indicating a full match) between the input sentence
and sentences in the source side of the translation
memory.
When presented with fuzzy matches, translators
can then avail of useful chunks in previous transla-
tions while composing the translation of a new sen-
tence. Most translators only consider a few sen-
tences that are most similar to the current input sen-
tence; this process can inherently improve the con-
sistency of translation, given that the new transla-
tions produced by translators are likely to be similar
to the target side of the fuzzy match they have con-
sulted.
Previous research as discussed in detail in Sec-
1239
</bodyText>
<note confidence="0.994773">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1239–1248,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.996811235294118">
tion 2 has focused on using fuzzy match score as
a threshold when using the target side of the fuzzy
matches to constrain the translation of the input
sentence. In our approach, we use a more fine-
grained discriminative learning method to determine
whether the target side of the fuzzy matches should
be used as a constraint in translating the input sen-
tence. We demonstrate that our method can consis-
tently improve translation quality.
The rest of the paper is organized as follows:
we begin by briefly introducing related research in
Section 2. We present our discriminative learning
method for consistent translation in Section 3 and
our feature design in Section 4. We report the exper-
imental results in Section 5 and conclude the paper
and point out avenues for future research in Section
6.
</bodyText>
<sectionHeader confidence="0.995377" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.999942368421053">
Despite the fact that TM and MT integration has
long existed as a major challenge in the localisation
industry, it has only recently received attention in
main-stream MT research. One can loosely combine
TM and MT at sentence (called segments in TMs)
level by choosing one of them (or both) to recom-
mend to the translators using automatic classifiers
(He et al., 2010), or simply using fuzzy match score
or MT confidence measures (Specia et al., 2009).
One can also tightly integrate TM with MT at the
sub-sentence level. The basic idea is as follows:
given a source sentence to translate, we firstly use
a TM system to retrieve the most similar ‘example’
source sentences together with their translations. If
matched chunks between input sentence and fuzzy
matches can be detected, we can directly re-use the
corresponding parts of the translation in the fuzzy
matches, and use an MT system to translate the re-
maining chunks.
As a matter of fact, implementing this idea is
pretty straightforward: a TM system can easily de-
tect the word alignment between the input sentence
and the source side of the fuzzy match by retracing
the paths used in calculating the fuzzy match score.
To obtain the translation for the matched chunks, we
just require the word alignment between source and
target TM matches, which can be addressed using
state-of-the-art word alignment techniques. More
importantly, albeit not explicitly spelled out in pre-
vious work, this method can potentially increase the
consistency of translation, as the translation of new
input sentences is closely informed and guided (or
constrained) by previously translated sentences.
There are several different ways of using the
translation information derived from fuzzy matches,
with the following two being the most widely
adopted: 1) to add these translations into a phrase
table as in (Bic¸ici and Dymetman, 2008; Simard and
Isabelle, 2009), or 2) to mark up the input sentence
using the relevant chunk translations in the fuzzy
match, and to use an MT system to translate the parts
that are not marked up, as in (Smith and Clark, 2009;
Koehn and Senellart, 2010; Zhechev and van Gen-
abith, 2010). It is worth mentioning that translation
consistency was not explicitly regarded as their pri-
mary motivation in this previous work. Our research
follows the direction of the second strand given that
consistency can no longer be guaranteed by con-
structing another phrase table.
However, to categorically reuse the translations
of matched chunks without any differentiation could
generate inferior translations given the fact that the
context of these matched chunks in the input sen-
tence could be completely different from the source
side of the fuzzy match. To address this problem,
both (Koehn and Senellart, 2010) and (Zhechev and
van Genabith, 2010) used fuzzy match score as a
threshold to determine whether to reuse the transla-
tions of the matched chunks. For example, (Koehn
and Senellart, 2010) showed that reusing these trans-
lations as large rules in a hierarchical system (Chi-
ang, 2005) can be beneficial when the fuzzy match
score is above 70%, while (Zhechev and van Gen-
abith, 2010) reported that it is only beneficial to a
phrase-based system when the fuzzy match score is
above 90%.
Despite being an informative measure, using
fuzzy match score as a threshold has a number of
limitations. Given the fact that fuzzy match score
is normally calculated based on Edit Distance (Lev-
enshtein, 1966), a low score does not necessarily
imply that the fuzzy match is harmful when used
to constrain an input sentence. For example, in
longer sentences where fuzzy match scores tend to
be low, some chunks and the corresponding trans-
lations within the sentences can still be useful. On
</bodyText>
<equation confidence="0.997476076923077">
1240
in (1):
1
wTw + C
ξi
2
�l
i=1
min
w,b,g
s. t. yi(wTφ(xi) + b) &gt; 1 − ξi
ξi &gt; 0
(1)
</equation>
<bodyText confidence="0.998545">
the other hand, a high score cannot fully guarantee
the usefulness of a particular translation. We address
this problem using discriminative learning.
</bodyText>
<sectionHeader confidence="0.9553635" genericHeader="method">
3 Constrained Translation with
Discriminative Learning
</sectionHeader>
<subsectionHeader confidence="0.999914">
3.1 Formulation of the Problem
</subsectionHeader>
<bodyText confidence="0.980662846153846">
Given a sentence e to translate, we retrieve the most
similar sentence e′ from the translation memory as-
sociated with target translation f′. The m com-
mon “phrases” ¯eim between e and e′ can be iden-
tified. Given the word alignment information be-
tween e′ and f′, one can easily obtain the corre-
sponding translations ¯f′im for each of the phrases in
¯eim . This process can derive a number of “phrase
′m &gt;, which can be used to specify
the translations of the matched phrases in the input
sentence. The remaining words without specified
translations will be translated by an MT system.
For example, given an input sentence e1e2 · · ·
</bodyText>
<equation confidence="0.98885875">
eiei+1 · · · eI, and a phrase pair &lt; e, �f′ &gt;, e� =
eiei+1, �f′ = f′jf′j+1 derived from the fuzzy match,
we can mark up the input sentence as:
e1e2 ··· &lt;tm=“f′jf′j+1”&gt; eiei+1 &lt; /tm&gt; ··· eI.
</equation>
<bodyText confidence="0.9999645">
Our method to constrain the translations using
TM fuzzy matches is similar to (Koehn and Senel-
lart, 2010), except that the word alignment between
e′ and f′ is the intersection of bidirectional GIZA++
(Och and Ney, 2003) posterior alignments. We use
the intersected word alignment to minimise the noise
introduced by word alignment of only one direction
in marking up the input sentence.
</bodyText>
<subsectionHeader confidence="0.999202">
3.2 Discriminative Learning
</subsectionHeader>
<bodyText confidence="0.999993">
Whether the translation information from the fuzzy
matches should be used or not (i.e. whether the input
sentence should be marked up) is determined using
a discriminative learning procedure. The translation
information refers to the “phrase pairs” derived us-
ing the method described in Section 3.1. We cast
this problem as a binary classification problem.
</bodyText>
<subsectionHeader confidence="0.964575">
3.2.1 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.999904833333333">
SVMs (Cortes and Vapnik, 1995) are binary classi-
fiers that classify an input instance based on decision
rules which minimise the regularised error function
where (xi, yi) ∈ Rn × {+1, −1} are l training in-
stances that are mapped by the function φ to a higher
dimensional space. w is the weight vector, ξ is the
relaxation variable and C &gt; 0 is the penalty param-
eter.
Solving SVMs is viable using a kernel function
K in (1) with K(xi, xj) = b(xi)T b(xj). We per-
form our experiments with the Radial Basis Func-
tion (RBF) kernel, as in (2):
</bodyText>
<equation confidence="0.865158">
K(xi, xj) = exp(−γ||xi − xj||2), γ &gt; 0 (2)
</equation>
<bodyText confidence="0.987811692307692">
When using SVMs with the RBF kernel, we have
two free parameters to tune on: the cost parameter
C in (1) and the radius parameter γ in (2).
In each of our experimental settings, the param-
eters C and γ are optimised by a brute-force grid
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
The SVM classifier will thus be able to predict
the usefulness of the TM fuzzy match, and deter-
mine whether the input sentence should be marked
up using relevant phrase pairs derived from the fuzzy
match before sending it to the SMT system for trans-
lation. The classifier uses features such as the fuzzy
match score, the phrase and lexical translation prob-
abilities of these relevant phrase pairs, and addi-
tional syntactic dependency features. Ideally the
classifier will decide to mark up the input sentence
if the translations of the marked phrases are accurate
when taken contextual information into account. As
large-scale manually annotated data is not available
for this task, we use automatic TER scores (Snover
et al., 2006) as the measure for training data annota-
tion.
We label the training examples as in (3):
�
</bodyText>
<equation confidence="0.99754075">
+1 if TER(w. markup) &lt; TER(w/o markup)
y=
−1 if TER(w/o markup) ≥ TER(w. markup)
(3)
</equation>
<bodyText confidence="0.45205675">
Each instance is associated with a set of features
which are discussed in more detail in Section 4.
pairs” &lt; em, f
1241
</bodyText>
<subsectionHeader confidence="0.557734">
3.2.2 Classification Confidence Estimation
</subsectionHeader>
<bodyText confidence="0.999972833333333">
We use the techniques proposed by (Platt, 1999) and
improved by (Lin et al., 2007) to convert classifica-
tion margin to posterior probability, so that we can
easily threshold our classifier (cf. Section 5.4.2).
Platt’s method estimates the posterior probability
with a sigmoid function, as in (4):
</bodyText>
<equation confidence="0.9984875">
1
Pr(y = 1|z) ,: PA,B(f) =1 + exp(Af + B) (4)
</equation>
<bodyText confidence="0.99989">
where f = f(z) is the decision function of the esti-
mated SVM. A and B are parameters that minimise
the cross-entropy error function F on the training
data, as in (5):
</bodyText>
<equation confidence="0.9995135">
(tilog(pi) + (1 − ti)log(1 − pi)),
N++2 if yi = +1
</equation>
<bodyText confidence="0.720835">
where pi = PA,B (fi), and ti = i
</bodyText>
<equation confidence="0.797919">
IN +2 if yi = −1
(5)
</equation>
<bodyText confidence="0.999973375">
where z = (A, B) is a parameter setting, and
N+ and N− are the numbers of observed positive
and negative examples, respectively, for the label yi.
These numbers are obtained using an internal cross-
validation on the training set.
where e is the sentence to translate, and s is the
source side of an entry in the TM. For fuzzy match
scores F, hfm roughly corresponds to 1 − F.
</bodyText>
<subsectionHeader confidence="0.995011">
4.2 Translation Features
</subsectionHeader>
<bodyText confidence="0.999678947368421">
We use four features related to translation probabil-
ities, i.e. the phrase translation and lexical probabil-
ities for the phrase pairs &lt; em, f′m &gt; derived us-
ing the method in Section 3.1. Specifically, we use
the phrase translation probabilities p(�f′m|em) and
′m), as well as the lexical translation prob-
abilities plex( f′m|�em) and plex(�em |f′m) as calcu-
lated in (Koehn et al., 2003). In cases where mul-
tiple phrase pairs are used to mark up one single
input sentence e, we use a unified score for each
of the four features, which is an average over the
corresponding feature in each phrase pair. The intu-
ition behind these features is as follows: phrase pairs
&lt; em, f′m &gt; derived from the fuzzy match should
also be reliable with respect to statistically produced
models.
We also have a count feature, i.e. the number of
phrases used to mark up the input sentence, and a
binary feature, i.e. whether the phrase table contains
</bodyText>
<equation confidence="0.884446714285714">
�l
i=1
F(z) = −
min
z=(A,B)
f
p(�em|
</equation>
<bodyText confidence="0.98927">
4 Feature Set at least one phrase pair &lt; em, f′m &gt; that is used to
The features used to train the discriminative classi- mark up the input sentence. f′m &gt; derived from
fier, all on the sentence level, are described in the 4.3 Dependency Features
following sections. Given the phrase pairs &lt; em,
</bodyText>
<subsectionHeader confidence="0.993945">
4.1 The TM Feature
</subsectionHeader>
<bodyText confidence="0.999899875">
The TM feature is the fuzzy match score, which in-
dicates the overall similarity between the input sen-
tence and the source side of the TM output. If the
input sentence is similar to the source side of the
matching segment, it is more likely that the match-
ing segment can be used to mark up the input sen-
tence.
The calculation of the fuzzy match score itself is
one of the core technologies in TM systems, and
varies among different vendors. We compute fuzzy
match cost as the minimum Edit Distance (Leven-
shtein, 1966) between the source and TM entry, nor-
malised by the length of the source as in (6), as
most of the current implementations are based on
edit distance while allowing some additional flexi-
ble matching.
</bodyText>
<equation confidence="0.9960915">
EditDistance(e, s) (6)
Len(e)
</equation>
<bodyText confidence="0.999976333333333">
the fuzzy match, and used to translate the corre-
sponding chunks of the input sentence (cf. Sec-
tion 3.1), these translations are more likely to be co-
herent in the context of the particular input sentence
if the matched parts on the input side are syntacti-
cally and semantically related.
For matched phrases em between the input sen-
tence and the source side of the fuzzy match, we de-
fine the contextual information of the input side us-
ing dependency relations between words em in em
and the remaining words ej in the input sentence e.
We use the Stanford parser to obtain the depen-
dency structure of the input sentence. We add
a pseudo-label SYS PUNCT to punctuation marks,
whose governor and dependent are both the punc-
tuation mark. The dependency features designed to
capture the context of the matched input phrases em
are as follows:
</bodyText>
<equation confidence="0.969888">
hf,,,,(e) = min
�
1242
</equation>
<bodyText confidence="0.999962793103448">
Coverage features measure the coverage of de-
pendency labels on the input sentence in order to
obtain a bigger picture of the matched parts in the
input. For each dependency label L, we consider its
head or modifier as covered if the corresponding in-
put word em is covered by a matched phrase em.
Our coverage features are the frequencies of gov-
ernor and dependent coverage calculated separately
for each dependency label.
Position features identify whether the head and
the tail of a sentence are matched, as these are the
cases in which the matched translation is not af-
fected by the preceding words (when it is the head)
or following words (when it is the tail), and is there-
fore more reliable. The feature is set to 1 if this hap-
pens, and to 0 otherwise. We distinguish among the
possible dependency labels, the head or the tail of
the sentence, and whether the aligned word is the
governor or the dependent. As a result, each per-
mutation of these possibilities constitutes a distinct
binary feature.
The consistency feature is a single feature which
determines whether matched phrases em belong to
a consistent dependency structure, instead of being
distributed discontinuously around in the input sen-
tence. We assume that a consistent structure is less
influenced by its surrounding context. We set this
feature to 1 if every word in em is dependent on an-
other word in em, and to 0 otherwise.
</bodyText>
<sectionHeader confidence="0.999562" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.993034">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.993581357142857">
Our data set is an English–Chinese translation mem-
ory with technical translation from Symantec, con-
sisting of 87K sentence pairs. The average sentence
length of the English training set is 13.3 words and
the size of the training set is comparable to the larger
TMs used in the industry. Detailed corpus statistics
about the training, development and test sets for the
SMT system are shown in Table 1.
The composition of test subsets based on fuzzy
match scores is shown in Table 2. We can see that
sentences in the test sets are longer than those in the
training data, implying a relatively difficult trans-
lation task. We train the SVM classifier using the
libSVM (Chang and Lin, 2001) toolkit. The SVM-
</bodyText>
<table confidence="0.999734666666667">
Train Develop Test
SENTENCES 86,602 762 943
ENG. TOKENS 1,148,126 13,955 20,786
ENG. VOC. 13,074 3,212 3,115
CHI. TOKENS 1,171,322 10,791 16,375
CHI. VOC. 12,823 3,212 1,431
</table>
<tableCaption confidence="0.935253">
Table 1: Corpus Statistics
</tableCaption>
<table confidence="0.999910142857143">
Scores Sentences Words W/S
(0.9, 1.0) 80 1526 19.0750
(0.8, 0.9] 96 1430 14.8958
(0.7, 0.8] 110 1596 14.5091
(0.6, 0.7] 74 1031 13.9324
(0.5, 0.6] 104 1811 17.4135
(0, 0.5] 479 8972 18.7307
</table>
<tableCaption confidence="0.9980405">
Table 2: Composition of test subsets based on fuzzy
match scores
</tableCaption>
<bodyText confidence="0.999969476190476">
training and validation is on the same training sen-
tences1 as the SMT system with 5-fold cross valida-
tion.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 5-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameter C in (1), we search in the range
[2−5, 215], while for parameter -y (2) we search in the
range [2−15, 23]. The step size is 2 on the exponent.
We conducted experiments using a standard log-
linear PB-SMT model: GIZA++ implementation of
IBM word alignment model 4 (Och and Ney, 2003),
the refinement and phrase-extraction heuristics de-
scribed in (Koehn et al., 2003), minimum-error-
rate training (Och, 2003), a 5-gram language model
with Kneser-Ney smoothing (Kneser and Ney, 1995)
trained with SRILM (Stolcke, 2002) on the Chinese
side of the training data, and Moses (Koehn et al.,
2007) which is capable of handling user-specified
translations for some portions of the input during de-
coding. The maximum phrase length is set to 7.
</bodyText>
<subsectionHeader confidence="0.989486">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.964252875">
The performance of the phrase-based SMT system
is measured by BLEU score (Papineni et al., 2002)
and TER (Snover et al., 2006). Significance test-
1We have around 87K sentence pairs in our training data.
However, for 67.5% of the input sentences, our MT system pro-
duces the same translation irrespective of whether the input sen-
tence is marked up or not.
1243
ing is carried out using approximate randomisation
(Noreen, 1989) with a 95% confidence level.
We also measure the quality of the classification
by precision and recall. Let A be the set of pre-
dicted markup input sentences, and B be the set
of input sentences where the markup version has a
lower TER score than the plain version. We stan-
dardly define precision P and recall R as in (7):
</bodyText>
<subsectionHeader confidence="0.98597">
5.3 Cross-fold translation
</subsectionHeader>
<bodyText confidence="0.999996714285714">
In order to obtain training samples for the classifier,
we need to label each sentence in the SMT training
data as to whether marking up the sentence can pro-
duce better translations. To achieve this, we translate
both the marked-up versions and plain versions of
the sentence and compare the two translations using
the sentence-level evaluation metric TER.
We do not make use of additional training data to
translate the sentences for SMT training, but instead
use cross-fold translation. We create a new training
corpus T by keeping 95% of the sentences in the
original training corpus, and creating a new test cor-
pus H by using the remaining 5% of the sentences.
Using this scheme we make 20 different pairs of cor-
pora (Ti, Hi) in such a way that each sentence from
the original training corpus is in exactly one Hi for
some 1 ≤ i ≤ 20. We train 20 different systems
using each Ti, and use each system to translate the
corresponding Hi as well as the marked-up version
of Hi using the procedure described in Section 3.1.
The development set is kept the same for all systems.
</bodyText>
<subsectionHeader confidence="0.970883">
5.4 Experimental Results
5.4.1 Translation Results
</subsectionHeader>
<bodyText confidence="0.995905272727273">
Table 3 contains the translation results of the SMT
system when we use discriminative learning to mark
up the input sentence (MARKUP-DL). The first row
(BASELINE) is the result of translating plain test
sets without any markup, while the second row is
the result when all the test sentences are marked
up. We also report the oracle scores, i.e. the up-
perbound of using our discriminative learning ap-
proach. As we can see from this table, we obtain sig-
nificantly inferior results compared to the the Base-
line system if we categorically mark up all the in-
</bodyText>
<table confidence="0.9907352">
TER BLEU
BASELINE 39.82 45.80
MARKUP 41.62 44.41
MARKUP-DL 39.61 46.46
ORACLE 37.27 48.32
</table>
<tableCaption confidence="0.999908">
Table 3: Performance of Discriminative Learning (%)
</tableCaption>
<bodyText confidence="0.999965058823529">
put sentences using phrase pairs derived from fuzzy
matches. This is reflected by an absolute 1.4 point
drop in BLEU score and a 1.8 point increase in TER.
On the other hand, both the oracle BLEU and TER
scores represent as much as a 2.5 point improve-
ment over the baseline. Our discriminative learning
method (MARKUP-DL), which automatically clas-
sifies whether an input sentence should be marked
up, leads to an increase of 0.7 absolute BLEU points
over the BASELINE, which is statistically signifi-
cant. We also observe a slight decrease in TER com-
pared to the BASELINE. Despite there being much
room for further improvement when compared to the
Oracle score, the discriminative learning method ap-
pears to be effective not only in maintaining transla-
tion consistency, but also a statistically significant
improvement in translation quality.
</bodyText>
<subsubsectionHeader confidence="0.762506">
5.4.2 Classification Confidence Thresholding
</subsubsectionHeader>
<bodyText confidence="0.999992952380953">
To further analyse our discriminative learning ap-
proach, we report the classification results on the test
set using the SVM classifier. We also investigate the
use of classification confidence, as described in Sec-
tion 3.2.2, as a threshold to boost classification pre-
cision if required. Table 4 shows the classification
and translation results when we use different con-
fidence thresholds. The default classification con-
fidence is 0.50, and the corresponding translation
results were described in Section 5.4.1. We inves-
tigate the impact of increasing classification confi-
dence on the performance of the classifier and the
translation results. As can be seen from Table 4,
increasing the classification confidence up to 0.70
leads to a steady increase in classification precision
with a corresponding sacrifice in recall. The fluc-
tuation in classification performance has an impact
on the translation results as measured by BLEU and
TER. We can see that the best BLEU as well as TER
scores are achieved when we set the classification
confidence to 0.60, representing a modest improve-
</bodyText>
<equation confidence="0.85107025">
P = |A n B ||A n B|
|A |, R = (7)
|B|
1244
</equation>
<table confidence="0.991037666666667">
Classification Confidence
0.50 0.55 0.60 0.65 0.70 0.75 0.80
BLEU 46.46 46.65 46.69 46.59 46.34 46.06 46.00
TER 39.61 39.46 39.32 39.36 39.52 39.71 39.71
P 60.00 68.67 70.31 74.47 72.97 64.28 88.89
R 32.14 29.08 22.96 17.86 13.78 9.18 4.08
</table>
<tableCaption confidence="0.999944">
Table 4: The impact of classification confidence thresholding
</tableCaption>
<bodyText confidence="0.999946818181818">
ment over the default setting (0.50). Despite the
higher precision when the confidence is set to 0.7,
the dramatic decrease in recall cannot be compen-
sated for by the increase in precision.
We can also observe from Table 4 that the recall
is quite low across the board, and the classification
results become unstable when we further increase
the level of confidence to above 0.70. This indicates
the degree of difficulty of this classification task, and
suggests some directions for future research as dis-
cussed at the end of this paper.
</bodyText>
<subsectionHeader confidence="0.916055">
5.4.3 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.999492142857143">
As discussed in Section 2, both (Koehn and Senel-
lart, 2010) and (Zhechev and van Genabith, 2010)
used fuzzy match score to determine whether the in-
put sentences should be marked up. The input sen-
tences are only marked up when the fuzzy match
score is above a certain threshold. We present the
results using this method in Table 5. From this ta-
</bodyText>
<table confidence="0.95779175">
Fuzzy Match Scores
0.50 0.60 0.70 0.80 0.90
BLEU 45.13 45.55 45.58 45.84 45.82
TER 40.99 40.62 40.56 40.29 40.07
</table>
<tableCaption confidence="0.983161">
Table 5: Performance using fuzzy match score for classi-
fication
</tableCaption>
<bodyText confidence="0.999906833333334">
ble, we can see an inferior performance compared to
the BASELINE results (cf. Table 3) when the fuzzy
match score is below 0.70. A modest gain can only
be achieved when the fuzzy match score is above
0.8. This is slightly different from the conclusions
drawn in (Koehn and Senellart, 2010), where gains
are observed when the fuzzy match score is above
0.7, and in (Zhechev and van Genabith, 2010) where
gains are only observed when the score is above 0.9.
Comparing Table 5 with Table 4, we can see that
our classification method is more effective. This
confirms our argument in the last paragraph of Sec-
tion 2, namely that fuzzy match score is not informa-
tive enough to determine the usefulness of the sub-
sentences in a fuzzy match, and that a more compre-
hensive set of features, as we have explored in this
paper, is essential for the discriminative learning-
based method to work.
</bodyText>
<table confidence="0.998429">
FM Scores w. markup w/o markup
[0,0.5] 37.75 62.24
(0.5,0.6] 40.64 59.36
(0.6,0.7] 40.94 59.06
(0.7,0.8] 46.67 53.33
(0.8,0.9] 54.28 45.72
(0.9,1.0] 44.14 55.86
</table>
<tableCaption confidence="0.977433666666667">
Table 6: Percentage of training sentences with markup
vs without markup grouped by fuzzy match (FM) score
ranges
</tableCaption>
<bodyText confidence="0.999981647058823">
To further validate our assumption, we analyse
the training sentences by grouping them accord-
ing to their fuzzy match score ranges. For each
group of sentences, we calculate the percentage of
sentences where markup (and respectively without
markup) can produce better translations. The statis-
tics are shown in Table 6. We can see that for sen-
tences with fuzzy match scores lower than 0.8, more
sentences can be better translated without markup.
For sentences where fuzzy match scores are within
the range (0.8, 0.9], more sentences can be better
translated with markup. However, within the range
(0.9, 1.0], surprisingly, actually more sentences re-
ceive better translation without markup. This indi-
cates that fuzzy match score is not a good measure to
predict whether fuzzy matches are beneficial when
used to constrain the translation of an input sentence.
</bodyText>
<subsectionHeader confidence="0.99877">
5.5 Contribution of Features
</subsectionHeader>
<bodyText confidence="0.999466333333333">
We also investigated the contribution of our differ-
ent feature sets. We are especially interested in
the contribution of dependency features, as they re-
</bodyText>
<equation confidence="0.858019333333333">
1245
Example 1
w/o markup after policy name , type the name of the policy ( it shows new host integrity
policy by default) .
Translation 在 “ 策略 ” 名称 后面 , its. 策略 0 名称 ( 名称 &amp;示 ;91 “ � 1机 完#性
策略 AVI ) 。
w. markup after policy name &lt;tm translation=“, its. 策略 名称 ( AVI &amp;示 “ �
1机 完#性 策略 ” ) 。”&gt;, type the name of the policy ( it shows new host
integrity policy by default) .&lt; /tm&gt;
Translation 在 “ 策略 ” 名称 后面 , its. 策略 名称 ( AVI &amp;示 “ � 1机 完#性 策略 ” ) 。
Reference 在 “ 策略 名称 ” 后面 , its. 策略 名称 ( AVI &amp;示 “ � 1机 完#性 策略 ” ) 。
Example 2
</equation>
<table confidence="0.9595784">
w/o markup changes apply only to the specific scan that you select.
Translation kA IR it用t 特定 4a- 0 规则 。
w. markup changes apply only to the specific scan that you select &lt;tm translation=“。”&gt;.&lt; /tm&gt;
Translation kA IR it用t 您 A 0 特定 4a- 。
Reference kA 只 应用t 您 A 0 特定 4a- 。
flect whether translation consistency can be captured
using syntactic knowledge. The classification and
TER BLEU P R
TM+TRANS 40.57 45.51 52.48 27.04
+DEP 39.61 46.46 60.00 32.14
</table>
<tableCaption confidence="0.999593">
Table 7: Contribution of Features (%)
</tableCaption>
<bodyText confidence="0.999631230769231">
translation results using different features are re-
ported in Table 7. We observe a significant improve-
ment in both classification precision and recall by
adding dependency (DEP) features on top of TM
and translation features. As a result, the translation
quality also significantly improves. This indicates
that dependency features which can capture struc-
tural and semantic similarities are effective in gaug-
ing the usefulness of the phrase pairs derived from
the fuzzy matches. Note also that without including
the dependency features, our discriminative learning
method cannot outperform the BASELINE (cf. Ta-
ble 3) in terms of translation quality.
</bodyText>
<subsectionHeader confidence="0.960649">
5.6 Improved Translations
</subsectionHeader>
<bodyText confidence="0.999989647058823">
In order to pinpoint the sources of improvements by
marking up the input sentence, we performed some
manual analysis of the output. We observe that the
improvements can broadly be attributed to two rea-
sons: 1) the use of long phrase pairs which are miss-
ing in the phrase table, and 2) deterministically using
highly reliable phrase pairs.
Phrase-based SMT systems normally impose a
limit on the length of phrase pairs for storage and
speed considerations. Our method can overcome
this limitation by retrieving and reusing long phrase
pairs on the fly. A similar idea, albeit from a dif-
ferent perspective, was explored by (Lopez, 2008),
where he proposed to construct a phrase table on the
fly for each sentence to be translated. Differently
from his approach, our method directly translates
part of the input sentence using fuzzy matches re-
trieved on the fly, with the rest of the sentence trans-
lated by the pre-trained MT system. We offer some
more insights into the advantages of our method by
means of a few examples.
Example 1 shows translation improvements by
using long phrase pairs. Compared to the refer-
ence translation, we can see that for the underlined
phrase, the translation without markup contains (i)
word ordering errors and (ii) a missing right quota-
tion mark. In Example 2, by specifying the transla-
tion of the final punctuation mark, the system cor-
rectly translates the relative clause ‘that you select’.
The translation of this relative clause is missing
when translating the input without markup. This
improvement can be partly attributed to the reduc-
tion in search errors by specifying the highly reliable
translations for phrases in an input sentence.
</bodyText>
<sectionHeader confidence="0.994275" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.990231957446809">
In this paper, we introduced a discriminative learn-
ing method to tightly integrate fuzzy matches re-
trieved using translation memory technologies with
phrase-based SMT systems to improve translation
consistency. We used an SVM classifier to predict
whether phrase pairs derived from fuzzy matches
could be used to constrain the translation of an in-
1246
put sentence. A number of feature functions includ-
ing a series of novel dependency features were used
to train the classifier. Experiments demonstrated
that discriminative learning is effective in improving
translation quality and is more informative than the
fuzzy match score used in previous research. We re-
port a statistically significant 0.9 absolute improve-
ment in BLEU score using a procedure to promote
translation consistency.
As mentioned in Section 2, the potential improve-
ment in sentence-level translation consistency us-
ing our method can be attributed to the fact that
the translation of new input sentences is closely in-
formed and guided (or constrained) by previously
translated sentences using global features such as
dependencies. However, it is worth noting that
the level of gains in translation consistency is also
dependent on the nature of the TM itself; a self-
contained coherent TM would facilitate consistent
translations. In the future, we plan to investigate
the impact of TM quality on translation consistency
when using our approach. Furthermore, we will ex-
plore methods to promote translation consistency at
document level.
Moreover, we also plan to experiment with
phrase-by-phrase classification instead of sentence-
by-sentence classification presented in this paper,
in order to obtain more stable classification results.
We also plan to label the training examples using
other sentence-level evaluation metrics such as Me-
teor (Banerjee and Lavie, 2005), and to incorporate
features that can measure syntactic similarities in
training the classifier, in the spirit of (Owczarzak et
al., 2007). Currently, only a standard phrase-based
SMT system is used, so we plan to test our method
on a hierarchical system (Chiang, 2005) to facilitate
direct comparison with (Koehn and Senellart, 2010).
We will also carry out experiments on other data sets
and for more language pairs.
</bodyText>
<sectionHeader confidence="0.995465" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999025">
This work is supported by Science Foundation Ire-
land (Grant No 07/CE/I1142) and part funded under
FP7 of the EC within the EuroMatrix+ project (grant
No 231720). The authors would like to thank the
reviewers for their insightful comments and sugges-
tions.
</bodyText>
<sectionHeader confidence="0.981946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998688836065574">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65–72, Ann Arbor, MI.
Ergun Bic¸ici and Marc Dymetman. 2008. Dynamic
translation memory: Using statistical machine trans-
lation to improve translation memory. In Proceedings
of the 9th Internation Conference on Intelligent Text
Processing and Computational Linguistics (CICLing),
pages 454–465, Haifa, Israel.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/˜cjlin/libsvm.
David Chiang. 2005. A hierarchical Phrase-Based model
for Statistical Machine Translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL’05), pages 263–270, Ann
Arbor, MI.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging SMT and TM with translation
recommendation. In Proceedings of the 48th Annual
Meeting ofthe Association for Computational Linguis-
tics, pages 622–630, Uppsala, Sweden.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume 1,
pages 181–184, Detroit, MI.
Philipp Koehn and Jean Senellart. 2010. Convergence of
translation memory and statistical machine translation.
In Proceedings of AMTA Workshop on MT Research
and the Translation Industry, pages 21–31, Denver,
CO.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of the 2003 Human Language Technology Conference
and the North American Chapter of the Association
for Computational Linguistics, pages 48–54, Edmon-
ton, AB, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
1247
ume Proceedings of the Demo and Poster Sessions,
pages 177–180, Prague, Czech Republic.
Vladimir Iosifovich Levenshtein. 1966. Binary codes ca-
pable of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707–710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt’s probabilistic outputs for support vec-
tor machines. Machine Learning, 68(3):267–276.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505–512, Manchester, UK, August.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19–51.
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 160–167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 104–111,
Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311–318, Philadelphia, PA.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classifiers,
pages 61–74.
Richard Sikes. 2007. Fuzzy matching in theory and prac-
tice. Multilingual, 18(6):39–43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation
environment. In Proceedings of the Twelfth Machine
Translation Summit (MT Summit XII), pages 120 –
127, Ottawa, Ontario, Canada.
James Smith and Stephen Clark. 2009. EBMT for SMT:
A new EBMT-SMT hybrid. In Proceedings of the 3rd
International Workshop on Example-Based Machine
Translation, pages 3–10, Dublin, Ireland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociationfor Machine Translation
in the Americas (AMTA-2006), pages 223–231, Cam-
bridge, MA, USA.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
confidence of machine translation quality estimates.
In Proceedings of the Twelfth Machine Translation
Summit (MT Summit XII), pages 136 – 143, Ottawa,
Ontario, Canada.
Andreas Stolcke. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, CO.
Ventsislav Zhechev and Josef van Genabith. 2010.
Seeding statistical machine translation with translation
memory output through tree-based structural align-
ment. In Proceedings of the Fourth Workshop on Syn-
tax and Structure in Statistical Translation, pages 43–
51, Beijing, China.
</reference>
<page confidence="0.706357">
1248
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.408076">
<title confidence="0.998922">Consistent Translation using Discriminative Learning: Translation Memory-inspired</title>
<author confidence="0.841428">Yifan Andy Josef van_Inc</author>
<author confidence="0.841428">Beijing</author>
<affiliation confidence="0.658914">for Next Generation School of Computing, Dublin City</affiliation>
<abstract confidence="0.99747295">We present a discriminative learning method to improve the consistency of translations in phrase-based Statistical Machine Translation (SMT) systems. Our method is inspired by Translation Memory (TM) systems which are widely used by human translators in industrial settings. We constrain the translation of an input sentence using the most similar ‘translation example’ retrieved from the TM. Differently from previous research which used simple fuzzy match thresholds, these constraints are imposed using discriminative learning to optimise the translation performance. We observe that using this method can benefit the SMT system by not only producing consistent translations, but also improved translation outputs. We report a 0.9 point improvement in terms of BLEU score on English–Chinese technical documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="33009" citStr="Banerjee and Lavie, 2005" startWordPosition="5567" endWordPosition="5570">of the TM itself; a selfcontained coherent TM would facilitate consistent translations. In the future, we plan to investigate the impact of TM quality on translation consistency when using our approach. Furthermore, we will explore methods to promote translation consistency at document level. Moreover, we also plan to experiment with phrase-by-phrase classification instead of sentenceby-sentence classification presented in this paper, in order to obtain more stable classification results. We also plan to label the training examples using other sentence-level evaluation metrics such as Meteor (Banerjee and Lavie, 2005), and to incorporate features that can measure syntactic similarities in training the classifier, in the spirit of (Owczarzak et al., 2007). Currently, only a standard phrase-based SMT system is used, so we plan to test our method on a hierarchical system (Chiang, 2005) to facilitate direct comparison with (Koehn and Senellart, 2010). We will also carry out experiments on other data sets and for more language pairs. Acknowledgments This work is supported by Science Foundation Ireland (Grant No 07/CE/I1142) and part funded under FP7 of the EC within the EuroMatrix+ project (grant No 231720). Th</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Marc Dymetman</author>
</authors>
<title>Dynamic translation memory: Using statistical machine translation to improve translation memory.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Internation Conference on Intelligent Text Processing and Computational Linguistics (CICLing),</booktitle>
<pages>454--465</pages>
<location>Haifa,</location>
<marker>Bic¸ici, Dymetman, 2008</marker>
<rawString>Ergun Bic¸ici and Marc Dymetman. 2008. Dynamic translation memory: Using statistical machine translation to improve translation memory. In Proceedings of the 9th Internation Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 454–465, Haifa, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.</title>
<date>2001</date>
<tech>tw/˜cjlin/libsvm.</tech>
<contexts>
<context position="18085" citStr="Chang and Lin, 2001" startWordPosition="3051" endWordPosition="3054">cal translation from Symantec, consisting of 87K sentence pairs. The average sentence length of the English training set is 13.3 words and the size of the training set is comparable to the larger TMs used in the industry. Detailed corpus statistics about the training, development and test sets for the SMT system are shown in Table 1. The composition of test subsets based on fuzzy match scores is shown in Table 2. We can see that sentences in the test sets are longer than those in the training data, implying a relatively difficult translation task. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. The SVMTrain Develop Test SENTENCES 86,602 762 943 ENG. TOKENS 1,148,126 13,955 20,786 ENG. VOC. 13,074 3,212 3,115 CHI. TOKENS 1,171,322 10,791 16,375 CHI. VOC. 12,823 3,212 1,431 Table 1: Corpus Statistics Scores Sentences Words W/S (0.9, 1.0) 80 1526 19.0750 (0.8, 0.9] 96 1430 14.8958 (0.7, 0.8] 110 1596 14.5091 (0.6, 0.7] 74 1031 13.9324 (0.5, 0.6] 104 1811 17.4135 (0, 0.5] 479 8972 18.7307 Table 2: Composition of test subsets based on fuzzy match scores training and validation is on the same training sentences1 as the SMT system with 5-fold cross validation. The SVM hyper-parame</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu. tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical Phrase-Based model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="7427" citStr="Chiang, 2005" startWordPosition="1166" endWordPosition="1168">er, to categorically reuse the translations of matched chunks without any differentiation could generate inferior translations given the fact that the context of these matched chunks in the input sentence could be completely different from the source side of the fuzzy match. To address this problem, both (Koehn and Senellart, 2010) and (Zhechev and van Genabith, 2010) used fuzzy match score as a threshold to determine whether to reuse the translations of the matched chunks. For example, (Koehn and Senellart, 2010) showed that reusing these translations as large rules in a hierarchical system (Chiang, 2005) can be beneficial when the fuzzy match score is above 70%, while (Zhechev and van Genabith, 2010) reported that it is only beneficial to a phrase-based system when the fuzzy match score is above 90%. Despite being an informative measure, using fuzzy match score as a threshold has a number of limitations. Given the fact that fuzzy match score is normally calculated based on Edit Distance (Levenshtein, 1966), a low score does not necessarily imply that the fuzzy match is harmful when used to constrain an input sentence. For example, in longer sentences where fuzzy match scores tend to be low, s</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical Phrase-Based model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="10095" citStr="Cortes and Vapnik, 1995" startWordPosition="1625" endWordPosition="1628"> Ney, 2003) posterior alignments. We use the intersected word alignment to minimise the noise introduced by word alignment of only one direction in marking up the input sentence. 3.2 Discriminative Learning Whether the translation information from the fuzzy matches should be used or not (i.e. whether the input sentence should be marked up) is determined using a discriminative learning procedure. The translation information refers to the “phrase pairs” derived using the method described in Section 3.1. We cast this problem as a binary classification problem. 3.2.1 Support Vector Machines SVMs (Cortes and Vapnik, 1995) are binary classifiers that classify an input instance based on decision rules which minimise the regularised error function where (xi, yi) ∈ Rn × {+1, −1} are l training instances that are mapped by the function φ to a higher dimensional space. w is the weight vector, ξ is the relaxation variable and C &gt; 0 is the penalty parameter. Solving SVMs is viable using a kernel function K in (1) with K(xi, xj) = b(xi)T b(xj). We perform our experiments with the Radial Basis Function (RBF) kernel, as in (2): K(xi, xj) = exp(−γ||xi − xj||2), γ &gt; 0 (2) When using SVMs with the RBF kernel, we have two fr</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Yanjun Ma</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Bridging SMT and TM with translation recommendation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>622--630</pages>
<location>Uppsala,</location>
<marker>He, Ma, van Genabith, Way, 2010</marker>
<rawString>Yifan He, Yanjun Ma, Josef van Genabith, and Andy Way. 2010. Bridging SMT and TM with translation recommendation. In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages 622–630, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<location>Detroit, MI.</location>
<contexts>
<context position="19314" citStr="Kneser and Ney, 1995" startWordPosition="3259" endWordPosition="3262"> tuned using the training data of the first fold in the 5-fold cross validation via a brute force grid search. More specifically, for parameter C in (1), we search in the range [2−5, 215], while for parameter -y (2) we search in the range [2−15, 23]. The step size is 2 on the exponent. We conducted experiments using a standard loglinear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) which is capable of handling user-specified translations for some portions of the input during decoding. The maximum phrase length is set to 7. 5.2 Evaluation The performance of the phrase-based SMT system is measured by BLEU score (Papineni et al., 2002) and TER (Snover et al., 2006). Significance test1We have around 87K sentence pairs in our training data. However, for 67.5% of the input sentences, our MT system produces the same translation irrespective of whether the input sentence </context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, volume 1, pages 181–184, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Jean Senellart</author>
</authors>
<title>Convergence of translation memory and statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of AMTA Workshop on MT Research and the Translation Industry,</booktitle>
<pages>21--31</pages>
<location>Denver, CO.</location>
<contexts>
<context position="6498" citStr="Koehn and Senellart, 2010" startWordPosition="1017" endWordPosition="1020">f translation, as the translation of new input sentences is closely informed and guided (or constrained) by previously translated sentences. There are several different ways of using the translation information derived from fuzzy matches, with the following two being the most widely adopted: 1) to add these translations into a phrase table as in (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009), or 2) to mark up the input sentence using the relevant chunk translations in the fuzzy match, and to use an MT system to translate the parts that are not marked up, as in (Smith and Clark, 2009; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010). It is worth mentioning that translation consistency was not explicitly regarded as their primary motivation in this previous work. Our research follows the direction of the second strand given that consistency can no longer be guaranteed by constructing another phrase table. However, to categorically reuse the translations of matched chunks without any differentiation could generate inferior translations given the fact that the context of these matched chunks in the input sentence could be completely different from the source side of the fuzzy match. To addre</context>
<context position="9368" citStr="Koehn and Senellart, 2010" startWordPosition="1512" endWordPosition="1516"> translations ¯f′im for each of the phrases in ¯eim . This process can derive a number of “phrase ′m &gt;, which can be used to specify the translations of the matched phrases in the input sentence. The remaining words without specified translations will be translated by an MT system. For example, given an input sentence e1e2 · · · eiei+1 · · · eI, and a phrase pair &lt; e, �f′ &gt;, e� = eiei+1, �f′ = f′jf′j+1 derived from the fuzzy match, we can mark up the input sentence as: e1e2 ··· &lt;tm=“f′jf′j+1”&gt; eiei+1 &lt; /tm&gt; ··· eI. Our method to constrain the translations using TM fuzzy matches is similar to (Koehn and Senellart, 2010), except that the word alignment between e′ and f′ is the intersection of bidirectional GIZA++ (Och and Ney, 2003) posterior alignments. We use the intersected word alignment to minimise the noise introduced by word alignment of only one direction in marking up the input sentence. 3.2 Discriminative Learning Whether the translation information from the fuzzy matches should be used or not (i.e. whether the input sentence should be marked up) is determined using a discriminative learning procedure. The translation information refers to the “phrase pairs” derived using the method described in Sec</context>
<context position="25125" citStr="Koehn and Senellart, 2010" startWordPosition="4236" endWordPosition="4240">er the default setting (0.50). Despite the higher precision when the confidence is set to 0.7, the dramatic decrease in recall cannot be compensated for by the increase in precision. We can also observe from Table 4 that the recall is quite low across the board, and the classification results become unstable when we further increase the level of confidence to above 0.70. This indicates the degree of difficulty of this classification task, and suggests some directions for future research as discussed at the end of this paper. 5.4.3 Comparison with Previous Work As discussed in Section 2, both (Koehn and Senellart, 2010) and (Zhechev and van Genabith, 2010) used fuzzy match score to determine whether the input sentences should be marked up. The input sentences are only marked up when the fuzzy match score is above a certain threshold. We present the results using this method in Table 5. From this taFuzzy Match Scores 0.50 0.60 0.70 0.80 0.90 BLEU 45.13 45.55 45.58 45.84 45.82 TER 40.99 40.62 40.56 40.29 40.07 Table 5: Performance using fuzzy match score for classification ble, we can see an inferior performance compared to the BASELINE results (cf. Table 3) when the fuzzy match score is below 0.70. A modest g</context>
</contexts>
<marker>Koehn, Senellart, 2010</marker>
<rawString>Philipp Koehn and Jean Senellart. 2010. Convergence of translation memory and statistical machine translation. In Proceedings of AMTA Workshop on MT Research and the Translation Industry, pages 21–31, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<location>Edmonton, AB,</location>
<contexts>
<context position="13465" citStr="Koehn et al., 2003" startWordPosition="2233" endWordPosition="2236">btained using an internal crossvalidation on the training set. where e is the sentence to translate, and s is the source side of an entry in the TM. For fuzzy match scores F, hfm roughly corresponds to 1 − F. 4.2 Translation Features We use four features related to translation probabilities, i.e. the phrase translation and lexical probabilities for the phrase pairs &lt; em, f′m &gt; derived using the method in Section 3.1. Specifically, we use the phrase translation probabilities p(�f′m|em) and ′m), as well as the lexical translation probabilities plex( f′m|�em) and plex(�em |f′m) as calculated in (Koehn et al., 2003). In cases where multiple phrase pairs are used to mark up one single input sentence e, we use a unified score for each of the four features, which is an average over the corresponding feature in each phrase pair. The intuition behind these features is as follows: phrase pairs &lt; em, f′m &gt; derived from the fuzzy match should also be reliable with respect to statistically produced models. We also have a count feature, i.e. the number of phrases used to mark up the input sentence, and a binary feature, i.e. whether the phrase table contains �l i=1 F(z) = − min z=(A,B) f p(�em| 4 Feature Set at le</context>
<context position="19200" citStr="Koehn et al., 2003" startWordPosition="3243" endWordPosition="3246"> is on the same training sentences1 as the SMT system with 5-fold cross validation. The SVM hyper-parameters are tuned using the training data of the first fold in the 5-fold cross validation via a brute force grid search. More specifically, for parameter C in (1), we search in the range [2−5, 215], while for parameter -y (2) we search in the range [2−15, 23]. The step size is 2 on the exponent. We conducted experiments using a standard loglinear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) which is capable of handling user-specified translations for some portions of the input during decoding. The maximum phrase length is set to 7. 5.2 Evaluation The performance of the phrase-based SMT system is measured by BLEU score (Papineni et al., 2002) and TER (Snover et al., 2006). Significance test1We have around 87K sentence pairs in our training data. However, for 67.</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics, pages 48–54, Edmonton, AB, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="19422" citStr="Koehn et al., 2007" startWordPosition="3278" endWordPosition="3281"> More specifically, for parameter C in (1), we search in the range [2−5, 215], while for parameter -y (2) we search in the range [2−15, 23]. The step size is 2 on the exponent. We conducted experiments using a standard loglinear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) which is capable of handling user-specified translations for some portions of the input during decoding. The maximum phrase length is set to 7. 5.2 Evaluation The performance of the phrase-based SMT system is measured by BLEU score (Papineni et al., 2002) and TER (Snover et al., 2006). Significance test1We have around 87K sentence pairs in our training data. However, for 67.5% of the input sentences, our MT system produces the same translation irrespective of whether the input sentence is marked up or not. 1243 ing is carried out using approximate randomisation (Noreen, 1989) with a 95% confi</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Iosifovich Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="7837" citStr="Levenshtein, 1966" startWordPosition="1236" endWordPosition="1238">old to determine whether to reuse the translations of the matched chunks. For example, (Koehn and Senellart, 2010) showed that reusing these translations as large rules in a hierarchical system (Chiang, 2005) can be beneficial when the fuzzy match score is above 70%, while (Zhechev and van Genabith, 2010) reported that it is only beneficial to a phrase-based system when the fuzzy match score is above 90%. Despite being an informative measure, using fuzzy match score as a threshold has a number of limitations. Given the fact that fuzzy match score is normally calculated based on Edit Distance (Levenshtein, 1966), a low score does not necessarily imply that the fuzzy match is harmful when used to constrain an input sentence. For example, in longer sentences where fuzzy match scores tend to be low, some chunks and the corresponding translations within the sentences can still be useful. On 1240 in (1): 1 wTw + C ξi 2 �l i=1 min w,b,g s. t. yi(wTφ(xi) + b) &gt; 1 − ξi ξi &gt; 0 (1) the other hand, a high score cannot fully guarantee the usefulness of a particular translation. We address this problem using discriminative learning. 3 Constrained Translation with Discriminative Learning 3.1 Formulation of the Pro</context>
<context position="14876" citStr="Levenshtein, 1966" startWordPosition="2494" endWordPosition="2496">ed in the 4.3 Dependency Features following sections. Given the phrase pairs &lt; em, 4.1 The TM Feature The TM feature is the fuzzy match score, which indicates the overall similarity between the input sentence and the source side of the TM output. If the input sentence is similar to the source side of the matching segment, it is more likely that the matching segment can be used to mark up the input sentence. The calculation of the fuzzy match score itself is one of the core technologies in TM systems, and varies among different vendors. We compute fuzzy match cost as the minimum Edit Distance (Levenshtein, 1966) between the source and TM entry, normalised by the length of the source as in (6), as most of the current implementations are based on edit distance while allowing some additional flexible matching. EditDistance(e, s) (6) Len(e) the fuzzy match, and used to translate the corresponding chunks of the input sentence (cf. Section 3.1), these translations are more likely to be coherent in the context of the particular input sentence if the matched parts on the input side are syntactically and semantically related. For matched phrases em between the input sentence and the source side of the fuzzy m</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Iosifovich Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsuan-Tien Lin</author>
<author>Chih-Jen Lin</author>
<author>Ruby C Weng</author>
</authors>
<title>A note on platt’s probabilistic outputs for support vector machines.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<volume>68</volume>
<issue>3</issue>
<contexts>
<context position="12147" citStr="Lin et al., 2007" startWordPosition="1991" endWordPosition="1994"> marked phrases are accurate when taken contextual information into account. As large-scale manually annotated data is not available for this task, we use automatic TER scores (Snover et al., 2006) as the measure for training data annotation. We label the training examples as in (3): � +1 if TER(w. markup) &lt; TER(w/o markup) y= −1 if TER(w/o markup) ≥ TER(w. markup) (3) Each instance is associated with a set of features which are discussed in more detail in Section 4. pairs” &lt; em, f 1241 3.2.2 Classification Confidence Estimation We use the techniques proposed by (Platt, 1999) and improved by (Lin et al., 2007) to convert classification margin to posterior probability, so that we can easily threshold our classifier (cf. Section 5.4.2). Platt’s method estimates the posterior probability with a sigmoid function, as in (4): 1 Pr(y = 1|z) ,: PA,B(f) =1 + exp(Af + B) (4) where f = f(z) is the decision function of the estimated SVM. A and B are parameters that minimise the cross-entropy error function F on the training data, as in (5): (tilog(pi) + (1 − ti)log(1 − pi)), N++2 if yi = +1 where pi = PA,B (fi), and ti = i IN +2 if yi = −1 (5) where z = (A, B) is a parameter setting, and N+ and N− are the numb</context>
</contexts>
<marker>Lin, Lin, Weng, 2007</marker>
<rawString>Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007. A note on platt’s probabilistic outputs for support vector machines. Machine Learning, 68(3):267–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Tera-scale translation models via pattern matching.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>505--512</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="30101" citStr="Lopez, 2008" startWordPosition="5114" endWordPosition="5115">he sources of improvements by marking up the input sentence, we performed some manual analysis of the output. We observe that the improvements can broadly be attributed to two reasons: 1) the use of long phrase pairs which are missing in the phrase table, and 2) deterministically using highly reliable phrase pairs. Phrase-based SMT systems normally impose a limit on the length of phrase pairs for storage and speed considerations. Our method can overcome this limitation by retrieving and reusing long phrase pairs on the fly. A similar idea, albeit from a different perspective, was explored by (Lopez, 2008), where he proposed to construct a phrase table on the fly for each sentence to be translated. Differently from his approach, our method directly translates part of the input sentence using fuzzy matches retrieved on the fly, with the rest of the sentence translated by the pre-trained MT system. We offer some more insights into the advantages of our method by means of a few examples. Example 1 shows translation improvements by using long phrase pairs. Compared to the reference translation, we can see that for the underlined phrase, the translation without markup contains (i) word ordering erro</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Tera-scale translation models via pattern matching. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 505–512, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience,</title>
<date>1989</date>
<location>New York, NY.</location>
<contexts>
<context position="20005" citStr="Noreen, 1989" startWordPosition="3377" endWordPosition="3378">and Moses (Koehn et al., 2007) which is capable of handling user-specified translations for some portions of the input during decoding. The maximum phrase length is set to 7. 5.2 Evaluation The performance of the phrase-based SMT system is measured by BLEU score (Papineni et al., 2002) and TER (Snover et al., 2006). Significance test1We have around 87K sentence pairs in our training data. However, for 67.5% of the input sentences, our MT system produces the same translation irrespective of whether the input sentence is marked up or not. 1243 ing is carried out using approximate randomisation (Noreen, 1989) with a 95% confidence level. We also measure the quality of the classification by precision and recall. Let A be the set of predicted markup input sentences, and B be the set of input sentences where the markup version has a lower TER score than the plain version. We standardly define precision P and recall R as in (7): 5.3 Cross-fold translation In order to obtain training samples for the classifier, we need to label each sentence in the SMT training data as to whether marking up the sentence can produce better translations. To achieve this, we translate both the marked-up versions and plain</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9482" citStr="Och and Ney, 2003" startWordPosition="1532" endWordPosition="1535">to specify the translations of the matched phrases in the input sentence. The remaining words without specified translations will be translated by an MT system. For example, given an input sentence e1e2 · · · eiei+1 · · · eI, and a phrase pair &lt; e, �f′ &gt;, e� = eiei+1, �f′ = f′jf′j+1 derived from the fuzzy match, we can mark up the input sentence as: e1e2 ··· &lt;tm=“f′jf′j+1”&gt; eiei+1 &lt; /tm&gt; ··· eI. Our method to constrain the translations using TM fuzzy matches is similar to (Koehn and Senellart, 2010), except that the word alignment between e′ and f′ is the intersection of bidirectional GIZA++ (Och and Ney, 2003) posterior alignments. We use the intersected word alignment to minimise the noise introduced by word alignment of only one direction in marking up the input sentence. 3.2 Discriminative Learning Whether the translation information from the fuzzy matches should be used or not (i.e. whether the input sentence should be marked up) is determined using a discriminative learning procedure. The translation information refers to the “phrase pairs” derived using the method described in Section 3.1. We cast this problem as a binary classification problem. 3.2.1 Support Vector Machines SVMs (Cortes and </context>
<context position="19117" citStr="Och and Ney, 2003" startWordPosition="3231" endWordPosition="3234">2: Composition of test subsets based on fuzzy match scores training and validation is on the same training sentences1 as the SMT system with 5-fold cross validation. The SVM hyper-parameters are tuned using the training data of the first fold in the 5-fold cross validation via a brute force grid search. More specifically, for parameter C in (1), we search in the range [2−5, 215], while for parameter -y (2) we search in the range [2−15, 23]. The step size is 2 on the exponent. We conducted experiments using a standard loglinear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) which is capable of handling user-specified translations for some portions of the input during decoding. The maximum phrase length is set to 7. 5.2 Evaluation The performance of the phrase-based SMT system is measured by BLEU score (Papineni et al., 2002) and TER (Snover et al., 2006). Signifi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="19240" citStr="Och, 2003" startWordPosition="3250" endWordPosition="3251">stem with 5-fold cross validation. The SVM hyper-parameters are tuned using the training data of the first fold in the 5-fold cross validation via a brute force grid search. More specifically, for parameter C in (1), we search in the range [2−5, 215], while for parameter -y (2) we search in the range [2−15, 23]. The step size is 2 on the exponent. We conducted experiments using a standard loglinear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) which is capable of handling user-specified translations for some portions of the input during decoding. The maximum phrase length is set to 7. 5.2 Evaluation The performance of the phrase-based SMT system is measured by BLEU score (Papineni et al., 2002) and TER (Snover et al., 2006). Significance test1We have around 87K sentence pairs in our training data. However, for 67.5% of the input sentences, our MT system</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Labelled dependencies in machine translation evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>104--111</pages>
<location>Prague, Czech Republic.</location>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Labelled dependencies in machine translation evaluation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 104–111, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="19678" citStr="Papineni et al., 2002" startWordPosition="3320" endWordPosition="3323">lementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) which is capable of handling user-specified translations for some portions of the input during decoding. The maximum phrase length is set to 7. 5.2 Evaluation The performance of the phrase-based SMT system is measured by BLEU score (Papineni et al., 2002) and TER (Snover et al., 2006). Significance test1We have around 87K sentence pairs in our training data. However, for 67.5% of the input sentences, our MT system produces the same translation irrespective of whether the input sentence is marked up or not. 1243 ing is carried out using approximate randomisation (Noreen, 1989) with a 95% confidence level. We also measure the quality of the classification by precision and recall. Let A be the set of predicted markup input sentences, and B be the set of input sentences where the markup version has a lower TER score than the plain version. We stan</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of Machine Translation. In 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<contexts>
<context position="12112" citStr="Platt, 1999" startWordPosition="1986" endWordPosition="1987">nce if the translations of the marked phrases are accurate when taken contextual information into account. As large-scale manually annotated data is not available for this task, we use automatic TER scores (Snover et al., 2006) as the measure for training data annotation. We label the training examples as in (3): � +1 if TER(w. markup) &lt; TER(w/o markup) y= −1 if TER(w/o markup) ≥ TER(w. markup) (3) Each instance is associated with a set of features which are discussed in more detail in Section 4. pairs” &lt; em, f 1241 3.2.2 Classification Confidence Estimation We use the techniques proposed by (Platt, 1999) and improved by (Lin et al., 2007) to convert classification margin to posterior probability, so that we can easily threshold our classifier (cf. Section 5.4.2). Platt’s method estimates the posterior probability with a sigmoid function, as in (4): 1 Pr(y = 1|z) ,: PA,B(f) =1 + exp(Af + B) (4) where f = f(z) is the decision function of the estimated SVM. A and B are parameters that minimise the cross-entropy error function F on the training data, as in (5): (tilog(pi) + (1 − ti)log(1 − pi)), N++2 if yi = +1 where pi = PA,B (fi), and ti = i IN +2 if yi = −1 (5) where z = (A, B) is a parameter </context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, pages 61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sikes</author>
</authors>
<title>Fuzzy matching in theory and practice.</title>
<date>2007</date>
<journal>Multilingual,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="2572" citStr="Sikes, 2007" startWordPosition="369" endWordPosition="370">ven that global contextual information is not normally incorporated, and that training data is usually noisy in nature, there is no guarantee that an SMT system can produce translations in a consistent manner. On the other hand, TM systems – widely used by translators in industrial environments for enterprise localisation by translators – can shed some light on mitigating this limitation. TM systems can assist translators by retrieving and displaying previously translated similar ‘example’ sentences (displayed as source-target pairs, widely called ‘fuzzy matches’ in the localisation industry (Sikes, 2007)). In TM systems, fuzzy matches are retrieved by calculating the similarity or the so-called ‘fuzzy match score’ (ranging from 0 to 1 with 0 indicating no matches and 1 indicating a full match) between the input sentence and sentences in the source side of the translation memory. When presented with fuzzy matches, translators can then avail of useful chunks in previous translations while composing the translation of a new sentence. Most translators only consider a few sentences that are most similar to the current input sentence; this process can inherently improve the consistency of translati</context>
</contexts>
<marker>Sikes, 2007</marker>
<rawString>Richard Sikes. 2007. Fuzzy matching in theory and practice. Multilingual, 18(6):39–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Pierre Isabelle</author>
</authors>
<title>Phrase-based machine translation in a computer-assisted translation environment.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII),</booktitle>
<pages>120--127</pages>
<location>Ottawa, Ontario, Canada.</location>
<contexts>
<context position="6276" citStr="Simard and Isabelle, 2009" startWordPosition="975" endWordPosition="978">ce and target TM matches, which can be addressed using state-of-the-art word alignment techniques. More importantly, albeit not explicitly spelled out in previous work, this method can potentially increase the consistency of translation, as the translation of new input sentences is closely informed and guided (or constrained) by previously translated sentences. There are several different ways of using the translation information derived from fuzzy matches, with the following two being the most widely adopted: 1) to add these translations into a phrase table as in (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009), or 2) to mark up the input sentence using the relevant chunk translations in the fuzzy match, and to use an MT system to translate the parts that are not marked up, as in (Smith and Clark, 2009; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010). It is worth mentioning that translation consistency was not explicitly regarded as their primary motivation in this previous work. Our research follows the direction of the second strand given that consistency can no longer be guaranteed by constructing another phrase table. However, to categorically reuse the translations of matched chunks </context>
</contexts>
<marker>Simard, Isabelle, 2009</marker>
<rawString>Michel Simard and Pierre Isabelle. 2009. Phrase-based machine translation in a computer-assisted translation environment. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), pages 120 – 127, Ottawa, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Smith</author>
<author>Stephen Clark</author>
</authors>
<title>EBMT for SMT: A new EBMT-SMT hybrid.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd International Workshop on Example-Based Machine Translation,</booktitle>
<pages>3--10</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="6471" citStr="Smith and Clark, 2009" startWordPosition="1013" endWordPosition="1016">rease the consistency of translation, as the translation of new input sentences is closely informed and guided (or constrained) by previously translated sentences. There are several different ways of using the translation information derived from fuzzy matches, with the following two being the most widely adopted: 1) to add these translations into a phrase table as in (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009), or 2) to mark up the input sentence using the relevant chunk translations in the fuzzy match, and to use an MT system to translate the parts that are not marked up, as in (Smith and Clark, 2009; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010). It is worth mentioning that translation consistency was not explicitly regarded as their primary motivation in this previous work. Our research follows the direction of the second strand given that consistency can no longer be guaranteed by constructing another phrase table. However, to categorically reuse the translations of matched chunks without any differentiation could generate inferior translations given the fact that the context of these matched chunks in the input sentence could be completely different from the source side o</context>
</contexts>
<marker>Smith, Clark, 2009</marker>
<rawString>James Smith and Stephen Clark. 2009. EBMT for SMT: A new EBMT-SMT hybrid. In Proceedings of the 3rd International Workshop on Example-Based Machine Translation, pages 3–10, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociationfor Machine Translation in the Americas (AMTA-2006),</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="11727" citStr="Snover et al., 2006" startWordPosition="1916" endWordPosition="1919">input sentence should be marked up using relevant phrase pairs derived from the fuzzy match before sending it to the SMT system for translation. The classifier uses features such as the fuzzy match score, the phrase and lexical translation probabilities of these relevant phrase pairs, and additional syntactic dependency features. Ideally the classifier will decide to mark up the input sentence if the translations of the marked phrases are accurate when taken contextual information into account. As large-scale manually annotated data is not available for this task, we use automatic TER scores (Snover et al., 2006) as the measure for training data annotation. We label the training examples as in (3): � +1 if TER(w. markup) &lt; TER(w/o markup) y= −1 if TER(w/o markup) ≥ TER(w. markup) (3) Each instance is associated with a set of features which are discussed in more detail in Section 4. pairs” &lt; em, f 1241 3.2.2 Classification Confidence Estimation We use the techniques proposed by (Platt, 1999) and improved by (Lin et al., 2007) to convert classification margin to posterior probability, so that we can easily threshold our classifier (cf. Section 5.4.2). Platt’s method estimates the posterior probability w</context>
<context position="19708" citStr="Snover et al., 2006" startWordPosition="3326" endWordPosition="3329">t model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) which is capable of handling user-specified translations for some portions of the input during decoding. The maximum phrase length is set to 7. 5.2 Evaluation The performance of the phrase-based SMT system is measured by BLEU score (Papineni et al., 2002) and TER (Snover et al., 2006). Significance test1We have around 87K sentence pairs in our training data. However, for 67.5% of the input sentences, our MT system produces the same translation irrespective of whether the input sentence is marked up or not. 1243 ing is carried out using approximate randomisation (Noreen, 1989) with a 95% confidence level. We also measure the quality of the classification by precision and recall. Let A be the set of predicted markup input sentences, and B be the set of input sentences where the markup version has a lower TER score than the plain version. We standardly define precision P and </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociationfor Machine Translation in the Americas (AMTA-2006), pages 223–231, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Craig Saunders</author>
<author>Marco Turchi</author>
<author>Zhuoran Wang</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Improving the confidence of machine translation quality estimates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII),</booktitle>
<pages>136--143</pages>
<location>Ottawa, Ontario, Canada.</location>
<contexts>
<context position="4825" citStr="Specia et al., 2009" startWordPosition="743" endWordPosition="746">e design in Section 4. We report the experimental results in Section 5 and conclude the paper and point out avenues for future research in Section 6. 2 Related Research Despite the fact that TM and MT integration has long existed as a major challenge in the localisation industry, it has only recently received attention in main-stream MT research. One can loosely combine TM and MT at sentence (called segments in TMs) level by choosing one of them (or both) to recommend to the translators using automatic classifiers (He et al., 2010), or simply using fuzzy match score or MT confidence measures (Specia et al., 2009). One can also tightly integrate TM with MT at the sub-sentence level. The basic idea is as follows: given a source sentence to translate, we firstly use a TM system to retrieve the most similar ‘example’ source sentences together with their translations. If matched chunks between input sentence and fuzzy matches can be detected, we can directly re-use the corresponding parts of the translation in the fuzzy matches, and use an MT system to translate the remaining chunks. As a matter of fact, implementing this idea is pretty straightforward: a TM system can easily detect the word alignment betw</context>
</contexts>
<marker>Specia, Saunders, Turchi, Wang, Shawe-Taylor, 2009</marker>
<rawString>Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang, and John Shawe-Taylor. 2009. Improving the confidence of machine translation quality estimates. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), pages 136 – 143, Ottawa, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="19349" citStr="Stolcke, 2002" startWordPosition="3266" endWordPosition="3267">t fold in the 5-fold cross validation via a brute force grid search. More specifically, for parameter C in (1), we search in the range [2−5, 215], while for parameter -y (2) we search in the range [2−15, 23]. The step size is 2 on the exponent. We conducted experiments using a standard loglinear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) which is capable of handling user-specified translations for some portions of the input during decoding. The maximum phrase length is set to 7. 5.2 Evaluation The performance of the phrase-based SMT system is measured by BLEU score (Papineni et al., 2002) and TER (Snover et al., 2006). Significance test1We have around 87K sentence pairs in our training data. However, for 67.5% of the input sentences, our MT system produces the same translation irrespective of whether the input sentence is marked up or not. 1243 ing is ca</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ventsislav Zhechev</author>
<author>Josef van Genabith</author>
</authors>
<title>Seeding statistical machine translation with translation memory output through tree-based structural alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>43--51</pages>
<location>Beijing, China.</location>
<marker>Zhechev, van Genabith, 2010</marker>
<rawString>Ventsislav Zhechev and Josef van Genabith. 2010. Seeding statistical machine translation with translation memory output through tree-based structural alignment. In Proceedings of the Fourth Workshop on Syntax and Structure in Statistical Translation, pages 43– 51, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>