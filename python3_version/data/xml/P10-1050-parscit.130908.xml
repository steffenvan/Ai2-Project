<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.977027">
Efficient Staggered Decoding for Sequence Labeling
</title>
<author confidence="0.996271">
Nobuhiro Kaji Yasuhiro Fujiwara Naoki Yoshinaga Masaru Kitsuregawa
</author>
<affiliation confidence="0.99934">
Institute of Industrial Science,
The University of Tokyo,
</affiliation>
<address confidence="0.747671">
4-6-1, Komaba, Meguro-ku, Tokyo, 153-8505 Japan
</address>
<email confidence="0.998634">
{kaji,fujiwara,ynaga,kisture}@tkl.iis.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999639368421053">
The Viterbi algorithm is the conventional
decoding algorithm most widely adopted
for sequence labeling. Viterbi decoding
is, however, prohibitively slow when the
label set is large, because its time com-
plexity is quadratic in the number of la-
bels. This paper proposes an exact decod-
ing algorithm that overcomes this prob-
lem. A novel property of our algorithm is
that it efficiently reduces the labels to be
decoded, while still allowing us to check
the optimality of the solution. Experi-
ments on three tasks (POS tagging, joint
POS tagging and chunking, and supertag-
ging) show that the new algorithm is sev-
eral orders of magnitude faster than the
basic Viterbi and a state-of-the-art algo-
rithm, CARPEDIEM (Esposito and Radi-
cioni, 2009).
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998954375">
In the past decade, sequence labeling algorithms
such as HMMs, CRFs, and Collins’ perceptrons
have been extensively studied in the field of NLP
(Rabiner, 1989; Lafferty et al., 2001; Collins,
2002). Now they are indispensable in a wide range
of NLP tasks including chunking, POS tagging,
NER and so on (Sha and Pereira, 2003; Tsuruoka
and Tsujii, 2005; Lin and Wu, 2009).
One important task in sequence labeling is how
to find the most probable label sequence from
among all possible ones. This task, referred to as
decoding, is usually carried out using the Viterbi
algorithm (Viterbi, 1967). The Viterbi algorithm
has O(NL2) time complexity,1 where N is the
input size and L is the number of labels. Al-
though the Viterbi algorithm is generally efficient,
</bodyText>
<footnote confidence="0.996144666666667">
1The first-order Markov assumption is made throughout
this paper, although our algorithm is applicable to higher-
order Markov models as well.
</footnote>
<bodyText confidence="0.999174742857143">
it becomes prohibitively slow when dealing with
a large number of labels, since its computational
cost is quadratic in L (Dietterich et al., 2008).
Unfortunately, several sequence-labeling prob-
lems in NLP involve a large number of labels. For
example, there are more than 40 and 2000 labels
in POS tagging and supertagging, respectively
(Brants, 2000; Matsuzaki et al., 2007). These
tasks incur much higher computational costs than
simpler tasks like NP chunking. What is worse,
the number of labels grows drastically if we jointly
perform multiple tasks. As we shall see later,
we need over 300 labels to reduce joint POS tag-
ging and chunking into the single sequence label-
ing problem. Although joint learning has attracted
much attention in recent years, how to perform de-
coding efficiently still remains an open problem.
In this paper, we present a new decoding algo-
rithm that overcomes this problem. The proposed
algorithm has three distinguishing properties: (1)
It is much more efficient than the Viterbi algorithm
when dealing with a large number of labels. (2) It
is an exact algorithm, that is, the optimality of the
solution is always guaranteed unlike approximate
algorithms. (3) It is automatic, requiring no task-
dependent hyperparameters that have to be manu-
ally adjusted.
Experiments evaluate our algorithm on three
tasks: POS tagging, joint POS tagging and chunk-
ing, and supertaggini. The results demonstrate
that our algorithm is up to several orders of mag-
nitude faster than the basic Viterbi algorithm and a
state-of-the-art algorithm (Esposito and Radicioni,
2009); it makes exact decoding practical even in
labeling problems with a large label set.
</bodyText>
<sectionHeader confidence="0.99806" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.998856">
We first provide a brief overview of sequence la-
beling and introduce related work.
</bodyText>
<footnote confidence="0.9888155">
2Our implementation is available at http://www.tkl.iis.u-
tokyo.ac.jp/˜kaji/staggered
</footnote>
<page confidence="0.953349">
485
</page>
<note confidence="0.9686605">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 485–494,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.892418">
2.1 Models
</subsectionHeader>
<bodyText confidence="0.9999216">
Sequence labeling is the problem of predicting la-
bel sequence y = {yn}Nn=1 for given token se-
quence x = {xn}Nn=1. This is typically done by
defining a score function f(x, y) and locating the
best label sequence: ymax = argmax f(x, y).
</bodyText>
<equation confidence="0.38868">
Y
</equation>
<bodyText confidence="0.9994976">
The form of f(x, y) is dependent on the learn-
ing model used. Here, we introduce two models
widely used in the literature.
Generative models HMM is the most famous
generative model for labeling token sequences
(Rabiner, 1989). In HMMs, the score function
f(x, y) is the joint probability distribution over
(x, y). If we assume a one-to-one correspondence
between the hidden states and the labels, the score
function can be written as:
</bodyText>
<equation confidence="0.9988538">
f(x, y) = log p(x, y)
= log p(x|y) + log p(y)
N N
log p(xn|yn)+ log p(yn|yn−1).
n=1 n=1
</equation>
<bodyText confidence="0.999848142857143">
The parameters log p(xn|yn) and log p(yn|yn−1)
are usually estimated using maximum likelihood
or the EM algorithm. Since parameter estimation
lies outside the scope of this paper, a detailed de-
scription is omitted.
Discriminative models Recent years have seen
the emergence of discriminative training methods
for sequence labeling (Lafferty et al., 2001; Tasker
et al., 2003; Collins, 2002; Tsochantaridis et al.,
2005). Among them, we focus on the perceptron
algorithm (Collins, 2002). Although we do not
discuss the other discriminative models, our algo-
rithm is equivalently applicable to them. The ma-
jor difference between those models lies in param-
eter estimation; the decoding process is virtually
the same.
In the perceptron, the score function f(x, y) is
given as f(x, y) = w · φ(x, y) where w is the
weight vector, and φ(x, y) is the feature vector
representation of the pair (x, y). By making the
first-order Markov assumption, we have
</bodyText>
<equation confidence="0.9799595">
f(x, y) = w · φ(x, y)
wkφk(x, yn−1, yn),
</equation>
<bodyText confidence="0.996357">
where K = |φ(x, y) |is the number of features, φk
is the k-th feature function, and wk is the weight
corresponding to it. Parameter w can be estimated
in the same way as in the conventional perceptron
algorithm. See (Collins, 2002) for details.
</bodyText>
<subsectionHeader confidence="0.998925">
2.2 Viterbi decoding
</subsectionHeader>
<bodyText confidence="0.999845875">
Given the score function f(x, y), we have to lo-
cate the best label sequence. This is usually per-
formed by applying the Viterbi algorithm. Let
ω(yn) be the best score of the partial label se-
quence ending with yn. The idea of the Viterbi
algorithm is to use dynamic programming to com-
pute ω(yn). In HMMs, ω(yn) can be can be de-
fined as
</bodyText>
<equation confidence="0.981516">
max
{ω(yn−1) + log p(yn|yn−1)} + log p(xn|yn).
yn−1
</equation>
<bodyText confidence="0.999753818181818">
Using this recursive definition, we can evaluate
ω(yn) for all yn. This results in the identification
of the best label sequence.
Although the Viterbi algorithm is commonly
adopted in past studies, it is not always efficient.
The computational cost of the Viterbi algorithm is
O(NL2), where N is the input length and L is
the number of labels; it is efficient enough if L
is small. However, if there are many labels, the
Viterbi algorithm becomes prohibitively slow be-
cause of its quadratic dependence on L.
</bodyText>
<sectionHeader confidence="0.624209" genericHeader="method">
2.3 Related work
</sectionHeader>
<bodyText confidence="0.999967260869565">
To the best of our knowledge, the Viterbi algo-
rithm is the only algorithm widely adopted in the
NLP field that offers exact decoding. In other
communities, several exact algorithms have al-
ready been proposed for handling large label sets.
While they are successful to some extent, they de-
mand strong assumptions that are unusual in NLP.
Moreover, none were challenged with standard
NLP tasks.
Felzenszwalb et al. (2003) presented a fast
inference algorithm for HMMs based on the as-
sumption that the hidden states can be embed-
ded in a grid space, and the transition probabil-
ity corresponds to the distance on that space. This
type of probability distribution is not common in
NLP tasks. Lifshits et al. (2007) proposed a
compression-based approach to speed up HMM
decoding. It assumes that the input sequence is
highly repetitive. Amongst others, CARPEDIEM
(Esposito and Radicioni, 2009) is the algorithm
closest to our work. It accelerates decoding by
assuming that the adjacent labels are not strongly
correlated. This assumption is appropriate for
</bodyText>
<equation confidence="0.878676">
K
k=1
N
n=1
</equation>
<page confidence="0.991932">
486
</page>
<bodyText confidence="0.999918057142857">
some NLP tasks. For example, as suggested in
(Liang et al., 2008), adjacent labels do not provide
strong information in POS tagging. However, the
applicability of this idea to other NLP tasks is still
unclear.
Approximate algorithms, such as beam search
or island-driven search, have been proposed for
speeding up decoding. Tsuruoka and Tsujii (2005)
proposed easiest-first deterministic decoding. Sid-
diqi and Moore (2005) presented the parameter ty-
ing approach for fast inference in HMMs. A simi-
lar idea was applied to CRFs as well (Cohn, 2006;
Jeong et al., 2009).
In general, approximate algorithms have the ad-
vantage of speed over exact algorithms. However,
both types of algorithms are still widely adopted
by practitioners, since exact algorithms have mer-
its other than speed. First, the optimality of the so-
lution is always guaranteed. It is hard for most of
the approximate algorithms to even bound the er-
ror rate. Second, approximate algorithms usually
require hyperparameters, which control the trade-
off between accuracy and efficiency (e.g., beam
width), and these have to be manually adjusted.
On the other hand, most of the exact algorithms,
including ours, do not require such a manual ef-
fort.
Despite these advantages, exact algorithms are
rarely used when dealing with a large number of
labels. This is because exact algorithms become
considerably slower than approximate algorithms
in such situations. The paper presents an exact al-
gorithm that avoids this problem; it provides the
research community with another option for han-
dling a lot of labels.
</bodyText>
<sectionHeader confidence="0.996127" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.999626857142857">
This section presents the new decoding algorithm.
The key is to reduce the number of labels ex-
amined. Our algorithm locates the best label se-
quence by iteratively solving labeling problems
with a reduced label set. This results in signifi-
cant time savings in practice, because each itera-
tion becomes much more efficient than solving the
original labeling problem. More importantly, our
algorithm always obtains the exact solution. This
is because the algorithm allows us to check the op-
timality of the solution achieved by using only the
reduced label set.
In the following discussions, we restrict our fo-
cus to HMMs for presentation clarity. Extension to
</bodyText>
<figureCaption confidence="0.965568333333333">
Figure 1: (a) An example of a lattice, where the
letters {A, B, C, D, E, F, G, H} represent labels
associated with nodes. (b) The degenerate lattice.
</figureCaption>
<bodyText confidence="0.649217">
the perceptron algorithm is presented in Section 4.
</bodyText>
<subsectionHeader confidence="0.997684">
3.1 Degenerate lattice
</subsectionHeader>
<bodyText confidence="0.999989">
We begin by introducing the degenerate lattice,
which plays a central role in our algorithm. Con-
sider the lattice in Figure 1(a). Following conven-
tion, we regard each path on the lattice as a label
sequence. Note that the label set is {A, B, C, D,
E, F, G, H}. By aggregating several nodes in the
same column of the lattice, we can transform the
original lattice into a simpler form, which we call
the degenerate lattice (Figure 1(b)).
Let us examine the intuition behind the degen-
erate lattice. Aggregating nodes can be viewed as
grouping several labels into a new one. Here, a
label is referred to as an active label if it is not ag-
gregated (e.g., A, B, C, and D in the first column
of Figure 1(b)), and otherwise as an inactive label
(i.e., dotted nodes). The new label, which is made
by grouping the inactive labels, is referred to as
a degenerate label (i.e., large nodes covering the
dotted ones). Two degenerate labels can be seen
as equivalent if their corresponding inactive label
sets are the same (e.g., degenerate labels in the first
and the last column). In this approach, each path
of the degenerate lattice can also be interpreted as
a label sequence. In this case, however, the label to
be assigned is either an active label or a degenerate
label.
We then define the parameters associated with
degenerate label z. For reasons that will become
clear later, they are set to the maxima among the
parameters of the inactive labels:
</bodyText>
<figure confidence="0.984688444444444">
(b)
A
B
C
D
A
A
B
A
B
C
D
(a)
A A A A
B B B B
C
D
E E E E
F F F F
G G G G
H H H H
C
D
C
D
C
D
</figure>
<equation confidence="0.79431525">
log p(x|z) = max log p(x|y&apos;), (1)
y�∈I(z)
log p(z|y) = max log p(y&apos;|y), (2)
y&apos;∈I(z)
log p(y|z) = max log p(y|y�), (3)
y&apos;∈I(z)
log p(z|z&apos;) = max log p(y�|y~&amp;quot;), (4)
y&apos;∈I(z),yII∈I(z&apos;)
</equation>
<page confidence="0.870107">
487
</page>
<figure confidence="0.99998652">
A A A A
A A
B
B
A
B
A
B
A A
B
C
D
B
C
D
A
B
C
D
A
B
C
D
(a) (b) (c)
(a)
A A A A
B B B B
C
D
E E E E
F F F F
G G G G
H H H H
C
D
C
D
C
D
(b)
A
B
C
D
A A
B
A
B
C
D
</figure>
<figureCaption confidence="0.964229">
Figure 2: (a) The path y = {A, E, G, C} of the
original lattice. (b) The path z of the degenerate
lattice that corresponds to y.
</figureCaption>
<bodyText confidence="0.937136636363637">
where y is an active label, z and z� are degenerate
labels, and I(z) denotes one-to-one mapping from
z to its corresponding inactive label set.
The degenerate lattice has an important prop-
erty which is the key to our algorithm:
Lemma 1. If the best path of the degenerate lat-
tice does not include any degenerate label, it is
equivalent to the best path of the original lattice.
Proof. Let zmax be the best path of the degenerate
lattice. Our goal is to prove that if zmax does not
include any degenerate label, then
</bodyText>
<equation confidence="0.679153">
Vy E Y, log p(x, y) &lt; log p(x, zmax) (5)
</equation>
<bodyText confidence="0.9999155">
where Y is the set of all paths on the original lat-
tice. We prove this by partitioning Y into two dis-
joint sets: Y0 and Y1, where Y0 is the subset of
Y appearing in the degenerate lattice. Notice that
zmax E Y0. Since zmax is the best path of the
degenerate lattice, we have
</bodyText>
<equation confidence="0.503579">
Vy E Y0, log p(x, y) &lt; log p(x, zmax).
</equation>
<bodyText confidence="0.9997236">
The equation holds when y = zmax. We next ex-
amine the label sequence y such that y E Y1. For
each path y E Y1, there exists a unique path z on
the degenerate lattice that corresponds to y (Fig-
ure 2). Therefore, we have
</bodyText>
<equation confidence="0.9447105">
Vy E Y1, ]z E Z, log p(x, y) &lt; log p(x, z)
&lt; log p(x, zmax)
</equation>
<bodyText confidence="0.996674">
where Z is the set of all paths of the degenerate
lattice. The inequality log p(x, y) &lt; log p(x, z)
can be proved by using Equations (1)-(4). Using
these results, we can complete (5).
</bodyText>
<figureCaption confidence="0.9249824">
Figure 3: (a) The best path of the initial degenerate
lattice, which is denoted by the line, is located. (b)
The active labels are expanded and the best path is
searched again. (c) The best path without degen-
erate labels is obtained.
</figureCaption>
<subsectionHeader confidence="0.999058">
3.2 Staggered decoding
</subsectionHeader>
<bodyText confidence="0.999933457142857">
Now we can describe our algorithm, which we call
staggered decoding. The algorithm successively
constructs degenerate lattices and checks whether
the best path includes degenerate labels. In build-
ing each degenerate lattice, labels with high prob-
ability p(y), estimated from training data, are pref-
erentially selected as the active label; the expecta-
tion is that such labels are likely to belong to the
best path. The algorithm is detailed as follows:
Initialization step The algorithm starts by build-
ing a degenerate lattice in which there is only
one active label in each column. We select la-
bel y with the highest p(y) as the active label.
Search step The best path of the degenerate lat-
tice is located (Figure 3(a)). This is done
by using the Viterbi algorithm (and pruning
technique, as we describe in Section 3.3). If
the best path does not include any degenerate
label, we can terminate the algorithm since it
is identical with the best path of the original
lattice according to Lemma 1. Otherwise, we
proceed to the next step.
Expansion step We double the number of the ac-
tive labels in the degenerate lattice. The new
active labels are selected from the current in-
active label set in descending order of p(y).
If the inactive label set becomes empty, we
simply reconstructed the original lattice. Af-
ter expanding the active labels, we go back to
the previous step (Figure 3(b)). This proce-
dure is repeated until the termination condi-
tion in the search step is satisfied, i.e., the best
path has no degenerate label (Figure 3(c)).
Compared to the Viterbi algorithm, staggered
decoding requires two additional computations for
</bodyText>
<page confidence="0.995164">
488
</page>
<bodyText confidence="0.999656">
training. First, we have to estimate p(y) so as to
select active labels in the initialization and expan-
sion step. Second, we have to compute the pa-
rameters regarding degenerate labels according to
Equations (1)-(4). Both impose trivial computa-
tion costs.
Presuming that we traverse the lattice from left to
right, ω(yn) can be defined as
</bodyText>
<equation confidence="0.8050395">
max {ω(yn−1) + log p(yn|yn−1)} + log p(xn|yn).
yn−1
</equation>
<bodyText confidence="0.9699835">
If we traverse the lattice from right to left, an anal-
ogous score w(yn) can be defined as
</bodyText>
<subsectionHeader confidence="0.997579">
3.3 Pruning
</subsectionHeader>
<bodyText confidence="0.999992733333333">
To achieve speed-up, it is crucial that staggered
decoding efficiently performs the search step. For
this purpose, we can basically use the Viterbi algo-
rithm. In earlier iterations, the Viterbi algorithm is
indeed efficient because the label set to be han-
dled is much smaller than the original one. In later
iterations, however, our algorithm drastically in-
creases the number of labels, making Viterbi de-
coding quite expensive.
To handle this problem, we propose a method of
pruning the lattice nodes. This technique is moti-
vated by the observation that the degenerate lattice
shares many active labels with the previous itera-
tion. In the remainder of Section3.3, we explain
the technique by taking the following steps:
</bodyText>
<listItem confidence="0.97813175">
• Section 3.3.1 examines a lower bound l such
that l ≤ maxy log p(x, y).
• Section 3.3.2 examines the maximum score
MAX(yn) in case token xn takes label yn:
</listItem>
<equation confidence="0.9621145">
MAX(yn) = max
y�n=yn log p(x, y&apos;).
</equation>
<bodyText confidence="0.73628225">
• Section 3.3.3 presents our pruning procedure.
The idea is that if MAX(yn) &lt; l, then the
node corresponding to yn can be removed
from consideration.
</bodyText>
<subsectionHeader confidence="0.836209">
3.3.1 Lower bound
</subsectionHeader>
<bodyText confidence="0.9999273">
Lower bound l can be trivially calculated in the
search step. This can be done by retaining the
best path among those consisting of only active
labels. The score of that path is obviously the
lower bound. Since the search step is repeated un-
til the termination criteria is met, we can update
the lower bound at every search step. As the it-
eration proceeds, the degenerate lattice becomes
closer to the original one, so the lower bound be-
comes tighter.
</bodyText>
<subsectionHeader confidence="0.87289">
3.3.2 Maximum score
</subsectionHeader>
<bodyText confidence="0.999963333333333">
The maximum score MAX(yn) can be computed
from the original lattice. Let ω(yn) be the best
score of the partial label sequence ending with yn.
</bodyText>
<equation confidence="0.931103">
log p(xn|yn) + max {w(yn+1) + log p(yn|yn+1)}.
yn+1
Using these two scores, we have
MAX(yn) = ω(yn) + 0(yn) − log p(xn|yn).
</equation>
<bodyText confidence="0.99293925">
Notice that updating ω(yn) or w(yn) is equivalent
to the forward or backward Viterbi algorithm, re-
spectively.
Although it is expensive to compute ω(yn) and
w(yn), we can efficiently estimate their upper
bounds. Let λ(yn) and λ(yn) be scores analogous
to ω(yn) and w(yn) that are computed using the
degenerate lattice. We have ω(yn) ≤ λ(yn) and
w(yn) ≤ �λ(yn), by following similar discussions
as raised in the proof of Lemma 1. Therefore, we
can still check whether MAX(yn) is smaller than l
by using λ(yn) and λ(yn):
</bodyText>
<equation confidence="0.996024">
MAX(yn) = ω(yn) + �ω(yn) − log p(xn|yn)
≤ λ(yn) + λ(yn) − log p(xn|yn)
&lt; l.
</equation>
<bodyText confidence="0.9995395">
For the sake of simplicity, we assume that yn is an
active label. Although we do not discuss the other
cases, our pruning technique is also applicable to
them. We just point out that, if yn is an inactive
label, then there exists a degenerate label zn in the
n-th column such that yn ∈ I(zn), and we can use
λ(zn) and λ(zn) instead of λ(yn) and λ(yn).
We compute λ(yn) and λ(yn) by using the
forward and backward Viterbi algorithm, respec-
tively. In the search step immediately following
initialization, we perform the forward Viterbi al-
gorithm to find the best path, that is, λ(yn) is
updated for all yn. In the next search step, the
backward Viterbi algorithm is carried out, and
�λ(yn) is updated. In the succeeding search steps,
these updates are alternated. As the algorithm pro-
gresses, λ(yn) and �λ(yn) become closer to ω(yn)
and w(yn).
</bodyText>
<subsectionHeader confidence="0.903538">
3.3.3 Pruning procedure
</subsectionHeader>
<bodyText confidence="0.9998425">
We make use of the bounds in pruning the lattice
nodes. To do this, we keep the values of l, λ(yn)
</bodyText>
<page confidence="0.998366">
489
</page>
<bodyText confidence="0.9841055">
and A(yn). They are set as l = −∞ and A(yn) =
¯A(yn) = ∞ in the initialization step, and are up-
dated in the search step. The lower bound l is up-
dated at the end of the search step, while A(yn)
and ¯A(yn) can be updated during the running of
the Viterbi algorithm. When A(yn) or A(yn) is
changed, we check whether MAX(yn) &lt; l holds
and the node is pruned if the condition is met.
</bodyText>
<subsectionHeader confidence="0.984516">
3.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999743">
We provide here a theoretical analysis of staggered
decoding. In the following proofs, L, V , and N
represent the number of original labels, the num-
ber of distinct tokens, and the length of input token
sequence, respectively. To simplify the discussion,
we assume that log2 L is an integer (e.g., L = 64).
We first introduce three lemmas:
</bodyText>
<equation confidence="0.782179">
Lemma 2. Staggered decoding requires at most
(log2 L + 1) iterations to terminate.
</equation>
<bodyText confidence="0.928232833333333">
Proof. We have 2m−1 active labels in the m-th
search step (m = 1, 2 ... ), which means we have
L active labels and no degenerate labels in the
(log2 L + 1)-th search step. Therefore, the algo-
rithm always terminates within (log2 L + 1) itera-
tions.
</bodyText>
<equation confidence="0.4419075">
Lemma 3. The number of degenerate labels is
log2 L.
</equation>
<bodyText confidence="0.995281066666667">
Proof. Since we create one new degenerate label
in all but the last expansion step, we have log2 L
degenerate labels.
Lemma 4. The Viterbi algorithm requires O(L2+
LV ) memory space and has O(NL2) time com-
plexity.
Proof. Since we need O(L2) and O(LV ) space to
keep the transition and emission probability ma-
trices, we need O(L2 + LV ) space to perform
the Viterbi algorithm. The time complexity of the
Viterbi algorithm is O(NL2) since there are NL
nodes in the lattice and it takes O(L) time to eval-
uate the score of each node.
The above statements allow us to establish our
main results:
</bodyText>
<construct confidence="0.8661015">
Theorem 1. Staggered decoding requires O(L2 +
LV ) memory space.
</construct>
<footnote confidence="0.623991333333333">
Proof. Since we have L original labels and log2 L
degenerate labels, staggered decoding requires
O((L+log2 L)2+(L+log2 L)V ) = O(L2+LV)
</footnote>
<figureCaption confidence="0.972793666666667">
Figure 4: Staggered decoding with column-wise
expansion: (a) The best path of the initial degen-
erate lattice, which does not pass through the de-
generate label in the first column. (b) Column-
wise expansion is performed and the best path is
searched again. Notice that the active label in the
first column is not expanded. (c) The final result.
memory space to perform Viterbi decoding in the
search step.
Theorem 2. Staggered decoding has O(N) best
case time complexity and O(NL2) worst case time
complexity.
</figureCaption>
<bodyText confidence="0.998393">
Proof. To perform the m-th search step, staggered
decoding requires the order of O(N4m−1) time
because we have 2m−1 active labels. Therefore, it
has O(EMm=1 N4m−1) time complexity if it termi-
nates after the M-th search step. In the best case,
M = 1, the time complexity is O(N). In the worst
case, M = log2 L + 1, the time complexity is the
order of O(NL2) because �log� L+1
</bodyText>
<equation confidence="0.9144725">
m=1 N4m−1 &lt;
43NL2.
</equation>
<bodyText confidence="0.9995975">
Theorem 1 shows that staggered decoding
asymptotically requires the same order of mem-
ory space as the Viterbi algorithm. Theorem 2 re-
veals that staggered decoding has the same order
of time complexity as the Viterbi algorithm even
in the worst case.
</bodyText>
<subsectionHeader confidence="0.959564">
3.5 Heuristic techniques
</subsectionHeader>
<bodyText confidence="0.999962916666666">
We present two heuristic techniques for further
speeding up our algorithm.
First, we can initialize the value of lower bound
l by selecting a path from the original lattice in
some way, and then computing the score of that
path. In our experiments, we use the path lo-
cated by the left-to-right deterministic decoding
(i.e., beam search with a beam width of 1). Al-
though this method requires an additional cost to
locate the path, it is very effective in practice. If
l is initialized in this manner, the best case time
complexity of our algorithm becomes O(NL).
</bodyText>
<figure confidence="0.999772611111111">
(a) (b) (c)
A A A A
A A
B
A
B
A
B
A A
B
r
D
A
B
A
B
r
D
</figure>
<page confidence="0.996656">
490
</page>
<bodyText confidence="0.999948555555556">
The second technique is for the expansion step.
Instead of the expansion technique described in
Section 3.2, we can expand the active labels in a
heuristic manner to keep the number of active la-
bels small:
Column-wise expansion step We double the
number of the active labels in the column
only if the best path of the degenerate lattice
passes through the degenerate label of that
column (Figure 4).
A drawback of this strategy is that the algorithm
requires N(log2 L+1) iterations in the worst case.
As the result, we can no longer derive a reasonable
upper bound for the time complexity. Neverthe-
less, column-wise expansion is highly effective in
practice as we will demonstrate in the experiment.
Note that Theorem 1 still holds true even if we use
column-wise expansion.
</bodyText>
<sectionHeader confidence="0.967846" genericHeader="method">
4 Extension to the Perceptron
</sectionHeader>
<bodyText confidence="0.9994005">
The discussion we have made so far can be applied
to perceptrons. This can be clarified by comparing
the score functions f(x, y). In HMMs, the score
function can be written as
</bodyText>
<equation confidence="0.971261181818182">
�
��
N
log(xn|yn) + log(yn|yn−1) .
n=1
In perceptrons, on the other hand, it is given as
~N �� �
�
w1 kφ1 k(x, yn) + w2 kφ2 k(x,yn−1,yn)
n=1
k k
</equation>
<bodyText confidence="0.999943964285714">
where we explicitly distinguish the unigram fea-
ture function φ1k and bigram feature function φ2k.
Comparing the form of the two functions, we can
see that our discussion on HMMs can be extended
to perceptrons by substituting Ek w1kφ1k(x, yn)
and Ek w2kφ2k(x, yn−1, yn) for log p(xn|yn) and
log p(yn|yn−1).
However, implementing the perceptron algo-
rithm is not straightforward. The problem is
that it is difficult, if not impossible, to compute
�k w1kφ1k(x, y) and Ek w2kφ2k(x, y, y&apos;) offline be-
cause they are dependent on the entire token se-
quence x, unlike log p(x|y) and log p(y|y�). Con-
sequently, we cannot evaluate the maxima analo-
gous to Equations (1)-(4) offline either.
For unigram features, we compute the maxi-
mum, maxy Ek w1kφ1k(x, y), as a preprocess in
the initialization step (cf. Equation (1)). This pre-
process requires O(NL) time, which is negligible
compared with the cost required by the Viterbi al-
gorithm.
Unfortunately, we cannot use the same tech-
nique for computing maxy,y, Ek w2kφ2k(x, y, y&apos;)
because a similar computation would take
O(NL2) time (cf. Equation (4)). For bigram fea-
tures, we compute its upper bound offline. For ex-
ample, the following bound was proposed by Es-
posito and Radicioni (2009):
</bodyText>
<equation confidence="0.977716">
� �
max w2 kφ2 k(x, y, y~) ≤ max w2kδ(0 &lt; w2k)
y,y, y,y,
k k
</equation>
<bodyText confidence="0.999973217391305">
where δ(·) is the delta function and the summa-
tions are taken over all feature functions associated
with both y and y&apos;. Intuitively, the upper bound
corresponds to an ideal case in which all features
with positive weight are activated.3 It can be com-
puted without any task-specific knowledge.
In practice, however, we can compute better
bounds based on task-specific knowledge. The
simplest case is that the bigram features are inde-
pendent of the token sequence x. In such a situ-
ation, we can trivially compute the exact maxima
offline, as we did in the case of HMMs. Fortu-
nately, such a feature set is quite common in NLP
problems and we could use this technique in our
experiments. Even if bigram features are depen-
dent on x, it is still possible to compute better
bounds if several features are mutually exclusive,
as discussed in (Esposito and Radicioni, 2009).
Finally, it is worth noting that we can use stag-
gered decoding in training perceptrons as well, al-
though such application lies outside the scope of
this paper. The algorithm does not support train-
ing acceleration for other discriminative models.
</bodyText>
<sectionHeader confidence="0.991656" genericHeader="method">
5 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.997216">
5.1 Setting
</subsectionHeader>
<bodyText confidence="0.9993951">
The proposed algorithm was evaluated with three
tasks: POS tagging, joint POS tagging and chunk-
ing (called joint tagging for short), and supertag-
ging. To reduce joint tagging into a single se-
quence labeling problem, we produced the labels
by concatenating the POS tag and the chunk tag
(BIO format), e.g., NN/B-NP. In the two tasks
other than supertagging, the input token is the
word. In supertagging, the token is the pair of the
word and its oracle POS tag.
</bodyText>
<footnote confidence="0.938937">
3We assume binary feature functions.
</footnote>
<page confidence="0.998134">
491
</page>
<tableCaption confidence="0.999816">
Table 1: Decoding speed (sent./sec).
</tableCaption>
<table confidence="0.9722522">
POS tagging Joint tagging Supertagging
VITERBI 4000 77 1.1
CARPEDIEM 8600 51 0.26
SD 8800 850 121
SD+C-EXP. 14,000 1600 300
</table>
<bodyText confidence="0.998803392857143">
The data sets we used for the three experiments
are the Penn TreeBank (PTB) corpus, CoNLL
2000 corpus, and an HPSG treebank built from the
PTB corpus (Matsuzaki et al., 2007). We used sec-
tions 02-21 of PTB for training, and section 23 for
testing. The number of labels in the three tasks is
45, 319 and 2602, respectively.
We used the perceptron algorithm for train-
ing. The models were averaged over 10 itera-
tions (Collins, 2002). For features, we basically
followed previous studies (Tsuruoka and Tsujii,
2005; Sha and Pereira, 2003; Ninomiya et al.,
2006). In POS tagging, we used unigrams of the
current and its neighboring words, word bigrams,
prefixes and suffixes of the current word, capital-
ization, and tag bigrams. In joint tagging, we also
used the same features. In supertagging, we used
POS unigrams and bigrams in addition to the same
features other than capitalization.
As the evaluation measure, we used the average
decoding speed (sentences/sec) to two significant
digits over five trials. To strictly measure the time
spent for decoding, we ignored the preprocessing
time, that is, the time for loading the model file
and converting the features (i.e., strings) into inte-
gers. We note that the accuracy was comparable to
the state-of-the-art in the three tasks: 97.08, 93.21,
and 91.20% respectively.
</bodyText>
<subsectionHeader confidence="0.866949">
5.2 Results and discussions
</subsectionHeader>
<bodyText confidence="0.999577214285715">
Table 1 presents the performance of our algo-
rithm. SD represents the proposed algorithm with-
out column-wise expansion, while SD+C-EXP.
uses column-wise expansion. For comparison, we
present the results of two baseline algorithms as
well: VITERBI and CARPEDIEM (Esposito and
Radicioni, 2009). In almost all settings, we see
that both of our algorithms outperformed the other
two. We also find that SD+C-EXP. performed con-
sistently better than SD. This indicates the effec-
tiveness of column-wise expansion.
Following VITERBI, CARPEDIEM is the most
relevant algorithm, for sequence labeling in NLP,
as discussed in Section 2.3. However, our results
</bodyText>
<tableCaption confidence="0.989064">
Table 2: The average number of iterations.
</tableCaption>
<table confidence="0.980734">
POS tagging Joint tagging Supertagging
SD 6.02 8.15 10.0
SD+C-EXP. 6.12 8.62 10.6
</table>
<tableCaption confidence="0.996919">
Table 3: Training time.
</tableCaption>
<table confidence="0.474026">
POS tagging Joint tagging Supertagging
VITERBI 100 sec. 20 min. 100 hour
SD+C-EXP. 37 sec. 1.5 min. 5.3 hour
</table>
<bodyText confidence="0.99982576">
demonstrated that CARPEDIEM worked poorly in
two of the three tasks. We consider this is because
the transition information is crucial for the two
tasks, and the assumption behind CARPEDIEM is
violated. In contrast, the proposed algorithms per-
formed reasonably well for all three tasks, demon-
strating the wide applicability of our algorithm.
Table 2 presents the average iteration num-
bers of SD and SD+C-EXP. We can observe
that the two algorithms required almost the same
number of iterations on average, although the
iteration number is not tightly bounded if we
use column-wise expansion. This indicates that
SD+C-EXP. virtually avoided performing extra it-
erations, while heuristically restricting active label
expansion.
Table 3 compares the training time spent by
VITERBI and SD+C-EXP. Although speeding up
perceptron training is a by-product, it is interest-
ing to see that our algorithm is in fact effective at
reducing the training time as well. The result also
indicates that the speed-up is more significant at
test time. This is probably because the model is
not predictive enough at the beginning of training,
and the pruning is not that effective.
</bodyText>
<subsectionHeader confidence="0.999227">
5.3 Comparison with approximate algorithm
</subsectionHeader>
<bodyText confidence="0.9999744">
Table 4 compares two exact algorithms (VITERBI
and SD+E-XP.) with beam search, which is the ap-
proximate algorithm widely adopted for sequence
labeling in NLP. For this experiment, the beam
width, B, was exhaustively calibrated: we tried B
= 11, 2, 4, 8, ...1 until the beam search achieved
comparable accuracy to the exact algorithms, i.e.,
the difference fell below 0.1 in our case.
We see that there is a substantial difference in
the performance between VITERBI and BEAM.
On the other hand, SD+C-EXP. reached speeds
very close to those of BEAM. In fact, they
achieved comparable performance in our exper-
iment. These results demonstrate that we could
successfully bridge the gap in the performance be-
</bodyText>
<page confidence="0.998444">
492
</page>
<tableCaption confidence="0.999926">
Table 4: Comparison with beam search (sent./sec).
</tableCaption>
<table confidence="0.836635">
POS tagging Joint tagging Supertagging
VITERBI 4000 77 1.1
SD+C-EXP. 14,000 1600 300
BEAM 18,000 2400 180
</table>
<bodyText confidence="0.985628">
tween exact and approximate algorithms, while re-
taining the advantages of exact algorithms.
</bodyText>
<sectionHeader confidence="0.985435" genericHeader="method">
6 Relation to coarse-to-fine approach
</sectionHeader>
<bodyText confidence="0.999962733333333">
Before concluding remarks, we briefly examine
the relationship between staggered decoding and
coarse-to-fine PCFG parsing (2006). In coarse-to-
fine parsing, the candidate parse trees are pruned
by using the parse forest produced by a coarse-
grained PCFG. Since the degenerate label can be
interpreted as a coarse-level label, one may con-
sider that staggered decoding is an instance of
coarse-to-fine approach. While there is some re-
semblance, there are at least two essential differ-
ences. First, coarse-to-fine approach is a heuristic
pruning, that is, it is not an exact algorithm. Sec-
ond, our algorithm does not always perform de-
coding at the fine-grained level. It is designed to
be able to stop decoding at the coarse-level.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999983529411765">
The sequence labeling algorithm is indispensable
to modern statistical NLP. However, the Viterbi
algorithm, which is the standard decoding algo-
rithm in NLP, is not efficient when we have to
deal with a large number of labels. In this paper
we presented staggered decoding, which provides
a principled way of resolving this problem. We
consider that it is a real alternative to the Viterbi
algorithm in various NLP tasks.
An interesting future direction is to extend the
proposed technique to handle more complex struc-
tures than the Markov chains, including semi-
Markov models and factorial HMMs (Sarawagi
and Cohen, 2004; Sutton et al., 2004). We hope
this work opens a new perspective on decoding al-
gorithms for a wide range of NLP problems, not
just sequence labeling.
</bodyText>
<sectionHeader confidence="0.985222" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9983878">
We wish to thank the anonymous reviewers for
their helpful comments, especially on the com-
putational complexity of our algorithm. We also
thank Yusuke Miyao for providing us with the
HPSG Treebank data.
</bodyText>
<sectionHeader confidence="0.998266" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999937166666666">
Thorsten Brants. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings ofANLP, pages 224–
231.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R. Shrivaths, Jeremy Moore, Michael Pozar,
and Theresa Vu. 2006. Multi-level coarse-to-fine
PCFG parsing. In Proceedings of NAACL, pages
168–175.
Trevor Cohn. 2006. Efficient inference in large con-
ditional random fields. In Proceedings of ECML,
pages 606–613.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1–8.
Thomas G. Dietterich, Pedro Domingos, Lise Getoor,
Stephen Muggleton, and Prasad Tadepalli. 2008.
Structured machine learning: the next ten years.
Machine Learning, 73(1):3–23.
Roberto Esposito and Daniele P. Radicioni. 2009.
CARPEDIEM: Optimizing the Viterbi algorithm
and applications to supervised sequential learning.
Jorunal of Machine Learning Research, 10:1851–
1880.
Pedro F. Felzenszwalb, Daniel P. Huttenlocher, and
Jon M. Kleinberg. 2003. Fast algorithms for large-
state-space HMMs with applications to Web usage
analysis. In Proceedings ofNIPS, pages 409–416.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Efficient inference of CRFs for large-scale
natural language data. In Proceedings of ACL-
IJCNLP Short Papers, pages 281–284.
John Lafferty, Andrew McCallum, and Fernand
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282–
289.
Percy Liang, Hal Daum´e III, and Dan Klein. 2008.
Structure compilation: Trading structure for fea-
tures. In Proceedings ofICML, pages 592–599.
Yury Lifshits, Shay Mozes, Oren Weimann, and Michal
Ziv-Ukelson. 2007. Speeding up HMM decod-
ing and training by exploiting sequence repetitions.
Computational Pattern Matching, pages 4–15.
Dekang Lin and Xiaoyun Wu. 2009. Phrae clustering
for discriminative training. In Proceedings ofACL-
IJCNLP, pages 1030–1038.
</reference>
<page confidence="0.991231">
493
</page>
<reference confidence="0.996759522727272">
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertagging
and CFG-filtering. In Proceedings of IJCAI, pages
1671–1676.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of EMNLP, pages
155–163.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. In Proceedings of The IEEE, pages
257–286.
Sunita Sarawagi and Willian W. Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Proceedings of NIPS, pages 1185–
1192.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL, pages 134–141.
Sajid M. Siddiqi and Andrew W. Moore. 2005. Fast
inference and learning in large-state-space HMMs.
In Proceedings ofICML, pages 800–807.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional ran-
dom fields: Factorized probabilistic models for la-
beling and segmenting sequence data. In Proceed-
ings ofICML.
Ben Tasker, Carlos Guestrin, and Daphe Koller. 2003.
Max-margin Markov networks. In Proceedings of
NIPS, pages 25–32.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453–1484.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of
HLT/EMNLP, pages 467–474.
Andrew J. Viterbi. 1967. Error bounds for convo-
lutional codes and an asymeptotically optimum de-
coding algorithm. IEEE Transactios on Information
Theory, 13(2):260–267.
</reference>
<page confidence="0.998992">
494
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.304291">
<title confidence="0.999229">Efficient Staggered Decoding for Sequence Labeling</title>
<author confidence="0.983566">Nobuhiro Kaji Yasuhiro Fujiwara Naoki Yoshinaga Masaru Kitsuregawa</author>
<affiliation confidence="0.999973">Institute of Industrial Science, The University of Tokyo,</affiliation>
<address confidence="0.97449">4-6-1, Komaba, Meguro-ku, Tokyo, 153-8505 Japan</address>
<abstract confidence="0.989010631578947">The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algoand Radi-</abstract>
<address confidence="0.435772">cioni, 2009).</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT - a statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings ofANLP,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="2285" citStr="Brants, 2000" startWordPosition="353" endWordPosition="354">lexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitively slow when dealing with a large number of labels, since its computational cost is quadratic in L (Dietterich et al., 2008). Unfortunately, several sequence-labeling problems in NLP involve a large number of labels. For example, there are more than 40 and 2000 labels in POS tagging and supertagging, respectively (Brants, 2000; Matsuzaki et al., 2007). These tasks incur much higher computational costs than simpler tasks like NP chunking. What is worse, the number of labels grows drastically if we jointly perform multiple tasks. As we shall see later, we need over 300 labels to reduce joint POS tagging and chunking into the single sequence labeling problem. Although joint learning has attracted much attention in recent years, how to perform decoding efficiently still remains an open problem. In this paper, we present a new decoding algorithm that overcomes this problem. The proposed algorithm has three distinguishin</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT - a statistical part-ofspeech tagger. In Proceedings ofANLP, pages 224– 231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Micha Elsner</author>
<author>Joseph Austerweil</author>
<author>David Ellis</author>
<author>Isaac Haxton</author>
<author>Catherine Hill</author>
<author>R Shrivaths</author>
<author>Jeremy Moore</author>
<author>Michael Pozar</author>
<author>Theresa Vu</author>
</authors>
<title>Multi-level coarse-to-fine PCFG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>168--175</pages>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, Vu, 2006</marker>
<rawString>Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil, David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths, Jeremy Moore, Michael Pozar, and Theresa Vu. 2006. Multi-level coarse-to-fine PCFG parsing. In Proceedings of NAACL, pages 168–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
</authors>
<title>Efficient inference in large conditional random fields.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML,</booktitle>
<pages>606--613</pages>
<contexts>
<context position="8521" citStr="Cohn, 2006" startWordPosition="1386" endWordPosition="1387">ly correlated. This assumption is appropriate for K k=1 N n=1 486 some NLP tasks. For example, as suggested in (Liang et al., 2008), adjacent labels do not provide strong information in POS tagging. However, the applicability of this idea to other NLP tasks is still unclear. Approximate algorithms, such as beam search or island-driven search, have been proposed for speeding up decoding. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong et al., 2009). In general, approximate algorithms have the advantage of speed over exact algorithms. However, both types of algorithms are still widely adopted by practitioners, since exact algorithms have merits other than speed. First, the optimality of the solution is always guaranteed. It is hard for most of the approximate algorithms to even bound the error rate. Second, approximate algorithms usually require hyperparameters, which control the tradeoff between accuracy and efficiency (e.g., beam width), and these have to be manually adjusted. On the other hand, most of the exact a</context>
</contexts>
<marker>Cohn, 2006</marker>
<rawString>Trevor Cohn. 2006. Efficient inference in large conditional random fields. In Proceedings of ECML, pages 606–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1234" citStr="Collins, 2002" startWordPosition="182" endWordPosition="183">operty of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, CARPEDIEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(NL2) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1The first-order Markov assumption is ma</context>
<context position="5137" citStr="Collins, 2002" startWordPosition="809" endWordPosition="810">n over (x, y). If we assume a one-to-one correspondence between the hidden states and the labels, the score function can be written as: f(x, y) = log p(x, y) = log p(x|y) + log p(y) N N log p(xn|yn)+ log p(yn|yn−1). n=1 n=1 The parameters log p(xn|yn) and log p(yn|yn−1) are usually estimated using maximum likelihood or the EM algorithm. Since parameter estimation lies outside the scope of this paper, a detailed description is omitted. Discriminative models Recent years have seen the emergence of discriminative training methods for sequence labeling (Lafferty et al., 2001; Tasker et al., 2003; Collins, 2002; Tsochantaridis et al., 2005). Among them, we focus on the perceptron algorithm (Collins, 2002). Although we do not discuss the other discriminative models, our algorithm is equivalently applicable to them. The major difference between those models lies in parameter estimation; the decoding process is virtually the same. In the perceptron, the score function f(x, y) is given as f(x, y) = w · φ(x, y) where w is the weight vector, and φ(x, y) is the feature vector representation of the pair (x, y). By making the first-order Markov assumption, we have f(x, y) = w · φ(x, y) wkφk(x, yn−1, yn), whe</context>
<context position="27914" citStr="Collins, 2002" startWordPosition="4889" endWordPosition="4890"> feature functions. 491 Table 1: Decoding speed (sent./sec). POS tagging Joint tagging Supertagging VITERBI 4000 77 1.1 CARPEDIEM 8600 51 0.26 SD 8800 850 121 SD+C-EXP. 14,000 1600 300 The data sets we used for the three experiments are the Penn TreeBank (PTB) corpus, CoNLL 2000 corpus, and an HPSG treebank built from the PTB corpus (Matsuzaki et al., 2007). We used sections 02-21 of PTB for training, and section 23 for testing. The number of labels in the three tasks is 45, 319 and 2602, respectively. We used the perceptron algorithm for training. The models were averaged over 10 iterations (Collins, 2002). For features, we basically followed previous studies (Tsuruoka and Tsujii, 2005; Sha and Pereira, 2003; Ninomiya et al., 2006). In POS tagging, we used unigrams of the current and its neighboring words, word bigrams, prefixes and suffixes of the current word, capitalization, and tag bigrams. In joint tagging, we also used the same features. In supertagging, we used POS unigrams and bigrams in addition to the same features other than capitalization. As the evaluation measure, we used the average decoding speed (sentences/sec) to two significant digits over five trials. To strictly measure the</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
<author>Pedro Domingos</author>
<author>Lise Getoor</author>
<author>Stephen Muggleton</author>
<author>Prasad Tadepalli</author>
</authors>
<title>Structured machine learning: the next ten years.</title>
<date>2008</date>
<booktitle>Machine Learning,</booktitle>
<volume>73</volume>
<issue>1</issue>
<contexts>
<context position="2081" citStr="Dietterich et al., 2008" startWordPosition="320" endWordPosition="323">o find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(NL2) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitively slow when dealing with a large number of labels, since its computational cost is quadratic in L (Dietterich et al., 2008). Unfortunately, several sequence-labeling problems in NLP involve a large number of labels. For example, there are more than 40 and 2000 labels in POS tagging and supertagging, respectively (Brants, 2000; Matsuzaki et al., 2007). These tasks incur much higher computational costs than simpler tasks like NP chunking. What is worse, the number of labels grows drastically if we jointly perform multiple tasks. As we shall see later, we need over 300 labels to reduce joint POS tagging and chunking into the single sequence labeling problem. Although joint learning has attracted much attention in rec</context>
</contexts>
<marker>Dietterich, Domingos, Getoor, Muggleton, Tadepalli, 2008</marker>
<rawString>Thomas G. Dietterich, Pedro Domingos, Lise Getoor, Stephen Muggleton, and Prasad Tadepalli. 2008. Structured machine learning: the next ten years. Machine Learning, 73(1):3–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Esposito</author>
<author>Daniele P Radicioni</author>
</authors>
<title>CARPEDIEM: Optimizing the Viterbi algorithm and applications to supervised sequential learning.</title>
<date>2009</date>
<journal>Jorunal of Machine Learning Research,</journal>
<volume>10</volume>
<pages>1880</pages>
<contexts>
<context position="1020" citStr="Esposito and Radicioni, 2009" startWordPosition="146" endWordPosition="150">terbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, CARPEDIEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viter</context>
<context position="3518" citStr="Esposito and Radicioni, 2009" startWordPosition="548" endWordPosition="551">operties: (1) It is much more efficient than the Viterbi algorithm when dealing with a large number of labels. (2) It is an exact algorithm, that is, the optimality of the solution is always guaranteed unlike approximate algorithms. (3) It is automatic, requiring no taskdependent hyperparameters that have to be manually adjusted. Experiments evaluate our algorithm on three tasks: POS tagging, joint POS tagging and chunking, and supertaggini. The results demonstrate that our algorithm is up to several orders of magnitude faster than the basic Viterbi algorithm and a state-of-the-art algorithm (Esposito and Radicioni, 2009); it makes exact decoding practical even in labeling problems with a large label set. 2 Preliminaries We first provide a brief overview of sequence labeling and introduce related work. 2Our implementation is available at http://www.tkl.iis.utokyo.ac.jp/˜kaji/staggered 485 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 485–494, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2.1 Models Sequence labeling is the problem of predicting label sequence y = {yn}Nn=1 for given token sequence x = {xn}Nn=1. This is typical</context>
<context position="7797" citStr="Esposito and Radicioni, 2009" startWordPosition="1267" endWordPosition="1270">cessful to some extent, they demand strong assumptions that are unusual in NLP. Moreover, none were challenged with standard NLP tasks. Felzenszwalb et al. (2003) presented a fast inference algorithm for HMMs based on the assumption that the hidden states can be embedded in a grid space, and the transition probability corresponds to the distance on that space. This type of probability distribution is not common in NLP tasks. Lifshits et al. (2007) proposed a compression-based approach to speed up HMM decoding. It assumes that the input sequence is highly repetitive. Amongst others, CARPEDIEM (Esposito and Radicioni, 2009) is the algorithm closest to our work. It accelerates decoding by assuming that the adjacent labels are not strongly correlated. This assumption is appropriate for K k=1 N n=1 486 some NLP tasks. For example, as suggested in (Liang et al., 2008), adjacent labels do not provide strong information in POS tagging. However, the applicability of this idea to other NLP tasks is still unclear. Approximate algorithms, such as beam search or island-driven search, have been proposed for speeding up decoding. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (200</context>
<context position="25606" citStr="Esposito and Radicioni (2009)" startWordPosition="4485" endWordPosition="4489">uently, we cannot evaluate the maxima analogous to Equations (1)-(4) offline either. For unigram features, we compute the maximum, maxy Ek w1kφ1k(x, y), as a preprocess in the initialization step (cf. Equation (1)). This preprocess requires O(NL) time, which is negligible compared with the cost required by the Viterbi algorithm. Unfortunately, we cannot use the same technique for computing maxy,y, Ek w2kφ2k(x, y, y&apos;) because a similar computation would take O(NL2) time (cf. Equation (4)). For bigram features, we compute its upper bound offline. For example, the following bound was proposed by Esposito and Radicioni (2009): � � max w2 kφ2 k(x, y, y~) ≤ max w2kδ(0 &lt; w2k) y,y, y,y, k k where δ(·) is the delta function and the summations are taken over all feature functions associated with both y and y&apos;. Intuitively, the upper bound corresponds to an ideal case in which all features with positive weight are activated.3 It can be computed without any task-specific knowledge. In practice, however, we can compute better bounds based on task-specific knowledge. The simplest case is that the bigram features are independent of the token sequence x. In such a situation, we can trivially compute the exact maxima offline, </context>
<context position="29121" citStr="Esposito and Radicioni, 2009" startWordPosition="5073" endWordPosition="5076">o strictly measure the time spent for decoding, we ignored the preprocessing time, that is, the time for loading the model file and converting the features (i.e., strings) into integers. We note that the accuracy was comparable to the state-of-the-art in the three tasks: 97.08, 93.21, and 91.20% respectively. 5.2 Results and discussions Table 1 presents the performance of our algorithm. SD represents the proposed algorithm without column-wise expansion, while SD+C-EXP. uses column-wise expansion. For comparison, we present the results of two baseline algorithms as well: VITERBI and CARPEDIEM (Esposito and Radicioni, 2009). In almost all settings, we see that both of our algorithms outperformed the other two. We also find that SD+C-EXP. performed consistently better than SD. This indicates the effectiveness of column-wise expansion. Following VITERBI, CARPEDIEM is the most relevant algorithm, for sequence labeling in NLP, as discussed in Section 2.3. However, our results Table 2: The average number of iterations. POS tagging Joint tagging Supertagging SD 6.02 8.15 10.0 SD+C-EXP. 6.12 8.62 10.6 Table 3: Training time. POS tagging Joint tagging Supertagging VITERBI 100 sec. 20 min. 100 hour SD+C-EXP. 37 sec. 1.5 </context>
</contexts>
<marker>Esposito, Radicioni, 2009</marker>
<rawString>Roberto Esposito and Daniele P. Radicioni. 2009. CARPEDIEM: Optimizing the Viterbi algorithm and applications to supervised sequential learning. Jorunal of Machine Learning Research, 10:1851– 1880.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro F Felzenszwalb</author>
<author>Daniel P Huttenlocher</author>
<author>Jon M Kleinberg</author>
</authors>
<title>Fast algorithms for largestate-space HMMs with applications to Web usage analysis.</title>
<date>2003</date>
<booktitle>In Proceedings ofNIPS,</booktitle>
<pages>409--416</pages>
<contexts>
<context position="7330" citStr="Felzenszwalb et al. (2003)" startWordPosition="1191" endWordPosition="1194"> is the number of labels; it is efficient enough if L is small. However, if there are many labels, the Viterbi algorithm becomes prohibitively slow because of its quadratic dependence on L. 2.3 Related work To the best of our knowledge, the Viterbi algorithm is the only algorithm widely adopted in the NLP field that offers exact decoding. In other communities, several exact algorithms have already been proposed for handling large label sets. While they are successful to some extent, they demand strong assumptions that are unusual in NLP. Moreover, none were challenged with standard NLP tasks. Felzenszwalb et al. (2003) presented a fast inference algorithm for HMMs based on the assumption that the hidden states can be embedded in a grid space, and the transition probability corresponds to the distance on that space. This type of probability distribution is not common in NLP tasks. Lifshits et al. (2007) proposed a compression-based approach to speed up HMM decoding. It assumes that the input sequence is highly repetitive. Amongst others, CARPEDIEM (Esposito and Radicioni, 2009) is the algorithm closest to our work. It accelerates decoding by assuming that the adjacent labels are not strongly correlated. This</context>
</contexts>
<marker>Felzenszwalb, Huttenlocher, Kleinberg, 2003</marker>
<rawString>Pedro F. Felzenszwalb, Daniel P. Huttenlocher, and Jon M. Kleinberg. 2003. Fast algorithms for largestate-space HMMs with applications to Web usage analysis. In Proceedings ofNIPS, pages 409–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Chin-Yew Lin</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Efficient inference of CRFs for large-scale natural language data.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP Short Papers,</booktitle>
<pages>281--284</pages>
<contexts>
<context position="8542" citStr="Jeong et al., 2009" startWordPosition="1388" endWordPosition="1391">d. This assumption is appropriate for K k=1 N n=1 486 some NLP tasks. For example, as suggested in (Liang et al., 2008), adjacent labels do not provide strong information in POS tagging. However, the applicability of this idea to other NLP tasks is still unclear. Approximate algorithms, such as beam search or island-driven search, have been proposed for speeding up decoding. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong et al., 2009). In general, approximate algorithms have the advantage of speed over exact algorithms. However, both types of algorithms are still widely adopted by practitioners, since exact algorithms have merits other than speed. First, the optimality of the solution is always guaranteed. It is hard for most of the approximate algorithms to even bound the error rate. Second, approximate algorithms usually require hyperparameters, which control the tradeoff between accuracy and efficiency (e.g., beam width), and these have to be manually adjusted. On the other hand, most of the exact algorithms, including </context>
</contexts>
<marker>Jeong, Lin, Lee, 2009</marker>
<rawString>Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee. 2009. Efficient inference of CRFs for large-scale natural language data. In Proceedings of ACLIJCNLP Short Papers, pages 281–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernand Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1218" citStr="Lafferty et al., 2001" startWordPosition="178" endWordPosition="181">his problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, CARPEDIEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(NL2) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1The first-order Markov </context>
<context position="5101" citStr="Lafferty et al., 2001" startWordPosition="801" endWordPosition="804">f(x, y) is the joint probability distribution over (x, y). If we assume a one-to-one correspondence between the hidden states and the labels, the score function can be written as: f(x, y) = log p(x, y) = log p(x|y) + log p(y) N N log p(xn|yn)+ log p(yn|yn−1). n=1 n=1 The parameters log p(xn|yn) and log p(yn|yn−1) are usually estimated using maximum likelihood or the EM algorithm. Since parameter estimation lies outside the scope of this paper, a detailed description is omitted. Discriminative models Recent years have seen the emergence of discriminative training methods for sequence labeling (Lafferty et al., 2001; Tasker et al., 2003; Collins, 2002; Tsochantaridis et al., 2005). Among them, we focus on the perceptron algorithm (Collins, 2002). Although we do not discuss the other discriminative models, our algorithm is equivalently applicable to them. The major difference between those models lies in parameter estimation; the decoding process is virtually the same. In the perceptron, the score function f(x, y) is given as f(x, y) = w · φ(x, y) where w is the weight vector, and φ(x, y) is the feature vector representation of the pair (x, y). By making the first-order Markov assumption, we have f(x, y) </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernand Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, pages 282– 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daum´e</author>
<author>Dan Klein</author>
</authors>
<title>Structure compilation: Trading structure for features.</title>
<date>2008</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>592--599</pages>
<marker>Liang, Daum´e, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daum´e III, and Dan Klein. 2008. Structure compilation: Trading structure for features. In Proceedings ofICML, pages 592–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yury Lifshits</author>
<author>Shay Mozes</author>
<author>Oren Weimann</author>
<author>Michal Ziv-Ukelson</author>
</authors>
<title>Speeding up HMM decoding and training by exploiting sequence repetitions.</title>
<date>2007</date>
<journal>Computational Pattern Matching,</journal>
<pages>4--15</pages>
<contexts>
<context position="7619" citStr="Lifshits et al. (2007)" startWordPosition="1242" endWordPosition="1245">ed in the NLP field that offers exact decoding. In other communities, several exact algorithms have already been proposed for handling large label sets. While they are successful to some extent, they demand strong assumptions that are unusual in NLP. Moreover, none were challenged with standard NLP tasks. Felzenszwalb et al. (2003) presented a fast inference algorithm for HMMs based on the assumption that the hidden states can be embedded in a grid space, and the transition probability corresponds to the distance on that space. This type of probability distribution is not common in NLP tasks. Lifshits et al. (2007) proposed a compression-based approach to speed up HMM decoding. It assumes that the input sequence is highly repetitive. Amongst others, CARPEDIEM (Esposito and Radicioni, 2009) is the algorithm closest to our work. It accelerates decoding by assuming that the adjacent labels are not strongly correlated. This assumption is appropriate for K k=1 N n=1 486 some NLP tasks. For example, as suggested in (Liang et al., 2008), adjacent labels do not provide strong information in POS tagging. However, the applicability of this idea to other NLP tasks is still unclear. Approximate algorithms, such as </context>
</contexts>
<marker>Lifshits, Mozes, Weimann, Ziv-Ukelson, 2007</marker>
<rawString>Yury Lifshits, Shay Mozes, Oren Weimann, and Michal Ziv-Ukelson. 2007. Speeding up HMM decoding and training by exploiting sequence repetitions. Computational Pattern Matching, pages 4–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrae clustering for discriminative training.</title>
<date>2009</date>
<booktitle>In Proceedings ofACLIJCNLP,</booktitle>
<pages>1030--1038</pages>
<contexts>
<context position="1407" citStr="Lin and Wu, 2009" startWordPosition="211" endWordPosition="214">ks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, CARPEDIEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(NL2) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitively slow when dealing with a large number of labels</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrae clustering for discriminative training. In Proceedings ofACLIJCNLP, pages 1030–1038.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficient HPSG parsing with supertagging and CFG-filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1671--1676</pages>
<contexts>
<context position="2310" citStr="Matsuzaki et al., 2007" startWordPosition="355" endWordPosition="358"> N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitively slow when dealing with a large number of labels, since its computational cost is quadratic in L (Dietterich et al., 2008). Unfortunately, several sequence-labeling problems in NLP involve a large number of labels. For example, there are more than 40 and 2000 labels in POS tagging and supertagging, respectively (Brants, 2000; Matsuzaki et al., 2007). These tasks incur much higher computational costs than simpler tasks like NP chunking. What is worse, the number of labels grows drastically if we jointly perform multiple tasks. As we shall see later, we need over 300 labels to reduce joint POS tagging and chunking into the single sequence labeling problem. Although joint learning has attracted much attention in recent years, how to perform decoding efficiently still remains an open problem. In this paper, we present a new decoding algorithm that overcomes this problem. The proposed algorithm has three distinguishing properties: (1) It is m</context>
<context position="27659" citStr="Matsuzaki et al., 2007" startWordPosition="4841" endWordPosition="4844">, we produced the labels by concatenating the POS tag and the chunk tag (BIO format), e.g., NN/B-NP. In the two tasks other than supertagging, the input token is the word. In supertagging, the token is the pair of the word and its oracle POS tag. 3We assume binary feature functions. 491 Table 1: Decoding speed (sent./sec). POS tagging Joint tagging Supertagging VITERBI 4000 77 1.1 CARPEDIEM 8600 51 0.26 SD 8800 850 121 SD+C-EXP. 14,000 1600 300 The data sets we used for the three experiments are the Penn TreeBank (PTB) corpus, CoNLL 2000 corpus, and an HPSG treebank built from the PTB corpus (Matsuzaki et al., 2007). We used sections 02-21 of PTB for training, and section 23 for testing. The number of labels in the three tasks is 45, 319 and 2602, respectively. We used the perceptron algorithm for training. The models were averaged over 10 iterations (Collins, 2002). For features, we basically followed previous studies (Tsuruoka and Tsujii, 2005; Sha and Pereira, 2003; Ninomiya et al., 2006). In POS tagging, we used unigrams of the current and its neighboring words, word bigrams, prefixes and suffixes of the current word, capitalization, and tag bigrams. In joint tagging, we also used the same features. </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Efficient HPSG parsing with supertagging and CFG-filtering. In Proceedings of IJCAI, pages 1671–1676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Extremely lexicalized models for accurate and fast HPSG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>155--163</pages>
<contexts>
<context position="28042" citStr="Ninomiya et al., 2006" startWordPosition="4906" endWordPosition="4909">CARPEDIEM 8600 51 0.26 SD 8800 850 121 SD+C-EXP. 14,000 1600 300 The data sets we used for the three experiments are the Penn TreeBank (PTB) corpus, CoNLL 2000 corpus, and an HPSG treebank built from the PTB corpus (Matsuzaki et al., 2007). We used sections 02-21 of PTB for training, and section 23 for testing. The number of labels in the three tasks is 45, 319 and 2602, respectively. We used the perceptron algorithm for training. The models were averaged over 10 iterations (Collins, 2002). For features, we basically followed previous studies (Tsuruoka and Tsujii, 2005; Sha and Pereira, 2003; Ninomiya et al., 2006). In POS tagging, we used unigrams of the current and its neighboring words, word bigrams, prefixes and suffixes of the current word, capitalization, and tag bigrams. In joint tagging, we also used the same features. In supertagging, we used POS unigrams and bigrams in addition to the same features other than capitalization. As the evaluation measure, we used the average decoding speed (sentences/sec) to two significant digits over five trials. To strictly measure the time spent for decoding, we ignored the preprocessing time, that is, the time for loading the model file and converting the fea</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Extremely lexicalized models for accurate and fast HPSG parsing. In Proceedings of EMNLP, pages 155–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of The IEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="1195" citStr="Rabiner, 1989" startWordPosition="176" endWordPosition="177">hat overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, CARPEDIEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(NL2) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1</context>
<context position="4450" citStr="Rabiner, 1989" startWordPosition="696" endWordPosition="697"> Association for Computational Linguistics, pages 485–494, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2.1 Models Sequence labeling is the problem of predicting label sequence y = {yn}Nn=1 for given token sequence x = {xn}Nn=1. This is typically done by defining a score function f(x, y) and locating the best label sequence: ymax = argmax f(x, y). Y The form of f(x, y) is dependent on the learning model used. Here, we introduce two models widely used in the literature. Generative models HMM is the most famous generative model for labeling token sequences (Rabiner, 1989). In HMMs, the score function f(x, y) is the joint probability distribution over (x, y). If we assume a one-to-one correspondence between the hidden states and the labels, the score function can be written as: f(x, y) = log p(x, y) = log p(x|y) + log p(y) N N log p(xn|yn)+ log p(yn|yn−1). n=1 n=1 The parameters log p(xn|yn) and log p(yn|yn−1) are usually estimated using maximum likelihood or the EM algorithm. Since parameter estimation lies outside the scope of this paper, a detailed description is omitted. Discriminative models Recent years have seen the emergence of discriminative training m</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. In Proceedings of The IEEE, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>Willian W Cohen</author>
</authors>
<title>SemiMarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1185--1192</pages>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and Willian W. Cohen. 2004. SemiMarkov conditional random fields for information extraction. In Proceedings of NIPS, pages 1185– 1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>134--141</pages>
<contexts>
<context position="1361" citStr="Sha and Pereira, 2003" startWordPosition="203" endWordPosition="206">timality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, CARPEDIEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(NL2) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitively s</context>
<context position="28018" citStr="Sha and Pereira, 2003" startWordPosition="4902" endWordPosition="4905">ng VITERBI 4000 77 1.1 CARPEDIEM 8600 51 0.26 SD 8800 850 121 SD+C-EXP. 14,000 1600 300 The data sets we used for the three experiments are the Penn TreeBank (PTB) corpus, CoNLL 2000 corpus, and an HPSG treebank built from the PTB corpus (Matsuzaki et al., 2007). We used sections 02-21 of PTB for training, and section 23 for testing. The number of labels in the three tasks is 45, 319 and 2602, respectively. We used the perceptron algorithm for training. The models were averaged over 10 iterations (Collins, 2002). For features, we basically followed previous studies (Tsuruoka and Tsujii, 2005; Sha and Pereira, 2003; Ninomiya et al., 2006). In POS tagging, we used unigrams of the current and its neighboring words, word bigrams, prefixes and suffixes of the current word, capitalization, and tag bigrams. In joint tagging, we also used the same features. In supertagging, we used POS unigrams and bigrams in addition to the same features other than capitalization. As the evaluation measure, we used the average decoding speed (sentences/sec) to two significant digits over five trials. To strictly measure the time spent for decoding, we ignored the preprocessing time, that is, the time for loading the model fil</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL, pages 134–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajid M Siddiqi</author>
<author>Andrew W Moore</author>
</authors>
<title>Fast inference and learning in large-state-space HMMs.</title>
<date>2005</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>800--807</pages>
<contexts>
<context position="8399" citStr="Siddiqi and Moore (2005)" startWordPosition="1360" endWordPosition="1364">o and Radicioni, 2009) is the algorithm closest to our work. It accelerates decoding by assuming that the adjacent labels are not strongly correlated. This assumption is appropriate for K k=1 N n=1 486 some NLP tasks. For example, as suggested in (Liang et al., 2008), adjacent labels do not provide strong information in POS tagging. However, the applicability of this idea to other NLP tasks is still unclear. Approximate algorithms, such as beam search or island-driven search, have been proposed for speeding up decoding. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong et al., 2009). In general, approximate algorithms have the advantage of speed over exact algorithms. However, both types of algorithms are still widely adopted by practitioners, since exact algorithms have merits other than speed. First, the optimality of the solution is always guaranteed. It is hard for most of the approximate algorithms to even bound the error rate. Second, approximate algorithms usually require hyperparameters, which control the tradeoff between </context>
</contexts>
<marker>Siddiqi, Moore, 2005</marker>
<rawString>Sajid M. Siddiqi and Andrew W. Moore. 2005. Fast inference and learning in large-state-space HMMs. In Proceedings ofICML, pages 800–807.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In Proceedings ofICML.</booktitle>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Tasker</author>
<author>Carlos Guestrin</author>
<author>Daphe Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="5122" citStr="Tasker et al., 2003" startWordPosition="805" endWordPosition="808">obability distribution over (x, y). If we assume a one-to-one correspondence between the hidden states and the labels, the score function can be written as: f(x, y) = log p(x, y) = log p(x|y) + log p(y) N N log p(xn|yn)+ log p(yn|yn−1). n=1 n=1 The parameters log p(xn|yn) and log p(yn|yn−1) are usually estimated using maximum likelihood or the EM algorithm. Since parameter estimation lies outside the scope of this paper, a detailed description is omitted. Discriminative models Recent years have seen the emergence of discriminative training methods for sequence labeling (Lafferty et al., 2001; Tasker et al., 2003; Collins, 2002; Tsochantaridis et al., 2005). Among them, we focus on the perceptron algorithm (Collins, 2002). Although we do not discuss the other discriminative models, our algorithm is equivalently applicable to them. The major difference between those models lies in parameter estimation; the decoding process is virtually the same. In the perceptron, the score function f(x, y) is given as f(x, y) = w · φ(x, y) where w is the weight vector, and φ(x, y) is the feature vector representation of the pair (x, y). By making the first-order Markov assumption, we have f(x, y) = w · φ(x, y) wkφk(x,</context>
</contexts>
<marker>Tasker, Guestrin, Koller, 2003</marker>
<rawString>Ben Tasker, Carlos Guestrin, and Daphe Koller. 2003. Max-margin Markov networks. In Proceedings of NIPS, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1453</pages>
<contexts>
<context position="5167" citStr="Tsochantaridis et al., 2005" startWordPosition="811" endWordPosition="814">If we assume a one-to-one correspondence between the hidden states and the labels, the score function can be written as: f(x, y) = log p(x, y) = log p(x|y) + log p(y) N N log p(xn|yn)+ log p(yn|yn−1). n=1 n=1 The parameters log p(xn|yn) and log p(yn|yn−1) are usually estimated using maximum likelihood or the EM algorithm. Since parameter estimation lies outside the scope of this paper, a detailed description is omitted. Discriminative models Recent years have seen the emergence of discriminative training methods for sequence labeling (Lafferty et al., 2001; Tasker et al., 2003; Collins, 2002; Tsochantaridis et al., 2005). Among them, we focus on the perceptron algorithm (Collins, 2002). Although we do not discuss the other discriminative models, our algorithm is equivalently applicable to them. The major difference between those models lies in parameter estimation; the decoding process is virtually the same. In the perceptron, the score function f(x, y) is given as f(x, y) = w · φ(x, y) where w is the weight vector, and φ(x, y) is the feature vector representation of the pair (x, y). By making the first-order Markov assumption, we have f(x, y) = w · φ(x, y) wkφk(x, yn−1, yn), where K = |φ(x, y) |is the number</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>467--474</pages>
<contexts>
<context position="1388" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="207" endWordPosition="210">n. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, CARPEDIEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(NL2) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitively slow when dealing with a lar</context>
<context position="8327" citStr="Tsuruoka and Tsujii (2005)" startWordPosition="1352" endWordPosition="1355">he input sequence is highly repetitive. Amongst others, CARPEDIEM (Esposito and Radicioni, 2009) is the algorithm closest to our work. It accelerates decoding by assuming that the adjacent labels are not strongly correlated. This assumption is appropriate for K k=1 N n=1 486 some NLP tasks. For example, as suggested in (Liang et al., 2008), adjacent labels do not provide strong information in POS tagging. However, the applicability of this idea to other NLP tasks is still unclear. Approximate algorithms, such as beam search or island-driven search, have been proposed for speeding up decoding. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong et al., 2009). In general, approximate algorithms have the advantage of speed over exact algorithms. However, both types of algorithms are still widely adopted by practitioners, since exact algorithms have merits other than speed. First, the optimality of the solution is always guaranteed. It is hard for most of the approximate algorithms to even bound the error rate. Second, approximate algorit</context>
<context position="27995" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="4898" endWordPosition="4901">ng Joint tagging Supertagging VITERBI 4000 77 1.1 CARPEDIEM 8600 51 0.26 SD 8800 850 121 SD+C-EXP. 14,000 1600 300 The data sets we used for the three experiments are the Penn TreeBank (PTB) corpus, CoNLL 2000 corpus, and an HPSG treebank built from the PTB corpus (Matsuzaki et al., 2007). We used sections 02-21 of PTB for training, and section 23 for testing. The number of labels in the three tasks is 45, 319 and 2602, respectively. We used the perceptron algorithm for training. The models were averaged over 10 iterations (Collins, 2002). For features, we basically followed previous studies (Tsuruoka and Tsujii, 2005; Sha and Pereira, 2003; Ninomiya et al., 2006). In POS tagging, we used unigrams of the current and its neighboring words, word bigrams, prefixes and suffixes of the current word, capitalization, and tag bigrams. In joint tagging, we also used the same features. In supertagging, we used POS unigrams and bigrams in addition to the same features other than capitalization. As the evaluation measure, we used the average decoding speed (sentences/sec) to two significant digits over five trials. To strictly measure the time spent for decoding, we ignored the preprocessing time, that is, the time fo</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proceedings of HLT/EMNLP, pages 467–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymeptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactios on Information Theory,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1629" citStr="Viterbi, 1967" startWordPosition="249" endWordPosition="250">2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(NL2) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitively slow when dealing with a large number of labels, since its computational cost is quadratic in L (Dietterich et al., 2008). Unfortunately, several sequence-labeling problems in NLP involve a large number of labels. For example, there are more than 40 and 2000 labels in </context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Andrew J. Viterbi. 1967. Error bounds for convolutional codes and an asymeptotically optimum decoding algorithm. IEEE Transactios on Information Theory, 13(2):260–267.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>