<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000195">
<title confidence="0.9979855">
Detecting Highly Confident Word Translations from Comparable
Corpora without Any Prior Knowledge
</title>
<author confidence="0.996172">
Ivan Vuli´c and Marie-Francine Moens
</author>
<affiliation confidence="0.999136">
Department of Computer Science
</affiliation>
<address confidence="0.704861333333333">
KU Leuven
Celestijnenlaan 200A
Leuven, Belgium
</address>
<email confidence="0.996497">
{ivan.vulic,marie-francine.moens}@cs.kuleuven.be
</email>
<sectionHeader confidence="0.993825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854894736842">
In this paper, we extend the work on using
latent cross-language topic models for iden-
tifying word translations across compara-
ble corpora. We present a novel precision-
oriented algorithm that relies on per-topic
word distributions obtained by the bilin-
gual LDA (BiLDA) latent topic model.
The algorithm aims at harvesting only the
most probable word translations across lan-
guages in a greedy fashion, without any
prior knowledge about the language pair,
relying on a symmetrization process and
the one-to-one constraint. We report our re-
sults for Italian-English and Dutch-English
language pairs that outperform the current
state-of-the-art results by a significant mar-
gin. In addition, we show how to use the al-
gorithm for the construction of high-quality
initial seed lexicons of translations.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906528301887">
Bilingual lexicons serve as an invaluable resource
of knowledge in various natural language pro-
cessing tasks, such as dictionary-based cross-
language information retrieval (Carbonell et al.,
1997; Levow et al., 2005) and statistical machine
translation (SMT) (Och and Ney, 2003). In or-
der to construct high quality bilingual lexicons for
different domains, one usually needs to possess
parallel corpora or build such lexicons by hand.
Compiling such lexicons manually is often an ex-
pensive and time-consuming task, whereas the
methods for mining the lexicons from parallel cor-
pora are not applicable for language pairs and do-
mains where such corpora is unavailable or miss-
ing. Therefore the focus of researchers turned to
comparable corpora, which consist of documents
with partially overlapping content, usually avail-
able in abundance. Thus, it is much easier to build
a high-volume comparable corpus. A representa-
tive example of such a comparable text collection
is Wikipedia, where one may observe articles dis-
cussing the similar topic, but strongly varying in
style, length and vocabulary, while still sharing a
certain amount of main concepts (or topics).
Over the years, several approaches for min-
ing translations from non-parallel corpora have
emerged (Rapp, 1995; Fung and Yee, 1998; Rapp,
1999; Diab and Finch, 2000; D´ejean et al., 2002;
Chiao and Zweigenbaum, 2002; Gaussier et al.,
2004; Fung and Cheung, 2004; Morin et al., 2007;
Haghighi et al., 2008; Shezaf and Rappoport,
2010; Laroche and Langlais, 2010), all sharing
the same Firthian assumption, often called the
distributionial hypothesis (Harris, 1954), which
states that words with a similar meaning are likely
to appear in similar contexts across languages.
All these methods have examined different rep-
resentations of word contexts and different meth-
ods for matching words across languages, but they
all have in common a need for a seed lexicon of
translations to efficiently bridge the gap between
languages. That seed lexicon is usually crawled
from the Web or obtained from parallel corpora.
Recently, Li et al. (2011) have proposed an ap-
proach that improves precision of the existing
methods for bilingual lexicon extraction, based
on improving the comparability of the corpus un-
der consideration, prior to extracting actual bilin-
gual lexicons. Other methods such as (Koehn and
Knight, 2002) try to design a bootstrapping algo-
rithm based on an initial seed lexicon of transla-
tions and various lexical evidences. However, the
quality of their initial seed lexicon is disputable,
</bodyText>
<page confidence="0.98924">
449
</page>
<note confidence="0.9765525">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 449–459,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999955329113924">
since the construction of their lexicon is language-
pair biased and cannot be completely employed
on distant languages. It solely relies on unsatis-
factory language-pair independent cross-language
clues such as words shared across languages.
Recent work from Vuli´c et al.(2011) utilized
the distributional hypothesis in a different direc-
tion. It attempts to abrogate the need of a seed lex-
icon as a prerequisite for bilingual lexicon extrac-
tion. They train a cross-language topic model on
document-aligned comparable corpora and intro-
duce different methods for identifying word trans-
lations across languages, underpinned by per-
topic word distributions from the trained topic
model. Due to the fact that they deal with compa-
rable Wikipedia data, their translation model con-
tains a lot of noise, and some words are poorly
translated simply because there are not enough
occurrences in the corpus. The goal of this work is
to design an algorithm which will learn to harvest
only the most probable translations from the per-
word topic distributions. The translations learned
by the algorithm then might serve as a highly ac-
curate, precision-based initial seed lexicon, which
can then be used as a tool for translating source
word vectors into the target language. The key ad-
vantage of such a lexicon lies in the fact that there
is no language-pair dependent prior knowledge
involved in its construction (e.g., orthographic
features). Hence, it is completely applicable to
any language pair for which there exist sufficient
comparable data for training of the topic model.
Since comparable corpora often construct a
very noisy environment, it is of the utmost impor-
tance for a precision-oriented algorithm to learn
when to stop the process of matching words, and
which candidate pairs are surely not translations
of each other. The method described in this paper
follows this intuition: while extracting a bilingual
lexicon, we try to rematch words, keeping only
the most confident candidate pairs and disregard-
ing all the others. After that step, the most con-
fident candidate pairs might be used with some
of the existing context-based techniques to find
translations for the words discarded in the pre-
vious step. The algorithm is based on: (1) the
assumption of symmetry, and (2) the one-to-one
constraint. The idea of symmetrization has been
borrowed from the symmetrization heuristics in-
troduced for word alignments in SMT (Och and
Ney, 2003), where the intersection heuristics is
employed for a precision-oriented algorithm. In
our setting, it basically means that we keep a
translation pair (ws, w�) if and only if, after the
symmetrization process, the top translation candi-
date for the source word ws is the target word wT
and vice versa. The one-to-one constraint aims
at matching the most confident candidates during
the early stages of the algorithm, and then exclud-
ing them from further search. The utility of the
constraint for parallel corpora has already been
evaluated by Melamed (2000).
The remainder of the paper is structured as
follows. Section 2 gives a brief overview of
the methods, relying on per-topic word distribu-
tions, which serve as the tool for computing cross-
language similarity between words. In Section
3, we motivate the main assumptions of the al-
gorithm and describe the full algorithm. Sec-
tion 4 justifies the underlying assumptions of
the algorithm by providing comparisons with a
current-state-of-the-art system for Italian-English
and Dutch-English language pairs. It also con-
tains another set of experiments which inves-
tigates the potential of the algorithm in build-
ing a language-pair unbiased seed lexicon, and
compares the lexicon with other seed lexicons.
Finally, Section 5 lists conclusion and possible
paths of future work.
</bodyText>
<sectionHeader confidence="0.747217" genericHeader="method">
2 Calculating Initial Cross-Language
</sectionHeader>
<subsectionHeader confidence="0.67947">
Word Similarity
</subsectionHeader>
<bodyText confidence="0.99997635">
This section gives a quick overview of the Cue
method, the TI method, and their combination,
described by Vuli´c et al.(2011), which proved to
be the most efficient and accurate for identify-
ing potential word translations once the cross-
language BiLDA topic model is trained and the
associated per-topic distributions are obtained for
both source and target corpora. The BiLDA
model we use is a natural extension of the stan-
dard LDA model and, along with the definition of
per-topic word distributions, has been presented
in (Ni et al., 2009; De Smet and Moens, 2009;
Mimno et al., 2009). BiLDA takes advantage of
the document alignment by using a single variable
that contains the topic distribution 0. This vari-
able is language-independent, because it is shared
by each of the paired bilingual comparable doc-
uments. Topics for each document are sampled
from 0, from which the words are then sampled
in conjugation with the vocabulary distribution 0
</bodyText>
<page confidence="0.998081">
450
</page>
<figureCaption confidence="0.9104355">
ure 1: Pate modl for bilingal Latent Drichle Allocat
Figure 1: The bilingual LDA (BiLDA) model
</figureCaption>
<bodyText confidence="0.908288">
(for language S) and ψ (for language T).
</bodyText>
<subsectionHeader confidence="0.9833">
2.1 Cue Method
</subsectionHeader>
<bodyText confidence="0.999989333333333">
A straightforward approach to express similarity
between words tries to emphasize the associative
relation in a natural way - modeling the proba-
bility P(wT2 �wi ), i.e. the probability that a tar-
get word wT2 will be generated as a response to a
cue source word wi , where the link between the
</bodyText>
<equation confidence="0.872405">
1
words is established via the shared topic space:
P(wT2 �w�1 ) = ��k=1 P(wT 2 �zk)P(zk�w� 1 ), where
K denotes the number of cross-language topics.
</equation>
<subsectionHeader confidence="0.994489">
2.2 TI Method
</subsectionHeader>
<bodyText confidence="0.994941105263158">
This approach constructs word vectors over a
shared space of cross-language topics, where val-
ues within vectors are the TF-ITF scores (term
frequency - inverse topic frequency), computed
in a completely analogical manner as the TF-
IDF scores for the original word-document space
(Manning and Sch¨utze, 1999). Term frequency,
given a source word ws and a topic zk, measures
the importance of the word ws within the particu-
lar topic zk, while inverse topical frequency (ITF)
of the word ws measures the general importance
of the source word ws across all topics. The fi-
nal TF-ITF score for the source word ws and the
topic zk is given by TF −ITFZ�k = TFZ�k ·ITFZ.
The TF-ITF scores for target words associated
with target topics are calculated in an analogical
manner and the standard cosine similarity is then
used to find the most similar target word vectors
for a given source word vector.
</bodyText>
<subsectionHeader confidence="0.999471">
2.3 Combining the Methods
</subsectionHeader>
<bodyText confidence="0.9993977">
Topic models have the ability to build clusters of
words which might not always co-occur together
in the same textual units and therefore add ex-
tra information of potential relatedness. These
two methods for automatic bilingual lexicon ex-
traction interpret and exploit underlying per-topic
word distributions in different ways, so combin-
ing the two should lead to even better results. The
two methods are linearly combined, with the over-
all score given by:
</bodyText>
<equation confidence="0.997139">
SimTI+Cue(w�1 , wT2 ) = λSimTI(w1 ,wT2 )
+ (1 − λ)SimCue(wl , wT2 ) (1)
</equation>
<bodyText confidence="0.99992368">
Both methods posses several desirable proper-
ties. According to Griffiths et al. (2007), the con-
ditioning for the Cue method automatically com-
promises between word frequency and semantic
relatedness since higher frequency words tend to
have higher probability across all topics, but the
distribution over topics P(zkJwi ) ensures that se-
mantically related topics dominate the sum. The
similar phenomenon is captured by the TI method
by the usage of TF, which rewards high frequency
words, and ITF, which assigns a higher impor-
tance for words semantically more related to a
specific topic. These properties are incorporated
in the combination of the methods. As the final
result, the combined method provides, for each
source word, a ranked list of target words with as-
sociated scores that measure the strength of cross-
language similarity. The higher the score, the
more confident a translation pair is. We will use
this observation in the next section during the al-
gorithm construction.
The lexicon constructed by solely applying the
combination of these methods without any addi-
tional assumptions will serve as a baseline in the
results section.
</bodyText>
<sectionHeader confidence="0.852152" genericHeader="method">
3 Constructing the Algorithm
</sectionHeader>
<bodyText confidence="0.99997175">
This section explains the underlying assumptions
of the algorithm: the assumption of symmetry
and the one-to-one assumption. Finally, it pro-
vides the complete outline of the algorithm.
</bodyText>
<subsectionHeader confidence="0.99997">
3.1 Assumption of Symmetry
</subsectionHeader>
<bodyText confidence="0.999944166666667">
First, we start with the intuition that the assump-
tion of symmetry strengthens the confidence of a
translation pair. In other words, if the most prob-
able translation candidate for a source word wi is
a target word wT2 and, vice versa, the most prob-
able translation candidate of the target word wT2
</bodyText>
<figure confidence="0.998015083333333">
zS wS
ji ji
α
e
MS
zT wT
ji ji
MT
D
φ
β
ψ
</figure>
<page confidence="0.995877">
451
</page>
<bodyText confidence="0.999775142857143">
is the source word wS1 , and their TI+Cue scores
are above a certain threshold, we can claim that
the words wS1 and wT2 are a translation pair. The
definition of the symmetric relation can also be
relaxed. Instead of observing only one top can-
didate from the lists, we can observe top N can-
didates from both sides and include them in the
search space, and then re-rank the potential candi-
dates taking into account their associated TI+Cue
scores and their respective positions in the list.
We will call N the search space depth. Here is
the outline of the re-ranking method if the search
space consists of the top N candidates on both
sides:
</bodyText>
<listItem confidence="0.774597083333333">
1. Given is a source word wSs , for which we ac-
tually want to find the most probable trans-
lation candidate. Initialize an empty list
Finals = {} in which target language
candidates with their recalculated associated
scores will be stored.
2. Obtain TI+Cue scores for all target words.
Keep only N best scoring target candidates:
{wTs,1, ... , wTs,N} along with their respective
scores.
3. For each target candidate from
{wTs,1, ... , wTs,N} acquire TI+Cue scores
</listItem>
<bodyText confidence="0.97204">
over the entire source vocabulary. Keep only
N best scoring source language candidates.
Each word wTs,i E {wTs,1, ... , wTs,N} now
has a list of N source language candidates
associated with it: {wSi,1, wSi,2 ... , wSi,N}.
</bodyText>
<listItem confidence="0.864511357142857">
4. For each target candidate word wTs,i E
{wTs,1, ... , wTs,N}, do as follows:
(a) If one of the words from the associated
list is the given source word wSs , re-
member: (1) the position m, denoting
how high in the list the word wSs was
found, and (2) the associated TI+Cue
score SimTI+Cue(wTs,i, wS i,m = wSs ).
Calculate:
(i) G1,i = SimTI+Cue(wSs , wTs,i)/i
(ii) G2,i = SimTI+Cue(wTs,i, wSi,m)/m
Following that, calculate GMi, the ge-
ometric mean of the values G1,i and
G2,i1: GMi = V/G1,i · G2,i. Add a tu-
</listItem>
<bodyText confidence="0.996043">
1Scores G1,i and G2,i are structured in such a way to
balance between positions in the ranked lists and the TI+Cue
scores, since they reward candidate words which have high
TI+Cue scores associated with them, and penalize words if
they are found lower in the list of potential candidates.
ple (wTs,i, GMi) to the list Finals.
(b) If we have reached the end of the list
for the target candidate word wTs,i with-
out finding the given source word wSs ,
and i &lt; N, continue with the next word
wTs,i+1. Do not add any tuple to Finals
in this step.
5. If the list Finals is not empty, sort the tuples
in the list in descending order according to
their GMi scores. The first element of the
sorted list contains a word wTs,high, the final
translation candidate of the source word wSs .
If the list Finals is not empty, the final re-
sult of this process will be the cross-language
word translation pair (wSs , wTs,high).
We will call this symmetrization process the
symmetrizing re-ranking. It attempts at push-
ing the correct cross-language synonym to the top
of the candidates list, taking into account both
the strength of similarities defined through the
TI+Cue scores in both directions, and positions
in ranked lists. A blatant example depicting how
this process helps boost precision is presented in
Figure 2. We can also design a thresholded variant
of this procedure by imposing an extra constraint.
When calculating target language candidates for
the source word wSs in Step 2, we proceed fur-
ther only if the first target candidate scores above
a certain threshold P and, additionally, in Step 3,
we keep lists of N source language candidates
for only those target words for which the first
source language candidate in their respective list
scored above the same threshold P. We will call
this procedure the thresholded symmetrizing re-
ranking, and this version will be employed in the
final algorithm.
</bodyText>
<subsectionHeader confidence="0.986797">
3.2 One-to-one Assumption
</subsectionHeader>
<bodyText confidence="0.999387666666667">
Melamed (2000) has already established that most
source words in parallel corpora tend to translate
to only one target word. That tendency is modeled
by the one-to-one assumption, which constrains
each source word to have at most one translation
on the target side. Melamed’s paper reports that
this bias leads to a significant positive impact on
precision and recall of bilingual lexicon extraction
from parallel corpora. This assumption should
also be reasonable for many types of comparable
corpora such as Wikipedia or news corpora, which
are topically aligned or cover similar themes. We
</bodyText>
<page confidence="0.995965">
452
</page>
<figureCaption confidence="0.999215666666667">
Figure 2: An example where the assumption of symmetry and the one-to-one assumption clearly help boost
precision. If we keep top N, = 3 candidates from both sides, the algorithm is able to detect that the correct
Dutch-English translation pair is (abdij, abbey). The TI+Cue method without any assumptions would result with
an indirect association (abdij, monastery). If only the one-to-one assumption was present, the algorithm would
greedily learn the correct direct association (monastery, klooster), remove those words from their respective
vocabularies and then again result with another indirect association (abdij, monk). By additionally employing
the assumption of symmetry with the re-ranking method from Subsection 3.1, the algorithm correctly learns
the translation pair (abdij, abbey). Correct translation pairs (klooster, monastery) and (monnik, monk) are also
obtained. Again here, the pair (monnik, monk) would not be obtained without the one-to-one assumption.
</figureCaption>
<figure confidence="0.99813616">
0.3049
0.1740
klooster
monnik
0.1338
benedictijn
0.2237
0.2266
klooster
abdij
0.1586
monk
0.1494
monnik
0.1131
abdij
0.1155
0.2549
abdij
abbey
0.1496
monnik
0.1288
klooster
monastery
</figure>
<bodyText confidence="0.99169590625">
will prove that the assumption leads to better pre-
cision scores even for bilingual lexicon extraction
from such comparable data. The intuition be-
hind introducing this constraint is fairly simple.
Without the assumption, the similarity scores be-
tween source and target words are calculated in-
dependently of each other. We will illustrate the
problem arising from the independence assump-
tion with an example.
Suppose we have an Italian word arcipelago,
and we would like to detect its correct English
translation (archipelago). However, after the
TI+Cue method is employed, and even after the
symmetrizing re-ranking process from the previ-
ous step is used, we still acquire a wrong transla-
tion candidate pair (arcipelago, island). Why is
that so? The word (arcipelago) (or its translation)
and the acquired translation (island) are semanti-
cally very close, and therefore have similar distri-
butions over cross-language topics, but island is a
much more frequent term. The TI+Cue method
concludes that two words are potential trans-
lations whenever their distributions over cross-
language topics are much more similar than ex-
pected by chance. Moreover, it gives a preference
to more frequent candidates, so it will eventually
end up learning an indirect association2 between
words arcipelago and island. The one-to-one as-
sumption should mitigate the problem of such in-
direct associations if we design our algorithm in
such a way that it learns the most confident direct
associations2 first:
</bodyText>
<footnote confidence="0.60268025">
2A direct association, as defined in (Melamed, 2000), is
an association between two words (in this setting found by
the TI+Cue method) where the two words are indeed mutual
translations. Otherwise, it is an indirect association.
</footnote>
<page confidence="0.99473">
453
</page>
<listItem confidence="0.968233">
1. Learn the correct direct association pair
(isola, island).
2. Remove the words isola and island from
their respective vocabularies.
3. Since island is not in the vocabulary, the
indirect association between arcipelago and
island is not present any more. The algo-
rithm learns the correct direct association
(arcipelago, archipelago).
</listItem>
<subsectionHeader confidence="0.992768">
3.3 The Algorithm
3.3.1 One-Vocabulary-Pass
</subsectionHeader>
<bodyText confidence="0.999653416666667">
First, we will provide a version of the algorithm
with a fixed threshold P which completes only
one pass through the source vocabulary. Let V S
denote a given source vocabulary, and let V T de-
note a given target vocabulary. We need to define
several parameters of the algorithm. Let N0 be
the initial maximum search space depth for the
thresholded symmetrizing re-ranking procedure.
In Figure 2, the current depth Nc is 3, while the
maximum depth might be set to a value higher
than 3. The algorithm with the fixed threshold P
proceeds as follows:
</bodyText>
<listItem confidence="0.99257265">
1. Initialize the maximum search space depth
NM = N0. Initialize an empty lexicon L.
2. For each source word wSs E V S do:
(a) Set the current search space depth Nc =
1.3
(b) Perform the thresholded symmetrizing
re-ranking procedure with the current
search space set to Nc and the threshold
P. If a translation pair (wSs , wTs,high) is
found, go to the Sub-step 2(d).
(c) If a translation pair is not found, and
Nc &lt; NM, increment the current
search space Nc = Nc + 1 and return to
the previous Sub-step 2(b). If a trans-
lation pair is not found and Nc = NM,
return to Step 2 and proceed with the
next word.
(d) For the found translation pair
(wSs , wTs,high), remove words wSs
and wTs,high from their respective
</listItem>
<bodyText confidence="0.678947666666667">
3The intuition here is simple – we are trying to detect
a direct association as high as possible in the list. In other
words, if the first translation candidate for the source word
isola is the target word island, and, vice versa, the first
translation candidate for the target word island is isola, we
do not need to expand our search depth, because these two
words are the most likely translations.
vocabularies: V S = V S − {wSs } and
VT = V T − {wTs,high} to satisfy the
one-to-one constraint. Add the pair
(wSs , wTs,high) to the lexicon L.
We will name this procedure the one-
vocabulary-pass and employ it later in an iter-
ative algorithm with a varying threshold and a
varying maximum search space depth.
</bodyText>
<subsectionHeader confidence="0.987176">
3.3.2 The Final Algorithm
</subsectionHeader>
<bodyText confidence="0.9989593">
Let us now define P0 as the initial threshold, let
Pf be the threshold at which we stop decreas-
ing the value for threshold and start expanding
our maximum search space depth for the thresh-
olded symmetrizing re-ranking, and let decp be a
value for which we decrease the current threshold
in each step. Finally, let Nf be the limit for the
maximum search space depth, and NM denote the
current maximum search space depth. The final
algorithm is given by:
</bodyText>
<listItem confidence="0.98579465">
1. Initialize the maximum search space depth
NM = N0 and the starting threshold P =
P0. Initialize an empty lexicon Lfinal.
2. Check the stopping criterion: If NM &gt; Nf,
go to Step 5, otherwise continue with Step 3.
3. Perform the one-vocabulary-pass with the
current values of P and NM. Whenever a
translation pair is found, it is added to the
lexicon Lfinal. Additionally, we can also
save the threshold and the depth at which that
pair was found.
4. Decrease P: P = P − decp, and check
if P &lt; Pf. If still not P &lt; Pf, go to
Step 3 and perform the one-vocabulary-pass
again. Otherwise, if P &lt; Pf and there are
still unmatched words in the source vocab-
ulary, reset P: P = P0, increment NM:
NM=NM+1 and go to Step 2.
5. Return Lfinal as the final output of the algo-
rithm.
</listItem>
<bodyText confidence="0.999864444444444">
The parameters of the algorithm model its be-
havior. Typically, we would like to set P0 to a high
value, and N0 to a low value, which makes our
constraints strict and narrows our search space,
and consequently, extracts less translation pairs
in the first steps of the algorithm, but the set
of those translation pairs should be highly accu-
rate. Once it is not possible to extract any more
pairs with such strict constraints, the algorithm re-
</bodyText>
<page confidence="0.998464">
454
</page>
<bodyText confidence="0.999955384615385">
laxes them by lowering the threshold and expand-
ing the search space by incrementing the max-
imum search space depth. The algorithm may
leave some of the source words unmatched, which
is also dependent on the parameters of the algo-
rithm, but, due to the one-to-one assumption, that
scenario also occurs whenever a target vocabulary
contains more words than a source vocabulary.
The number of operations of the algorithm also
depends on the parameters, but it mostly depends
on the sizes of the given vocabularies. The com-
plexity is O(|V S||VT 1), but the algorithm is com-
putationally feasible even for large vocabularies.
</bodyText>
<sectionHeader confidence="0.999757" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.977623">
4.1 Training Collections
</subsectionHeader>
<bodyText confidence="0.999965814814815">
The data used for training of the models is col-
lected from various sources and varies strongly in
theme, style, length and its comparableness. In
order to reduce data sparsity, we keep only lem-
matized non-proper noun forms.
For Italian-English language pair, we use
18,898 Wikipedia article pairs to train BiLDA,
covering different themes with different scopes
and subtopics being addressed. Document align-
ment is established via interlingual links from the
Wikipedia metadata. Our vocabularies consist of
7,160 Italian nouns and 9,116 English nouns.
For Dutch-English language pair, we use 7, 602
Wikipedia article pairs, and 6,206 Europarl doc-
ument pairs, and combine them for training.4 Our
final vocabularies consist of 15, 284 Dutch nouns
and 12,715 English nouns.
Unlike, for instance, Wikipedia articles, where
document alignment is established via interlin-
gual links, in some cases it is necessary to perform
document alignment as the initial step. Since our
work focuses on Wikipedia data, we will not get
into detail with algorithms for document align-
ment. An IR-based method for document align-
ment is given in (Utiyama and Isahara, 2003;
Munteanu and Marcu, 2005), and a feature-based
method can be found in (Vu et al., 2009).
</bodyText>
<subsectionHeader confidence="0.946167">
4.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9996705">
All our experiments rely on BiLDA training
with comparable data. Corpora and software for
</bodyText>
<footnote confidence="0.703659">
4In case of Europarl, we use only the evidence of docu-
ment alignment during the training and do not benefit from
the parallelness of the sentences in the corpus.
</footnote>
<bodyText confidence="0.9998775">
BiLDA training are obtained from Vuli´c et al.
(2011). We train the BiLDA model with 2000
topics using Gibbs sampling, since that number
of topics displays the best performance in their
paper. The linear interpolation parameter for the
combined TI+Cue method is set to A = 0.1.
The parameters of the algorithm, adjusted on a
set of 500 randomly sampled Italian words, are set
to the following values in all experiments, except
where noted different: Po = 0.20, Pf = 0.00,
decp=0.01, No=3, and Nf = 10.
The initial ground truth for our source vocab-
ularies has been constructed by the freely avail-
able Google Translate tool. The final ground truth
for our test sets has been established after we
have manually revised the list of pairs obtained by
Google Translate, deleting incorrect entries and
adding additional correct entries. All translation
candidates are evaluated against this benchmark
lexicon.
</bodyText>
<subsectionHeader confidence="0.995339">
4.3 Experiment I: Do Our Assumptions Help
Lexicon Extraction?
</subsectionHeader>
<bodyText confidence="0.999975318181818">
With this set of experiments, we wanted to test
whether both the assumption of symmetry and
the one-to-one assumption are useful in improv-
ing precision of the initial TI+Cue lexicon extrac-
tion method. We compare three different lexicon
extraction algorithms: (1) the basic TI+Cue ex-
traction algorithm (LALG-BASIC) which serves
as the baseline algorithm5, (2) the algorithm from
Section 3, but without the one-to-one assump-
tion (LALG-SYM), meaning that if we find a
translation pair, we still keep words from the
translation pair in their respective vocabularies,
and (3) the complete algorithm from Section 3
(LALG-ALL). In order to evaluate these lexicon
extraction algorithms for both Italian-English and
Dutch-English, we have constructed a test set of
650 Italian nouns, and a test set of 1000 Dutch
nouns of high and medium frequency. Precision
scores for both language pairs and for all lexicon
extraction algorithms are provided in Table 1.
Based on these results, it is clearly visible that
both assumptions our algorithm makes are valid
</bodyText>
<footnote confidence="0.923904833333333">
5We have also tested whether LALG-BASIC outperforms
a method modeling direct co-occurrence, that uses cosine
to detect similarity between word vectors consisting of TF-
IDF scores in the shared document space (Cimiano et al.,
2009). Precision using that method is significantly lower,
e.g. 0.5538 vs. 0.6708 of LALG-BASIC for Italian-English.
</footnote>
<page confidence="0.991847">
455
</page>
<table confidence="0.98550375">
LEX Algorithm Italian-English Dutch-English
LALG-BASIC 0.6708 0.6560
LALG-SYM 0.6862 0.6780
LALG-ALL 0.7215 0.7170
</table>
<tableCaption confidence="0.9974">
Table 1: Precision scores on our test sets for the 3 dif-
ferent lexicon extraction algorithms.
</tableCaption>
<bodyText confidence="0.997670666666667">
and contribute to better overall scores. Therefore
in all further experiments we will use the LALG-
ALL extraction algorithm.
</bodyText>
<subsectionHeader confidence="0.997892">
4.4 Experiment II: How Does Thresholding
Affect Precision?
</subsectionHeader>
<bodyText confidence="0.999997777777778">
The next set of experiments aims at exploring how
precision scores change while we gradually de-
crease threshold values. The main goal of these
experiments is to detect when to stop with the ex-
traction of translation candidates in order to pre-
serve a lexicon of only highly accurate transla-
tions. We have fixed the maximum search space
depth N0 = Nf = 3. We used the same test sets
from Experiment I. Figure 3 displays the change
of precision in relation to different threshold val-
ues, where we start harvesting translations from
the threshold P0 = 0.2 down to Pf = 0.0. Since
our goal is to extract as many correct translation
pairs as possible, but without decreasing the pre-
cision scores, we have also examined what impact
this gradual decrease of threshold also has on the
number of extracted translations. We have opted
for the F,3 measure (van Rijsbergen, 1979):
</bodyText>
<equation confidence="0.887845">
2 Precision · Recall
F� = (1 + Q )Q2 · Precision + Recall (2)
</equation>
<bodyText confidence="0.99977575">
Since our task is precision-oriented, we have set
Q = 0.5. F0.5 measure values precision as twice
as important as recall. The F0.5 scores are also
provided in Figure 3.
</bodyText>
<subsectionHeader confidence="0.991661">
4.5 Experiment III: Building a Seed Lexicon
</subsectionHeader>
<bodyText confidence="0.999926">
Finally, we wanted to test how many accurate
translation pairs our best scoring LALG-ALL al-
gorithm is able to acquire from the entire source
vocabulary, with very high precision still remain-
ing paramount. The obtained highly-precise seed
lexicon then might be employed for an additional
bootstrapping procedure similar to (Koehn and
Knight, 2002; Fung and Cheung, 2004) or sim-
ply for translating context vectors as in (Gaussier
et al., 2004).
</bodyText>
<figure confidence="0.999018222222222">
1
0. 5
0.
0. 5
0.
0. 5
0.
0. 5
0.2 0.15 0.1 0.05 0
</figure>
<figureCaption confidence="0.801733571428571">
Figure 3: Precision and F0.5 scores in relation to
threshold values. We can observe that the algorithm
retrieves only highly accurate translations for both lan-
guage pairs while the threshold goes down from value
0.2 to 0.1, while precision starts to drop significantly
after the threshold of 0.1. F0.5 scores also reach their
peaks within that threshold region.
</figureCaption>
<bodyText confidence="0.999967481481482">
If we do not know anything about a given lan-
guage pair, we can only use words shared across
languages as lexical clues for the construction of
a seed lexicon. It often leads to a low precision
lexicon, since many false friends are detected.
For Italian-English, we have found 431 nouns
shared between the two languages, of which 350
were correct translations, leading to a precision
of 0.8121. As an illustration, if we take the
first 431 translation pairs retrieved by LALG-
ALL, there are 427 correct translation pairs, lead-
ing to a precision of 0.9907. Some pairs do
not share any orthographic similarities: (uccello,
bird), (tastiera, keyboard), (salute, health), (terre-
moto, earthquake) etc.
Following Koehn and Knight (2002), we have
also employed simple transformation rules for the
adoption of words from one language to another.
The rules specific to the Italian-English transla-
tion process that have been employed are: (R1) if
an Italian noun ends in −ione, but not in −zione,
strip the final e to obtain the corresponding En-
glish noun. Otherwise, strip the suffix −zione,
and append −tion; (R2) if a noun ends in −ia,
but not in −zia or −fia, replace the suffix −ia
with −y. If a noun ends in −zia, replace the suf-
fix with −cy and if a noun ends in −fia, replace
</bodyText>
<page confidence="0.997505">
456
</page>
<table confidence="0.988734375">
Italian-English Dutch-English
Lexicon # Correct Precision F0.5 # Correct Precision F0.5
LEX-1 350 0.8121 0.1876 898 0.8618 0.2308
LEX-2 766 0.8938 0.3473 1376 0.9011 0.3216
LEX-LALG 782 0.8958 0.3524 1106 0.9559 0.2778
LEX-1+LEX-LALG 1070 0.8785 0.4290 1860 0.9082 0.3961
LEX-R+LEX-LALG 1141 0.9239 0.4548 1507 0.9642 0.3500
LEX-2+LEX-LALG 1429 0.8926 0.5102 2261 0.9217 0.4505
</table>
<tableCaption confidence="0.996479">
Table 2: A comparison of different lexicons. For lexicons employing our LALG-ALL algorithm, only translation
candidates that scored above the threshold P = 0.11 have been kept.
</tableCaption>
<bodyText confidence="0.9996914">
it with −phy. Similar rules have been introduced
for Dutch-English: the suffix −tie is replaced by
−tion, −sie by −sion, and −teit by −ty.
Finally, we have compared the results of the
following constructed lexicons:
</bodyText>
<listItem confidence="0.99615475">
• A lexicon containing only words shared
across languages (LEX-1).
• A lexicon containing shared words and trans-
lation pairs found by applying the language-
specific transformation rules (LEX-2).
• A lexicon containing only translation pairs
obtained by the LALG-ALL algorithm that
score above a certain threshold P (LEX-
LALG).
• A combination of the lexicons LEX-1 and
LEX-LALG (LEX-1+LEX-LALG). Non-
matching duplicates are resolved by taking
the translation pair from LEX-LALG as the
correct one. Note that this lexicon is com-
pletely language-pair independent.
• A lexicon combining only translation pairs
found by applying the language-specific
transformation rules and LEX-LALG (LEX-
R+LEX-LALG).
• A combination of the lexicons LEX-2 and
</listItem>
<bodyText confidence="0.966550068965517">
LEX-LALG, where non-matching dupli-
cates are resolved by taking the translation
pair from LEX-LALG if it is present in
LEX-1, and from LEX-2 otherwise (LEX-
2+LEX-LALG).
According to the results from Table 2, we can
conclude that adding translation pairs extracted
by our LALG-ALL algorithm has a major posi-
tive impact on both precision and coverage. Ob-
taining results for two different language pairs
proves that the approach is generic and appli-
cable to any other language pairs. The previ-
ous approach relying on work from Koehn and
Knight (2002) has been outperformed in terms of
precision and coverage. Additionally, we have
shown that adding simple translation rules for lan-
guages sharing same roots might lead to even bet-
ter scores (LEX-2+LEX-LALG). However, it is
not always possible to rely on such knowledge,
and the usefulness of the designed LALG-ALL
algorithm really comes to the fore when the algo-
rithm is applied on distant language pairs which
do not share many words and cognates, and word
translation rules cannot be easily established. In
such cases, without any prior knowledge about the
languages involved in a translation process, one is
left with the linguistically unbiased LEX-1+LEX-
LALG lexicon, which also displays a promising
performance.
</bodyText>
<sectionHeader confidence="0.998177" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999955666666667">
We have designed an algorithm that focuses on ac-
quiring and keeping only highly confident trans-
lation candidates from multilingual comparable
corpora. By employing the algorithm we have
improved precision scores of the methods rely-
ing on per-topic word distributions from a cross-
language topic model. We have shown that the al-
gorithm is able to produce a highly reliable bilin-
gual seed lexicon even when all other lexical clues
are absent, thus making our algorithm suitable
even for unrelated language pairs. In future work,
we plan to further improve the algorithm and use
it as a source of translational evidence for differ-
ent alignment tasks in the setting of non-parallel
corpora.
</bodyText>
<sectionHeader confidence="0.998294" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.991771">
The research has been carried out in the frame-
work of the TermWise Knowledge Platform (IOF-
KP/09/001) funded by the Industrial Research
Fund K.U. Leuven, Belgium.
</bodyText>
<page confidence="0.99766">
457
</page>
<sectionHeader confidence="0.990266" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883696428571">
Jaime G. Carbonell, Jaime G. Yang, Robert E. Fred-
erking, Ralf D. Brown, Yibing Geng, Danny Lee,
Yiming Frederking, Robert E, Ralf D. Geng, and
Yiming Yang. 1997. Translingual information re-
trieval: A comparative evaluation. In Proceedings
of the 15th International Joint Conference on Arti-
ficial Intelligence, pages 708–714.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings
of the 19th International Conference on Computa-
tional Linguistics, pages 1–5.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus
latent concept models for cross-language informa-
tion retrieval. In Proceedings of the 21st Inter-
national Joint Conference on Artifical Intelligence,
pages 1513–1518.
Wim De Smet and Marie-Francine Moens. 2009.
Cross-language linking of news stories on the Web
using interlingual topic modeling. In Proceedings
of the CIKM 2009 Workshop on Social Web Search
and Mining, pages 57–64.
Herv´e D´ejean, ´Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th International Conference
on Computational Linguistics, pages 1–7.
Mona T. Diab and Steve Finch. 2000. A statis-
tical translation model using comparable corpora.
In Proceedings of the 6th Triennial Conference on
Recherche d’Information Assist´ee par Ordinateur
(RIAO), pages 1500–1508.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and EM. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, pages 57–63.
Pascale Fung and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics,
pages 414–420.
Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herv´e D´ejean. 2004. A geomet-
ric view on bilingual lexicon extraction from com-
parable corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 526–533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic represen-
tation. Psychological Review, 114(2):211–244.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, pages 771–779.
Zellig S. Harris. 1954. Distributional structure. Word
10, (23):146–162.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9–16.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 617–625.
Gina-Anne Levow, Douglas W. Oard, and Philip
Resnik. 2005. Dictionary-based techniques for
cross-language information retrieval. Information
Processing and Management, 41:523–547.
Bo Li, Eric Gaussier, and Akiko Aizawa. 2011. Clus-
tering comparable corpora for bilingual lexicon ex-
traction. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 473–478.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press, Cambridge, MA, USA.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26:221–249.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 880–889.
Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable cor-
pora. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics,
pages 664–671.
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Computational Lin-
guistics, 31:477–504.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng
Chen. 2009. Mining multilingual topics from
Wikipedia. In Proceedings of the 18th International
World Wide Web Conference, pages 1155–1156.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, pages 320–322.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
</reference>
<page confidence="0.983106">
458
</page>
<reference confidence="0.9991332">
Meeting of the Association for Computational Lin-
guistics, pages 519–526.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual
lexicon generation using non-aligned signatures. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 98–
107.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 72–79.
C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworth.
Thuy Vu, Ai Ti Aw, and Min Zhang. 2009. Feature-
based method for document alignment in compara-
ble news corpora. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 843–851.
Ivan Vuli´c, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from compara-
ble corpora using latent topic models. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 479–484.
</reference>
<page confidence="0.999125">
459
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.540223">
<title confidence="0.9953785">Detecting Highly Confident Word Translations from Corpora without Any Prior Knowledge</title>
<author confidence="0.994752">Ivan Vuli´c</author>
<author confidence="0.994752">Marie-Francine</author>
<affiliation confidence="0.961804">Department of Computer KU Celestijnenlaan</affiliation>
<address confidence="0.605731">Leuven,</address>
<abstract confidence="0.9994163">In this paper, we extend the work on using latent cross-language topic models for identifying word translations across comparable corpora. We present a novel precisionoriented algorithm that relies on per-topic distributions obtained by the bilin- LDA latent topic model. The algorithm aims at harvesting only the most probable word translations across languages in a greedy fashion, without any prior knowledge about the language pair, relying on a symmetrization process and the one-to-one constraint. We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a significant margin. In addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>Jaime G Yang</author>
<author>Robert E Frederking</author>
<author>Ralf D Brown</author>
<author>Yibing Geng</author>
<author>Danny Lee</author>
<author>Yiming Frederking</author>
<author>E Robert</author>
<author>Ralf D Geng</author>
<author>Yiming Yang</author>
</authors>
<title>Translingual information retrieval: A comparative evaluation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 15th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>708--714</pages>
<contexts>
<context position="1274" citStr="Carbonell et al., 1997" startWordPosition="176" endWordPosition="179">a greedy fashion, without any prior knowledge about the language pair, relying on a symmetrization process and the one-to-one constraint. We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a significant margin. In addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations. 1 Introduction Bilingual lexicons serve as an invaluable resource of knowledge in various natural language processing tasks, such as dictionary-based crosslanguage information retrieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (SMT) (Och and Ney, 2003). In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping</context>
</contexts>
<marker>Carbonell, Yang, Frederking, Brown, Geng, Lee, Frederking, Robert, Geng, Yang, 1997</marker>
<rawString>Jaime G. Carbonell, Jaime G. Yang, Robert E. Frederking, Ralf D. Brown, Yibing Geng, Danny Lee, Yiming Frederking, Robert E, Ralf D. Geng, and Yiming Yang. 1997. Translingual information retrieval: A comparative evaluation. In Proceedings of the 15th International Joint Conference on Artificial Intelligence, pages 708–714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Looking for candidate translational equivalents in specialized, comparable corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="2454" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="362" endWordPosition="365">consist of documents with partially overlapping content, usually available in abundance. Thus, it is much easier to build a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between la</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for candidate translational equivalents in specialized, comparable corpora. In Proceedings of the 19th International Conference on Computational Linguistics, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Antje Schultz</author>
<author>Sergej Sizov</author>
<author>Philipp Sorg</author>
<author>Steffen Staab</author>
</authors>
<title>Explicit versus latent concept models for cross-language information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artifical Intelligence,</booktitle>
<pages>1513--1518</pages>
<contexts>
<context position="28103" citStr="Cimiano et al., 2009" startWordPosition="4611" endWordPosition="4614">n extraction algorithms for both Italian-English and Dutch-English, we have constructed a test set of 650 Italian nouns, and a test set of 1000 Dutch nouns of high and medium frequency. Precision scores for both language pairs and for all lexicon extraction algorithms are provided in Table 1. Based on these results, it is clearly visible that both assumptions our algorithm makes are valid 5We have also tested whether LALG-BASIC outperforms a method modeling direct co-occurrence, that uses cosine to detect similarity between word vectors consisting of TFIDF scores in the shared document space (Cimiano et al., 2009). Precision using that method is significantly lower, e.g. 0.5538 vs. 0.6708 of LALG-BASIC for Italian-English. 455 LEX Algorithm Italian-English Dutch-English LALG-BASIC 0.6708 0.6560 LALG-SYM 0.6862 0.6780 LALG-ALL 0.7215 0.7170 Table 1: Precision scores on our test sets for the 3 different lexicon extraction algorithms. and contribute to better overall scores. Therefore in all further experiments we will use the LALGALL extraction algorithm. 4.4 Experiment II: How Does Thresholding Affect Precision? The next set of experiments aims at exploring how precision scores change while we gradually</context>
</contexts>
<marker>Cimiano, Schultz, Sizov, Sorg, Staab, 2009</marker>
<rawString>Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp Sorg, and Steffen Staab. 2009. Explicit versus latent concept models for cross-language information retrieval. In Proceedings of the 21st International Joint Conference on Artifical Intelligence, pages 1513–1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wim De Smet</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Cross-language linking of news stories on the Web using interlingual topic modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the CIKM 2009 Workshop on Social Web Search and Mining,</booktitle>
<pages>57--64</pages>
<marker>De Smet, Moens, 2009</marker>
<rawString>Wim De Smet and Marie-Francine Moens. 2009. Cross-language linking of news stories on the Web using interlingual topic modeling. In Proceedings of the CIKM 2009 Workshop on Social Web Search and Mining, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herv´e D´ejean</author>
<author>´Eric Gaussier</author>
<author>Fatia Sadat</author>
</authors>
<title>An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1--7</pages>
<marker>D´ejean, Gaussier, Sadat, 2002</marker>
<rawString>Herv´e D´ejean, ´Eric Gaussier, and Fatia Sadat. 2002. An approach based on multilingual thesauri and model combination for bilingual lexicon extraction. In Proceedings of the 19th International Conference on Computational Linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona T Diab</author>
<author>Steve Finch</author>
</authors>
<title>A statistical translation model using comparable corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Triennial Conference on Recherche d’Information Assist´ee par Ordinateur (RIAO),</booktitle>
<pages>1500--1508</pages>
<contexts>
<context position="2403" citStr="Diab and Finch, 2000" startWordPosition="354" endWordPosition="357">archers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance. Thus, it is much easier to build a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of tr</context>
</contexts>
<marker>Diab, Finch, 2000</marker>
<rawString>Mona T. Diab and Steve Finch. 2000. A statistical translation model using comparable corpora. In Proceedings of the 6th Triennial Conference on Recherche d’Information Assist´ee par Ordinateur (RIAO), pages 1500–1508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Percy Cheung</author>
</authors>
<title>Mining verynon-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>57--63</pages>
<contexts>
<context position="2500" citStr="Fung and Cheung, 2004" startWordPosition="370" endWordPosition="373">ent, usually available in abundance. Thus, it is much easier to build a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled </context>
<context position="30123" citStr="Fung and Cheung, 2004" startWordPosition="4944" endWordPosition="4947">� = (1 + Q )Q2 · Precision + Recall (2) Since our task is precision-oriented, we have set Q = 0.5. F0.5 measure values precision as twice as important as recall. The F0.5 scores are also provided in Figure 3. 4.5 Experiment III: Building a Seed Lexicon Finally, we wanted to test how many accurate translation pairs our best scoring LALG-ALL algorithm is able to acquire from the entire source vocabulary, with very high precision still remaining paramount. The obtained highly-precise seed lexicon then might be employed for an additional bootstrapping procedure similar to (Koehn and Knight, 2002; Fung and Cheung, 2004) or simply for translating context vectors as in (Gaussier et al., 2004). 1 0. 5 0. 0. 5 0. 0. 5 0. 0. 5 0.2 0.15 0.1 0.05 0 Figure 3: Precision and F0.5 scores in relation to threshold values. We can observe that the algorithm retrieves only highly accurate translations for both language pairs while the threshold goes down from value 0.2 to 0.1, while precision starts to drop significantly after the threshold of 0.1. F0.5 scores also reach their peaks within that threshold region. If we do not know anything about a given language pair, we can only use words shared across languages as lexical </context>
</contexts>
<marker>Fung, Cheung, 2004</marker>
<rawString>Pascale Fung and Percy Cheung. 2004. Mining verynon-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 57–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>414--420</pages>
<contexts>
<context position="2369" citStr="Fung and Yee, 1998" startWordPosition="348" endWordPosition="351">ing. Therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance. Thus, it is much easier to build a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in comm</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proceedings of the 17th International Conference on Computational Linguistics, pages 414–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Gaussier</author>
<author>Jean-Michel Renders</author>
<author>Irina Matveeva</author>
<author>Cyril Goutte</author>
<author>Herv´e D´ejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>526--533</pages>
<marker>Gaussier, Renders, Matveeva, Goutte, D´ejean, 2004</marker>
<rawString>Eric Gaussier, Jean-Michel Renders, Irina Matveeva, Cyril Goutte, and Herv´e D´ejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 526–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="10757" citStr="Griffiths et al. (2007)" startWordPosition="1708" endWordPosition="1711">Methods Topic models have the ability to build clusters of words which might not always co-occur together in the same textual units and therefore add extra information of potential relatedness. These two methods for automatic bilingual lexicon extraction interpret and exploit underlying per-topic word distributions in different ways, so combining the two should lead to even better results. The two methods are linearly combined, with the overall score given by: SimTI+Cue(w�1 , wT2 ) = λSimTI(w1 ,wT2 ) + (1 − λ)SimCue(wl , wT2 ) (1) Both methods posses several desirable properties. According to Griffiths et al. (2007), the conditioning for the Cue method automatically compromises between word frequency and semantic relatedness since higher frequency words tend to have higher probability across all topics, but the distribution over topics P(zkJwi ) ensures that semantically related topics dominate the sum. The similar phenomenon is captured by the TI method by the usage of TF, which rewards high frequency words, and ITF, which assigns a higher importance for words semantically more related to a specific topic. These properties are incorporated in the combination of the methods. As the final result, the comb</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="2543" citStr="Haghighi et al., 2008" startWordPosition="378" endWordPosition="381">it is much easier to build a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corp</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<journal>Word</journal>
<volume>10</volume>
<pages>23--146</pages>
<contexts>
<context position="2702" citStr="Harris, 1954" startWordPosition="401" endWordPosition="402">discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (2011) have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, based on improving </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. Word 10, (23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="3448" citStr="Koehn and Knight, 2002" startWordPosition="518" endWordPosition="521">ds have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (2011) have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, based on improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons. Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. However, the quality of their initial seed lexicon is disputable, 449 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 449–459, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics since the construction of their lexicon is languagepair biased and cannot be completely employed on distant languages. It solely relies on unsatisfactory language-pair independent cross-language clue</context>
<context position="30099" citStr="Koehn and Knight, 2002" startWordPosition="4940" endWordPosition="4943">: 2 Precision · Recall F� = (1 + Q )Q2 · Precision + Recall (2) Since our task is precision-oriented, we have set Q = 0.5. F0.5 measure values precision as twice as important as recall. The F0.5 scores are also provided in Figure 3. 4.5 Experiment III: Building a Seed Lexicon Finally, we wanted to test how many accurate translation pairs our best scoring LALG-ALL algorithm is able to acquire from the entire source vocabulary, with very high precision still remaining paramount. The obtained highly-precise seed lexicon then might be employed for an additional bootstrapping procedure similar to (Koehn and Knight, 2002; Fung and Cheung, 2004) or simply for translating context vectors as in (Gaussier et al., 2004). 1 0. 5 0. 0. 5 0. 0. 5 0. 0. 5 0.2 0.15 0.1 0.05 0 Figure 3: Precision and F0.5 scores in relation to threshold values. We can observe that the algorithm retrieves only highly accurate translations for both language pairs while the threshold goes down from value 0.2 to 0.1, while precision starts to drop significantly after the threshold of 0.1. F0.5 scores also reach their peaks within that threshold region. If we do not know anything about a given language pair, we can only use words shared acro</context>
<context position="31338" citStr="Koehn and Knight (2002)" startWordPosition="5151" endWordPosition="5154">lexical clues for the construction of a seed lexicon. It often leads to a low precision lexicon, since many false friends are detected. For Italian-English, we have found 431 nouns shared between the two languages, of which 350 were correct translations, leading to a precision of 0.8121. As an illustration, if we take the first 431 translation pairs retrieved by LALGALL, there are 427 correct translation pairs, leading to a precision of 0.9907. Some pairs do not share any orthographic similarities: (uccello, bird), (tastiera, keyboard), (salute, health), (terremoto, earthquake) etc. Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another. The rules specific to the Italian-English translation process that have been employed are: (R1) if an Italian noun ends in −ione, but not in −zione, strip the final e to obtain the corresponding English noun. Otherwise, strip the suffix −zione, and append −tion; (R2) if a noun ends in −ia, but not in −zia or −fia, replace the suffix −ia with −y. If a noun ends in −zia, replace the suffix with −cy and if a noun ends in −fia, replace 456 Italian-English Dutch-English Lexicon # Correct Prec</context>
<context position="33939" citStr="Koehn and Knight (2002)" startWordPosition="5569" endWordPosition="5572">rules and LEX-LALG (LEXR+LEX-LALG). • A combination of the lexicons LEX-2 and LEX-LALG, where non-matching duplicates are resolved by taking the translation pair from LEX-LALG if it is present in LEX-1, and from LEX-2 otherwise (LEX2+LEX-LALG). According to the results from Table 2, we can conclude that adding translation pairs extracted by our LALG-ALL algorithm has a major positive impact on both precision and coverage. Obtaining results for two different language pairs proves that the approach is generic and applicable to any other language pairs. The previous approach relying on work from Koehn and Knight (2002) has been outperformed in terms of precision and coverage. Additionally, we have shown that adding simple translation rules for languages sharing same roots might lead to even better scores (LEX-2+LEX-LALG). However, it is not always possible to rely on such knowledge, and the usefulness of the designed LALG-ALL algorithm really comes to the fore when the algorithm is applied on distant language pairs which do not share many words and cognates, and word translation rules cannot be easily established. In such cases, without any prior knowledge about the languages involved in a translation proce</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Laroche</author>
<author>Philippe Langlais</author>
</authors>
<title>Revisiting context-based projection methods for termtranslation spotting in comparable corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>617--625</pages>
<contexts>
<context position="2600" citStr="Laroche and Langlais, 2010" startWordPosition="386" endWordPosition="389"> corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (2011) have proposed an approach</context>
</contexts>
<marker>Laroche, Langlais, 2010</marker>
<rawString>Audrey Laroche and Philippe Langlais. 2010. Revisiting context-based projection methods for termtranslation spotting in comparable corpora. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 617–625.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
<author>Douglas W Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Dictionary-based techniques for cross-language information retrieval.</title>
<date>2005</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>41--523</pages>
<contexts>
<context position="1295" citStr="Levow et al., 2005" startWordPosition="180" endWordPosition="183">t any prior knowledge about the language pair, relying on a symmetrization process and the one-to-one constraint. We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a significant margin. In addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations. 1 Introduction Bilingual lexicons serve as an invaluable resource of knowledge in various natural language processing tasks, such as dictionary-based crosslanguage information retrieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (SMT) (Och and Ney, 2003). In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually ava</context>
</contexts>
<marker>Levow, Oard, Resnik, 2005</marker>
<rawString>Gina-Anne Levow, Douglas W. Oard, and Philip Resnik. 2005. Dictionary-based techniques for cross-language information retrieval. Information Processing and Management, 41:523–547.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Li</author>
</authors>
<title>Eric Gaussier, and Akiko Aizawa.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>473--478</pages>
<marker>Li, 2011</marker>
<rawString>Bo Li, Eric Gaussier, and Akiko Aizawa. 2011. Clustering comparable corpora for bilingual lexicon extraction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 473–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--221</pages>
<contexts>
<context position="6843" citStr="Melamed (2000)" startWordPosition="1059" endWordPosition="1060"> introduced for word alignments in SMT (Och and Ney, 2003), where the intersection heuristics is employed for a precision-oriented algorithm. In our setting, it basically means that we keep a translation pair (ws, w�) if and only if, after the symmetrization process, the top translation candidate for the source word ws is the target word wT and vice versa. The one-to-one constraint aims at matching the most confident candidates during the early stages of the algorithm, and then excluding them from further search. The utility of the constraint for parallel corpora has already been evaluated by Melamed (2000). The remainder of the paper is structured as follows. Section 2 gives a brief overview of the methods, relying on per-topic word distributions, which serve as the tool for computing crosslanguage similarity between words. In Section 3, we motivate the main assumptions of the algorithm and describe the full algorithm. Section 4 justifies the underlying assumptions of the algorithm by providing comparisons with a current-state-of-the-art system for Italian-English and Dutch-English language pairs. It also contains another set of experiments which investigates the potential of the algorithm in b</context>
<context position="16175" citStr="Melamed (2000)" startWordPosition="2646" endWordPosition="2647">hresholded variant of this procedure by imposing an extra constraint. When calculating target language candidates for the source word wSs in Step 2, we proceed further only if the first target candidate scores above a certain threshold P and, additionally, in Step 3, we keep lists of N source language candidates for only those target words for which the first source language candidate in their respective list scored above the same threshold P. We will call this procedure the thresholded symmetrizing reranking, and this version will be employed in the final algorithm. 3.2 One-to-one Assumption Melamed (2000) has already established that most source words in parallel corpora tend to translate to only one target word. That tendency is modeled by the one-to-one assumption, which constrains each source word to have at most one translation on the target side. Melamed’s paper reports that this bias leads to a significant positive impact on precision and recall of bilingual lexicon extraction from parallel corpora. This assumption should also be reasonable for many types of comparable corpora such as Wikipedia or news corpora, which are topically aligned or cover similar themes. We 452 Figure 2: An exam</context>
<context position="19454" citStr="Melamed, 2000" startWordPosition="3143" endWordPosition="3144"> but island is a much more frequent term. The TI+Cue method concludes that two words are potential translations whenever their distributions over crosslanguage topics are much more similar than expected by chance. Moreover, it gives a preference to more frequent candidates, so it will eventually end up learning an indirect association2 between words arcipelago and island. The one-to-one assumption should mitigate the problem of such indirect associations if we design our algorithm in such a way that it learns the most confident direct associations2 first: 2A direct association, as defined in (Melamed, 2000), is an association between two words (in this setting found by the TI+Cue method) where the two words are indeed mutual translations. Otherwise, it is an indirect association. 453 1. Learn the correct direct association pair (isola, island). 2. Remove the words isola and island from their respective vocabularies. 3. Since island is not in the vocabulary, the indirect association between arcipelago and island is not present any more. The algorithm learns the correct direct association (arcipelago, archipelago). 3.3 The Algorithm 3.3.1 One-Vocabulary-Pass First, we will provide a version of the</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26:221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>880--889</pages>
<contexts>
<context position="8251" citStr="Mimno et al., 2009" startWordPosition="1281" endWordPosition="1284">l Cross-Language Word Similarity This section gives a quick overview of the Cue method, the TI method, and their combination, described by Vuli´c et al.(2011), which proved to be the most efficient and accurate for identifying potential word translations once the crosslanguage BiLDA topic model is trained and the associated per-topic distributions are obtained for both source and target corpora. The BiLDA model we use is a natural extension of the standard LDA model and, along with the definition of per-topic word distributions, has been presented in (Ni et al., 2009; De Smet and Moens, 2009; Mimno et al., 2009). BiLDA takes advantage of the document alignment by using a single variable that contains the topic distribution 0. This variable is language-independent, because it is shared by each of the paired bilingual comparable documents. Topics for each document are sampled from 0, from which the words are then sampled in conjugation with the vocabulary distribution 0 450 ure 1: Pate modl for bilingal Latent Drichle Allocat Figure 1: The bilingual LDA (BiLDA) model (for language S) and ψ (for language T). 2.1 Cue Method A straightforward approach to express similarity between words tries to emphasize</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880–889.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>B´eatrice Daille</author>
<author>Koichi Takeuchi</author>
<author>Kyo Kageura</author>
</authors>
<title>Bilingual terminology mining - using brain, not brawn comparable corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>664--671</pages>
<contexts>
<context position="2520" citStr="Morin et al., 2007" startWordPosition="374" endWordPosition="377">in abundance. Thus, it is much easier to build a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obta</context>
</contexts>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi, and Kyo Kageura. 2007. Bilingual terminology mining - using brain, not brawn comparable corpora. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 664–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--477</pages>
<contexts>
<context position="25524" citStr="Munteanu and Marcu, 2005" startWordPosition="4196" endWordPosition="4199">. For Dutch-English language pair, we use 7, 602 Wikipedia article pairs, and 6,206 Europarl document pairs, and combine them for training.4 Our final vocabularies consist of 15, 284 Dutch nouns and 12,715 English nouns. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Since our work focuses on Wikipedia data, we will not get into detail with algorithms for document alignment. An IR-based method for document alignment is given in (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), and a feature-based method can be found in (Vu et al., 2009). 4.2 Experimental Setup All our experiments rely on BiLDA training with comparable data. Corpora and software for 4In case of Europarl, we use only the evidence of document alignment during the training and do not benefit from the parallelness of the sentences in the corpus. BiLDA training are obtained from Vuli´c et al. (2011). We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper. The linear interpolation parameter for the combined TI+Cue method is</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31:477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Jian Hu</author>
<author>Zheng Chen</author>
</authors>
<title>Mining multilingual topics from Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th International World Wide Web Conference,</booktitle>
<pages>1155--1156</pages>
<contexts>
<context position="8205" citStr="Ni et al., 2009" startWordPosition="1272" endWordPosition="1275">paths of future work. 2 Calculating Initial Cross-Language Word Similarity This section gives a quick overview of the Cue method, the TI method, and their combination, described by Vuli´c et al.(2011), which proved to be the most efficient and accurate for identifying potential word translations once the crosslanguage BiLDA topic model is trained and the associated per-topic distributions are obtained for both source and target corpora. The BiLDA model we use is a natural extension of the standard LDA model and, along with the definition of per-topic word distributions, has been presented in (Ni et al., 2009; De Smet and Moens, 2009; Mimno et al., 2009). BiLDA takes advantage of the document alignment by using a single variable that contains the topic distribution 0. This variable is language-independent, because it is shared by each of the paired bilingual comparable documents. Topics for each document are sampled from 0, from which the words are then sampled in conjugation with the vocabulary distribution 0 450 ure 1: Pate modl for bilingal Latent Drichle Allocat Figure 1: The bilingual LDA (BiLDA) model (for language S) and ψ (for language T). 2.1 Cue Method A straightforward approach to expre</context>
</contexts>
<marker>Ni, Sun, Hu, Chen, 2009</marker>
<rawString>Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2009. Mining multilingual topics from Wikipedia. In Proceedings of the 18th International World Wide Web Conference, pages 1155–1156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1357" citStr="Och and Ney, 2003" startWordPosition="189" endWordPosition="192">metrization process and the one-to-one constraint. We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a significant margin. In addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations. 1 Introduction Bilingual lexicons serve as an invaluable resource of knowledge in various natural language processing tasks, such as dictionary-based crosslanguage information retrieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (SMT) (Och and Ney, 2003). In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance. Thus, it is much easier to build a high-v</context>
<context position="6287" citStr="Och and Ney, 2003" startWordPosition="967" endWordPosition="970">The method described in this paper follows this intuition: while extracting a bilingual lexicon, we try to rematch words, keeping only the most confident candidate pairs and disregarding all the others. After that step, the most confident candidate pairs might be used with some of the existing context-based techniques to find translations for the words discarded in the previous step. The algorithm is based on: (1) the assumption of symmetry, and (2) the one-to-one constraint. The idea of symmetrization has been borrowed from the symmetrization heuristics introduced for word alignments in SMT (Och and Ney, 2003), where the intersection heuristics is employed for a precision-oriented algorithm. In our setting, it basically means that we keep a translation pair (ws, w�) if and only if, after the symmetrization process, the top translation candidate for the source word ws is the target word wT and vice versa. The one-to-one constraint aims at matching the most confident candidates during the early stages of the algorithm, and then excluding them from further search. The utility of the constraint for parallel corpora has already been evaluated by Melamed (2000). The remainder of the paper is structured a</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>320--322</pages>
<contexts>
<context position="2349" citStr="Rapp, 1995" startWordPosition="346" endWordPosition="347">able or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance. Thus, it is much easier to build a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but t</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 320–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>519--526</pages>
<contexts>
<context position="2381" citStr="Rapp, 1999" startWordPosition="352" endWordPosition="353">ocus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance. Thus, it is much easier to build a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need fo</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 519–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphna Shezaf</author>
<author>Ari Rappoport</author>
</authors>
<title>Bilingual lexicon generation using non-aligned signatures.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>98--107</pages>
<contexts>
<context position="2571" citStr="Shezaf and Rappoport, 2010" startWordPosition="382" endWordPosition="385">ild a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics). Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (20</context>
</contexts>
<marker>Shezaf, Rappoport, 2010</marker>
<rawString>Daphna Shezaf and Ari Rappoport. 2010. Bilingual lexicon generation using non-aligned signatures. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 98– 107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Reliable measures for aligning Japanese-English news articles and sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>72--79</pages>
<contexts>
<context position="25497" citStr="Utiyama and Isahara, 2003" startWordPosition="4192" endWordPosition="4195">uns and 9,116 English nouns. For Dutch-English language pair, we use 7, 602 Wikipedia article pairs, and 6,206 Europarl document pairs, and combine them for training.4 Our final vocabularies consist of 15, 284 Dutch nouns and 12,715 English nouns. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Since our work focuses on Wikipedia data, we will not get into detail with algorithms for document alignment. An IR-based method for document alignment is given in (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), and a feature-based method can be found in (Vu et al., 2009). 4.2 Experimental Setup All our experiments rely on BiLDA training with comparable data. Corpora and software for 4In case of Europarl, we use only the evidence of document alignment during the training and do not benefit from the parallelness of the sentences in the corpus. BiLDA training are obtained from Vuli´c et al. (2011). We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper. The linear interpolation parameter for th</context>
</contexts>
<marker>Utiyama, Isahara, 2003</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2003. Reliable measures for aligning Japanese-English news articles and sentences. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 72–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<location>Butterworth.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Butterworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Vu</author>
<author>Ai Ti Aw</author>
<author>Min Zhang</author>
</authors>
<title>Featurebased method for document alignment in comparable news corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>843--851</pages>
<contexts>
<context position="25586" citStr="Vu et al., 2009" startWordPosition="4208" endWordPosition="4211">s, and 6,206 Europarl document pairs, and combine them for training.4 Our final vocabularies consist of 15, 284 Dutch nouns and 12,715 English nouns. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Since our work focuses on Wikipedia data, we will not get into detail with algorithms for document alignment. An IR-based method for document alignment is given in (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), and a feature-based method can be found in (Vu et al., 2009). 4.2 Experimental Setup All our experiments rely on BiLDA training with comparable data. Corpora and software for 4In case of Europarl, we use only the evidence of document alignment during the training and do not benefit from the parallelness of the sentences in the corpus. BiLDA training are obtained from Vuli´c et al. (2011). We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper. The linear interpolation parameter for the combined TI+Cue method is set to A = 0.1. The parameters of the algorithm, adjusted on </context>
</contexts>
<marker>Vu, Aw, Zhang, 2009</marker>
<rawString>Thuy Vu, Ai Ti Aw, and Min Zhang. 2009. Featurebased method for document alignment in comparable news corpora. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 843–851.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Wim De Smet</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Identifying word translations from comparable corpora using latent topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>479--484</pages>
<marker>Vuli´c, De Smet, Moens, 2011</marker>
<rawString>Ivan Vuli´c, Wim De Smet, and Marie-Francine Moens. 2011. Identifying word translations from comparable corpora using latent topic models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 479–484.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>