<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9976175">
A Composite Kernel to Extract Relations between Entities with
both Flat and Structured Features
</title>
<author confidence="0.993781">
Min Zhang Jie Zhang Jian Su Guodong Zhou
</author>
<affiliation confidence="0.979364">
Institute for Infocomm Research
</affiliation>
<address confidence="0.846042">
21 Heng Mui Keng Terrace, Singapore 119613
</address>
<email confidence="0.957254">
{mzhang, zhangjie, sujian, zhougd}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.99692" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922894736842">
This paper proposes a novel composite ker-
nel for relation extraction. The composite
kernel consists of two individual kernels: an
entity kernel that allows for entity-related
features and a convolution parse tree kernel
that models syntactic information of relation
examples. The motivation of our method is
to fully utilize the nice properties of kernel
methods to explore diverse knowledge for
relation extraction. Our study illustrates that
the composite kernel can effectively capture
both flat and structured features without the
need for extensive feature engineering, and
can also easily scale to include more fea-
tures. Evaluation on the ACE corpus shows
that our method outperforms the previous
best-reported methods and significantly out-
performs previous two dependency tree ker-
nels for relation extraction.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987463194029851">
The goal of relation extraction is to find various
predefined semantic relations between pairs of
entities in text. The research on relation extrac-
tion has been promoted by the Message Under-
standing Conferences (MUCs) (MUC, 1987-
1998) and Automatic Content Extraction (ACE)
program (ACE, 2002-2005). According to the
ACE Program, an entity is an object or set of ob-
jects in the world and a relation is an explicitly
or implicitly stated relationship among entities.
For example, the sentence “Bill Gates is chair-
man and chief software architect of Microsoft
Corporation.” conveys the ACE-style relation
“EMPLOYMENT.exec” between the entities
“Bill Gates” (PERSON.Name) and “Microsoft
Corporation” (ORGANIZATION. Commercial).
In this paper, we address the problem of rela-
tion extraction using kernel methods (Schölkopf
and Smola, 2001). Many feature-based learning
algorithms involve only the dot-product between
feature vectors. Kernel methods can be regarded
as a generalization of the feature-based methods
by replacing the dot-product with a kernel func-
tion between two vectors, or even between two
objects. A kernel function is a similarity function
satisfying the properties of being symmetric and
positive-definite. Recently, kernel methods are
attracting more interests in the NLP study due to
their ability of implicitly exploring huge amounts
of structured features using the original represen-
tation of objects. For example, the kernels for
structured natural language data, such as parse
tree kernel (Collins and Duffy, 2001), string ker-
nel (Lodhi et al., 2002) and graph kernel (Suzuki
et al., 2003) are example instances of the well-
known convolution kernels1 in NLP. In relation
extraction, typical work on kernel methods in-
cludes: Zelenko et al. (2003), Culotta and Soren-
sen (2004) and Bunescu and Mooney (2005).
This paper presents a novel composite kernel
to explore diverse knowledge for relation extrac-
tion. The composite kernel consists of an entity
kernel and a convolution parse tree kernel. Our
study demonstrates that the composite kernel is
very effective for relation extraction. It also
shows without the need for extensive feature en-
gineering the composite kernel can not only cap-
ture most of the flat features used in the previous
work but also exploit the useful syntactic struc-
ture features effectively. An advantage of our
method is that the composite kernel can easily
cover more knowledge by introducing more ker-
nels. Evaluation on the ACE corpus shows that
our method outperforms the previous best-
reported methods and significantly outperforms
the previous kernel methods due to its effective
exploration of various syntactic features.
The rest of the paper is organized as follows.
In Section 2, we review the previous work. Sec-
tion 3 discusses our composite kernel. Section 4
reports the experimental results and our observa-
tions. Section 5 compares our method with the
1 Convolution kernels were proposed for a discrete structure
by Haussler (1999) in the machine learning field. This
framework defines a kernel between input objects by apply-
ing convolution “sub-kernels” that are the kernels for the
decompositions (parts) of the objects.
</bodyText>
<page confidence="0.978731">
825
</page>
<note confidence="0.5332425">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999843">
previous work from the viewpoint of feature ex-
ploration. We conclude our work and indicate the
future work in Section 6.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999897155555556">
Many techniques on relation extraction, such as
rule-based (MUC, 1987-1998; Miller et al.,
2000), feature-based (Kambhatla 2004; Zhou et
al., 2005) and kernel-based (Zelenko et al., 2003;
Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005), have been proposed in the litera-
ture.
Rule-based methods for this task employ a
number of linguistic rules to capture various rela-
tion patterns. Miller et al. (2000) addressed the
task from the syntactic parsing viewpoint and
integrated various tasks such as POS tagging, NE
tagging, syntactic parsing, template extraction
and relation extraction using a generative model.
Feature-based methods (Kambhatla, 2004;
Zhou et al., 2005; Zhao and Grishman, 20052)
for this task employ a large amount of diverse
linguistic features, such as lexical, syntactic and
semantic features. These methods are very effec-
tive for relation extraction and show the best-
reported performance on the ACE corpus. How-
ever, the problems are that these diverse features
have to be manually calibrated and the hierarchi-
cal structured information in a parse tree is not
well preserved in their parse tree-related features,
which only represent simple flat path informa-
tion connecting two entities in the parse tree
through a path of non-terminals and a list of base
phrase chunks.
Prior kernel-based methods for this task focus
on using individual tree kernels to exploit tree
structure-related features. Zelenko et al. (2003)
developed a kernel over parse trees for relation
extraction. The kernel matches nodes from roots
to leaf nodes recursively layer by layer in a top-
down manner. Culotta and Sorensen (2004) gen-
eralized it to estimate similarity between depend-
ency trees. Their tree kernels require the match-
able nodes to be at the same layer counting from
the root and to have an identical path of ascend-
ing nodes from the roots to the current nodes.
The two constraints make their kernel high preci-
sion but very low recall on the ACE 2003 corpus.
Bunescu and Mooney (2005) proposed another
dependency tree kernel for relation extraction.
</bodyText>
<footnote confidence="0.926905">
2 We classify the feature-based kernel defined in (Zhao and
Grishman, 2005) into the feature-based methods since their
kernels can be easily represented by the dot-products be-
tween explicit feature vectors.
</footnote>
<bodyText confidence="0.999887586206897">
Their kernel simply counts the number of com-
mon word classes at each position in the shortest
paths between two entities in dependency trees.
The kernel requires the two paths to have the
same length; otherwise the kernel value is zero.
Therefore, although this kernel shows perform-
ance improvement over the previous one (Culotta
and Sorensen, 2004), the constraint makes the
two dependency kernels share the similar behav-
ior: good precision but much lower recall on the
ACE corpus.
The above discussion shows that, although
kernel methods can explore the huge amounts of
implicit (structured) features, until now the fea-
ture-based methods enjoy more success. One
may ask: how can we make full use of the nice
properties of kernel methods and define an effec-
tive kernel for relation extraction?
In this paper, we study how relation extraction
can benefit from the elegant properties of kernel
methods: 1) implicitly exploring (structured) fea-
tures in a high dimensional space; and 2) the nice
mathematical properties, for example, the sum,
product, normalization and polynomial expan-
sion of existing kernels is a valid kernel
(Schölkopf and Smola, 2001). We also demon-
strate how our composite kernel effectively cap-
tures the diverse knowledge for relation extrac-
tion.
</bodyText>
<sectionHeader confidence="0.99323" genericHeader="method">
3 Composite Kernel for Relation Ex-
traction
</sectionHeader>
<bodyText confidence="0.999777666666667">
In this section, we define the composite kernel
and study the effective representation of a rela-
tion instance.
</bodyText>
<subsectionHeader confidence="0.998729">
3.1 Composite Kernel
</subsectionHeader>
<bodyText confidence="0.987432076923077">
Our composite kernel consists of an entity kernel
and a convolution parse tree kernel. To our
knowledge, convolution kernels have not been
explored for relation extraction.
(1) Entity Kernel: The ACE 2003 data defines
four entity features: entity headword, entity type
and subtype (only for GPE), and mention type
while the ACE 2004 data makes some modifica-
tions and introduces a new feature “LDC men-
tion type”. Our statistics on the ACE data reveals
that the entity features impose a strong constraint
on relation types. Therefore, we design a linear
kernel to explicitly capture such features:
</bodyText>
<equation confidence="0.991243">
KL (R1, R2) = ∑ i=i,2 KE (R1 .Ei, R2 .Ei ) (1)
</equation>
<bodyText confidence="0.994324">
where R1 and R2 stands for two relation instances,
Ei means the ith entity of a relation instance, and
</bodyText>
<page confidence="0.991258">
826
</page>
<bodyText confidence="0.9864105">
KE(•,•) is a simple kernel function over the fea-
tures of entities:
</bodyText>
<equation confidence="0.999858">
K E E = ∑ C E f E f
E ( 1, 2) i ( 1 . i , 2 . i ) (2)
</equation>
<bodyText confidence="0.98951384">
where fi represents the ith entity feature, and the
function C(•,•) returns 1 if the two feature val-
ues are identical and 0 otherwise. KE(•,•) re-
turns the number of feature values in common of
two entities.
(2) Convolution Parse Tree Kernel: A convo-
lution kernel aims to capture structured informa-
tion in terms of substructures. Here we use the
same convolution parse tree kernel as described
in Collins and Duffy (2001) for syntactic parsing
and Moschitti (2004) for semantic role labeling.
Generally, we can represent a parse tree T by a
vector of integer counts of each sub-tree type
(regardless of its ancestors):
φ(T) = (# subtree1(T), ..., # subtreei(T), ..., #
subtreen(T) )
where # subtreei(T) is the occurrence number of
the ith sub-tree type (subtreei) in T. Since the
number of different sub-trees is exponential with
the parse tree size, it is computationally infeasi-
ble to directly use the feature vectorφ(T) . To
solve this computational issue, Collins and Duffy
(2001) proposed the following parse tree kernel
to calculate the dot product between the above
high dimensional vectors implicitly.
</bodyText>
<equation confidence="0.997888666666667">
K T T
( , ) ( ), ( )
=&lt; φ φ
T T
1 2 1 2
# subtreei(T1)⋅ #subtreei(T2) (3)
(∑ I i ) (
( )
n ⋅ ∑ I i )
( )
n
n N 1
1 ∈ subtree n N subtree 2
1 2 ∈ 2
= ∑ n1 ∈ N1 ∑ n2 ∈ N2 ∆ (n1,n2)
</equation>
<bodyText confidence="0.861642555555556">
where N1 and N2 are the sets of nodes in trees T1
and T2, respectively, and ( )
I subtree i n is a function
that is 1 iff the subtreei occurs with root at node n
and zero otherwise, and ∆(n1,n2) is the number of
the common subtrees rooted at n1 and n2, i.e.
∆(n1, n2) = ∑i Isubtreei (n1 ) ⋅ Isubtreei (n2 )
∆(n1 , n2) can be computed by the following recur-
sive rules:
</bodyText>
<listItem confidence="0.778614">
(1) if the productions (CFP rules) at n1 and n2
are different, ∆(n1, n2) = 0;
(2) else if both n1 and n2 are pre-terminals (POS
tags), ∆(n1, n2) = 1× λ ;
( 1 )
(3) else, ∆( , )
</listItem>
<equation confidence="0.9979682">
n n λ =
= ∏ +∆
nc n (1 ( ( , ), ( , )))
ch n j ch n j ,
1 2 j 1 1 2
</equation>
<bodyText confidence="0.989703625">
where nc(n1) is the child number of n1, ch(n,j) is
the jth child of node n andλ (0&lt;λ &lt;1) is the de-
cay factor in order to make the kernel value less
variable with respect to the subtree sizes. In ad-
dition, the recursive rule (3) holds because given
two nodes with the same children, one can con-
struct common sub-trees using these children and
common sub-trees of further offspring.
The parse tree kernel counts the number of
common sub-trees as the syntactic similarity
measure between two relation instances. The
time complexity for computing this kernel
is O( |N1  |⋅  |N2|) .
In this paper, two composite kernels are de-
fined by combing the above two individual ker-
nels in the following ways:
</bodyText>
<equation confidence="0.9723815">
1) Linear combination:
K (R ,R )=α•K ˆ L(R, R )+(1−α)•K ˆ (T ,T )(4)
1 1 2 1 2 1 2
Here, Kˆ(•,•) is the normalized3 K(•,•) and α
</equation>
<bodyText confidence="0.828634666666667">
is the coefficient. Evaluation on the development
set shows that this composite kernel yields the
best performance when α is set to 0.4.
</bodyText>
<listItem confidence="0.667898">
2) Polynomial expansion:
</listItem>
<equation confidence="0.987897">
K2(R„R2)=α•J L(R„R2)+(1−α)•K(T„T2)(5)
</equation>
<bodyText confidence="0.96853847826087">
Here, Kˆ(•,•) is the normalizedK(•,•), Kp(•,•)
is the polynomial expansion of K(•,•) with de-
gree d=2, i.e. Kp(•,•) = (K(•,•)+1)2 , and α is the
coefficient. Evaluation on the development set
shows that this composite kernel yields the best
performance when α is set to 0.23.
The polynomial expansion aims to explore the
entity bi-gram features, esp. the combined fea-
tures from the first and second entities, respec-
tively. In addition, due to the different scales of
the values of the two individual kernels, they are
normalized before combination. This can avoid
one kernel value being overwhelmed by that of
another one.
The entity kernel formulated by eqn. (1) is a
proper kernel since it simply calculates the dot
product of the entity feature vectors. The tree
kernel formulated by eqn. (3) is proven to be a
proper kernel (Collins and Duffy, 2001). Since
kernel function set is closed under normalization,
polynomial expansion and linear combination
(Schölkopf and Smola, 2001), the two composite
kernels are also proper kernels.
</bodyText>
<page confidence="0.849271">
3 A kernel K(x, y) can be normalized by dividing it by
</page>
<equation confidence="0.829257857142857">
K(x, x)• K(y, y) .
&gt;
=
=
∑
∑i
i
</equation>
<page confidence="0.975995">
827
</page>
<subsectionHeader confidence="0.993034">
3.2 Relation Instance Spaces
</subsectionHeader>
<bodyText confidence="0.9999625">
A relation instance is encapsulated by a parse
tree. Thus, it is critical to understand which por-
tion of a parse tree is important in the kernel cal-
culation. We study five cases as shown in Fig.1.
</bodyText>
<listItem confidence="0.99408875">
(1) Minimum Complete Tree (MCT): the com-
plete sub-tree rooted by the nearest common an-
cestor of the two entities under consideration.
(2) Path-enclosed Tree (PT): the smallest com-
</listItem>
<bodyText confidence="0.845155">
mon sub-tree including the two entities. In other
words, the sub-tree is enclosed by the shortest
path linking the two entities in the parse tree (this
path is also commonly-used as the path tree fea-
ture in the feature-based methods).
</bodyText>
<listItem confidence="0.989603888888889">
(3) Context-Sensitive Path Tree (CPT): the PT
extended with the 1st left word of entity 1 and the
1st right word of entity 2.
(4) Flattened Path-enclosed Tree (FPT): the
PT with the single in and out arcs of non-
terminal nodes (except POS nodes) removed.
(5) Flattened CPT (FCPT): the CPT with the
single in and out arcs of non-terminal nodes (ex-
cept POS nodes) removed.
</listItem>
<bodyText confidence="0.99892008">
Fig. 1 illustrates different representations of an
example relation instance. T1 is MCT for the
relation instance, where the sub-tree circled by a
dashed line is PT, which is also shown in T2 for
clarity. The only difference between MCT and
PT lies in that MCT does not allow partial pro-
duction rules (for example, NP4PP is a partial
production rule while NP4NP+PP is an entire
production rule in the top of T2). For instance,
only the most-right child in the most-left sub-tree
[NP [CD 200] [JJ domestic] [E1-PER ...]] of T1
is kept in T2. By comparing the performance of
T1 and T2, we can evaluate the effect of sub-trees
with partial production rules as shown in T2 and
the necessity of keeping the whole left and right
context sub-trees as shown in T1 in relation ex-
traction. T3 is CPT, where the two sub-trees cir-
cled by dashed lines are included as the context
to T2 and make T3 context-sensitive. This is to
evaluate whether the limited context information
in CPT can boost performance. FPT in T4 is
formed by removing the two circled nodes in T2.
This is to study whether and how the elimination
of single non-terminal nodes affects the perform-
ance of relation extraction.
</bodyText>
<figure confidence="0.97762775">
T1): MCT
T3):CPT
T2): PT
T4): FPT
</figure>
<figureCaption confidence="0.989276">
Figure 1. Different representations of a relation instance in the example sentence “...provide bene-
</figureCaption>
<bodyText confidence="0.986996333333333">
fits to 200 domestic partners of their own workers in New York”, where the phrase type
“E1-PER” denotes that the current node is the 1st entity with type “PERSON”, and like-
wise for the others. The relation instance is excerpted from the ACE 2003 corpus, where
a relation “SOCIAL.Other-Personal” exists between entities “partners” (PER) and
“workers” (PER). We use Charniak’s parser (Charniak, 2001) to parse the example sen-
tence. To save space, the FCPT is not shown here.
</bodyText>
<sectionHeader confidence="0.999884" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99769">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999851189189189">
Data: We use the English portion of both the
ACE 2003 and 2004 corpora from LDC in our
experiments. In the ACE 2003 data, the training
set consists of 674 documents and 9683 relation
instances while the test set consists of 97 docu-
ments and 1386 relation instances. The ACE
2003 data defines 5 entity types, 5 major relation
types and 24 relation subtypes. The ACE 2004
data contains 451 documents and 5702 relation
instances. It redefines 7 entity types, 7 major re-
lation types and 23 subtypes. Since Zhao and
Grishman (2005) use a 5-fold cross-validation on
a subset of the 2004 data (newswire and broad-
cast news domains, containing 348 documents
and 4400 relation instances), for comparison, we
use the same setting (5-fold cross-validation on
the same subset of the 2004 data, but the 5 parti-
tions may not be the same) for the ACE 2004
data. Both corpora are parsed using Charniak’s
parser (Charniak, 2001). We iterate over all pairs
of entity mentions occurring in the same sen-
tence to generate potential relation instances. In
this paper, we only measure the performance of
relation extraction models on “true” mentions
with “true” chaining of coreference (i.e. as anno-
tated by LDC annotators).
Implementation: We formalize relation extrac-
tion as a multi-class classification problem. SVM
is selected as our classifier. We adopt the one vs.
others strategy and select the one with the largest
margin as the final answer. The training parame-
ters are chosen using cross-validation (C=2.4
(SVM); λ =0.4(tree kernel)). In our implementa-
tion, we use the binary SVMLight (Joachims,
1998) and Tree Kernel Tools (Moschitti, 2004).
Precision (P), Recall (R) and F-measure (F) are
adopted to measure the performance.
</bodyText>
<subsectionHeader confidence="0.98367">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9994775">
In this subsection, we report the experiments of
different kernel setups for different purposes.
</bodyText>
<listItem confidence="0.851533">
(1) Tree Kernel only over Different Relation
Instance Spaces: In order to better study the im-
pact of the syntactic structure information in a
</listItem>
<bodyText confidence="0.970441714285714">
parse tree on relation extraction, we remove the
entity-related information from parse trees by
replacing the entity-related phrase types (“E1-
PER” and so on as shown in Fig. 1) with “NP”.
Table 1 compares the performance of 5 tree ker-
nel setups on the ACE 2003 data using the tree
structure information only. It shows that:
</bodyText>
<listItem confidence="0.800862785714286">
• Overall the five different relation instance
spaces are all somewhat effective for relation
extraction. This suggests that structured syntactic
information has good predication power for rela-
tion extraction and the structured syntactic in-
formation can be well captured by the tree kernel.
• MCT performs much worse than the others.
The reasons may be that MCT includes too
much left and right context information, which
may introduce many noisy features and cause
over-fitting (high precision and very low recall
as shown in Table 1). This suggests that only
keeping the complete (not partial) production
rules in MCT does harm performance.
• PT achieves the best performance. This means
that only keeping the portion of a parse tree en-
closed by the shortest path between entities can
model relations better than all others. This may
be due to that most significant information is
with PT and including context information may
introduce too much noise. Although context
may include some useful information, it is still a
problem to correctly utilize such useful informa-
tion in the tree kernel for relation extraction.
• CPT performs a bit worse than PT. In some
cases (e.g. in sentence “the merge of company A
and company B....”, “merge” is a critical con-
text word), the context information is helpful.
However, the effective scope of context is hard
to determine given the complexity and variabil-
ity of natural languages.
• The two flattened trees perform worse than the
original trees. This suggests that the single non-
terminal nodes are useful for relation extraction.
Evaluation on the ACE 2004 data also shows
that PT achieves the best performance (72.5/56.7
/63.6 in P/R/F). More evaluations with the entity
type and order information incorporated into tree
nodes (“E1-PER”, “E2-PER” and “E-GPE” as
shown in Fig. 1) also show that PT performs best
with 76.1/62.6/68.7 in P/R/F on the 2003 data
and 74.1/62.4/67.7 in P/R/F on the 2004 data.
</listItem>
<table confidence="0.998606625">
Instance Spaces P(%) R(%) F
Minimum Complete Tree 38.4 51.3
77.5 53.8 61.9
(MCT) 48.6 59.2
Path-enclosed Tree (PT) 72.8 51.7 60.4
Context-Sensitive PT(CPT) 75.9 47.2 58.2
Flattened PT 72.7
Flattened CPT 76.1
</table>
<tableCaption confidence="0.984954">
Table 1. five different tree kernel setups on the
</tableCaption>
<bodyText confidence="0.470378333333333">
ACE 2003 five major types using the parse
tree structure information only (regardless of
any entity-related information)
</bodyText>
<page confidence="0.984782">
829
</page>
<table confidence="0.999351">
PTs (with Tree Struc- P(%) R(%) F
ture Information only)
Entity kernel only 75.1 42.7 54.4
(79.5) (34.6) (48.2)
Tree kernel only 72.5 56.7 63.6
(72.8) (53.8) (61.9)
Composite kernel 1 73.5 67.0 70.1
(linear combination) (76.3) (63.0) (69.1)
Composite kernel 2 76.1 68.4 72.1
(polynomial expansion) (77.3) (65.6) (70.9)
</table>
<tableCaption confidence="0.997686">
Table 2. Performance comparison of different
</tableCaption>
<bodyText confidence="0.90364375">
kernel setups over the ACE major types of
both the 2003 data (the numbers in parenthe-
ses) and the 2004 data (the numbers outside
parentheses)
</bodyText>
<listItem confidence="0.864867260869565">
(2) Composite Kernels: Table 2 compares the
performance of different kernel setups on the
ACE major types. It clearly shows that:
• The composite kernels achieve significant per-
formance improvement over the two individual
kernels. This indicates that the flat and the struc-
tured features are complementary and the com-
posite kernels can well integrate them: 1) the
flat entity information captured by the entity
kernel; 2) the structured syntactic connection
information between the two entities captured
by the tree kernel.
• The composite kernel via the polynomial ex-
pansion outperforms the one via the linear com-
bination by ~2 in F-measure. It suggests that the
bi-gram entity features are very useful.
• The entity features are quite useful, which can
achieve F-measures of 54.4/48.2 alone and can
boost the performance largely by ~7 (70.1-
63.2/69.1-61.9) in F-measure when combining
with the tree kernel.
• It is interesting that the ACE 2004 data shows
consistent better performance on all setups than
</listItem>
<bodyText confidence="0.970470266666667">
the 2003 data although the ACE 2003 data is
two times larger than the ACE 2004 data. This
may be due to two reasons: 1) The ACE 2004
data defines two new entity types and re-defines
the relation types and subtypes in order to re-
duce the inconsistency between LDC annota-
tors. 2) More importantly, the ACE 2004 data
defines 43 entity subtypes while there are only 3
subtypes in the 2003 data. The detailed classifi-
cation in the 2004 data leads to significant per-
formance improvement of 6.2 (54.4-48.2) in F-
measure over that on the 2003 data.
Our composite kernel can achieve
77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over
the ACE 2003/2004 major types, respectively.
</bodyText>
<table confidence="0.999562357142857">
Methods (2002/2003 data) P(%) R(%) F
Ours: composite kernel 2 77.3 65.6 70.9
(polynomial expansion) (64.9) (51.2) (57.2)
Zhou et al. (2005): 77.2 60.7 68.0
feature-based SVM (63.1) (49.5) (55.5)
Kambhatla (2004): (-) (-) (-)
feature-based ME (63.5) (45.2) (52.8)
Ours: tree kernel with en- 76.1 62.6 68.7
tity information at node (62.4) (48.5) (54.6)
Bunescu and Mooney 65.5 43.8 52.5
(2005): shortest path de- (-) (-) (-)
pendency kernel
Culotta and Sorensen 67.1 35.0 45.8
(2004): dependency kernel (-) (-) (-)
</table>
<tableCaption confidence="0.8228515">
Table 3. Performance comparison on the ACE
2003/2003 data over both 5 major types (the
numbers outside parentheses) and 24 subtypes
(the numbers in parentheses)
</tableCaption>
<table confidence="0.999665333333333">
Methods (2004 data) P(%) R(%) F
Ours: composite kernel 2 76.1 68.4 72.1
(polynomial expansion) (68.6) (59.3) (63.6)
Zhao and Grishman (2005): 69.2 70.5 70.4
feature-based kernel
(-) (-) (-)
</table>
<tableCaption confidence="0.770183">
Table 4. Performance comparison on the ACE
2004 data over both 7 major types (the numbers
outside parentheses) and 23 subtypes (the num-
bers in parentheses)
</tableCaption>
<listItem confidence="0.676042">
(3) Performance Comparison: Tables 3 and 4
</listItem>
<bodyText confidence="0.981000714285714">
compare our method with previous work on the
ACE 2002/2003/2004 data, respectively. They
show that our method outperforms the previous
methods and significantly outperforms the previ-
ous two dependency kernels4. This may be due to
two reasons: 1) the dependency tree (Culotta and
Sorensen, 2004) and the shortest path (Bunescu
and Mooney, 2005) lack the internal hierarchical
phrase structure information, so their correspond-
ing kernels can only carry out node-matching
directly over the nodes with word tokens; 2) the
parse tree kernel has less constraints. That is, it is
4 Bunescu and Mooney (2005) used the ACE 2002 corpus,
including 422 documents, which is known to have many
inconsistencies than the 2003 version. Culotta and Sorensen
(2004) used a generic ACE corpus including about 800
documents (no corpus version is specified). Since the testing
corpora are in different sizes and versions, strictly speaking,
it is not ready to compare these methods exactly and fairly.
Therefore Table 3 is only for reference purpose. We just
hope that we can get a few clues from this table.
</bodyText>
<page confidence="0.990226">
830
</page>
<bodyText confidence="0.999734777777778">
not restricted by the two constraints of the two
dependency kernels (identical layer and ances-
tors for the matchable nodes and identical length
of two shortest paths, as discussed in Section 2).
The above experiments verify the effective-
ness of our composite kernels for relation extrac-
tion. They suggest that the parse tree kernel can
effectively explore the syntactic features which
are critical for relation extraction.
</bodyText>
<table confidence="0.924013">
Error Type # of error instances
2004 data 2003 data
False Negative 198 416
False Positive 115 171
Cross Type 62 96
</table>
<tableCaption confidence="0.920252">
Table 5. Error distribution of major types on
</tableCaption>
<bodyText confidence="0.994703714285714">
both the 2003 and 2004 data for the compos-
ite kernel by polynomial expansion
(4) Error Analysis: Table 5 reports the error
distribution of the polynomial composite kernel
over the major types on the ACE data. It shows
that 83.5%(198+115/198+115+62) / 85.8%(416
+171/416+171+96) of the errors result from rela-
tion detection and only 16.5%/14.2% of the er-
rors result from relation characterization. This
may be due to data imbalance and sparseness
issues since we find that the negative samples are
8 times more than the positive samples in the
training set. Nevertheless, it clearly directs our
future work.
</bodyText>
<sectionHeader confidence="0.999831" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999953">
In this section, we compare our method with the
previous work from the feature engineering
viewpoint and report some other observations
and issues in our experiments.
</bodyText>
<subsectionHeader confidence="0.998929">
5.1 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.988720225">
This is to explain more about why our method
performs better and significantly outperforms the
previous two dependency tree kernels from the
theoretical viewpoint.
(1) Compared with Feature-based Methods:
The basic difference lies in the relation instance
representation (parse tree vs. feature vector) and
the similarity calculation mechanism (kernel
function vs. dot-product). The main difference is
the different feature spaces. Regarding the parse
tree features, our method implicitly represents a
parse tree by a vector of integer counts of each
sub-tree type, i.e., we consider the entire sub-tree
types and their occurring frequencies. In this way,
the parse tree-related features (the path features
and the chunking features) used in the feature-
based methods are embedded (as a subset) in our
feature space. Moreover, the in-between word
features and the entity-related features used in
the feature-based methods are also captured by
the tree kernel and the entity kernel, respectively.
Therefore our method has the potential of effec-
tively capturing not only most of the previous
flat features but also the useful syntactic struc-
ture features.
(2) Compared with Previous Kernels: Since
our method only counts the occurrence of each
sub-tree without considering the layer and the
ancestors of the root node of the sub-tree, our
method is not limited by the constraints (identi-
cal layer and ancestors for the matchable nodes,
as discussed in Section 2) in Culotta and Soren-
sen (2004). Moreover, the difference between
our method and Bunescu and Mooney (2005) is
that their kernel is defined on the shortest path
between two entities instead of the entire sub-
trees. However, the path does not maintain the
tree structure information. In addition, their ker-
nel requires the two paths to have the same
length. Such constraint is too strict.
</bodyText>
<subsectionHeader confidence="0.957486">
5.2 Other Issues
</subsectionHeader>
<bodyText confidence="0.977502892857143">
(1) Speed Issue: The recursively-defined convo-
lution kernel is much slower compared to fea-
ture-based classifiers. In this paper, the speed
issue is solved in three ways. First, the inclusion
of the entity kernel makes the composite kernel
converge fast. Furthermore, we find that the
small portion (PT) of a full parse tree can effec-
tively represent a relation instance. This signifi-
cantly improves the speed. Finally, the parse tree
kernel requires exact match between two sub-
trees, which normally does not occur very fre-
quently. Collins and Duffy (2001) report that in
practice, running time for the parse tree kernel is
more close to linear (O(JN1J+JN2J), rather than
O(JN1J*JN2J ). As a result, using the PC with Intel
P4 3.0G CPU and 2G RAM, our system only
takes about 110 minutes and 30 minutes to do
training on the ACE 2003 (~77k training in-
stances) and 2004 (~33k training instances) data,
respectively.
(2) Further Improvement: One of the potential
problems in the parse tree kernel is that it carries
out exact matches between sub-trees, so that this
kernel fails to handle sparse phrases (i.e. “a car”
vs. “a red car”) and near-synonymic grammar
tags (for example, the variations of a verb (i.e.
go, went, gone)). To some degree, it could possi-
bly lead to over-fitting and compromise the per-
</bodyText>
<page confidence="0.993448">
831
</page>
<bodyText confidence="0.999850615384616">
formance. However, the above issues can be
handled by allowing grammar-driven partial rule
matching and other approximate matching
mechanisms in the parse tree kernel calculation.
Finally, it is worth noting that by introducing
more individual kernels our method can easily
scale to cover more features from a multitude of
sources (e.g. Wordnet, gazetteers, etc) that can
be brought to bear on the task of relation extrac-
tion. In addition, we can also easily implement
the feature weighting scheme by adjusting the
eqn.(2) and the rule (2) in calculating ∆(n1,n2)
(see subsection 3.1).
</bodyText>
<sectionHeader confidence="0.998771" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999967973684211">
Kernel functions have nice properties. In this
paper, we have designed a composite kernel for
relation extraction. Benefiting from the nice
properties of the kernel methods, the composite
kernel could well explore and combine the flat
entity features and the structured syntactic fea-
tures, and therefore outperforms previous best-
reported feature-based methods on the ACE cor-
pus. To our knowledge, this is the first research
to demonstrate that, without the need for exten-
sive feature engineering, an individual tree ker-
nel achieves comparable performance with the
feature-based methods. This shows that the syn-
tactic features embedded in a parse tree are par-
ticularly useful for relation extraction and which
can be well captured by the parse tree kernel. In
addition, we find that the relation instance repre-
sentation (selecting effective portions of parse
trees for kernel calculations) is very important
for relation extraction.
The most immediate extension of our work is
to improve the accuracy of relation detection.
This can be done by capturing more features by
including more individual kernels, such as the
WordNet-based semantic kernel (Basili et al.,
2005) and other feature-based kernels. We can
also benefit from machine learning algorithms to
study how to solve the data imbalance and
sparseness issues from the learning algorithm
viewpoint. In the future work, we will design a
more flexible tree kernel for more accurate simi-
larity measure.
Acknowledgements: We would like to thank
Dr. Alessandro Moschitti for his great help in
using his Tree Kernel Toolkits and fine-tuning
the system. We also would like to thank the three
anonymous reviewers for their invaluable sug-
gestions.
</bodyText>
<sectionHeader confidence="0.998408" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99744536">
ACE. 2002-2005. The Automatic Content Extraction
Projects. http://www.ldc.upenn.edu/Projects /ACE/
Basili R., Cammisa M. and Moschitti A. 2005. A Se-
mantic Kernel to classify text with very few train-
ing examples. ICML-2005
Bunescu R. C. and Mooney R. J. 2005. A Shortest
Path Dependency Kernel for Relation Extraction.
EMNLP-2005
Charniak E. 2001. Immediate-head Parsing for Lan-
guage Models. ACL-2001
Collins M. and Duffy N. 2001. Convolution Kernels
for Natural Language. NIPS-2001
Culotta A. and Sorensen J. 2004. Dependency Tree
Kernel for Relation Extraction. ACL-2004
Haussler D. 1999. Convolution Kernels on Discrete
Structures. Technical Report UCS-CRL-99-10,
University of California, Santa Cruz.
Joachims T. 1998. Text Categorization with Support
Vecor Machine: learning with many relevant fea-
tures. ECML-1998
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy models
for extracting relations. ACL-2004 (poster)
Lodhi H., Saunders C., Shawe-Taylor J., Cristianini
N. and Watkins C. 2002. Text classification using
string kernel. Journal of Machine Learning Re-
search, 2002(2):419-444
Miller S., Fox H., Ramshaw L. and Weischedel R.
2000. A novel use of statistical parsing to extract
information from text. NAACL-2000
Moschitti A. 2004. A Study on Convolution Kernels
for Shallow Semantic Parsing. ACL-2004
MUC. 1987-1998. http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/
Schölkopf B. and Smola A. J. 2001. Learning with
Kernels: SVM, Regularization, Optimization and
Beyond. MIT Press, Cambridge, MA 407-423
Suzuki J., Hirao T., Sasaki Y. and Maeda E. 2003.
Hierarchical Directed Acyclic Graph Kernel:
Methods for Structured Natural Language Data.
ACL-2003
Zelenko D., Aone C. and Richardella A. 2003. Kernel
Methods for Relation Extraction. Journal of Ma-
chine Learning Research. 2003(2):1083-1106
Zhao S.B. and Grishman R. 2005. Extracting Rela-
tions with Integrated Information Using Kernel
Methods. ACL-2005
Zhou G.D., Su J, Zhang J. and Zhang M. 2005. Ex-
ploring Various Knowledge in Relation Extraction.
ACL-2005
</reference>
<page confidence="0.997442">
832
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.873341">
<title confidence="0.9982985">A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features</title>
<author confidence="0.999897">Min Zhang Jie Zhang Jian Su Guodong Zhou</author>
<affiliation confidence="0.999892">Institute for Infocomm Research</affiliation>
<address confidence="0.97983">21 Heng Mui Keng Terrace, Singapore 119613</address>
<email confidence="0.942541">mzhang@i2r.a-star.edu.sg</email>
<email confidence="0.942541">zhangjie@i2r.a-star.edu.sg</email>
<email confidence="0.942541">sujian@i2r.a-star.edu.sg</email>
<email confidence="0.942541">zhougd@i2r.a-star.edu.sg</email>
<abstract confidence="0.99708575">This paper proposes a novel composite kernel for relation extraction. The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction. Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>The Automatic Content Extraction Projects.</title>
<note>http://www.ldc.upenn.edu/Projects /ACE/</note>
<marker></marker>
<rawString>ACE. 2002-2005. The Automatic Content Extraction Projects. http://www.ldc.upenn.edu/Projects /ACE/</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M Cammisa</author>
<author>A Moschitti</author>
</authors>
<title>A Semantic Kernel to classify text with very few training examples.</title>
<date>2005</date>
<pages>2005</pages>
<marker>Basili, Cammisa, Moschitti, 2005</marker>
<rawString>Basili R., Cammisa M. and Moschitti A. 2005. A Semantic Kernel to classify text with very few training examples. ICML-2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Bunescu</author>
<author>R J Mooney</author>
</authors>
<title>A Shortest Path Dependency Kernel for Relation Extraction.</title>
<date>2005</date>
<pages>2005</pages>
<contexts>
<context position="2931" citStr="Bunescu and Mooney (2005)" startWordPosition="434" endWordPosition="437">and positive-definite. Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects. For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP. In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). This paper presents a novel composite kernel to explore diverse knowledge for relation extraction. The composite kernel consists of an entity kernel and a convolution parse tree kernel. Our study demonstrates that the composite kernel is very effective for relation extraction. It also shows without the need for extensive feature engineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively. An advantage of our method is that the composite kernel can easily cover more knowledge by </context>
<context position="4861" citStr="Bunescu and Mooney, 2005" startWordPosition="734" endWordPosition="737">for the decompositions (parts) of the objects. 825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832, Sydney, July 2006. c�2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration. We conclude our work and indicate the future work in Section 6. 2 Related Work Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature. Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns. Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model. Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features. These methods are very</context>
<context position="6615" citStr="Bunescu and Mooney (2005)" startWordPosition="1019" endWordPosition="1022">rnels to exploit tree structure-related features. Zelenko et al. (2003) developed a kernel over parse trees for relation extraction. The kernel matches nodes from roots to leaf nodes recursively layer by layer in a topdown manner. Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees. Their tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending nodes from the roots to the current nodes. The two constraints make their kernel high precision but very low recall on the ACE 2003 corpus. Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction. 2 We classify the feature-based kernel defined in (Zhao and Grishman, 2005) into the feature-based methods since their kernels can be easily represented by the dot-products between explicit feature vectors. Their kernel simply counts the number of common word classes at each position in the shortest paths between two entities in dependency trees. The kernel requires the two paths to have the same length; otherwise the kernel value is zero. Therefore, although this kernel shows performance improvement over the previous one (Culot</context>
<context position="24306" citStr="Bunescu and Mooney, 2005" startWordPosition="4053" endWordPosition="4056">) (63.6) Zhao and Grishman (2005): 69.2 70.5 70.4 feature-based kernel (-) (-) (-) Table 4. Performance comparison on the ACE 2004 data over both 7 major types (the numbers outside parentheses) and 23 subtypes (the numbers in parentheses) (3) Performance Comparison: Tables 3 and 4 compare our method with previous work on the ACE 2002/2003/2004 data, respectively. They show that our method outperforms the previous methods and significantly outperforms the previous two dependency kernels4. This may be due to two reasons: 1) the dependency tree (Culotta and Sorensen, 2004) and the shortest path (Bunescu and Mooney, 2005) lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints. That is, it is 4 Bunescu and Mooney (2005) used the ACE 2002 corpus, including 422 documents, which is known to have many inconsistencies than the 2003 version. Culotta and Sorensen (2004) used a generic ACE corpus including about 800 documents (no corpus version is specified). Since the testing corpora are in different sizes and versions, strictly speaking, it is not ready to compa</context>
<context position="28024" citStr="Bunescu and Mooney (2005)" startWordPosition="4645" endWordPosition="4648">y the tree kernel and the entity kernel, respectively. Therefore our method has the potential of effectively capturing not only most of the previous flat features but also the useful syntactic structure features. (2) Compared with Previous Kernels: Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in Culotta and Sorensen (2004). Moreover, the difference between our method and Bunescu and Mooney (2005) is that their kernel is defined on the shortest path between two entities instead of the entire subtrees. However, the path does not maintain the tree structure information. In addition, their kernel requires the two paths to have the same length. Such constraint is too strict. 5.2 Other Issues (1) Speed Issue: The recursively-defined convolution kernel is much slower compared to feature-based classifiers. In this paper, the speed issue is solved in three ways. First, the inclusion of the entity kernel makes the composite kernel converge fast. Furthermore, we find that the small portion (PT) </context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Bunescu R. C. and Mooney R. J. 2005. A Shortest Path Dependency Kernel for Relation Extraction. EMNLP-2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head Parsing for Language Models.</title>
<date>2001</date>
<contexts>
<context position="15972" citStr="Charniak, 2001" startWordPosition="2695" endWordPosition="2696">nation of single non-terminal nodes affects the performance of relation extraction. T1): MCT T3):CPT T2): PT T4): FPT Figure 1. Different representations of a relation instance in the example sentence “...provide benefits to 200 domestic partners of their own workers in New York”, where the phrase type “E1-PER” denotes that the current node is the 1st entity with type “PERSON”, and likewise for the others. The relation instance is excerpted from the ACE 2003 corpus, where a relation “SOCIAL.Other-Personal” exists between entities “partners” (PER) and “workers” (PER). We use Charniak’s parser (Charniak, 2001) to parse the example sentence. To save space, the FCPT is not shown here. 4 Experiments 4.1 Experimental Setting Data: We use the English portion of both the ACE 2003 and 2004 corpora from LDC in our experiments. In the ACE 2003 data, the training set consists of 674 documents and 9683 relation instances while the test set consists of 97 documents and 1386 relation instances. The ACE 2003 data defines 5 entity types, 5 major relation types and 24 relation subtypes. The ACE 2004 data contains 451 documents and 5702 relation instances. It redefines 7 entity types, 7 major relation types and 23 </context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Charniak E. 2001. Immediate-head Parsing for Language Models. ACL-2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<pages>2001</pages>
<contexts>
<context position="2642" citStr="Collins and Duffy, 2001" startWordPosition="385" endWordPosition="388"> feature vectors. Kernel methods can be regarded as a generalization of the feature-based methods by replacing the dot-product with a kernel function between two vectors, or even between two objects. A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite. Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects. For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP. In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). This paper presents a novel composite kernel to explore diverse knowledge for relation extraction. The composite kernel consists of an entity kernel and a convolution parse tree kernel. Our study demonstrates that the composite kernel is very effective for relation extraction. It also shows without the need </context>
<context position="9622" citStr="Collins and Duffy (2001)" startWordPosition="1527" endWordPosition="1530">or two relation instances, Ei means the ith entity of a relation instance, and 826 KE(•,•) is a simple kernel function over the features of entities: K E E = ∑ C E f E f E ( 1, 2) i ( 1 . i , 2 . i ) (2) where fi represents the ith entity feature, and the function C(•,•) returns 1 if the two feature values are identical and 0 otherwise. KE(•,•) returns the number of feature values in common of two entities. (2) Convolution Parse Tree Kernel: A convolution kernel aims to capture structured information in terms of substructures. Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling. Generally, we can represent a parse tree T by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = (# subtree1(T), ..., # subtreei(T), ..., # subtreen(T) ) where # subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vectorφ(T) . To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree ke</context>
<context position="12976" citStr="Collins and Duffy, 2001" startWordPosition="2174" endWordPosition="2177">ds the best performance when α is set to 0.23. The polynomial expansion aims to explore the entity bi-gram features, esp. the combined features from the first and second entities, respectively. In addition, due to the different scales of the values of the two individual kernels, they are normalized before combination. This can avoid one kernel value being overwhelmed by that of another one. The entity kernel formulated by eqn. (1) is a proper kernel since it simply calculates the dot product of the entity feature vectors. The tree kernel formulated by eqn. (3) is proven to be a proper kernel (Collins and Duffy, 2001). Since kernel function set is closed under normalization, polynomial expansion and linear combination (Schölkopf and Smola, 2001), the two composite kernels are also proper kernels. 3 A kernel K(x, y) can be normalized by dividing it by K(x, x)• K(y, y) . &gt; = = ∑ ∑i i 827 3.2 Relation Instance Spaces A relation instance is encapsulated by a parse tree. Thus, it is critical to understand which portion of a parse tree is important in the kernel calculation. We study five cases as shown in Fig.1. (1) Minimum Complete Tree (MCT): the complete sub-tree rooted by the nearest common ancestor of the </context>
<context position="28876" citStr="Collins and Duffy (2001)" startWordPosition="4786" endWordPosition="4789">ave the same length. Such constraint is too strict. 5.2 Other Issues (1) Speed Issue: The recursively-defined convolution kernel is much slower compared to feature-based classifiers. In this paper, the speed issue is solved in three ways. First, the inclusion of the entity kernel makes the composite kernel converge fast. Furthermore, we find that the small portion (PT) of a full parse tree can effectively represent a relation instance. This significantly improves the speed. Finally, the parse tree kernel requires exact match between two subtrees, which normally does not occur very frequently. Collins and Duffy (2001) report that in practice, running time for the parse tree kernel is more close to linear (O(JN1J+JN2J), rather than O(JN1J*JN2J ). As a result, using the PC with Intel P4 3.0G CPU and 2G RAM, our system only takes about 110 minutes and 30 minutes to do training on the ACE 2003 (~77k training instances) and 2004 (~33k training instances) data, respectively. (2) Further Improvement: One of the potential problems in the parse tree kernel is that it carries out exact matches between sub-trees, so that this kernel fails to handle sparse phrases (i.e. “a car” vs. “a red car”) and near-synonymic gram</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins M. and Duffy N. 2001. Convolution Kernels for Natural Language. NIPS-2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency Tree Kernel for Relation Extraction.</title>
<date>2004</date>
<pages>2004</pages>
<contexts>
<context position="2901" citStr="Culotta and Sorensen (2004)" startWordPosition="428" endWordPosition="432">e properties of being symmetric and positive-definite. Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects. For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP. In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). This paper presents a novel composite kernel to explore diverse knowledge for relation extraction. The composite kernel consists of an entity kernel and a convolution parse tree kernel. Our study demonstrates that the composite kernel is very effective for relation extraction. It also shows without the need for extensive feature engineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively. An advantage of our method is that the composite kernel can e</context>
<context position="4834" citStr="Culotta and Sorensen, 2004" startWordPosition="730" endWordPosition="733">rnels” that are the kernels for the decompositions (parts) of the objects. 825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832, Sydney, July 2006. c�2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration. We conclude our work and indicate the future work in Section 6. 2 Related Work Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature. Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns. Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model. Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic featu</context>
<context position="6248" citStr="Culotta and Sorensen (2004)" startWordPosition="952" endWordPosition="955">ually calibrated and the hierarchical structured information in a parse tree is not well preserved in their parse tree-related features, which only represent simple flat path information connecting two entities in the parse tree through a path of non-terminals and a list of base phrase chunks. Prior kernel-based methods for this task focus on using individual tree kernels to exploit tree structure-related features. Zelenko et al. (2003) developed a kernel over parse trees for relation extraction. The kernel matches nodes from roots to leaf nodes recursively layer by layer in a topdown manner. Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees. Their tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending nodes from the roots to the current nodes. The two constraints make their kernel high precision but very low recall on the ACE 2003 corpus. Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction. 2 We classify the feature-based kernel defined in (Zhao and Grishman, 2005) into the feature-based methods since their kernels can be easily represented by the dot-pro</context>
<context position="24257" citStr="Culotta and Sorensen, 2004" startWordPosition="4045" endWordPosition="4048"> 76.1 68.4 72.1 (polynomial expansion) (68.6) (59.3) (63.6) Zhao and Grishman (2005): 69.2 70.5 70.4 feature-based kernel (-) (-) (-) Table 4. Performance comparison on the ACE 2004 data over both 7 major types (the numbers outside parentheses) and 23 subtypes (the numbers in parentheses) (3) Performance Comparison: Tables 3 and 4 compare our method with previous work on the ACE 2002/2003/2004 data, respectively. They show that our method outperforms the previous methods and significantly outperforms the previous two dependency kernels4. This may be due to two reasons: 1) the dependency tree (Culotta and Sorensen, 2004) and the shortest path (Bunescu and Mooney, 2005) lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints. That is, it is 4 Bunescu and Mooney (2005) used the ACE 2002 corpus, including 422 documents, which is known to have many inconsistencies than the 2003 version. Culotta and Sorensen (2004) used a generic ACE corpus including about 800 documents (no corpus version is specified). Since the testing corpora are in different sizes and vers</context>
<context position="27949" citStr="Culotta and Sorensen (2004)" startWordPosition="4633" endWordPosition="4637">entity-related features used in the feature-based methods are also captured by the tree kernel and the entity kernel, respectively. Therefore our method has the potential of effectively capturing not only most of the previous flat features but also the useful syntactic structure features. (2) Compared with Previous Kernels: Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in Culotta and Sorensen (2004). Moreover, the difference between our method and Bunescu and Mooney (2005) is that their kernel is defined on the shortest path between two entities instead of the entire subtrees. However, the path does not maintain the tree structure information. In addition, their kernel requires the two paths to have the same length. Such constraint is too strict. 5.2 Other Issues (1) Speed Issue: The recursively-defined convolution kernel is much slower compared to feature-based classifiers. In this paper, the speed issue is solved in three ways. First, the inclusion of the entity kernel makes the compos</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Culotta A. and Sorensen J. 2004. Dependency Tree Kernel for Relation Extraction. ACL-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Convolution Kernels on Discrete Structures.</title>
<date>1999</date>
<tech>Technical Report UCS-CRL-99-10,</tech>
<institution>University of California,</institution>
<location>Santa Cruz.</location>
<contexts>
<context position="4091" citStr="Haussler (1999)" startWordPosition="621" endWordPosition="622">he composite kernel can easily cover more knowledge by introducing more kernels. Evaluation on the ACE corpus shows that our method outperforms the previous bestreported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features. The rest of the paper is organized as follows. In Section 2, we review the previous work. Section 3 discusses our composite kernel. Section 4 reports the experimental results and our observations. Section 5 compares our method with the 1 Convolution kernels were proposed for a discrete structure by Haussler (1999) in the machine learning field. This framework defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. 825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832, Sydney, July 2006. c�2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration. We conclude our work and indicate the future work in Section 6. 2 Related Work Many techniques on relation extraction, such as rule-based (MUC, 1987-1</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>Haussler D. 1999. Convolution Kernels on Discrete Structures. Technical Report UCS-CRL-99-10, University of California, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vecor Machine: learning with many relevant features.</title>
<date>1998</date>
<pages>1998</pages>
<contexts>
<context position="17673" citStr="Joachims, 1998" startWordPosition="2979" endWordPosition="2980">e same sentence to generate potential relation instances. In this paper, we only measure the performance of relation extraction models on “true” mentions with “true” chaining of coreference (i.e. as annotated by LDC annotators). Implementation: We formalize relation extraction as a multi-class classification problem. SVM is selected as our classifier. We adopt the one vs. others strategy and select the one with the largest margin as the final answer. The training parameters are chosen using cross-validation (C=2.4 (SVM); λ =0.4(tree kernel)). In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004). Precision (P), Recall (R) and F-measure (F) are adopted to measure the performance. 4.2 Experimental Results In this subsection, we report the experiments of different kernel setups for different purposes. (1) Tree Kernel only over Different Relation Instance Spaces: In order to better study the impact of the syntactic structure information in a parse tree on relation extraction, we remove the entity-related information from parse trees by replacing the entity-related phrase types (“E1- PER” and so on as shown in Fig. 1) with “NP”. Table 1 compares the</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Joachims T. 1998. Text Categorization with Support Vecor Machine: learning with many relevant features. ECML-1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kambhatla</author>
</authors>
<title>Combining lexical, syntactic and semantic features with Maximum Entropy models for extracting relations.</title>
<date>2004</date>
<note>ACL-2004 (poster)</note>
<contexts>
<context position="4747" citStr="Kambhatla 2004" startWordPosition="718" endWordPosition="719">work defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. 825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832, Sydney, July 2006. c�2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration. We conclude our work and indicate the future work in Section 6. 2 Related Work Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature. Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns. Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model. Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a lar</context>
<context position="23111" citStr="Kambhatla (2004)" startWordPosition="3864" endWordPosition="3865">C annotators. 2) More importantly, the ACE 2004 data defines 43 entity subtypes while there are only 3 subtypes in the 2003 data. The detailed classification in the 2004 data leads to significant performance improvement of 6.2 (54.4-48.2) in Fmeasure over that on the 2003 data. Our composite kernel can achieve 77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over the ACE 2003/2004 major types, respectively. Methods (2002/2003 data) P(%) R(%) F Ours: composite kernel 2 77.3 65.6 70.9 (polynomial expansion) (64.9) (51.2) (57.2) Zhou et al. (2005): 77.2 60.7 68.0 feature-based SVM (63.1) (49.5) (55.5) Kambhatla (2004): (-) (-) (-) feature-based ME (63.5) (45.2) (52.8) Ours: tree kernel with en- 76.1 62.6 68.7 tity information at node (62.4) (48.5) (54.6) Bunescu and Mooney 65.5 43.8 52.5 (2005): shortest path de- (-) (-) (-) pendency kernel Culotta and Sorensen 67.1 35.0 45.8 (2004): dependency kernel (-) (-) (-) Table 3. Performance comparison on the ACE 2003/2003 data over both 5 major types (the numbers outside parentheses) and 24 subtypes (the numbers in parentheses) Methods (2004 data) P(%) R(%) F Ours: composite kernel 2 76.1 68.4 72.1 (polynomial expansion) (68.6) (59.3) (63.6) Zhao and Grishman (20</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Kambhatla N. 2004. Combining lexical, syntactic and semantic features with Maximum Entropy models for extracting relations. ACL-2004 (poster)</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lodhi</author>
<author>C Saunders</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
<author>C Watkins</author>
</authors>
<title>Text classification using string kernel.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2002--2</pages>
<contexts>
<context position="2678" citStr="Lodhi et al., 2002" startWordPosition="392" endWordPosition="395">egarded as a generalization of the feature-based methods by replacing the dot-product with a kernel function between two vectors, or even between two objects. A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite. Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects. For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP. In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). This paper presents a novel composite kernel to explore diverse knowledge for relation extraction. The composite kernel consists of an entity kernel and a convolution parse tree kernel. Our study demonstrates that the composite kernel is very effective for relation extraction. It also shows without the need for extensive feature engineering th</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Lodhi H., Saunders C., Shawe-Taylor J., Cristianini N. and Watkins C. 2002. Text classification using string kernel. Journal of Machine Learning Research, 2002(2):419-444</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>H Fox</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<pages>2000</pages>
<contexts>
<context position="4716" citStr="Miller et al., 2000" startWordPosition="713" endWordPosition="716">he machine learning field. This framework defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. 825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832, Sydney, July 2006. c�2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration. We conclude our work and indicate the future work in Section 6. 2 Related Work Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature. Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns. Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model. Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Miller S., Fox H., Ramshaw L. and Weischedel R. 2000. A novel use of statistical parsing to extract information from text. NAACL-2000</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>A Study on Convolution Kernels for Shallow Semantic Parsing.</title>
<date>2004</date>
<pages>2004</pages>
<contexts>
<context position="9665" citStr="Moschitti (2004)" startWordPosition="1535" endWordPosition="1536">of a relation instance, and 826 KE(•,•) is a simple kernel function over the features of entities: K E E = ∑ C E f E f E ( 1, 2) i ( 1 . i , 2 . i ) (2) where fi represents the ith entity feature, and the function C(•,•) returns 1 if the two feature values are identical and 0 otherwise. KE(•,•) returns the number of feature values in common of two entities. (2) Convolution Parse Tree Kernel: A convolution kernel aims to capture structured information in terms of substructures. Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling. Generally, we can represent a parse tree T by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = (# subtree1(T), ..., # subtreei(T), ..., # subtreen(T) ) where # subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vectorφ(T) . To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between t</context>
<context position="17713" citStr="Moschitti, 2004" startWordPosition="2985" endWordPosition="2986">elation instances. In this paper, we only measure the performance of relation extraction models on “true” mentions with “true” chaining of coreference (i.e. as annotated by LDC annotators). Implementation: We formalize relation extraction as a multi-class classification problem. SVM is selected as our classifier. We adopt the one vs. others strategy and select the one with the largest margin as the final answer. The training parameters are chosen using cross-validation (C=2.4 (SVM); λ =0.4(tree kernel)). In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004). Precision (P), Recall (R) and F-measure (F) are adopted to measure the performance. 4.2 Experimental Results In this subsection, we report the experiments of different kernel setups for different purposes. (1) Tree Kernel only over Different Relation Instance Spaces: In order to better study the impact of the syntactic structure information in a parse tree on relation extraction, we remove the entity-related information from parse trees by replacing the entity-related phrase types (“E1- PER” and so on as shown in Fig. 1) with “NP”. Table 1 compares the performance of 5 tree kernel setups on </context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Moschitti A. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. ACL-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC</author>
</authors>
<date>1987</date>
<note>http://www.itl.nist.gov/iaui/894.02/ related_projects/muc/</note>
<contexts>
<context position="1334" citStr="MUC, 1987" startWordPosition="195" endWordPosition="196">posite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction. 1 Introduction The goal of relation extraction is to find various predefined semantic relations between pairs of entities in text. The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) (MUC, 1987- 1998) and Automatic Content Extraction (ACE) program (ACE, 2002-2005). According to the ACE Program, an entity is an object or set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities. For example, the sentence “Bill Gates is chairman and chief software architect of Microsoft Corporation.” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (PERSON.Name) and “Microsoft Corporation” (ORGANIZATION. Commercial). In this paper, we address the problem of relation extraction using kernel methods (Schölkopf and Smola</context>
<context position="4689" citStr="MUC, 1987" startWordPosition="711" endWordPosition="712">sler (1999) in the machine learning field. This framework defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. 825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832, Sydney, July 2006. c�2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration. We conclude our work and indicate the future work in Section 6. 2 Related Work Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature. Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns. Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model. Feature-based methods (Kambhatla, 2004; Zhou et al., </context>
</contexts>
<marker>MUC, 1987</marker>
<rawString>MUC. 1987-1998. http://www.itl.nist.gov/iaui/894.02/ related_projects/muc/</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Schölkopf</author>
<author>A J Smola</author>
</authors>
<title>Learning with Kernels: SVM, Regularization, Optimization and Beyond.</title>
<date>2001</date>
<pages>407--423</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA</location>
<contexts>
<context position="1941" citStr="Schölkopf and Smola, 2001" startWordPosition="282" endWordPosition="285">s (MUCs) (MUC, 1987- 1998) and Automatic Content Extraction (ACE) program (ACE, 2002-2005). According to the ACE Program, an entity is an object or set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities. For example, the sentence “Bill Gates is chairman and chief software architect of Microsoft Corporation.” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (PERSON.Name) and “Microsoft Corporation” (ORGANIZATION. Commercial). In this paper, we address the problem of relation extraction using kernel methods (Schölkopf and Smola, 2001). Many feature-based learning algorithms involve only the dot-product between feature vectors. Kernel methods can be regarded as a generalization of the feature-based methods by replacing the dot-product with a kernel function between two vectors, or even between two objects. A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite. Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects. For example, </context>
<context position="8041" citStr="Schölkopf and Smola, 2001" startWordPosition="1243" endWordPosition="1246">rnel methods can explore the huge amounts of implicit (structured) features, until now the feature-based methods enjoy more success. One may ask: how can we make full use of the nice properties of kernel methods and define an effective kernel for relation extraction? In this paper, we study how relation extraction can benefit from the elegant properties of kernel methods: 1) implicitly exploring (structured) features in a high dimensional space; and 2) the nice mathematical properties, for example, the sum, product, normalization and polynomial expansion of existing kernels is a valid kernel (Schölkopf and Smola, 2001). We also demonstrate how our composite kernel effectively captures the diverse knowledge for relation extraction. 3 Composite Kernel for Relation Extraction In this section, we define the composite kernel and study the effective representation of a relation instance. 3.1 Composite Kernel Our composite kernel consists of an entity kernel and a convolution parse tree kernel. To our knowledge, convolution kernels have not been explored for relation extraction. (1) Entity Kernel: The ACE 2003 data defines four entity features: entity headword, entity type and subtype (only for GPE), and mention t</context>
<context position="13106" citStr="Schölkopf and Smola, 2001" startWordPosition="2191" endWordPosition="2194">mbined features from the first and second entities, respectively. In addition, due to the different scales of the values of the two individual kernels, they are normalized before combination. This can avoid one kernel value being overwhelmed by that of another one. The entity kernel formulated by eqn. (1) is a proper kernel since it simply calculates the dot product of the entity feature vectors. The tree kernel formulated by eqn. (3) is proven to be a proper kernel (Collins and Duffy, 2001). Since kernel function set is closed under normalization, polynomial expansion and linear combination (Schölkopf and Smola, 2001), the two composite kernels are also proper kernels. 3 A kernel K(x, y) can be normalized by dividing it by K(x, x)• K(y, y) . &gt; = = ∑ ∑i i 827 3.2 Relation Instance Spaces A relation instance is encapsulated by a parse tree. Thus, it is critical to understand which portion of a parse tree is important in the kernel calculation. We study five cases as shown in Fig.1. (1) Minimum Complete Tree (MCT): the complete sub-tree rooted by the nearest common ancestor of the two entities under consideration. (2) Path-enclosed Tree (PT): the smallest common sub-tree including the two entities. In other w</context>
</contexts>
<marker>Schölkopf, Smola, 2001</marker>
<rawString>Schölkopf B. and Smola A. J. 2001. Learning with Kernels: SVM, Regularization, Optimization and Beyond. MIT Press, Cambridge, MA 407-423</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>T Hirao</author>
<author>Y Sasaki</author>
<author>E Maeda</author>
</authors>
<title>Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data.</title>
<date>2003</date>
<contexts>
<context position="2717" citStr="Suzuki et al., 2003" startWordPosition="399" endWordPosition="402">ture-based methods by replacing the dot-product with a kernel function between two vectors, or even between two objects. A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite. Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects. For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP. In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). This paper presents a novel composite kernel to explore diverse knowledge for relation extraction. The composite kernel consists of an entity kernel and a convolution parse tree kernel. Our study demonstrates that the composite kernel is very effective for relation extraction. It also shows without the need for extensive feature engineering the composite kernel can not only capture</context>
</contexts>
<marker>Suzuki, Hirao, Sasaki, Maeda, 2003</marker>
<rawString>Suzuki J., Hirao T., Sasaki Y. and Maeda E. 2003. Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data. ACL-2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel Methods for Relation Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<pages>2003--2</pages>
<contexts>
<context position="2872" citStr="Zelenko et al. (2003)" startWordPosition="424" endWordPosition="427"> function satisfying the properties of being symmetric and positive-definite. Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects. For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP. In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). This paper presents a novel composite kernel to explore diverse knowledge for relation extraction. The composite kernel consists of an entity kernel and a convolution parse tree kernel. Our study demonstrates that the composite kernel is very effective for relation extraction. It also shows without the need for extensive feature engineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively. An advantage of our method is th</context>
<context position="4806" citStr="Zelenko et al., 2003" startWordPosition="726" endWordPosition="729">ng convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. 825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832, Sydney, July 2006. c�2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration. We conclude our work and indicate the future work in Section 6. 2 Related Work Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature. Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns. Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model. Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse linguistic features, such as lexical, </context>
<context position="6061" citStr="Zelenko et al. (2003)" startWordPosition="921" endWordPosition="924">hese methods are very effective for relation extraction and show the bestreported performance on the ACE corpus. However, the problems are that these diverse features have to be manually calibrated and the hierarchical structured information in a parse tree is not well preserved in their parse tree-related features, which only represent simple flat path information connecting two entities in the parse tree through a path of non-terminals and a list of base phrase chunks. Prior kernel-based methods for this task focus on using individual tree kernels to exploit tree structure-related features. Zelenko et al. (2003) developed a kernel over parse trees for relation extraction. The kernel matches nodes from roots to leaf nodes recursively layer by layer in a topdown manner. Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees. Their tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending nodes from the roots to the current nodes. The two constraints make their kernel high precision but very low recall on the ACE 2003 corpus. Bunescu and Mooney (2005) proposed another dependency tree kernel for r</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Zelenko D., Aone C. and Richardella A. 2003. Kernel Methods for Relation Extraction. Journal of Machine Learning Research. 2003(2):1083-1106</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Zhao</author>
<author>R Grishman</author>
</authors>
<title>Extracting Relations with Integrated Information Using Kernel Methods.</title>
<date>2005</date>
<pages>2005</pages>
<contexts>
<context position="5318" citStr="Zhao and Grishman, 2005" startWordPosition="803" endWordPosition="806"> Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature. Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns. Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model. Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features. These methods are very effective for relation extraction and show the bestreported performance on the ACE corpus. However, the problems are that these diverse features have to be manually calibrated and the hierarchical structured information in a parse tree is not well preserved in their parse tree-related features, which only represent simple flat path information connecting two entities in the parse tree through a path of non-terminals and a list of base phrase chunks. Pr</context>
<context position="6756" citStr="Zhao and Grishman, 2005" startWordPosition="1039" endWordPosition="1042">l matches nodes from roots to leaf nodes recursively layer by layer in a topdown manner. Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees. Their tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending nodes from the roots to the current nodes. The two constraints make their kernel high precision but very low recall on the ACE 2003 corpus. Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction. 2 We classify the feature-based kernel defined in (Zhao and Grishman, 2005) into the feature-based methods since their kernels can be easily represented by the dot-products between explicit feature vectors. Their kernel simply counts the number of common word classes at each position in the shortest paths between two entities in dependency trees. The kernel requires the two paths to have the same length; otherwise the kernel value is zero. Therefore, although this kernel shows performance improvement over the previous one (Culotta and Sorensen, 2004), the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on t</context>
<context position="16612" citStr="Zhao and Grishman (2005)" startWordPosition="2806" endWordPosition="2809">ample sentence. To save space, the FCPT is not shown here. 4 Experiments 4.1 Experimental Setting Data: We use the English portion of both the ACE 2003 and 2004 corpora from LDC in our experiments. In the ACE 2003 data, the training set consists of 674 documents and 9683 relation instances while the test set consists of 97 documents and 1386 relation instances. The ACE 2003 data defines 5 entity types, 5 major relation types and 24 relation subtypes. The ACE 2004 data contains 451 documents and 5702 relation instances. It redefines 7 entity types, 7 major relation types and 23 subtypes. Since Zhao and Grishman (2005) use a 5-fold cross-validation on a subset of the 2004 data (newswire and broadcast news domains, containing 348 documents and 4400 relation instances), for comparison, we use the same setting (5-fold cross-validation on the same subset of the 2004 data, but the 5 partitions may not be the same) for the ACE 2004 data. Both corpora are parsed using Charniak’s parser (Charniak, 2001). We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances. In this paper, we only measure the performance of relation extraction models on “true” mentions </context>
<context position="23714" citStr="Zhao and Grishman (2005)" startWordPosition="3959" endWordPosition="3962">5.5) Kambhatla (2004): (-) (-) (-) feature-based ME (63.5) (45.2) (52.8) Ours: tree kernel with en- 76.1 62.6 68.7 tity information at node (62.4) (48.5) (54.6) Bunescu and Mooney 65.5 43.8 52.5 (2005): shortest path de- (-) (-) (-) pendency kernel Culotta and Sorensen 67.1 35.0 45.8 (2004): dependency kernel (-) (-) (-) Table 3. Performance comparison on the ACE 2003/2003 data over both 5 major types (the numbers outside parentheses) and 24 subtypes (the numbers in parentheses) Methods (2004 data) P(%) R(%) F Ours: composite kernel 2 76.1 68.4 72.1 (polynomial expansion) (68.6) (59.3) (63.6) Zhao and Grishman (2005): 69.2 70.5 70.4 feature-based kernel (-) (-) (-) Table 4. Performance comparison on the ACE 2004 data over both 7 major types (the numbers outside parentheses) and 23 subtypes (the numbers in parentheses) (3) Performance Comparison: Tables 3 and 4 compare our method with previous work on the ACE 2002/2003/2004 data, respectively. They show that our method outperforms the previous methods and significantly outperforms the previous two dependency kernels4. This may be due to two reasons: 1) the dependency tree (Culotta and Sorensen, 2004) and the shortest path (Bunescu and Mooney, 2005) lack th</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Zhao S.B. and Grishman R. 2005. Extracting Relations with Integrated Information Using Kernel Methods. ACL-2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>J Su</author>
<author>J Zhang</author>
<author>M Zhang</author>
</authors>
<title>Exploring Various Knowledge in Relation Extraction.</title>
<date>2005</date>
<pages>2005</pages>
<contexts>
<context position="4767" citStr="Zhou et al., 2005" startWordPosition="720" endWordPosition="723">ernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. 825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832, Sydney, July 2006. c�2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration. We conclude our work and indicate the future work in Section 6. 2 Related Work Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature. Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns. Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model. Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse</context>
<context position="23039" citStr="Zhou et al. (2005)" startWordPosition="3852" endWordPosition="3855">elation types and subtypes in order to reduce the inconsistency between LDC annotators. 2) More importantly, the ACE 2004 data defines 43 entity subtypes while there are only 3 subtypes in the 2003 data. The detailed classification in the 2004 data leads to significant performance improvement of 6.2 (54.4-48.2) in Fmeasure over that on the 2003 data. Our composite kernel can achieve 77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over the ACE 2003/2004 major types, respectively. Methods (2002/2003 data) P(%) R(%) F Ours: composite kernel 2 77.3 65.6 70.9 (polynomial expansion) (64.9) (51.2) (57.2) Zhou et al. (2005): 77.2 60.7 68.0 feature-based SVM (63.1) (49.5) (55.5) Kambhatla (2004): (-) (-) (-) feature-based ME (63.5) (45.2) (52.8) Ours: tree kernel with en- 76.1 62.6 68.7 tity information at node (62.4) (48.5) (54.6) Bunescu and Mooney 65.5 43.8 52.5 (2005): shortest path de- (-) (-) (-) pendency kernel Culotta and Sorensen 67.1 35.0 45.8 (2004): dependency kernel (-) (-) (-) Table 3. Performance comparison on the ACE 2003/2003 data over both 5 major types (the numbers outside parentheses) and 24 subtypes (the numbers in parentheses) Methods (2004 data) P(%) R(%) F Ours: composite kernel 2 76.1 68.</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Zhou G.D., Su J, Zhang J. and Zhang M. 2005. Exploring Various Knowledge in Relation Extraction. ACL-2005</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>