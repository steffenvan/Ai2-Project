<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000419">
<title confidence="0.996181">
Whose thumb is it anyway?
Classifying author personality from weblog text
</title>
<author confidence="0.997925">
Jon Oberlander
</author>
<affiliation confidence="0.998511">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.8209485">
2 Buccleuch Place
Edinburgh, EH8 9LW
</address>
<email confidence="0.998789">
j.oberlander@ed.ac.uk
</email>
<author confidence="0.994006">
Scott Nowson
</author>
<affiliation confidence="0.998499">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.8208335">
2 Buccleuch Place
Edinburgh, EH8 9LW
</address>
<email confidence="0.998743">
s.nowson@ed.ac.uk
</email>
<sectionHeader confidence="0.993885" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939">
We report initial results on the relatively
novel task of automatic classification of
author personality. Using a corpus of per-
sonal weblogs, or ‘blogs’, we investigate
the accuracy that can be achieved when
classifying authors on four important per-
sonality traits. We explore both binary and
multiple classification, using differing sets
of n-gram features. Results are promising
for all four traits examined.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867516129033">
There is now considerable interest in affective lan-
guage processing. Work focusses on analysing
subjective features of text or speech, such as sen-
timent, opinion, emotion or point of view (Pang
et al., 2002; Turney, 2002; Dave et al., 2003; Liu
et al., 2003; Pang and Lee, 2005; Shanahan et al.,
2005). Discussing affective computing in general,
Picard (1997) notes that phenomena vary in du-
ration, ranging from short-lived feelings, through
emotions, to moods, and ultimately to long-lived,
slowly-changing personality characteristics.
Within computational linguistics, most work
has focussed on sentiment and opinion concern-
ing specific entities or events, and on binary clas-
sifications of these. For instance, both Pang and
Lee (2002) and Turney (2002) consider the thumbs
up/thumbs down decision: is a film review posi-
tive or negative? However, Pang and Lee (2005)
point out that ranking items or comparing re-
views will benefit from finer-grained classifica-
tions, over multiple ordered classes: is a film re-
view two- or three- or four-star? And at the same
time, some work now considers longer-term af-
fective states. For example, Mishne (2005) aims
to classify the primary mood of weblog post-
ings; the study encompasses both fine-grained
(but non-ordered) multiple classification (frus-
trated/loved/etc.) and coarse-grained binary clas-
sification (active/passive, positive/negative).
This paper is about the move to finer-grained
multiple classifications; and also about weblogs.
But it is also about even more persistent affec-
tive states; in particular, it focusses on classifying
author personality. We would argue that ongo-
ing work on sentiment analysis or opinion-mining
stands to benefit from progress on personality-
classification. The reason is that people vary in
personality, and they vary in how they appraise
events—and hence, in how strongly they phrase
their praise or condemnation. Reiter and Sripada
(2004) suggest that lexical choice may sometimes
be determined by a writer’s idiolect—their per-
sonal language preferences. We suggest that while
idiolect can be a matter of accident or experience,
it may also reflect systematic, personality-based
differences. This can help explain why, as Pang
and Lee (2005) note, one person’s four star re-
view is another’s two-star. To put it more bluntly,
if you’re not a very outgoing sort of person, then
your thumbs up might be mistaken for someone
else’s thumbs down. But how do we distinguish
such people? Or, if we spot a thumbs-up review,
how can we tell whose thumb it is, anyway?
The paper is structured as follows. It introduces
trait theories of personality, notes work to date on
personality classification, and raises some ques-
tions. It then outlines the weblog corpus and the
experiments, which compare classification accura-
cies for four personality dimensions, seven tasks,
and five feature selection policies. We discuss the
implications of the results, and related work, and
end with suggestions for next steps.
</bodyText>
<page confidence="0.969682">
627
</page>
<note confidence="0.748073666666667">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 627–634,
Sydney, July 2006. c�2006 Association for Computational Linguistics
2 Background: traits and language
</note>
<bodyText confidence="0.919475604166666">
Cattell’s pioneering work led to the isolation of
16 primary personality factors, and later work on
secondary factors led to Costa and McCrae’s five-
factor model, closely related to the ‘Big Five’
models emerging from lexical research (Costa and
McCrae, 1992). Each factor gives a continu-
ous dimension for personality scoring. These
are: Extraversion; Neuroticism; Openness; Agree-
ableness; and Conscientiousness (Matthews et al.,
2003). Work has also investigated whether scores
on these dimensions correlate with language use
(Scherer, 1979; Dewaele and Furnham, 1999).
Building on the earlier work of Gottschalk and
Gleser, Pennebaker and colleagues secured signif-
icant results using the Linguistic Inquiry and Word
Count text analysis program (Pennebaker et al.,
2001). This primarily counts relative frequencies
of word-stems in pre-defined semantic and syn-
tactic categories. It shows, for instance, that high
Neuroticism scorers use: more first person singu-
lar and negative emotion words; and fewer arti-
cles and positive emotion words (Pennebaker and
King, 1999).
So, can a text classifier trained on such features
predict the author personality? We know of only
one published study: Argamon et al. (2005) fo-
cussed on Extraversion and Neuroticism, dividing
Pennebaker and King’s (1999) population into the
top- and bottom-third scorers on a dimension, and
discarding the middle third. For both dimensions,
using a restricted feature set, they report binary
classification accuracy of around 58%: an 8% ab-
solute improvement over their baseline. Although
mood is more malleable, work on it is also relevant
(Mishne, 2005). Using a more typical feature set
(including n-grams of words and parts-of-speech),
the best mood classification accuracy was 66%, for
‘confused’. At a coarser grain, moods could be
classified with accuracies of 57% (active vs. pas-
sive), and 60% (positive vs. negative).
So, Argamon et al. used a restricted feature set
for binary classification on two dimensions: Ex-
traversion and Neuroticism. Given this, we now
pursue three questions. (1) Can we improve per-
formance on a similar binary classification task?
(2) How accurate can classification be on the other
dimensions? (3) How accurate can multiple—
three-way or five-way—classification be?
</bodyText>
<sectionHeader confidence="0.903897" genericHeader="method">
3 The weblog corpus
</sectionHeader>
<subsectionHeader confidence="0.997631">
3.1 Construction
</subsectionHeader>
<bodyText confidence="0.999758060606061">
A corpus of personal weblog (‘blog’) text has been
gathered (Nowson, 2006). Participants were re-
cruited directly via e-mail to suitable candidates,
and indirectly by word-of-mouth: many partici-
pants wrote about the study in their blogs. Par-
ticipants were first required to answer sociobio-
graphic and personality questionnaires. The per-
sonality instrument has specifically been validated
for online completion (Buchanan, 2001). It was
derived from the 50-item IPIP implementation of
Costa and McCrae’s (1992) revised NEO person-
ality inventory; participants rate themselves on 41-
items using a 5-point Likert scale. This provides
scores for Neuroticism, Extraversion, Openness,
Agreeableness and Conscientiousness.
After completing this stage, participants were
requested to submit one month’s worth of prior
weblog postings. The month was pre-specified so
as to reduce the effects of an individual choos-
ing what they considered their ‘best’ or ‘preferred’
month. Raw submissions were marked-up using
XML so as to automate extraction of the desired
text. Text was also marked-up by post type, such
as purely personal, commentary reporting of ex-
ternal matters, or direct posting of internet memes
such as quizzes. The corpus consisted of 71 par-
ticipants (47 females, 24 males; average ages 27.8
and 29.4, respectively) and only the text marked
as ‘personal’ from each weblog, approximately
410,000 words. To eliminate undue influence of
particularly verbose individuals, the size of each
weblog file was truncated at the mean word count
plus 2 standard deviations.
</bodyText>
<subsectionHeader confidence="0.999729">
3.2 Personality distribution
</subsectionHeader>
<bodyText confidence="0.998958538461538">
It might be thought that bloggers are more Ex-
travert than most (because they express themselves
in public); or perhaps that they are less Extravert
(because they keep diaries in the first place). In
fact, plotting the Extraversion scores for the cor-
pus authors gives an apparently normal distribu-
tion; and the same applies for three other dimen-
sions. However, scores for Openness to experi-
ence are not normally distributed. Perhaps blog-
gers are more Open than average; or perhaps there
is response bias. Without a comparison sample of
matched non-bloggers, one cannot say, and Open-
ness is not discussed further in this paper.
</bodyText>
<page confidence="0.998987">
628
</page>
<sectionHeader confidence="0.999403" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999996571428571">
We are thus confined to classifying on four per-
sonality dimensions. However, a number of other
variables remain: different learning algorithms
can be employed; authors in the corpus can be
grouped in several ways, leading to various classi-
fication tasks; and more or less restricted linguistic
feature sets can be used as input to the classifier.
</bodyText>
<subsectionHeader confidence="0.992369">
4.1 Algorithms
</subsectionHeader>
<bodyText confidence="0.999827588235294">
Support Vector Machines (SVM) appear to work
well for binary sentiment classification tasks, so
Argamon et al. (2003) and Pang and Lee (2005)
consider One-vs-All, or All-vs-All, variants on
SVM, to permit multiple classifications. Choice
of algorithm is not our focus, but it remains to
be seen whether SVM outperforms Naive Bayes
(NB) for personality classification. Thus, we will
use both on the binary Tasks 1 to 3 (defined in sec-
tion 4.2.1), for each of the personality dimensions,
and each of the manually-selected feature sets
(Levels I to IV, defined in section 4.3). Whichever
performs better overall is then reported in full, and
used for the multiple Tasks 4 to 7 (defined in sec-
tion 4.2.2). Both approaches are applied as imple-
mented in the WEKA toolkit (Witten and Frank,
1999) and use 10-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.983142">
4.2 Tasks
</subsectionHeader>
<bodyText confidence="0.999993909090909">
For any blog, we have available the scores, on con-
tinuous scales, of its author on four personality di-
mensions. But for the classifier, the task can be
made more or less easy, by grouping authors on
each of the dimensions. The simplest tasks are, of
course, binary: given the sequence of words from
a blog, the classifier simply has to decide whether
the author is (for instance) high or low in Agree-
ableness. Binary tasks vary in difficulty, depend-
ing on whether authors scoring in the middle of a
dimension are left out, or not; and if they are left
out, what proportion of authors are left out.
More complex tasks will also vary in difficulty
depending on who is left out. But in the cases
considered here, middle authors are now included.
For a three-way task, the classifier must decide
if an author is high, medium or low; and those
authors known to score between these categories
may, or may not, be left out. In the most challeng-
ing five-way task, no-one is left out. The point of
considering such tasks is to gradually approximate
the most challenging task of all: continuous rating.
</bodyText>
<subsubsectionHeader confidence="0.60215">
4.2.1 Binary classification tasks
</subsubsectionHeader>
<bodyText confidence="0.9994625">
In these task variants, the goal is to classify au-
thors as either high or low scorers on a dimension:
</bodyText>
<listItem confidence="0.94417255">
1. The easiest approach is to keep the high and
low groups as far apart as possible: high scor-
ers (H) are those whose scores fall above
1 SD above the mean; low scorers (L) are
those whose scores fall below 1 SD below the
mean.
2. Task-1 creates distinct groups, at the price of
excluding over 50% of the corpus from the
analysis. To include more of the corpus, pa-
rameters are relaxed: the high group (HH)
includes anyone whose score is above .5 SD
above the mean; the low group (LL) is simi-
larly placed below.
3. The most obvious task (but not the easiest)
arises from dividing the corpus in half about
the mean score. This creates high (HHH) and
low (LLL) groups, covering the entire pop-
ulation. Inevitably, some HHH scorers will
actually have scores much closer to those of
LLL scorers than to other HHH scorers.
</listItem>
<bodyText confidence="0.981258">
These sub-groups are tabulated in Table 1, giv-
ing the size of each group within each trait. Note
that in Task-2, the standard-deviation-based divi-
sions contain very nearly the top third and bottom
third of the population for each dimension. Hence,
Task-2 is closest in proportion to the division by
thirds used in Argamon et al. (2005).
</bodyText>
<table confidence="0.9969946875">
Lowest ... Highest
1 L – H
2 LL – HH
3 LLL HHH
N1 12 – 13
N2 25 – 22
N3 39 32
E1 11 – 12
E2 23 – 24
E3 32 39
A1 11 – 13
A2 22 – 21
A3 34 37
C1 11 – 14
C2 17 – 27
C3 30 41
</table>
<tableCaption confidence="0.995498">
Table 1: Binary task groups: division method and
</tableCaption>
<bodyText confidence="0.598891">
author numbers. N = Neuroticism; E = Extraver-
sion; A = Agreeableness; C = Conscientiousness.
</bodyText>
<page confidence="0.988637">
629
</page>
<listItem confidence="0.981757857142857">
4.2.2 Multiple classification tasks
4. Takes the greatest distinction between high
(H) and low (L) groups from Task-1, and
adds a medium group, but attempts to reduce
the possibility of inter-group confusion by in-
cluding only the smaller medium (m) group
omitted from Task-2. Not all subjects are
therefore included in this analysis. Since the
three groups to be classified are completely
distinct, this should be the easiest of the four
multiple-class tasks.
5. Following Task-4, this uses the most distinct
high (H) and low (L) groups, but now consid-
ers all remaining subjects medium (M).
6. Following Task-2, this uses the larger high
(hH) and low (Ll) groups, with all those in
between forming the medium (m) group.
7. Using the distinction between the high and
low groups of Task-5 and -6, this creates a
5-way split: highest (H), relatively high (h),
medium (m), relatively low (l) and lowest
</listItem>
<bodyText confidence="0.98669125">
(L). With the greatest number of classes, this
task is the hardest.
These sub-groups are tabulated in Table 2, giving
the size of each group within each trait.
</bodyText>
<table confidence="0.984951761904762">
Lowest ... Highest
4 L – m – H
5 L M H
6 Ll m hH
7 L l m h H
N4 12 – 24 – 13
N5 12 46 13
N6 25 24 22
N7 12 13 24 9 13
E4 11 – 24 – 12
E5 11 48 12
E6 23 24 24
E7 11 12 24 12 12
A4 11 – 28 – 13
A5 11 47 13
A6 22 28 21
A7 11 11 28 8 13
C4 11 – 27 – 14
C5 11 46 14
C6 17 27 27
C7 11 6 27 13 14
</table>
<tableCaption confidence="0.721966">
Table 2: 3-way/5-way task groups: division
method and author numbers. N = Neuroticism; E
= Extraversion; A = Agreeableness; C = Consci-
entiousness.
</tableCaption>
<subsectionHeader confidence="0.989693">
4.3 Feature selection
</subsectionHeader>
<bodyText confidence="0.999985959183674">
There are many possible features that can be
used for automatic text classification. These ex-
periments use essentially word-based bi- and tri-
grams. It should be noted, however, that some
generalisations have been made: all proper nouns
were identified via CLAWS tagging using the
WMatrix tool (Rayson, 2003), and replaced with
a single marker (NP1); punctuation was collapsed
into a single marker (&lt;p&gt;); and additional tags
correspond to non-linguistic features of blogs—
for instance, &lt;SOP&gt; and &lt;EOP&gt; were used the
mark the start and end of individual blogs posts.
Word n-gram approaches provide a large feature
space with which to work. But in the general
interest of computational tractability, it is useful
to reduce the size of the feature set. There are
many automatic approaches to feature selection,
exploiting, for instance, information gain (Quin-
lan, 1993). However, ‘manual’ methods can of-
fer principled ways of both reducing the size of
the set and avoiding overfitting. We therefore ex-
plore the effect of different levels of restriction on
the feature sets, and compare them with automatic
feature selection. The levels of restriction are as
follows:
I The least restricted feature set consists of the
n-grams most commonly occurring within the
blog corpus. Therefore, the feature set for
each personality dimension is to be drawn
from the same pool. The difference lies in the
number of features selected: the size of the set
will match that of the next level of restriction.
II The next set includes only those n-grams
which were distinctive for the two extremes
of each personality trait. Only features with
a corpus frequency &gt;5 are included. This al-
lows accurate log-likelihood G2 statistics to
be computed (Rayson, 2003). Distinct collo-
cations are identified via a three way compar-
ison between the H and L groups in Task-1
(see section 4.2.1) and a third, neutral group.
This neutral group contains all those individ-
uals who fell in the medium group (M) for
all four traits in the study; the resulting group
was of comparable size to the H and L groups
for each trait. Hence, this approach selects
features using only a subset of the corpus. N-
gram software was used to identify and count
collocations within a sub-corpus (Banerjee
</bodyText>
<page confidence="0.988786">
630
</page>
<bodyText confidence="0.999924617021277">
and Pedersen, 2003). For each feature found,
its frequency and relative frequency are calcu-
lated. This permits relative frequency ratios
and log-likelihood comparisons to be made
between High-Low, High-Neutral and Low-
Neutral. Only features that prove distinctive
for the H or L groups with a significance of
p &lt; .01 are included in the feature set.
III The next set takes into account the possibil-
ity that, for a group used in Level-II, an n-
gram may be used relatively frequently, but
only because a small number of authors in a
group use it very frequently, while others in
the same group use it not at all. To enter the
Level-III set, an n-gram meeting the Level-II
criteria must also be used by at least 50%1 of
the individuals within the subgroup for which
it is reported to be distinctive.
IV While Level-III guards against excessive indi-
vidual influence, it may abstract too far from
the fine-grained variation within a personality
trait. The final manual set therefore includes
only those n-grams that meet the Level-II cri-
teria with p &lt; .001, meet the Level-III crite-
ria, and also correlate significantly (p &lt; .05)
with individual personality trait scores.
V Finally, it is possible to allow the n-gram fea-
ture set to be selected automatically during
training. The set to be selected from is the
broadest of the manually filtered sets, those
n-grams that meet the Level-II criteria. The
approach adopted is to use the defaults within
the WEKA toolkit: Best First search with the
CfsSubsetEval evaluator (Witten and Frank,
1999).
Thus, a key question is when—if ever—a ‘man-
ual’ feature selection policy outperforms the auto-
matic selection carried out under Level-V. Levels-
II and -III are of particular interest, since they con-
tain features derived from a subset of the corpus.
Since different sub-groups are considered for each
personality trait, the feature sets which meet the
increasingly stringent criteria vary in size. Table 3
contains the size of each of the four manually-
determined feature sets for each of the four per-
sonality traits. Note again that the number of n-
grams selected from the most frequent in the cor-
</bodyText>
<footnote confidence="0.847418">
1Conservatively rounded down in the case of an odd num-
ber of subjects.
</footnote>
<table confidence="0.9971418">
I II III IV V
N 747 747 169 22 19
E 701 701 167 11 20
A 823 823 237 36 34
C 704 704 197 22 25
</table>
<tableCaption confidence="0.9884">
Table 3: Number of n-grams per set.
</tableCaption>
<table confidence="0.99794505882353">
Low High
N [was that] [this year]
[NP1 &lt;p&gt; NP1] [to eat]
[&lt;p&gt; after] [slowly &lt;p&gt;]
[is that] [and buy]
E [point in] [and he]
[last night &lt;p&gt;] [cool &lt;p&gt;]
[it the] [&lt;p&gt; NP1]
[is to] [to her]
A [thank god] [this is not]
[have any] [&lt;p&gt; it is]
[have to] [&lt;p&gt; after]
[turn up] [not have]
C [a few weeks] [by the way]
[case &lt;p&gt;] [&lt;p&gt; i hope]
[okay &lt;p&gt;] [how i]
[the game] [kind of]
</table>
<tableCaption confidence="0.980604">
Table 4: Examples of significant Low and High
</tableCaption>
<bodyText confidence="0.948554222222222">
n-grams from the Level-IV set.
pus for Level-I matches the size of the set for
Level-II. In addition, the features automatically se-
lected are task-dependent, so the Level-V sets vary
in size; here, the Table shows the number of fea-
tures selected for Task-2.
To illustrate the types of n-grams in the feature
sets, Table 4 contains four of the most significant
n-grams from Level-IV for each personality class.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999904">
For each of the 60 binary classification tasks (1
to 3), the performance of the two approaches was
compared. Naive Bayes outperformed Support
Vector Machines on 41/60, with 14 wins for SVM
and 5 draws. With limited space available, we
therefore discuss only the results for NB, and use
NB for Task-4 to -7. The results for the binary
tasks are displayed in Table 5. Those for the mul-
tiple tasks are displayed in Table 6. Baseline is the
majority classification. The most accurate perfor-
mance of a feature set for each task is highlighted
</bodyText>
<page confidence="0.997154">
631
</page>
<table confidence="0.999899692307692">
Task Base Lv.I Lv.II Lv.III Lv.IV Lv.V
N1 52.0 52.0 92.0 84.0 96.0 92.0
N2 53.2 51.1 63.8 68.1 83.6 85.1
N3 54.9 54.9 60.6 53.5 71.9 83.1
E1 52.2 56.5 91.3 95.7 87.0 100.0
E2 51.1 44.7 74.5 72.3 66.0 93.6
E3 54.9 50.7 53.5 59.2 64.8 85.9
A1 54.2 62.5 100.0 100.0 95.8 100.0
A2 51.2 60.5 81.4 79.1 72.1 97.7
A3 52.1 53.5 60.6 69.0 66.2 93.0
C1 56.0 52.0 100.0 100.0 84.0 92.0
C2 61.2 54.5 77.3 81.8 72.7 93.2
C3 57.7 54.9 63.4 71.8 70.4 84.5
</table>
<tableCaption confidence="0.98612">
Table 5: Naive Bayes performance on binary
</tableCaption>
<bodyText confidence="0.85425975">
tasks. Raw % accuracy for 4 personality dimen-
sions, 3 tasks, and 5 feature selection policies.
in bold while the second most accurate is marked
italic.
</bodyText>
<sectionHeader confidence="0.998514" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99992325">
Let us consider the results as they bear in turn on
the three main questions posed earlier: Can we im-
prove on Argamon et al.’s (2005) performance on
binary classification for the Extraversion and Neu-
roticism dimensions? How accurately can we clas-
sify on the four personality dimensions? And how
does performance on multiple classification com-
pare with that on binary classification?
Before addressing these questions, we note the
relatively good performance of NB compared with
‘vanilla’ SVM on the binary classification tasks.
We also note that automatic selection generally
outperforms ‘manual’ selection; however overfit-
ting is very likely when examining just 71 data
points. Therefore, we do not discuss the Level-V
results further.
</bodyText>
<subsectionHeader confidence="0.999113">
6.1 Extraversion and Neuroticism
</subsectionHeader>
<bodyText confidence="0.9996091">
The first main question relates to the feature sets
chosen, because the main issue is whether word n-
grams can give reasonable results on the Extraver-
sion and Neuroticism classification tasks. Of the
current binary classification tasks, Task-2 is most
closely comparable to Argamon et al.’s. Here, the
best performance for Extraversion was returned
by the ‘manual’ Level-II feature set, closely fol-
lowed by Level-III. The accuracy of 74.5% repre-
sents a 23.4% absolute improvement over baseline
</bodyText>
<table confidence="0.999788823529412">
Task Base Lv.I Lv.II Lv.III Lv.IV Lv.V
N4 49.0 49.0 81.6 65.3 77.6 85.7
N5 64.8 60.6 76.1 67.6 67.6 94.4
N6 35.2 31.0 47.9 46.5 66.2 70.4
N7 33.8 31.0 49.3 38.0 42.3 47.9
E4 51.1 44.7 74.5 59.6 53.2 78.7
E5 67.6 60.6 83.1 67.6 54.9 90.1
E6 33.8 23.9 53.5 46.5 46.5 56.3
E7 33.8 44.7 39.4 29.6 38.0 40.8
A4 53.8 51.9 90.4 78.8 67.3 80.8
A5 66.2 59.2 83.1 84.5 74.6 80.3
A6 39.4 31.0 67.6 60.6 56.3 85.9
A7 39.4 33.8 69.8 60.6 50.7 47.9
C4 51.9 53.8 92.3 65.4 67.3 82.7
C5 64.8 62.0 74.6 69.0 62.0 83.1
C6 38.0 39.4 59.2 59.2 50.7 78.9
C7 38.0 36.6 62.0 45.1 45.1 49.3
</table>
<tableCaption confidence="0.977325">
Table 6: Naive Bayes performance on multiple
</tableCaption>
<bodyText confidence="0.976376066666667">
tasks. Raw % accuracy for 4 personality dimen-
sions, 4 tasks, and 5 feature selection policies.
(45.8% relative improvement; we report relative
improvement over baseline because baseline accu-
racies vary between tasks). The best performance
for Neuroticism was returned by Level-IV. The ac-
curacy of 83.6% represents a 30.4% absolute im-
provement over baseline (57.1% relative improve-
ment).
Argamon et al.’s feature set combined in-
sights from computational stylometrics (Koppel et
al., 2002; Argamon et al., 2003) and systemic-
functional grammar. Their focus on function
words and appraisal-related features was intended
to provide more general and informative features
than the usual n-grams. Now, it is unlikely that
weblogs are easier to categorise than the genres
studied by Argamon et al. So there are instead at
least two reasons for the improvement we report.
First, although we did not use systemic-
functional linguistic features, we did test n-grams
selected according to more or less strict policies.
So, considering the manual policies, it seems that
the Level-IV was the best-performing set for Neu-
roticism. This might be expected, given that
Level-IV potentially overfits, allowing features to
be derived from the full corpus. However, in
spite of this, Level-II pproved best for Extraver-
sion. Secondly, in classifying an individual as high
or low on some dimension, Argamon et al. had
</bodyText>
<page confidence="0.996011">
632
</page>
<bodyText confidence="0.999912714285714">
(for some of their materials) 500 words from that
individual, whereas we had approximately 5000
words. The availability of more words per indi-
vidual is to likely to help greatly in training. Ad-
ditionally, a greater volume of text increases the
chances that a long term ‘property’ such as per-
sonality will emerge
</bodyText>
<subsectionHeader confidence="0.999684">
6.2 Binary classification of all dimensions
</subsectionHeader>
<bodyText confidence="0.999991470588235">
The second question concerns the relative ease
of classifying the different dimensions. Across
each of Task-1 to -3, we find that classification
accuracies for Agreeableness and Conscientious-
ness tend to be higher than those for Extraver-
sion and Neuroticism. In all but two cases, the
automatically generated feature set (V) performs
best. Putting this to one side, of the manually
constructed sets, the unrestricted set (I) performs
worst, often below the baseline, while Level-IV is
the best for classifying each task of Neuroticism.
Overall, II and III are better than IV, although the
difference is not large.
As tasks increase in difficulty—as high and low
groups become closer together, and the left-out
middle shrinks—performance drops. But accu-
racy is still respectable.
</bodyText>
<subsectionHeader confidence="0.999222">
6.3 Beyond binary classification
</subsectionHeader>
<bodyText confidence="0.999811642857143">
The final question is about how classification ac-
curacy suffers as the classification task becomes
more subtle. As expected, we find that as we add
more categories, the tasks are harder: compare the
results in the Tables for Task-1, -5 and -7. And,
as with the binary tasks, if fewer mid-scoring in-
dividuals are left out, the task is typically harder:
compare results for Task-4 and 5. It does seem that
some personality dimensions respond to task dif-
ficulty more robustly than others. For instance, on
the hardest task, the best Extraversion classifica-
tion accuracy is 10.9% absolute over the baseline
(32.2% relative), while the best Agreeableness ac-
curacy is 30.4% absolute over the baseline (77.2%
relative). It is notable that the feature set which
return the best results—bar the automatic set V—
tends to be Level-II, excepting for Neuroticism on
Task-6, where Level-IV considerably outperforms
the other sets.
A supplementary question is how the best clas-
sifiers compare with human performance on this
task. Mishne (2005) reports that, for general
mood classification on weblogs, the accuracy of
his automatic classifier is comparable to human
performance. There are also general results on
human personality classification performance in
computer-mediated communication, which sug-
gest that at least some dimensions can be ac-
curately judged even when computer-mediated.
Vazire and Gosling (2004) report that for personal
websites, relative accuracy ofjudgment was, in de-
scending order: Openness &gt; Extraversion &gt; Neu-
roticism &gt; Agreeableness &gt; Conscientiousness.
Similarly, Gill et al. (2006) report that for personal
e-mail, Extraversion is more accurately judged
than Neuroticism. The current study does not have
a set of human judgments to report. For now, it is
interesting to note that the performance profile for
the best classifiers, on the simplest tasks, appears
to diverge from the general human profile, instead
ranking on raw accuracy: Agreeableness &gt; Con-
scientiousness &gt; Neuroticism &gt; Extraversion.
</bodyText>
<sectionHeader confidence="0.984613" genericHeader="conclusions">
7 Conclusion and next steps
</sectionHeader>
<bodyText confidence="0.9999442">
This paper has reported the first stages of our in-
vestigations into classification of author personal-
ity from weblog text. Results are quite promis-
ing, and comparable across all four personality
traits. It seems that even a small selection of fea-
tures found to exhibit an empirical relationship
with personality traits can be used to generate rea-
sonably accurate classification results. Naturally,
there are still many paths to explore. Simple re-
gression analyses are reported in Nowson (2006);
however, for classification, a more thorough com-
parison of different machine learning methodolo-
gies is required. A richer set of features besides
n-grams should be checked, and we should not ig-
nore the potential effectiveness of unigrams in this
task (Pang et al., 2002). A completely new test
set can be gathered, so as to further guard against
overfitting, and to explore systematically the ef-
fects of the amount of training data available for
each author. And as just discussed, comparison
with human personality classification accuracy is
potentially very interesting.
However, it does seem that we are making
progress towards being able to deal with a real-
istic task: if we spot a thumbs-up review in a we-
blog, we should be able to check other text in that
weblog, and tell whose thumb it is; or more accu-
rately, what kind of person’s thumb it is, anyway.
And that in turn should help tell us how high the
thumb is really being held.
</bodyText>
<page confidence="0.998989">
633
</page>
<sectionHeader confidence="0.997158" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99988725">
We are grateful for the helpful advice of Mirella
Lapata, and our three anonymous reviewers. The
second author was supported by a studentship
from the Economic and Social Research Council.
</bodyText>
<sectionHeader confidence="0.998445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999729757575758">
Shlomo Argamon, Marin Saric, and Sterling S. Stein.
2003. Style mining of electronic messages for mul-
tiple authorship discrimination: first results. In Pro-
ceedings of SIGKDD, pages 475–480.
Shlomo Argamon, Sushant Dhawle, Moshe Koppel,
and James W. Pennebaker. 2005. Lexical predic-
tors of personality type. In Proceedings of the 2005
Joint Annual Meeting of the Interface and the Clas-
sification Society ofNorth America.
Satanjeev Banerjee and Ted Pedersen. 2003. The de-
sign, implementation, and use of the ngram statistics
package. In Proceedings of the Fourth International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 370–381, Mexico City.
Tom Buchanan. 2001. Online implementation of an
IPIP five factor personality inventory [web page].
http://users.wmin.ac.uk/∼buchant/wwwffi/
introduction.html [Accessed 25/10/05].
Paul T. Costa and Robert R. McCrae, 1992. Re-
vised NEO Personality Inventory (NEO-PI-R) and
NEO Five-Factor Inventory (NEO-FFI): Profes-
sional Manual. Odessa, FL: Psychological Assess-
ment Resources.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of the 12th International Conference on
World Wide Web, pages 519–528. ACM Press.
Jean-Marc Dewaele and Adrian Furnham. 1999. Ex-
traversion: The unloved variable in applied linguis-
tic research. Language Learning, 49:509–544.
Alastair J. Gill, Jon Oberlander, and Elizabeth Austin.
2006. Rating e-mail personality at zero acquain-
tance. Personality and Individual Differences,
40:497–507.
Moshe Koppel, Shlomo Argamon, and Arat Shimoni.
2002. Automatically categorizing written texts by
author gender. Literary and Linguistic Computing,
17(4):401–412.
Hugo Liu, Henry Lieberman, and Ted Selker. 2003.
A model of textual affect sensing using real-world
knowledge. In Proceedings of the 7th International
Conference on Intelligent User Interfaces.
Gerald Matthews, Ian J. Deary, and Martha C. White-
man. 2003. Personality Traits. Cambridge Univer-
sity Press, Cambridge, 2nd edition.
Gilad Mishne. 2005. Experiments with mood classifi-
cation in blog posts. In Proceedings ofACM SIGIR
2005 Workshop on Stylistic Analysis of Text for In-
formation Access.
Scott Nowson. 2006. The Language of Weblogs: A
study ofgenre and individual differences. Ph.D. the-
sis, University of Edinburgh.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting of the ACL, pages 115–124.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79–86.
James W. Pennebaker and Laura King. 1999. Lin-
guistic styles: Language use as an individual differ-
ence. Journal ofPersonality and Social Psychology,
77:1296–1312.
James W. Pennebaker, Martha E. Francis, and Roger J.
Booth. 2001. Linguistic Inquiry and Word Count
2001. Lawrence Erlbaum Associates, Mahwah, NJ.
Rosalind W. Picard. 1997. Affective Computing. MIT
Press, Cambridge, Ma.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Paul Rayson. 2003. Wmatrix: A statistical method and
software tool for linguistic analysis through corpus
comparison. Ph.D. thesis, Lancaster University.
Ehud Reiter and Somayajulu Sripada. 2004. Contex-
tual influences on near-synonym choice. In Pro-
ceedings of the Third International Conference on
Natural Language Generation.
Klaus Scherer. 1979. Personality markers in speech.
In K. R. Scherer and H. Giles, editors, Social Mark-
ers in Speech, pages 147–209. Cambridge Univer-
sity Press, Cambridge.
James G. Shanahan, Yan Qu, and Janyce Weibe, edi-
tors. 2005. Computing Attitude and Affect in Text.
Springer, Dordrecht, Netherlands.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unspervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the ACL, pages 417–424.
Simine Vazire and Sam D. Gosling. 2004. e-
perceptions: Personality impressions based on per-
sonal websites. Journal of Personality and Social
Psychology, 87:123–132.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann.
</reference>
<page confidence="0.998604">
634
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.445376">
<title confidence="0.9943445">Whose thumb is it anyway? Classifying author personality from weblog text</title>
<author confidence="0.999981">Jon Oberlander</author>
<affiliation confidence="0.900884666666667">School of Informatics University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.982726">Edinburgh, EH8 9LW</address>
<email confidence="0.996737">j.oberlander@ed.ac.uk</email>
<author confidence="0.995472">Scott Nowson</author>
<affiliation confidence="0.900890666666667">School of Informatics University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.97409">Edinburgh, EH8 9LW</address>
<email confidence="0.997773">s.nowson@ed.ac.uk</email>
<abstract confidence="0.996223">We report initial results on the relatively novel task of automatic classification of author personality. Using a corpus of personal weblogs, or ‘blogs’, we investigate the accuracy that can be achieved when classifying authors on four important personality traits. We explore both binary and multiple classification, using differing sets of n-gram features. Results are promising for all four traits examined.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Marin Saric</author>
<author>Sterling S Stein</author>
</authors>
<title>Style mining of electronic messages for multiple authorship discrimination: first results.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGKDD,</booktitle>
<pages>475--480</pages>
<contexts>
<context position="8936" citStr="Argamon et al. (2003)" startWordPosition="1359" endWordPosition="1362">. Without a comparison sample of matched non-bloggers, one cannot say, and Openness is not discussed further in this paper. 628 4 Experiments We are thus confined to classifying on four personality dimensions. However, a number of other variables remain: different learning algorithms can be employed; authors in the corpus can be grouped in several ways, leading to various classification tasks; and more or less restricted linguistic feature sets can be used as input to the classifier. 4.1 Algorithms Support Vector Machines (SVM) appear to work well for binary sentiment classification tasks, so Argamon et al. (2003) and Pang and Lee (2005) consider One-vs-All, or All-vs-All, variants on SVM, to permit multiple classifications. Choice of algorithm is not our focus, but it remains to be seen whether SVM outperforms Naive Bayes (NB) for personality classification. Thus, we will use both on the binary Tasks 1 to 3 (defined in section 4.2.1), for each of the personality dimensions, and each of the manually-selected feature sets (Levels I to IV, defined in section 4.3). Whichever performs better overall is then reported in full, and used for the multiple Tasks 4 to 7 (defined in section 4.2.2). Both approaches</context>
<context position="22871" citStr="Argamon et al., 2003" startWordPosition="3853" endWordPosition="3856"> C6 38.0 39.4 59.2 59.2 50.7 78.9 C7 38.0 36.6 62.0 45.1 45.1 49.3 Table 6: Naive Bayes performance on multiple tasks. Raw % accuracy for 4 personality dimensions, 4 tasks, and 5 feature selection policies. (45.8% relative improvement; we report relative improvement over baseline because baseline accuracies vary between tasks). The best performance for Neuroticism was returned by Level-IV. The accuracy of 83.6% represents a 30.4% absolute improvement over baseline (57.1% relative improvement). Argamon et al.’s feature set combined insights from computational stylometrics (Koppel et al., 2002; Argamon et al., 2003) and systemicfunctional grammar. Their focus on function words and appraisal-related features was intended to provide more general and informative features than the usual n-grams. Now, it is unlikely that weblogs are easier to categorise than the genres studied by Argamon et al. So there are instead at least two reasons for the improvement we report. First, although we did not use systemicfunctional linguistic features, we did test n-grams selected according to more or less strict policies. So, considering the manual policies, it seems that the Level-IV was the best-performing set for Neurotic</context>
</contexts>
<marker>Argamon, Saric, Stein, 2003</marker>
<rawString>Shlomo Argamon, Marin Saric, and Sterling S. Stein. 2003. Style mining of electronic messages for multiple authorship discrimination: first results. In Proceedings of SIGKDD, pages 475–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Sushant Dhawle</author>
<author>Moshe Koppel</author>
<author>James W Pennebaker</author>
</authors>
<title>Lexical predictors of personality type.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Joint Annual Meeting of the Interface and the Classification Society ofNorth America.</booktitle>
<contexts>
<context position="5123" citStr="Argamon et al. (2005)" startWordPosition="770" endWordPosition="773">rlier work of Gottschalk and Gleser, Pennebaker and colleagues secured significant results using the Linguistic Inquiry and Word Count text analysis program (Pennebaker et al., 2001). This primarily counts relative frequencies of word-stems in pre-defined semantic and syntactic categories. It shows, for instance, that high Neuroticism scorers use: more first person singular and negative emotion words; and fewer articles and positive emotion words (Pennebaker and King, 1999). So, can a text classifier trained on such features predict the author personality? We know of only one published study: Argamon et al. (2005) focussed on Extraversion and Neuroticism, dividing Pennebaker and King’s (1999) population into the top- and bottom-third scorers on a dimension, and discarding the middle third. For both dimensions, using a restricted feature set, they report binary classification accuracy of around 58%: an 8% absolute improvement over their baseline. Although mood is more malleable, work on it is also relevant (Mishne, 2005). Using a more typical feature set (including n-grams of words and parts-of-speech), the best mood classification accuracy was 66%, for ‘confused’. At a coarser grain, moods could be cla</context>
<context position="12033" citStr="Argamon et al. (2005)" startWordPosition="1907" endWordPosition="1910"> task (but not the easiest) arises from dividing the corpus in half about the mean score. This creates high (HHH) and low (LLL) groups, covering the entire population. Inevitably, some HHH scorers will actually have scores much closer to those of LLL scorers than to other HHH scorers. These sub-groups are tabulated in Table 1, giving the size of each group within each trait. Note that in Task-2, the standard-deviation-based divisions contain very nearly the top third and bottom third of the population for each dimension. Hence, Task-2 is closest in proportion to the division by thirds used in Argamon et al. (2005). Lowest ... Highest 1 L – H 2 LL – HH 3 LLL HHH N1 12 – 13 N2 25 – 22 N3 39 32 E1 11 – 12 E2 23 – 24 E3 32 39 A1 11 – 13 A2 22 – 21 A3 34 37 C1 11 – 14 C2 17 – 27 C3 30 41 Table 1: Binary task groups: division method and author numbers. N = Neuroticism; E = Extraversion; A = Agreeableness; C = Conscientiousness. 629 4.2.2 Multiple classification tasks 4. Takes the greatest distinction between high (H) and low (L) groups from Task-1, and adds a medium group, but attempts to reduce the possibility of inter-group confusion by including only the smaller medium (m) group omitted from Task-2. Not a</context>
</contexts>
<marker>Argamon, Dhawle, Koppel, Pennebaker, 2005</marker>
<rawString>Shlomo Argamon, Sushant Dhawle, Moshe Koppel, and James W. Pennebaker. 2005. Lexical predictors of personality type. In Proceedings of the 2005 Joint Annual Meeting of the Interface and the Classification Society ofNorth America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>The design, implementation, and use of the ngram statistics package.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>370--381</pages>
<location>Mexico City.</location>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. The design, implementation, and use of the ngram statistics package. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, pages 370–381, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Buchanan</author>
</authors>
<title>Online implementation of an IPIP five factor personality inventory [web page]. http://users.wmin.ac.uk/∼buchant/wwwffi/ introduction.html [Accessed 25/10/05].</title>
<date>2001</date>
<contexts>
<context position="6646" citStr="Buchanan, 2001" startWordPosition="1001" endWordPosition="1002">sification task? (2) How accurate can classification be on the other dimensions? (3) How accurate can multiple— three-way or five-way—classification be? 3 The weblog corpus 3.1 Construction A corpus of personal weblog (‘blog’) text has been gathered (Nowson, 2006). Participants were recruited directly via e-mail to suitable candidates, and indirectly by word-of-mouth: many participants wrote about the study in their blogs. Participants were first required to answer sociobiographic and personality questionnaires. The personality instrument has specifically been validated for online completion (Buchanan, 2001). It was derived from the 50-item IPIP implementation of Costa and McCrae’s (1992) revised NEO personality inventory; participants rate themselves on 41- items using a 5-point Likert scale. This provides scores for Neuroticism, Extraversion, Openness, Agreeableness and Conscientiousness. After completing this stage, participants were requested to submit one month’s worth of prior weblog postings. The month was pre-specified so as to reduce the effects of an individual choosing what they considered their ‘best’ or ‘preferred’ month. Raw submissions were marked-up using XML so as to automate ext</context>
</contexts>
<marker>Buchanan, 2001</marker>
<rawString>Tom Buchanan. 2001. Online implementation of an IPIP five factor personality inventory [web page]. http://users.wmin.ac.uk/∼buchant/wwwffi/ introduction.html [Accessed 25/10/05].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul T Costa</author>
<author>Robert R McCrae</author>
</authors>
<title>Assessment Resources.</title>
<date>1992</date>
<booktitle>Revised NEO Personality Inventory (NEO-PI-R) and NEO Five-Factor Inventory (NEO-FFI): Professional Manual.</booktitle>
<publisher>Psychological</publisher>
<location>Odessa, FL:</location>
<contexts>
<context position="4172" citStr="Costa and McCrae, 1992" startWordPosition="630" endWordPosition="633">ersonality dimensions, seven tasks, and five feature selection policies. We discuss the implications of the results, and related work, and end with suggestions for next steps. 627 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 627–634, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Background: traits and language Cattell’s pioneering work led to the isolation of 16 primary personality factors, and later work on secondary factors led to Costa and McCrae’s fivefactor model, closely related to the ‘Big Five’ models emerging from lexical research (Costa and McCrae, 1992). Each factor gives a continuous dimension for personality scoring. These are: Extraversion; Neuroticism; Openness; Agreeableness; and Conscientiousness (Matthews et al., 2003). Work has also investigated whether scores on these dimensions correlate with language use (Scherer, 1979; Dewaele and Furnham, 1999). Building on the earlier work of Gottschalk and Gleser, Pennebaker and colleagues secured significant results using the Linguistic Inquiry and Word Count text analysis program (Pennebaker et al., 2001). This primarily counts relative frequencies of word-stems in pre-defined semantic and s</context>
</contexts>
<marker>Costa, McCrae, 1992</marker>
<rawString>Paul T. Costa and Robert R. McCrae, 1992. Revised NEO Personality Inventory (NEO-PI-R) and NEO Five-Factor Inventory (NEO-FFI): Professional Manual. Odessa, FL: Psychological Assessment Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th International Conference on World Wide Web,</booktitle>
<pages>519--528</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="982" citStr="Dave et al., 2003" startWordPosition="142" endWordPosition="145">l task of automatic classification of author personality. Using a corpus of personal weblogs, or ‘blogs’, we investigate the accuracy that can be achieved when classifying authors on four important personality traits. We explore both binary and multiple classification, using differing sets of n-gram features. Results are promising for all four traits examined. 1 Introduction There is now considerable interest in affective language processing. Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al., 2002; Turney, 2002; Dave et al., 2003; Liu et al., 2003; Pang and Lee, 2005; Shanahan et al., 2005). Discussing affective computing in general, Picard (1997) notes that phenomena vary in duration, ranging from short-lived feelings, through emotions, to moods, and ultimately to long-lived, slowly-changing personality characteristics. Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these. For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative?</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of the 12th International Conference on World Wide Web, pages 519–528. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Marc Dewaele</author>
<author>Adrian Furnham</author>
</authors>
<title>Extraversion: The unloved variable in applied linguistic research.</title>
<date>1999</date>
<booktitle>Language Learning,</booktitle>
<pages>49--509</pages>
<contexts>
<context position="4482" citStr="Dewaele and Furnham, 1999" startWordPosition="672" endWordPosition="675">mputational Linguistics 2 Background: traits and language Cattell’s pioneering work led to the isolation of 16 primary personality factors, and later work on secondary factors led to Costa and McCrae’s fivefactor model, closely related to the ‘Big Five’ models emerging from lexical research (Costa and McCrae, 1992). Each factor gives a continuous dimension for personality scoring. These are: Extraversion; Neuroticism; Openness; Agreeableness; and Conscientiousness (Matthews et al., 2003). Work has also investigated whether scores on these dimensions correlate with language use (Scherer, 1979; Dewaele and Furnham, 1999). Building on the earlier work of Gottschalk and Gleser, Pennebaker and colleagues secured significant results using the Linguistic Inquiry and Word Count text analysis program (Pennebaker et al., 2001). This primarily counts relative frequencies of word-stems in pre-defined semantic and syntactic categories. It shows, for instance, that high Neuroticism scorers use: more first person singular and negative emotion words; and fewer articles and positive emotion words (Pennebaker and King, 1999). So, can a text classifier trained on such features predict the author personality? We know of only o</context>
</contexts>
<marker>Dewaele, Furnham, 1999</marker>
<rawString>Jean-Marc Dewaele and Adrian Furnham. 1999. Extraversion: The unloved variable in applied linguistic research. Language Learning, 49:509–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alastair J Gill</author>
<author>Jon Oberlander</author>
<author>Elizabeth Austin</author>
</authors>
<title>Rating e-mail personality at zero acquaintance. Personality and Individual Differences,</title>
<date>2006</date>
<pages>40--497</pages>
<contexts>
<context position="26525" citStr="Gill et al. (2006)" startWordPosition="4425" endWordPosition="4428">ith human performance on this task. Mishne (2005) reports that, for general mood classification on weblogs, the accuracy of his automatic classifier is comparable to human performance. There are also general results on human personality classification performance in computer-mediated communication, which suggest that at least some dimensions can be accurately judged even when computer-mediated. Vazire and Gosling (2004) report that for personal websites, relative accuracy ofjudgment was, in descending order: Openness &gt; Extraversion &gt; Neuroticism &gt; Agreeableness &gt; Conscientiousness. Similarly, Gill et al. (2006) report that for personal e-mail, Extraversion is more accurately judged than Neuroticism. The current study does not have a set of human judgments to report. For now, it is interesting to note that the performance profile for the best classifiers, on the simplest tasks, appears to diverge from the general human profile, instead ranking on raw accuracy: Agreeableness &gt; Conscientiousness &gt; Neuroticism &gt; Extraversion. 7 Conclusion and next steps This paper has reported the first stages of our investigations into classification of author personality from weblog text. Results are quite promising, </context>
</contexts>
<marker>Gill, Oberlander, Austin, 2006</marker>
<rawString>Alastair J. Gill, Jon Oberlander, and Elizabeth Austin. 2006. Rating e-mail personality at zero acquaintance. Personality and Individual Differences, 40:497–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Shlomo Argamon</author>
<author>Arat Shimoni</author>
</authors>
<title>Automatically categorizing written texts by author gender.</title>
<date>2002</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>17--4</pages>
<contexts>
<context position="22848" citStr="Koppel et al., 2002" startWordPosition="3849" endWordPosition="3852">0 74.6 69.0 62.0 83.1 C6 38.0 39.4 59.2 59.2 50.7 78.9 C7 38.0 36.6 62.0 45.1 45.1 49.3 Table 6: Naive Bayes performance on multiple tasks. Raw % accuracy for 4 personality dimensions, 4 tasks, and 5 feature selection policies. (45.8% relative improvement; we report relative improvement over baseline because baseline accuracies vary between tasks). The best performance for Neuroticism was returned by Level-IV. The accuracy of 83.6% represents a 30.4% absolute improvement over baseline (57.1% relative improvement). Argamon et al.’s feature set combined insights from computational stylometrics (Koppel et al., 2002; Argamon et al., 2003) and systemicfunctional grammar. Their focus on function words and appraisal-related features was intended to provide more general and informative features than the usual n-grams. Now, it is unlikely that weblogs are easier to categorise than the genres studied by Argamon et al. So there are instead at least two reasons for the improvement we report. First, although we did not use systemicfunctional linguistic features, we did test n-grams selected according to more or less strict policies. So, considering the manual policies, it seems that the Level-IV was the best-perf</context>
</contexts>
<marker>Koppel, Argamon, Shimoni, 2002</marker>
<rawString>Moshe Koppel, Shlomo Argamon, and Arat Shimoni. 2002. Automatically categorizing written texts by author gender. Literary and Linguistic Computing, 17(4):401–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Liu</author>
<author>Henry Lieberman</author>
<author>Ted Selker</author>
</authors>
<title>A model of textual affect sensing using real-world knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th International Conference on Intelligent User Interfaces.</booktitle>
<contexts>
<context position="1000" citStr="Liu et al., 2003" startWordPosition="146" endWordPosition="149"> classification of author personality. Using a corpus of personal weblogs, or ‘blogs’, we investigate the accuracy that can be achieved when classifying authors on four important personality traits. We explore both binary and multiple classification, using differing sets of n-gram features. Results are promising for all four traits examined. 1 Introduction There is now considerable interest in affective language processing. Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al., 2002; Turney, 2002; Dave et al., 2003; Liu et al., 2003; Pang and Lee, 2005; Shanahan et al., 2005). Discussing affective computing in general, Picard (1997) notes that phenomena vary in duration, ranging from short-lived feelings, through emotions, to moods, and ultimately to long-lived, slowly-changing personality characteristics. Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these. For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative? However, Pang and</context>
</contexts>
<marker>Liu, Lieberman, Selker, 2003</marker>
<rawString>Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A model of textual affect sensing using real-world knowledge. In Proceedings of the 7th International Conference on Intelligent User Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Matthews</author>
<author>Ian J Deary</author>
<author>Martha C Whiteman</author>
</authors>
<title>Personality Traits.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<note>2nd edition.</note>
<contexts>
<context position="4348" citStr="Matthews et al., 2003" startWordPosition="653" endWordPosition="656">27 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 627–634, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Background: traits and language Cattell’s pioneering work led to the isolation of 16 primary personality factors, and later work on secondary factors led to Costa and McCrae’s fivefactor model, closely related to the ‘Big Five’ models emerging from lexical research (Costa and McCrae, 1992). Each factor gives a continuous dimension for personality scoring. These are: Extraversion; Neuroticism; Openness; Agreeableness; and Conscientiousness (Matthews et al., 2003). Work has also investigated whether scores on these dimensions correlate with language use (Scherer, 1979; Dewaele and Furnham, 1999). Building on the earlier work of Gottschalk and Gleser, Pennebaker and colleagues secured significant results using the Linguistic Inquiry and Word Count text analysis program (Pennebaker et al., 2001). This primarily counts relative frequencies of word-stems in pre-defined semantic and syntactic categories. It shows, for instance, that high Neuroticism scorers use: more first person singular and negative emotion words; and fewer articles and positive emotion w</context>
</contexts>
<marker>Matthews, Deary, Whiteman, 2003</marker>
<rawString>Gerald Matthews, Ian J. Deary, and Martha C. Whiteman. 2003. Personality Traits. Cambridge University Press, Cambridge, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilad Mishne</author>
</authors>
<title>Experiments with mood classification in blog posts.</title>
<date>2005</date>
<booktitle>In Proceedings ofACM SIGIR 2005 Workshop on Stylistic Analysis of Text for Information Access.</booktitle>
<contexts>
<context position="1890" citStr="Mishne (2005)" startWordPosition="284" endWordPosition="285">s. Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these. For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative? However, Pang and Lee (2005) point out that ranking items or comparing reviews will benefit from finer-grained classifications, over multiple ordered classes: is a film review two- or three- or four-star? And at the same time, some work now considers longer-term affective states. For example, Mishne (2005) aims to classify the primary mood of weblog postings; the study encompasses both fine-grained (but non-ordered) multiple classification (frustrated/loved/etc.) and coarse-grained binary classification (active/passive, positive/negative). This paper is about the move to finer-grained multiple classifications; and also about weblogs. But it is also about even more persistent affective states; in particular, it focusses on classifying author personality. We would argue that ongoing work on sentiment analysis or opinion-mining stands to benefit from progress on personalityclassification. The reas</context>
<context position="5537" citStr="Mishne, 2005" startWordPosition="835" endWordPosition="836">s and positive emotion words (Pennebaker and King, 1999). So, can a text classifier trained on such features predict the author personality? We know of only one published study: Argamon et al. (2005) focussed on Extraversion and Neuroticism, dividing Pennebaker and King’s (1999) population into the top- and bottom-third scorers on a dimension, and discarding the middle third. For both dimensions, using a restricted feature set, they report binary classification accuracy of around 58%: an 8% absolute improvement over their baseline. Although mood is more malleable, work on it is also relevant (Mishne, 2005). Using a more typical feature set (including n-grams of words and parts-of-speech), the best mood classification accuracy was 66%, for ‘confused’. At a coarser grain, moods could be classified with accuracies of 57% (active vs. passive), and 60% (positive vs. negative). So, Argamon et al. used a restricted feature set for binary classification on two dimensions: Extraversion and Neuroticism. Given this, we now pursue three questions. (1) Can we improve performance on a similar binary classification task? (2) How accurate can classification be on the other dimensions? (3) How accurate can mult</context>
<context position="25956" citStr="Mishne (2005)" startWordPosition="4347" endWordPosition="4348">lity dimensions respond to task difficulty more robustly than others. For instance, on the hardest task, the best Extraversion classification accuracy is 10.9% absolute over the baseline (32.2% relative), while the best Agreeableness accuracy is 30.4% absolute over the baseline (77.2% relative). It is notable that the feature set which return the best results—bar the automatic set V— tends to be Level-II, excepting for Neuroticism on Task-6, where Level-IV considerably outperforms the other sets. A supplementary question is how the best classifiers compare with human performance on this task. Mishne (2005) reports that, for general mood classification on weblogs, the accuracy of his automatic classifier is comparable to human performance. There are also general results on human personality classification performance in computer-mediated communication, which suggest that at least some dimensions can be accurately judged even when computer-mediated. Vazire and Gosling (2004) report that for personal websites, relative accuracy ofjudgment was, in descending order: Openness &gt; Extraversion &gt; Neuroticism &gt; Agreeableness &gt; Conscientiousness. Similarly, Gill et al. (2006) report that for personal e-mai</context>
</contexts>
<marker>Mishne, 2005</marker>
<rawString>Gilad Mishne. 2005. Experiments with mood classification in blog posts. In Proceedings ofACM SIGIR 2005 Workshop on Stylistic Analysis of Text for Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Nowson</author>
</authors>
<title>The Language of Weblogs: A study ofgenre and individual differences.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="6295" citStr="Nowson, 2006" startWordPosition="952" endWordPosition="953">’. At a coarser grain, moods could be classified with accuracies of 57% (active vs. passive), and 60% (positive vs. negative). So, Argamon et al. used a restricted feature set for binary classification on two dimensions: Extraversion and Neuroticism. Given this, we now pursue three questions. (1) Can we improve performance on a similar binary classification task? (2) How accurate can classification be on the other dimensions? (3) How accurate can multiple— three-way or five-way—classification be? 3 The weblog corpus 3.1 Construction A corpus of personal weblog (‘blog’) text has been gathered (Nowson, 2006). Participants were recruited directly via e-mail to suitable candidates, and indirectly by word-of-mouth: many participants wrote about the study in their blogs. Participants were first required to answer sociobiographic and personality questionnaires. The personality instrument has specifically been validated for online completion (Buchanan, 2001). It was derived from the 50-item IPIP implementation of Costa and McCrae’s (1992) revised NEO personality inventory; participants rate themselves on 41- items using a 5-point Likert scale. This provides scores for Neuroticism, Extraversion, Opennes</context>
<context position="27466" citStr="Nowson (2006)" startWordPosition="4575" endWordPosition="4576">ad ranking on raw accuracy: Agreeableness &gt; Conscientiousness &gt; Neuroticism &gt; Extraversion. 7 Conclusion and next steps This paper has reported the first stages of our investigations into classification of author personality from weblog text. Results are quite promising, and comparable across all four personality traits. It seems that even a small selection of features found to exhibit an empirical relationship with personality traits can be used to generate reasonably accurate classification results. Naturally, there are still many paths to explore. Simple regression analyses are reported in Nowson (2006); however, for classification, a more thorough comparison of different machine learning methodologies is required. A richer set of features besides n-grams should be checked, and we should not ignore the potential effectiveness of unigrams in this task (Pang et al., 2002). A completely new test set can be gathered, so as to further guard against overfitting, and to explore systematically the effects of the amount of training data available for each author. And as just discussed, comparison with human personality classification accuracy is potentially very interesting. However, it does seem tha</context>
</contexts>
<marker>Nowson, 2006</marker>
<rawString>Scott Nowson. 2006. The Language of Weblogs: A study ofgenre and individual differences. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="1020" citStr="Pang and Lee, 2005" startWordPosition="150" endWordPosition="153"> author personality. Using a corpus of personal weblogs, or ‘blogs’, we investigate the accuracy that can be achieved when classifying authors on four important personality traits. We explore both binary and multiple classification, using differing sets of n-gram features. Results are promising for all four traits examined. 1 Introduction There is now considerable interest in affective language processing. Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al., 2002; Turney, 2002; Dave et al., 2003; Liu et al., 2003; Pang and Lee, 2005; Shanahan et al., 2005). Discussing affective computing in general, Picard (1997) notes that phenomena vary in duration, ranging from short-lived feelings, through emotions, to moods, and ultimately to long-lived, slowly-changing personality characteristics. Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these. For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative? However, Pang and Lee (2005) point ou</context>
<context position="2968" citStr="Pang and Lee (2005)" startWordPosition="439" endWordPosition="442"> would argue that ongoing work on sentiment analysis or opinion-mining stands to benefit from progress on personalityclassification. The reason is that people vary in personality, and they vary in how they appraise events—and hence, in how strongly they phrase their praise or condemnation. Reiter and Sripada (2004) suggest that lexical choice may sometimes be determined by a writer’s idiolect—their personal language preferences. We suggest that while idiolect can be a matter of accident or experience, it may also reflect systematic, personality-based differences. This can help explain why, as Pang and Lee (2005) note, one person’s four star review is another’s two-star. To put it more bluntly, if you’re not a very outgoing sort of person, then your thumbs up might be mistaken for someone else’s thumbs down. But how do we distinguish such people? Or, if we spot a thumbs-up review, how can we tell whose thumb it is, anyway? The paper is structured as follows. It introduces trait theories of personality, notes work to date on personality classification, and raises some questions. It then outlines the weblog corpus and the experiments, which compare classification accuracies for four personality dimensio</context>
<context position="8960" citStr="Pang and Lee (2005)" startWordPosition="1364" endWordPosition="1367">ple of matched non-bloggers, one cannot say, and Openness is not discussed further in this paper. 628 4 Experiments We are thus confined to classifying on four personality dimensions. However, a number of other variables remain: different learning algorithms can be employed; authors in the corpus can be grouped in several ways, leading to various classification tasks; and more or less restricted linguistic feature sets can be used as input to the classifier. 4.1 Algorithms Support Vector Machines (SVM) appear to work well for binary sentiment classification tasks, so Argamon et al. (2003) and Pang and Lee (2005) consider One-vs-All, or All-vs-All, variants on SVM, to permit multiple classifications. Choice of algorithm is not our focus, but it remains to be seen whether SVM outperforms Naive Bayes (NB) for personality classification. Thus, we will use both on the binary Tasks 1 to 3 (defined in section 4.2.1), for each of the personality dimensions, and each of the manually-selected feature sets (Levels I to IV, defined in section 4.3). Whichever performs better overall is then reported in full, and used for the multiple Tasks 4 to 7 (defined in section 4.2.2). Both approaches are applied as implemen</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the ACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="949" citStr="Pang et al., 2002" startWordPosition="136" endWordPosition="139">al results on the relatively novel task of automatic classification of author personality. Using a corpus of personal weblogs, or ‘blogs’, we investigate the accuracy that can be achieved when classifying authors on four important personality traits. We explore both binary and multiple classification, using differing sets of n-gram features. Results are promising for all four traits examined. 1 Introduction There is now considerable interest in affective language processing. Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al., 2002; Turney, 2002; Dave et al., 2003; Liu et al., 2003; Pang and Lee, 2005; Shanahan et al., 2005). Discussing affective computing in general, Picard (1997) notes that phenomena vary in duration, ranging from short-lived feelings, through emotions, to moods, and ultimately to long-lived, slowly-changing personality characteristics. Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these. For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a </context>
<context position="27738" citStr="Pang et al., 2002" startWordPosition="4617" endWordPosition="4620">ing, and comparable across all four personality traits. It seems that even a small selection of features found to exhibit an empirical relationship with personality traits can be used to generate reasonably accurate classification results. Naturally, there are still many paths to explore. Simple regression analyses are reported in Nowson (2006); however, for classification, a more thorough comparison of different machine learning methodologies is required. A richer set of features besides n-grams should be checked, and we should not ignore the potential effectiveness of unigrams in this task (Pang et al., 2002). A completely new test set can be gathered, so as to further guard against overfitting, and to explore systematically the effects of the amount of training data available for each author. And as just discussed, comparison with human personality classification accuracy is potentially very interesting. However, it does seem that we are making progress towards being able to deal with a realistic task: if we spot a thumbs-up review in a weblog, we should be able to check other text in that weblog, and tell whose thumb it is; or more accurately, what kind of person’s thumb it is, anyway. And that </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Laura King</author>
</authors>
<title>Linguistic styles: Language use as an individual difference.</title>
<date>1999</date>
<journal>Journal ofPersonality and Social Psychology,</journal>
<pages>77--1296</pages>
<contexts>
<context position="4980" citStr="Pennebaker and King, 1999" startWordPosition="746" endWordPosition="749"> has also investigated whether scores on these dimensions correlate with language use (Scherer, 1979; Dewaele and Furnham, 1999). Building on the earlier work of Gottschalk and Gleser, Pennebaker and colleagues secured significant results using the Linguistic Inquiry and Word Count text analysis program (Pennebaker et al., 2001). This primarily counts relative frequencies of word-stems in pre-defined semantic and syntactic categories. It shows, for instance, that high Neuroticism scorers use: more first person singular and negative emotion words; and fewer articles and positive emotion words (Pennebaker and King, 1999). So, can a text classifier trained on such features predict the author personality? We know of only one published study: Argamon et al. (2005) focussed on Extraversion and Neuroticism, dividing Pennebaker and King’s (1999) population into the top- and bottom-third scorers on a dimension, and discarding the middle third. For both dimensions, using a restricted feature set, they report binary classification accuracy of around 58%: an 8% absolute improvement over their baseline. Although mood is more malleable, work on it is also relevant (Mishne, 2005). Using a more typical feature set (includi</context>
</contexts>
<marker>Pennebaker, King, 1999</marker>
<rawString>James W. Pennebaker and Laura King. 1999. Linguistic styles: Language use as an individual difference. Journal ofPersonality and Social Psychology, 77:1296–1312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Martha E Francis</author>
<author>Roger J Booth</author>
</authors>
<title>Linguistic Inquiry and Word Count 2001. Lawrence Erlbaum Associates,</title>
<date>2001</date>
<location>Mahwah, NJ.</location>
<contexts>
<context position="4684" citStr="Pennebaker et al., 2001" startWordPosition="702" endWordPosition="705">vefactor model, closely related to the ‘Big Five’ models emerging from lexical research (Costa and McCrae, 1992). Each factor gives a continuous dimension for personality scoring. These are: Extraversion; Neuroticism; Openness; Agreeableness; and Conscientiousness (Matthews et al., 2003). Work has also investigated whether scores on these dimensions correlate with language use (Scherer, 1979; Dewaele and Furnham, 1999). Building on the earlier work of Gottschalk and Gleser, Pennebaker and colleagues secured significant results using the Linguistic Inquiry and Word Count text analysis program (Pennebaker et al., 2001). This primarily counts relative frequencies of word-stems in pre-defined semantic and syntactic categories. It shows, for instance, that high Neuroticism scorers use: more first person singular and negative emotion words; and fewer articles and positive emotion words (Pennebaker and King, 1999). So, can a text classifier trained on such features predict the author personality? We know of only one published study: Argamon et al. (2005) focussed on Extraversion and Neuroticism, dividing Pennebaker and King’s (1999) population into the top- and bottom-third scorers on a dimension, and discarding</context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>James W. Pennebaker, Martha E. Francis, and Roger J. Booth. 2001. Linguistic Inquiry and Word Count 2001. Lawrence Erlbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosalind W Picard</author>
</authors>
<title>Affective Computing.</title>
<date>1997</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Ma.</location>
<contexts>
<context position="1102" citStr="Picard (1997)" startWordPosition="163" endWordPosition="164"> accuracy that can be achieved when classifying authors on four important personality traits. We explore both binary and multiple classification, using differing sets of n-gram features. Results are promising for all four traits examined. 1 Introduction There is now considerable interest in affective language processing. Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al., 2002; Turney, 2002; Dave et al., 2003; Liu et al., 2003; Pang and Lee, 2005; Shanahan et al., 2005). Discussing affective computing in general, Picard (1997) notes that phenomena vary in duration, ranging from short-lived feelings, through emotions, to moods, and ultimately to long-lived, slowly-changing personality characteristics. Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these. For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative? However, Pang and Lee (2005) point out that ranking items or comparing reviews will benefit from finer-grained classifi</context>
</contexts>
<marker>Picard, 1997</marker>
<rawString>Rosalind W. Picard. 1997. Affective Computing. MIT Press, Cambridge, Ma.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="14736" citStr="Quinlan, 1993" startWordPosition="2433" endWordPosition="2435">via CLAWS tagging using the WMatrix tool (Rayson, 2003), and replaced with a single marker (NP1); punctuation was collapsed into a single marker (&lt;p&gt;); and additional tags correspond to non-linguistic features of blogs— for instance, &lt;SOP&gt; and &lt;EOP&gt; were used the mark the start and end of individual blogs posts. Word n-gram approaches provide a large feature space with which to work. But in the general interest of computational tractability, it is useful to reduce the size of the feature set. There are many automatic approaches to feature selection, exploiting, for instance, information gain (Quinlan, 1993). However, ‘manual’ methods can offer principled ways of both reducing the size of the set and avoiding overfitting. We therefore explore the effect of different levels of restriction on the feature sets, and compare them with automatic feature selection. The levels of restriction are as follows: I The least restricted feature set consists of the n-grams most commonly occurring within the blog corpus. Therefore, the feature set for each personality dimension is to be drawn from the same pool. The difference lies in the number of features selected: the size of the set will match that of the nex</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rayson</author>
</authors>
<title>Wmatrix: A statistical method and software tool for linguistic analysis through corpus comparison.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Lancaster University.</institution>
<contexts>
<context position="14177" citStr="Rayson, 2003" startWordPosition="2346" endWordPosition="2347">4 24 E7 11 12 24 12 12 A4 11 – 28 – 13 A5 11 47 13 A6 22 28 21 A7 11 11 28 8 13 C4 11 – 27 – 14 C5 11 46 14 C6 17 27 27 C7 11 6 27 13 14 Table 2: 3-way/5-way task groups: division method and author numbers. N = Neuroticism; E = Extraversion; A = Agreeableness; C = Conscientiousness. 4.3 Feature selection There are many possible features that can be used for automatic text classification. These experiments use essentially word-based bi- and trigrams. It should be noted, however, that some generalisations have been made: all proper nouns were identified via CLAWS tagging using the WMatrix tool (Rayson, 2003), and replaced with a single marker (NP1); punctuation was collapsed into a single marker (&lt;p&gt;); and additional tags correspond to non-linguistic features of blogs— for instance, &lt;SOP&gt; and &lt;EOP&gt; were used the mark the start and end of individual blogs posts. Word n-gram approaches provide a large feature space with which to work. But in the general interest of computational tractability, it is useful to reduce the size of the feature set. There are many automatic approaches to feature selection, exploiting, for instance, information gain (Quinlan, 1993). However, ‘manual’ methods can offer pri</context>
<context position="15609" citStr="Rayson, 2003" startWordPosition="2578" endWordPosition="2579">levels of restriction are as follows: I The least restricted feature set consists of the n-grams most commonly occurring within the blog corpus. Therefore, the feature set for each personality dimension is to be drawn from the same pool. The difference lies in the number of features selected: the size of the set will match that of the next level of restriction. II The next set includes only those n-grams which were distinctive for the two extremes of each personality trait. Only features with a corpus frequency &gt;5 are included. This allows accurate log-likelihood G2 statistics to be computed (Rayson, 2003). Distinct collocations are identified via a three way comparison between the H and L groups in Task-1 (see section 4.2.1) and a third, neutral group. This neutral group contains all those individuals who fell in the medium group (M) for all four traits in the study; the resulting group was of comparable size to the H and L groups for each trait. Hence, this approach selects features using only a subset of the corpus. Ngram software was used to identify and count collocations within a sub-corpus (Banerjee 630 and Pedersen, 2003). For each feature found, its frequency and relative frequency are</context>
</contexts>
<marker>Rayson, 2003</marker>
<rawString>Paul Rayson. 2003. Wmatrix: A statistical method and software tool for linguistic analysis through corpus comparison. Ph.D. thesis, Lancaster University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
</authors>
<title>Contextual influences on near-synonym choice.</title>
<date>2004</date>
<booktitle>In Proceedings of the Third International Conference on Natural Language Generation.</booktitle>
<contexts>
<context position="2665" citStr="Reiter and Sripada (2004)" startWordPosition="393" endWordPosition="396">oved/etc.) and coarse-grained binary classification (active/passive, positive/negative). This paper is about the move to finer-grained multiple classifications; and also about weblogs. But it is also about even more persistent affective states; in particular, it focusses on classifying author personality. We would argue that ongoing work on sentiment analysis or opinion-mining stands to benefit from progress on personalityclassification. The reason is that people vary in personality, and they vary in how they appraise events—and hence, in how strongly they phrase their praise or condemnation. Reiter and Sripada (2004) suggest that lexical choice may sometimes be determined by a writer’s idiolect—their personal language preferences. We suggest that while idiolect can be a matter of accident or experience, it may also reflect systematic, personality-based differences. This can help explain why, as Pang and Lee (2005) note, one person’s four star review is another’s two-star. To put it more bluntly, if you’re not a very outgoing sort of person, then your thumbs up might be mistaken for someone else’s thumbs down. But how do we distinguish such people? Or, if we spot a thumbs-up review, how can we tell whose t</context>
</contexts>
<marker>Reiter, Sripada, 2004</marker>
<rawString>Ehud Reiter and Somayajulu Sripada. 2004. Contextual influences on near-synonym choice. In Proceedings of the Third International Conference on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Scherer</author>
</authors>
<title>Personality markers in speech. In</title>
<date>1979</date>
<booktitle>Social Markers in Speech,</booktitle>
<pages>147--209</pages>
<editor>K. R. Scherer and H. Giles, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4454" citStr="Scherer, 1979" startWordPosition="670" endWordPosition="671">ociation for Computational Linguistics 2 Background: traits and language Cattell’s pioneering work led to the isolation of 16 primary personality factors, and later work on secondary factors led to Costa and McCrae’s fivefactor model, closely related to the ‘Big Five’ models emerging from lexical research (Costa and McCrae, 1992). Each factor gives a continuous dimension for personality scoring. These are: Extraversion; Neuroticism; Openness; Agreeableness; and Conscientiousness (Matthews et al., 2003). Work has also investigated whether scores on these dimensions correlate with language use (Scherer, 1979; Dewaele and Furnham, 1999). Building on the earlier work of Gottschalk and Gleser, Pennebaker and colleagues secured significant results using the Linguistic Inquiry and Word Count text analysis program (Pennebaker et al., 2001). This primarily counts relative frequencies of word-stems in pre-defined semantic and syntactic categories. It shows, for instance, that high Neuroticism scorers use: more first person singular and negative emotion words; and fewer articles and positive emotion words (Pennebaker and King, 1999). So, can a text classifier trained on such features predict the author pe</context>
</contexts>
<marker>Scherer, 1979</marker>
<rawString>Klaus Scherer. 1979. Personality markers in speech. In K. R. Scherer and H. Giles, editors, Social Markers in Speech, pages 147–209. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James G Shanahan</author>
</authors>
<date>2005</date>
<booktitle>Computing Attitude and Affect in Text.</booktitle>
<editor>Qu, and Janyce Weibe, editors.</editor>
<publisher>Springer,</publisher>
<location>Yan</location>
<marker>Shanahan, 2005</marker>
<rawString>James G. Shanahan, Yan Qu, and Janyce Weibe, editors. 2005. Computing Attitude and Affect in Text. Springer, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unspervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="963" citStr="Turney, 2002" startWordPosition="140" endWordPosition="141">elatively novel task of automatic classification of author personality. Using a corpus of personal weblogs, or ‘blogs’, we investigate the accuracy that can be achieved when classifying authors on four important personality traits. We explore both binary and multiple classification, using differing sets of n-gram features. Results are promising for all four traits examined. 1 Introduction There is now considerable interest in affective language processing. Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al., 2002; Turney, 2002; Dave et al., 2003; Liu et al., 2003; Pang and Lee, 2005; Shanahan et al., 2005). Discussing affective computing in general, Picard (1997) notes that phenomena vary in duration, ranging from short-lived feelings, through emotions, to moods, and ultimately to long-lived, slowly-changing personality characteristics. Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these. For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review po</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unspervised classification of reviews. In Proceedings of the 40th Annual Meeting of the ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simine Vazire</author>
<author>Sam D Gosling</author>
</authors>
<title>eperceptions: Personality impressions based on personal websites.</title>
<date>2004</date>
<journal>Journal of Personality and Social Psychology,</journal>
<pages>87--123</pages>
<contexts>
<context position="26330" citStr="Vazire and Gosling (2004)" startWordPosition="4397" endWordPosition="4400">bar the automatic set V— tends to be Level-II, excepting for Neuroticism on Task-6, where Level-IV considerably outperforms the other sets. A supplementary question is how the best classifiers compare with human performance on this task. Mishne (2005) reports that, for general mood classification on weblogs, the accuracy of his automatic classifier is comparable to human performance. There are also general results on human personality classification performance in computer-mediated communication, which suggest that at least some dimensions can be accurately judged even when computer-mediated. Vazire and Gosling (2004) report that for personal websites, relative accuracy ofjudgment was, in descending order: Openness &gt; Extraversion &gt; Neuroticism &gt; Agreeableness &gt; Conscientiousness. Similarly, Gill et al. (2006) report that for personal e-mail, Extraversion is more accurately judged than Neuroticism. The current study does not have a set of human judgments to report. For now, it is interesting to note that the performance profile for the best classifiers, on the simplest tasks, appears to diverge from the general human profile, instead ranking on raw accuracy: Agreeableness &gt; Conscientiousness &gt; Neuroticism &gt;</context>
</contexts>
<marker>Vazire, Gosling, 2004</marker>
<rawString>Simine Vazire and Sam D. Gosling. 2004. eperceptions: Personality impressions based on personal websites. Journal of Personality and Social Psychology, 87:123–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="9608" citStr="Witten and Frank, 1999" startWordPosition="1473" endWordPosition="1476">All-vs-All, variants on SVM, to permit multiple classifications. Choice of algorithm is not our focus, but it remains to be seen whether SVM outperforms Naive Bayes (NB) for personality classification. Thus, we will use both on the binary Tasks 1 to 3 (defined in section 4.2.1), for each of the personality dimensions, and each of the manually-selected feature sets (Levels I to IV, defined in section 4.3). Whichever performs better overall is then reported in full, and used for the multiple Tasks 4 to 7 (defined in section 4.2.2). Both approaches are applied as implemented in the WEKA toolkit (Witten and Frank, 1999) and use 10-fold cross validation. 4.2 Tasks For any blog, we have available the scores, on continuous scales, of its author on four personality dimensions. But for the classifier, the task can be made more or less easy, by grouping authors on each of the dimensions. The simplest tasks are, of course, binary: given the sequence of words from a blog, the classifier simply has to decide whether the author is (for instance) high or low in Agreeableness. Binary tasks vary in difficulty, depending on whether authors scoring in the middle of a dimension are left out, or not; and if they are left out</context>
<context position="17659" citStr="Witten and Frank, 1999" startWordPosition="2928" endWordPosition="2931">tion within a personality trait. The final manual set therefore includes only those n-grams that meet the Level-II criteria with p &lt; .001, meet the Level-III criteria, and also correlate significantly (p &lt; .05) with individual personality trait scores. V Finally, it is possible to allow the n-gram feature set to be selected automatically during training. The set to be selected from is the broadest of the manually filtered sets, those n-grams that meet the Level-II criteria. The approach adopted is to use the defaults within the WEKA toolkit: Best First search with the CfsSubsetEval evaluator (Witten and Frank, 1999). Thus, a key question is when—if ever—a ‘manual’ feature selection policy outperforms the automatic selection carried out under Level-V. LevelsII and -III are of particular interest, since they contain features derived from a subset of the corpus. Since different sub-groups are considered for each personality trait, the feature sets which meet the increasingly stringent criteria vary in size. Table 3 contains the size of each of the four manuallydetermined feature sets for each of the four personality traits. Note again that the number of ngrams selected from the most frequent in the cor1Cons</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Ian H. Witten and Eibe Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>