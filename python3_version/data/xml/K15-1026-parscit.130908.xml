<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002764">
<title confidence="0.997537">
Symmetric Pattern Based Word Embeddings
for Improved Word Similarity Prediction
</title>
<author confidence="0.999586">
Roy Schwartz,&apos; Roi Reichart,2 Ari Rappoport&apos;
</author>
<affiliation confidence="0.999363">
&apos;Institute of Computer Science, The Hebrew University
</affiliation>
<address confidence="0.70975">
2Technion, IIT
</address>
<email confidence="0.995019">
{roys02|arir}@cs.huji.ac.il roiri@ie.technion.ac.il
</email>
<sectionHeader confidence="0.997313" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991914285714">
We present a novel word level vector rep-
resentation based on symmetric patterns
(SPs). For this aim we automatically ac-
quire SPs (e.g., “X and Y”) from a large
corpus of plain text, and generate vectors
where each coordinate represents the co-
occurrence in SPs of the represented word
with another word of the vocabulary. Our
representation has three advantages over
existing alternatives: First, being based on
symmetric word relationships, it is highly
suitable for word similarity prediction.
Particularly, on the SimLex999 word simi-
larity dataset, our model achieves a Spear-
man’s ρ score of 0.517, compared to 0.462
of the state-of-the-art word2vec model. In-
terestingly, our model performs exception-
ally well on verbs, outperforming state-
of-the-art baselines by 20.2–41.5%. Sec-
ond, pattern features can be adapted to the
needs of a target NLP application. For ex-
ample, we show that we can easily control
whether the embeddings derived from SPs
deem antonym pairs (e.g. (big,small)) as
similar or dissimilar, an important distinc-
tion for tasks such as word classification
and sentiment analysis. Finally, we show
that a simple combination of the word sim-
ilarity scores generated by our method and
by word2vec results in a superior predic-
tive power over that of each individual
model, scoring as high as 0.563 in Spear-
man’s ρ on SimLex999. This emphasizes
the differences between the signals cap-
tured by each of the models.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999865875">
In the last decade, vector space modeling (VSM)
for word representation (a.k.a word embedding),
has become a key tool in NLP. Most approaches to
word representation follow the distributional hy-
pothesis (Harris, 1954), which states that words
that co-occur in similar contexts are likely to have
similar meanings.
VSMs differ in the way they exploit word co-
occurrence statistics. Earlier works (see (Turney et
al., 2010)) encode this information directly in the
features of the word vector representation. More
Recently, Neural Networks have become promi-
nent in word representation learning (Bengio et
al., 2003; Collobert and Weston, 2008; Collobert
et al., 2011; Mikolov et al., 2013a; Pennington et
al., 2014, inter alia). Most of these models aim
to learn word vectors that maximize a language
model objective, thus capturing the tendencies of
the represented words to co-occur in the training
corpus. VSM approaches have resulted in highly
useful word embeddings, obtaining high quality
results on various semantic tasks (Baroni et al.,
2014).
Interestingly, the impressive results of these
models are achieved despite the shallow linguis-
tic information most of them consider, which is
limited to the tendency of words to co-occur to-
gether in a pre-specified context window. Particu-
larly, very little information is encoded about the
syntactic and semantic relations between the par-
ticipating words, and, instead, a bag-of-words ap-
proach is taken.1
This bag-of-words approach, however, comes
with a cost. As recently shown by Hill et al.
(2014), despite the impressive results VSMs that
take this approach obtain on modeling word as-
sociation, they are much less successful in model-
ing word similarity. Indeed, when evaluating these
VSMs with datasets such as wordsim353 (Finkel-
stein et al., 2001), where the word pair scores re-
</bodyText>
<footnote confidence="0.965582666666667">
1A few recent VSMs go beyond the bag-of-words as-
sumption and consider deeper linguistic information in word
representation. We address this line of work in Section 2.
</footnote>
<page confidence="0.813057">
258
</page>
<note confidence="0.9813615">
Proceedings of the 19th Conference on Computational Language Learning, pages 258–267,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999926347826087">
flect association rather than similarity (and there-
fore the (cup,coffee) pair is scored higher than
the (car,train) pair), the Spearman correlation be-
tween their scores and the human scores often
crosses the 0.7 level. However, when evaluat-
ing with datasets such as SimLex999 (Hill et al.,
2014), where the pair scores reflect similarity, the
correlation of these models with human judgment
is below 0.5 (Section 6).
In order to address the challenge in model-
ing word similarity, we propose an alternative,
pattern-based, approach to word representation. In
previous work patterns were used to represent a
variety of semantic relations, including hyponymy
(Hearst, 1992), meronymy (Berland and Charniak,
1999) and antonymy (Lin et al., 2003). Here, in
order to capture similarity between words, we use
Symmetric patterns (SPs), such as “X and Y” and
“X as well as Y”, where each of the words in the
pair can take either the X or the Y position. Sym-
metric patterns have shown useful for representing
similarity between words in various NLP tasks in-
cluding lexical acquisition (Widdows and Dorow,
2002), word clustering (Davidov and Rappoport,
2006) and classification of words to semantic cat-
egories (Schwartz et al., 2014). However, to the
best of our knowledge, they have not been applied
to vector space word representation.
Our representation is constructed in the follow-
ing way (Section 3). For each word w, we con-
struct a vector v of size V , where V is the size of
the lexicon. Each element in v represents the co-
occurrence in SPs of w with another word in the
lexicon, which results in a sparse word represen-
tation. Unlike most previous works that applied
SPs to NLP tasks, we do not use a hard coded set
of patterns. Instead, we extract a set of SPs from
plain text using an unsupervised algorithm (Davi-
dov and Rappoport, 2006). This substantially re-
duces the human supervision our model requires
and makes it applicable for practically every lan-
guage for which a large corpus of text is available.
Our SP-based word representation is flexible.
Particularly, by exploiting the semantics of the
pattern based features, our representation can be
adapted to fit the specific needs of target NLP ap-
plications. In Section 4 we exemplify this prop-
erty through the ability of our model to con-
trol whether its word representations will deem
antonyms similar or dissimilar. Antonyms are
words that have opposite semantic meanings (e.g.,
(small,big)), yet, due to their tendency to co-occur
in the same context, they are often assigned sim-
ilar vectors by co-occurrence based representa-
tion models (Section 6). Controlling the model
judgment of antonym pairs is highly useful for
NLP tasks: in some tasks, like word classification,
antonym pairs such as (small,big) belong to the
same class (size adjectives), while in other tasks,
like sentiment analysis, identifying the difference
between them is crucial. As discussed in Section
4, we believe that this flexibility holds for various
other pattern types and for other lexical semantic
relations (e.g. hypernymy, the is-a relation, which
holds in word pairs such as (dog,animal)).
We experiment (Section 6) with the SimLex999
dataset (Hill et al., 2014), consisting of 999 pairs
of words annotated by human subjects for similar-
ity. When comparing the correlation between the
similarity scores derived from our learned repre-
sentation and the human scores, our representation
receives a Spearman correlation coefficient score
(p) of 0.517, outperforming six strong baselines,
including the state-of-the-art word2vec (Mikolov
et al., 2013a) embeddings, by 5.5–16.7%. Our
model performs particularly well on the verb por-
tion of SimLex999 (222 verb pairs), achieving a
Spearman score of 0.578 compared to scores of
0.163–0.376 of the baseline models, an astonish-
ing improvement of 20.2–41.5%. Our analysis re-
veals that the antonym adjustment capability of
our model is vital for its success.
We further demonstrate that the word pair
scores produced by our model can be combined
with those of word2vec to get an improved pre-
dictive power for word similarity. The combined
scores result in a Spearman’s p correlation of
0.563, a further 4.6% improvement compared to
our model, and a total of 10.1–21.3% improve-
ment over the baseline models. This suggests that
the models provide complementary information
about word semantics.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.962780375">
Vector Space Models for Lexical Semantics.
Research on vector spaces for word representation
dates back to the early 1970’s (Salton, 1971). In
traditional methods, a vector for each word w is
generated, with each coordinate representing the
co-occurrence of w and another context item of in-
terest – most often a word but possibly also a sen-
tence, a document or other items. The feature rep-
</bodyText>
<page confidence="0.996533">
259
</page>
<bodyText confidence="0.999968064516129">
resentation generated by this basic construction is
sometimes post-processed using techniques such
as Positive Pointwise Mutual Information (PPMI)
normalization and dimensionality reduction. For
recent surveys, see (Turney et al., 2010; Clark,
2012; Erk, 2012).
Most VSM works share two important charac-
teristics. First, they encode co-occurrence statis-
tics from an input corpus directly into the word
vector features. Second, they consider very lit-
tle information on the syntactic and semantic rela-
tions between the represented word and its context
items. Instead, a bag-of-words approach is taken.
Recently, there is a surge of work focusing on
Neural Network (NN) algorithms for word repre-
sentations learning (Bengio et al., 2003; Collobert
and Weston, 2008; Mnih and Hinton, 2009; Col-
lobert et al., 2011; Dhillon et al., 2011; Mikolov
et al., 2013a; Mnih and Kavukcuoglu, 2013; Le-
bret and Collobert, 2014; Pennington et al., 2014).
Like the more traditional models, these works also
take the bag-of-words approach, encoding only
shallow co-occurrence information between lin-
guistic items. However, they encode this informa-
tion into their objective, often a language model,
rather than directly into the features.
Consider, for example, the successful word2vec
model (Mikolov et al., 2013a). Its continuous-bag-
of-words architecture is designed to predict a word
given its past and future context. The resulted ob-
jective function is:
</bodyText>
<equation confidence="0.997092666666667">
T
max log p(wt|wt−c,... , wt−1, wt+1,... , wt+c)
t=1
</equation>
<bodyText confidence="0.9910916">
where T is the number of words in the corpus,
and c is a pre-determined window size. Another
word2vec architecture, skip-gram, aims to predict
the past and future context given a word. Its ob-
jective is:
</bodyText>
<equation confidence="0.999443666666667">
T
max � log p(wt+j|wt)
t=1 −c≤j≤c,j6=0
</equation>
<bodyText confidence="0.999900333333333">
In both cases the objective function relates to the
co-occurrence of words within a context window.
A small number of works went beyond the bag-
of-words assumption, considering deeper relation-
ships between linguistic items. The Strudel sys-
tem (Baroni et al., 2010) represents a word using
the clusters of lexico-syntactic patterns in which
it occurs. Murphy et al. (2012) represented words
through their co-occurrence with other words in
syntactic dependency relations, and then used the
Non-Negative Sparse Embedding (NNSE) method
to reduce the dimension of the resulted represen-
tation. Levy and Goldberg (2014) extended the
skip-gram word2vec model with negative sam-
pling (Mikolov et al., 2013b) by basing the word
co-occurrence window on the dependency parse
tree of the sentence. Bollegala et al. (2015) re-
placed bag-of-words contexts with various pat-
terns (lexical, POS and dependency).
We introduce a symmetric pattern based ap-
proach to word representation which is particu-
larly suitable for capturing word similarity. In ex-
periments we show the superiority of our model
over six models of the above three families: (a)
bag-of-words models that encode co-occurrence
statistics directly in features; (b) NN models that
implement the bag-of-words approach in their ob-
jective; and (c) models that go beyond the bag-of-
words assumption.
Similarity vs. Association Most recent VSM
research does not distinguish between association
and similarity in a principled way, although no-
table exceptions exist. Turney (2012) constructed
two VSMs with the explicit goal of capturing ei-
ther similarity or association. A classifier that
uses the output of these models was able to pre-
dict whether two concepts are associated, sim-
ilar or both. Agirre et al. (2009) partitioned
the wordsim353 dataset into two subsets, one fo-
cused on similarity and the other on association.
They demonstrated the importance of the associ-
ation/similarity distinction by showing that some
VSMs perform relatively well on one subset while
others perform comparatively better on the other.
Recently, Hill et al. (2014) presented the Sim-
Lex999 dataset consisting of 999 word pairs
judged by humans for similarity only. The partic-
ipating words belong to a variety of POS tags and
concreteness levels, arguably providing a more re-
alistic sample of the English lexicon. Using their
dataset the authors show the tendency of VSMs
that take the bag-of-words approach to capture as-
sociation much better than similarity. This obser-
vation motivates our work.
Symmetric Patterns. Patterns (symmetric or
not) were found useful in a variety of NLP
tasks, including identification of word relations
such as hyponymy (Hearst, 1992), meronymy
(Berland and Charniak, 1999) and antonymy (Lin
et al., 2003). Patterns have also been applied to
</bodyText>
<page confidence="0.960947">
260
</page>
<bodyText confidence="0.999743256410256">
tackle sentence level tasks such as identification
of sarcasm (Tsur et al., 2010), sentiment analysis
(Davidov et al., 2010) and authorship attribution
(Schwartz et al., 2013).
Symmetric patterns (SPs) were employed in var-
ious NLP tasks to capture different aspects of word
similarity. Widdows and Dorow (2002) used SPs
for the task of lexical acquisition. Dorow et al.
(2005) and Davidov and Rappoport (2006) used
them to perform unsupervised clustering of words.
Kozareva et al. (2008) used SPs to classify proper
names (e.g., fish names, singer names). Feng et
al. (2013) used SPs to build a connotation lexicon,
and Schwartz et al. (2014) used SPs to perform
minimally supervised classification of words into
semantic categories.
While some of these works used a hand crafted
set of SPs (Widdows and Dorow, 2002; Dorow et
al., 2005; Kozareva et al., 2008; Feng et al., 2013),
Davidov and Rappoport (2006) introduced a fully
unsupervised algorithm for the extraction of SPs.
Here we apply their algorithm in order to reduce
the required human supervision and demonstrate
the language independence of our approach.
Antonyms. A useful property of our model is
its ability to control the representation of antonym
pairs. Outside the VSM literature several works
identified antonyms using word co-occurrence
statistics, manually and automatically induced pat-
terns, the WordNet lexicon and thesauri (Lin et al.,
2003; Turney, 2008; Wang et al., 2010; Moham-
mad et al., 2013; Schulte im Walde and Koper,
2013; Roth and Schulte im Walde, 2014). Re-
cently, Yih et al. (2012), Chang et al. (2013)
and Ono et al. (2015) proposed word represen-
tation methods that assign dissimilar vectors to
antonyms. Unlike our unsupervised model, which
uses plain text only, these works used the WordNet
lexicon and a thesaurus.
</bodyText>
<sectionHeader confidence="0.992674" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999980166666667">
In this section we describe our approach for gener-
ating pattern-based word embeddings. We start by
describing symmetric patterns (SPs), continue to
show how SPs can be acquired automatically from
text, and, finally, explain how these SPs are used
for word embedding construction.
</bodyText>
<subsectionHeader confidence="0.996701">
3.1 Symmetric Patterns
</subsectionHeader>
<bodyText confidence="0.9932755">
Lexico-syntactic patterns are sequences of words
and wildcards (Hearst, 1992). Examples of pat-
</bodyText>
<table confidence="0.936192">
Candidate Examples of Instances
“X of Y” “point of view”, “years of age”
“X the Y” “around the world”, “over the past”
“X to Y” “nothing to do”, “like to see”
“X and Y” “men and women”, “oil and gas”
“X in Y” “keep in mind”, “put in place”
“X of the Y” “rest of the world”, “end of the war”
</table>
<tableCaption confidence="0.999547">
Table 1:
</tableCaption>
<bodyText confidence="0.999875069767442">
The six most frequent pattern candidates that contain exactly
two wildcards and 1-3 words in our corpus.
terns include “X such as Y”, “X or Y” and “X is
a Y”. When patterns are instantiated in text, wild-
cards are replaced by words. For example, the pat-
tern “X is a Y”, with the X and Y wildcards, can
be instantiated in phrases like “Guffy is a dog”.
Symmetric patterns are a special type of patterns
that contain exactly two wildcards and that tend
to be instantiated by wildcard pairs such that each
member of the pair can take the X or the Y posi-
tion. For example, the symmetry of the pattern “X
or Y” is exemplified by the semantically plausible
expressions “cats or dogs” and “dogs or cats”.
Previous works have shown that words that co-
occur in SPs are semantically similar (Section 2).
In this work we use symmetric patterns to repre-
sent words. Our hypothesis is that such represen-
tation would reflect word similarity (i.e., that sim-
ilar vectors would represent similar words). Our
experiments show that this is indeed the case.
Symmetric Patterns Extraction. Most works
that used SPs manually constructed a set of such
patterns. The most prominent patterns in these
works are “X and Y” and “X or Y” (Widdows and
Dorow, 2002; Feng et al., 2013). In this work we
follow (Davidov and Rappoport, 2006) and apply
an unsupervised algorithm for the automatic ex-
traction of SPs from plain text.
This algorithm starts by defining an SP template
to be a sequence of 3-5 tokens, consisting of ex-
actly two wildcards, and 1-3 words. It then tra-
verses a corpus, looking for frequent pattern can-
didates that match this template. Table 1 shows the
six most frequent pattern candidates, along with
common instances of these patterns.
The algorithm continues by traversing the pat-
tern candidates and selecting a pattern p if a large
portion of the pairs of words wi, wj that co-occur
in p co-occur both in the (X = wi,Y = wj) form
and in the (X = wj,Y = wi) form. Consider, for
example, the pattern candidate “X and Y”, and the
pair of words “cat”,“dog”. Both pattern instances
</bodyText>
<page confidence="0.987091">
261
</page>
<bodyText confidence="0.999253866666667">
“cat and dog” and “dog and cat” are likely to be
seen in a large corpus. If this property holds for a
large portion2 of the pairs of words that co-occur
in this pattern, it is selected as symmetric. On the
other hand, the pattern candidate “X of Y” is in
fact asymmetric: pairs of words such as “point”,
“view” tend to come only in the (X = “point”,Y
= “view”) form and not the other way around.
The reader is referred to (Davidov and Rappoport,
2006) for a more formal description of this algo-
rithm. The resulting pattern set we use in this pa-
per is “X and Y”, “X or Y”, “X and the Y”, “from
X to Y”, “X or the Y”, “X as well as Y”, “X or a
Y”,“X rather than Y”, “X nor Y”, “X and one Y”,
“either X or Y”.
</bodyText>
<subsectionHeader confidence="0.997956">
3.2 SP-based Word Embeddings
</subsectionHeader>
<bodyText confidence="0.9998045">
In order to generate word embeddings, our model
requires a large corpus C, and a set of SPs P. The
model first computes a symmetric matrix M of
size V x V (where V is the size of the lexicon).
In this matrix, Mi,j is the co-occurrence count of
both wi,wj and wj,wi in all patterns p E P. For
example, if wi,wj co-occur 1 time in p1 and 3
times in p5, while wj,wi co-occur 7 times in p9,
then Mi,j = Mj,i = 1 + 3 + 7 = 11. We then
compute the Positive Pointwise Mutual Informa-
tion (PPMI) of M, denoted by M∗.3 The vector
representation of the word wi (denoted by vi) is
the ith row in M∗.
Smoothing. In order to decrease the sparsity of
our representation, we apply a simple smoothing
technique. For each word wi, Win denotes the top
n vectors with the smallest cosine-distance from
vi. We define the word embedding of wi to be
</bodyText>
<equation confidence="0.915804">
�v′i = vi + α · v
v∈Win
</equation>
<bodyText confidence="0.999886666666666">
where α is a smoothing factor.4 This process re-
duces the sparsity of our vector representation. For
example, when n = 0 (i.e., no smoothing), the
average number of non-zero values per vector is
only 0.3K (where the vector size is —250K). When
n = 250, this number reaches —14K.
</bodyText>
<footnote confidence="0.9702776">
2We use 15% of the pairs of words as a threshold.
3PPMI was shown useful for various co-occurrence mod-
els (Baroni et al., 2014).
4We tune n and α using a development set (Section 5).
Typical values for n and α are 250 and 7, respectively.
</footnote>
<sectionHeader confidence="0.970058" genericHeader="method">
4 Antonym Representation
</sectionHeader>
<bodyText confidence="0.999985857142858">
In this section we show how our model allows us
to adjust the representation of pairs of antonyms to
the needs of a subsequent NLP task. This property
will later be demonstrated to have a substantial im-
pact on performance.
Antonyms are pairs of words with an opposite
meaning (e.g., (tall,short)). As the members of
an antonym pair tend to occur in the same con-
text, their word embeddings are often similar. For
example, in the skip-gram model (Mikolov et al.,
2013a), the score of the (accept,reject) pair is 0.73,
and the score of (long,short) is 0.71. Our SP-based
word embeddings also exhibit a similar behavior.
The question of whether antonyms are simi-
lar or not is not a trivial one. On the one hand,
some NLP tasks might benefit from representing
antonyms as similar. For example, in word classi-
fication tasks, words such as “big” and “small” po-
tentially belong to the same class (size adjectives),
and thus representing them as similar is desired.
On the other hand, antonyms are very dissimilar
by definition. This distinction is crucial in tasks
such as search, where a query such as “tall build-
ings” might be poorly processed if the representa-
tions of “tall” and “short” are similar.
In light of this, we construct our word embed-
dings to be controllable of antonyms. That is, our
model contains an antonym parameter that can be
turned on in order to generate word embeddings
that represent antonyms as dissimilar, and turned
off to represent them as similar.
To implement this mechanism, we follow (Lin
et al., 2003), who showed that two patterns are par-
ticularly indicative of antonymy – “from X to Y”
and “either X or Y” (e.g., “from bottom to top”,
“either high or low”). As it turns out, these two
patterns are also symmetric, and are discovered by
our automatic algorithm. Henceforth, we refer to
these two patterns as antonym patterns.
Based on this observation, we present a variant
of our model, which is designed to assign dissim-
ilar vector representations to antonyms. We de-
fine two new matrices: MSP and MAP, which are
computed similarly to M∗ (see Section 3.2), only
with different SP sets. MSP is computed using
the original set of SPs, excluding the two antonym
patterns, while MAP is computed using the two
antonym patterns only.
Then, we define an antonym-sensitive, co-
</bodyText>
<page confidence="0.94289">
262
</page>
<bodyText confidence="0.405312">
occurrence matrix M+AN to be
</bodyText>
<equation confidence="0.982789">
M+AN = MSP − Q · MAP
</equation>
<bodyText confidence="0.999970333333333">
where Q is a weighting parameter.5 Similarly to
M∗, the antonym-sensitive word representation of
the ith word is the ith row in M+AN.
Discussion. The case of antonyms presented in
this paper is an example of one relation that a
pattern based representation model can control.
This property can be potentially extended to addi-
tional word relations, as long as they can be iden-
tified using patterns. Consider, for example, the
hypernymy relation (is-a, as in the (apple,fruit)
pair). This relation can be accurately identified
using patterns such as “X such as Y” and “X like
Y” (Hearst, 1992). Consequently, it is likely that
a pattern-based model can be adapted to control
its predictions with respect to this relation using
a method similar to the one we use to control
antonym representation. We consider this a strong
motivation for a deeper investigation of pattern-
based VSMs in future work.
We next turn to empirically evaluate the perfor-
mance of our model in estimating word similarity.
</bodyText>
<sectionHeader confidence="0.997831" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.899499">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999842">
Evaluation Dataset. We experiment with the
SimLex999 dataset (Hill et al., 2014),6 consisting
of 999 pairs of words. Each pair in this dataset
was annotated by roughly 50 human subjects, who
were asked to score the similarity between the pair
members. SimLex999 has several appealing prop-
erties, including its size, part-of-speech diversity,
and diversity in the level of concreteness of the
participating words.
We follow a 10-fold cross-validation experi-
mental protocol. In each fold, we randomly sam-
ple 25% of the SimLex999 word pairs (-250
pairs) and use them as a development set for pa-
rameter tuning. We use the remaining 75% of the
pairs (-750 pairs) as a test set. We report the av-
erage of the results we got in the 10 folds.
Training Corpus. We use an 8G words corpus,
constructed using the word2vec script.7 Through
this script we also apply a pre-processing step
</bodyText>
<footnote confidence="0.94436">
5We tune β using a development set (Section 5). Typical
values are 7 and 10.
6www.cl.cam.ac.uk/˜fh295/simlex.html
7code.google.com/p/word2vec/source/
browse/trunk/demo-train-big-model-v1.sh
</footnote>
<bodyText confidence="0.9987998">
which employs the word2phrase tool (Mikolov
et al., 2013c) to merge common word pairs and
triples to expression tokens. Our corpus consists
of four datasets: (a) The 2012 and 2013 crawled
news articles from the ACL 2014 workshop on sta-
tistical machine translation (Bojar et al., 2014);8
(b) The One Billion Word Benchmark of Chelba
et al. (2013);9 (c) The UMBC corpus (Han et al.,
2013);10 and (d) The September 2014 dump of the
English Wikipedia.11
</bodyText>
<subsectionHeader confidence="0.998964">
5.2 Baselines
</subsectionHeader>
<bodyText confidence="0.99985">
We compare our model against six baselines: one
that encodes bag-of-words co-occurrence statistics
into its features (model 1 below), three NN models
that encode the same type of information into their
objective function (models 2-4), and two mod-
els that go beyond the bag-of-words assumption
(models 5-6). Unless stated otherwise, all models
are trained on our training corpus.
</bodyText>
<listItem confidence="0.808907">
1. BOW. A simple model where each coordi-
</listItem>
<bodyText confidence="0.959711666666667">
nate corresponds to the co-occurrence count of the
represented word with another word in the train-
ing corpus. The resulted features are re-weighted
according to PPMI. The model’s window size pa-
rameter is tuned on the development set.12
2-3. word2vec. The state-of-the-art word2vec
toolkit (Mikolov et al., 2013a)13 offers two
word embedding architectures: continuous-bag-
of-words (CBOW) and skip-gram. We follow the
recommendations of the word2vec script for set-
ting the parameters of both models, and tune the
window size on the development set.14
4. GloVe. GloVe (Pennington et al., 2014)15 is
a global log-bilinear regression model for word
embedding generation, which trains only on the
nonzero elements in a co-occurrence matrix. We
use the parameters suggested by the authors, and
tune the window size on the development set.16
</bodyText>
<footnote confidence="0.842742071428572">
8http://www.statmt.org/wmt14/training-
monolingual-news-crawl/
9http://www.statmt.org/lm-benchmark/
1-billion-word-language-modeling-
benchmark-r13output.tar.gz
10http://ebiquity.umbc.edu/redirect/to/
resource/id/351/UMBC-webbase-corpus
11dumps.wikimedia.org/enwiki/latest/
enwiki-latest-pages-articles.xml.bz2
12The value 2 is almost constantly selected.
13https://code.google.com/p/word2vec/
14Window size 2 is generally selected for both models.
15nlp.stanford.edu/projects/glove/
16Window size 2 is generally selected.
</footnote>
<page confidence="0.996917">
263
</page>
<listItem confidence="0.955679538461538">
5. NNSE. The NNSE model (Murphy et al.,
2012). As no full implementation of this model
is available online, we use the off-the-shelf em-
beddings available at the authors’ website,17 tak-
ing the full document and dependency model with
2500 dimensions. Embeddings were computed us-
ing a dataset about twice as big as our corpus.
6. Dep. The modified, dependency-based, skip-
gram model (Levy and Goldberg, 2014). To gen-
erate dependency links, we use the Stanford POS
Tagger (Toutanova et al., 2003)18 and the MALT
parser (Nivre et al., 2006).19 We follow the pa-
rameters suggested by the authors.
</listItem>
<subsectionHeader confidence="0.979584">
5.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.99996625">
For evaluation we follow the standard VSM litera-
ture: the score assigned to each pair of words by a
model m is the cosine similarity between the vec-
tors induced by m for the participating words. m’s
quality is evaluated by computing the Spearman
correlation coefficient score (p) between the rank-
ing derived from m’s scores and the one derived
from the human scores.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999956294117647">
Main Result. Table 2 presents our results. Our
model outperforms the baselines by a margin of
5.5–16.7% in the Spearman’s correlation coeffi-
cient (p). Note that the capability of our model to
control antonym representation has a substantial
impact, boosting its performance from p = 0.434
when the antonym parameter is turned off to p =
0.517 when it is turned on.
Model Combination. We turn to explore
whether our pattern-based model and our best
baseline, skip-gram, which implements a bag-of-
words approach, can be combined to provide an
improved predictive power.
For each pair of words in the test set, we take a
linear combination of the cosine similarity score
computed using our embeddings and the score
computed using the skip-gram (SG) embeddings:
</bodyText>
<equation confidence="0.98383">
f+(wi,wj) = γ·fSP(wi,wj)+(1−γ)·fSG(wi,wj)
</equation>
<bodyText confidence="0.999230666666667">
In this equation f&lt;m&gt;(wi, wj) is the cosine
similarity between the vector representations of
words wi and wj according to model m, and γ is a
</bodyText>
<footnote confidence="0.999632">
17http://www.cs.cmu.edu/˜bmurphy/NNSE/
18nlp.stanford.edu/software/
19http://www.maltparser.org/index.html
</footnote>
<table confidence="0.999042545454546">
Model Spearman’s ρ
GloVe 0.35
BOW 0.423
CBOW 0.43
Dep 0.436
NNSE 0.455
skip-gram 0.462
SP(−) 0.434
SP(+) 0.517
Joint (SP(+), skip-gram) 0.563
Average Human Score 0.651
</table>
<tableCaption confidence="0.999218">
Table 2:
</tableCaption>
<figureCaption confidence="0.829131571428571">
Spearman’s ρ scores of our SP-based model with the antonym
parameter turned on (SP(+)) or off (SP(−)) and of the base-
lines described in Section 5.2. Joint (SP(+), skip-gram) is
an interpolation of the scores produced by skip-gram and our
SP(+) model. Average Human Score is the average correla-
tion of a single annotator with the average score of all anno-
tators, taken from (Hill et al., 2014).
</figureCaption>
<bodyText confidence="0.995785294117647">
weighting parameter tuned on the development set
(a common value is 0.8).
As shown in Table 2, this combination forms the
top performing model on SimLex999, achieving a
Spearman’s p score of 0.563. This score is 4.6%
higher than the score of our model, and a 10.1–
21.3% improvement compared to the baselines.
wordsim353 Experiments. The wordsim353
dataset (Finkelstein et al., 2001) is frequently used
for evaluating word representations. In order to
be compatible with previous work, we experiment
with this dataset as well. As our word embeddings
are designed to support word similarity rather than
relatedness, we focus on the similarity subset of
this dataset, according to the division presented in
(Agirre et al., 2009).
As noted by (Hill et al., 2014), the word pair
scores in both subsets of wordsim353 reflect word
association. This is because the two subsets cre-
ated by (Agirre et al., 2009) keep the original
wordsim353 scores, produced by human evalua-
tors that were instructed to score according to as-
sociation rather than similarity. Consequently, we
expect our model to perform worse on this dataset
compared to a dataset, such as SimLex999, whose
annotators were guided to score word pairs ac-
cording to similarity.
Contrary to SimLex999, wordsim353 treats
antonyms as similar. For example, the similarity
score of the (life,death) and (profit,loss) pairs are
7.88 and 7.63 respectively, on a 0-10 scale. Con-
sequently, we turn the antonym parameter off for
this experiment.
Table 3 presents the results. As expected, our
</bodyText>
<page confidence="0.993427">
264
</page>
<table confidence="0.999111222222222">
Model Spearman’s p
GloVe 0.677
Dep 0.712
BOW 0.729
CBOW 0.734
NNSE 0.78
skip-gram 0.792
SP(−) 0.728
Average Human Score 0.756
</table>
<tableCaption confidence="0.997098">
Table 3:
</tableCaption>
<table confidence="0.977753636363636">
Spearman’s p scores for the similarity portion of wordsim353
(Agirre et al., 2009). SP(−) is our model with the antonym
parameter turned off. Other abbreviations are as in Table 2.
Model Adj. Nouns Verbs
GloVe 0.571 0.377 0.163
Dep 0.54 0.449 0.376
BOW 0.548 0.451 0.276
CBOW 0.579 0.48 0.252
NNSE 0.594 0.487 0.318
skip-gram 0.604 0.501 0.307
SP(+) 0.663 0.497 0.578
</table>
<tableCaption confidence="0.73175175">
Table 4:
A POS-based analysis of the various models. Numbers are
the Spearman’s p scores of each model on each of the respec-
tive portions of SimLex999.
</tableCaption>
<bodyText confidence="0.998184357142857">
model is not as successful on a dataset that doesn’t
reflect pure similarity. Yet, it still crosses the p =
0.7 score, a quite high performance level.
Part-of-Speech Analysis. We next perform a
POS-based evaluation of the participating models,
using the three portions of the SimLex999: 666
pairs of nouns, 222 pairs of verbs, and 111 pairs of
adjectives. Table 4 indicates that our SP(+) model
is exceptionally successful in predicting verb and
adjective similarity. On verbs, SP(+) obtains a
score of p = 0.578, a 20.2–41.5% improvement
over the baselines. On adjectives, SP(+) performs
even better (p = 0.663), an improvement of 5.9–
12.3% over the baselines. On nouns, SP(+) is
second only to skip-gram, though with very small
margin (0.497 vs. 0.501), and is outperforming the
other baselines by 1–12%. The lower performance
of our model on nouns might partially explain its
relatively low performance on wordsim353, which
is composed exclusively of nouns.
Analysis of Antonyms. We now turn to a qual-
itative analysis, in order to understand the im-
pact of our modeling decisions on the scores of
antonym word pairs. Table 5 presents examples of
antonym pairs taken from the SimLex999 dataset,
along with their relative ranking among all pairs
in the set, as judged by our model (SP(+) with
Q = 10 or SP(−) with Q = −1) and by the best
</bodyText>
<table confidence="0.999191444444444">
Pair of Words +ANSP skip-gram
-AN
new - old 1 6 6
narrow - wide 1 7 8
necessary - unnecessary 2 2 9
bottom - top 3 8 10
absence - presence 4 7 9
receive - send 1 9 8
fail - succeed 1 8 6
</table>
<tableCaption confidence="0.999821">
Table 5:
</tableCaption>
<bodyText confidence="0.978005613636364">
Examples of antonym pairs and their decile in the similarity
ranking of our SP model with the antonym parameter turned
on (+AN, 3=10) or off (-AN, 3=-1), and of the skip-gram
model, the best baseline. All examples are judged in the low-
est decile (1) by SimLex999’s annotators.
baseline representation (skip-gram). Each pair of
words is assigned a score between 1 and 10 by
each model, where a score of M means that the
pair is ranked at the M’th decile. The examples
in the table are taken from the first (lowest) decile
according to SimLex999’s human evaluators. The
table shows that when the antonym parameter is
off, our model generally recognizes antonyms as
similar. In contrast, when the parameter is on,
ranks of antonyms substantially decrease.
Antonymy as Word Analogy. One of the most
notable features of the skip-gram model is that
some geometric relations between its vectors
translate to semantic relations between the repre-
sented words (Mikolov et al., 2013c), e.g.:
vwoman − vman + vking ≈ vqueen
It is therefore possible that a similar method can
be applied to capture antonymy – a useful property
that our model was demonstrated to have.
To test this hypothesis, we generated a set of
200 analogy questions of the form ”X - Y + Z =
?” where X and Y are antonyms, and Z is a word
with an unknown antonym.20 Example questions
include: “stupid - smart + life = ?” (death) and
“huge - tiny + arrive = ?” (leave). We applied
the standard word analogy evaluation (Mikolov et
al., 2013c) on this dataset with the skip-gram em-
beddings, and found that results are quite poor:
3.5% accuracy (compared to an average 56% ac-
curacy this model obtains on a standard word anal-
ogy dataset (Mikolov et al., 2013a)). Given these
results, the question of whether skip-gram is capa-
20Two human annotators selected a list of potential
antonym pairs from SimLex999 and wordsim353. We took
the intersection of their selections (26 antonym pairs) and
randomly generated 200 analogy questions, each containing
two antonym pairs. The dataset can be found in www.cs.
huji.ac.il/˜roys02/papers/sp—embeddings/
antonymy—analogy—questions.zip
</bodyText>
<page confidence="0.99337">
265
</page>
<bodyText confidence="0.977868">
ble of accounting for antonyms remains open.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999988095238096">
We presented a symmetric pattern based model for
word vector representation. On SimLex999, our
model is superior to six strong baselines, including
the state-of-the-art word2vec skip-gram model by
as much as 5.5–16.7% in Spearman’s ρ score. We
have shown that this gain is largely attributed to
the remarkably high performance of our model on
verbs, where it outperforms all baselines by 20.2–
41.5%. We further demonstrated the adaptabil-
ity of our model to antonym judgment specifica-
tions, and its complementary nature with respect
to word2vec.
In future work we intend to extend our pattern-
based word representation framework beyond
symmetric patterns. As discussed in Section 4,
other types of patterns have the potential to further
improve the expressive power of word vectors. A
particularly interesting challenge is to enhance our
pattern-based approach with bag-of-words infor-
mation, thus enjoying the provable advantages of
both frameworks.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999899857142857">
We would like to thank Elad Eban for his helpful
advice. This research was funded (in part) by the
Intel Collaborative Research Institute for Compu-
tational Intelligence (ICRI-CI) and the Israel Min-
istry of Science and Technology Center of Knowl-
edge in Machine Learning and Artificial Intelli-
gence (Grant number 3-9243).
</bodyText>
<sectionHeader confidence="0.995813" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990542577464789">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In Proc. of
HLT-NAACL.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based seman-
tic model based on properties and types. Cognitive
Science.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proc. of
ACL.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. JMLR.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proc. of ACL.
Ondˇrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Christof Monz, Matt
Post, and Lucia Specia, editors. 2014. Proc. of the
Ninth Workshop on Statistical Machine Translation.
Danushka Bollegala, Takanori Maehara, Yuichi
Yoshida, and Ken ichi Kawarabayashi. 2015.
Learning word representations from relational
graphs. In Proc. of AAAI.
Kai-wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-Relational Latent Semantic Analysis.
In Proc. of EMNLP.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, and Phillipp Koehn. 2013. One
billion word benchmark for measuring progress in
statistical language modeling. CoRR.
Stephen Clark. 2012. Vector space models of lexi-
cal meaning. Handbook of Contemporary Seman-
ticssecond edition, pages 1–42.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Proc. of
ICML.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537.
Dmitry Davidov and Ari Rappoport. 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
In Proc. of ACL-Coling.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proc. of Coling.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Proc. of NIPS.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, and Elisha Moses.
2005. Using Curvature and Markov Clustering in
Graphs for Lexical Acquisition and Word Sense Dis-
crimination.
Katrin Erk. 2012. Vector Space Models of Word
Meaning and Phrase Meaning: A Survey. Language
and Linguistics Compass, 6(10):635–653.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proc. of
ACL.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proc. of WWW.
</reference>
<page confidence="0.977023">
266
</page>
<reference confidence="0.999898567307692">
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Jonathan Weese. 2013.
Umbc ebiquity-core: Semantic textual similarity
systems. In Proc. of *SEM.
Zellig Harris. 1954. Distributional structure. Word.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of Coling
– Volume 2.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with
(genuine) similarity estimation. arXiv:1408.3456
[cs.CL].
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proc. of ACL-
HLT.
R´emi Lebret and Ronan Collobert. 2014. Word em-
beddings through hellinger pca. In Proc. of EACL.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proc. of ACL (Volume
2: Short Papers).
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In Proc. of IJCAI.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. of NIPS.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proc. of NAACL-HLT.
Andriy Mnih and Geoffrey E Hinton. 2009. A scalable
hierarchical distributed language model. In Proc. of
NIPS.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Proc. of NIPS.
Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics, 39(3):555–590.
Brian Murphy, Partha Pratim Talukdar, and Tom
Mitchell. 2012. Learning Effective and In-
terpretable Semantic Models using Non-Negative
Sparse Embedding. In Proc. of Coling.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proc. of LREC.
Masataka Ono, Makoto Miwa, and Yutaka Sasaki.
2015. Word embedding-based antonym detection
using thesauri and distributional information. In
Proc. of NAACL.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proc. of EMNLP.
Michael Roth and Sabine Schulte im Walde. 2014.
Combining Word Patterns and Discourse Markers
for Paradigmatic Relation Classification. In Proc.
ofACL.
Gerard Salton. 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Process-
ing. Prentice-Hall, Inc., Upper Saddle River, NJ,
USA.
Sabine Schulte im Walde and Maximilian Koper. 2013.
Pattern-based distinction of paradigmatic relations
for german nouns, verbs, adjectives. Language Pro-
cessing and Knowledge in the Web, pages 184–198.
Roy Schwartz, Oren Tsur, Ari Rappoport, and Moshe
Koppel. 2013. Authorship attribution of micro-
messages. In Proc. of EMNLP.
Roy Schwartz, Roi Reichart, and Ari Rappoport. 2014.
Minimally supervised classification to semantic cat-
egories using automatically acquired symmetric pat-
terns. In Proc. of Coling.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proc. of NAACL.
Oren Tsur, Dmitry Davidov, and Ari Rappoport.
2010. Icwsm–a great catchy name: Semi-supervised
recognition of sarcastic sentences in online product
reviews. In Proc. of ICWSM.
Peter D. Turney, Patrick Pantel, et al. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Artificial Intelligence research.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
Proc. of Coling.
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Research,
pages 533–585.
Wenbo Wang, Christopher Thomas, Amit Sheth, and
Victor Chan. 2010. Pattern-based synonym and
antonym extraction. In Proc. ofACM.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Proc.
of Coling.
Wen-tau Yih, Geoffrey Zweig, and John C. Platt. 2012.
Polarity inducing latent semantic analysis. In Proc.
of EMNLP-CoNLL.
</reference>
<page confidence="0.997298">
267
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.429000">
<title confidence="0.9879165">Symmetric Pattern Based Word for Improved Word Similarity Prediction</title>
<author confidence="0.578006">of Computer Science</author>
<author confidence="0.578006">The Hebrew</author>
<affiliation confidence="0.620189">IIT</affiliation>
<email confidence="0.977175">roiri@ie.technion.ac.il</email>
<abstract confidence="0.994773333333333">We present a novel word level vector repbased on patterns For this aim we automatically ac- SPs (e.g., “X from a large corpus of plain text, and generate vectors where each coordinate represents the cooccurrence in SPs of the represented word with another word of the vocabulary. Our representation has three advantages over existing alternatives: First, being based on symmetric word relationships, it is highly suitable for word similarity prediction. Particularly, on the SimLex999 word similarity dataset, our model achieves a Spearof 0.517, compared to 0.462 of the state-of-the-art word2vec model. Interestingly, our model performs exceptionally well on verbs, outperforming stateof-the-art baselines by 20.2–41.5%. Second, pattern features can be adapted to the needs of a target NLP application. For example, we show that we can easily control whether the embeddings derived from SPs antonym pairs (e.g. as similar or dissimilar, an important distinction for tasks such as word classification and sentiment analysis. Finally, we show that a simple combination of the word similarity scores generated by our method and by word2vec results in a superior predictive power over that of each individual model, scoring as high as 0.563 in Spear- SimLex999. This emphasizes the differences between the signals captured by each of the models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pasca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="12125" citStr="Agirre et al. (2009)" startWordPosition="1907" endWordPosition="1910">f-words models that encode co-occurrence statistics directly in features; (b) NN models that implement the bag-of-words approach in their objective; and (c) models that go beyond the bag-ofwords assumption. Similarity vs. Association Most recent VSM research does not distinguish between association and similarity in a principled way, although notable exceptions exist. Turney (2012) constructed two VSMs with the explicit goal of capturing either similarity or association. A classifier that uses the output of these models was able to predict whether two concepts are associated, similar or both. Agirre et al. (2009) partitioned the wordsim353 dataset into two subsets, one focused on similarity and the other on association. They demonstrated the importance of the association/similarity distinction by showing that some VSMs perform relatively well on one subset while others perform comparatively better on the other. Recently, Hill et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset</context>
<context position="29885" citStr="Agirre et al., 2009" startWordPosition="4862" endWordPosition="4865">ation forms the top performing model on SimLex999, achieving a Spearman’s p score of 0.563. This score is 4.6% higher than the score of our model, and a 10.1– 21.3% improvement compared to the baselines. wordsim353 Experiments. The wordsim353 dataset (Finkelstein et al., 2001) is frequently used for evaluating word representations. In order to be compatible with previous work, we experiment with this dataset as well. As our word embeddings are designed to support word similarity rather than relatedness, we focus on the similarity subset of this dataset, according to the division presented in (Agirre et al., 2009). As noted by (Hill et al., 2014), the word pair scores in both subsets of wordsim353 reflect word association. This is because the two subsets created by (Agirre et al., 2009) keep the original wordsim353 scores, produced by human evaluators that were instructed to score according to association rather than similarity. Consequently, we expect our model to perform worse on this dataset compared to a dataset, such as SimLex999, whose annotators were guided to score word pairs according to similarity. Contrary to SimLex999, wordsim353 treats antonyms as similar. For example, the similarity score</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Brian Murphy</author>
<author>Eduard Barbu</author>
<author>Massimo Poesio</author>
</authors>
<title>Strudel: A corpus-based semantic model based on properties and types. Cognitive Science.</title>
<date>2010</date>
<contexts>
<context position="10638" citStr="Baroni et al., 2010" startWordPosition="1677" endWordPosition="1680"> past and future context. The resulted objective function is: T max log p(wt|wt−c,... , wt−1, wt+1,... , wt+c) t=1 where T is the number of words in the corpus, and c is a pre-determined window size. Another word2vec architecture, skip-gram, aims to predict the past and future context given a word. Its objective is: T max � log p(wt+j|wt) t=1 −c≤j≤c,j6=0 In both cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (le</context>
</contexts>
<marker>Baroni, Murphy, Barbu, Poesio, 2010</marker>
<rawString>Marco Baroni, Brian Murphy, Eduard Barbu, and Massimo Poesio. 2010. Strudel: A corpus-based semantic model based on properties and types. Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2741" citStr="Baroni et al., 2014" startWordPosition="419" endWordPosition="422">e this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Particularly, very little information is encoded about the syntactic and semantic relations between the participating words, and, instead, a bag-of-words approach is taken.1 This bag-of-words approach, however, comes with a cost. As recently shown by Hill et al. (2014), despite the impressive results VSMs that take this approach obtain on modeling word association,</context>
<context position="19741" citStr="Baroni et al., 2014" startWordPosition="3252" endWordPosition="3255">our representation, we apply a simple smoothing technique. For each word wi, Win denotes the top n vectors with the smallest cosine-distance from vi. We define the word embedding of wi to be �v′i = vi + α · v v∈Win where α is a smoothing factor.4 This process reduces the sparsity of our vector representation. For example, when n = 0 (i.e., no smoothing), the average number of non-zero values per vector is only 0.3K (where the vector size is —250K). When n = 250, this number reaches —14K. 2We use 15% of the pairs of words as a threshold. 3PPMI was shown useful for various co-occurrence models (Baroni et al., 2014). 4We tune n and α using a development set (Section 5). Typical values for n and α are 250 and 7, respectively. 4 Antonym Representation In this section we show how our model allows us to adjust the representation of pairs of antonyms to the needs of a subsequent NLP task. This property will later be demonstrated to have a substantial impact on performance. Antonyms are pairs of words with an opposite meaning (e.g., (tall,short)). As the members of an antonym pair tend to occur in the same context, their word embeddings are often similar. For example, in the skip-gram model (Mikolov et al., 20</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="2305" citStr="Bengio et al., 2003" startWordPosition="351" endWordPosition="354">oduction In the last decade, vector space modeling (VSM) for word representation (a.k.a word embedding), has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tende</context>
<context position="9376" citStr="Bengio et al., 2003" startWordPosition="1475" endWordPosition="1478">intwise Mutual Information (PPMI) normalization and dimensionality reduction. For recent surveys, see (Turney et al., 2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architectu</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4575" citStr="Berland and Charniak, 1999" startWordPosition="700" endWordPosition="703">cored higher than the (car,train) pair), the Spearman correlation between their scores and the human scores often crosses the 0.7 level. However, when evaluating with datasets such as SimLex999 (Hill et al., 2014), where the pair scores reflect similarity, the correlation of these models with human judgment is below 0.5 (Section 6). In order to address the challenge in modeling word similarity, we propose an alternative, pattern-based, approach to word representation. In previous work patterns were used to represent a variety of semantic relations, including hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Here, in order to capture similarity between words, we use Symmetric patterns (SPs), such as “X and Y” and “X as well as Y”, where each of the words in the pair can take either the X or the Y position. Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition (Widdows and Dorow, 2002), word clustering (Davidov and Rappoport, 2006) and classification of words to semantic categories (Schwartz et al., 2014). However, to the best of our knowledge, they have not been applied to vector space wor</context>
<context position="13095" citStr="Berland and Charniak, 1999" startWordPosition="2057" endWordPosition="2060"> the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g.</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<date>2014</date>
<booktitle>Proc. of the Ninth Workshop on Statistical Machine Translation.</booktitle>
<editor>Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, and Lucia Specia, editors.</editor>
<contexts>
<context position="3243" citStr="(2014)" startWordPosition="500" endWordPosition="500">y useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Particularly, very little information is encoded about the syntactic and semantic relations between the participating words, and, instead, a bag-of-words approach is taken.1 This bag-of-words approach, however, comes with a cost. As recently shown by Hill et al. (2014), despite the impressive results VSMs that take this approach obtain on modeling word association, they are much less successful in modeling word similarity. Indeed, when evaluating these VSMs with datasets such as wordsim353 (Finkelstein et al., 2001), where the word pair scores re1A few recent VSMs go beyond the bag-of-words assumption and consider deeper linguistic information in word representation. We address this line of work in Section 2. 258 Proceedings of the 19th Conference on Computational Language Learning, pages 258–267, Beijing, China, July 30-31, 2015. c�2015 Association for Com</context>
<context position="10986" citStr="(2014)" startWordPosition="1731" endWordPosition="1731">th cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN </context>
<context position="12458" citStr="(2014)" startWordPosition="1960" endWordPosition="1960">table exceptions exist. Turney (2012) constructed two VSMs with the explicit goal of capturing either similarity or association. A classifier that uses the output of these models was able to predict whether two concepts are associated, similar or both. Agirre et al. (2009) partitioned the wordsim353 dataset into two subsets, one focused on similarity and the other on association. They demonstrated the importance of the association/similarity distinction by showing that some VSMs perform relatively well on one subset while others perform comparatively better on the other. Recently, Hill et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), </context>
<context position="13810" citStr="(2014)" startWordPosition="2176" endWordPosition="2176">identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM liter</context>
</contexts>
<marker>2014</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, and Lucia Specia, editors. 2014. Proc. of the Ninth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Takanori Maehara</author>
<author>Yuichi Yoshida</author>
<author>Ken ichi Kawarabayashi</author>
</authors>
<title>Learning word representations from relational graphs.</title>
<date>2015</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="11181" citStr="Bollegala et al. (2015)" startWordPosition="1759" endWordPosition="1762">relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN models that implement the bag-of-words approach in their objective; and (c) models that go beyond the bag-ofwords assumption. Similarity vs. Association Most recent VSM research does not distingu</context>
</contexts>
<marker>Bollegala, Maehara, Yoshida, Kawarabayashi, 2015</marker>
<rawString>Danushka Bollegala, Takanori Maehara, Yuichi Yoshida, and Ken ichi Kawarabayashi. 2015. Learning word representations from relational graphs. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-wei Chang</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
</authors>
<title>Multi-Relational Latent Semantic Analysis.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="14756" citStr="Chang et al. (2013)" startWordPosition="2325" endWordPosition="2328"> of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. 3 Model In this section we describe our approach for generating pattern-based word embeddings. We start by describing symmetric patterns (SPs), continue to show how SPs can be acquired automatically from text, and, finally, explain how these SPs are used for word embedding construction. 3.1 Symmetric Patterns Lexico-syntactic patterns are sequences of words and wildcards (Hearst, </context>
</contexts>
<marker>Chang, Yih, Meek, 2013</marker>
<rawString>Kai-wei Chang, Wen-tau Yih, and Christopher Meek. 2013. Multi-Relational Latent Semantic Analysis. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Tomas Mikolov</author>
<author>Mike Schuster</author>
<author>Qi Ge</author>
<author>Thorsten Brants</author>
<author>Phillipp Koehn</author>
</authors>
<title>One billion word benchmark for measuring progress in statistical language modeling.</title>
<date>2013</date>
<publisher>CoRR.</publisher>
<contexts>
<context position="24665" citStr="Chelba et al. (2013)" startWordPosition="4078" endWordPosition="4081"> using the word2vec script.7 Through this script we also apply a pre-processing step 5We tune β using a development set (Section 5). Typical values are 7 and 10. 6www.cl.cam.ac.uk/˜fh295/simlex.html 7code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh which employs the word2phrase tool (Mikolov et al., 2013c) to merge common word pairs and triples to expression tokens. Our corpus consists of four datasets: (a) The 2012 and 2013 crawled news articles from the ACL 2014 workshop on statistical machine translation (Bojar et al., 2014);8 (b) The One Billion Word Benchmark of Chelba et al. (2013);9 (c) The UMBC corpus (Han et al., 2013);10 and (d) The September 2014 dump of the English Wikipedia.11 5.2 Baselines We compare our model against six baselines: one that encodes bag-of-words co-occurrence statistics into its features (model 1 below), three NN models that encode the same type of information into their objective function (models 2-4), and two models that go beyond the bag-of-words assumption (models 5-6). Unless stated otherwise, all models are trained on our training corpus. 1. BOW. A simple model where each coordinate corresponds to the co-occurrence count of the represented</context>
</contexts>
<marker>Chelba, Mikolov, Schuster, Ge, Brants, Koehn, 2013</marker>
<rawString>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. 2013. One billion word benchmark for measuring progress in statistical language modeling. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector space models of lexical meaning. Handbook of Contemporary Semanticssecond edition,</title>
<date>2012</date>
<pages>1--42</pages>
<contexts>
<context position="8892" citStr="Clark, 2012" startWordPosition="1401" endWordPosition="1402">l Semantics. Research on vector spaces for word representation dates back to the early 1970’s (Salton, 1971). In traditional methods, a vector for each word w is generated, with each coordinate representing the co-occurrence of w and another context item of interest – most often a word but possibly also a sentence, a document or other items. The feature rep259 resentation generated by this basic construction is sometimes post-processed using techniques such as Positive Pointwise Mutual Information (PPMI) normalization and dimensionality reduction. For recent surveys, see (Turney et al., 2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2</context>
</contexts>
<marker>Clark, 2012</marker>
<rawString>Stephen Clark. 2012. Vector space models of lexical meaning. Handbook of Contemporary Semanticssecond edition, pages 1–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="2333" citStr="Collobert and Weston, 2008" startWordPosition="355" endWordPosition="358">decade, vector space modeling (VSM) for word representation (a.k.a word embedding), has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur tog</context>
<context position="9404" citStr="Collobert and Weston, 2008" startWordPosition="1479" endWordPosition="1482">ation (PPMI) normalization and dimensionality reduction. For recent surveys, see (Turney et al., 2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<pages>12--2493</pages>
<publisher>JMLR,</publisher>
<contexts>
<context position="2357" citStr="Collobert et al., 2011" startWordPosition="359" endWordPosition="362">g (VSM) for word representation (a.k.a word embedding), has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified</context>
<context position="9451" citStr="Collobert et al., 2011" startWordPosition="1487" endWordPosition="1491">tion. For recent surveys, see (Turney et al., 2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The res</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. JMLR, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. In</title>
<date>2006</date>
<booktitle>Proc. of ACL-Coling.</booktitle>
<contexts>
<context position="5013" citStr="Davidov and Rappoport, 2006" startWordPosition="776" endWordPosition="779">d, approach to word representation. In previous work patterns were used to represent a variety of semantic relations, including hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Here, in order to capture similarity between words, we use Symmetric patterns (SPs), such as “X and Y” and “X as well as Y”, where each of the words in the pair can take either the X or the Y position. Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition (Widdows and Dorow, 2002), word clustering (Davidov and Rappoport, 2006) and classification of words to semantic categories (Schwartz et al., 2014). However, to the best of our knowledge, they have not been applied to vector space word representation. Our representation is constructed in the following way (Section 3). For each word w, we construct a vector v of size V , where V is the size of the lexicon. Each element in v represents the cooccurrence in SPs of w with another word in the lexicon, which results in a sparse word representation. Unlike most previous works that applied SPs to NLP tasks, we do not use a hard coded set of patterns. Instead, we extract a </context>
<context position="13577" citStr="Davidov and Rappoport (2006)" startWordPosition="2134" endWordPosition="2137">seful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm i</context>
<context position="16982" citStr="Davidov and Rappoport, 2006" startWordPosition="2716" endWordPosition="2719"> and “dogs or cats”. Previous works have shown that words that cooccur in SPs are semantically similar (Section 2). In this work we use symmetric patterns to represent words. Our hypothesis is that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. This algorithm starts by defining an SP template to be a sequence of 3-5 tokens, consisting of exactly two wildcards, and 1-3 words. It then traverses a corpus, looking for frequent pattern candidates that match this template. Table 1 shows the six most frequent pattern candidates, along with common instances of these patterns. The algorithm continues by traversing the pattern candidates and selecting a pattern p if a large portion of the pairs of words wi, wj that co-occur in p co-occur both in the (X = </context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2006. Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. In Proc. of ACL-Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proc. of Coling.</booktitle>
<contexts>
<context position="13292" citStr="Davidov et al., 2010" startWordPosition="2089" endWordPosition="2092">tic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic ca</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proc. of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca. In</title>
<date>2011</date>
<booktitle>Proc. of NIPS.</booktitle>
<contexts>
<context position="9473" citStr="Dhillon et al., 2011" startWordPosition="1492" endWordPosition="1495">, see (Turney et al., 2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective functi</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer Dhillon, Dean P Foster, and Lyle H Ungar. 2011. Multi-view learning of word embeddings via cca. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
<author>Dominic Widdows</author>
<author>Katarina Ling</author>
<author>JeanPierre Eckmann</author>
<author>Danilo Sergi</author>
<author>Elisha Moses</author>
</authors>
<title>Using Curvature and Markov Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination.</title>
<date>2005</date>
<contexts>
<context position="13544" citStr="Dorow et al. (2005)" startWordPosition="2129" endWordPosition="2132">ric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs</context>
</contexts>
<marker>Dorow, Widdows, Ling, Eckmann, Sergi, Moses, 2005</marker>
<rawString>Beate Dorow, Dominic Widdows, Katarina Ling, JeanPierre Eckmann, Danilo Sergi, and Elisha Moses. 2005. Using Curvature and Markov Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector Space Models of Word Meaning and Phrase Meaning: A Survey. Language and Linguistics Compass,</title>
<date>2012</date>
<pages>6--10</pages>
<contexts>
<context position="8904" citStr="Erk, 2012" startWordPosition="1403" endWordPosition="1404">Research on vector spaces for word representation dates back to the early 1970’s (Salton, 1971). In traditional methods, a vector for each word w is generated, with each coordinate representing the co-occurrence of w and another context item of interest – most often a word but possibly also a sentence, a document or other items. The feature rep259 resentation generated by this basic construction is sometimes post-processed using techniques such as Positive Pointwise Mutual Information (PPMI) normalization and dimensionality reduction. For recent surveys, see (Turney et al., 2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih a</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector Space Models of Word Meaning and Phrase Meaning: A Survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Jun Seok Kang</author>
<author>Polina Kuznetsova</author>
<author>Yejin Choi</author>
</authors>
<title>Connotation lexicon: A dash of sentiment beneath the surface meaning.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="13742" citStr="Feng et al. (2013)" startWordPosition="2161" endWordPosition="2164">03). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability t</context>
<context position="16928" citStr="Feng et al., 2013" startWordPosition="2707" endWordPosition="2710">tically plausible expressions “cats or dogs” and “dogs or cats”. Previous works have shown that words that cooccur in SPs are semantically similar (Section 2). In this work we use symmetric patterns to represent words. Our hypothesis is that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. This algorithm starts by defining an SP template to be a sequence of 3-5 tokens, consisting of exactly two wildcards, and 1-3 words. It then traverses a corpus, looking for frequent pattern candidates that match this template. Table 1 shows the six most frequent pattern candidates, along with common instances of these patterns. The algorithm continues by traversing the pattern candidates and selecting a pattern p if a large portion of the pairs of word</context>
</contexts>
<marker>Feng, Kang, Kuznetsova, Choi, 2013</marker>
<rawString>Song Feng, Jun Seok Kang, Polina Kuznetsova, and Yejin Choi. 2013. Connotation lexicon: A dash of sentiment beneath the surface meaning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="3495" citStr="Finkelstein et al., 2001" startWordPosition="536" endWordPosition="540">ider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Particularly, very little information is encoded about the syntactic and semantic relations between the participating words, and, instead, a bag-of-words approach is taken.1 This bag-of-words approach, however, comes with a cost. As recently shown by Hill et al. (2014), despite the impressive results VSMs that take this approach obtain on modeling word association, they are much less successful in modeling word similarity. Indeed, when evaluating these VSMs with datasets such as wordsim353 (Finkelstein et al., 2001), where the word pair scores re1A few recent VSMs go beyond the bag-of-words assumption and consider deeper linguistic information in word representation. We address this line of work in Section 2. 258 Proceedings of the 19th Conference on Computational Language Learning, pages 258–267, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics flect association rather than similarity (and therefore the (cup,coffee) pair is scored higher than the (car,train) pair), the Spearman correlation between their scores and the human scores often crosses the 0.7 level. However, w</context>
<context position="29542" citStr="Finkelstein et al., 2001" startWordPosition="4808" endWordPosition="4811">nt (SP(+), skip-gram) is an interpolation of the scores produced by skip-gram and our SP(+) model. Average Human Score is the average correlation of a single annotator with the average score of all annotators, taken from (Hill et al., 2014). weighting parameter tuned on the development set (a common value is 0.8). As shown in Table 2, this combination forms the top performing model on SimLex999, achieving a Spearman’s p score of 0.563. This score is 4.6% higher than the score of our model, and a 10.1– 21.3% improvement compared to the baselines. wordsim353 Experiments. The wordsim353 dataset (Finkelstein et al., 2001) is frequently used for evaluating word representations. In order to be compatible with previous work, we experiment with this dataset as well. As our word embeddings are designed to support word similarity rather than relatedness, we focus on the similarity subset of this dataset, according to the division presented in (Agirre et al., 2009). As noted by (Hill et al., 2014), the word pair scores in both subsets of wordsim353 reflect word association. This is because the two subsets created by (Agirre et al., 2009) keep the original wordsim353 scores, produced by human evaluators that were inst</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay L Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Jonathan Weese</author>
</authors>
<title>Umbc ebiquity-core: Semantic textual similarity systems.</title>
<date>2013</date>
<booktitle>In Proc. of *SEM.</booktitle>
<contexts>
<context position="24706" citStr="Han et al., 2013" startWordPosition="4086" endWordPosition="4089">ript we also apply a pre-processing step 5We tune β using a development set (Section 5). Typical values are 7 and 10. 6www.cl.cam.ac.uk/˜fh295/simlex.html 7code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh which employs the word2phrase tool (Mikolov et al., 2013c) to merge common word pairs and triples to expression tokens. Our corpus consists of four datasets: (a) The 2012 and 2013 crawled news articles from the ACL 2014 workshop on statistical machine translation (Bojar et al., 2014);8 (b) The One Billion Word Benchmark of Chelba et al. (2013);9 (c) The UMBC corpus (Han et al., 2013);10 and (d) The September 2014 dump of the English Wikipedia.11 5.2 Baselines We compare our model against six baselines: one that encodes bag-of-words co-occurrence statistics into its features (model 1 below), three NN models that encode the same type of information into their objective function (models 2-4), and two models that go beyond the bag-of-words assumption (models 5-6). Unless stated otherwise, all models are trained on our training corpus. 1. BOW. A simple model where each coordinate corresponds to the co-occurrence count of the represented word with another word in the training c</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. 2013. Umbc ebiquity-core: Semantic textual similarity systems. In Proc. of *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<note>Distributional structure. Word.</note>
<contexts>
<context position="1911" citStr="Harris, 1954" startWordPosition="292" endWordPosition="293">ks such as word classification and sentiment analysis. Finally, we show that a simple combination of the word similarity scores generated by our method and by word2vec results in a superior predictive power over that of each individual model, scoring as high as 0.563 in Spearman’s ρ on SimLex999. This emphasizes the differences between the signals captured by each of the models. 1 Introduction In the last decade, vector space modeling (VSM) for word representation (a.k.a word embedding), has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, th</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of Coling –</booktitle>
<volume>2</volume>
<contexts>
<context position="4536" citStr="Hearst, 1992" startWordPosition="697" endWordPosition="698">he (cup,coffee) pair is scored higher than the (car,train) pair), the Spearman correlation between their scores and the human scores often crosses the 0.7 level. However, when evaluating with datasets such as SimLex999 (Hill et al., 2014), where the pair scores reflect similarity, the correlation of these models with human judgment is below 0.5 (Section 6). In order to address the challenge in modeling word similarity, we propose an alternative, pattern-based, approach to word representation. In previous work patterns were used to represent a variety of semantic relations, including hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Here, in order to capture similarity between words, we use Symmetric patterns (SPs), such as “X and Y” and “X as well as Y”, where each of the words in the pair can take either the X or the Y position. Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition (Widdows and Dorow, 2002), word clustering (Davidov and Rappoport, 2006) and classification of words to semantic categories (Schwartz et al., 2014). However, to the best of our knowledge, they ha</context>
<context position="13056" citStr="Hearst, 1992" startWordPosition="2054" endWordPosition="2055">l et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) </context>
<context position="15361" citStr="Hearst, 1992" startWordPosition="2419" endWordPosition="2420">. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. 3 Model In this section we describe our approach for generating pattern-based word embeddings. We start by describing symmetric patterns (SPs), continue to show how SPs can be acquired automatically from text, and, finally, explain how these SPs are used for word embedding construction. 3.1 Symmetric Patterns Lexico-syntactic patterns are sequences of words and wildcards (Hearst, 1992). Examples of patCandidate Examples of Instances “X of Y” “point of view”, “years of age” “X the Y” “around the world”, “over the past” “X to Y” “nothing to do”, “like to see” “X and Y” “men and women”, “oil and gas” “X in Y” “keep in mind”, “put in place” “X of the Y” “rest of the world”, “end of the war” Table 1: The six most frequent pattern candidates that contain exactly two wildcards and 1-3 words in our corpus. terns include “X such as Y”, “X or Y” and “X is a Y”. When patterns are instantiated in text, wildcards are replaced by words. For example, the pattern “X is a Y”, with the X and</context>
<context position="22820" citStr="Hearst, 1992" startWordPosition="3786" endWordPosition="3787"> M+AN = MSP − Q · MAP where Q is a weighting parameter.5 Similarly to M∗, the antonym-sensitive word representation of the ith word is the ith row in M+AN. Discussion. The case of antonyms presented in this paper is an example of one relation that a pattern based representation model can control. This property can be potentially extended to additional word relations, as long as they can be identified using patterns. Consider, for example, the hypernymy relation (is-a, as in the (apple,fruit) pair). This relation can be accurately identified using patterns such as “X such as Y” and “X like Y” (Hearst, 1992). Consequently, it is likely that a pattern-based model can be adapted to control its predictions with respect to this relation using a method similar to the one we use to control antonym representation. We consider this a strong motivation for a deeper investigation of patternbased VSMs in future work. We next turn to empirically evaluate the performance of our model in estimating word similarity. 5 Experimental Setup 5.1 Datasets Evaluation Dataset. We experiment with the SimLex999 dataset (Hill et al., 2014),6 consisting of 999 pairs of words. Each pair in this dataset was annotated by roug</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of Coling – Volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<note>arXiv:1408.3456 [cs.CL].</note>
<contexts>
<context position="3243" citStr="Hill et al. (2014)" startWordPosition="497" endWordPosition="500">ted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Particularly, very little information is encoded about the syntactic and semantic relations between the participating words, and, instead, a bag-of-words approach is taken.1 This bag-of-words approach, however, comes with a cost. As recently shown by Hill et al. (2014), despite the impressive results VSMs that take this approach obtain on modeling word association, they are much less successful in modeling word similarity. Indeed, when evaluating these VSMs with datasets such as wordsim353 (Finkelstein et al., 2001), where the word pair scores re1A few recent VSMs go beyond the bag-of-words assumption and consider deeper linguistic information in word representation. We address this line of work in Section 2. 258 Proceedings of the 19th Conference on Computational Language Learning, pages 258–267, Beijing, China, July 30-31, 2015. c�2015 Association for Com</context>
<context position="7068" citStr="Hill et al., 2014" startWordPosition="1117" endWordPosition="1120">ion models (Section 6). Controlling the model judgment of antonym pairs is highly useful for NLP tasks: in some tasks, like word classification, antonym pairs such as (small,big) belong to the same class (size adjectives), while in other tasks, like sentiment analysis, identifying the difference between them is crucial. As discussed in Section 4, we believe that this flexibility holds for various other pattern types and for other lexical semantic relations (e.g. hypernymy, the is-a relation, which holds in word pairs such as (dog,animal)). We experiment (Section 6) with the SimLex999 dataset (Hill et al., 2014), consisting of 999 pairs of words annotated by human subjects for similarity. When comparing the correlation between the similarity scores derived from our learned representation and the human scores, our representation receives a Spearman correlation coefficient score (p) of 0.517, outperforming six strong baselines, including the state-of-the-art word2vec (Mikolov et al., 2013a) embeddings, by 5.5–16.7%. Our model performs particularly well on the verb portion of SimLex999 (222 verb pairs), achieving a Spearman score of 0.578 compared to scores of 0.163–0.376 of the baseline models, an asto</context>
<context position="12458" citStr="Hill et al. (2014)" startWordPosition="1957" endWordPosition="1960"> although notable exceptions exist. Turney (2012) constructed two VSMs with the explicit goal of capturing either similarity or association. A classifier that uses the output of these models was able to predict whether two concepts are associated, similar or both. Agirre et al. (2009) partitioned the wordsim353 dataset into two subsets, one focused on similarity and the other on association. They demonstrated the importance of the association/similarity distinction by showing that some VSMs perform relatively well on one subset while others perform comparatively better on the other. Recently, Hill et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), </context>
<context position="23336" citStr="Hill et al., 2014" startWordPosition="3867" endWordPosition="3870">lation can be accurately identified using patterns such as “X such as Y” and “X like Y” (Hearst, 1992). Consequently, it is likely that a pattern-based model can be adapted to control its predictions with respect to this relation using a method similar to the one we use to control antonym representation. We consider this a strong motivation for a deeper investigation of patternbased VSMs in future work. We next turn to empirically evaluate the performance of our model in estimating word similarity. 5 Experimental Setup 5.1 Datasets Evaluation Dataset. We experiment with the SimLex999 dataset (Hill et al., 2014),6 consisting of 999 pairs of words. Each pair in this dataset was annotated by roughly 50 human subjects, who were asked to score the similarity between the pair members. SimLex999 has several appealing properties, including its size, part-of-speech diversity, and diversity in the level of concreteness of the participating words. We follow a 10-fold cross-validation experimental protocol. In each fold, we randomly sample 25% of the SimLex999 word pairs (-250 pairs) and use them as a development set for parameter tuning. We use the remaining 75% of the pairs (-750 pairs) as a test set. We repo</context>
<context position="29157" citStr="Hill et al., 2014" startWordPosition="4746" endWordPosition="4749">software/ 19http://www.maltparser.org/index.html Model Spearman’s ρ GloVe 0.35 BOW 0.423 CBOW 0.43 Dep 0.436 NNSE 0.455 skip-gram 0.462 SP(−) 0.434 SP(+) 0.517 Joint (SP(+), skip-gram) 0.563 Average Human Score 0.651 Table 2: Spearman’s ρ scores of our SP-based model with the antonym parameter turned on (SP(+)) or off (SP(−)) and of the baselines described in Section 5.2. Joint (SP(+), skip-gram) is an interpolation of the scores produced by skip-gram and our SP(+) model. Average Human Score is the average correlation of a single annotator with the average score of all annotators, taken from (Hill et al., 2014). weighting parameter tuned on the development set (a common value is 0.8). As shown in Table 2, this combination forms the top performing model on SimLex999, achieving a Spearman’s p score of 0.563. This score is 4.6% higher than the score of our model, and a 10.1– 21.3% improvement compared to the baselines. wordsim353 Experiments. The wordsim353 dataset (Finkelstein et al., 2001) is frequently used for evaluating word representations. In order to be compatible with previous work, we experiment with this dataset as well. As our word embeddings are designed to support word similarity rather t</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv:1408.3456 [cs.CL].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proc. of ACLHLT.</booktitle>
<contexts>
<context position="13655" citStr="Kozareva et al. (2008)" startWordPosition="2146" endWordPosition="2149">hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language </context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proc. of ACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R´emi Lebret</author>
<author>Ronan Collobert</author>
</authors>
<title>Word embeddings through hellinger pca.</title>
<date>2014</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="9552" citStr="Lebret and Collobert, 2014" startWordPosition="1504" endWordPosition="1508"> two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective function is: T max log p(wt|wt−c,... , wt−1, wt+1,... , wt+c) t=1 where T is the numb</context>
</contexts>
<marker>Lebret, Collobert, 2014</marker>
<rawString>R´emi Lebret and Ronan Collobert. 2014. Word embeddings through hellinger pca. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<volume>2</volume>
<institution>Short Papers).</institution>
<contexts>
<context position="10986" citStr="Levy and Goldberg (2014)" startWordPosition="1728" endWordPosition="1731"> −c≤j≤c,j6=0 In both cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN </context>
<context position="26963" citStr="Levy and Goldberg, 2014" startWordPosition="4393" endWordPosition="4396">bz2 12The value 2 is almost constantly selected. 13https://code.google.com/p/word2vec/ 14Window size 2 is generally selected for both models. 15nlp.stanford.edu/projects/glove/ 16Window size 2 is generally selected. 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM literature: the score assigned to each pair of words by a model m is the cosine similarity between the vectors induced by m for the participating words. m’s quality is evaluated by computing the Spearman correlation coefficient score (p) between the ranking derived from m’s scores and the one derived from the human scores. 6 Results Main Result. Table 2 pr</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proc. of ACL (Volume 2: Short Papers).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao</author>
<author>Lijuan Qin</author>
<author>Ming Zhou</author>
</authors>
<title>Identifying synonyms among distributionally similar words.</title>
<date>2003</date>
<booktitle>In Proc. of IJCAI.</booktitle>
<contexts>
<context position="4607" citStr="Lin et al., 2003" startWordPosition="706" endWordPosition="709">he Spearman correlation between their scores and the human scores often crosses the 0.7 level. However, when evaluating with datasets such as SimLex999 (Hill et al., 2014), where the pair scores reflect similarity, the correlation of these models with human judgment is below 0.5 (Section 6). In order to address the challenge in modeling word similarity, we propose an alternative, pattern-based, approach to word representation. In previous work patterns were used to represent a variety of semantic relations, including hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Here, in order to capture similarity between words, we use Symmetric patterns (SPs), such as “X and Y” and “X as well as Y”, where each of the words in the pair can take either the X or the Y position. Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition (Widdows and Dorow, 2002), word clustering (Davidov and Rappoport, 2006) and classification of words to semantic categories (Schwartz et al., 2014). However, to the best of our knowledge, they have not been applied to vector space word representation. Our representa</context>
<context position="13127" citStr="Lin et al., 2003" startWordPosition="2063" endWordPosition="2066">ord pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Fen</context>
<context position="14582" citStr="Lin et al., 2003" startWordPosition="2292" endWordPosition="2295">s and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. 3 Model In this section we describe our approach for generating pattern-based word embeddings. We start by describing symmetric patterns (SPs), continue to show how SPs can be acquired automatically from text,</context>
<context position="21406" citStr="Lin et al., 2003" startWordPosition="3540" endWordPosition="3543">thus representing them as similar is desired. On the other hand, antonyms are very dissimilar by definition. This distinction is crucial in tasks such as search, where a query such as “tall buildings” might be poorly processed if the representations of “tall” and “short” are similar. In light of this, we construct our word embeddings to be controllable of antonyms. That is, our model contains an antonym parameter that can be turned on in order to generate word embeddings that represent antonyms as dissimilar, and turned off to represent them as similar. To implement this mechanism, we follow (Lin et al., 2003), who showed that two patterns are particularly indicative of antonymy – “from X to Y” and “either X or Y” (e.g., “from bottom to top”, “either high or low”). As it turns out, these two patterns are also symmetric, and are discovered by our automatic algorithm. Henceforth, we refer to these two patterns as antonym patterns. Based on this observation, we present a variant of our model, which is designed to assign dissimilar vector representations to antonyms. We define two new matrices: MSP and MAP, which are computed similarly to M∗ (see Section 3.2), only with different SP sets. MSP is comput</context>
</contexts>
<marker>Lin, Zhao, Qin, Zhou, 2003</marker>
<rawString>Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally similar words. In Proc. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="2379" citStr="Mikolov et al., 2013" startWordPosition="363" endWordPosition="366">ntation (a.k.a word embedding), has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Parti</context>
<context position="7450" citStr="Mikolov et al., 2013" startWordPosition="1171" endWordPosition="1174">y holds for various other pattern types and for other lexical semantic relations (e.g. hypernymy, the is-a relation, which holds in word pairs such as (dog,animal)). We experiment (Section 6) with the SimLex999 dataset (Hill et al., 2014), consisting of 999 pairs of words annotated by human subjects for similarity. When comparing the correlation between the similarity scores derived from our learned representation and the human scores, our representation receives a Spearman correlation coefficient score (p) of 0.517, outperforming six strong baselines, including the state-of-the-art word2vec (Mikolov et al., 2013a) embeddings, by 5.5–16.7%. Our model performs particularly well on the verb portion of SimLex999 (222 verb pairs), achieving a Spearman score of 0.578 compared to scores of 0.163–0.376 of the baseline models, an astonishing improvement of 20.2–41.5%. Our analysis reveals that the antonym adjustment capability of our model is vital for its success. We further demonstrate that the word pair scores produced by our model can be combined with those of word2vec to get an improved predictive power for word similarity. The combined scores result in a Spearman’s p correlation of 0.563, a further 4.6%</context>
<context position="9495" citStr="Mikolov et al., 2013" startWordPosition="1496" endWordPosition="1499">2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective function is: T max log p(wt|</context>
<context position="11069" citStr="Mikolov et al., 2013" startWordPosition="1741" endWordPosition="1744"> within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN models that implement the bag-of-words approach in their objective; and (c) models </context>
<context position="20343" citStr="Mikolov et al., 2013" startWordPosition="3359" endWordPosition="3362">roni et al., 2014). 4We tune n and α using a development set (Section 5). Typical values for n and α are 250 and 7, respectively. 4 Antonym Representation In this section we show how our model allows us to adjust the representation of pairs of antonyms to the needs of a subsequent NLP task. This property will later be demonstrated to have a substantial impact on performance. Antonyms are pairs of words with an opposite meaning (e.g., (tall,short)). As the members of an antonym pair tend to occur in the same context, their word embeddings are often similar. For example, in the skip-gram model (Mikolov et al., 2013a), the score of the (accept,reject) pair is 0.73, and the score of (long,short) is 0.71. Our SP-based word embeddings also exhibit a similar behavior. The question of whether antonyms are similar or not is not a trivial one. On the one hand, some NLP tasks might benefit from representing antonyms as similar. For example, in word classification tasks, words such as “big” and “small” potentially belong to the same class (size adjectives), and thus representing them as similar is desired. On the other hand, antonyms are very dissimilar by definition. This distinction is crucial in tasks such as </context>
<context position="24376" citStr="Mikolov et al., 2013" startWordPosition="4029" endWordPosition="4032">ample 25% of the SimLex999 word pairs (-250 pairs) and use them as a development set for parameter tuning. We use the remaining 75% of the pairs (-750 pairs) as a test set. We report the average of the results we got in the 10 folds. Training Corpus. We use an 8G words corpus, constructed using the word2vec script.7 Through this script we also apply a pre-processing step 5We tune β using a development set (Section 5). Typical values are 7 and 10. 6www.cl.cam.ac.uk/˜fh295/simlex.html 7code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh which employs the word2phrase tool (Mikolov et al., 2013c) to merge common word pairs and triples to expression tokens. Our corpus consists of four datasets: (a) The 2012 and 2013 crawled news articles from the ACL 2014 workshop on statistical machine translation (Bojar et al., 2014);8 (b) The One Billion Word Benchmark of Chelba et al. (2013);9 (c) The UMBC corpus (Han et al., 2013);10 and (d) The September 2014 dump of the English Wikipedia.11 5.2 Baselines We compare our model against six baselines: one that encodes bag-of-words co-occurrence statistics into its features (model 1 below), three NN models that encode the same type of information i</context>
<context position="33860" citStr="Mikolov et al., 2013" startWordPosition="5545" endWordPosition="5548">e between 1 and 10 by each model, where a score of M means that the pair is ranked at the M’th decile. The examples in the table are taken from the first (lowest) decile according to SimLex999’s human evaluators. The table shows that when the antonym parameter is off, our model generally recognizes antonyms as similar. In contrast, when the parameter is on, ranks of antonyms substantially decrease. Antonymy as Word Analogy. One of the most notable features of the skip-gram model is that some geometric relations between its vectors translate to semantic relations between the represented words (Mikolov et al., 2013c), e.g.: vwoman − vman + vking ≈ vqueen It is therefore possible that a similar method can be applied to capture antonymy – a useful property that our model was demonstrated to have. To test this hypothesis, we generated a set of 200 analogy questions of the form ”X - Y + Z = ?” where X and Y are antonyms, and Z is a word with an unknown antonym.20 Example questions include: “stupid - smart + life = ?” (death) and “huge - tiny + arrive = ?” (leave). We applied the standard word analogy evaluation (Mikolov et al., 2013c) on this dataset with the skip-gram embeddings, and found that results are</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="2379" citStr="Mikolov et al., 2013" startWordPosition="363" endWordPosition="366">ntation (a.k.a word embedding), has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Parti</context>
<context position="7450" citStr="Mikolov et al., 2013" startWordPosition="1171" endWordPosition="1174">y holds for various other pattern types and for other lexical semantic relations (e.g. hypernymy, the is-a relation, which holds in word pairs such as (dog,animal)). We experiment (Section 6) with the SimLex999 dataset (Hill et al., 2014), consisting of 999 pairs of words annotated by human subjects for similarity. When comparing the correlation between the similarity scores derived from our learned representation and the human scores, our representation receives a Spearman correlation coefficient score (p) of 0.517, outperforming six strong baselines, including the state-of-the-art word2vec (Mikolov et al., 2013a) embeddings, by 5.5–16.7%. Our model performs particularly well on the verb portion of SimLex999 (222 verb pairs), achieving a Spearman score of 0.578 compared to scores of 0.163–0.376 of the baseline models, an astonishing improvement of 20.2–41.5%. Our analysis reveals that the antonym adjustment capability of our model is vital for its success. We further demonstrate that the word pair scores produced by our model can be combined with those of word2vec to get an improved predictive power for word similarity. The combined scores result in a Spearman’s p correlation of 0.563, a further 4.6%</context>
<context position="9495" citStr="Mikolov et al., 2013" startWordPosition="1496" endWordPosition="1499">2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective function is: T max log p(wt|</context>
<context position="11069" citStr="Mikolov et al., 2013" startWordPosition="1741" endWordPosition="1744"> within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN models that implement the bag-of-words approach in their objective; and (c) models </context>
<context position="20343" citStr="Mikolov et al., 2013" startWordPosition="3359" endWordPosition="3362">roni et al., 2014). 4We tune n and α using a development set (Section 5). Typical values for n and α are 250 and 7, respectively. 4 Antonym Representation In this section we show how our model allows us to adjust the representation of pairs of antonyms to the needs of a subsequent NLP task. This property will later be demonstrated to have a substantial impact on performance. Antonyms are pairs of words with an opposite meaning (e.g., (tall,short)). As the members of an antonym pair tend to occur in the same context, their word embeddings are often similar. For example, in the skip-gram model (Mikolov et al., 2013a), the score of the (accept,reject) pair is 0.73, and the score of (long,short) is 0.71. Our SP-based word embeddings also exhibit a similar behavior. The question of whether antonyms are similar or not is not a trivial one. On the one hand, some NLP tasks might benefit from representing antonyms as similar. For example, in word classification tasks, words such as “big” and “small” potentially belong to the same class (size adjectives), and thus representing them as similar is desired. On the other hand, antonyms are very dissimilar by definition. This distinction is crucial in tasks such as </context>
<context position="24376" citStr="Mikolov et al., 2013" startWordPosition="4029" endWordPosition="4032">ample 25% of the SimLex999 word pairs (-250 pairs) and use them as a development set for parameter tuning. We use the remaining 75% of the pairs (-750 pairs) as a test set. We report the average of the results we got in the 10 folds. Training Corpus. We use an 8G words corpus, constructed using the word2vec script.7 Through this script we also apply a pre-processing step 5We tune β using a development set (Section 5). Typical values are 7 and 10. 6www.cl.cam.ac.uk/˜fh295/simlex.html 7code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh which employs the word2phrase tool (Mikolov et al., 2013c) to merge common word pairs and triples to expression tokens. Our corpus consists of four datasets: (a) The 2012 and 2013 crawled news articles from the ACL 2014 workshop on statistical machine translation (Bojar et al., 2014);8 (b) The One Billion Word Benchmark of Chelba et al. (2013);9 (c) The UMBC corpus (Han et al., 2013);10 and (d) The September 2014 dump of the English Wikipedia.11 5.2 Baselines We compare our model against six baselines: one that encodes bag-of-words co-occurrence statistics into its features (model 1 below), three NN models that encode the same type of information i</context>
<context position="33860" citStr="Mikolov et al., 2013" startWordPosition="5545" endWordPosition="5548">e between 1 and 10 by each model, where a score of M means that the pair is ranked at the M’th decile. The examples in the table are taken from the first (lowest) decile according to SimLex999’s human evaluators. The table shows that when the antonym parameter is off, our model generally recognizes antonyms as similar. In contrast, when the parameter is on, ranks of antonyms substantially decrease. Antonymy as Word Analogy. One of the most notable features of the skip-gram model is that some geometric relations between its vectors translate to semantic relations between the represented words (Mikolov et al., 2013c), e.g.: vwoman − vman + vking ≈ vqueen It is therefore possible that a similar method can be applied to capture antonymy – a useful property that our model was demonstrated to have. To test this hypothesis, we generated a set of 200 analogy questions of the form ”X - Y + Z = ?” where X and Y are antonyms, and Z is a word with an unknown antonym.20 Example questions include: “stupid - smart + life = ?” (death) and “huge - tiny + arrive = ?” (leave). We applied the standard word analogy evaluation (Mikolov et al., 2013c) on this dataset with the skip-gram embeddings, and found that results are</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="2379" citStr="Mikolov et al., 2013" startWordPosition="363" endWordPosition="366">ntation (a.k.a word embedding), has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Parti</context>
<context position="7450" citStr="Mikolov et al., 2013" startWordPosition="1171" endWordPosition="1174">y holds for various other pattern types and for other lexical semantic relations (e.g. hypernymy, the is-a relation, which holds in word pairs such as (dog,animal)). We experiment (Section 6) with the SimLex999 dataset (Hill et al., 2014), consisting of 999 pairs of words annotated by human subjects for similarity. When comparing the correlation between the similarity scores derived from our learned representation and the human scores, our representation receives a Spearman correlation coefficient score (p) of 0.517, outperforming six strong baselines, including the state-of-the-art word2vec (Mikolov et al., 2013a) embeddings, by 5.5–16.7%. Our model performs particularly well on the verb portion of SimLex999 (222 verb pairs), achieving a Spearman score of 0.578 compared to scores of 0.163–0.376 of the baseline models, an astonishing improvement of 20.2–41.5%. Our analysis reveals that the antonym adjustment capability of our model is vital for its success. We further demonstrate that the word pair scores produced by our model can be combined with those of word2vec to get an improved predictive power for word similarity. The combined scores result in a Spearman’s p correlation of 0.563, a further 4.6%</context>
<context position="9495" citStr="Mikolov et al., 2013" startWordPosition="1496" endWordPosition="1499">2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective function is: T max log p(wt|</context>
<context position="11069" citStr="Mikolov et al., 2013" startWordPosition="1741" endWordPosition="1744"> within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN models that implement the bag-of-words approach in their objective; and (c) models </context>
<context position="20343" citStr="Mikolov et al., 2013" startWordPosition="3359" endWordPosition="3362">roni et al., 2014). 4We tune n and α using a development set (Section 5). Typical values for n and α are 250 and 7, respectively. 4 Antonym Representation In this section we show how our model allows us to adjust the representation of pairs of antonyms to the needs of a subsequent NLP task. This property will later be demonstrated to have a substantial impact on performance. Antonyms are pairs of words with an opposite meaning (e.g., (tall,short)). As the members of an antonym pair tend to occur in the same context, their word embeddings are often similar. For example, in the skip-gram model (Mikolov et al., 2013a), the score of the (accept,reject) pair is 0.73, and the score of (long,short) is 0.71. Our SP-based word embeddings also exhibit a similar behavior. The question of whether antonyms are similar or not is not a trivial one. On the one hand, some NLP tasks might benefit from representing antonyms as similar. For example, in word classification tasks, words such as “big” and “small” potentially belong to the same class (size adjectives), and thus representing them as similar is desired. On the other hand, antonyms are very dissimilar by definition. This distinction is crucial in tasks such as </context>
<context position="24376" citStr="Mikolov et al., 2013" startWordPosition="4029" endWordPosition="4032">ample 25% of the SimLex999 word pairs (-250 pairs) and use them as a development set for parameter tuning. We use the remaining 75% of the pairs (-750 pairs) as a test set. We report the average of the results we got in the 10 folds. Training Corpus. We use an 8G words corpus, constructed using the word2vec script.7 Through this script we also apply a pre-processing step 5We tune β using a development set (Section 5). Typical values are 7 and 10. 6www.cl.cam.ac.uk/˜fh295/simlex.html 7code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh which employs the word2phrase tool (Mikolov et al., 2013c) to merge common word pairs and triples to expression tokens. Our corpus consists of four datasets: (a) The 2012 and 2013 crawled news articles from the ACL 2014 workshop on statistical machine translation (Bojar et al., 2014);8 (b) The One Billion Word Benchmark of Chelba et al. (2013);9 (c) The UMBC corpus (Han et al., 2013);10 and (d) The September 2014 dump of the English Wikipedia.11 5.2 Baselines We compare our model against six baselines: one that encodes bag-of-words co-occurrence statistics into its features (model 1 below), three NN models that encode the same type of information i</context>
<context position="33860" citStr="Mikolov et al., 2013" startWordPosition="5545" endWordPosition="5548">e between 1 and 10 by each model, where a score of M means that the pair is ranked at the M’th decile. The examples in the table are taken from the first (lowest) decile according to SimLex999’s human evaluators. The table shows that when the antonym parameter is off, our model generally recognizes antonyms as similar. In contrast, when the parameter is on, ranks of antonyms substantially decrease. Antonymy as Word Analogy. One of the most notable features of the skip-gram model is that some geometric relations between its vectors translate to semantic relations between the represented words (Mikolov et al., 2013c), e.g.: vwoman − vman + vking ≈ vqueen It is therefore possible that a similar method can be applied to capture antonymy – a useful property that our model was demonstrated to have. To test this hypothesis, we generated a set of 200 analogy questions of the form ”X - Y + Z = ?” where X and Y are antonyms, and Z is a word with an unknown antonym.20 Example questions include: “stupid - smart + life = ?” (death) and “huge - tiny + arrive = ?” (leave). We applied the standard word analogy evaluation (Mikolov et al., 2013c) on this dataset with the skip-gram embeddings, and found that results are</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="9427" citStr="Mnih and Hinton, 2009" startWordPosition="1483" endWordPosition="1486">nd dimensionality reduction. For recent surveys, see (Turney et al., 2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="9524" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="1500" endWordPosition="1503"> 2012). Most VSM works share two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective function is: T max log p(wt|wt−c,... , wt−1, wt+1,... , w</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Bonnie J Dorr</author>
<author>Graeme Hirst</author>
<author>Peter D Turney</author>
</authors>
<title>Computing lexical contrast.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="14638" citStr="Mohammad et al., 2013" startWordPosition="2302" endWordPosition="2306">al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. 3 Model In this section we describe our approach for generating pattern-based word embeddings. We start by describing symmetric patterns (SPs), continue to show how SPs can be acquired automatically from text, and, finally, explain how these SPs are used for word e</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, Turney, 2013</marker>
<rawString>Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and Peter D. Turney. 2013. Computing lexical contrast. Computational Linguistics, 39(3):555–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Pratim Talukdar</author>
<author>Tom Mitchell</author>
</authors>
<title>Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding.</title>
<date>2012</date>
<booktitle>In Proc. of Coling.</booktitle>
<contexts>
<context position="10745" citStr="Murphy et al. (2012)" startWordPosition="1694" endWordPosition="1697">+c) t=1 where T is the number of words in the corpus, and c is a pre-determined window size. Another word2vec architecture, skip-gram, aims to predict the past and future context given a word. Its objective is: T max � log p(wt+j|wt) t=1 −c≤j≤c,j6=0 In both cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is</context>
<context position="26604" citStr="Murphy et al., 2012" startWordPosition="4335" endWordPosition="4338"> and tune the window size on the development set.16 8http://www.statmt.org/wmt14/trainingmonolingual-news-crawl/ 9http://www.statmt.org/lm-benchmark/ 1-billion-word-language-modelingbenchmark-r13output.tar.gz 10http://ebiquity.umbc.edu/redirect/to/ resource/id/351/UMBC-webbase-corpus 11dumps.wikimedia.org/enwiki/latest/ enwiki-latest-pages-articles.xml.bz2 12The value 2 is almost constantly selected. 13https://code.google.com/p/word2vec/ 14Window size 2 is generally selected for both models. 15nlp.stanford.edu/projects/glove/ 16Window size 2 is generally selected. 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM</context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Pratim Talukdar, and Tom Mitchell. 2012. Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding. In Proc. of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="27093" citStr="Nivre et al., 2006" startWordPosition="4416" endWordPosition="4419">odels. 15nlp.stanford.edu/projects/glove/ 16Window size 2 is generally selected. 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM literature: the score assigned to each pair of words by a model m is the cosine similarity between the vectors induced by m for the participating words. m’s quality is evaluated by computing the Spearman correlation coefficient score (p) between the ranking derived from m’s scores and the one derived from the human scores. 6 Results Main Result. Table 2 presents our results. Our model outperforms the baselines by a margin of 5.5–16.7% in the Spearman’s correlation coefficient (p). No</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masataka Ono</author>
<author>Makoto Miwa</author>
<author>Yutaka Sasaki</author>
</authors>
<title>Word embedding-based antonym detection using thesauri and distributional information.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="14778" citStr="Ono et al. (2015)" startWordPosition="2330" endWordPosition="2333">heir algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. 3 Model In this section we describe our approach for generating pattern-based word embeddings. We start by describing symmetric patterns (SPs), continue to show how SPs can be acquired automatically from text, and, finally, explain how these SPs are used for word embedding construction. 3.1 Symmetric Patterns Lexico-syntactic patterns are sequences of words and wildcards (Hearst, 1992). Examples of pat</context>
</contexts>
<marker>Ono, Miwa, Sasaki, 2015</marker>
<rawString>Masataka Ono, Makoto Miwa, and Yutaka Sasaki. 2015. Word embedding-based antonym detection using thesauri and distributional information. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2405" citStr="Pennington et al., 2014" startWordPosition="367" endWordPosition="370">edding), has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Particularly, very little infor</context>
<context position="9578" citStr="Pennington et al., 2014" startWordPosition="1509" endWordPosition="1512">cs. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective function is: T max log p(wt|wt−c,... , wt−1, wt+1,... , wt+c) t=1 where T is the number of words in the corpus,</context>
<context position="25792" citStr="Pennington et al., 2014" startWordPosition="4254" endWordPosition="4257">W. A simple model where each coordinate corresponds to the co-occurrence count of the represented word with another word in the training corpus. The resulted features are re-weighted according to PPMI. The model’s window size parameter is tuned on the development set.12 2-3. word2vec. The state-of-the-art word2vec toolkit (Mikolov et al., 2013a)13 offers two word embedding architectures: continuous-bagof-words (CBOW) and skip-gram. We follow the recommendations of the word2vec script for setting the parameters of both models, and tune the window size on the development set.14 4. GloVe. GloVe (Pennington et al., 2014)15 is a global log-bilinear regression model for word embedding generation, which trains only on the nonzero elements in a co-occurrence matrix. We use the parameters suggested by the authors, and tune the window size on the development set.16 8http://www.statmt.org/wmt14/trainingmonolingual-news-crawl/ 9http://www.statmt.org/lm-benchmark/ 1-billion-word-language-modelingbenchmark-r13output.tar.gz 10http://ebiquity.umbc.edu/redirect/to/ resource/id/351/UMBC-webbase-corpus 11dumps.wikimedia.org/enwiki/latest/ enwiki-latest-pages-articles.xml.bz2 12The value 2 is almost constantly selected. 13ht</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Combining Word Patterns and Discourse Markers for Paradigmatic Relation Classification. In</title>
<date>2014</date>
<booktitle>Proc. ofACL.</booktitle>
<marker>Roth, Walde, 2014</marker>
<rawString>Michael Roth and Sabine Schulte im Walde. 2014. Combining Word Patterns and Discourse Markers for Paradigmatic Relation Classification. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>The SMART Retrieval System: Experiments</title>
<date>1971</date>
<booktitle>in Automatic Document Processing.</booktitle>
<publisher>Prentice-Hall, Inc.,</publisher>
<location>Upper Saddle River, NJ, USA.</location>
<contexts>
<context position="8389" citStr="Salton, 1971" startWordPosition="1323" endWordPosition="1324">is vital for its success. We further demonstrate that the word pair scores produced by our model can be combined with those of word2vec to get an improved predictive power for word similarity. The combined scores result in a Spearman’s p correlation of 0.563, a further 4.6% improvement compared to our model, and a total of 10.1–21.3% improvement over the baseline models. This suggests that the models provide complementary information about word semantics. 2 Related Work Vector Space Models for Lexical Semantics. Research on vector spaces for word representation dates back to the early 1970’s (Salton, 1971). In traditional methods, a vector for each word w is generated, with each coordinate representing the co-occurrence of w and another context item of interest – most often a word but possibly also a sentence, a document or other items. The feature rep259 resentation generated by this basic construction is sometimes post-processed using techniques such as Positive Pointwise Mutual Information (PPMI) normalization and dimensionality reduction. For recent surveys, see (Turney et al., 2010; Clark, 2012; Erk, 2012). Most VSM works share two important characteristics. First, they encode co-occurrenc</context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>Gerard Salton. 1971. The SMART Retrieval System: Experiments in Automatic Document Processing. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Maximilian Koper</author>
</authors>
<title>Pattern-based distinction of paradigmatic relations for german nouns, verbs, adjectives.</title>
<date>2013</date>
<booktitle>Language Processing and Knowledge in the Web,</booktitle>
<pages>184--198</pages>
<contexts>
<context position="14672" citStr="Walde and Koper, 2013" startWordPosition="2309" endWordPosition="2312">idov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. 3 Model In this section we describe our approach for generating pattern-based word embeddings. We start by describing symmetric patterns (SPs), continue to show how SPs can be acquired automatically from text, and, finally, explain how these SPs are used for word embedding construction. 3.1 Symmetr</context>
</contexts>
<marker>Walde, Koper, 2013</marker>
<rawString>Sabine Schulte im Walde and Maximilian Koper. 2013. Pattern-based distinction of paradigmatic relations for german nouns, verbs, adjectives. Language Processing and Knowledge in the Web, pages 184–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
<author>Moshe Koppel</author>
</authors>
<title>Authorship attribution of micromessages.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="13343" citStr="Schwartz et al., 2013" startWordPosition="2096" endWordPosition="2099">aset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand cra</context>
</contexts>
<marker>Schwartz, Tsur, Rappoport, Koppel, 2013</marker>
<rawString>Roy Schwartz, Oren Tsur, Ari Rappoport, and Moshe Koppel. 2013. Authorship attribution of micromessages. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Minimally supervised classification to semantic categories using automatically acquired symmetric patterns.</title>
<date>2014</date>
<booktitle>In Proc. of Coling.</booktitle>
<contexts>
<context position="5088" citStr="Schwartz et al., 2014" startWordPosition="788" endWordPosition="791">ent a variety of semantic relations, including hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Here, in order to capture similarity between words, we use Symmetric patterns (SPs), such as “X and Y” and “X as well as Y”, where each of the words in the pair can take either the X or the Y position. Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition (Widdows and Dorow, 2002), word clustering (Davidov and Rappoport, 2006) and classification of words to semantic categories (Schwartz et al., 2014). However, to the best of our knowledge, they have not been applied to vector space word representation. Our representation is constructed in the following way (Section 3). For each word w, we construct a vector v of size V , where V is the size of the lexicon. Each element in v represents the cooccurrence in SPs of w with another word in the lexicon, which results in a sparse word representation. Unlike most previous works that applied SPs to NLP tasks, we do not use a hard coded set of patterns. Instead, we extract a set of SPs from plain text using an unsupervised algorithm (Davidov and Rap</context>
<context position="13810" citStr="Schwartz et al. (2014)" startWordPosition="2173" endWordPosition="2176">l tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM liter</context>
</contexts>
<marker>Schwartz, Reichart, Rappoport, 2014</marker>
<rawString>Roy Schwartz, Roi Reichart, and Ari Rappoport. 2014. Minimally supervised classification to semantic categories using automatically acquired symmetric patterns. In Proc. of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc. of NAACL.</booktitle>
<contexts>
<context position="27050" citStr="Toutanova et al., 2003" startWordPosition="4408" endWordPosition="4411">4Window size 2 is generally selected for both models. 15nlp.stanford.edu/projects/glove/ 16Window size 2 is generally selected. 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM literature: the score assigned to each pair of words by a model m is the cosine similarity between the vectors induced by m for the participating words. m’s quality is evaluated by computing the Spearman correlation coefficient score (p) between the ranking derived from m’s scores and the one derived from the human scores. 6 Results Main Result. Table 2 presents our results. Our model outperforms the baselines by a margin of 5.5–16.7% in the</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Icwsm–a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews.</title>
<date>2010</date>
<booktitle>In Proc. of ICWSM.</booktitle>
<contexts>
<context position="13249" citStr="Tsur et al., 2010" startWordPosition="2083" endWordPosition="2086">levels, arguably providing a more realistic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervis</context>
</contexts>
<marker>Tsur, Davidov, Rappoport, 2010</marker>
<rawString>Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010. Icwsm–a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews. In Proc. of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence research.</journal>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In Proc. of Coling.</booktitle>
<contexts>
<context position="14596" citStr="Turney, 2008" startWordPosition="2296" endWordPosition="2297"> Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. 3 Model In this section we describe our approach for generating pattern-based word embeddings. We start by describing symmetric patterns (SPs), continue to show how SPs can be acquired automatically from text, and, finally,</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In Proc. of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dual-space model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>533--585</pages>
<contexts>
<context position="11889" citStr="Turney (2012)" startWordPosition="1868" endWordPosition="1869">ce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN models that implement the bag-of-words approach in their objective; and (c) models that go beyond the bag-ofwords assumption. Similarity vs. Association Most recent VSM research does not distinguish between association and similarity in a principled way, although notable exceptions exist. Turney (2012) constructed two VSMs with the explicit goal of capturing either similarity or association. A classifier that uses the output of these models was able to predict whether two concepts are associated, similar or both. Agirre et al. (2009) partitioned the wordsim353 dataset into two subsets, one focused on similarity and the other on association. They demonstrated the importance of the association/similarity distinction by showing that some VSMs perform relatively well on one subset while others perform comparatively better on the other. Recently, Hill et al. (2014) presented the SimLex999 datase</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter D. Turney. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research, pages 533–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbo Wang</author>
<author>Christopher Thomas</author>
<author>Amit Sheth</author>
<author>Victor Chan</author>
</authors>
<title>Pattern-based synonym and antonym extraction.</title>
<date>2010</date>
<booktitle>In Proc. ofACM.</booktitle>
<contexts>
<context position="14615" citStr="Wang et al., 2010" startWordPosition="2298" endWordPosition="2301"> 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. 3 Model In this section we describe our approach for generating pattern-based word embeddings. We start by describing symmetric patterns (SPs), continue to show how SPs can be acquired automatically from text, and, finally, explain how these </context>
</contexts>
<marker>Wang, Thomas, Sheth, Chan, 2010</marker>
<rawString>Wenbo Wang, Christopher Thomas, Amit Sheth, and Victor Chan. 2010. Pattern-based synonym and antonym extraction. In Proc. ofACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Beate Dorow</author>
</authors>
<title>A graph model for unsupervised lexical acquisition.</title>
<date>2002</date>
<booktitle>In Proc. of Coling.</booktitle>
<contexts>
<context position="4966" citStr="Widdows and Dorow, 2002" startWordPosition="770" endWordPosition="773">ty, we propose an alternative, pattern-based, approach to word representation. In previous work patterns were used to represent a variety of semantic relations, including hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Here, in order to capture similarity between words, we use Symmetric patterns (SPs), such as “X and Y” and “X as well as Y”, where each of the words in the pair can take either the X or the Y position. Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition (Widdows and Dorow, 2002), word clustering (Davidov and Rappoport, 2006) and classification of words to semantic categories (Schwartz et al., 2014). However, to the best of our knowledge, they have not been applied to vector space word representation. Our representation is constructed in the following way (Section 3). For each word w, we construct a vector v of size V , where V is the size of the lexicon. Each element in v represents the cooccurrence in SPs of w with another word in the lexicon, which results in a sparse word representation. Unlike most previous works that applied SPs to NLP tasks, we do not use a har</context>
<context position="13478" citStr="Widdows and Dorow (2002)" startWordPosition="2117" endWordPosition="2120">is observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) i</context>
<context position="16908" citStr="Widdows and Dorow, 2002" startWordPosition="2703" endWordPosition="2706"> exemplified by the semantically plausible expressions “cats or dogs” and “dogs or cats”. Previous works have shown that words that cooccur in SPs are semantically similar (Section 2). In this work we use symmetric patterns to represent words. Our hypothesis is that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. This algorithm starts by defining an SP template to be a sequence of 3-5 tokens, consisting of exactly two wildcards, and 1-3 words. It then traverses a corpus, looking for frequent pattern candidates that match this template. Table 1 shows the six most frequent pattern candidates, along with common instances of these patterns. The algorithm continues by traversing the pattern candidates and selecting a pattern p if a large portion </context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Dominic Widdows and Beate Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proc. of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
<author>John C Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="14735" citStr="Yih et al. (2012)" startWordPosition="2321" endWordPosition="2324"> for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. 3 Model In this section we describe our approach for generating pattern-based word embeddings. We start by describing symmetric patterns (SPs), continue to show how SPs can be acquired automatically from text, and, finally, explain how these SPs are used for word embedding construction. 3.1 Symmetric Patterns Lexico-syntactic patterns are sequences of words an</context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>Wen-tau Yih, Geoffrey Zweig, and John C. Platt. 2012. Polarity inducing latent semantic analysis. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>