<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001121">
<title confidence="0.945069">
Transliteration Alignment
</title>
<author confidence="0.973401">
Vladimir Pervouchine, Haizhou Li Bo Lin
</author>
<affiliation confidence="0.949868">
Institute for Infocomm Research School of Computer Engineering
</affiliation>
<address confidence="0.760082">
A*STAR, Singapore 138632 NTU, Singapore 639798
</address>
<email confidence="0.962158">
{vpervouchine,hli}@i2r.a-star.edu.sg linbo@pmail.ntu.edu.sg
</email>
<sectionHeader confidence="0.992919" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999825">
This paper studies transliteration align-
ment, its evaluation metrics and applica-
tions. We propose a new evaluation met-
ric, alignment entropy, grounded on the
information theory, to evaluate the align-
ment quality without the need for the gold
standard reference and compare the metric
with F-score. We study the use of phono-
logical features and affinity statistics for
transliteration alignment at phoneme and
grapheme levels. The experiments show
that better alignment consistently leads to
more accurate transliteration. In transliter-
ation modeling application, we achieve a
mean reciprocal rate (MRR) of 0.773 on
Xinhua personal name corpus, a signifi-
cant improvement over other reported re-
sults on the same corpus. In transliteration
validation application, we achieve 4.48%
equal error rate on a large LDC corpus.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999762592592593">
Transliteration is a process of rewriting a word
from a source language to a target language in a
different writing system using the word’s phono-
logical equivalent. The word and its translitera-
tion forma transliteration pair. Many efforts have
been devoted to two areas of studies where there
is a need to establish the correspondence between
graphemes or phonemes between a transliteration
pair, also known as transliteration alignment.
One area is the generative transliteration model-
ing (Knight and Graehl, 1998), which studies how
to convert a word from one language to another us-
ing statistical models. Since the models are trained
on an aligned parallel corpus, the resulting statisti-
cal models can only be as good as the alignment of
the corpus. Another area is the transliteration vali-
dation, which studies the ways to validate translit-
eration pairs. For example Knight and Graehl
(1998) use the lexicon frequency, Qu and Grefen-
stette (2004) use the statistics in a monolingual
corpus and the Web, Kuo et al. (2007) use proba-
bilities estimated from the transliteration model to
validate transliteration candidates. In this paper,
we propose using the alignment distance between
the a bilingual pair of words to establish the evi-
dence of transliteration candidacy. An example of
transliteration pair alignment is shown in Figure 1.
</bodyText>
<figure confidence="0.953831333333333">
grapheme tokens
source graphemes
target graphemes
</figure>
<figureCaption confidence="0.901852">
Figure 1: An example of grapheme alignment (Al-
ice, ]IJI7WT), where a Chinese grapheme, a char-
acter, is aligned to an English grapheme token.
</figureCaption>
<bodyText confidence="0.999876590909091">
Like the word alignment in statistical ma-
chine translation (MT), transliteration alignment
becomes one of the important topics in machine
transliteration, which has several unique chal-
lenges. Firstly, the grapheme sequence in a word
is not delimited into grapheme tokens, resulting
in an additional level of complexity. Secondly, to
maintain the phonological equivalence, the align-
ment has to make sense at both grapheme and
phoneme levels of the source and target languages.
This paper reports progress in our ongoing spoken
language translation project, where we are inter-
ested in the alignment problem of personal name
transliteration from English to Chinese.
This paper is organized as follows. In Section 2,
we discuss the prior work. In Section 3, we in-
troduce both statistically and phonologically mo-
tivated alignment techniques and in Section 4 we
advocate an evaluation metric, alignment entropy
that measures the alignment quality. We report the
experiments in Section 5. Finally, we conclude in
Section 6.
</bodyText>
<equation confidence="0.939512">
c1 c2 c3
艾 丽 斯
e1 e2 e3
e1 e2 e3 e4
A L I C E
e5
</equation>
<page confidence="0.985201">
136
</page>
<note confidence="0.999616">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 136–144,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.997331" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999749347222222">
A number of transliteration studies have touched
on the alignment issue as a part of the translit-
eration modeling process, where alignment is
needed at levels of graphemes and phonemes. In
their seminal paper Knight and Graehl (1998) de-
scribed a transliteration approach that transfers the
grapheme representation of a word via the pho-
netic representation, which is known as phoneme-
based transliteration technique (Virga and Khu-
danpur, 2003; Meng et al., 2001; Jung et al.,
2000; Gao et al., 2004). Another technique is
to directly transfer the grapheme, known as di-
rect orthographic mapping, that was shown to
be simple and effective (Li et al., 2004). Some
other approaches that use both source graphemes
and phonemes were also reported with good per-
formance (Oh and Choi, 2002; Al-Onaizan and
Knight, 2002; Bilac and Tanaka, 2004).
To align a bilingual training corpus, some take a
phonological approach, in which the crafted map-
ping rules encode the prior linguistic knowledge
about the source and target languages directly into
the system (Wan and Verspoor, 1998; Meng et al.,
2001; Jiang et al., 2007; Xu et al., 2006). Oth-
ers adopt a statistical approach, in which the affin-
ity between phonemes or graphemes is learned
from the corpus (Gao et al., 2004; AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003).
In the phoneme-based technique where an in-
termediate level of phonetic representation is used
as the pivot, alignment between graphemes and
phonemes of the source and target words is
needed (Oh and Choi, 2005). If source and tar-
get languages have different phoneme sets, align-
ment between the the different phonemes is also
required (Knight and Graehl, 1998). Although
the direct orthographic mapping approach advo-
cates a direct transfer of grapheme at run-time,
we still need to establish the grapheme correspon-
dence at the model training stage, when phoneme
level alignment can help.
It is apparent that the quality of transliteration
alignment of a training corpus has a significant
impact on the resulting transliteration model and
its performance. Although there are many stud-
ies of evaluation metrics of word alignment for
MT (Lambert, 2008), there has been much less re-
ported work on evaluation metrics of translitera-
tion alignment. In MT, the quality of training cor-
pus alignment A is often measured relatively to
the gold standard, or the ground truth alignment
!9, which is a manual alignment of the corpus or
a part of it. Three evaluation metrics are used:
precision, recall, and F-score, the latter being a
function of the former two. They indicate how
close the alignment under investigation is to the
gold standard alignment (Mihalcea and Pedersen,
2003). Denoting the number of cross-lingual map-
pings that are common in both A and !9 as CAG,
the number of cross-lingual mappings in A as CA
and the number of cross-lingual mappings in !9 as
CG, precision Pr is given as CAG/CA, recall Rc
as CAG/CG and F-score as 2Pr · Rc/(Pr + Rc).
Note that these metrics hinge on the availability
of the gold standard, which is often not available.
In this paper we propose a novel evaluation metric
for transliteration alignment grounded on the in-
formation theory. One important property of this
metric is that it does not require a gold standard
alignment as a reference. We will also show that
how this metric is used in generative transliteration
modeling and transliteration validation.
</bodyText>
<sectionHeader confidence="0.972242" genericHeader="method">
3 Transliteration alignment techniques
</sectionHeader>
<bodyText confidence="0.992586466666667">
We assume in this paper that the source language
is English and the target language is Chinese, al-
though the technique is not restricted to English-
Chinese alignment.
Let a word in the source language (English) be
{ei} = {e1 ... eI} and its transliteration in the
target language (Chinese) be {cj} = {c1 ... ci},
ei E E, cj E C, and E, C being the English and
Chinese sets of characters, or graphemes, respec-
tively. Aligning {ei} and {cj} means for each tar-
get grapheme token cj finding a source grapheme
token em, which is an English substring in {ei}
that corresponds to cj, as shown in the example in
Figure 1. As Chinese is syllabic, we use a Chinese
character cj as the target grapheme token.
</bodyText>
<subsectionHeader confidence="0.998733">
3.1 Grapheme affinity alignment
</subsectionHeader>
<bodyText confidence="0.9889082">
Given a distance function between graphemes of
the source and target languages d(ei, cj), the prob-
lem of alignment can be formulated as a dynamic
programming problem with the following function
to minimize:
</bodyText>
<equation confidence="0.992508">
Dij = min(Di−1j−1 + d(ei, cj),
Dij−1 + d(*, cj), (1)
Di−1j + d(ei, *))
</equation>
<page confidence="0.950056">
137
</page>
<figure confidence="0.8970648">
A L I C E
AE L AH S
AY l i siz
艾
丽 斯
</figure>
<bodyText confidence="0.9983525">
Here the asterisk * denotes a null grapheme that
is introduced to facilitate the alignment between
graphemes of different lengths. The minimum dis-
tance achieved is then given by
</bodyText>
<equation confidence="0.977286555555556">
graphemes
source
phonemes
phonemes
target
graphemes
I
D= d(ei, cθ(i)) (2)
i=1
</equation>
<bodyText confidence="0.998704333333333">
where j = B(i) is the correspondence between the
source and target graphemes. The alignment can
be performed via the Expectation-Maximization
(EM) by starting with a random initial alignment
and calculating the affinity matrix count(ei, cj)
over the whole parallel corpus, where element
(i, j) is the number of times character ei was
aligned to cj. From the affinity matrix conditional
probabilities P(ei|cj) can be estimated as
</bodyText>
<equation confidence="0.987341666666667">
EP (ei|cj) = count(ei, cj)/ count(ei, cj) (3)
j
Alignment j = B(i) between {ei} and {cj} that
maximizes probability
P =11 P(cθ(i)|ei) (4)
i
</equation>
<bodyText confidence="0.950455">
is also the same alignment that minimizes align-
ment distance D:
</bodyText>
<equation confidence="0.983078">
ED = − log P = − log P(cθ(i)|ei) (5)
i
</equation>
<bodyText confidence="0.999702166666667">
In other words, equations (2) and (5) are the same
when we have the distance function d(ei, cj) =
− log P(cj|ei). Minimizing the overall distance
over a training corpus, we conduct EM iterations
until the convergence is achieved.
This technique solely relies on the affinity
statistics derived from training corpus, thus is
called grapheme affinity alignment. It is also
equally applicable for alignment between a pair of
symbol sequences representing either graphemes
or phonemes. (Gao et al., 2004; AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003).
</bodyText>
<subsectionHeader confidence="0.999519">
3.2 Grapheme alignment via phonemes
</subsectionHeader>
<bodyText confidence="0.962060181818182">
Transliteration is about finding phonological
equivalent. It is therefore a natural choice to use
the phonetic representation as the pivot. It is
common though that the sound inventory differs
from one language to another, resulting in differ-
ent phonetic representations for source and tar-
get words. Continuing with the earlier example,
Figure 2: An example of English-Chinese translit-
eration alignment via phonetic representations.
Figure 2 shows the correspondence between the
graphemes and phonemes of English word “Al-
ice” and its Chinese transliteration, with CMU
phoneme set used for English (Chase, 1997) and
IIR phoneme set for Chinese (Li et al., 2007a).
A Chinese character is often mapped to a unique
sequence of Chinese phonemes. Therefore, if
we align English characters {ei} and Chinese
phonemes {cpk} (cpk ∈ CP set of Chinese
phonemes) well, we almost succeed in aligning
English and Chinese grapheme tokens. Alignment
between {ei} and {cpk} becomes the main task in
this paper.
</bodyText>
<subsectionHeader confidence="0.819404">
3.2.1 Phoneme affinity alignment
</subsectionHeader>
<bodyText confidence="0.999979">
Let the phonetic transcription of English word
{ei} be {epn}, epn ∈ EP, where EP is the set of
English phonemes. Alignment between {ei} and
{epn}, as well as between {epn} and {cpk} can
be performed via EM as described above. We esti-
mate conditional probability of Chinese phoneme
cpk after observing English character ei as
</bodyText>
<equation confidence="0.974337">
P(cpk|ei) = E P(cpk|epn)P(epn|ei) (6)
{ep.}
</equation>
<bodyText confidence="0.997877909090909">
We use the distance function between English
graphemes and Chinese phonemes d(ei, cpk) =
− log P(cpk|ei) to perform the initial alignment
between {ei} and {cpk} via dynamic program-
ming, followed by the EM iterations until con-
vergence. The estimates for P(cpk|epn) and
P(epn|ei) are obtained from the affinity matrices:
the former from the alignment of English and Chi-
nese phonetic representations, the latter from the
alignment of English words and their phonetic rep-
resentations.
</bodyText>
<subsectionHeader confidence="0.756711">
3.2.2 Phonological alignment
</subsectionHeader>
<bodyText confidence="0.99249625">
Alignment between the phonetic representations
of source and target words can also be achieved
using the linguistic knowledge of phonetic sim-
ilarity. Oh and Choi (2002) define classes of
</bodyText>
<page confidence="0.992584">
138
</page>
<bodyText confidence="0.990534763636364">
phonemes and assign various distances between
phonemes of different classes. In contrast, we
make use of phonological descriptors to define the
similarity between phonemes in this paper.
Perhaps the most common way to measure the
phonetic similarity is to compute the distances be-
tween phoneme features (Kessler, 2005). Such
features have been introduced in many ways, such
as perceptual attributes or articulatory attributes.
Recently, Tao et al. (2006) and Yoon et al. (2007)
have studied the use of phonological features and
manually assigned phonological distance to mea-
sure the similarity of transliterated words for ex-
tracting transliterations from a comparable corpus.
We adopt the binary-valued articulatory at-
tributes as the phonological descriptors, which are
used to describe the CMU and IIR phoneme sets
for English and Chinese Mandarin respectively.
Withgott and Chen (1993) define a feature vec-
tor of phonological descriptors for English sounds.
We extend the idea by defining a 21-element bi-
nary feature vector for each English and Chinese
phoneme. Each element of the feature vector
represents presence or absence of a phonologi-
cal descriptor that differentiates various kinds of
phonemes, e.g. vowels from consonants, front
from back vowels, nasals from fricatives, etc1.
In this way, a phoneme is described by a fea-
ture vector. We express the similarity between
two phonemes by the Hamming distance, also
called the phonological distance, between the two
feature vectors. A difference in one descriptor
between two phonemes increases their distance
by 1. As the descriptors are chosen to differenti-
ate between sounds, the distance between similar
phonemes is low, while that between two very dif-
ferent phonemes, such as a vowel and a consonant,
is high. The null phoneme, added to both English
and Chinese phoneme sets, has a constant distance
to any actual phonemes, which is higher than that
between any two actual phonemes.
We use the phonological distance to perform
the initial alignment between English and Chi-
nese phonetic representations of words. After that
we proceed with recalculation of the distances be-
tween phonemes using the affinity matrix as de-
scribed in Section 3.1 and realign the corpus again.
We continue the iterations until convergence is
1The complete table of English and Chinese phonemes
with their descriptors, as well as the translitera-
tion system demo is available at http://translit.i2r.a-
star.edu.sg/demos/transliteration/
reached. Because of the use of phonological de-
scriptors for the initial alignment, we call this tech-
nique the phonological alignment.
</bodyText>
<sectionHeader confidence="0.915999" genericHeader="method">
4 Transliteration alignment entropy
</sectionHeader>
<bodyText confidence="0.999983142857143">
Having aligned the graphemes between two lan-
guages, we want to measure how good the align-
ment is. Aligning the graphemes means aligning
the English substrings, called the source grapheme
tokens, to Chinese characters, the target grapheme
tokens. Intuitively, the more consistent the map-
ping is, the better the alignment will be. We can
quantify the consistency of alignment via align-
ment entropy grounded on information theory.
Given a corpus of aligned transliteration pairs,
we calculate count(cj, em), the number of times
each Chinese grapheme token (character) cj is
mapped to each English grapheme token em. We
use the counts to estimate probabilities
</bodyText>
<equation confidence="0.988742">
EP(em, cj) = count(cj, em)/ count(cj, em)
m,j
EP(em|cj) = count(cj, em)/ count(cj, em)
m
</equation>
<bodyText confidence="0.999240333333333">
The alignment entropy of the transliteration corpus
is the weighted average of the entropy values for
all Chinese tokens:
</bodyText>
<equation confidence="0.9974842">
H = − E P (cj) E P(�em|cj) log P(�em|cj)
j m
E= − P(�em, cj) log P(�em|cj)
m,j
(7)
</equation>
<bodyText confidence="0.999787916666667">
Alignment entropy indicates the uncertainty of
mapping between the English and Chinese tokens
resulting from alignment. We expect and will
show that this estimate is a good indicator of the
alignment quality, and is as effective as the F-
score, but without the need for a gold standard ref-
erence. A lower alignment entropy suggests that
each Chinese token tends to be mapped to fewer
distinct English tokens, reflecting better consis-
tency. We expect a good alignment to have a
sharp cross-lingual mapping with low alignment
entropy.
</bodyText>
<sectionHeader confidence="0.999711" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999917666666667">
We use two transliteration corpora: Xinhua cor-
pus (Xinhua News Agency, 1992) of 37,637
personal name pairs and LDC Chinese-English
</bodyText>
<page confidence="0.995475">
139
</page>
<bodyText confidence="0.99992719047619">
named entity list LDC2005T34 (Linguistic Data
Consortium, 2005), containing 673,390 personal
name pairs. The LDC corpus is referred to as
LDC05 for short hereafter. For the results to be
comparable with other studies, we follow the same
splitting of Xinhua corpus as that in (Li et al.,
2007b) having a training and testing set of 34,777
and 2,896 names respectively. In contrast to the
well edited Xinhua corpus, LDC05 contains erro-
neous entries. We have manually verified and cor-
rected around 240,000 pairs to clean up the corpus.
As a result, we arrive at a set of 560,768 English-
Chinese (EC) pairs that follow the Chinese pho-
netic rules, and a set of 83,403 English-Japanese
Kanji (EJ) pairs, which follow the Japanese pho-
netic rules, and the rest 29,219 pairs (REST) be-
ing labeled as incorrect transliterations. Next we
conduct three experiments to study 1) alignment
entropy vs. F-score, 2) the impact of alignment
quality on transliteration accuracy, and 3) how to
validate transliteration using alignment metrics.
</bodyText>
<subsectionHeader confidence="0.997215">
5.1 Alignment entropy vs. F-score
</subsectionHeader>
<bodyText confidence="0.993455965517241">
As mentioned earlier, for English-Chinese
grapheme alignment, the main task is to align En-
glish graphemes to Chinese phonemes. Phonetic
transcription for the English names in Xinhua
corpus are obtained by a grapheme-to-phoneme
(G2P) converter (Lenzo, 1997), which generates
phoneme sequence without providing the exact
correspondence between the graphemes and
phonemes. G2P converter is trained on the CMU
dictionary (Lenzo, 2008).
We align English grapheme and phonetic repre-
sentations e − ep with the affinity alignment tech-
nique (Section 3.1) in 3 iterations. We further
align the English and Chinese phonetic represen-
tations ep − cp via both affinity and phonological
alignment techniques, by carrying out 6 and 7 it-
erations respectively. The alignment methods are
schematically shown in Figure 3.
To study how alignment entropy varies accord-
ing to different quality of alignment, we would
like to have many different alignment results. We
pair the intermediate results from the e − ep and
ep − cp alignment iterations (see Figure 3) to
form e − ep − cp alignments between English
graphemes and Chinese phonemes and let them
converge through few more iterations, as shown
in Figure 4. In this way, we arrive at a total of 114
phonological and 80 affinity alignments of differ-
ent quality.
</bodyText>
<figureCaption confidence="0.8168469">
Figure 3: Aligning English graphemes to
phonemes e−ep and English phonemes to Chinese
phonemes ep−cp. Intermediate e−ep and ep−cp
alignments are used for producing e − ep − cp
alignments.
Figure 4: Example of aligning English graphemes
to Chinese phonemes. Each combination of e−ep
and ep − cp alignments is used to derive the initial
distance d(eZ7 cpk), resulting in several e−ep−cp
alignments due to the affinity alignment iterations.
</figureCaption>
<bodyText confidence="0.99992336">
We have manually aligned a random set of
3,000 transliteration pairs from the Xinhua train-
ing set to serve as the gold standard, on which we
calculate the precision, recall and F-score as well
as alignment entropy for each alignment. Each
alignment is reflected as a data point in Figures 5a
and 5b. From the figures, we can observe a clear
correlation between the alignment entropy and F-
score, that validates the effectiveness of alignment
entropy as an evaluation metric. Note that we
don’t need the gold standard reference for report-
ing the alignment entropy.
We also notice that the data points seem to form
clusters inside which the value of F-score changes
insignificantly as the alignment entropy changes.
Further investigation reveals that this could be due
to the limited number of entries in the gold stan-
dard. The 3,000 names in the gold standard are not
enough to effectively reflect the change across dif-
ferent alignments. F-score requires a large gold
standard which is not always available. In con-
trast, because the alignment entropy doesn’t de-
pend on the gold standard, one can easily report
the alignment performance on any unaligned par-
allel corpus.
</bodyText>
<figure confidence="0.997024444444445">
English
graphemes
lei}
afÞnity alignment
e − ep iteration 1
e − ep iteration 2
e − ep iteration 3
English
phonemes
{ep.1
afÞnity alignment
ep − cp iteration 1
ep − cp iteration 2
...
ep − cp iteration 6
Chinese
phonemes
{cpk}
phonological alignment
ep − cp iteration 1
ep − cp iteration 2
...
ep − cp iteration 7
e − ep
alignments
ep − cp
afÞnity /
phonological
alignments
iteration 1
iteration 2
iteration 3
iteration 1
iteration 2
...
iteration n
calculating
d(es, cpk)
afÞnity
alignment
e − ep − cp
iteration 1
iteration 2
etc
140
F-score
!&amp;quot;(&amp;
!&amp;quot;($
!&amp;quot;(!
!&amp;quot;##
!&amp;quot;#&apos;
!&amp;quot;#&amp;
!&amp;quot;#$
2.45
)* $&amp;quot;&amp;* $&amp;quot;** 2.65
$&amp;quot;&apos;
Alignment entropy
(a) 80 affinity alignments
2.�5
)* $&amp;quot;&amp;* $&amp;quot;** 2.�5
$&apos;
Alignment entropy
(b) 114 phonological alignments
</figure>
<figureCaption confidence="0.998602">
Figure 5: Correlation between F-score and align-
</figureCaption>
<bodyText confidence="0.598076666666667">
ment entropy for Xinhua training set alignments.
Results for precision and recall have similar trends
.
</bodyText>
<subsectionHeader confidence="0.9807775">
5.2 Impact of alignment quality on
transliteration accuracy
</subsectionHeader>
<bodyText confidence="0.999958142857144">
We now further study how the alignment affects
the generative transliteration model in the frame-
work of the joint source-channel model (Li et al.,
2004). This model performs transliteration by
maximizing the joint probability of the source and
target names P({ei}, {cj}), where the source and
target names are sequences of English and Chi-
nese grapheme tokens. The joint probability is
expressed as a chain product of a series of condi-
tional probabilities of token pairs P({ei}, {cj}) =
P((ek, ck)|(�ek−1, ck−1)), k = 1... N, where we
limit the history to one preceding pair, resulting in
a bigram model. The conditional probabilities for
token pairs are estimated from the aligned training
corpus. We use this model because it was shown
to be simple yet accurate (Ekbal et al., 2006; Li
et al., 2007b). We train a model for each of the
114 phonological alignments and the 80 affinity
alignments in Section 5.1 and conduct translitera-
tion experiment on the Xinhua test data.
During transliteration, an input English name
is first decoded into a lattice of all possible En-
glish and Chinese grapheme token pairs. Then the
joint source-channel transliteration model is used
to score the lattice to obtain a ranked list of m most
likely Chinese transliterations (m-best list).
We measure transliteration accuracy as the
mean reciprocal rank (MRR) (Kantor and
Voorhees, 2000). If there is only one correct
Chinese transliteration of the k-th English word
and it is found at the rk-th position in the m-best
list, its reciprocal rank is 1/rk. If the list contains
no correct transliterations, the reciprocal rank is
0. In case of multiple correct transliterations, we
take the one that gives the highest reciprocal rank.
MRR is the average of the reciprocal ranks across
all words in the test set. It is commonly used as
a measure of transliteration accuracy, and also
allows us to make a direct comparison with other
reported work (Li et al., 2007b).
We take m = 20 and measure MRR on Xinhua
test set for each alignment of Xinhua training set
as described in Section 5.1. We report MRR and
the alignment entropy in Figures 6a and 7a for the
affinity and phonological alignments respectively.
The highest MRR we achieve is 0.771 for affin-
ity alignments and 0.773 for phonological align-
ments. This is a significant improvement over the
MRR of 0.708 reported in (Li et al., 2007b) on the
same data. We also observe that the phonological
alignment technique produces, on average, better
alignments than the affinity alignment technique
in terms of both the alignment entropy and MRR.
We also report the MRR and F-scores for each
alignment in Figures 6b and 7b, from which we
observe that alignment entropy has stronger corre-
lation with MRR than F-score does. The Spear-
man’s rank correlation coefficients are −0.89 and
−0.88 for data in Figure 6a and 7a respectively.
This once again demonstrates the desired property
of alignment entropy as an evaluation metric of
alignment.
To validate our findings from Xinhua corpus,
we further carry out experiments on the EC set
of LDC05 containing 560,768 entries. We split
the set into 5 almost equal subsets for cross-
validation: in each of 5 experiments one subset is
used for testing and the remaining ones for train-
ing. Since LDC05 contains one-to-many English-
Chinese transliteration pairs, we make sure that an
English name only appears in one subset.
Note that the EC set of LDC05 contains
many names of non-English, and, generally, non-
European origin. This makes the G2P converter
less accurate, as it is trained on an English pho-
netic dictionary. We therefore only apply the affin-
ity alignment technique to align the EC set. We
</bodyText>
<table confidence="0.6067955">
F-score
!&amp;quot;(&amp;
!&amp;quot;($
!&amp;quot;(!
!&amp;quot;##
!&amp;quot;#&apos;
!&amp;quot;#&amp;
!&amp;quot;#$
</table>
<page confidence="0.953327">
141
142
</page>
<figure confidence="0.99944552">
MRR
0.775
0.770
0.765
0.760
0.755
0.750
2.35 2.45 2.55 2.65
Alignment entropy
(a) 80 affinity alignments
MRR
0.775
0.770
0.765
0.760
0.755
0.750
0.82 0.84 0.86 0.88 0.90 0.92 0.94
F‐score
(b) 80 affinity alignments
2.35 2.45 2.55 2.65
Alignment entropy
(a) 114 phonological alignments
MRR
(b) 114 phonological alignments
</figure>
<figureCaption confidence="0.91009225">
Figure 6: Mean reciprocal ratio on Xinhua test
set vs. alignment entropy and F-score for mod-
els trained with different affinity alignments.
use each iteration of the alignment in the translit-
eration modeling and present the resulting MRR
along with alignment entropy in Figure 8. The
MRR results are the averages of five values pro-
duced in the five-fold cross-validations.
</figureCaption>
<bodyText confidence="0.975714826086956">
We observe a clear correlation between the
alignment entropy and transliteration accuracy ex-
pressed by MRR on LDC05 corpus, similar to that
on Xinhua corpus, with the Spearman’s rank cor-
relation coefficient of −0.77. We obtain the high-
est average MRR of 0.720 on the EC set.
5.3 Validating transliteration using
alignment measure
Transliteration validation is a hypothesis test that
decides whether a given transliteration pair is gen-
uine or not. Instead of using the lexicon fre-
quency (Knight and Graehl, 1998) or Web statis-
tics (Qu and Grefenstette, 2004), we propose vali-
dating transliteration pairs according to the align-
ment distance D between the aligned English
graphemes and Chinese phonemes (see equations
(2) and (5)). A distance function d(ei7 cpk) is
established from each alignment on the Xinhua
training set as discussed in Section 5.2.
An audit of LDC05 corpus groups the corpus
into three sets: an English-Chinese (EC) set of
560,768 samples, an English-Japanese (EJ) set
of 83,403 samples and the REST set of 29,219
</bodyText>
<figureCaption confidence="0.998668333333333">
Figure 7: Mean reciprocal ratio on Xinhua test
set vs. alignment entropy and F-score for models
trained with different phonological alignments.
</figureCaption>
<figure confidence="0.901734">
MRR
</figure>
<figureCaption confidence="0.9914845">
Figure 8: Mean reciprocal ratio vs. alignment en-
tropy for alignments of EC set.
</figureCaption>
<bodyText confidence="0.994499473684211">
samples that are not transliteration pairs. We
mark the EC name pairs as genuine and the rest
112,622 name pairs that do not follow the Chi-
nese phonetic rules as false transliterations, thus
creating the ground truth labels for an English-
Chinese transliteration validation experiment. In
other words, LDC05 has 560,768 genuine translit-
eration pairs and 112,622 false ones.
We run one iteration of alignment over LDC05
(both genuine and false) with the distance func-
tion d(ei7 cpk) derived from the affinity matrix of
one aligned Xinhua training set. In this way, each
transliteration pair in LDC05 provides an align-
ment distance. One can expect that a genuine
transliteration pair typically aligns well, leading
to a low distance, while a false transliteration pair
will do otherwise. To remove the effect of word
length, we normalize the distance by the English
name length, the Chinese phonetic transcription
</bodyText>
<figure confidence="0.982929823529412">
0.82 0.84 0.86 0.88 0.90 0.92 0.94
F‐score
1.90 2.00 2.10 2.20
Alignment entropy
MRR
0.775
0.770
0.765
0.760
0.755
0.750
0.775
0.770
0.765
0.760
0.755
0.750
0.720
0.718
0.716
0.714
0.712
0.710
0.708
0.706
0.704
length, and the sum of both, producing scores,
score2 and score3 respectively.
1 2 5 10 20
Miss probability (%)
(a) DET with score,, score2,
score3.
(b) DET results vs. three different
alignment quality.
</figure>
<figureCaption confidence="0.9874485">
Figure 9: Detection error tradeoff (DET) curves
for transliteration validation on LDC05.
</figureCaption>
<bodyText confidence="0.999926826086957">
We can now classify each LDC05 name pair as
genuine or false by having a hypothesis test. When
the test score is lower than a pre-set threshold, the
name pair is accepted as genuine, otherwise false.
In this way, each pre-set threshold will present two
types of errors, a false alarm and a miss-detect
rate. A common way to present such results is via
the detection error tradeoff (DET) curves, which
show all possible decision points, and the equal er-
ror rate (EER), when false alarm and miss-detect
rates are equal.
Figure 9a shows three DET curves based on
scores, score2 and score3 respectively for one
one alignment solution on the Xinhua training set.
The horizontal axis is the probability of miss-
detecting a genuine transliteration, while the verti-
cal one is the probability of false-alarms. It is clear
that out of the three, score2 gives the best results.
We select the alignments of Xinhua training
set that produce the highest and the lowest MRR.
We also randomly select three other alignments
that produce different MRR values from the pool
of 114 phonological and 80 affinity alignments.
</bodyText>
<table confidence="0.987001875">
Xinhua train Alignment entropy MRR on Xinhua LDC
set alignment of Xinhua train set test set classification
EER, %
1 2.396 0.773 4.48
2 2.529 0.764 4.52
3 2.586 0.761 4.51
4 2.621 0.757 4.71
5 2.625 0.754 4.70
</table>
<tableCaption confidence="0.904264666666667">
Table 1: Equal error ratio of LDC transliteration
pair validation for different alignments of Xinhua
training set.
</tableCaption>
<bodyText confidence="0.999913">
We use each alignment to derive distance func-
tion d(eZ7 cpO. Table 1 shows the EER of LDC05
validation using score2, along with the alignment
entropy of the Xinhua training set that derives
d(eZ7 cpO, and the MRR on Xinhua test set in the
generative transliteration experiment (see Section
5.2) for all 5 alignments. To avoid cluttering Fig-
ure 9b, we show the DET curves for alignments
1, 2 and 5 only. We observe that distance func-
tion derived from better aligned Xinhua corpus,
as measured by both our alignment entropy met-
ric and MRR, leads to a higher validation accuracy
consistently on LDC05.
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999885458333333">
We conclude that the alignment entropy is a re-
liable indicator of the alignment quality, as con-
firmed by our experiments on both Xinhua and
LDC corpora. Alignment entropy does not re-
quire the gold standard reference, it thus can be
used to evaluate alignments of large transliteration
corpora and is possibly to give more reliable esti-
mate of alignment quality than the F-score metric
as shown in our transliteration experiment.
The alignment quality of training corpus has
a significant impact on the transliteration mod-
els. We achieve the highest MRR of 0.773 on
Xinhua corpus with phonological alignment tech-
nique, which represents a significant performance
gain over other reported results. Phonological
alignment outperforms affinity alignment on clean
database.
We propose using alignment distance to validate
transliterations. A high quality alignment on a
small verified corpus such as Xinhua can be effec-
tively used to validate a large noisy corpus, such
as LDC05. We believe that this property would be
useful in transliteration extraction, cross-lingual
information retrieval applications.
</bodyText>
<figure confidence="0.996888857142857">
False alarm probability (%)
1 2 5 10 20
score2
EER: 4.48 %
EER: 4.80 %
score3
score1
EER: 7.13 %
1 2 5 10
Miss probability (%)
False alarm probability (%)
1 2 5 10
Entropy: 2.396
MRR: 0.773
EER: 4.48 %
Entropy: 2.625
MRR: 0.754
EER: 4.70%
Entropy: 2.529
MRR: 0.764
EER: 4.52%
</figure>
<page confidence="0.995526">
143
</page>
<sectionHeader confidence="0.995486" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999869294117647">
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Sta-
tistical transliteration for English-Arabic cross lan-
guage information retrieval. In Proc. ACM CIKM.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc. ACL
Workshop: Computational Apporaches to Semitic
Languages.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In Proc.
COLING, pages 597–603.
Lin L. Chase. 1997. Error-responsive feedback mech-
anisms for speech recognizers. Ph.D. thesis, CMU.
Asif Ekbal, Sudip Kumar Naskar, and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration. In Proc. COLING/ACL,
pages 191–198
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374–381.
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with web min-
ing and transliteration. In IJCAI, pages 1629–1634.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An English to Korean transliteration model of
extended Markov window. In Proc. COLING, vol-
ume 1.
Paul. B. Kantor and Ellen. M. Voorhees. 2000. The
TREC-5 confusion track: comparing retrieval meth-
ods for scanned text. Information Retrieval, 2:165–
176.
Brett Kessler. 2005. Phonetic comparison algo-
rithms. Transactions of the Philological Society,
103(2):243–260.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2007.
A phonetic similarity model for automatic extraction
of transliteration pairs. ACM Trans. Asian Language
Information Processing, 6(2).
Patrik Lambert. 2008. Exploiting lexical informa-
tion and discriminative alignment training in statis-
tical machine translation. Ph.D. thesis, Universitat
Polit`ecnica de Catalunya, Barcelona, Spain.
Kevin Lenzo. 1997. t2p: text-to-phoneme converter
builder. http://www.cs.cmu.edu/˜lenzo/t2p/.
Kevin Lenzo. 2008. The CMU pronounc-
ing dictionary. http://www.speech.cs.cmu.edu/cgi-
bin/cmudict.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. ACL, pages 159–166.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007a. A
vector space modeling approach to spoken language
identification. IEEE Trans. Acoust., Speech, Signal
Process., 15(1):271–284.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007b. Semantic transliteration of personal
names. In Proc. ACL, pages 120–127.
Linguistic Data Consortium. 2005. LDC Chinese-
English name entity lists LDC2005T34.
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Proc. HLT-NAACL,
pages 1–10.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002.
Jong-Hoon Oh and Key-Sun Choi. 2005. Machine
learning based english-to-korean transliteration us-
ing grapheme and phoneme information. IEICE
Trans. Information and Systems, E88-D(7):1737–
1748.
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in
Latin script via language identification and corpus
validation. In Proc. ACL, pages 183–190.
Tao Tao, Su-Youn Yoon, Andrew Fisterd, Richard
Sproat, and ChengXiang Zhai. 2006. Unsupervised
named entity transliteration using temporal and pho-
netic correlation. In Proc. EMNLP, pages 250–257.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352–1356.
M. M. Withgott and F. R. Chen. 1993. Computational
models of American speech. Centre for the study of
language and information.
Xinhua News Agency. 1992. Chinese transliteration
offoreign personal names. The Commercial Press.
LiLi Xu, Atsushi Fujii, and Tetsuya Ishikawa. 2006.
Modeling impression in probabilistic transliteration
into Chinese. In Proc. EMNLP, pages 242–249.
Su-Youn Yoon, Kyoung-Young Kim, and Richard
Sproat. 2007. Multilingual transliteration using fea-
ture based phonetic method. In Proc. ACL, pages
112–119.
</reference>
<page confidence="0.998592">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918661">
<title confidence="0.999483">Transliteration Alignment</title>
<author confidence="0.998505">Vladimir Pervouchine</author>
<author confidence="0.998505">Haizhou Li Bo Lin</author>
<affiliation confidence="0.994919">Institute for Infocomm Research School of Computer Engineering</affiliation>
<address confidence="0.986755">A*STAR, Singapore 138632 NTU, Singapore 639798</address>
<email confidence="0.990824">linbo@pmail.ntu.edu.sg</email>
<abstract confidence="0.997108619047619">This paper studies transliteration alignment, its evaluation metrics and applications. We propose a new evaluation metgrounded on the information theory, to evaluate the alignquality without the need for the and compare the metric We study the use of phonological features and affinity statistics for transliteration alignment at phoneme and grapheme levels. The experiments show that better alignment consistently leads to more accurate transliteration. In transliteration modeling application, we achieve a mean reciprocal rate (MRR) of 0.773 on Xinhua personal name corpus, a significant improvement over other reported results on the same corpus. In transliteration validation application, we achieve 4.48% equal error rate on a large LDC corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nasreen AbdulJaleel</author>
<author>Leah S Larkey</author>
</authors>
<title>Statistical transliteration for English-Arabic cross language information retrieval.</title>
<date>2003</date>
<booktitle>In Proc. ACM CIKM.</booktitle>
<contexts>
<context position="5137" citStr="AbdulJaleel and Larkey, 2003" startWordPosition="811" endWordPosition="814">her approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). In the phoneme-based technique where an intermediate level of phonetic representation is used as the pivot, alignment between graphemes and phonemes of the source and target words is needed (Oh and Choi, 2005). If source and target languages have different phoneme sets, alignment between the the different phonemes is also required (Knight and Graehl, 1998). Although the direct orthographic mapping approach advocates a direct transfer of grapheme at run-time, we still need to establish the grapheme correspondence at the model training stage, when phoneme level alig</context>
<context position="9804" citStr="AbdulJaleel and Larkey, 2003" startWordPosition="1602" endWordPosition="1605">lso the same alignment that minimizes alignment distance D: ED = − log P = − log P(cθ(i)|ei) (5) i In other words, equations (2) and (5) are the same when we have the distance function d(ei, cj) = − log P(cj|ei). Minimizing the overall distance over a training corpus, we conduct EM iterations until the convergence is achieved. This technique solely relies on the affinity statistics derived from training corpus, thus is called grapheme affinity alignment. It is also equally applicable for alignment between a pair of symbol sequences representing either graphemes or phonemes. (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). 3.2 Grapheme alignment via phonemes Transliteration is about finding phonological equivalent. It is therefore a natural choice to use the phonetic representation as the pivot. It is common though that the sound inventory differs from one language to another, resulting in different phonetic representations for source and target words. Continuing with the earlier example, Figure 2: An example of English-Chinese transliteration alignment via phonetic representations. Figure 2 shows the correspondence between the graphemes and phonemes of English word “Alice” and its </context>
</contexts>
<marker>AbdulJaleel, Larkey, 2003</marker>
<rawString>Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical transliteration for English-Arabic cross language information retrieval. In Proc. ACM CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Machine transliteration of names in arabic text.</title>
<date>2002</date>
<booktitle>In Proc. ACL Workshop: Computational Apporaches to Semitic Languages.</booktitle>
<contexts>
<context position="4656" citStr="Al-Onaizan and Knight, 2002" startWordPosition="730" endWordPosition="733">mes. In their seminal paper Knight and Graehl (1998) described a transliteration approach that transfers the grapheme representation of a word via the phonetic representation, which is known as phonemebased transliteration technique (Virga and Khudanpur, 2003; Meng et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). In the phoneme-based technique where an intermediate level of phonetic representation is </context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Machine transliteration of names in arabic text. In Proc. ACL Workshop: Computational Apporaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Hozumi Tanaka</author>
</authors>
<title>A hybrid back-transliteration system for Japanese.</title>
<date>2004</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>597--603</pages>
<contexts>
<context position="4681" citStr="Bilac and Tanaka, 2004" startWordPosition="734" endWordPosition="737">night and Graehl (1998) described a transliteration approach that transfers the grapheme representation of a word via the phonetic representation, which is known as phonemebased transliteration technique (Virga and Khudanpur, 2003; Meng et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). In the phoneme-based technique where an intermediate level of phonetic representation is used as the pivot, alignm</context>
</contexts>
<marker>Bilac, Tanaka, 2004</marker>
<rawString>Slaven Bilac and Hozumi Tanaka. 2004. A hybrid back-transliteration system for Japanese. In Proc. COLING, pages 597–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin L Chase</author>
</authors>
<title>Error-responsive feedback mechanisms for speech recognizers.</title>
<date>1997</date>
<tech>Ph.D. thesis, CMU.</tech>
<contexts>
<context position="10480" citStr="Chase, 1997" startWordPosition="1704" endWordPosition="1705">s Transliteration is about finding phonological equivalent. It is therefore a natural choice to use the phonetic representation as the pivot. It is common though that the sound inventory differs from one language to another, resulting in different phonetic representations for source and target words. Continuing with the earlier example, Figure 2: An example of English-Chinese transliteration alignment via phonetic representations. Figure 2 shows the correspondence between the graphemes and phonemes of English word “Alice” and its Chinese transliteration, with CMU phoneme set used for English (Chase, 1997) and IIR phoneme set for Chinese (Li et al., 2007a). A Chinese character is often mapped to a unique sequence of Chinese phonemes. Therefore, if we align English characters {ei} and Chinese phonemes {cpk} (cpk ∈ CP set of Chinese phonemes) well, we almost succeed in aligning English and Chinese grapheme tokens. Alignment between {ei} and {cpk} becomes the main task in this paper. 3.2.1 Phoneme affinity alignment Let the phonetic transcription of English word {ei} be {epn}, epn ∈ EP, where EP is the set of English phonemes. Alignment between {ei} and {epn}, as well as between {epn} and {cpk} ca</context>
</contexts>
<marker>Chase, 1997</marker>
<rawString>Lin L. Chase. 1997. Error-responsive feedback mechanisms for speech recognizers. Ph.D. thesis, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asif Ekbal</author>
<author>Sudip Kumar Naskar</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>A modified joint source-channel model for transliteration.</title>
<date>2006</date>
<booktitle>In Proc. COLING/ACL,</booktitle>
<pages>191--198</pages>
<contexts>
<context position="21912" citStr="Ekbal et al., 2006" startWordPosition="3552" endWordPosition="3555"> performs transliteration by maximizing the joint probability of the source and target names P({ei}, {cj}), where the source and target names are sequences of English and Chinese grapheme tokens. The joint probability is expressed as a chain product of a series of conditional probabilities of token pairs P({ei}, {cj}) = P((ek, ck)|(�ek−1, ck−1)), k = 1... N, where we limit the history to one preceding pair, resulting in a bigram model. The conditional probabilities for token pairs are estimated from the aligned training corpus. We use this model because it was shown to be simple yet accurate (Ekbal et al., 2006; Li et al., 2007b). We train a model for each of the 114 phonological alignments and the 80 affinity alignments in Section 5.1 and conduct transliteration experiment on the Xinhua test data. During transliteration, an input English name is first decoded into a lattice of all possible English and Chinese grapheme token pairs. Then the joint source-channel transliteration model is used to score the lattice to obtain a ranked list of m most likely Chinese transliterations (m-best list). We measure transliteration accuracy as the mean reciprocal rank (MRR) (Kantor and Voorhees, 2000). If there is</context>
</contexts>
<marker>Ekbal, Naskar, Bandyopadhyay, 2006</marker>
<rawString>Asif Ekbal, Sudip Kumar Naskar, and Sivaji Bandyopadhyay. 2006. A modified joint source-channel model for transliteration. In Proc. COLING/ACL, pages 191–198</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Gao</author>
<author>Kam-Fai Wong</author>
<author>Wai Lam</author>
</authors>
<title>Phoneme-based transliteration of foreign names for OOV problem.</title>
<date>2004</date>
<booktitle>In Proc. IJCNLP,</booktitle>
<pages>374--381</pages>
<contexts>
<context position="4345" citStr="Gao et al., 2004" startWordPosition="679" endWordPosition="682">and the 4th IJCNLP of the AFNLP, pages 136–144, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Related Work A number of transliteration studies have touched on the alignment issue as a part of the transliteration modeling process, where alignment is needed at levels of graphemes and phonemes. In their seminal paper Knight and Graehl (1998) described a transliteration approach that transfers the grapheme representation of a word via the phonetic representation, which is known as phonemebased transliteration technique (Virga and Khudanpur, 2003; Meng et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et </context>
<context position="9774" citStr="Gao et al., 2004" startWordPosition="1598" endWordPosition="1601">(i)|ei) (4) i is also the same alignment that minimizes alignment distance D: ED = − log P = − log P(cθ(i)|ei) (5) i In other words, equations (2) and (5) are the same when we have the distance function d(ei, cj) = − log P(cj|ei). Minimizing the overall distance over a training corpus, we conduct EM iterations until the convergence is achieved. This technique solely relies on the affinity statistics derived from training corpus, thus is called grapheme affinity alignment. It is also equally applicable for alignment between a pair of symbol sequences representing either graphemes or phonemes. (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). 3.2 Grapheme alignment via phonemes Transliteration is about finding phonological equivalent. It is therefore a natural choice to use the phonetic representation as the pivot. It is common though that the sound inventory differs from one language to another, resulting in different phonetic representations for source and target words. Continuing with the earlier example, Figure 2: An example of English-Chinese transliteration alignment via phonetic representations. Figure 2 shows the correspondence between the graphemes and phonemes of</context>
</contexts>
<marker>Gao, Wong, Lam, 2004</marker>
<rawString>Wei Gao, Kam-Fai Wong, and Wai Lam. 2004. Phoneme-based transliteration of foreign names for OOV problem. In Proc. IJCNLP, pages 374–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Ming Zhou</author>
<author>Lee-Feng Chien</author>
<author>Cheng Niu</author>
</authors>
<title>Named entity translation with web mining and transliteration.</title>
<date>2007</date>
<booktitle>In IJCAI,</booktitle>
<pages>1629--1634</pages>
<contexts>
<context position="4954" citStr="Jiang et al., 2007" startWordPosition="779" endWordPosition="782">l., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). In the phoneme-based technique where an intermediate level of phonetic representation is used as the pivot, alignment between graphemes and phonemes of the source and target words is needed (Oh and Choi, 2005). If source and target languages have different phoneme sets, alignment between the the different phonemes is also required (Knight and Graehl, 1998). Although the direct orthogr</context>
</contexts>
<marker>Jiang, Zhou, Chien, Niu, 2007</marker>
<rawString>Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng Niu. 2007. Named entity translation with web mining and transliteration. In IJCAI, pages 1629–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung Young Jung</author>
<author>SungLim Hong</author>
<author>Eunok Paek</author>
</authors>
<title>An English to Korean transliteration model of extended Markov window.</title>
<date>2000</date>
<booktitle>In Proc. COLING,</booktitle>
<volume>1</volume>
<contexts>
<context position="4326" citStr="Jung et al., 2000" startWordPosition="675" endWordPosition="678">Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 136–144, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Related Work A number of transliteration studies have touched on the alignment issue as a part of the transliteration modeling process, where alignment is needed at levels of graphemes and phonemes. In their seminal paper Knight and Graehl (1998) described a transliteration approach that transfers the grapheme representation of a word via the phonetic representation, which is known as phonemebased transliteration technique (Virga and Khudanpur, 2003; Meng et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et a</context>
</contexts>
<marker>Jung, Hong, Paek, 2000</marker>
<rawString>Sung Young Jung, SungLim Hong, and Eunok Paek. 2000. An English to Korean transliteration model of extended Markov window. In Proc. COLING, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kantor</author>
<author>Ellen M Voorhees</author>
</authors>
<title>The TREC-5 confusion track: comparing retrieval methods for scanned text. Information Retrieval,</title>
<date>2000</date>
<pages>2--165</pages>
<contexts>
<context position="22499" citStr="Kantor and Voorhees, 2000" startWordPosition="3646" endWordPosition="3649">simple yet accurate (Ekbal et al., 2006; Li et al., 2007b). We train a model for each of the 114 phonological alignments and the 80 affinity alignments in Section 5.1 and conduct transliteration experiment on the Xinhua test data. During transliteration, an input English name is first decoded into a lattice of all possible English and Chinese grapheme token pairs. Then the joint source-channel transliteration model is used to score the lattice to obtain a ranked list of m most likely Chinese transliterations (m-best list). We measure transliteration accuracy as the mean reciprocal rank (MRR) (Kantor and Voorhees, 2000). If there is only one correct Chinese transliteration of the k-th English word and it is found at the rk-th position in the m-best list, its reciprocal rank is 1/rk. If the list contains no correct transliterations, the reciprocal rank is 0. In case of multiple correct transliterations, we take the one that gives the highest reciprocal rank. MRR is the average of the reciprocal ranks across all words in the test set. It is commonly used as a measure of transliteration accuracy, and also allows us to make a direct comparison with other reported work (Li et al., 2007b). We take m = 20 and measu</context>
</contexts>
<marker>Kantor, Voorhees, 2000</marker>
<rawString>Paul. B. Kantor and Ellen. M. Voorhees. 2000. The TREC-5 confusion track: comparing retrieval methods for scanned text. Information Retrieval, 2:165– 176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett Kessler</author>
</authors>
<title>Phonetic comparison algorithms.</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<contexts>
<context position="12284" citStr="Kessler, 2005" startWordPosition="1990" endWordPosition="1991">ter from the alignment of English words and their phonetic representations. 3.2.2 Phonological alignment Alignment between the phonetic representations of source and target words can also be achieved using the linguistic knowledge of phonetic similarity. Oh and Choi (2002) define classes of 138 phonemes and assign various distances between phonemes of different classes. In contrast, we make use of phonological descriptors to define the similarity between phonemes in this paper. Perhaps the most common way to measure the phonetic similarity is to compute the distances between phoneme features (Kessler, 2005). Such features have been introduced in many ways, such as perceptual attributes or articulatory attributes. Recently, Tao et al. (2006) and Yoon et al. (2007) have studied the use of phonological features and manually assigned phonological distance to measure the similarity of transliterated words for extracting transliterations from a comparable corpus. We adopt the binary-valued articulatory attributes as the phonological descriptors, which are used to describe the CMU and IIR phoneme sets for English and Chinese Mandarin respectively. Withgott and Chen (1993) define a feature vector of pho</context>
</contexts>
<marker>Kessler, 2005</marker>
<rawString>Brett Kessler. 2005. Phonetic comparison algorithms. Transactions of the Philological Society, 103(2):243–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="1593" citStr="Knight and Graehl, 1998" startWordPosition="227" endWordPosition="230">nsliteration validation application, we achieve 4.48% equal error rate on a large LDC corpus. 1 Introduction Transliteration is a process of rewriting a word from a source language to a target language in a different writing system using the word’s phonological equivalent. The word and its transliteration forma transliteration pair. Many efforts have been devoted to two areas of studies where there is a need to establish the correspondence between graphemes or phonemes between a transliteration pair, also known as transliteration alignment. One area is the generative transliteration modeling (Knight and Graehl, 1998), which studies how to convert a word from one language to another using statistical models. Since the models are trained on an aligned parallel corpus, the resulting statistical models can only be as good as the alignment of the corpus. Another area is the transliteration validation, which studies the ways to validate transliteration pairs. For example Knight and Graehl (1998) use the lexicon frequency, Qu and Grefenstette (2004) use the statistics in a monolingual corpus and the Web, Kuo et al. (2007) use probabilities estimated from the transliteration model to validate transliteration cand</context>
<context position="4081" citStr="Knight and Graehl (1998)" startWordPosition="636" endWordPosition="639">n 4 we advocate an evaluation metric, alignment entropy that measures the alignment quality. We report the experiments in Section 5. Finally, we conclude in Section 6. c1 c2 c3 艾 丽 斯 e1 e2 e3 e1 e2 e3 e4 A L I C E e5 136 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 136–144, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Related Work A number of transliteration studies have touched on the alignment issue as a part of the transliteration modeling process, where alignment is needed at levels of graphemes and phonemes. In their seminal paper Knight and Graehl (1998) described a transliteration approach that transfers the grapheme representation of a word via the phonetic representation, which is known as phonemebased transliteration technique (Virga and Khudanpur, 2003; Meng et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004)</context>
<context position="5525" citStr="Knight and Graehl, 1998" startWordPosition="873" endWordPosition="876">Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). In the phoneme-based technique where an intermediate level of phonetic representation is used as the pivot, alignment between graphemes and phonemes of the source and target words is needed (Oh and Choi, 2005). If source and target languages have different phoneme sets, alignment between the the different phonemes is also required (Knight and Graehl, 1998). Although the direct orthographic mapping approach advocates a direct transfer of grapheme at run-time, we still need to establish the grapheme correspondence at the model training stage, when phoneme level alignment can help. It is apparent that the quality of transliteration alignment of a training corpus has a significant impact on the resulting transliteration model and its performance. Although there are many studies of evaluation metrics of word alignment for MT (Lambert, 2008), there has been much less reported work on evaluation metrics of transliteration alignment. In MT, the quality</context>
<context position="26052" citStr="Knight and Graehl, 1998" startWordPosition="4241" endWordPosition="4244">ntropy in Figure 8. The MRR results are the averages of five values produced in the five-fold cross-validations. We observe a clear correlation between the alignment entropy and transliteration accuracy expressed by MRR on LDC05 corpus, similar to that on Xinhua corpus, with the Spearman’s rank correlation coefficient of −0.77. We obtain the highest average MRR of 0.720 on the EC set. 5.3 Validating transliteration using alignment measure Transliteration validation is a hypothesis test that decides whether a given transliteration pair is genuine or not. Instead of using the lexicon frequency (Knight and Graehl, 1998) or Web statistics (Qu and Grefenstette, 2004), we propose validating transliteration pairs according to the alignment distance D between the aligned English graphemes and Chinese phonemes (see equations (2) and (5)). A distance function d(ei7 cpk) is established from each alignment on the Xinhua training set as discussed in Section 5.2. An audit of LDC05 corpus groups the corpus into three sets: an English-Chinese (EC) set of 560,768 samples, an English-Japanese (EJ) set of 83,403 samples and the REST set of 29,219 Figure 7: Mean reciprocal ratio on Xinhua test set vs. alignment entropy and F</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Shea Kuo</author>
<author>Haizhou Li</author>
<author>Ying-Kuei Yang</author>
</authors>
<title>A phonetic similarity model for automatic extraction of transliteration pairs.</title>
<date>2007</date>
<journal>ACM Trans. Asian Language Information Processing,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="2101" citStr="Kuo et al. (2007)" startWordPosition="314" endWordPosition="317">n as transliteration alignment. One area is the generative transliteration modeling (Knight and Graehl, 1998), which studies how to convert a word from one language to another using statistical models. Since the models are trained on an aligned parallel corpus, the resulting statistical models can only be as good as the alignment of the corpus. Another area is the transliteration validation, which studies the ways to validate transliteration pairs. For example Knight and Graehl (1998) use the lexicon frequency, Qu and Grefenstette (2004) use the statistics in a monolingual corpus and the Web, Kuo et al. (2007) use probabilities estimated from the transliteration model to validate transliteration candidates. In this paper, we propose using the alignment distance between the a bilingual pair of words to establish the evidence of transliteration candidacy. An example of transliteration pair alignment is shown in Figure 1. grapheme tokens source graphemes target graphemes Figure 1: An example of grapheme alignment (Alice, ]IJI7WT), where a Chinese grapheme, a character, is aligned to an English grapheme token. Like the word alignment in statistical machine translation (MT), transliteration alignment be</context>
</contexts>
<marker>Kuo, Li, Yang, 2007</marker>
<rawString>Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2007. A phonetic similarity model for automatic extraction of transliteration pairs. ACM Trans. Asian Language Information Processing, 6(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
</authors>
<title>Exploiting lexical information and discriminative alignment training in statistical machine translation.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Polit`ecnica de Catalunya,</institution>
<location>Barcelona,</location>
<contexts>
<context position="6014" citStr="Lambert, 2008" startWordPosition="951" endWordPosition="952">anguages have different phoneme sets, alignment between the the different phonemes is also required (Knight and Graehl, 1998). Although the direct orthographic mapping approach advocates a direct transfer of grapheme at run-time, we still need to establish the grapheme correspondence at the model training stage, when phoneme level alignment can help. It is apparent that the quality of transliteration alignment of a training corpus has a significant impact on the resulting transliteration model and its performance. Although there are many studies of evaluation metrics of word alignment for MT (Lambert, 2008), there has been much less reported work on evaluation metrics of transliteration alignment. In MT, the quality of training corpus alignment A is often measured relatively to the gold standard, or the ground truth alignment !9, which is a manual alignment of the corpus or a part of it. Three evaluation metrics are used: precision, recall, and F-score, the latter being a function of the former two. They indicate how close the alignment under investigation is to the gold standard alignment (Mihalcea and Pedersen, 2003). Denoting the number of cross-lingual mappings that are common in both A and </context>
</contexts>
<marker>Lambert, 2008</marker>
<rawString>Patrik Lambert. 2008. Exploiting lexical information and discriminative alignment training in statistical machine translation. Ph.D. thesis, Universitat Polit`ecnica de Catalunya, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lenzo</author>
</authors>
<date>1997</date>
<note>t2p: text-to-phoneme converter builder. http://www.cs.cmu.edu/˜lenzo/t2p/.</note>
<contexts>
<context position="17552" citStr="Lenzo, 1997" startWordPosition="2824" endWordPosition="2825">ch follow the Japanese phonetic rules, and the rest 29,219 pairs (REST) being labeled as incorrect transliterations. Next we conduct three experiments to study 1) alignment entropy vs. F-score, 2) the impact of alignment quality on transliteration accuracy, and 3) how to validate transliteration using alignment metrics. 5.1 Alignment entropy vs. F-score As mentioned earlier, for English-Chinese grapheme alignment, the main task is to align English graphemes to Chinese phonemes. Phonetic transcription for the English names in Xinhua corpus are obtained by a grapheme-to-phoneme (G2P) converter (Lenzo, 1997), which generates phoneme sequence without providing the exact correspondence between the graphemes and phonemes. G2P converter is trained on the CMU dictionary (Lenzo, 2008). We align English grapheme and phonetic representations e − ep with the affinity alignment technique (Section 3.1) in 3 iterations. We further align the English and Chinese phonetic representations ep − cp via both affinity and phonological alignment techniques, by carrying out 6 and 7 iterations respectively. The alignment methods are schematically shown in Figure 3. To study how alignment entropy varies according to dif</context>
</contexts>
<marker>Lenzo, 1997</marker>
<rawString>Kevin Lenzo. 1997. t2p: text-to-phoneme converter builder. http://www.cs.cmu.edu/˜lenzo/t2p/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lenzo</author>
</authors>
<title>The CMU pronouncing dictionary.</title>
<date>2008</date>
<note>http://www.speech.cs.cmu.edu/cgibin/cmudict.</note>
<contexts>
<context position="17726" citStr="Lenzo, 2008" startWordPosition="2848" endWordPosition="2849">ntropy vs. F-score, 2) the impact of alignment quality on transliteration accuracy, and 3) how to validate transliteration using alignment metrics. 5.1 Alignment entropy vs. F-score As mentioned earlier, for English-Chinese grapheme alignment, the main task is to align English graphemes to Chinese phonemes. Phonetic transcription for the English names in Xinhua corpus are obtained by a grapheme-to-phoneme (G2P) converter (Lenzo, 1997), which generates phoneme sequence without providing the exact correspondence between the graphemes and phonemes. G2P converter is trained on the CMU dictionary (Lenzo, 2008). We align English grapheme and phonetic representations e − ep with the affinity alignment technique (Section 3.1) in 3 iterations. We further align the English and Chinese phonetic representations ep − cp via both affinity and phonological alignment techniques, by carrying out 6 and 7 iterations respectively. The alignment methods are schematically shown in Figure 3. To study how alignment entropy varies according to different quality of alignment, we would like to have many different alignment results. We pair the intermediate results from the e − ep and ep − cp alignment iterations (see Fi</context>
</contexts>
<marker>Lenzo, 2008</marker>
<rawString>Kevin Lenzo. 2008. The CMU pronouncing dictionary. http://www.speech.cs.cmu.edu/cgibin/cmudict.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="4500" citStr="Li et al., 2004" startWordPosition="705" endWordPosition="708">have touched on the alignment issue as a part of the transliteration modeling process, where alignment is needed at levels of graphemes and phonemes. In their seminal paper Knight and Graehl (1998) described a transliteration approach that transfers the grapheme representation of a word via the phonetic representation, which is known as phonemebased transliteration technique (Virga and Khudanpur, 2003; Meng et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al</context>
<context position="21282" citStr="Li et al., 2004" startWordPosition="3447" endWordPosition="3450">afÞnity alignment e − ep − cp iteration 1 iteration 2 etc 140 F-score !&amp;quot;(&amp; !&amp;quot;($ !&amp;quot;(! !&amp;quot;## !&amp;quot;#&apos; !&amp;quot;#&amp; !&amp;quot;#$ 2.45 )* $&amp;quot;&amp;* $&amp;quot;** 2.65 $&amp;quot;&apos; Alignment entropy (a) 80 affinity alignments 2.�5 )* $&amp;quot;&amp;* $&amp;quot;** 2.�5 $&apos; Alignment entropy (b) 114 phonological alignments Figure 5: Correlation between F-score and alignment entropy for Xinhua training set alignments. Results for precision and recall have similar trends . 5.2 Impact of alignment quality on transliteration accuracy We now further study how the alignment affects the generative transliteration model in the framework of the joint source-channel model (Li et al., 2004). This model performs transliteration by maximizing the joint probability of the source and target names P({ei}, {cj}), where the source and target names are sequences of English and Chinese grapheme tokens. The joint probability is expressed as a chain product of a series of conditional probabilities of token pairs P({ei}, {cj}) = P((ek, ck)|(�ek−1, ck−1)), k = 1... N, where we limit the history to one preceding pair, resulting in a bigram model. The conditional probabilities for token pairs are estimated from the aligned training corpus. We use this model because it was shown to be simple ye</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. In Proc. ACL, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Bin Ma</author>
<author>Chin-Hui Lee</author>
</authors>
<title>A vector space modeling approach to spoken language identification.</title>
<date>2007</date>
<journal>IEEE Trans. Acoust., Speech, Signal Process.,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="10529" citStr="Li et al., 2007" startWordPosition="1712" endWordPosition="1715">l equivalent. It is therefore a natural choice to use the phonetic representation as the pivot. It is common though that the sound inventory differs from one language to another, resulting in different phonetic representations for source and target words. Continuing with the earlier example, Figure 2: An example of English-Chinese transliteration alignment via phonetic representations. Figure 2 shows the correspondence between the graphemes and phonemes of English word “Alice” and its Chinese transliteration, with CMU phoneme set used for English (Chase, 1997) and IIR phoneme set for Chinese (Li et al., 2007a). A Chinese character is often mapped to a unique sequence of Chinese phonemes. Therefore, if we align English characters {ei} and Chinese phonemes {cpk} (cpk ∈ CP set of Chinese phonemes) well, we almost succeed in aligning English and Chinese grapheme tokens. Alignment between {ei} and {cpk} becomes the main task in this paper. 3.2.1 Phoneme affinity alignment Let the phonetic transcription of English word {ei} be {epn}, epn ∈ EP, where EP is the set of English phonemes. Alignment between {ei} and {epn}, as well as between {epn} and {cpk} can be performed via EM as described above. We esti</context>
<context position="16531" citStr="Li et al., 2007" startWordPosition="2661" endWordPosition="2664">nds to be mapped to fewer distinct English tokens, reflecting better consistency. We expect a good alignment to have a sharp cross-lingual mapping with low alignment entropy. 5 Experiments We use two transliteration corpora: Xinhua corpus (Xinhua News Agency, 1992) of 37,637 personal name pairs and LDC Chinese-English 139 named entity list LDC2005T34 (Linguistic Data Consortium, 2005), containing 673,390 personal name pairs. The LDC corpus is referred to as LDC05 for short hereafter. For the results to be comparable with other studies, we follow the same splitting of Xinhua corpus as that in (Li et al., 2007b) having a training and testing set of 34,777 and 2,896 names respectively. In contrast to the well edited Xinhua corpus, LDC05 contains erroneous entries. We have manually verified and corrected around 240,000 pairs to clean up the corpus. As a result, we arrive at a set of 560,768 EnglishChinese (EC) pairs that follow the Chinese phonetic rules, and a set of 83,403 English-Japanese Kanji (EJ) pairs, which follow the Japanese phonetic rules, and the rest 29,219 pairs (REST) being labeled as incorrect transliterations. Next we conduct three experiments to study 1) alignment entropy vs. F-scor</context>
<context position="21929" citStr="Li et al., 2007" startWordPosition="3556" endWordPosition="3559">ation by maximizing the joint probability of the source and target names P({ei}, {cj}), where the source and target names are sequences of English and Chinese grapheme tokens. The joint probability is expressed as a chain product of a series of conditional probabilities of token pairs P({ei}, {cj}) = P((ek, ck)|(�ek−1, ck−1)), k = 1... N, where we limit the history to one preceding pair, resulting in a bigram model. The conditional probabilities for token pairs are estimated from the aligned training corpus. We use this model because it was shown to be simple yet accurate (Ekbal et al., 2006; Li et al., 2007b). We train a model for each of the 114 phonological alignments and the 80 affinity alignments in Section 5.1 and conduct transliteration experiment on the Xinhua test data. During transliteration, an input English name is first decoded into a lattice of all possible English and Chinese grapheme token pairs. Then the joint source-channel transliteration model is used to score the lattice to obtain a ranked list of m most likely Chinese transliterations (m-best list). We measure transliteration accuracy as the mean reciprocal rank (MRR) (Kantor and Voorhees, 2000). If there is only one correct</context>
<context position="23499" citStr="Li et al., 2007" startWordPosition="3822" endWordPosition="3825">ks across all words in the test set. It is commonly used as a measure of transliteration accuracy, and also allows us to make a direct comparison with other reported work (Li et al., 2007b). We take m = 20 and measure MRR on Xinhua test set for each alignment of Xinhua training set as described in Section 5.1. We report MRR and the alignment entropy in Figures 6a and 7a for the affinity and phonological alignments respectively. The highest MRR we achieve is 0.771 for affinity alignments and 0.773 for phonological alignments. This is a significant improvement over the MRR of 0.708 reported in (Li et al., 2007b) on the same data. We also observe that the phonological alignment technique produces, on average, better alignments than the affinity alignment technique in terms of both the alignment entropy and MRR. We also report the MRR and F-scores for each alignment in Figures 6b and 7b, from which we observe that alignment entropy has stronger correlation with MRR than F-score does. The Spearman’s rank correlation coefficients are −0.89 and −0.88 for data in Figure 6a and 7a respectively. This once again demonstrates the desired property of alignment entropy as an evaluation metric of alignment. To </context>
</contexts>
<marker>Li, Ma, Lee, 2007</marker>
<rawString>Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007a. A vector space modeling approach to spoken language identification. IEEE Trans. Acoust., Speech, Signal Process., 15(1):271–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Khe Chai Sim</author>
<author>Jin-Shea Kuo</author>
<author>Minghui Dong</author>
</authors>
<title>Semantic transliteration of personal names.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="10529" citStr="Li et al., 2007" startWordPosition="1712" endWordPosition="1715">l equivalent. It is therefore a natural choice to use the phonetic representation as the pivot. It is common though that the sound inventory differs from one language to another, resulting in different phonetic representations for source and target words. Continuing with the earlier example, Figure 2: An example of English-Chinese transliteration alignment via phonetic representations. Figure 2 shows the correspondence between the graphemes and phonemes of English word “Alice” and its Chinese transliteration, with CMU phoneme set used for English (Chase, 1997) and IIR phoneme set for Chinese (Li et al., 2007a). A Chinese character is often mapped to a unique sequence of Chinese phonemes. Therefore, if we align English characters {ei} and Chinese phonemes {cpk} (cpk ∈ CP set of Chinese phonemes) well, we almost succeed in aligning English and Chinese grapheme tokens. Alignment between {ei} and {cpk} becomes the main task in this paper. 3.2.1 Phoneme affinity alignment Let the phonetic transcription of English word {ei} be {epn}, epn ∈ EP, where EP is the set of English phonemes. Alignment between {ei} and {epn}, as well as between {epn} and {cpk} can be performed via EM as described above. We esti</context>
<context position="16531" citStr="Li et al., 2007" startWordPosition="2661" endWordPosition="2664">nds to be mapped to fewer distinct English tokens, reflecting better consistency. We expect a good alignment to have a sharp cross-lingual mapping with low alignment entropy. 5 Experiments We use two transliteration corpora: Xinhua corpus (Xinhua News Agency, 1992) of 37,637 personal name pairs and LDC Chinese-English 139 named entity list LDC2005T34 (Linguistic Data Consortium, 2005), containing 673,390 personal name pairs. The LDC corpus is referred to as LDC05 for short hereafter. For the results to be comparable with other studies, we follow the same splitting of Xinhua corpus as that in (Li et al., 2007b) having a training and testing set of 34,777 and 2,896 names respectively. In contrast to the well edited Xinhua corpus, LDC05 contains erroneous entries. We have manually verified and corrected around 240,000 pairs to clean up the corpus. As a result, we arrive at a set of 560,768 EnglishChinese (EC) pairs that follow the Chinese phonetic rules, and a set of 83,403 English-Japanese Kanji (EJ) pairs, which follow the Japanese phonetic rules, and the rest 29,219 pairs (REST) being labeled as incorrect transliterations. Next we conduct three experiments to study 1) alignment entropy vs. F-scor</context>
<context position="21929" citStr="Li et al., 2007" startWordPosition="3556" endWordPosition="3559">ation by maximizing the joint probability of the source and target names P({ei}, {cj}), where the source and target names are sequences of English and Chinese grapheme tokens. The joint probability is expressed as a chain product of a series of conditional probabilities of token pairs P({ei}, {cj}) = P((ek, ck)|(�ek−1, ck−1)), k = 1... N, where we limit the history to one preceding pair, resulting in a bigram model. The conditional probabilities for token pairs are estimated from the aligned training corpus. We use this model because it was shown to be simple yet accurate (Ekbal et al., 2006; Li et al., 2007b). We train a model for each of the 114 phonological alignments and the 80 affinity alignments in Section 5.1 and conduct transliteration experiment on the Xinhua test data. During transliteration, an input English name is first decoded into a lattice of all possible English and Chinese grapheme token pairs. Then the joint source-channel transliteration model is used to score the lattice to obtain a ranked list of m most likely Chinese transliterations (m-best list). We measure transliteration accuracy as the mean reciprocal rank (MRR) (Kantor and Voorhees, 2000). If there is only one correct</context>
<context position="23499" citStr="Li et al., 2007" startWordPosition="3822" endWordPosition="3825">ks across all words in the test set. It is commonly used as a measure of transliteration accuracy, and also allows us to make a direct comparison with other reported work (Li et al., 2007b). We take m = 20 and measure MRR on Xinhua test set for each alignment of Xinhua training set as described in Section 5.1. We report MRR and the alignment entropy in Figures 6a and 7a for the affinity and phonological alignments respectively. The highest MRR we achieve is 0.771 for affinity alignments and 0.773 for phonological alignments. This is a significant improvement over the MRR of 0.708 reported in (Li et al., 2007b) on the same data. We also observe that the phonological alignment technique produces, on average, better alignments than the affinity alignment technique in terms of both the alignment entropy and MRR. We also report the MRR and F-scores for each alignment in Figures 6b and 7b, from which we observe that alignment entropy has stronger correlation with MRR than F-score does. The Spearman’s rank correlation coefficients are −0.89 and −0.88 for data in Figure 6a and 7a respectively. This once again demonstrates the desired property of alignment entropy as an evaluation metric of alignment. To </context>
</contexts>
<marker>Li, Sim, Kuo, Dong, 2007</marker>
<rawString>Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui Dong. 2007b. Semantic transliteration of personal names. In Proc. ACL, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<title>LDC ChineseEnglish name entity lists LDC2005T34.</title>
<date>2005</date>
<contexts>
<context position="16303" citStr="Consortium, 2005" startWordPosition="2623" endWordPosition="2624">ct and will show that this estimate is a good indicator of the alignment quality, and is as effective as the Fscore, but without the need for a gold standard reference. A lower alignment entropy suggests that each Chinese token tends to be mapped to fewer distinct English tokens, reflecting better consistency. We expect a good alignment to have a sharp cross-lingual mapping with low alignment entropy. 5 Experiments We use two transliteration corpora: Xinhua corpus (Xinhua News Agency, 1992) of 37,637 personal name pairs and LDC Chinese-English 139 named entity list LDC2005T34 (Linguistic Data Consortium, 2005), containing 673,390 personal name pairs. The LDC corpus is referred to as LDC05 for short hereafter. For the results to be comparable with other studies, we follow the same splitting of Xinhua corpus as that in (Li et al., 2007b) having a training and testing set of 34,777 and 2,896 names respectively. In contrast to the well edited Xinhua corpus, LDC05 contains erroneous entries. We have manually verified and corrected around 240,000 pairs to clean up the corpus. As a result, we arrive at a set of 560,768 EnglishChinese (EC) pairs that follow the Chinese phonetic rules, and a set of 83,403 E</context>
</contexts>
<marker>Consortium, 2005</marker>
<rawString>Linguistic Data Consortium. 2005. LDC ChineseEnglish name entity lists LDC2005T34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen M Meng</author>
<author>Wai-Kit Lo</author>
<author>Berlin Chen</author>
<author>Karen Tang</author>
</authors>
<title>Generate phonetic cognates to handle name entities in English-Chinese cross-language spoken document retrieval.</title>
<date>2001</date>
<booktitle>In Proc. ASRU.</booktitle>
<contexts>
<context position="4307" citStr="Meng et al., 2001" startWordPosition="671" endWordPosition="674">of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 136–144, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Related Work A number of transliteration studies have touched on the alignment issue as a part of the transliteration modeling process, where alignment is needed at levels of graphemes and phonemes. In their seminal paper Knight and Graehl (1998) described a transliteration approach that transfers the grapheme representation of a word via the phonetic representation, which is known as phonemebased transliteration technique (Virga and Khudanpur, 2003; Meng et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspo</context>
</contexts>
<marker>Meng, Lo, Chen, Tang, 2001</marker>
<rawString>Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen Tang. 2001. Generate phonetic cognates to handle name entities in English-Chinese cross-language spoken document retrieval. In Proc. ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="6536" citStr="Mihalcea and Pedersen, 2003" startWordPosition="1037" endWordPosition="1040">erformance. Although there are many studies of evaluation metrics of word alignment for MT (Lambert, 2008), there has been much less reported work on evaluation metrics of transliteration alignment. In MT, the quality of training corpus alignment A is often measured relatively to the gold standard, or the ground truth alignment !9, which is a manual alignment of the corpus or a part of it. Three evaluation metrics are used: precision, recall, and F-score, the latter being a function of the former two. They indicate how close the alignment under investigation is to the gold standard alignment (Mihalcea and Pedersen, 2003). Denoting the number of cross-lingual mappings that are common in both A and !9 as CAG, the number of cross-lingual mappings in A as CA and the number of cross-lingual mappings in !9 as CG, precision Pr is given as CAG/CA, recall Rc as CAG/CG and F-score as 2Pr · Rc/(Pr + Rc). Note that these metrics hinge on the availability of the gold standard, which is often not available. In this paper we propose a novel evaluation metric for transliteration alignment grounded on the information theory. One important property of this metric is that it does not require a gold standard alignment as a refer</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Rada Mihalcea and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Proc. HLT-NAACL, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
</authors>
<title>An EnglishKorean transliteration model using pronunciation and contextual rules.</title>
<date>2002</date>
<booktitle>In Proc. COLING</booktitle>
<contexts>
<context position="4627" citStr="Oh and Choi, 2002" startWordPosition="726" endWordPosition="729">graphemes and phonemes. In their seminal paper Knight and Graehl (1998) described a transliteration approach that transfers the grapheme representation of a word via the phonetic representation, which is known as phonemebased transliteration technique (Virga and Khudanpur, 2003; Meng et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). In the phoneme-based technique where an intermediate level o</context>
<context position="11943" citStr="Oh and Choi (2002)" startWordPosition="1936" endWordPosition="1939">hinese phonemes d(ei, cpk) = − log P(cpk|ei) to perform the initial alignment between {ei} and {cpk} via dynamic programming, followed by the EM iterations until convergence. The estimates for P(cpk|epn) and P(epn|ei) are obtained from the affinity matrices: the former from the alignment of English and Chinese phonetic representations, the latter from the alignment of English words and their phonetic representations. 3.2.2 Phonological alignment Alignment between the phonetic representations of source and target words can also be achieved using the linguistic knowledge of phonetic similarity. Oh and Choi (2002) define classes of 138 phonemes and assign various distances between phonemes of different classes. In contrast, we make use of phonological descriptors to define the similarity between phonemes in this paper. Perhaps the most common way to measure the phonetic similarity is to compute the distances between phoneme features (Kessler, 2005). Such features have been introduced in many ways, such as perceptual attributes or articulatory attributes. Recently, Tao et al. (2006) and Yoon et al. (2007) have studied the use of phonological features and manually assigned phonological distance to measur</context>
</contexts>
<marker>Oh, Choi, 2002</marker>
<rawString>Jong-Hoon Oh and Key-Sun Choi. 2002. An EnglishKorean transliteration model using pronunciation and contextual rules. In Proc. COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
</authors>
<title>Machine learning based english-to-korean transliteration using grapheme and phoneme information.</title>
<date>2005</date>
<journal>IEICE Trans. Information and Systems,</journal>
<volume>88</volume>
<pages>1748</pages>
<contexts>
<context position="5376" citStr="Oh and Choi, 2005" startWordPosition="849" endWordPosition="852"> which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). In the phoneme-based technique where an intermediate level of phonetic representation is used as the pivot, alignment between graphemes and phonemes of the source and target words is needed (Oh and Choi, 2005). If source and target languages have different phoneme sets, alignment between the the different phonemes is also required (Knight and Graehl, 1998). Although the direct orthographic mapping approach advocates a direct transfer of grapheme at run-time, we still need to establish the grapheme correspondence at the model training stage, when phoneme level alignment can help. It is apparent that the quality of transliteration alignment of a training corpus has a significant impact on the resulting transliteration model and its performance. Although there are many studies of evaluation metrics of</context>
</contexts>
<marker>Oh, Choi, 2005</marker>
<rawString>Jong-Hoon Oh and Key-Sun Choi. 2005. Machine learning based english-to-korean transliteration using grapheme and phoneme information. IEICE Trans. Information and Systems, E88-D(7):1737– 1748.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Qu</author>
<author>Gregory Grefenstette</author>
</authors>
<title>Finding ideographic representations of Japanese names written in Latin script via language identification and corpus validation.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="2027" citStr="Qu and Grefenstette (2004)" startWordPosition="299" endWordPosition="303">respondence between graphemes or phonemes between a transliteration pair, also known as transliteration alignment. One area is the generative transliteration modeling (Knight and Graehl, 1998), which studies how to convert a word from one language to another using statistical models. Since the models are trained on an aligned parallel corpus, the resulting statistical models can only be as good as the alignment of the corpus. Another area is the transliteration validation, which studies the ways to validate transliteration pairs. For example Knight and Graehl (1998) use the lexicon frequency, Qu and Grefenstette (2004) use the statistics in a monolingual corpus and the Web, Kuo et al. (2007) use probabilities estimated from the transliteration model to validate transliteration candidates. In this paper, we propose using the alignment distance between the a bilingual pair of words to establish the evidence of transliteration candidacy. An example of transliteration pair alignment is shown in Figure 1. grapheme tokens source graphemes target graphemes Figure 1: An example of grapheme alignment (Alice, ]IJI7WT), where a Chinese grapheme, a character, is aligned to an English grapheme token. Like the word align</context>
<context position="26098" citStr="Qu and Grefenstette, 2004" startWordPosition="4249" endWordPosition="4252">averages of five values produced in the five-fold cross-validations. We observe a clear correlation between the alignment entropy and transliteration accuracy expressed by MRR on LDC05 corpus, similar to that on Xinhua corpus, with the Spearman’s rank correlation coefficient of −0.77. We obtain the highest average MRR of 0.720 on the EC set. 5.3 Validating transliteration using alignment measure Transliteration validation is a hypothesis test that decides whether a given transliteration pair is genuine or not. Instead of using the lexicon frequency (Knight and Graehl, 1998) or Web statistics (Qu and Grefenstette, 2004), we propose validating transliteration pairs according to the alignment distance D between the aligned English graphemes and Chinese phonemes (see equations (2) and (5)). A distance function d(ei7 cpk) is established from each alignment on the Xinhua training set as discussed in Section 5.2. An audit of LDC05 corpus groups the corpus into three sets: an English-Chinese (EC) set of 560,768 samples, an English-Japanese (EJ) set of 83,403 samples and the REST set of 29,219 Figure 7: Mean reciprocal ratio on Xinhua test set vs. alignment entropy and F-score for models trained with different phono</context>
</contexts>
<marker>Qu, Grefenstette, 2004</marker>
<rawString>Yan Qu and Gregory Grefenstette. 2004. Finding ideographic representations of Japanese names written in Latin script via language identification and corpus validation. In Proc. ACL, pages 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Tao</author>
<author>Su-Youn Yoon</author>
<author>Andrew Fisterd</author>
<author>Richard Sproat</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Unsupervised named entity transliteration using temporal and phonetic correlation.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>250--257</pages>
<contexts>
<context position="12420" citStr="Tao et al. (2006)" startWordPosition="2008" endWordPosition="2011">ic representations of source and target words can also be achieved using the linguistic knowledge of phonetic similarity. Oh and Choi (2002) define classes of 138 phonemes and assign various distances between phonemes of different classes. In contrast, we make use of phonological descriptors to define the similarity between phonemes in this paper. Perhaps the most common way to measure the phonetic similarity is to compute the distances between phoneme features (Kessler, 2005). Such features have been introduced in many ways, such as perceptual attributes or articulatory attributes. Recently, Tao et al. (2006) and Yoon et al. (2007) have studied the use of phonological features and manually assigned phonological distance to measure the similarity of transliterated words for extracting transliterations from a comparable corpus. We adopt the binary-valued articulatory attributes as the phonological descriptors, which are used to describe the CMU and IIR phoneme sets for English and Chinese Mandarin respectively. Withgott and Chen (1993) define a feature vector of phonological descriptors for English sounds. We extend the idea by defining a 21-element binary feature vector for each English and Chinese</context>
</contexts>
<marker>Tao, Yoon, Fisterd, Sproat, Zhai, 2006</marker>
<rawString>Tao Tao, Su-Youn Yoon, Andrew Fisterd, Richard Sproat, and ChengXiang Zhai. 2006. Unsupervised named entity transliteration using temporal and phonetic correlation. In Proc. EMNLP, pages 250–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Virga</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Transliteration of proper names in cross-lingual information retrieval.</title>
<date>2003</date>
<booktitle>In Proc. ACL MLNER.</booktitle>
<contexts>
<context position="4288" citStr="Virga and Khudanpur, 2003" startWordPosition="666" endWordPosition="670">L I C E e5 136 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 136–144, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Related Work A number of transliteration studies have touched on the alignment issue as a part of the transliteration modeling process, where alignment is needed at levels of graphemes and phonemes. In their seminal paper Knight and Graehl (1998) described a transliteration approach that transfers the grapheme representation of a word via the phonetic representation, which is known as phonemebased transliteration technique (Virga and Khudanpur, 2003; Meng et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the sys</context>
<context position="9832" citStr="Virga and Khudanpur, 2003" startWordPosition="1606" endWordPosition="1609">nimizes alignment distance D: ED = − log P = − log P(cθ(i)|ei) (5) i In other words, equations (2) and (5) are the same when we have the distance function d(ei, cj) = − log P(cj|ei). Minimizing the overall distance over a training corpus, we conduct EM iterations until the convergence is achieved. This technique solely relies on the affinity statistics derived from training corpus, thus is called grapheme affinity alignment. It is also equally applicable for alignment between a pair of symbol sequences representing either graphemes or phonemes. (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). 3.2 Grapheme alignment via phonemes Transliteration is about finding phonological equivalent. It is therefore a natural choice to use the phonetic representation as the pivot. It is common though that the sound inventory differs from one language to another, resulting in different phonetic representations for source and target words. Continuing with the earlier example, Figure 2: An example of English-Chinese transliteration alignment via phonetic representations. Figure 2 shows the correspondence between the graphemes and phonemes of English word “Alice” and its Chinese transliteration, wit</context>
</contexts>
<marker>Virga, Khudanpur, 2003</marker>
<rawString>Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of proper names in cross-lingual information retrieval. In Proc. ACL MLNER.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Cornelia Maria Verspoor</author>
</authors>
<title>Automatic English-Chinese name transliteration for development of multilingual resources.</title>
<date>1998</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>1352--1356</pages>
<contexts>
<context position="4915" citStr="Wan and Verspoor, 1998" startWordPosition="771" endWordPosition="774">g et al., 2001; Jung et al., 2000; Gao et al., 2004). Another technique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). In the phoneme-based technique where an intermediate level of phonetic representation is used as the pivot, alignment between graphemes and phonemes of the source and target words is needed (Oh and Choi, 2005). If source and target languages have different phoneme sets, alignment between the the different phonemes is also required (Knight and Gra</context>
</contexts>
<marker>Wan, Verspoor, 1998</marker>
<rawString>Stephen Wan and Cornelia Maria Verspoor. 1998. Automatic English-Chinese name transliteration for development of multilingual resources. In Proc. COLING, pages 1352–1356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Withgott</author>
<author>F R Chen</author>
</authors>
<title>Computational models of American speech. Centre for the study of language and information.</title>
<date>1993</date>
<contexts>
<context position="12853" citStr="Withgott and Chen (1993)" startWordPosition="2073" endWordPosition="2076"> the distances between phoneme features (Kessler, 2005). Such features have been introduced in many ways, such as perceptual attributes or articulatory attributes. Recently, Tao et al. (2006) and Yoon et al. (2007) have studied the use of phonological features and manually assigned phonological distance to measure the similarity of transliterated words for extracting transliterations from a comparable corpus. We adopt the binary-valued articulatory attributes as the phonological descriptors, which are used to describe the CMU and IIR phoneme sets for English and Chinese Mandarin respectively. Withgott and Chen (1993) define a feature vector of phonological descriptors for English sounds. We extend the idea by defining a 21-element binary feature vector for each English and Chinese phoneme. Each element of the feature vector represents presence or absence of a phonological descriptor that differentiates various kinds of phonemes, e.g. vowels from consonants, front from back vowels, nasals from fricatives, etc1. In this way, a phoneme is described by a feature vector. We express the similarity between two phonemes by the Hamming distance, also called the phonological distance, between the two feature vector</context>
</contexts>
<marker>Withgott, Chen, 1993</marker>
<rawString>M. M. Withgott and F. R. Chen. 1993. Computational models of American speech. Centre for the study of language and information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinhua News Agency</author>
</authors>
<title>Chinese transliteration offoreign personal names.</title>
<date>1992</date>
<publisher>The Commercial Press.</publisher>
<contexts>
<context position="16181" citStr="Agency, 1992" startWordPosition="2606" endWordPosition="2607"> entropy indicates the uncertainty of mapping between the English and Chinese tokens resulting from alignment. We expect and will show that this estimate is a good indicator of the alignment quality, and is as effective as the Fscore, but without the need for a gold standard reference. A lower alignment entropy suggests that each Chinese token tends to be mapped to fewer distinct English tokens, reflecting better consistency. We expect a good alignment to have a sharp cross-lingual mapping with low alignment entropy. 5 Experiments We use two transliteration corpora: Xinhua corpus (Xinhua News Agency, 1992) of 37,637 personal name pairs and LDC Chinese-English 139 named entity list LDC2005T34 (Linguistic Data Consortium, 2005), containing 673,390 personal name pairs. The LDC corpus is referred to as LDC05 for short hereafter. For the results to be comparable with other studies, we follow the same splitting of Xinhua corpus as that in (Li et al., 2007b) having a training and testing set of 34,777 and 2,896 names respectively. In contrast to the well edited Xinhua corpus, LDC05 contains erroneous entries. We have manually verified and corrected around 240,000 pairs to clean up the corpus. As a res</context>
</contexts>
<marker>Agency, 1992</marker>
<rawString>Xinhua News Agency. 1992. Chinese transliteration offoreign personal names. The Commercial Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LiLi Xu</author>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Modeling impression in probabilistic transliteration into Chinese.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>242--249</pages>
<contexts>
<context position="4972" citStr="Xu et al., 2006" startWordPosition="783" endWordPosition="786">echnique is to directly transfer the grapheme, known as direct orthographic mapping, that was shown to be simple and effective (Li et al., 2004). Some other approaches that use both source graphemes and phonemes were also reported with good performance (Oh and Choi, 2002; Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004). To align a bilingual training corpus, some take a phonological approach, in which the crafted mapping rules encode the prior linguistic knowledge about the source and target languages directly into the system (Wan and Verspoor, 1998; Meng et al., 2001; Jiang et al., 2007; Xu et al., 2006). Others adopt a statistical approach, in which the affinity between phonemes or graphemes is learned from the corpus (Gao et al., 2004; AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003). In the phoneme-based technique where an intermediate level of phonetic representation is used as the pivot, alignment between graphemes and phonemes of the source and target words is needed (Oh and Choi, 2005). If source and target languages have different phoneme sets, alignment between the the different phonemes is also required (Knight and Graehl, 1998). Although the direct orthographic mapping appr</context>
</contexts>
<marker>Xu, Fujii, Ishikawa, 2006</marker>
<rawString>LiLi Xu, Atsushi Fujii, and Tetsuya Ishikawa. 2006. Modeling impression in probabilistic transliteration into Chinese. In Proc. EMNLP, pages 242–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su-Youn Yoon</author>
<author>Kyoung-Young Kim</author>
<author>Richard Sproat</author>
</authors>
<title>Multilingual transliteration using feature based phonetic method.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>112--119</pages>
<contexts>
<context position="12443" citStr="Yoon et al. (2007)" startWordPosition="2013" endWordPosition="2016">source and target words can also be achieved using the linguistic knowledge of phonetic similarity. Oh and Choi (2002) define classes of 138 phonemes and assign various distances between phonemes of different classes. In contrast, we make use of phonological descriptors to define the similarity between phonemes in this paper. Perhaps the most common way to measure the phonetic similarity is to compute the distances between phoneme features (Kessler, 2005). Such features have been introduced in many ways, such as perceptual attributes or articulatory attributes. Recently, Tao et al. (2006) and Yoon et al. (2007) have studied the use of phonological features and manually assigned phonological distance to measure the similarity of transliterated words for extracting transliterations from a comparable corpus. We adopt the binary-valued articulatory attributes as the phonological descriptors, which are used to describe the CMU and IIR phoneme sets for English and Chinese Mandarin respectively. Withgott and Chen (1993) define a feature vector of phonological descriptors for English sounds. We extend the idea by defining a 21-element binary feature vector for each English and Chinese phoneme. Each element </context>
</contexts>
<marker>Yoon, Kim, Sproat, 2007</marker>
<rawString>Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat. 2007. Multilingual transliteration using feature based phonetic method. In Proc. ACL, pages 112–119.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>