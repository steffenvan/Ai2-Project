<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.926559">
A Fast Fertility Hidden Markov Model for Word Alignment Using MCMC
</title>
<author confidence="0.995937">
Shaojun Zhao and Daniel Gildea
</author>
<affiliation confidence="0.999188">
Department of Computer Science
University of Rochester
</affiliation>
<sectionHeader confidence="0.98882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999611071428571">
A word in one language can be translated to
zero, one, or several words in other languages.
Using word fertility features has been shown
to be useful in building word alignment mod-
els for statistical machine translation. We built
a fertility hidden Markov model by adding fer-
tility to the hidden Markov model. This model
not only achieves lower alignment error rate
than the hidden Markov model, but also runs
faster. It is similar in some ways to IBM
Model 4, but is much easier to understand. We
use Gibbs sampling for parameter estimation,
which is more principled than the neighbor-
hood method used in IBM Model 4.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999826078431373">
IBM models and the hidden Markov model (HMM)
for word alignment are the most influential statistical
word alignment models (Brown et al., 1993; Vogel et
al., 1996; Och and Ney, 2003). There are three kinds
of important information for word alignment mod-
els: lexicality, locality and fertility. IBM Model 1
uses only lexical information; IBM Model 2 and the
hidden Markov model take advantage of both lexi-
cal and locality information; IBM Models 4 and 5
use all three kinds of information, and they remain
the state of the art despite the fact that they were de-
veloped almost two decades ago.
Recent experiments on large datasets have shown
that the performance of the hidden Markov model is
very close to IBM Model 4. Nevertheless, we be-
lieve that IBM Model 4 is essentially a better model
because it exploits the fertility of words in the tar-
get language. However, IBM Model 4 is so com-
plex that most researches use the GIZA++ software
package (Och and Ney, 2003), and IBM Model 4 it-
self is treated as a black box. The complexity in IBM
Model 4 makes it hard to understand and to improve.
Our goal is to build a model that includes lexicality,
locality, and fertility; and, at the same time, to make
it easy to understand. We also want it to be accurate
and computationally efficient.
There have been many years of research on word
alignment. Our work is different from others in
essential ways. Most other researchers take either
the HMM alignments (Liang et al., 2006) or IBM
Model 4 alignments (Cherry and Lin, 2003) as in-
put and perform post-processing, whereas our model
is a potential replacement for the HMM and IBM
Model 4. Directly modeling fertility makes our
model fundamentally different from others. Most
models have limited ability to model fertility. Liang
et al. (2006) learn the alignment in both translation
directions jointly, essentially pushing the fertility to-
wards 1. ITG models (Wu, 1997) assume the fer-
tility to be either zero or one. It can model phrases,
but the phrase has to be contiguous. There have been
works that try to simulate fertility using the hidden
Markov model (Toutanova et al., 2002; Deng and
Byrne, 2005), but we prefer to model fertility di-
rectly.
Our model is a coherent generative model that
combines the HMM and IBM Model 4. It is easier to
understand than IBM Model 4 (see Section 3). Our
model also removes several undesired properties in
IBM Model 4. We use Gibbs sampling instead of a
heuristic-based neighborhood method for parameter
</bodyText>
<page confidence="0.97772">
596
</page>
<note confidence="0.818181">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999418">
estimation. Our distortion parameters are similar to
IBM Model 2 and the HMM, while IBM Model 4
uses inverse distortion (Brown et al., 1993). Our
model assumes that fertility follows a Poisson distri-
bution, while IBM Model 4 assumes a multinomial
distribution, and has to learn a much larger number
of parameters, which makes it slower and less reli-
able. Our model is much faster than IBM Model 4.
In fact, we will show that it is also faster than the
HMM, and has lower alignment error rate than the
HMM.
Parameter estimation for word alignment models
that model fertility is more difficult than for mod-
els without fertility. Brown et al. (1993) and Och
and Ney (2003) first compute the Viterbi alignments
for simpler models, then consider only some neigh-
bors of the Viterbi alignments for modeling fertil-
ity. If the optimal alignment is not in those neigh-
bors, this method will not be able find the opti-
mal alignment. We use the Markov Chain Monte
Carlo (MCMC) method for training and decoding,
which has nice probabilistic guarantees. DeNero et
al. (2008) applied the Markov Chain Monte Carlo
method to word alignment for machine translation;
they do not model word fertility.
</bodyText>
<sectionHeader confidence="0.987438" genericHeader="method">
2 Statistical Word Alignment Models
</sectionHeader>
<subsectionHeader confidence="0.930486">
2.1 Alignment and Fertility
</subsectionHeader>
<bodyText confidence="0.984381147058824">
Given a source sentence fJ1 = f1, f2,.. . , fJ and a
target sentence eI1 = e1, e2, ... , eI, we define the
alignments between the two sentences as a subset of
the Cartesian product of the word positions. Fol-
lowing Brown et al. (1993), we assume that each
source word is aligned to exactly one target word.
We denote as aJ1 = a1, a2, . . . , aJ the alignments
between fJ1 and eI1. When a word fj is not aligned
with any word e, aj is 0. For convenience, we add
an empty word ǫ to the target sentence at position 0
(i.e., e0 = ǫ). However, as we will see, we have
to add more than one empty word for the HMM.
In order to compute the “jump probability” in the
HMM model, we need to know the position of the
aligned target word for the previous source word. If
the previous source word aligns to an empty word,
we could use the position of the empty word to indi-
cate the nearest previous source word that does not
align to an empty word. For this reason, we use a
total of I + 1 empty words for the HMM model1.
Moore (2004) also suggested adding multiple empty
words to the target sentence for IBM Model 1. After
we add I +1 empty words to the target sentence, the
alignment is a mapping from source to target word
positions:
a:j—*i,i=aj
where j = 1, 2, ... , J and i = 1, 2, ... , 2I + 1.
Words from position I + 1 to 2I + 1 in the target
sentence are all empty words.
We allow each source word to align with exactly
one target word, but each target word may align with
multiple source words.
The fertility φi of a word ei at position i is defined
as the number of aligned source words:
</bodyText>
<equation confidence="0.962321">
J
φi = δ(aj,i)
j=1
where δ is the Kronecker delta function:
� 1 if x = y
δ(x, y) = 0 otherwise
</equation>
<bodyText confidence="0.994353">
In particular, the fertility of all empty words in
the target sentence is 2I+1 We define
</bodyText>
<equation confidence="0.9304698">
g �i=I+1 φi. E _
E2I+1
i=I+1 φi. For a bilingual sentence pair e2I+1
1 and
fJ1 , we have EIi=1 φi + φǫ = J.
</equation>
<bodyText confidence="0.9999455">
The inverted alignments for position i in the tar-
get sentence are a set Bi, such that each element in
Bi is aligned with i, and all alignments of i are in
Bi. Inverted alignments are explicitly used in IBM
Models 3, 4 and 5, but not in our model, which is
one reason that our model is easier to understand.
</bodyText>
<subsectionHeader confidence="0.434786">
2.2 IBM Model 1 and HMM
</subsectionHeader>
<bodyText confidence="0.99037725">
IBM Model 1 and the HMM are both generative
models, and both start by defining the probabil-
ity of alignments and source sentence given the
target sentence: P(aJ1 , fJ1 |e2I+1
</bodyText>
<footnote confidence="0.956660142857143">
1 ); the data likeli-
hood can be computed by summing over alignments:
1If fj−1 does not align with an empty word and fj aligns
with an empty word, we want to record the position of the target
word that fj−1 aligns with. There are I + 1 possibilities: fj is
the first word in the source sentence, or fj−1 aligns with one of
the target word.
</footnote>
<page confidence="0.97015">
597
</page>
<equation confidence="0.859637">
P(fJ1 |e2I+1
1 ) = PaJ P(aJ1 ,fJ1 |eiI+1). The align-
</equation>
<bodyText confidence="0.913226">
ments aJ1 are the hidden variables. The expectation
maximization algorithm is used to learn the parame-
ters such that the data likelihood is maximized.
</bodyText>
<subsectionHeader confidence="0.501729">
Without loss of generality, P(aJ1 , fJ1 |e2I+1
</subsectionHeader>
<bodyText confidence="0.9494098">
1 ) can
be decomposed into length probabilities, distor-
tion probabilities (also called alignment probabil-
ities), and lexical probabilities (also called transla-
tion probabilities):
</bodyText>
<equation confidence="0.987418125">
P(aJ1 , fJ1 |e2I+1
1 )
�
P(fj|fj−1
1 , aj 1, e2I+1
1 )
where P(J|e2I+1
1 ) is a length probability,
P(aj|fj−1
1 , aj−1
1 , e2I+1
1 ) is a distortion prob-
ability and P(fj|fj−1
1 , aj1, e2I+1
1 ) is a lexical
probability.
</equation>
<bodyText confidence="0.7970435">
IBM Model 1 assumes a uniform distortion prob-
ability, a length probability that depends only on the
length of the target sentence, and a lexical probabil-
ity that depends only on the aligned target word:
</bodyText>
<equation confidence="0.993261">
J
P(a1 fi |eil+�) = (2 +
(J|1�JYP(fj|eaj)
j=1
</equation>
<bodyText confidence="0.999981">
The hidden Markov model assumes a length prob-
ability that depends only on the length of the target
sentence, a distortion probability that depends only
on the previous alignment and the length of the tar-
get sentence, and a lexical probability that depends
only on the aligned target word:
</bodyText>
<equation confidence="0.996679">
P(aJ1 , fJ1 |e2I+1
1 ) =
P(aj|aj−1, I)P(fj|eaj)
</equation>
<bodyText confidence="0.9997945">
In order to make the HMM work correctly, we en-
force the following constraints (Och and Ney, 2003):
</bodyText>
<equation confidence="0.999931333333333">
P(i + I + 1|i′, I) = p0δ(i, i′)
P(i + I + 1|i′ + I + 1, I) = p0δ(i, i′)
P(i|i′ + I + 1,I) = P(i|i′,I)
</equation>
<bodyText confidence="0.999848363636364">
where the first two equations imply that the proba-
bility of jumping to an empty word is either 0 or p0,
and the third equation implies that the probability of
jumping from a non-empty word is the same as the
probability ofjumping from the corespondent empty
word.
The absolute position in the HMM is not impor-
tant, because we re-parametrize the distortion prob-
ability in terms of the distance between adjacent
alignment points (Vogel et al., 1996; Och and Ney,
2003):
</bodyText>
<equation confidence="0.994238">
P (i|i′, I) = c(i − i′) P
i″c(i′′ − i′)
</equation>
<bodyText confidence="0.999700676470588">
where c( ) is the count of jumps of a given distance.
In IBM Model 1, the word order does not mat-
ter. The HMM is more likely to align a source
word to a target word that is adjacent to the previ-
ous aligned target word, which is more suitable than
IBM Model 1 because adjacent words tend to form
phrases.
For these two models, in theory, the fertility for
a target word can be as large as the length of the
source sentence. In practice, the fertility for a target
word in IBM Model 1 is not very big except for rare
target words, which can become a garbage collector,
and align to many source words (Brown et al., 1993;
Och and Ney, 2003; Moore, 2004). The HMM is
less likely to have this garbage collector problem be-
cause of the alignment probability constraint. How-
ever, fertility is an inherent cross-language property
and these two models cannot assign consistent fer-
tility to words. This is our motivation for adding fer-
tility to these two models, and we expect that the re-
sulting models will perform better than the baseline
models. Because the HMM performs much better
than IBM Model 1, we expect that the fertility hid-
den Markov model will perform much better than
the fertility IBM Model 1. Throughout the paper,
“our model” refers to the fertility hidden Markov
model.
Due to space constraints, we are unable to pro-
vide details for IBM Models 3, 4 and 5; see Brown
et al. (1993) and Och and Ney (2003). But we want
to point out that the locality property modeled in the
HMM is missing in IBM Model 3, and is modeled
invertedly in IBM Model 4. IBM Model 5 removes
deficiency (Brown et al., 1993; Och and Ney, 2003)
</bodyText>
<figure confidence="0.99329185">
J
Y
j=1
= P(J|e2I+1
1 )
P(aj, fj|fj−1
1 , aj−1
1 , e2I+1
1 )
J
Y
j=1
= P(J|e2I+1
1 )
(P(aj|f1j−1,aj1−1,e12I+1)
×
J
Y
j=1
P(J|I)
</figure>
<page confidence="0.988863">
598
</page>
<bodyText confidence="0.999752">
from IBM Model 4, but it is computationally very
expensive due to the larger number of parameters
than IBM Model 4, and IBM Model 5 often provides
no improvement on alignment accuracy.
</bodyText>
<sectionHeader confidence="0.97522" genericHeader="method">
3 Fertility Hidden Markov Model
</sectionHeader>
<bodyText confidence="0.960626">
Our fertility IBM Model 1 and fertility HMM
are both generative models and start by defin-
ing the probability of fertilities (for each
non-empty target word and all empty words),
alignments, and the source sentence given
the target sentence: P(φI1, φǫ, aJ1 , fJ1 |e2I+1
</bodyText>
<equation confidence="0.76035">
1 );
</equation>
<bodyText confidence="0.7681625">
the data likelihood can be computed by
summing over fertilities and alignments:
</bodyText>
<equation confidence="0.985266">
P(fJ1 |e2I+1
1 ) _ P(φI1, φǫ, aJ1, f1 |eil+1)•
φ1,φǫ,a1
</equation>
<bodyText confidence="0.990469142857143">
The fertility for a non-empty word ei is a random
variable φi, and we assume φi follows a Poisson dis-
tribution Poisson(φi; λ(ei)). The sum of the fer-
tilities of all the empty words (φǫ) grows with the
length of the target sentence. Therefore, we assume
that φǫ follows a Poisson distribution with parameter
Iλ(ǫ).
</bodyText>
<equation confidence="0.981301375">
Now P(φI1, φǫ, aJ1, fJ1 |e2I+1
1 ) can be decomposed
in the following way:
P (φI 1, φǫ, aJ 1 , fJ 1 |e2I+1
1 )
_ P(φI1|e2I+1
1 )P(φǫ|φI1, e2I+1
1 ) ×
J
j−1 j−1 2I+1 I
P(aj , fj  |f1 , a1 , e1 φ1,φǫ)
λ(ei)φie−λ(ei) ×
φi!
(Iλ(ǫ))φǫ e−Iλ(ǫ) ×
φǫ!
�
P(aj|fj−1
1 , aj−1
1 , e2I+1
1 , φI1, φǫ) ×
�
P(fj|fj−1
1 , aj1, e2I+1
1 , φI1, φǫ)
</equation>
<bodyText confidence="0.999914625">
Superficially, we only try to model the length
probability more accurately. However, we also en-
force the fertility for the same target word across the
corpus to be consistent. The expected fertility for a
non-empty word ei is λ(ei), and the expected fertil-
ity for all empty words is Iλ(ǫ). Any fertility value
has a non-zero probability, but fertility values that
are further away from the mean have low probabil-
ity. IBM Models 3, 4, and 5 use a multinomial distri-
bution for fertility, which has a much larger number
of parameters to learn. Our model has only one pa-
rameter for each target word, which can be learned
more reliably.
In the fertility IBM Model 1, we assume that
the distortion probability is uniform, and the lexical
probability depends only on the aligned target word:
</bodyText>
<equation confidence="0.998868111111111">
P (φI 1, φǫ, aJ 1 , fJ 1 |e2I+1
1 )
λ(ei)φie−λ(ei)
×
φi!
(Iλ(ǫ))φǫ e−(Iλ(ǫ))
×
φǫ!
P(fj|eaj) (1)
</equation>
<bodyText confidence="0.999801">
In the fertility HMM, we assume that the distor-
tion probability depends only on the previous align-
ment and the length of the target sentence, and that
the lexical probability depends only on the aligned
target word:
</bodyText>
<equation confidence="0.969698709677419">
P (φI 1, φǫ, aJ 1 , fJ 1 |e2I+1
1 )
λ(ei)φie−λ(ei)
×
φi!
(Iλ(ǫ))φǫ e−(Iλ(ǫ))
×
φǫ!
YJ P(aj|aj−1, I)P(fj|eaj) (2)
j=1
When we compute P(fJ1 |e2I+1
1 ), we only sum
over fertilities that agree with the alignments:
P(aJ1 , fJ1 |e2I+1
1 )
Y
j=1
_ YI
i=1
YJ
j=1
_ YI
i=1
1
YJ
j=1
(2I + 1)J
_ YI
i=1
PfJ2I+ X1) _
(1  |e 1 aJ
</equation>
<page confidence="0.8740965">
1
599
</page>
<bodyText confidence="0.998988">
where auxiliary function is:
</bodyText>
<equation confidence="0.992091685714285">
X=
aJ
1
P˜ (aJ 1 |e2I+1
1 ,fJ 1 )log P(aJ1,fJ1 |e2I+1
1 )
X
ξ1(e)(
f
X−
e
P(f|e) − 1)
X
ξ2(a′)(
a
P(a|a′) − 1)
X−
a′
X=
φI1,φǫ
P(φI J J 2I+1
1, φǫ, a1 , f1 |e1 )
YI
i=1
XJ
j=1

δ 
P(aJ 1 ,fJ 1 |e2I+1
1 )
� I J J 2I+1
P(φ1, φǫ, a1 , f1 |e1 ) X
δ(aj,i), φi X
L(P(f|e), P(a|a′), λ(e), ξ1(e), ξ2(a′))
δ(aj,i),φǫ (3)
</equation>
<bodyText confidence="0.982153">
In the last two lines of Equation 3, φǫ and each
φi are not free variables, but are determined by
the alignments. Because we only sum over fer-
tilities that are consistent with the alignments, we
have PfJ1 P(fJ1 |e2I+1) &lt; 1, and our model is de-
ficient, similar to IBM Models 3 and 4 (Brown et
al., 1993). We can remove the deficiency for fertil-
ity IBM Model 1 by assuming a different distortion
probability: the distortion probability is 0 if fertility
is not consistent with alignments, and uniform oth-
erwise. The total number of consistent fertility and
alignments is ;! Replacing ( 1 )J with
</bodyText>
<equation confidence="0.770318">
φǫ! j lj=1 φi!* 2I+1
φǫ! j lj= 1 φi!
</equation>
<bodyText confidence="0.6395574">
J! , we have:
Because P(aJ1 , fJ1 |e2I+1
1 ) is in the exponential
family, we get a closed form for the parameters from
expected counts:
</bodyText>
<equation confidence="0.998218448979592">
=
=
Ps c(f|e; f(s), e(s))
P f Ps c(f |e; f (s), e(s))
Ps c(a|a′; f(s),e(s))
Pa Ps c(a |a′; f (s), e(s))))
Ps c(φ|e; f(s), e(s))
P (6)
s c(k|e; f(s), e(s))
where s is the number of bilingual sentences, and
c(f|e; fJ1 , e2I+1
1 ) = X P˜(aJ1 |fJ1 , e2I+1
1 ) X
aJ
1
X δ(fj, f)δ(ei, e)
j
 2I+1X XJ
δ  i=I+1 j=1
P(f|e)
P(a|a′)
λ(e) =
P˜(aJ1 |fJ1 , e2I+1
1 ) X
1
X
c(a|a′; fJ1 , e2I+1
1 ) =
aJ
P(φI J J 2I+1
1, φǫ, a1 , f1 |e1 )
= YI λ(ei)φie−λ(ei) X X δ(aj, a)δ(aj−1, a′)
i=1 j
X
c(φ|e; fJ1 , e2I+1
1 ) =
aJ
1
1
J!
YJ
j=1
X
i
P˜(aJ1 |fJ1 , e2I+1
1 ) X
φiδ(ei, e)
(Iλ(ǫ))φǫ e−(Iλ(ǫ)) X
P(fj|eaj)
</equation>
<bodyText confidence="0.999782">
In our experiments, we did not find a noticeable
change in terms of alignment accuracy by removing
the deficiency.
</bodyText>
<sectionHeader confidence="0.994474" genericHeader="method">
4 Expectation Maximization Algorithm
</sectionHeader>
<bodyText confidence="0.998961">
We estimate the parameters by maximizing
</bodyText>
<equation confidence="0.9864588">
P(fJ1 |e2I+1
1 ) using the expectation maximization
(EM) algorithm (Dempster et al., 1977). The
c(k|e; fJ1 , e1I +1) = X k(ei)δ(ei, e)
i
</equation>
<bodyText confidence="0.999861666666667">
These equations are for the fertility hidden
Markov model. For the fertility IBM Model 1, we
do not need to estimate the distortion probability.
</bodyText>
<sectionHeader confidence="0.899352" genericHeader="method">
5 Gibbs Sampling for Fertility HMM
</sectionHeader>
<bodyText confidence="0.992453">
Although we can estimate the parameters by using
the EM algorithm, in order to compute the expected
</bodyText>
<page confidence="0.987416">
600
</page>
<tableCaption confidence="0.4373185">
Algorithm 1: One iteration of E-step: draw
t samples for each aj for each sentence pair
</tableCaption>
<equation confidence="0.819582">
(fJ1 , e2I+1
1 ) in the corpus
</equation>
<bodyText confidence="0.942495818181818">
counts, we have to sum over all possible alignments
aJ1, which is, unfortunately, exponential. We devel-
oped a Gibbs sampling algorithm (Geman and Ge-
man, 1984) to compute the expected counts.
For each target sentence e2I+1 1and source sen-
tence fJ1 , we initialize the alignment aj for each
source word fj using the Viterbi alignments from
IBM Model 1. During the training stage, we try all
2I + 1 possible alignments for aj but fix all other
alignments.2 We choose alignment aj with probabil-
ity P(aj|a1, ··· aj−1, aj+1 ··· aJ, fJ1 , e2I+1
</bodyText>
<equation confidence="0.91903975">
1 ), which
can be computed in the following way:
J 2I+1
P(aj  |a1, ··· aj−1, aj+1, ··· aJ, f1 , e1 )
P(aJ1 , fJ1 |e2I+1
1 )
= � 1 ) (7)
aj P(aJ 1 , fJ 1 |e2I+1
</equation>
<bodyText confidence="0.9972045">
For each alignment variable aj, we choose t sam-
ples. We scan through the corpus many times until
we are satisfied with the parameters we learned us-
ing Equations 4, 5, and 6. This Gibbs sampling
method updates parameters constantly, so it is an
“online learning” algorithm. However, this sampling
method needs a large amount of communication be-
tween machines in order to keep the parameters up
to date if we compute the expected counts in parallel.
Instead, we do “batch learning”: we fix the parame-
ters, scan through the entire corpus and compute ex-
pected counts in parallel (E-step); then combine all
the counts together and update the parameters (M-
step). This is analogous to what IBM models and
the HMM do in the EM algorithms. The algorithm
for the E-step on one machine (all machines are in-
dependent) is in Algorithm 1.
For the fertility hidden Markov model, updating
P(aJ1 , fJ1 |e2I+1
1 ) whenever we change the alignment
aj can be done in constant time, so the complexity
of choosing t samples for all aj (j = 1, 2, ... , J) is
O(tIJ). This is the same complexity as the HMM
if t is O(I), and it has lower complexity if t is a
constant. Surprisingly, we can achieve better results
than the HMM by computing as few as 1 sample
for each alignment, so the fertility hidden Markov
model is much faster than the HMM. Even when
choosing t such that our model is 5 times faster than
the HMM, we achieve better results.
</bodyText>
<footnote confidence="0.995396">
2For fertility IBM Model 1, we only need to compute I + 1
</footnote>
<table confidence="0.445041277777778">
values because e2I+1
I+1 are identical empty words.
for (fJ1 , e2I+1
1 ) in the corpus do
Initialize aJ1 with IBM Model 1;
for t do
for j do
for i do
aj = i;
Compute P(aJ1, fJ1 |e2I+1
1 );
end
Draw a sample for aj using
Equation 7;
Update counts;
end
end
end
</table>
<bodyText confidence="0.997070851851852">
We also consider initializing the alignments using
the HMM Viterbi algorithm in the E-step. In this
case, the fertility hidden Markov model is not faster
than the HMM. Fortunately, initializing using IBM
Model 1 Viterbi does not decrease the accuracy in
any noticeable way, and reduces the complexity of
the Gibbs sampling algorithm.
In the testing stage, the sampling algorithm is the
same as above except that we keep the alignments
aJ 1 that maximize P(aJ1, fJ1 |e2I+1
1 ). We need more
samples in the testing stage because it is unlikely
to get to the optimal alignments by sampling a few
times for each alignment. On the contrary, in the
above training stage, although the samples are not
accurate enough to represent the distribution defined
by Equation 7 for each alignment aj, it is accurate
enough for computing the expected counts, which
are defined at the corpus level. Interestingly, we
found that throwing away the fertility and using the
HMM Viterbi decoding achieves same results as the
sampling approach (we can ignore the difference be-
cause it is tiny), but is faster. Therefore, we use
Gibbs sampling for learning and the HMM Viterbi
decoder for testing.
Gibbs sampling for the fertility IBM Model 1 is
similar but simpler. We omit the details here.
</bodyText>
<page confidence="0.995699">
601
</page>
<table confidence="0.9998872">
Alignment Model P R AER
IBM1 49.6 55.3 47.8
IBM1F 55.4 57.1 43.8
HMM 62.6 59.5 39.0
en → cn HMMF-1 65.4 59.1 37.9
HMMF-5 66.8 60.8 36.2
HMMF-30 67.8 62.3 34.9
IBM4 66.8 64.1 34.5
IBM1 52.6 53.7 46.9
IBM1F 55.9 56.4 43.9
HMM 66.1 62.1 35.9
cn → en HMMF-1 68.6 60.2 35.7
HMMF-5 71.1 62.2 33.5
HMMF-30 71.1 62.7 33.2
IBM4 69.3 68.5 31.1
</table>
<tableCaption confidence="0.894669">
Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1,
5, and 30 for the fertility HMM.
</tableCaption>
<figure confidence="0.998951647058823">
AER
0.48
0.46
0.44
0.42
0.38
0.36
0.34
0.32
0.4
IBM1
IBM1F
HMM
HMMF-1
HMMF-5
HMMF-30
IBM4
</figure>
<figureCaption confidence="0.999552">
Figure 1: AER comparison (en→cn)
</figureCaption>
<page confidence="0.792766">
602
</page>
<figure confidence="0.999664176470588">
AER
0.48
0.46
0.44
0.42
0.38
0.36
0.34
0.32
0.4
IBM1
IBM1F
HMM
HMMF-1
HMMF-5
HMMF-30
IBM4
</figure>
<figureCaption confidence="0.997542333333333">
Figure 2: AER comparison (cn →en)
Figure 3: Training time comparison. The training time for each model is calculated from scratch. For example, the
training time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3.
</figureCaption>
<figure confidence="0.996844857142857">
training time [seconds]
4000
2000
5000
3000
1000
0
IBM1
IBM1F
HMM
HMMF-1
HMMF-5
HMMF-30
IBM4
</figure>
<page confidence="0.99658">
603
</page>
<sectionHeader confidence="0.999054" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999925952941177">
We evaluated our model by computing the word
alignment and machine translation quality. We use
the alignment error rate (AER) as the word align-
ment evaluation criterion. Let A be the alignments
output by word alignment system, P be a set of pos-
sible alignments, and S be a set of sure alignments
both labeled by human beings. S is a subset of P.
Precision, recall, and AER are defined as follows:
AER is an extension to F-score. Lower AER is bet-
ter.
We evaluate our fertility models on a Chinese-
English corpus. The Chinese-English data taken
from FBIS newswire data, and has 380K sentence
pairs, and we use the first 100K sentence pairs as
our training data. We used hand-aligned data as ref-
erence. The Chinese-English data has 491 sentence
pairs.
We initialize IBM Model 1 and the fertility IBM
Model 1 with a uniform distribution. We smooth
all parameters (λ(e) and P(f|e)) by adding a small
value (10−8), so they never become too small. We
run both models for 5 iterations. AER results are
computed using the IBM Model 1 Viterbi align-
ments, and the Viterbi alignments obtained from the
Gibbs sampling algorithm.
We initialize the HMM and the fertility HMM
with the parameters learned in the 5th iteration of
IBM Model 1. We smooth all parameters (λ(e),
P(a|a′) and P(f|e)) by adding a small value (10−8).
We run both models for 5 iterations. AER results are
computed using traditional HMM Viterbi decoding
for both models.
It is always difficult to determine how many sam-
ples are enough for sampling algorithms. However,
both fertility models achieve better results than their
baseline models using a small amount of samples.
For the fertility IBM Model 1, we sample 10 times
for each aj, and restart 3 times in the training stage;
we sample 100 times and restart 12 times in the test-
ing stage. For the fertility HMM, we sample 30
times for each aj with no restarting in the training
stage; no sampling in the testing stage because we
use traditional HMM Viterbi decoding for testing.
More samples give no further improvement.
Initially, the fertility IBM Model 1 and fertility
HMM did not perform well. If a target word e
only appeared a few times in the training corpus, our
model cannot reliably estimate the parameter λ(e).
Hence, smoothing is needed. One may try to solve
it by forcing all these words to share a same pa-
rameter λ(einfrequent). Unfortunately, this does not
solve the problem because all infrequent words tend
to have larger fertility than they should. We solve
the problem in the following way: estimate the pa-
rameter λ(enon empty) for all non-empty words, all
infrequent words share this parameter. We consider
words that appear less than 10 times as infrequent
words.
Table 1, Figure 1, and Figure 2 shows the AER
results for different models. We can see that the fer-
tility IBM Model 1 consistently outperforms IBM
Model 1, and the fertility HMM consistently outper-
forms the HMM.
The fertility HMM not only has lower AER than
the HMM, it also runs faster than the HMM. Fig-
ure 3 show the training time for different models.
In fact, with just 1 sample for each alignment, our
model archives lower AER than the HMM, and runs
more than 5 times faster than the HMM. It is pos-
sible to use sampling instead of dynamic program-
ming in the HMM to reduce the training time with
no decrease in AER (often an increase). We con-
clude that the fertility HMM not only has better AER
results, but also runs faster than the hidden Markov
model.
We also evaluate our model by computing the
machine translation BLEU score (Papineni et al.,
2002) using the Moses system (Koehn et al., 2007).
The training data is the same as the above word
alignment evaluation bitexts, with alignments for
each model symmetrized using the grow-diag-final
heuristic. Our test is 633 sentences of up to length
50, with four references. Results are shown in Ta-
ble 2; we see that better word alignment results do
not lead to better translations.
</bodyText>
<figure confidence="0.734323916666667">
recall = |A n S|
|S|
|A n P|
precision =
|A|
AER(S,P,A) = 1 _ |A n S |+ |A n P|
|A |+ |S|
604
Model BLEU
HMM 19.55
HMMF-30 19.26
IBM4 18.77
</figure>
<tableCaption confidence="0.941391">
Table 2: BLEU results
</tableCaption>
<sectionHeader confidence="0.995957" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999943533333333">
We developed a fertility hidden Markov model
that runs faster and has lower AER than the
HMM. Our model is thus much faster than IBM
Model 4. Our model is also easier to understand
than IBM Model 4. The Markov Chain Monte Carlo
method used in our model is more principled than
the heuristic-based neighborhood method in IBM
Model 4. While better word alignment results do not
necessarily correspond to better translation quality,
our translation results are comparable in translation
quality to both the HMM and IBM Model 4.
Acknowledgments We would like to thank Tagy-
oung Chung, Matt Post, and the anonymous review-
ers for helpful comments. This work was supported
by NSF grants IIS-0546554 and IIS-0910611.
</bodyText>
<sectionHeader confidence="0.998742" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999950965517241">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
ACL-03.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1–21.
John DeNero, Alexandre Bouchard-Cote, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of EMNLP.
Yonggang Deng and William Byrne. 2005. HMM word
and phrase alignment for statistical machine transla-
tion. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing, pages 169–176, Van-
couver, British Columbia, Canada, October. Associa-
tion for Computational Linguistics.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distribution, and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, PAMI-6(6):721–741,
November.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings ofACL, Demonstration Session, pages 177–180.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In North American Association for Com-
putational Linguistics (NAACL), pages 104–111.
Robert C. Moore. 2004. Improving IBM word alignment
Model 1. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL’04),
Main Volume, pages 518–525, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings ofACL-
02.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to HMM-based statis-
tical word alignment models. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), pages 87–94.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING-96, pages 836–841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
</reference>
<page confidence="0.998744">
605
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.545881">
<title confidence="0.999885">A Fast Fertility Hidden Markov Model for Word Alignment Using MCMC</title>
<author confidence="0.99814">Zhao</author>
<affiliation confidence="0.999093">Department of Computer University of Rochester</affiliation>
<abstract confidence="0.969482266666667">A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="937" citStr="Brown et al., 1993" startWordPosition="153" endWordPosition="156">ding word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. 1 Introduction IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility. IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe</context>
<context position="3623" citStr="Brown et al., 1993" startWordPosition="611" endWordPosition="614">coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics estimation. Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (Brown et al., 1993). Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable. Our model is much faster than IBM Model 4. In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility. Brown et al. (1993) and Och and Ney (2003) first compute the Viterbi alignments for simpler models, then consider</context>
<context position="4960" citStr="Brown et al. (1993)" startWordPosition="845" endWordPosition="848">ors, this method will not be able find the optimal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. 2 Statistical Word Alignment Models 2.1 Alignment and Fertility Given a source sentence fJ1 = f1, f2,.. . , fJ and a target sentence eI1 = e1, e2, ... , eI, we define the alignments between the two sentences as a subset of the Cartesian product of the word positions. Following Brown et al. (1993), we assume that each source word is aligned to exactly one target word. We denote as aJ1 = a1, a2, . . . , aJ the alignments between fJ1 and eI1. When a word fj is not aligned with any word e, aj is 0. For convenience, we add an empty word ǫ to the target sentence at position 0 (i.e., e0 = ǫ). However, as we will see, we have to add more than one empty word for the HMM. In order to compute the “jump probability” in the HMM model, we need to know the position of the aligned target word for the previous source word. If the previous source word aligns to an empty word, we could use the position </context>
<context position="10014" citStr="Brown et al., 1993" startWordPosition="1831" endWordPosition="1834"> where c( ) is the count of jumps of a given distance. In IBM Model 1, the word order does not matter. The HMM is more likely to align a source word to a target word that is adjacent to the previous aligned target word, which is more suitable than IBM Model 1 because adjacent words tend to form phrases. For these two models, in theory, the fertility for a target word can be as large as the length of the source sentence. In practice, the fertility for a target word in IBM Model 1 is not very big except for rare target words, which can become a garbage collector, and align to many source words (Brown et al., 1993; Och and Ney, 2003; Moore, 2004). The HMM is less likely to have this garbage collector problem because of the alignment probability constraint. However, fertility is an inherent cross-language property and these two models cannot assign consistent fertility to words. This is our motivation for adding fertility to these two models, and we expect that the resulting models will perform better than the baseline models. Because the HMM performs much better than IBM Model 1, we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1. Throughout the pap</context>
<context position="14493" citStr="Brown et al., 1993" startWordPosition="2696" endWordPosition="2699">= aJ 1 P˜ (aJ 1 |e2I+1 1 ,fJ 1 )log P(aJ1,fJ1 |e2I+1 1 ) X ξ1(e)( f X− e P(f|e) − 1) X ξ2(a′)( a P(a|a′) − 1) X− a′ X= φI1,φǫ P(φI J J 2I+1 1, φǫ, a1 , f1 |e1 ) YI i=1 XJ j=1  δ  P(aJ 1 ,fJ 1 |e2I+1 1 ) � I J J 2I+1 P(φ1, φǫ, a1 , f1 |e1 ) X δ(aj,i), φi X L(P(f|e), P(a|a′), λ(e), ξ1(e), ξ2(a′)) δ(aj,i),φǫ (3) In the last two lines of Equation 3, φǫ and each φi are not free variables, but are determined by the alignments. Because we only sum over fertilities that are consistent with the alignments, we have PfJ1 P(fJ1 |e2I+1) &lt; 1, and our model is deficient, similar to IBM Models 3 and 4 (Brown et al., 1993). We can remove the deficiency for fertility IBM Model 1 by assuming a different distortion probability: the distortion probability is 0 if fertility is not consistent with alignments, and uniform otherwise. The total number of consistent fertility and alignments is ;! Replacing ( 1 )J with φǫ! j lj=1 φi!* 2I+1 φǫ! j lj= 1 φi! J! , we have: Because P(aJ1 , fJ1 |e2I+1 1 ) is in the exponential family, we get a closed form for the parameters from expected counts: = = Ps c(f|e; f(s), e(s)) P f Ps c(f |e; f (s), e(s)) Ps c(a|a′; f(s),e(s)) Pa Ps c(a |a′; f (s), e(s)))) Ps c(φ|e; f(s), e(s)) P (6) </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-03.</booktitle>
<contexts>
<context position="2315" citStr="Cherry and Lin, 2003" startWordPosition="399" endWordPosition="402">st researches use the GIZA++ software package (Och and Ney, 2003), and IBM Model 4 itself is treated as a black box. The complexity in IBM Model 4 makes it hard to understand and to improve. Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand. We also want it to be accurate and computationally efficient. There have been many years of research on word alignment. Our work is different from others in essential ways. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al.</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. In Proceedings of ACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="15834" citStr="Dempster et al., 1977" startWordPosition="2967" endWordPosition="2970"> ) X aJ 1 X δ(fj, f)δ(ei, e) j  2I+1X XJ δ  i=I+1 j=1 P(f|e) P(a|a′) λ(e) = P˜(aJ1 |fJ1 , e2I+1 1 ) X 1 X c(a|a′; fJ1 , e2I+1 1 ) = aJ P(φI J J 2I+1 1, φǫ, a1 , f1 |e1 ) = YI λ(ei)φie−λ(ei) X X δ(aj, a)δ(aj−1, a′) i=1 j X c(φ|e; fJ1 , e2I+1 1 ) = aJ 1 1 J! YJ j=1 X i P˜(aJ1 |fJ1 , e2I+1 1 ) X φiδ(ei, e) (Iλ(ǫ))φǫ e−(Iλ(ǫ)) X P(fj|eaj) In our experiments, we did not find a noticeable change in terms of alignment accuracy by removing the deficiency. 4 Expectation Maximization Algorithm We estimate the parameters by maximizing P(fJ1 |e2I+1 1 ) using the expectation maximization (EM) algorithm (Dempster et al., 1977). The c(k|e; fJ1 , e1I +1) = X k(ei)δ(ei, e) i These equations are for the fertility hidden Markov model. For the fertility IBM Model 1, we do not need to estimate the distortion probability. 5 Gibbs Sampling for Fertility HMM Although we can estimate the parameters by using the EM algorithm, in order to compute the expected 600 Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pair (fJ1 , e2I+1 1 ) in the corpus counts, we have to sum over all possible alignments aJ1, which is, unfortunately, exponential. We developed a Gibbs sampling algorithm (Geman and Gema</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cote</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4541" citStr="DeNero et al. (2008)" startWordPosition="770" endWordPosition="773">ster than the HMM, and has lower alignment error rate than the HMM. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility. Brown et al. (1993) and Och and Ney (2003) first compute the Viterbi alignments for simpler models, then consider only some neighbors of the Viterbi alignments for modeling fertility. If the optimal alignment is not in those neighbors, this method will not be able find the optimal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. 2 Statistical Word Alignment Models 2.1 Alignment and Fertility Given a source sentence fJ1 = f1, f2,.. . , fJ and a target sentence eI1 = e1, e2, ... , eI, we define the alignments between the two sentences as a subset of the Cartesian product of the word positions. Following Brown et al. (1993), we assume that each source word is aligned to exactly one target word. We denote as aJ1 = a1, a2, . . . , aJ the alignments between fJ1 and eI1. When a word fj is not aligned with</context>
</contexts>
<marker>DeNero, Bouchard-Cote, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cote, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>HMM word and phrase alignment for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>169--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="2944" citStr="Deng and Byrne, 2005" startWordPosition="503" endWordPosition="506">t and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics estimation. Our distortion parameters are similar to IBM Mode</context>
</contexts>
<marker>Deng, Byrne, 2005</marker>
<rawString>Yonggang Deng and William Byrne. 2005. HMM word and phrase alignment for statistical machine translation. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 169–176, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distribution, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="16442" citStr="Geman and Geman, 1984" startWordPosition="3075" endWordPosition="3079"> et al., 1977). The c(k|e; fJ1 , e1I +1) = X k(ei)δ(ei, e) i These equations are for the fertility hidden Markov model. For the fertility IBM Model 1, we do not need to estimate the distortion probability. 5 Gibbs Sampling for Fertility HMM Although we can estimate the parameters by using the EM algorithm, in order to compute the expected 600 Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pair (fJ1 , e2I+1 1 ) in the corpus counts, we have to sum over all possible alignments aJ1, which is, unfortunately, exponential. We developed a Gibbs sampling algorithm (Geman and Geman, 1984) to compute the expected counts. For each target sentence e2I+1 1and source sentence fJ1 , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probability P(aj|a1, ··· aj−1, aj+1 ··· aJ, fJ1 , e2I+1 1 ), which can be computed in the following way: J 2I+1 P(aj |a1, ··· aj−1, aj+1, ··· aJ, f1 , e1 ) P(aJ1 , fJ1 |e2I+1 1 ) = � 1 ) (7) aj P(aJ 1 , fJ 1 |e2I+1 For each alignment variable aj, we choose t samples. We scan </context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distribution, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6(6):721–741, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL, Demonstration Session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="24638" citStr="Koehn et al., 2007" startWordPosition="4561" endWordPosition="4564">ster than the HMM. Figure 3 show the training time for different models. In fact, with just 1 sample for each alignment, our model archives lower AER than the HMM, and runs more than 5 times faster than the HMM. It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER (often an increase). We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model. We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007). The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic. Our test is 633 sentences of up to length 50, with four references. Results are shown in Table 2; we see that better word alignment results do not lead to better translations. recall = |A n S| |S| |A n P| precision = |A| AER(S,P,A) = 1 _ |A n S |+ |A n P| |A |+ |S| 604 Model BLEU HMM 19.55 HMMF-30 19.26 IBM4 18.77 Table 2: BLEU results 7 Conclusion We developed a fertility hidden Markov model that runs faster and has lower AER than the </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofACL, Demonstration Session, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In North American Association for Computational Linguistics (NAACL),</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2266" citStr="Liang et al., 2006" startWordPosition="390" endWordPosition="393">age. However, IBM Model 4 is so complex that most researches use the GIZA++ software package (Och and Ney, 2003), and IBM Model 4 itself is treated as a black box. The complexity in IBM Model 4 makes it hard to understand and to improve. Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand. We also want it to be accurate and computationally efficient. There have been many years of research on word alignment. Our work is different from others in essential ways. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertilit</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In North American Association for Computational Linguistics (NAACL), pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving IBM word alignment Model 1.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>518--525</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5746" citStr="Moore (2004)" startWordPosition="1009" endWordPosition="1010">ed with any word e, aj is 0. For convenience, we add an empty word ǫ to the target sentence at position 0 (i.e., e0 = ǫ). However, as we will see, we have to add more than one empty word for the HMM. In order to compute the “jump probability” in the HMM model, we need to know the position of the aligned target word for the previous source word. If the previous source word aligns to an empty word, we could use the position of the empty word to indicate the nearest previous source word that does not align to an empty word. For this reason, we use a total of I + 1 empty words for the HMM model1. Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. After we add I +1 empty words to the target sentence, the alignment is a mapping from source to target word positions: a:j—*i,i=aj where j = 1, 2, ... , J and i = 1, 2, ... , 2I + 1. Words from position I + 1 to 2I + 1 in the target sentence are all empty words. We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility φi of a word ei at position i is defined as the number of aligned source words: J φi = δ(aj,i) j=1 where δ is the K</context>
<context position="10047" citStr="Moore, 2004" startWordPosition="1839" endWordPosition="1840">given distance. In IBM Model 1, the word order does not matter. The HMM is more likely to align a source word to a target word that is adjacent to the previous aligned target word, which is more suitable than IBM Model 1 because adjacent words tend to form phrases. For these two models, in theory, the fertility for a target word can be as large as the length of the source sentence. In practice, the fertility for a target word in IBM Model 1 is not very big except for rare target words, which can become a garbage collector, and align to many source words (Brown et al., 1993; Och and Ney, 2003; Moore, 2004). The HMM is less likely to have this garbage collector problem because of the alignment probability constraint. However, fertility is an inherent cross-language property and these two models cannot assign consistent fertility to words. This is our motivation for adding fertility to these two models, and we expect that the resulting models will perform better than the baseline models. Because the HMM performs much better than IBM Model 1, we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1. Throughout the paper, “our model” refers to the fer</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving IBM word alignment Model 1. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 518–525, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="977" citStr="Och and Ney, 2003" startWordPosition="161" endWordPosition="164">al machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. 1 Introduction IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility. IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe that IBM Model 4 is essentially a bette</context>
<context position="4152" citStr="Och and Ney (2003)" startWordPosition="705" endWordPosition="708">o IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (Brown et al., 1993). Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable. Our model is much faster than IBM Model 4. In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility. Brown et al. (1993) and Och and Ney (2003) first compute the Viterbi alignments for simpler models, then consider only some neighbors of the Viterbi alignments for modeling fertility. If the optimal alignment is not in those neighbors, this method will not be able find the optimal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. 2 Statistical Word Alignment Models 2.1 Alignment and Fertility Given a source sentence f</context>
<context position="8785" citStr="Och and Ney, 2003" startWordPosition="1588" endWordPosition="1591">obability that depends only on the length of the target sentence, and a lexical probability that depends only on the aligned target word: J P(a1 fi |eil+�) = (2 + (J|1�JYP(fj|eaj) j=1 The hidden Markov model assumes a length probability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P(aJ1 , fJ1 |e2I+1 1 ) = P(aj|aj−1, I)P(fj|eaj) In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): P(i + I + 1|i′, I) = p0δ(i, i′) P(i + I + 1|i′ + I + 1, I) = p0δ(i, i′) P(i|i′ + I + 1,I) = P(i|i′,I) where the first two equations imply that the probability of jumping to an empty word is either 0 or p0, and the third equation implies that the probability of jumping from a non-empty word is the same as the probability ofjumping from the corespondent empty word. The absolute position in the HMM is not important, because we re-parametrize the distortion probability in terms of the distance between adjacent alignment points (Vogel et al., 1996; Och and Ney, 2003): P (i|i′, I) = c(i − i′) P i″</context>
<context position="10033" citStr="Och and Ney, 2003" startWordPosition="1835" endWordPosition="1838">ount of jumps of a given distance. In IBM Model 1, the word order does not matter. The HMM is more likely to align a source word to a target word that is adjacent to the previous aligned target word, which is more suitable than IBM Model 1 because adjacent words tend to form phrases. For these two models, in theory, the fertility for a target word can be as large as the length of the source sentence. In practice, the fertility for a target word in IBM Model 1 is not very big except for rare target words, which can become a garbage collector, and align to many source words (Brown et al., 1993; Och and Ney, 2003; Moore, 2004). The HMM is less likely to have this garbage collector problem because of the alignment probability constraint. However, fertility is an inherent cross-language property and these two models cannot assign consistent fertility to words. This is our motivation for adding fertility to these two models, and we expect that the resulting models will perform better than the baseline models. Because the HMM performs much better than IBM Model 1, we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1. Throughout the paper, “our model” ref</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL02.</booktitle>
<contexts>
<context position="24594" citStr="Papineni et al., 2002" startWordPosition="4553" endWordPosition="4556">nly has lower AER than the HMM, it also runs faster than the HMM. Figure 3 show the training time for different models. In fact, with just 1 sample for each alignment, our model archives lower AER than the HMM, and runs more than 5 times faster than the HMM. It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER (often an increase). We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model. We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007). The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic. Our test is 633 sentences of up to length 50, with four references. Results are shown in Table 2; we see that better word alignment results do not lead to better translations. recall = |A n S| |S| |A n P| precision = |A| AER(S,P,A) = 1 _ |A n S |+ |A n P| |A |+ |S| 604 Model BLEU HMM 19.55 HMMF-30 19.26 IBM4 18.77 Table 2: BLEU results 7 Conclusion We developed a fertility hidden Markov model </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings ofACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>H Tolga Ilhan</author>
<author>Christopher D Manning</author>
</authors>
<title>Extensions to HMM-based statistical word alignment models.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>87--94</pages>
<contexts>
<context position="2921" citStr="Toutanova et al., 2002" startWordPosition="499" endWordPosition="502">y and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics estimation. Our distortion parameters </context>
</contexts>
<marker>Toutanova, Ilhan, Manning, 2002</marker>
<rawString>Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Manning. 2002. Extensions to HMM-based statistical word alignment models. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 87–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING-96,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="957" citStr="Vogel et al., 1996" startWordPosition="157" endWordPosition="160">models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. 1 Introduction IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility. IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe that IBM Model 4 is</context>
<context position="9335" citStr="Vogel et al., 1996" startWordPosition="1694" endWordPosition="1697"> correctly, we enforce the following constraints (Och and Ney, 2003): P(i + I + 1|i′, I) = p0δ(i, i′) P(i + I + 1|i′ + I + 1, I) = p0δ(i, i′) P(i|i′ + I + 1,I) = P(i|i′,I) where the first two equations imply that the probability of jumping to an empty word is either 0 or p0, and the third equation implies that the probability of jumping from a non-empty word is the same as the probability ofjumping from the corespondent empty word. The absolute position in the HMM is not important, because we re-parametrize the distortion probability in terms of the distance between adjacent alignment points (Vogel et al., 1996; Och and Ney, 2003): P (i|i′, I) = c(i − i′) P i″c(i′′ − i′) where c( ) is the count of jumps of a given distance. In IBM Model 1, the word order does not matter. The HMM is more likely to align a source word to a target word that is adjacent to the previous aligned target word, which is more suitable than IBM Model 1 because adjacent words tend to form phrases. For these two models, in theory, the fertility for a target word can be as large as the length of the source sentence. In practice, the fertility for a target word in IBM Model 1 is not very big except for rare target words, which can</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING-96, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2708" citStr="Wu, 1997" startWordPosition="462" endWordPosition="463">ny years of research on word alignment. Our work is different from others in essential ways. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 20</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>