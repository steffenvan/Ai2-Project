<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.996104">
K-means Clustering with Feature Hashing
</title>
<author confidence="0.993729">
Hajime Senuma
</author>
<affiliation confidence="0.9986525">
Department of Computer Science
University of Tokyo
</affiliation>
<address confidence="0.893506">
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
</address>
<email confidence="0.997754">
hajime.senuma@gmail.com
</email>
<sectionHeader confidence="0.996646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995015">
One of the major problems of K-means is
that one must use dense vectors for its cen-
troids, and therefore it is infeasible to store
such huge vectors in memory when the feature
space is high-dimensional. We address this is-
sue by using feature hashing (Weinberger et
al., 2009), a dimension-reduction technique,
which can reduce the size of dense vectors
while retaining sparsity of sparse vectors. Our
analysis gives theoretical motivation and jus-
tification for applying feature hashing to K-
means, by showing how much will the objec-
tive of K-means be (additively) distorted. Fur-
thermore, to empirically verify our method,
we experimented on a document clustering
task.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998956804347826">
In natural language processing (NLP) and text min-
ing, clustering methods are crucial for various tasks
such as document clustering. Among them, K-
means (MacQueen, 1967; Lloyd, 1982) is “the most
important flat clustering algorithm” (Manning et al.,
2008) both for its simplicity and performance.
One of the major problems of K-means is that it
has K centroids which are dense vectors where K
is the number of clusters. Thus, it is infeasible to
store them in memory and slow to compute if the di-
mension of inputs is huge, as is often the case with
NLP and text mining tasks. A well-known heuris-
tic is truncating after the most significant features
(Manning et al., 2008), but it is difficult to analyze
its effect and to determine which features are signif-
icant.
Recently, Weinberger et al. (2009) introduced fea-
ture hashing, a simple yet effective and analyzable
dimension-reduction technique for large-scale mul-
titask learning. The idea is to combine features
which have the same hash value. For example, given
a hash function h and a vector x, if h(1012) =
h(41234) = 42, we make a new vector y by set-
ting y42 = x1012 + x41234 (or equally possibly
x1012−x41234, −x1012+x41234, or −x1012−x41234).
This trick greatly reduces the size of dense vec-
tors, since the maximum index value becomes
equivalent to the maximum hash value of h. Further-
more, unlike random projection (Achlioptas, 2003;
Boutsidis et al., 2010), feature hashing retains spar-
sity of sparse input vectors. An additional useful
trait for NLP tasks is that it can save much memory
by eliminating an alphabet storage (see the prelim-
inaries for detail). The authors also justified their
method by showing that with feature hashing, dot-
product is unbiased, and the length of each vector
is well-preserved with high probability under some
conditions.
Plausibly this technique is useful also for clus-
tering methods such as K-means. In this paper, to
motivate applying feature hashing to K-means, we
show the residual sum of squares, the objective of
K-means, is well-preserved under feature hashing.
We also demonstrate an experiment on document
clustering and see the feature size can be shrunk into
3.5% of the original in this case.
</bodyText>
<page confidence="0.96766">
122
</page>
<subsectionHeader confidence="0.142805">
Proceedings of the ACL-HLT 2011 Student Session, pages 122–126,
</subsectionHeader>
<bodyText confidence="0.311305">
Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.969619" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.940288">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.999987333333333">
In this paper,  ||·  ||denotes the Euclidean norm, and
h·, ·i does the dot product. δi,j is the Kronecker’s
delta, that is, δi,j = 1 if i = j and 0 otherwise.
</bodyText>
<subsectionHeader confidence="0.999378">
2.2 K-means
</subsectionHeader>
<bodyText confidence="0.9876910625">
Although we do not describe the famous algorithm
of K-means (MacQueen, 1967; Lloyd, 1982) here,
we remind the reader of its overall objective for
later analysis. If we want to group input vec-
tors into K clusters, K-means can surely output
clusters ω1, ...ωK and their corresponding vectors
µ1, ..., µK such that they locally minimize the resid-
ual sum of squares (RSS) which is defined as
||x − µk||2.
In the algorithm, µk is made into the mean of the
vectors in a cluster ωk. Hence comes the name K-
means.
Note that RSS can be regarded as a metric since
the sum of each metric (in this case, squared Eu-
clidean distance) becomes also a metric by con-
structing a 1-norm product metric.
</bodyText>
<subsectionHeader confidence="0.999426">
2.3 Additive distortion
</subsectionHeader>
<bodyText confidence="0.99904475">
Suppose one wants to embed a metric space (X, d)
into another one (X&apos;, d&apos;) by a mapping φ. Its ad-
ditive distortion is the infimum of c which, for any
observed x, y ∈ X, satisfies the following condition:
</bodyText>
<equation confidence="0.808991">
d(x, y) − c ≤ d&apos;(φ(x), φ(y)) ≤ d(x, y) + c.
</equation>
<subsectionHeader confidence="0.999493">
2.4 Hashing tricks
</subsectionHeader>
<bodyText confidence="0.9998607">
According to an account by John Langford 1, a
co-author of papers on feature hashing (Shi et al.,
2009; Weinberger et al., 2009), hashing tricks for
dimension-reduction were implemented in various
machine learning libraries including Vowpal Wab-
bit, which he realesed in 2007.
Ganchev and Dredze (2008) named their hashing
trick random feature mixing and empirically sup-
ported it by experimenting on NLP tasks. It is simi-
lar to feature hashing except lacking of a binary hash
</bodyText>
<footnote confidence="0.746344">
1http://hunch.net/˜jl/projects/hash_
reps/index.html
</footnote>
<bodyText confidence="0.90617464">
function. The paper also showed that hashing tricks
are useful to eliminate alphabet storage.
Shi et al. (2009) suggested hash kernel, that is,
dot product on a hashed space. They conducted thor-
ough research both theoretically and experimentally,
extending this technique to classification of graphs
and multi-class classification. Although they tested
K-means in an experiment, it was used for classifi-
cation but not for clustering.
Weinberger et al. (2009) 2 introduced a technique
feature hashing (a function itself is called the hashed
feature map), which incorporates a binary hash func-
tion into hashing tricks in order to guarantee the hash
kernel is unbiased. They also showed applications
to various real-world applications such as multitask
learning and collaborative filtering. Though their
proof for exponential tail bounds in the original pa-
per was refuted later, they reproved it under some
extra conditions in the latest version. Below is the
definition.
Definition 2.1. Let S be a set of hashable features,
h be a hash function h : S → {1, ..., m}, and ξ be
ξ : S → {±1}. The hashed feature map φ(h,ξ) :
RISI → Rm is a function such that the i-th element
of φ(h,ξ)(x) is given by
</bodyText>
<equation confidence="0.96521925">
�
φ(h,ξ)
i(x) =
j:h(j)=i
</equation>
<bodyText confidence="0.912367125">
If h and ξ are clear from the context, we simply
write φ(h,ξ) as φ.
As well, a kernel function is defined on a hashed
feature map.
Definition 2.2. The hash kernel h·, ·iφ is defined as
hx, x&apos;iφ = hφ(x),φ(x&apos;)i.
They also proved the following theorem, which
we use in our analysis.
</bodyText>
<construct confidence="0.844315">
Theorem 2.3. The hash kernel is unbiased, that is,
Eφ[hx, x&apos;iφ] = hx, x&apos;i.
The variance is
</construct>
<equation confidence="0.62960275">
1 � �
V arφ[hx, x&apos;iφ] = m � �x2i x02j + xix&apos;ixjx&apos;�.
j
i7�j
</equation>
<footnote confidence="0.996120333333333">
2The latest version of this paper is at arXiv http://
arxiv.org/abs/0902.2206, with correction to Theorem
3 in the original paper included in the Proceeding of ICML ’09.
</footnote>
<equation confidence="0.531043166666667">
K
�
k=1
�
xEωk
ξ(j)xj.
</equation>
<page confidence="0.982218">
123
</page>
<subsectionHeader confidence="0.799237">
2.4.1 Eliminating alphabet storage
</subsectionHeader>
<bodyText confidence="0.999919111111111">
In this kind of hashing tricks, an index of inputs
do not have to be an integer but can be any hash-
able value, including a string. Ganchev and Dredze
(2008) argued this property is useful particularly for
implementing NLP applications, since we do not
anymore need an alphabet, a dictionary which maps
features to parameters.
Let us explain in detail. In NLP, features can be
often expediently expressed with strings. For in-
stance, a feature ‘the current word ends with -ing’
can be expressed as a string cur:end:ing (here
we suppose : is a control character). Since indices
of dense vectors (which may be implemented with
arrays) must be integers, traditionally we need a dic-
tionary to map these strings to integers, which may
waste much memory. Feature hashing removes this
memory waste by converting strings to integers with
on-the-fly computation.
</bodyText>
<sectionHeader confidence="0.991767" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9955252">
For dimension-reduction to K-means, we propose
a new method hashed K-means. Suppose you have
N input vectors x1, ..., xN. Given a hashed fea-
ture map φ, hashed K-means runs K-means on
φ(x1), ..., φ(xN) instead of the original ones.
</bodyText>
<equation confidence="0.91363025">
Then, DRSS is defined as follows:
||φ(x) − µk||2
||x − µk||2|.
Before analysis, we define a notation for the (Eu-
clidean) length under a hashed space:
Definition 4.2. The hash length  ||· ||� is defined as
||x||� = ||φ(x)||
� �
= hφ(x), φ(x)i = hx, xi�.
Note that it is clear from Theorem 2.3 that
EO[||x||2�] = ||x||2, and equivalently EO[||x||2 � −
||x||2] = 0.
</equation>
<bodyText confidence="0.992382333333333">
In order to show distortion, we want to use Cheby-
shev’s inequality. To this end, it is vital to know the
expectation and variance of the sum of squared hash
lengths. Because the variance of the sum of ran-
dom variables derives from each covariance between
pairs of variables, first we show the covariance be-
tween the squared hash length of two vectors.
Lemma 4.3. The covariance between the squared
hash length of two vectors x, y ∈ R&apos; is
</bodyText>
<equation confidence="0.9654662">
DRSS = |
K
k=1
�
xEwk
−
K
k=1
�
xEwk
,
m
Y)
CovO( ||x||0, ||y 0) = (x,
4 Analysis
</equation>
<bodyText confidence="0.950062777777778">
In this section, we show clusters obtained by the
hashed K-means are also good clusters in the orig-
inal space with high probability. While Weinberger
et al. (2009) proved a theorem on (multiplicative)
distortion for Euclidean distance under some tight
conditions, we illustrate (additive) distortion for
RSS. Since K-means is a process which monoton-
ically decreases RSS in each step, if RSS is not dis-
torted so much by feature hashing, we can expect
results to be reliable to some extent.
Let us define the difference of the residual sum of
squares (DRSS).
Definition 4.1. Let ω1, ...ωK be clusters, µ1, ..., µK
be their corresponding centroids in the original
space, φ be a hashed feature map, and µo, ..., µK be
their corresponding centroids in the hashed space.
where
ψ(x, y) = 2 � xixjyiyj.
i7�j
This lemma can be proven by the same technique
described in the Appendix A of Weinberger et al.
(2009).
Now we see the following lemma.
Lemma 4.4. Suppose we have N vectors
x1, ..., xN. Let us define X = Ei ||xi||2 � −
Ei ||xi||2 = E �||xi||2 � − ||xi||2� . Then, for any
i
</bodyText>
<equation confidence="0.667282">
E &gt; 0,
�� d N N �
P �|X |≥ √m i=1 j=1 ψ(xi, xj) � ≤ 2
E 1 .
</equation>
<page confidence="0.988928">
124
</page>
<bodyText confidence="0.8939825">
Proof. This is an application of Chebyshev’s in-
equality. Namely, for any c &gt; 0,
</bodyText>
<equation confidence="0.987994">
P (|X − Eφ[X] |&gt; c�V arφ[X]) &lt; 62.
</equation>
<bodyText confidence="0.9766788">
Since the expectation of a sum is the sum of ex-
pectations we readily know the zero expectation:
Eφ[X] = 0.
Since adding constants to the inputs of covariance
does not change its result, from Lemma 4.3, for any
</bodyText>
<equation confidence="0.972444">
x, y E Rn,
Covφ(||x||2φ − ||x||2, ||y||2φ − ||y||2) = ψ(x, y)
m
</equation>
<bodyText confidence="0.99628">
Because the variance of the sum of random vari-
ables is the sum of the covariances between every
pair of them,
</bodyText>
<equation confidence="0.987148">
1
V arφ[X] =
m
</equation>
<bodyText confidence="0.978985833333333">
Finally, we see the following theorem for additive
distortion.
Theorem 4.5. Let Ψ be the sum of ψ(x, y) for any
observed pair of x, y, each of which expresses the
difference between an example and its correspond-
ing centroid. Then, for any c,
</bodyText>
<equation confidence="0.91823">
Ψ
P (|DRSS |&gt; 0 &lt;
E2m.
Thus, if m &gt; γ−1Ψc−2 where 0 &lt; γ &lt;= 1, with
</equation>
<bodyText confidence="0.976389076923077">
probability at least 1−γ, RSS is additively distorted
by c.
Proof. Note that a hashed feature map φ(h,ξ) is lin-
ear, since φ(x) = Mx with a matrix M such
that Mi,j = ξ(i)δh(i),j. By this liearlity, µφk =
|ωk|−1 E ∈ωk φ(x) = φ(|ωk|−1 E ∈ωk x) =
φ(µk). Reapplying linearlity to this result, we have
||φ(x)−µφk||2 = ||x−µk||2φ. Lemma 4.4 completes
the proof.
The existence of Ψ in the theorem suggests that to
use feature hashing, we should remove useless fea-
tures which have high values from data in advance.
For example, if frequencies of words are used as
</bodyText>
<figure confidence="0.916759">
10 100 1000 10000 100000 1e+06
hash size m
</figure>
<figureCaption confidence="0.9916565">
Figure 1: The change of F5-measure along with the hash
size
</figureCaption>
<bodyText confidence="0.999877">
features, function words should be ignored not only
because they give no information for clustering but
also because their high frequencies magnify distor-
tion.
</bodyText>
<sectionHeader confidence="0.998196" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99989775">
To empirically verify our method, from 20 News-
groups, a dataset for document classification or clus-
tering 3, we chose 6 classes and randomly drew 100
documents for each class.
We used unigrams and bigrams as features and ran
our method for various hash sizes m (Figure 1). The
number of unigrams is 33,017 and bigrams 109,395,
so the feature size in the original space is 142,412.
To measure performance, we used the F5 mea-
sure (Manning et al., 2008). The scheme counts
correctness pairwisely. For example, if a docu-
ment pair in an output cluster is actually in the
same class, it is counted as true positive. In con-
trast, if it is actually in the different class, it is
counted as false positive. Following this man-
ner, a contingency table can be made as follows:
</bodyText>
<table confidence="0.77688425">
Same cluster Diff. clusters
Same class TP FN
Diff. classes FP TN
β2P + R
</table>
<bodyText confidence="0.936256">
where the precision P = TP/(TP + FP) and the
recall R = TP/(TP + FN).
</bodyText>
<footnote confidence="0.55731">
3http://people.csail.mit.edu/jrennie/20Newsgroups/
</footnote>
<figure confidence="0.99527019047619">
hashed k-means
.
N
i=1
ψ(xi,xj).
N
j=1
F5 measure 0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
Now, Fβ measure can be defined as
(β2 + 1)PR
Fβ =
</figure>
<page confidence="0.993595">
125
</page>
<bodyText confidence="0.999920368421053">
In short, F5 measure strongly favors precision to
recall. Manning et al. (2008) stated that in some
cases separating similar documents is more unfavor-
able than putting dissimilar documents together, and
in such cases the F,3 measure (where Q &gt; 1) is a
good evaluation criterion.
At the first look, it seems odd that performance
can be higher than the original where m is low. A
possible hypothesis is that since K-means only lo-
cally minimizes RSS but in general there are many
local minima which are far from the global optimal
point, therefore distortion can be sometimes useful
to escape from a bad local minimum and reach a
better one. As a rule, however, large distortion kills
clustering performance as shown in the figure.
Although clustering is heavily case-dependent, in
this experiment, the resulting clusters are still reli-
able where the hash size is 3.5% of the original fea-
ture space size (around 5,000).
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999951285714286">
Arthur and Vassilvitskii (2007) proposed K-
means++, an improved version of K-means which
guarantees its RSS is upper-bounded. Combining
their method and the feature hashing as shown in our
paper will produce a new efficient method (possibly
it can be named hashed K-means++). We will ana-
lyze and experiment with this method in the future.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999573">
In this paper, we argued that applying feature hash-
ing to K-means is beneficial for memory-efficiency.
Our analysis theoretically motivated this combina-
tion. We supported our argument and analysis by
an experiment on document clustering, showing we
could safely shrink memory-usage into 3.5% of the
original in our case. In the future, we will analyze
the technique on other learning methods such as K-
means++ and experiment on various real-data NLP
tasks.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999305">
We are indebted to our supervisors, Jun’ichi Tsujii
and Takuya Matsuzaki. We are also grateful to the
anonymous reviewers for their helpful and thought-
ful comments.
</bodyText>
<sectionHeader confidence="0.994328" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999087685714286">
Dimitris Achlioptas. 2003. Database-friendly random
projections: Johnson-Lindenstrauss with binary coins.
Journal of Computer and System Sciences, 66(4):671–
687, June.
David Arthur and Sergei Vassilvitskii. 2007. k-means++
: The Advantages of Careful Seeding. In Proceedings
of the Eighteenth Annual ACM-SIAM Symposium on
Discrete Algorithms, pages 1027–1035.
Christos Boutsidis, Anastasios Zouzias, and Petros
Drineas. 2010. Random Projections for k-means
Clustering. In Advances in Neural Information Pro-
cessing Systems 23, number iii, pages 298–306.
Kuzman Ganchev and Mark Dredze. 2008. Small Statis-
tical Models by Random Feature Mixing. In Proceed-
ings of the ACL08 HLT Workshop on Mobile Language
Processing, pages 19–20.
Stuart P. Lloyd. 1982. Least Squares Quantization in
PCM. IEEE Transactions on Information Theory,
28(2):129–137.
J MacQueen. 1967. Some Methods for Classification
and Analysis of Multivariate Observations. In Pro-
ceedings of 5th Berkeley Symposium on Mathematical
Statistics and Probability, pages 281–297.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash Kernels for Structured Data. Journal of Machine
Learning Research, 10:2615–2637.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature Hash-
ing for Large Scale Multitask Learning. In Proceed-
ings of the 26th International Conference on Machine
Learning.
</reference>
<page confidence="0.998427">
126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.624196">
<title confidence="0.999772">Clustering with Feature Hashing</title>
<author confidence="0.705022">Hajime</author>
<affiliation confidence="0.9996835">Department of Computer University of</affiliation>
<address confidence="0.931491">7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033,</address>
<email confidence="0.99966">hajime.senuma@gmail.com</email>
<abstract confidence="0.997030470588235">of the major problems of is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional. We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our analysis gives theoretical motivation and jusfor applying feature hashing to means, by showing how much will the objecof be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dimitris Achlioptas</author>
</authors>
<title>Database-friendly random projections: Johnson-Lindenstrauss with binary coins.</title>
<date>2003</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>66</volume>
<issue>4</issue>
<pages>687</pages>
<contexts>
<context position="2255" citStr="Achlioptas, 2003" startWordPosition="365" endWordPosition="366">Weinberger et al. (2009) introduced feature hashing, a simple yet effective and analyzable dimension-reduction technique for large-scale multitask learning. The idea is to combine features which have the same hash value. For example, given a hash function h and a vector x, if h(1012) = h(41234) = 42, we make a new vector y by setting y42 = x1012 + x41234 (or equally possibly x1012−x41234, −x1012+x41234, or −x1012−x41234). This trick greatly reduces the size of dense vectors, since the maximum index value becomes equivalent to the maximum hash value of h. Furthermore, unlike random projection (Achlioptas, 2003; Boutsidis et al., 2010), feature hashing retains sparsity of sparse input vectors. An additional useful trait for NLP tasks is that it can save much memory by eliminating an alphabet storage (see the preliminaries for detail). The authors also justified their method by showing that with feature hashing, dotproduct is unbiased, and the length of each vector is well-preserved with high probability under some conditions. Plausibly this technique is useful also for clustering methods such as K-means. In this paper, to motivate applying feature hashing to K-means, we show the residual sum of squa</context>
</contexts>
<marker>Achlioptas, 2003</marker>
<rawString>Dimitris Achlioptas. 2003. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences, 66(4):671– 687, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Arthur</author>
<author>Sergei Vassilvitskii</author>
</authors>
<title>k-means++ : The Advantages of Careful Seeding.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,</booktitle>
<pages>1027--1035</pages>
<contexts>
<context position="13637" citStr="Arthur and Vassilvitskii (2007)" startWordPosition="2393" endWordPosition="2396"> be higher than the original where m is low. A possible hypothesis is that since K-means only locally minimizes RSS but in general there are many local minima which are far from the global optimal point, therefore distortion can be sometimes useful to escape from a bad local minimum and reach a better one. As a rule, however, large distortion kills clustering performance as shown in the figure. Although clustering is heavily case-dependent, in this experiment, the resulting clusters are still reliable where the hash size is 3.5% of the original feature space size (around 5,000). 6 Future Work Arthur and Vassilvitskii (2007) proposed Kmeans++, an improved version of K-means which guarantees its RSS is upper-bounded. Combining their method and the feature hashing as shown in our paper will produce a new efficient method (possibly it can be named hashed K-means++). We will analyze and experiment with this method in the future. 7 Conclusion In this paper, we argued that applying feature hashing to K-means is beneficial for memory-efficiency. Our analysis theoretically motivated this combination. We supported our argument and analysis by an experiment on document clustering, showing we could safely shrink memory-usag</context>
</contexts>
<marker>Arthur, Vassilvitskii, 2007</marker>
<rawString>David Arthur and Sergei Vassilvitskii. 2007. k-means++ : The Advantages of Careful Seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1027–1035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Boutsidis</author>
<author>Anastasios Zouzias</author>
<author>Petros Drineas</author>
</authors>
<title>Random Projections for k-means Clustering.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems 23, number iii,</booktitle>
<pages>298--306</pages>
<contexts>
<context position="2280" citStr="Boutsidis et al., 2010" startWordPosition="367" endWordPosition="370">(2009) introduced feature hashing, a simple yet effective and analyzable dimension-reduction technique for large-scale multitask learning. The idea is to combine features which have the same hash value. For example, given a hash function h and a vector x, if h(1012) = h(41234) = 42, we make a new vector y by setting y42 = x1012 + x41234 (or equally possibly x1012−x41234, −x1012+x41234, or −x1012−x41234). This trick greatly reduces the size of dense vectors, since the maximum index value becomes equivalent to the maximum hash value of h. Furthermore, unlike random projection (Achlioptas, 2003; Boutsidis et al., 2010), feature hashing retains sparsity of sparse input vectors. An additional useful trait for NLP tasks is that it can save much memory by eliminating an alphabet storage (see the preliminaries for detail). The authors also justified their method by showing that with feature hashing, dotproduct is unbiased, and the length of each vector is well-preserved with high probability under some conditions. Plausibly this technique is useful also for clustering methods such as K-means. In this paper, to motivate applying feature hashing to K-means, we show the residual sum of squares, the objective of K-m</context>
</contexts>
<marker>Boutsidis, Zouzias, Drineas, 2010</marker>
<rawString>Christos Boutsidis, Anastasios Zouzias, and Petros Drineas. 2010. Random Projections for k-means Clustering. In Advances in Neural Information Processing Systems 23, number iii, pages 298–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Mark Dredze</author>
</authors>
<title>Small Statistical Models by Random Feature Mixing.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL08 HLT Workshop on Mobile Language Processing,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4687" citStr="Ganchev and Dredze (2008)" startWordPosition="783" endWordPosition="786">ructing a 1-norm product metric. 2.3 Additive distortion Suppose one wants to embed a metric space (X, d) into another one (X&apos;, d&apos;) by a mapping φ. Its additive distortion is the infimum of c which, for any observed x, y ∈ X, satisfies the following condition: d(x, y) − c ≤ d&apos;(φ(x), φ(y)) ≤ d(x, y) + c. 2.4 Hashing tricks According to an account by John Langford 1, a co-author of papers on feature hashing (Shi et al., 2009; Weinberger et al., 2009), hashing tricks for dimension-reduction were implemented in various machine learning libraries including Vowpal Wabbit, which he realesed in 2007. Ganchev and Dredze (2008) named their hashing trick random feature mixing and empirically supported it by experimenting on NLP tasks. It is similar to feature hashing except lacking of a binary hash 1http://hunch.net/˜jl/projects/hash_ reps/index.html function. The paper also showed that hashing tricks are useful to eliminate alphabet storage. Shi et al. (2009) suggested hash kernel, that is, dot product on a hashed space. They conducted thorough research both theoretically and experimentally, extending this technique to classification of graphs and multi-class classification. Although they tested K-means in an experi</context>
<context position="6952" citStr="Ganchev and Dredze (2008)" startWordPosition="1174" endWordPosition="1177"> = hφ(x),φ(x&apos;)i. They also proved the following theorem, which we use in our analysis. Theorem 2.3. The hash kernel is unbiased, that is, Eφ[hx, x&apos;iφ] = hx, x&apos;i. The variance is 1 � � V arφ[hx, x&apos;iφ] = m � �x2i x02j + xix&apos;ixjx&apos;�. j i7�j 2The latest version of this paper is at arXiv http:// arxiv.org/abs/0902.2206, with correction to Theorem 3 in the original paper included in the Proceeding of ICML ’09. K � k=1 � xEωk ξ(j)xj. 123 2.4.1 Eliminating alphabet storage In this kind of hashing tricks, an index of inputs do not have to be an integer but can be any hashable value, including a string. Ganchev and Dredze (2008) argued this property is useful particularly for implementing NLP applications, since we do not anymore need an alphabet, a dictionary which maps features to parameters. Let us explain in detail. In NLP, features can be often expediently expressed with strings. For instance, a feature ‘the current word ends with -ing’ can be expressed as a string cur:end:ing (here we suppose : is a control character). Since indices of dense vectors (which may be implemented with arrays) must be integers, traditionally we need a dictionary to map these strings to integers, which may waste much memory. Feature h</context>
</contexts>
<marker>Ganchev, Dredze, 2008</marker>
<rawString>Kuzman Ganchev and Mark Dredze. 2008. Small Statistical Models by Random Feature Mixing. In Proceedings of the ACL08 HLT Workshop on Mobile Language Processing, pages 19–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart P Lloyd</author>
</authors>
<title>Least Squares Quantization in PCM.</title>
<date>1982</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="1047" citStr="Lloyd, 1982" startWordPosition="158" endWordPosition="159">Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our analysis gives theoretical motivation and justification for applying feature hashing to Kmeans, by showing how much will the objective of K-means be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task. 1 Introduction In natural language processing (NLP) and text mining, clustering methods are crucial for various tasks such as document clustering. Among them, Kmeans (MacQueen, 1967; Lloyd, 1982) is “the most important flat clustering algorithm” (Manning et al., 2008) both for its simplicity and performance. One of the major problems of K-means is that it has K centroids which are dense vectors where K is the number of clusters. Thus, it is infeasible to store them in memory and slow to compute if the dimension of inputs is huge, as is often the case with NLP and text mining tasks. A well-known heuristic is truncating after the most significant features (Manning et al., 2008), but it is difficult to analyze its effect and to determine which features are significant. Recently, Weinberg</context>
<context position="3502" citStr="Lloyd, 1982" startWordPosition="569" endWordPosition="570">ell-preserved under feature hashing. We also demonstrate an experiment on document clustering and see the feature size can be shrunk into 3.5% of the original in this case. 122 Proceedings of the ACL-HLT 2011 Student Session, pages 122–126, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics 2 Preliminaries 2.1 Notation In this paper, ||· ||denotes the Euclidean norm, and h·, ·i does the dot product. δi,j is the Kronecker’s delta, that is, δi,j = 1 if i = j and 0 otherwise. 2.2 K-means Although we do not describe the famous algorithm of K-means (MacQueen, 1967; Lloyd, 1982) here, we remind the reader of its overall objective for later analysis. If we want to group input vectors into K clusters, K-means can surely output clusters ω1, ...ωK and their corresponding vectors µ1, ..., µK such that they locally minimize the residual sum of squares (RSS) which is defined as ||x − µk||2. In the algorithm, µk is made into the mean of the vectors in a cluster ωk. Hence comes the name Kmeans. Note that RSS can be regarded as a metric since the sum of each metric (in this case, squared Euclidean distance) becomes also a metric by constructing a 1-norm product metric. 2.3 Add</context>
</contexts>
<marker>Lloyd, 1982</marker>
<rawString>Stuart P. Lloyd. 1982. Least Squares Quantization in PCM. IEEE Transactions on Information Theory, 28(2):129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J MacQueen</author>
</authors>
<title>Some Methods for Classification and Analysis of Multivariate Observations.</title>
<date>1967</date>
<booktitle>In Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability,</booktitle>
<pages>281--297</pages>
<contexts>
<context position="1033" citStr="MacQueen, 1967" startWordPosition="156" endWordPosition="157">eature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our analysis gives theoretical motivation and justification for applying feature hashing to Kmeans, by showing how much will the objective of K-means be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task. 1 Introduction In natural language processing (NLP) and text mining, clustering methods are crucial for various tasks such as document clustering. Among them, Kmeans (MacQueen, 1967; Lloyd, 1982) is “the most important flat clustering algorithm” (Manning et al., 2008) both for its simplicity and performance. One of the major problems of K-means is that it has K centroids which are dense vectors where K is the number of clusters. Thus, it is infeasible to store them in memory and slow to compute if the dimension of inputs is huge, as is often the case with NLP and text mining tasks. A well-known heuristic is truncating after the most significant features (Manning et al., 2008), but it is difficult to analyze its effect and to determine which features are significant. Rece</context>
<context position="3488" citStr="MacQueen, 1967" startWordPosition="567" endWordPosition="568">of K-means, is well-preserved under feature hashing. We also demonstrate an experiment on document clustering and see the feature size can be shrunk into 3.5% of the original in this case. 122 Proceedings of the ACL-HLT 2011 Student Session, pages 122–126, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics 2 Preliminaries 2.1 Notation In this paper, ||· ||denotes the Euclidean norm, and h·, ·i does the dot product. δi,j is the Kronecker’s delta, that is, δi,j = 1 if i = j and 0 otherwise. 2.2 K-means Although we do not describe the famous algorithm of K-means (MacQueen, 1967; Lloyd, 1982) here, we remind the reader of its overall objective for later analysis. If we want to group input vectors into K clusters, K-means can surely output clusters ω1, ...ωK and their corresponding vectors µ1, ..., µK such that they locally minimize the residual sum of squares (RSS) which is defined as ||x − µk||2. In the algorithm, µk is made into the mean of the vectors in a cluster ωk. Hence comes the name Kmeans. Note that RSS can be regarded as a metric since the sum of each metric (in this case, squared Euclidean distance) becomes also a metric by constructing a 1-norm product m</context>
</contexts>
<marker>MacQueen, 1967</marker>
<rawString>J MacQueen. 1967. Some Methods for Classification and Analysis of Multivariate Observations. In Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability, pages 281–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qinfeng Shi</author>
<author>James Petterson</author>
<author>Gideon Dror</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>S V N Vishwanathan</author>
</authors>
<title>Hash Kernels for Structured Data.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--2615</pages>
<contexts>
<context position="4488" citStr="Shi et al., 2009" startWordPosition="755" endWordPosition="758">in a cluster ωk. Hence comes the name Kmeans. Note that RSS can be regarded as a metric since the sum of each metric (in this case, squared Euclidean distance) becomes also a metric by constructing a 1-norm product metric. 2.3 Additive distortion Suppose one wants to embed a metric space (X, d) into another one (X&apos;, d&apos;) by a mapping φ. Its additive distortion is the infimum of c which, for any observed x, y ∈ X, satisfies the following condition: d(x, y) − c ≤ d&apos;(φ(x), φ(y)) ≤ d(x, y) + c. 2.4 Hashing tricks According to an account by John Langford 1, a co-author of papers on feature hashing (Shi et al., 2009; Weinberger et al., 2009), hashing tricks for dimension-reduction were implemented in various machine learning libraries including Vowpal Wabbit, which he realesed in 2007. Ganchev and Dredze (2008) named their hashing trick random feature mixing and empirically supported it by experimenting on NLP tasks. It is similar to feature hashing except lacking of a binary hash 1http://hunch.net/˜jl/projects/hash_ reps/index.html function. The paper also showed that hashing tricks are useful to eliminate alphabet storage. Shi et al. (2009) suggested hash kernel, that is, dot product on a hashed space.</context>
</contexts>
<marker>Shi, Petterson, Dror, Langford, Smola, Vishwanathan, 2009</marker>
<rawString>Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and S.V.N. Vishwanathan. 2009. Hash Kernels for Structured Data. Journal of Machine Learning Research, 10:2615–2637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Weinberger</author>
<author>Anirban Dasgupta</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>Josh Attenberg</author>
</authors>
<title>Feature Hashing for Large Scale Multitask Learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1663" citStr="Weinberger et al. (2009)" startWordPosition="265" endWordPosition="268">d, 1982) is “the most important flat clustering algorithm” (Manning et al., 2008) both for its simplicity and performance. One of the major problems of K-means is that it has K centroids which are dense vectors where K is the number of clusters. Thus, it is infeasible to store them in memory and slow to compute if the dimension of inputs is huge, as is often the case with NLP and text mining tasks. A well-known heuristic is truncating after the most significant features (Manning et al., 2008), but it is difficult to analyze its effect and to determine which features are significant. Recently, Weinberger et al. (2009) introduced feature hashing, a simple yet effective and analyzable dimension-reduction technique for large-scale multitask learning. The idea is to combine features which have the same hash value. For example, given a hash function h and a vector x, if h(1012) = h(41234) = 42, we make a new vector y by setting y42 = x1012 + x41234 (or equally possibly x1012−x41234, −x1012+x41234, or −x1012−x41234). This trick greatly reduces the size of dense vectors, since the maximum index value becomes equivalent to the maximum hash value of h. Furthermore, unlike random projection (Achlioptas, 2003; Boutsi</context>
<context position="4514" citStr="Weinberger et al., 2009" startWordPosition="759" endWordPosition="762">ence comes the name Kmeans. Note that RSS can be regarded as a metric since the sum of each metric (in this case, squared Euclidean distance) becomes also a metric by constructing a 1-norm product metric. 2.3 Additive distortion Suppose one wants to embed a metric space (X, d) into another one (X&apos;, d&apos;) by a mapping φ. Its additive distortion is the infimum of c which, for any observed x, y ∈ X, satisfies the following condition: d(x, y) − c ≤ d&apos;(φ(x), φ(y)) ≤ d(x, y) + c. 2.4 Hashing tricks According to an account by John Langford 1, a co-author of papers on feature hashing (Shi et al., 2009; Weinberger et al., 2009), hashing tricks for dimension-reduction were implemented in various machine learning libraries including Vowpal Wabbit, which he realesed in 2007. Ganchev and Dredze (2008) named their hashing trick random feature mixing and empirically supported it by experimenting on NLP tasks. It is similar to feature hashing except lacking of a binary hash 1http://hunch.net/˜jl/projects/hash_ reps/index.html function. The paper also showed that hashing tricks are useful to eliminate alphabet storage. Shi et al. (2009) suggested hash kernel, that is, dot product on a hashed space. They conducted thorough r</context>
<context position="8933" citStr="Weinberger et al. (2009)" startWordPosition="1526" endWordPosition="1529">quality. To this end, it is vital to know the expectation and variance of the sum of squared hash lengths. Because the variance of the sum of random variables derives from each covariance between pairs of variables, first we show the covariance between the squared hash length of two vectors. Lemma 4.3. The covariance between the squared hash length of two vectors x, y ∈ R&apos; is DRSS = | K k=1 � xEwk − K k=1 � xEwk , m Y) CovO( ||x||0, ||y 0) = (x, 4 Analysis In this section, we show clusters obtained by the hashed K-means are also good clusters in the original space with high probability. While Weinberger et al. (2009) proved a theorem on (multiplicative) distortion for Euclidean distance under some tight conditions, we illustrate (additive) distortion for RSS. Since K-means is a process which monotonically decreases RSS in each step, if RSS is not distorted so much by feature hashing, we can expect results to be reliable to some extent. Let us define the difference of the residual sum of squares (DRSS). Definition 4.1. Let ω1, ...ωK be clusters, µ1, ..., µK be their corresponding centroids in the original space, φ be a hashed feature map, and µo, ..., µK be their corresponding centroids in the hashed space</context>
</contexts>
<marker>Weinberger, Dasgupta, Langford, Smola, Attenberg, 2009</marker>
<rawString>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature Hashing for Large Scale Multitask Learning. In Proceedings of the 26th International Conference on Machine Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>