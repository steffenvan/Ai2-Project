<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001282">
<title confidence="0.5628115">
Textual Properties and Task Based Evaluation: Investigating the Role of
Surface Properties, Structure and Content.
</title>
<author confidence="0.988939">
Albert Gatt Franc¸ois Portet
</author>
<affiliation confidence="0.9990025">
Institute of Linguistics Laboratoire d’Informatique de Grenoble
University of Malta Grenoble Institute of Technology
</affiliation>
<email confidence="0.989698">
albert.gatt@um.edu.mt francois.portet@imag.fr
</email>
<sectionHeader confidence="0.994579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947846153846">
This paper investigates the relationship be-
tween the results of an extrinsic, task-
based evaluation of an NLG system and
various metrics measuring both surface
and deep semantic textual properties, in-
cluding relevance. The latter rely heav-
ily on domain knowledge. We show that
they correlate systematically with some
measures of performance. The core ar-
gument of this paper is that more domain
knowledge-based metrics shed more light
on the relationship between deep semantic
properties of a text and task performance.
</bodyText>
<sectionHeader confidence="0.998774" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999838108695652">
Evaluation methodology in NLG has generated a
lot of interest. Some recent work suggested that
the relationship between various intrinsic and ex-
trinsic evaluation methods (Sp¨arck-Jones and Gal-
liers, 1996) is not straightforward (Reiter and
Belz, 2009; Gatt and Belz, to appear), leading to
some arguments for more domain-specific intrin-
sic metrics (Foster, 2008). One reason why these
issues are important is that reliable intrinsic eval-
uation metrics that correlate with performance in
an extrinsic, task-based setting can inform system
development. Indeed, this is often the stated pur-
pose of evaluation metrics such as BLEU (Papineni
et al., 2002) and ROUGE (Lin and Hovy, 2003),
which were originally characterised as evaluation
‘understudies’.
In this paper we take up these questions in the
context of a knowledge-based NLG system, BT-45
(Portet et al., 2009), which summarises medical
data for decision support purposes in a Neona-
tal Intensive Care Unit (NICU). Our extrinsic
data comes from an experiment involving com-
plex medical decision making based on automati-
cally generated and human-authored texts (van der
Meulen et al., 2009). This gives us the oppor-
tunity to directly compare the textual character-
istics of generated and human-written summaries
and their relationship to decision-making perfor-
mance. The present work uses data from an ear-
lier study (Gatt and Portet, 2009), which presented
some preliminary results along these lines for the
system in question. We extend this work in a num-
ber of ways. Our principal aim is to test the va-
lidity not only of general-purpose metrics which
measure surface properties of text, but also of met-
rics which make use of domain knowledge, in the
sense that they attempt to relate the ‘deep seman-
tics’ of the texts to extrinsic factors, based on an
ontology for the BT-45 domain.
After an overview of related work in section 2,
the BT-45 system, its domain ontology and the ex-
trinsic evaluation are described in section 3. The
ontology plays an important role in the evaluation
metrics presented in Section 5. Finally, the eval-
uation of the methods is presented in Section 6,
before discussing and concluding in Section 7.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999971153846154">
In NLG evaluation, extrinsic, task-based methods
play a significant role (Reiter et al., 2003; Karasi-
mos and Isard, 2004; Stock et al., 2007). De-
pending on the study design, these studies often
leave open the question of precisely which as-
pects of a system (and of the text it generates)
contribute to success or failure. Intrinsic NLG
evaluations often involve ratings of text quality
or responses to questionnaires (Lester and Porter,
1997; Callaway and Lester, 2002; Foster, 2008),
with some studies using post-editing by human ex-
perts (Reiter et al., 2005). Automatically com-
puted metrics exploiting corpora, such as BLEU,
NIST and ROUGE, have mainly been used in eval-
uations of the coverage and quality of morphosyn-
tactic realisers (Langkilde-Geary, 2002; Callaway,
2003), though they have recently also been used
for subtasks such as Referring Expression Gener-
ation (Gatt and Belz, to appear) as well as end-to-
end weather forecasting systems (Reiter and Belz,
2009). The widespread use of these metrics in
NLP partly rests on the fact that they are quick
and cheap, but there is controversy about their re-
liability both in MT (Calliston-Burch et al., 2006)
and summarisation (Dorr et al., 2005; Liu and Liu,
2008). As noted in Section 1, similar questions
have been raised in NLG. One of the problems
associated with these metrics is that they rely on
the notion of a ‘gold standard’, which is not al-
ways precisely definable given multiple solutions
to the same generation, summarisation or transla-
tion task. These observations underlie recent de-
velopments in Summarisation evaluation such as
the Pyramid method (Nenkova and Passonneau,
2004), which in addition also emphasises content
overlap with a set of reference summaries, rather
than n-gram matches.
It is interesting to note that, with some excep-
tions (Foster, 2008), most of the methodologi-
cal studies on intrinsic evaluation cited here have
focused on ‘generic’ metrics (corpus-based au-
tomatic measures being foremost among them),
none of which use domain knowledge to quantify
those aspects of a text related to its content. There
is some work in Summarisation that suggests that
incorporating more knowledge improves results.
For example, Yoo and Song (Yoo et al., 2007)
used the Medical Subject Headings (MeSH) to
construct graphs representing the high-level con-
tent of documents, which are then used to clus-
ter documents by topic, each cluster being used to
produce a summary. In (Plaza et al., 2009), the
authors have proposed a summarisation method
based on WordNet concepts and showed that this
higher level representation improves the summari-
sation task.
The principal aim of this paper is to develop
metrics with which to compare texts using domain
knowledge – in the form of the ontology used in
the BT-45 system – and to correlate results to hu-
man decision-making performance. The resulting
metrics focus on aspects of content, structure and
relevance that are shown to correlate meaningfully
with task performance, in contrast to other, more
surface-oriented ones (such as ROUGE).
</bodyText>
<sectionHeader confidence="0.456999" genericHeader="method">
3 The BT-45 System
</sectionHeader>
<bodyText confidence="0.945393257142857">
BT-45(Portet et al., 2009) was designed to gen-
erate a textual summary of 45 minutes of patient
data in a Neonatal Intensive Care Unit (NICU), of
the kind shown in Figure 1(a). The corresponding
summary for the same data shown in Figure 1(b)
is a two-step consensus summary written by two
expert neonatologists. These two summaries cor-
respond to two of the conditions in the task-based
evaluation experiment described below.
In BT-45, summaries such as Figure 1(a) were
generated from raw input data consisting of (a)
physiological signals measured using sensors for
various parameters (such as heart rate); and (b)
discrete events logged by medical staff (e.g. drug
administration). The system was based on a
pipeline architecture which extends the standard
NLG tasks such as document planning and mi-
croplanning with preliminary stages for data anal-
ysis and reasoning. The texts generated were de-
scriptive, that is, they kept interpretation to a min-
imum (for example, the system did not make di-
agnoses). Nor were they generated with a bias to-
wards specific problems or actions that could be
considered desirable for a clinician to take in a par-
ticular context.
Every stage of the generation process made use
of a domain-specific ontology of around 550 con-
cepts, an excerpt of which is shown in Figure 1(c).
The ontology classified objects of type EVENT
and ENTITY into several subtypes; for example,
a DRUG ADMINISTRATION is an INTERVENTION,
which means it involves an agent and a patient.
The ontology functioned as a repository of declar-
ative knowledge, on the basis of which produc-
tion rules were defined to support reasoning in or-
der to make abstractions and to identify relations
(such as causality) between events detected in the
data based on their ontological class and their spe-
cific properties. In addition to the standard IS-A
links, the ontology contains functional relation-
ships which connect events to concepts represent-
ing physiological systems (such as the respiratory
or cardiovascular systems); these are referred to as
functional concepts. For example, in Figure 1(c), a
FEED event is linked to NUTRITION, meaning that
it is primarily relevant to the nutritional system.
These links were included in the ontology follow-
ing consultation with a senior neonatal consultant
after the development of BT-45 was completed.
Their inclusion was motivated by the knowledge-
based evaluation metrics developed for the pur-
poses of the present study, and discussed further
Over the next 38 minutes T1 stayed at around
37.4.
By 13:33 TcPO2 had rapidly decreased to 2.7.
Previously HR had increased to 173.
By 13:35 there had been 2 successive desatura-
tions down to 56. Previously T2 had increased
to 35.5.
By 13:40 SaO2 had rapidly decreased to 79.
At the start of the monitoring period: HR
baseline is 145-155, oxygen saturation is
99%, pO2 = 4.9 and CO2 = 10.3 Mean BP is
37-47; T1 and T2 are 37.3degC and 34.6degC
respectively.
At 13:33 there is a desaturation to 59%, which
is accompanied by a drop in pO2 to 1.3 and
a decrease in HR to 122. The blood pressure
rises toward the end of this episode to 49. These
parameters return to their baselines by 13:37.
</bodyText>
<figure confidence="0.967986">
(a) BT-45 summary (b) Human summary (c) Ontology excerpt
</figure>
<figureCaption confidence="0.999888">
Figure 1: Excerpts from Human and BT-45 summaries, and ontology example.
</figureCaption>
<bodyText confidence="0.999713794117647">
in Section 5.
The task-based experiment to evaluate BT-45
was conducted off-ward and involved a group of
35 clinicians, who were exposed to 24 scenarios,
each covering approximately 45 minutes of patient
data, together with a short introductory text which
gave some background about the patient. The pa-
tient data was then presented in one of three condi-
tions: graphically using a time-series plot, and tex-
tually in the form of a consensus summary written
by human experts (H; Figure 1(b)) and one gener-
ated automatically by BT-45(C; Figure 1(a)). Like
the BT-45 texts, the H texts did not give interpre-
tations or diagnoses and every effort was made not
to bias a reader in favour of certain courses of ac-
tion. A Latin Square design was used to ensure
that each scenario was shown to an equal number
of participants in each condition, while no partici-
pant saw the same scenario in more than one con-
dition.
For each scenario, the task was to select one
or more appropriate clinical actions from a prede-
fined set of 18, one of which was ‘no action’. Se-
lections had to be made within three minutes, after
which the scenario timed out. The same choice of
18 actions was given in each scenario s, but for
each one, two neonatal experts identified the sub-
sets of appropriate (AP3), inappropriate/potentially
harmful (INAP3) and neutral actions. One of the
appropriate actions was also deemed to be the ‘tar-
get’, that is, the most important action to take.
In three scenarios, the ‘target’ was ‘no action’.
For each participant p and scenario s, the perfor-
mance score P3p was based on the proportion PAP.
of actions selected out of AP3, and the proportion
PINAP3 selected out of the set of inappropriate ac-
tions INAP3: P3 p = PAPS − PINAP3 ∈ [−1, 1].
Overall, decision making in the H condition was
better (P3 = .45SD=.10) than either C (P3 =
.41SD=.13) or G (P3 = .40SD=.15). No sig-
nificant difference was found between the latter
two, but the H texts were significantly better than
the C texts, as revealed in a by-subjects ANOVA
(F(1, 31) = 5.266,p &lt; 0.05). We also performed
a post-hoc analysis, comparing the proportions of
appropriate actions selected, PAP and that of inap-
propriate actions PINAP in the H and C conditions
across scenarios. In addition, we computed a dif-
ferent score SPAP, defined as the proportion of ap-
propriate actions selected by a participant within
a scenario out of the total number of actions se-
lected (effectively a measure of ‘precision’). A
comparison between means for these three scores
obtained across scenarios showed no significant
differences.
In the analysis reported in Section 6, we com-
pare our textual metrics to both the global score
P as well as to these three other performance in-
dicators. In various follow-up analyses (van der
Meulen et al., 2009; Reiter et al., 2008), it was
found that the three scenarios in which the tar-
get action was ‘no action’ may have misled some
participants, insofar as this option was included
among a set of other actions, some of which were
themselves deemed appropriate or at least neutral
(in the sense that they could be carried out without
harming the patient). We shall therefore exclude
these scenarios from our analyses.
</bodyText>
<figure confidence="0.985241588235294">
&lt;P&gt;
At 14:15 hours
&lt;EVENT TYPE=&amp;quot;HEEL_PRICK&amp;quot; ID=&amp;quot;e11&amp;quot;&gt;
a heel prick is done.
&lt;/EVENT&gt;
&lt;EVENT TYPE=&amp;quot;TREND&amp;quot; SOURCE=&amp;quot;HR&amp;quot; DIRECTION=&amp;quot;increasing&amp;quot; ID=&amp;quot;e12&amp;quot;&gt;
The HR increases
&lt;/EVENT&gt;
at this point and for 7 minutes from the start of this procedure
&lt;EVENT CARDINALITY=&amp;quot;3&amp;quot; SOURCE=&amp;quot;SaO2&amp;quot; TYPE=&amp;quot;ARTIFACT&amp;quot; ID=&amp;quot;e13&amp;quot;&gt;
there is a lot of artefact in the oxygen saturation trace.
&lt;/EVENT&gt;
&lt;/P&gt;
&lt;TREL ARG0=&amp;quot;e11&amp;quot; ARG1=&amp;quot;TIMESTAMP&amp;quot; RELATION=&amp;quot;at&amp;quot; /&gt;
&lt;TREL ARG0=&amp;quot;e12&amp;quot; ARG1=&amp;quot;e11&amp;quot; RELATION=&amp;quot;starts&amp;quot; /&gt;
&lt;TREL ARG0=&amp;quot;e13&amp;quot; ARG1=&amp;quot;e11&amp;quot; RELATION=&amp;quot;starts&amp;quot; /&gt;
(a) Annotation (b) Normalised tree
</figure>
<figureCaption confidence="0.999508">
Figure 2: Fragment of an annotated summary and normalised tree representation.
</figureCaption>
<sectionHeader confidence="0.957187" genericHeader="method">
4 Corpus Annotation
</sectionHeader>
<bodyText confidence="0.999962127272728">
For this study, we annotated the H and C texts
from our experiment using the ontology, in or-
der to make both their semantic content and struc-
ture explicit. Figure 2(a) shows an excerpt from
an annotated text. Every paragraph of the text is
marked up explicitly. All segments of the text cor-
responding to an ontology EVENT are marked up
with a TYPE (the name of the concept in the on-
tology) and other properties, such as DIRECTION
and SOURCE in the case of trends in physiolog-
ical parameters. The CARDINALITY attribute is
used to indicate that a single text segment abstracts
over several occurrences in the data; for example,
the statement about artefacts in the example corre-
sponds to three such episodes in the data.
In addition to events, the markup also includes
separate nodes for all the temporal (TREL) and
discourse (DREL) relations which are explicitly
mentioned in the text, typically using adverbial or
prepositional phrases or verbs of causality. Ev-
ery TREL and DREL points to two arguments and
has a RELATION attribute. In the case of a TREL,
the value is one of the temporal relations de-
fined by (Allen, 1983). For DRELs, values were
restricted to CAUSE and CONTRAST (Mann and
S.Thompson, 1988). One of the arguments of a
TREL can be a timestamp, rather than an event.
This is the case for the first sentence in the frag-
ment, where event e11 is specified as having oc-
curred at a specific time (at 14:15). By contrast,
r4 is a relation between e11 and e12, where
the RELATION is STARTS, indicating that the text
specifies that e11 is used by the author as the an-
chor point to specify the start of e12, as reflected
by the expression at this point.
The markup provided the basis on which many
of the metrics described in the following section
were computed. Based on the annotation, we
used a normalised structural representation of the
texts as shown in Figure 2(b), consisting of PARA-
GRAPH (P) nodes which subsume events and rela-
tions. Relations dominate their event arguments.
For example, the starts TREL holding between
e12 and e11 is represented by a STARTS node
subsuming the two events. In case an event is
dominated by more than one relation (for exam-
ple, it is temporally related to two events, as e11
is in Figure 2(a), we maintain the tree structure by
creating two copies of the event, which are sub-
sumed by the two relations. Thus, the normalised
tree representation is a ‘compiled out’ version of
the graph representing all events and their rela-
tions. The tree representation is better suited to
our needs, given the complexity of comparing two
graphs.
</bodyText>
<sectionHeader confidence="0.998318" genericHeader="method">
5 Metrics
</sectionHeader>
<bodyText confidence="0.997658375">
The evaluation metrics used to score texts written
by domain experts and those generated by the BT-
45 system fall into three main classes, described
below.
Semantic content and structure To compare
both the content and the structure of texts, we used
three measures. The first quantifies the number
of EVENT nodes in an annotated text, defined as
</bodyText>
<equation confidence="0.7175725">
E
e∈E c, where E is the set of events mentioned,
</equation>
<bodyText confidence="0.999628644859813">
and c is the value of the CARDINALITY attribute
of an event e E E. Similarly, we computed the
number of temporal (TREL) and discourse (DREL)
relations mentioned in a text. We also used the
Tree Edit Distance metric to compute the distance
between the tree representations of the H and C
texts (see Figure 2(b)). This measure computes
the minimum number of node insertions, dele-
tions and substitutions required to transform one
tree into another and therefore takes into account
not only the content (events and relations) but
also its structural arrangement in the text. The
edit distance between two trees is computed using
the standard Levenshtein edit distance algorithm,
computed over a string that represents the preorder
traversal of the two trees, using a cost of 1 for in-
sertions and deletions, and 2 for substitutions.
N-gram overlap As a measure of n-gram over-
lap, we use ROUGE-n, which measures simple
n−gram overlap (in the present paper we use
n = 4). We also use ROUGE-SU, in which over-
lap is computed using skip-bigrams while also ac-
counting for unigrams that a text has in common
with its reference (in order to avoid bias against
texts which share several unigrams but few skip
bigrams).
Domain-dependent relevance metrics The
metrics described so far make use of domain
knowledge only to the extent that this is reflected
in the textual markup. We now consider a family
of metrics which are much more heavily reliant
on domain-specific knowledge structures and
reasoning. In our domain, the relevance of a
text in a given experimental scenario s can be
defined in terms of whether the events it mentions
have some relationship to the appropriate clinical
actions (APs). We attempt to model some aspects
of this using a weighting strategy and reasoning
rules.
Recall from Section 3 that fc’s represent the
various physiological systems to which an event
or action can be related. Therefore, each event e
mentioned in a text can be related to a set of pos-
sible actions using the functional concepts fc(e)
to which that event is linked in the ontology. Let
Es,t be the set of events mentioned in text t for
scenario s. An event e E Es ,t references an action
a iff FC(e) n FC(a) � 0. Our hypothesis is that
an appropriate action is more likely to be taken if
there are events which reference it in the text – that
is, if the text mentions things which are directly or
indirectly relevant to the action. For instance, if a
text mentions events related to the RESPIRATION
fc, a clinician might be more likely to make a de-
cision to manage a patient’s ventilation support.
It is worth emphasising that, since both the BT-
45 and human-authored texts were descriptive and
were not written or generated with the appropriate
actions in mind, the hypothesis that the relevance
of the content to the appropriate actions might in-
crease the likelihood of these actions being chosen
is far from a foregone conclusion.
Part of the novelty in this way of quantifying
relevance lies in its use of the knowledge (i.e. the
ontology) that is already available to the system,
rather than asking human experts to rate the rele-
vance of a text, a time-consuming process which
could be subject to experts’ personal biases. How-
ever, this way of conceptualising relevance gener-
ates links to too many actions for one event. It is
often the case that an event, through its association
with a functional concept, references more than
one action, but not all of these are appropriate.
For example, a change in oxygen saturation can
be related to RESPIRATION, which itself is related
to several respiration-related actions in a scenario,
only some of which are appropriate. Clearly, rele-
vance depends not only on a physiological connec-
tion between an event and a phsiological system
(functional concept), but also on the context, that
is, the other events and their relative importance
in a given scenario. Another factor that needs to
be taken into account is the overall probability of
an action. Some actions are performed routinely,
while others tend to be associated with emergen-
cies (e.g. a nappy change is much more frequent
over all than resuscitating a patient). This means
that some actions – even appropriate ones – may
have been less likely to be selected even though
they were referenced by the text and were appro-
priate.
We prune unwarranted connections between
events and actions by taking into account (a) a pa-
tient’s current status (described in the text and in
the background information given to experimental
participants); (b) the fact that some actions have
much higher prior probabilities than others be-
cause they are performed more routinely; (c) the
fact that some events may be more important than
others (e.g. resuscitation is much more important
than a nappy change). Based on this, we define the
weight of an action a as follows:
Where E is the set of events in the text, Ae the
set of actions related to event e, e.importance ∈
N+ the importance of the event e and Pr(a) the
prior probability of action a. All weights are nor-
malised so that the following inequalities hold:
</bodyText>
<equation confidence="0.99007875">
E Pr(a) ∗ e.importance
aEAe EaEAe Pr(a) = e.importance (2)
E Wa = 1 (3)
aEA
</equation>
<bodyText confidence="0.992288087719298">
where A is the set of all possible actions. The idea
is that an event e makes some contribution (pos-
sibly 0) to the relevance of some actions Ae, and
the total weight of the event is distributed among
all actions related to it using (a) the prior probabil-
ity Pr(a) of each action (the most frequent action
will have more weight) and (b) the importance of
the event. At the end of the process each action
would be assigned a score representing the accu-
mulated weights of the events, which is then nor-
malised, so that Ea∈A Wa = 1.
The prior probability in the equation is meant
to reflect our earlier observation that clinical ac-
tions differ in the frequency with which they are
performed and this may bias their selection. Pri-
ors were computed using maximum likelihood es-
timates from a large database containing exhaus-
tive annotations of clinical actions recorded by an
on-site research nurse over a period of 4 months in
a NICU, which contains a total of 43,889 records
of actions (Hunter et al., 2003).
The importance value in equation (1) is meant
to reflect the fact that events in the text do not
attract the attention of a reader to the same ex-
tent, since they do not have the same degree
of ‘severity’ or ‘surprise’. We operationalise
this by identifying the superconcepts in the on-
tology (PATHOLOGICAL-FUNCTION, DISORDER,
SURGICAL-INTERVENTION, etc.) which could be
thought of as representing ‘drastic’ occurrences.
To these we added the concept of a TREND which
corresponds to a change in a physiological param-
eter (such as an increase in heart rate), based on the
rationale that the primary aim of NICU staff is to
keep a patient stable, so that any physiological in-
stability warrants an intervention. The importance
of events subsumed by these superconcepts was
then set to be three times that of ‘normal’ events.
Finally, we apply knowledge-based rules to
prune the number of actions Ae related to an event
e. As an example, a decision to intubate a baby
depends not only on events in the text which ref-
erence this action, but also on whether the baby is
already intubated. This can be assessed by check-
ing whether s/he is on CMV (a type of ventila-
tion which is only used after intubation). The rule
is represented as INTUBATE → ¬on(baby, CMV).
Although such rules are extremely rough, they do
help to prune inconsistencies.
Two scores were computed for both human
and computer texts using equation (1). RELs,t
is the sum of weights of actions referenced in
a text t for scenario s which are appropriate:
RELs,t = Ea∈Aap Wa. Conversely, IRRELs,t
quantifies the weights of actions referenced in t
for scenario s which are inappropriate: IRRELs,t =
Ea∈Ainap Wa.
</bodyText>
<sectionHeader confidence="0.998619" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999970318181818">
In what follows, we report two-tailed Pearson’s r
correlations to compare our metrics to the three
performance measures discussed in Section 3: P,
the global performance score; PAPP and PINAPP, the
proportion of appropriate (resp. inappropriate) ac-
tions selected from the subsets of in/appropriate
(resp. inappropriate) actions in a scenario; and
SPAPP, the proportion of appropriate actions se-
lected by a participant out of the set of actions se-
lected. The last three are included because they
shed light more directly on the extent to which
experimental participants chose correctly or incor-
rectly. In case a metric measures similarity or dif-
ference between texts, the correlation reported is
with the difference between the H scores and the C
scores. Where relevant, we also report correlations
with the absolute mean performance scores within
the H and/or C conditions. Correlations exclude
the three scenarios which had ‘no action’ as the
target appropriate action, though where relevant,
we will indicate whether the correlations change
when these scenarios are also included.
</bodyText>
<subsectionHeader confidence="0.998811">
6.1 Content and Structure
</subsectionHeader>
<bodyText confidence="0.999483">
Overall, the C texts mentioned significantly fewer
events than the H texts (t20 = 2.44,p = .05),
and also mentioned fewer temporal and discourse
relations explicitly (t20 = 3.70, p &lt; .05). In
</bodyText>
<equation confidence="0.962562">
EeEE e.importance (1)
Wa =
</equation>
<table confidence="0.884145166666667">
Pr(a)*e. importance
PeEE Pa∈Ae Pr(a)
P (H-C) PAPP (H-C) SPAPP (H-C) PINAP (H-C)
Events (H-C) .43♦ .42♣ .02 -.09
Relations (H-C) .34 .30 0 -.15
Tree Edit .36 .33 .09 -.14
</table>
<tableCaption confidence="0.763772">
Table 1: Correlations between performance differences and content/structure measures. ♦significant at
p = .05; ♣approaches significance at p = .06
</tableCaption>
<table confidence="0.997858">
Absolute Scores (C) Differences (H-C)
P PAP PINAP SPAP P PAP PINAP SPAP
R-4 .33 .38 .2 -.03 -.19 -.2 -.01 -.1
R-SU -.03 -.02 .05 -.31 .04 .01 -.1 .13
</table>
<bodyText confidence="0.999785159090909">
the case of the H texts, the number of events
and relations did not correlate significantly with
any of the performance scores. In the case of
the C texts, the number of relations mentioned
was significantly negatively correlated to PINAP
(r = −.49,p &lt; .05), and positively correlated
to SPAPP (r = .7,p &lt; .001). This suggests
that temporal and discourse relations made texts
more understandable and resulted in more appro-
priate actions being taken. More unexpectedly, the
number of events mentioned was negatively cor-
related to PAPP (r = −.53,p &lt; .05) and to P
(r = −.5, p &lt; .05). This may have been due to the
C texts mentioning a number of events that were
relatively unimportant and/or irrelevant to the ap-
propriate actions.
Table 1 displays correlations between perfor-
mance differences between H and C, and differ-
ences in number of events and relations, as well
as Tree Edit Distance. The positive correlation
between the number of events mentioned and P
suggests that a larger amount of content in the
H texts is partially responsible for the difference
in decision-making accuracy by experimental par-
ticipants. This is further supported by the fact
that the correlation with the difference in PAPP ap-
proaches significance. It is worth noting that none
of these correlations are significant when means
from the three ‘no action’ scenarios are included
in the computation. This further supports our ear-
lier conclusion that these three scenarios are out-
liers. Somewhat surprisingly, Tree Edit Distance
does not correlate significantly with any of the per-
formance differences, though the correlations go
in the expected directions (positive in the case of
P, SPAPP and PAPP, negative in the case of PINAP).
This may be due to the high variance in the Edit
Distance scores (mean: 66.5; SD: 34.8).
Overall, these results show that differences in
both content and structure made the H texts supe-
rior and human texts did a much better job at ex-
plicitly relating events or situating them in time,
which is crucial for comprehension and correct
decision-making. This point has previously been
</bodyText>
<tableCaption confidence="0.7112245">
Table 2: Correlations between ROUGE and perfor-
mance scores in the C condition. ♦significant at
</tableCaption>
<equation confidence="0.581982">
p = .05.
</equation>
<bodyText confidence="0.993257">
made in relation to the same data on the basis of a
qualitative study (Reiter et al., 2008).
</bodyText>
<subsectionHeader confidence="0.994976">
6.2 N-gram Overlap
</subsectionHeader>
<bodyText confidence="0.999907115384616">
Correlations with ROUGE-4 and ROUGE-SU are
shown in Table 2 both for absolute performance
scores on the C texts, and for the differences be-
tween H and C. This is because ROUGE can be
interpreted in two ways: on the one hand, it mea-
sures the ‘quality’ of C texts relative to the ref-
erence human texts; on the other it also indicates
similarity between C and H.
There are no significant correlations between
ROUGE and any of our performance measures. Al-
though this leaves open the question of whether a
different set of performance measures, or a differ-
ent experiment, would evince a more systematic
covariation, the results suggest that it is not sur-
face similarity (to the extent that this is measured
by ROUGE) that is contributing to better decision
making. It is however worth noting that some cor-
relations with ROUGE-4, namely those involving
P and PAPP, do turn out significant when the ‘no
action’ scenarios are included. This turns out to
be solely due to one of the ‘no action’ scenar-
ios, which had a much higher ROUGE-4 score than
the others, possibly because the corresponding hu-
man text was comparatively brief and the number
of events mentioned in the two texts was roughly
equal (11 for the C text, 12 for the H text).
</bodyText>
<subsectionHeader confidence="0.992176">
6.3 Knowledge Based Relevance Metrics
</subsectionHeader>
<bodyText confidence="0.99901375">
Finally, we compare our knowledge-based mea-
sures of the relevance of the content to appropri-
ate actions (REL) and to inappropriate actions (IR-
REL). The correlations between each measure and
</bodyText>
<table confidence="0.99745">
Human (H) BT-45 (C)
P PAP PINAP SPAP P PAP PINAP SPAP
REL .14 .11 -.14 .60♦ .33 .24 -.49♦ .7♦
IRREL -.25 -.22 .1 -.56♦ -.34 -.26 .43 -.62♦
</table>
<tableCaption confidence="0.9838665">
Table 3: Correlations between knowledge-based relevance scores and absolute performance scores in the
C and H conditions. ♦significant at p ≤ .05.
</tableCaption>
<bodyText confidence="0.998274">
the absolute performance scores in each condition
are displayed in Table 3.
The absolute scores in Table 3 show that both
REL and IRREL are significantly correlated to
SPAPP, the proportion of appropriate actions out
of the actions selected by participants. The cor-
relations are in the expected direction: there is a
strong tendency for participants to choose more
appropriate actions when REL is high, and the re-
verse is true for IRREL. In the case of the C texts,
there is also a negative correlation (as expected)
between REL and PINAP, though this is the only
one that reaches significance with this variable. It
therefore appears that the knowledge-based rele-
vance measures evince a meaningful relationship
with at least some of the more ‘direct’ measures of
performance (those assessing the relative prefer-
ence of participants for appropriate actions based
on a textual summary), though not with the global
preference score P. One possible reason for the
low correlations with the latter is that the two mea-
sures attempt to quantify directly the relevance of
the content units in a text to in/appropriate courses
of action; hence, they have a more direct relation-
ship to measures of proportions of the courses of
actions chosen.
</bodyText>
<sectionHeader confidence="0.995861" genericHeader="conclusions">
7 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999995637931035">
We conclude this paper with some observations
about the relative merit of different measures of
textual characteristics. ‘Standard’, surface-based
measures such as (ROUGE) do not display any sys-
tematic relationship with our extrinsic measures
of performance, recalling similar observations in
the NLG literature (Gatt and Belz, to appear) and
in MT and Summarisation (Calliston-Burch et al.,
2006; Dorr et al., 2005). Some authors have also
reported that ROUGE does not correlate well with
human judgements of NLG texts (Reiter and Belz,
2009). On the other hand, we do find some evi-
dence that the amount of content in texts, and the
extent to which they explicitly relate content el-
ements temporally and rhetorically, may impact
decision-making. The significant correlations ob-
served between the number of relations in a text
and the extrinsic measures are worth emphasis-
ing, as they suggest a significant role not only for
content, but also rhetorical and temporal structure,
something that many metrics do not take into ac-
count.
Perhaps the most important contribution of this
paper has been to emphasise knowledge-based as-
pects of textual evaluation, not only by measur-
ing content units and structure, but also by de-
veloping a motivated relevance metric, the cru-
cial assumption being that the utility of a sum-
mary is contingent on its managing to convey in-
formation that will motivate a reader to take the
‘right’ course of action. The strong correlations
between the relevance measures and the extent to
which people chose the correct actions (or more
accurately, chose more correct actions) vindicates
this assumption.
Some of the correlations which turned out not to
be significant may be due to ‘noise’ in the data, in
particular, high variance in the performance scores
(as suggested by the standard deviations for P
given in Section 3). They therefore do not war-
rant the conclusion that no relationship exists be-
tween a particular measure and extrinsic task per-
formance; nevertheless, where other studies have
noted similar gaps, the trends in question may be
systematic and general. This, however, can only
be ascertained in further follow-up studies.
This paper has investigated the relationship be-
tween a number of intrinsic measures of text qual-
ity and decision-making performance based on an
external task. Emphasis was placed on metrics
that quantify aspects of semantics, relevance and
structure. We have also compared generated texts
to their human-authored counterparts to identify
differences which can motivate further system im-
provements. Future work will focus on further ex-
ploring metrics that reflect the relevance of a text,
as well as the role of temporal and discourse struc-
ture in conveying the intended meaning.
</bodyText>
<sectionHeader confidence="0.994331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999858473684211">
J. F. Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832–843.
Charles B. Callaway and James C. Lester. 2002.
Narrative prose generation. Arti�cial Intelligence,
139(2):213–252.
C. B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proc. IJCAI’03.
C. Calliston-Burch, M. Osborne, and P. Koehn. 2006.
Re-evaluating the role of BLEU in machine transla-
tion research. In Proc. EACL’06.
B. J. Dorr, C. Monz, S. President, R. Schwartz, and
D. Zajic. 2005. A methodology for extrinsic evalu-
ation of text summarization: Does ROUGE correlate?
In Proc. Workshop on Intrinsic and Extrinsic Evalu-
ation Measures.
M.E. Foster. 2008. Automated metrics that agree with
human judgements on generated output for an em-
bodied conversational agent. In Proc. INLG’08.
A. Gatt and A. Belz. to appear. Introducing shared task
evaluation to NLG: The TUNA shared task evalua-
tion challenges. In E. Krahmer and M. Theune, ed-
itors, Empirical Methods in Natural Language Gen-
eration. Springer.
A. Gatt and F. Portet. 2009. Text content and task
performance in the evaluation of a natural language
generation system. In Proc. RANLP’09.
J. Hunter, G. Ewing, L. Ferguson, Y. Freer, R. Logie,
P. McCue, and N. McIntosh. 2003. The NEONATE
database. In Proc. IDAMAP’03.
A. Karasimos and A. Isard. 2004. Multilingual eval-
uation of a natural language generation system. In
Proc. LREC’04.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG’02.
J.C. Lester and B.W. Porter. 1997. Developing and
empirically evaluating robust explanation genera-
tors: The KNIGHT experiments. Computational Lin-
guistics, 23(1):65–101.
C-Y Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. in.
In Proc. of HLT-NAACL’03.
F. Liu and Y. Liu. 2008. Correlation between rouge
and human evaluation of extractive meeting sum-
maries. In Proc. ACL’08.
W. C. Mann and S.Thompson. 1988. Rhetorical struc-
ture theory: Towards a functional theory of text or-
ganisation. Text, 8(3):243–281.
A. Nenkova and R. Passonneau. 2004. Evaluating
content selection in summarisation: The Pyramid
method. In Proc. NAACL-HLT’04.
S. Papineni, T. Roukos, W. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proc. ACL’02.
L Plaza, A D´ıaz, and P Gerv´as P. 2009. Auto-
matic summarization of news using wordnet concept
graphs. best paper award. In Proc. IADIS’09.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic generation
of textual summaries from neonatal intensive care
data. Arti�cial Intelligence, 173(7–8):789–816.
E. Reiter and A. Belz. 2009. An investigation into the
validity of some metrics for automatically evaluat-
ing Natural Language Generation systems. Compu-
tational Linguistics, 35(4):529–558.
E. Reiter, R. Robertson, and L. Osman. 2003. Lessons
from a failure: Generating tailored smoking cessa-
tion letters. Arti�cial Intelligence, 144:41–58.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated
weather forecasts. Arti�cial Intelligence, 167:137–
169.
E. Reiter, A. Gatt, F. Portet, and M. van der Meulen.
2008. The importance of narrative and other lessons
from an evaluation of an NLG system that sum-
marises clinical data. In Proc. INLG’08.
K. Sp¨arck-Jones and J. R. Galliers. 1996. Evaluating
natural language processing systems: An analysis
and review. Springer, Berlin.
O. Stock, M. Zancanaro, P. Busetta, C. Callaway,
A. Krueger, M. Kruppa, T. Kuflik, E. Not, and
C. Rocchi. 2007. Adaptive, intelligent presen-
tation of information for the museum visitor in
PEACH. User Modeling and User-Adapted Interac-
tion, 17(3):257–304.
M. van der Meulen, R. H. Logie, Y. Freer, C. Sykes,
N. McIntosh, and J. Hunter. 2009. When a graph is
poorer than 100 words. Applied Cognitive Psychol-
ogy, 24(1):77–89.
I. Yoo, X. Hu, and I-Y Song. 2007. A coherent
graph-based semantic clustering and summarization
approach for biomedical literature and a new sum-
marization evaluation method. BMC Bioinformat-
ics, 8(9).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.450214">
<note confidence="0.652146666666667">Textual Properties and Task Based Evaluation: Investigating the Role Surface Properties, Structure and Content. Gatt Portet</note>
<affiliation confidence="0.998887">Institute of Linguistics Laboratoire d’Informatique de Grenoble University of Malta Grenoble Institute of Technology</affiliation>
<email confidence="0.906832">albert.gatt@um.edu.mtfrancois.portet@imag.fr</email>
<abstract confidence="0.999368928571428">This paper investigates the relationship between the results of an extrinsic, taskevaluation of an and various metrics measuring both surface and deep semantic textual properties, including relevance. The latter rely heavily on domain knowledge. We show that they correlate systematically with some measures of performance. The core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of a text and task performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
</authors>
<title>Maintaining knowledge about temporal intervals.</title>
<date>1983</date>
<journal>Communications of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="14399" citStr="Allen, 1983" startWordPosition="2356" endWordPosition="2357">ribute is used to indicate that a single text segment abstracts over several occurrences in the data; for example, the statement about artefacts in the example corresponds to three such episodes in the data. In addition to events, the markup also includes separate nodes for all the temporal (TREL) and discourse (DREL) relations which are explicitly mentioned in the text, typically using adverbial or prepositional phrases or verbs of causality. Every TREL and DREL points to two arguments and has a RELATION attribute. In the case of a TREL, the value is one of the temporal relations defined by (Allen, 1983). For DRELs, values were restricted to CAUSE and CONTRAST (Mann and S.Thompson, 1988). One of the arguments of a TREL can be a timestamp, rather than an event. This is the case for the first sentence in the fragment, where event e11 is specified as having occurred at a specific time (at 14:15). By contrast, r4 is a relation between e11 and e12, where the RELATION is STARTS, indicating that the text specifies that e11 is used by the author as the anchor point to specify the start of e12, as reflected by the expression at this point. The markup provided the basis on which many of the metrics des</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>J. F. Allen. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM, 26(11):832–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles B Callaway</author>
<author>James C Lester</author>
</authors>
<title>Narrative prose generation.</title>
<date>2002</date>
<journal>Arti�cial Intelligence,</journal>
<volume>139</volume>
<issue>2</issue>
<contexts>
<context position="3512" citStr="Callaway and Lester, 2002" startWordPosition="547" endWordPosition="550">rics presented in Section 5. Finally, the evaluation of the methods is presented in Section 6, before discussing and concluding in Section 7. 2 Related Work In NLG evaluation, extrinsic, task-based methods play a significant role (Reiter et al., 2003; Karasimos and Isard, 2004; Stock et al., 2007). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and che</context>
</contexts>
<marker>Callaway, Lester, 2002</marker>
<rawString>Charles B. Callaway and James C. Lester. 2002. Narrative prose generation. Arti�cial Intelligence, 139(2):213–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C B Callaway</author>
</authors>
<title>Evaluating coverage for large symbolic NLG grammars.</title>
<date>2003</date>
<booktitle>In Proc. IJCAI’03.</booktitle>
<contexts>
<context position="3820" citStr="Callaway, 2003" startWordPosition="596" endWordPosition="597">tudy design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘g</context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>C. B. Callaway. 2003. Evaluating coverage for large symbolic NLG grammars. In Proc. IJCAI’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Calliston-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In Proc. EACL’06.</booktitle>
<contexts>
<context position="4206" citStr="Calliston-Burch et al., 2006" startWordPosition="660" endWordPosition="663">perts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summarisation evaluation such as the Pyramid method (Nenkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches.</context>
<context position="31611" citStr="Calliston-Burch et al., 2006" startWordPosition="5290" endWordPosition="5293">antify directly the relevance of the content units in a text to in/appropriate courses of action; hence, they have a more direct relationship to measures of proportions of the courses of actions chosen. 7 Discussion and Conclusions We conclude this paper with some observations about the relative merit of different measures of textual characteristics. ‘Standard’, surface-based measures such as (ROUGE) do not display any systematic relationship with our extrinsic measures of performance, recalling similar observations in the NLG literature (Gatt and Belz, to appear) and in MT and Summarisation (Calliston-Burch et al., 2006; Dorr et al., 2005). Some authors have also reported that ROUGE does not correlate well with human judgements of NLG texts (Reiter and Belz, 2009). On the other hand, we do find some evidence that the amount of content in texts, and the extent to which they explicitly relate content elements temporally and rhetorically, may impact decision-making. The significant correlations observed between the number of relations in a text and the extrinsic measures are worth emphasising, as they suggest a significant role not only for content, but also rhetorical and temporal structure, something that man</context>
</contexts>
<marker>Calliston-Burch, Osborne, Koehn, 2006</marker>
<rawString>C. Calliston-Burch, M. Osborne, and P. Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In Proc. EACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Dorr</author>
<author>C Monz</author>
<author>S President</author>
<author>R Schwartz</author>
<author>D Zajic</author>
</authors>
<title>A methodology for extrinsic evaluation of text summarization: Does ROUGE correlate?</title>
<date>2005</date>
<booktitle>In Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures.</booktitle>
<contexts>
<context position="4243" citStr="Dorr et al., 2005" startWordPosition="666" endWordPosition="669">ed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summarisation evaluation such as the Pyramid method (Nenkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches. It is interesting to note that, with</context>
<context position="31631" citStr="Dorr et al., 2005" startWordPosition="5294" endWordPosition="5297">of the content units in a text to in/appropriate courses of action; hence, they have a more direct relationship to measures of proportions of the courses of actions chosen. 7 Discussion and Conclusions We conclude this paper with some observations about the relative merit of different measures of textual characteristics. ‘Standard’, surface-based measures such as (ROUGE) do not display any systematic relationship with our extrinsic measures of performance, recalling similar observations in the NLG literature (Gatt and Belz, to appear) and in MT and Summarisation (Calliston-Burch et al., 2006; Dorr et al., 2005). Some authors have also reported that ROUGE does not correlate well with human judgements of NLG texts (Reiter and Belz, 2009). On the other hand, we do find some evidence that the amount of content in texts, and the extent to which they explicitly relate content elements temporally and rhetorically, may impact decision-making. The significant correlations observed between the number of relations in a text and the extrinsic measures are worth emphasising, as they suggest a significant role not only for content, but also rhetorical and temporal structure, something that many metrics do not tak</context>
</contexts>
<marker>Dorr, Monz, President, Schwartz, Zajic, 2005</marker>
<rawString>B. J. Dorr, C. Monz, S. President, R. Schwartz, and D. Zajic. 2005. A methodology for extrinsic evaluation of text summarization: Does ROUGE correlate? In Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
</authors>
<title>Automated metrics that agree with human judgements on generated output for an embodied conversational agent.</title>
<date>2008</date>
<booktitle>In Proc. INLG’08.</booktitle>
<contexts>
<context position="1212" citStr="Foster, 2008" startWordPosition="171" endWordPosition="172">rrelate systematically with some measures of performance. The core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of a text and task performance. 1 Introduction Evaluation methodology in NLG has generated a lot of interest. Some recent work suggested that the relationship between various intrinsic and extrinsic evaluation methods (Sp¨arck-Jones and Galliers, 1996) is not straightforward (Reiter and Belz, 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation ‘understudies’. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Portet et al., 2009), which summarises medical data for decision support purposes in a Neonatal Intensive Care Unit (</context>
<context position="3527" citStr="Foster, 2008" startWordPosition="551" endWordPosition="552">. Finally, the evaluation of the methods is presented in Section 6, before discussing and concluding in Section 7. 2 Related Work In NLG evaluation, extrinsic, task-based methods play a significant role (Reiter et al., 2003; Karasimos and Isard, 2004; Stock et al., 2007). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there i</context>
<context position="4874" citStr="Foster, 2008" startWordPosition="769" endWordPosition="770">8). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summarisation evaluation such as the Pyramid method (Nenkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches. It is interesting to note that, with some exceptions (Foster, 2008), most of the methodological studies on intrinsic evaluation cited here have focused on ‘generic’ metrics (corpus-based automatic measures being foremost among them), none of which use domain knowledge to quantify those aspects of a text related to its content. There is some work in Summarisation that suggests that incorporating more knowledge improves results. For example, Yoo and Song (Yoo et al., 2007) used the Medical Subject Headings (MeSH) to construct graphs representing the high-level content of documents, which are then used to cluster documents by topic, each cluster being used to pr</context>
</contexts>
<marker>Foster, 2008</marker>
<rawString>M.E. Foster. 2008. Automated metrics that agree with human judgements on generated output for an embodied conversational agent. In Proc. INLG’08.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Gatt</author>
<author>A Belz</author>
</authors>
<title>to appear. Introducing shared task evaluation to NLG: The TUNA shared task evaluation challenges.</title>
<booktitle>Empirical Methods in Natural Language Generation.</booktitle>
<editor>In E. Krahmer and M. Theune, editors,</editor>
<publisher>Springer.</publisher>
<marker>Gatt, Belz, </marker>
<rawString>A. Gatt and A. Belz. to appear. Introducing shared task evaluation to NLG: The TUNA shared task evaluation challenges. In E. Krahmer and M. Theune, editors, Empirical Methods in Natural Language Generation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
<author>F Portet</author>
</authors>
<title>Text content and task performance in the evaluation of a natural language generation system.</title>
<date>2009</date>
<booktitle>In Proc. RANLP’09.</booktitle>
<contexts>
<context position="2240" citStr="Gatt and Portet, 2009" startWordPosition="330" endWordPosition="333">ke up these questions in the context of a knowledge-based NLG system, BT-45 (Portet et al., 2009), which summarises medical data for decision support purposes in a Neonatal Intensive Care Unit (NICU). Our extrinsic data comes from an experiment involving complex medical decision making based on automatically generated and human-authored texts (van der Meulen et al., 2009). This gives us the opportunity to directly compare the textual characteristics of generated and human-written summaries and their relationship to decision-making performance. The present work uses data from an earlier study (Gatt and Portet, 2009), which presented some preliminary results along these lines for the system in question. We extend this work in a number of ways. Our principal aim is to test the validity not only of general-purpose metrics which measure surface properties of text, but also of metrics which make use of domain knowledge, in the sense that they attempt to relate the ‘deep semantics’ of the texts to extrinsic factors, based on an ontology for the BT-45 domain. After an overview of related work in section 2, the BT-45 system, its domain ontology and the extrinsic evaluation are described in section 3. The ontolog</context>
</contexts>
<marker>Gatt, Portet, 2009</marker>
<rawString>A. Gatt and F. Portet. 2009. Text content and task performance in the evaluation of a natural language generation system. In Proc. RANLP’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hunter</author>
<author>G Ewing</author>
<author>L Ferguson</author>
<author>Y Freer</author>
<author>R Logie</author>
<author>P McCue</author>
<author>N McIntosh</author>
</authors>
<title>The NEONATE database.</title>
<date>2003</date>
<booktitle>In Proc. IDAMAP’03.</booktitle>
<contexts>
<context position="22425" citStr="Hunter et al., 2003" startWordPosition="3753" endWordPosition="3756">e process each action would be assigned a score representing the accumulated weights of the events, which is then normalised, so that Ea∈A Wa = 1. The prior probability in the equation is meant to reflect our earlier observation that clinical actions differ in the frequency with which they are performed and this may bias their selection. Priors were computed using maximum likelihood estimates from a large database containing exhaustive annotations of clinical actions recorded by an on-site research nurse over a period of 4 months in a NICU, which contains a total of 43,889 records of actions (Hunter et al., 2003). The importance value in equation (1) is meant to reflect the fact that events in the text do not attract the attention of a reader to the same extent, since they do not have the same degree of ‘severity’ or ‘surprise’. We operationalise this by identifying the superconcepts in the ontology (PATHOLOGICAL-FUNCTION, DISORDER, SURGICAL-INTERVENTION, etc.) which could be thought of as representing ‘drastic’ occurrences. To these we added the concept of a TREND which corresponds to a change in a physiological parameter (such as an increase in heart rate), based on the rationale that the primary ai</context>
</contexts>
<marker>Hunter, Ewing, Ferguson, Freer, Logie, McCue, McIntosh, 2003</marker>
<rawString>J. Hunter, G. Ewing, L. Ferguson, Y. Freer, R. Logie, P. McCue, and N. McIntosh. 2003. The NEONATE database. In Proc. IDAMAP’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Karasimos</author>
<author>A Isard</author>
</authors>
<title>Multilingual evaluation of a natural language generation system.</title>
<date>2004</date>
<booktitle>In Proc. LREC’04.</booktitle>
<contexts>
<context position="3164" citStr="Karasimos and Isard, 2004" startWordPosition="489" endWordPosition="493">ledge, in the sense that they attempt to relate the ‘deep semantics’ of the texts to extrinsic factors, based on an ontology for the BT-45 domain. After an overview of related work in section 2, the BT-45 system, its domain ontology and the extrinsic evaluation are described in section 3. The ontology plays an important role in the evaluation metrics presented in Section 5. Finally, the evaluation of the methods is presented in Section 6, before discussing and concluding in Section 7. 2 Related Work In NLG evaluation, extrinsic, task-based methods play a significant role (Reiter et al., 2003; Karasimos and Isard, 2004; Stock et al., 2007). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyn</context>
</contexts>
<marker>Karasimos, Isard, 2004</marker>
<rawString>A. Karasimos and A. Isard. 2004. Multilingual evaluation of a natural language generation system. In Proc. LREC’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<booktitle>In Proc. INLG’02.</booktitle>
<contexts>
<context position="3803" citStr="Langkilde-Geary, 2002" startWordPosition="594" endWordPosition="595">07). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on t</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>I. Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Proc. INLG’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Lester</author>
<author>B W Porter</author>
</authors>
<title>Developing and empirically evaluating robust explanation generators: The KNIGHT experiments.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="3485" citStr="Lester and Porter, 1997" startWordPosition="543" endWordPosition="546">ole in the evaluation metrics presented in Section 5. Finally, the evaluation of the methods is presented in Section 6, before discussing and concluding in Section 7. 2 Related Work In NLG evaluation, extrinsic, task-based methods play a significant role (Reiter et al., 2003; Karasimos and Isard, 2004; Stock et al., 2007). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact </context>
</contexts>
<marker>Lester, Porter, 1997</marker>
<rawString>J.C. Lester and B.W. Porter. 1997. Developing and empirically evaluating robust explanation generators: The KNIGHT experiments. Computational Linguistics, 23(1):65–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics. in.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL’03.</booktitle>
<contexts>
<context position="1531" citStr="Lin and Hovy, 2003" startWordPosition="220" endWordPosition="223">Some recent work suggested that the relationship between various intrinsic and extrinsic evaluation methods (Sp¨arck-Jones and Galliers, 1996) is not straightforward (Reiter and Belz, 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation ‘understudies’. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Portet et al., 2009), which summarises medical data for decision support purposes in a Neonatal Intensive Care Unit (NICU). Our extrinsic data comes from an experiment involving complex medical decision making based on automatically generated and human-authored texts (van der Meulen et al., 2009). This gives us the opportunity to directly compare the textual characteristics of generated and human-written summaries and their relation</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C-Y Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. in. In Proc. of HLT-NAACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
<author>Y Liu</author>
</authors>
<title>Correlation between rouge and human evaluation of extractive meeting summaries.</title>
<date>2008</date>
<booktitle>In Proc. ACL’08.</booktitle>
<contexts>
<context position="4263" citStr="Liu and Liu, 2008" startWordPosition="670" endWordPosition="673">ng corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summarisation evaluation such as the Pyramid method (Nenkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches. It is interesting to note that, with some exceptions (Fo</context>
</contexts>
<marker>Liu, Liu, 2008</marker>
<rawString>F. Liu and Y. Liu. 2008. Correlation between rouge and human evaluation of extractive meeting summaries. In Proc. ACL’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical structure theory: Towards a functional theory of text organisation.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. C. Mann and S.Thompson. 1988. Rhetorical structure theory: Towards a functional theory of text organisation. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarisation: The Pyramid method.</title>
<date>2004</date>
<booktitle>In Proc. NAACL-HLT’04.</booktitle>
<contexts>
<context position="4692" citStr="Nenkova and Passonneau, 2004" startWordPosition="738" endWordPosition="741">tly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summarisation evaluation such as the Pyramid method (Nenkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches. It is interesting to note that, with some exceptions (Foster, 2008), most of the methodological studies on intrinsic evaluation cited here have focused on ‘generic’ metrics (corpus-based automatic measures being foremost among them), none of which use domain knowledge to quantify those aspects of a text related to its content. There is some work in Summarisation that suggests that incorporating more knowledge improves results. For example, Yoo and Song (Yoo et al., 2007) used the </context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and R. Passonneau. 2004. Evaluating content selection in summarisation: The Pyramid method. In Proc. NAACL-HLT’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Papineni</author>
<author>T Roukos</author>
<author>W Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL’02.</booktitle>
<contexts>
<context position="1500" citStr="Papineni et al., 2002" startWordPosition="214" endWordPosition="217"> has generated a lot of interest. Some recent work suggested that the relationship between various intrinsic and extrinsic evaluation methods (Sp¨arck-Jones and Galliers, 1996) is not straightforward (Reiter and Belz, 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation ‘understudies’. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Portet et al., 2009), which summarises medical data for decision support purposes in a Neonatal Intensive Care Unit (NICU). Our extrinsic data comes from an experiment involving complex medical decision making based on automatically generated and human-authored texts (van der Meulen et al., 2009). This gives us the opportunity to directly compare the textual characteristics of generated and human-writt</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>S. Papineni, T. Roukos, W. Ward, and W. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proc. ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Plaza</author>
<author>A D´ıaz</author>
<author>P Gerv´as P</author>
</authors>
<title>Automatic summarization of news using wordnet concept graphs. best paper award.</title>
<date>2009</date>
<booktitle>In Proc. IADIS’09.</booktitle>
<marker>Plaza, D´ıaz, P, 2009</marker>
<rawString>L Plaza, A D´ıaz, and P Gerv´as P. 2009. Automatic summarization of news using wordnet concept graphs. best paper award. In Proc. IADIS’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Portet</author>
<author>E Reiter</author>
<author>A Gatt</author>
<author>J Hunter</author>
<author>S Sripada</author>
<author>Y Freer</author>
<author>C Sykes</author>
</authors>
<title>Automatic generation of textual summaries from neonatal intensive care data.</title>
<date>2009</date>
<journal>Arti�cial Intelligence,</journal>
<pages>173--7</pages>
<contexts>
<context position="1715" citStr="Portet et al., 2009" startWordPosition="248" endWordPosition="251"> 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation ‘understudies’. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Portet et al., 2009), which summarises medical data for decision support purposes in a Neonatal Intensive Care Unit (NICU). Our extrinsic data comes from an experiment involving complex medical decision making based on automatically generated and human-authored texts (van der Meulen et al., 2009). This gives us the opportunity to directly compare the textual characteristics of generated and human-written summaries and their relationship to decision-making performance. The present work uses data from an earlier study (Gatt and Portet, 2009), which presented some preliminary results along these lines for the system</context>
<context position="6148" citStr="Portet et al., 2009" startWordPosition="971" endWordPosition="974">have proposed a summarisation method based on WordNet concepts and showed that this higher level representation improves the summarisation task. The principal aim of this paper is to develop metrics with which to compare texts using domain knowledge – in the form of the ontology used in the BT-45 system – and to correlate results to human decision-making performance. The resulting metrics focus on aspects of content, structure and relevance that are shown to correlate meaningfully with task performance, in contrast to other, more surface-oriented ones (such as ROUGE). 3 The BT-45 System BT-45(Portet et al., 2009) was designed to generate a textual summary of 45 minutes of patient data in a Neonatal Intensive Care Unit (NICU), of the kind shown in Figure 1(a). The corresponding summary for the same data shown in Figure 1(b) is a two-step consensus summary written by two expert neonatologists. These two summaries correspond to two of the conditions in the task-based evaluation experiment described below. In BT-45, summaries such as Figure 1(a) were generated from raw input data consisting of (a) physiological signals measured using sensors for various parameters (such as heart rate); and (b) discrete ev</context>
</contexts>
<marker>Portet, Reiter, Gatt, Hunter, Sripada, Freer, Sykes, 2009</marker>
<rawString>F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada, Y. Freer, and C. Sykes. 2009. Automatic generation of textual summaries from neonatal intensive care data. Arti�cial Intelligence, 173(7–8):789–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>A Belz</author>
</authors>
<title>An investigation into the validity of some metrics for automatically evaluating Natural Language Generation systems.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="1100" citStr="Reiter and Belz, 2009" startWordPosition="152" endWordPosition="155">deep semantic textual properties, including relevance. The latter rely heavily on domain knowledge. We show that they correlate systematically with some measures of performance. The core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of a text and task performance. 1 Introduction Evaluation methodology in NLG has generated a lot of interest. Some recent work suggested that the relationship between various intrinsic and extrinsic evaluation methods (Sp¨arck-Jones and Galliers, 1996) is not straightforward (Reiter and Belz, 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation ‘understudies’. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Porte</context>
<context position="4015" citStr="Reiter and Belz, 2009" startWordPosition="626" endWordPosition="629">ften involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summa</context>
<context position="31758" citStr="Reiter and Belz, 2009" startWordPosition="5315" endWordPosition="5318"> of proportions of the courses of actions chosen. 7 Discussion and Conclusions We conclude this paper with some observations about the relative merit of different measures of textual characteristics. ‘Standard’, surface-based measures such as (ROUGE) do not display any systematic relationship with our extrinsic measures of performance, recalling similar observations in the NLG literature (Gatt and Belz, to appear) and in MT and Summarisation (Calliston-Burch et al., 2006; Dorr et al., 2005). Some authors have also reported that ROUGE does not correlate well with human judgements of NLG texts (Reiter and Belz, 2009). On the other hand, we do find some evidence that the amount of content in texts, and the extent to which they explicitly relate content elements temporally and rhetorically, may impact decision-making. The significant correlations observed between the number of relations in a text and the extrinsic measures are worth emphasising, as they suggest a significant role not only for content, but also rhetorical and temporal structure, something that many metrics do not take into account. Perhaps the most important contribution of this paper has been to emphasise knowledge-based aspects of textual </context>
</contexts>
<marker>Reiter, Belz, 2009</marker>
<rawString>E. Reiter and A. Belz. 2009. An investigation into the validity of some metrics for automatically evaluating Natural Language Generation systems. Computational Linguistics, 35(4):529–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Robertson</author>
<author>L Osman</author>
</authors>
<title>Lessons from a failure: Generating tailored smoking cessation letters.</title>
<date>2003</date>
<journal>Arti�cial Intelligence,</journal>
<pages>144--41</pages>
<contexts>
<context position="3137" citStr="Reiter et al., 2003" startWordPosition="485" endWordPosition="488">ke use of domain knowledge, in the sense that they attempt to relate the ‘deep semantics’ of the texts to extrinsic factors, based on an ontology for the BT-45 domain. After an overview of related work in section 2, the BT-45 system, its domain ontology and the extrinsic evaluation are described in section 3. The ontology plays an important role in the evaluation metrics presented in Section 5. Finally, the evaluation of the methods is presented in Section 6, before discussing and concluding in Section 7. 2 Related Work In NLG evaluation, extrinsic, task-based methods play a significant role (Reiter et al., 2003; Karasimos and Isard, 2004; Stock et al., 2007). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the covera</context>
</contexts>
<marker>Reiter, Robertson, Osman, 2003</marker>
<rawString>E. Reiter, R. Robertson, and L. Osman. 2003. Lessons from a failure: Generating tailored smoking cessation letters. Arti�cial Intelligence, 144:41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
<author>J Hunter</author>
<author>J Yu</author>
<author>I Davy</author>
</authors>
<title>Choosing words in computer-generated weather forecasts.</title>
<date>2005</date>
<journal>Arti�cial Intelligence,</journal>
<volume>167</volume>
<pages>169</pages>
<contexts>
<context position="3604" citStr="Reiter et al., 2005" startWordPosition="562" endWordPosition="565">fore discussing and concluding in Section 7. 2 Related Work In NLG evaluation, extrinsic, task-based methods play a significant role (Reiter et al., 2003; Karasimos and Isard, 2004; Stock et al., 2007). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 200</context>
</contexts>
<marker>Reiter, Sripada, Hunter, Yu, Davy, 2005</marker>
<rawString>E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005. Choosing words in computer-generated weather forecasts. Arti�cial Intelligence, 167:137– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>A Gatt</author>
<author>F Portet</author>
<author>M van der Meulen</author>
</authors>
<title>The importance of narrative and other lessons from an evaluation of an NLG system that summarises clinical data. In</title>
<date>2008</date>
<booktitle>Proc. INLG’08.</booktitle>
<marker>Reiter, Gatt, Portet, van der Meulen, 2008</marker>
<rawString>E. Reiter, A. Gatt, F. Portet, and M. van der Meulen. 2008. The importance of narrative and other lessons from an evaluation of an NLG system that summarises clinical data. In Proc. INLG’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sp¨arck-Jones</author>
<author>J R Galliers</author>
</authors>
<title>Evaluating natural language processing systems: An analysis and review.</title>
<date>1996</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<marker>Sp¨arck-Jones, Galliers, 1996</marker>
<rawString>K. Sp¨arck-Jones and J. R. Galliers. 1996. Evaluating natural language processing systems: An analysis and review. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Stock</author>
<author>M Zancanaro</author>
<author>P Busetta</author>
<author>C Callaway</author>
<author>A Krueger</author>
<author>M Kruppa</author>
<author>T Kuflik</author>
<author>E Not</author>
<author>C Rocchi</author>
</authors>
<title>Adaptive, intelligent presentation of information for the museum visitor in PEACH. User Modeling and User-Adapted Interaction,</title>
<date>2007</date>
<contexts>
<context position="3185" citStr="Stock et al., 2007" startWordPosition="494" endWordPosition="497">ey attempt to relate the ‘deep semantics’ of the texts to extrinsic factors, based on an ontology for the BT-45 domain. After an overview of related work in section 2, the BT-45 system, its domain ontology and the extrinsic evaluation are described in section 3. The ontology plays an important role in the evaluation metrics presented in Section 5. Finally, the evaluation of the methods is presented in Section 6, before discussing and concluding in Section 7. 2 Related Work In NLG evaluation, extrinsic, task-based methods play a significant role (Reiter et al., 2003; Karasimos and Isard, 2004; Stock et al., 2007). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Lan</context>
</contexts>
<marker>Stock, Zancanaro, Busetta, Callaway, Krueger, Kruppa, Kuflik, Not, Rocchi, 2007</marker>
<rawString>O. Stock, M. Zancanaro, P. Busetta, C. Callaway, A. Krueger, M. Kruppa, T. Kuflik, E. Not, and C. Rocchi. 2007. Adaptive, intelligent presentation of information for the museum visitor in PEACH. User Modeling and User-Adapted Interaction, 17(3):257–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M van der Meulen</author>
<author>R H Logie</author>
<author>Y Freer</author>
<author>C Sykes</author>
<author>N McIntosh</author>
<author>J Hunter</author>
</authors>
<title>When a graph is poorer than 100 words.</title>
<date>2009</date>
<journal>Applied Cognitive Psychology,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>van der Meulen, Logie, Freer, Sykes, McIntosh, Hunter, 2009</marker>
<rawString>M. van der Meulen, R. H. Logie, Y. Freer, C. Sykes, N. McIntosh, and J. Hunter. 2009. When a graph is poorer than 100 words. Applied Cognitive Psychology, 24(1):77–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Yoo</author>
<author>X Hu</author>
<author>I-Y Song</author>
</authors>
<title>A coherent graph-based semantic clustering and summarization approach for biomedical literature and a new summarization evaluation method.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<issue>9</issue>
<contexts>
<context position="5282" citStr="Yoo et al., 2007" startWordPosition="831" endWordPosition="834">nkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches. It is interesting to note that, with some exceptions (Foster, 2008), most of the methodological studies on intrinsic evaluation cited here have focused on ‘generic’ metrics (corpus-based automatic measures being foremost among them), none of which use domain knowledge to quantify those aspects of a text related to its content. There is some work in Summarisation that suggests that incorporating more knowledge improves results. For example, Yoo and Song (Yoo et al., 2007) used the Medical Subject Headings (MeSH) to construct graphs representing the high-level content of documents, which are then used to cluster documents by topic, each cluster being used to produce a summary. In (Plaza et al., 2009), the authors have proposed a summarisation method based on WordNet concepts and showed that this higher level representation improves the summarisation task. The principal aim of this paper is to develop metrics with which to compare texts using domain knowledge – in the form of the ontology used in the BT-45 system – and to correlate results to human decision-maki</context>
</contexts>
<marker>Yoo, Hu, Song, 2007</marker>
<rawString>I. Yoo, X. Hu, and I-Y Song. 2007. A coherent graph-based semantic clustering and summarization approach for biomedical literature and a new summarization evaluation method. BMC Bioinformatics, 8(9).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>