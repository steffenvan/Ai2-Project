<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.130024">
<note confidence="0.816202">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 136-138, Lisbon, Portugal, 2000.
</note>
<title confidence="0.998686">
A Context Sensitive Maximum Likelihood Approach to Chunking
</title>
<author confidence="0.787531">
Christer Johansson
</author>
<affiliation confidence="0.573109">
Electrotechnical Laboratories Machine Understanding Division
1-1-4 Umez ono, Tsukuba. 305 Ibaraki, JAPAN
</affiliation>
<sectionHeader confidence="0.998757" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981653846154">
In Brill&apos;s (1994) groundbreaking work on parts-
of-speech tagging, the starting point was to as-
sign each word its most common tag. An ex-
tension to this first step is to utilize the lexical
context (i.e., words and punctuation) surround-
ing the word. This approach could obviously be
used for ordering tags into higher order units
(referred to as chunks) using chunk labels.
This paper will investigate the performance
of simply picking the most likely tag for a given
context, under the condition that a larger con-
text is allowed to override the most likely la-
bel of a smaller context. The results could be
extended by secondary error correction as in
Brill&apos;s tagger, but this exercise is left to the
reader to allow us to concentrate on the perfor-
mance based on storing and retrieving the most
likely examples only.
More sophisticated methods may use more
than one stored context to determine the la-
bel that best fits the current context (Van den
Bosch and Daelemans, 1998; Zavrel and Daele-
mans, 1997; Skousen, 1989, inter al.). The
method of this paper uses only one context to
determine the best label, but may decrease the
size of the context until a full match is found.
</bodyText>
<sectionHeader confidence="0.955592" genericHeader="keywords">
2 Outline of the procedure
</sectionHeader>
<subsectionHeader confidence="0.924699">
2.1 &amp;quot;Training&amp;quot;
</subsectionHeader>
<bodyText confidence="0.910562">
The training of this mechanism is to determine
which patterns in the training set are the most
likely. Only tag information is used. A filter to
convert a tag with a context into a chunk-label
is constructed as follows:
0) Construct symmetric n-contexts from the
training corpus. A 1-context is simply the most
common chunk-label for each tag. A 3-context
is the tag followed by the tag before and after
it, i.e., [to t_i t±iPabel. Similarly, a 5-context,
(i.e., [t_2 [ to ti] t+2]: label (of to)), is
represented [to t_i t+1 t_2 t+2]:label. Finally, a
7-context is represented as [to t_1 t1 t_2
t_3 t+3Pabel. It was verified that results do not
significantly improve using larger contexts than
5-contexts.
</bodyText>
<listItem confidence="0.987592583333333">
1) For each set of n-contexts, determine
the most frequent label for each occurring n-
context. For example, the tag CC most fre-
quently has the label B-NP if the context is
PRP CC RP. The most frequent label for CC
without extra context is &amp;quot;0&amp;quot;.
2) To save some storage space, the most fre-
quent label in an n-context is only added if it
is different from its nearest lower order context.
For example, the label B-NP can be added for
a 3-context since PRP CC RP gives a different
result from CC alone.
</listItem>
<subsectionHeader confidence="0.999585">
2.2 Testing
</subsectionHeader>
<bodyText confidence="0.9993608">
Testing is done by constructing the maximum
context for each tag, and look it up in the
database of the most likely patterns. If the
largest context cannot be found the context is
diminished step-by-step.
</bodyText>
<listItem confidence="0.951649666666667">
3) In the test phase we need to form the
longest contexts used in training (e.g., 7-
contexts). The first word to get a chunk label
is &apos;Rockwell&apos; (Rockwell International Corp. &apos;s)
and its corresponding 7-context (without its la-
bel) is NNP = NNP = NNP = POS, where
</listItem>
<bodyText confidence="0.93595975">
is a tag for a blank line (i.e., no text tag) since
this is the very first few words.
4) The only rule for chunk-labeling is to look
up the closest surviving n-context and output
its label. Simply look up [to ti t1 t_2 t+2 t_3
t4.3] ... [to] in that order until the context is
found. The [to] context alone produces a Fo=1
of 77.
</bodyText>
<page confidence="0.997921">
136
</page>
<sectionHeader confidence="0.999509" genericHeader="introduction">
3 Results
</sectionHeader>
<bodyText confidence="0.999989555555555">
The evaluation program shows that this simple
procedure reaches its best result for 5-contexts
(table 1) with 92.46% label accuracy and phrase
correctness measured by Fo=1 = 87.23. How-
ever, the improvement from 3-contexts to 5-
contexts is insignificant, as 3-contexts reached
92.41% accuracy and Fo=1=87.09. The results
for 7-contexts is almost identical to 5-contexts
(92.44% and Fo=1=87.21). This is taken as the
limit performance due to the size of the training
corpus.
In a larger training corpus, the most common
longer contexts are likely to be useful but in a
small set the longer contexts may occur with
very low frequencies making it hard to deter-
mine if the label of such contexts is the best
guess for unseen samples.
These results are the best that could be ex-
pected without generalization. In order to do
better, the method has to generalize to unseen
contexts, e.g., by using some notion of close
matching contexts (instances), to be able to use
longer context even when some of that context
has not been previously recorded. In addition,
the tag-structure could be productively utilized.
The presented method has treated all labels as
arbitrary, atomic and independent symbols.
</bodyText>
<subsectionHeader confidence="0.999038">
3.1 Computational complexity
</subsectionHeader>
<bodyText confidence="0.999980263157895">
Using rule 2 from section 2.1, 45 patterns &apos;sur-
vived&apos; for 1-contexts, and 3225, 71022, 38541
for 3-,5- and 7-contexts respectively, i.e., a total
of 45, 3270, 74292, 109563 using all contexts up
to and including 1-, 3-, 5- and 7-contexts. Each
unique context can be retrieved in one logical
step (i.e., a hash-table lookup). There are obvi-
ously many patterns in the database - but the
complexity of the task is limited to the number
of look-ups necessary.
There is a maximum of four hash-table look-
ups for each tag (i.e., when the 7-, 5-, and 3-
contexts does not exist in the database the most
likely label of the current tag will be used).
Good performance can be obtained within a
maximum of 2 look-ups for each label (i.e., us-
ing only 1- and 3-contexts) and the best results
were obtained with a maximum of 3 look-ups
per label.
</bodyText>
<sectionHeader confidence="0.996052" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.99997137037037">
The memory-based approach seemingly postu-
lates innate tags in the processing machinery.
The author has found very little discussion on
how the tags are thought to correspond to real-
ity, a fact that was also pointed out, not so long
ago, by Palmeri (1998). However, a few papers
aiming towards automatic &apos;label&apos;, &apos;feature&apos; or
&apos;tag&apos; creation are available (Miikkulainen and
Dyer, 1991; Johansson, 1999).
It is undeniable that, from a practical per-
spective, it is possible to reach very high perfor-
mance on tasks, such as tagging, that demand
a choice from a known set of alternatives by
estimating statistical properties (e.g., the most
likely label) from a large enough training set.
This makes the method extremely useful for
quick development of tools, which can be used
in practical applications such as text retrieval
and machine translation; but also in linguis-
tic research; e.g., finding examples of specific
grammatical constructions in large collections
of data.
A challenge for future research is how tags
could be constructed automatically, and what
kind of information would be necessary to de-
tect the relevant tag dimensions for some lin-
guistically motivated task.
</bodyText>
<sectionHeader confidence="0.986768" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99998995">
It was shown that using context made it possible
to improve performance of maximum likelihood
prediction. It was suggested that the limit of
performance for this method is implicitly given
by the size of the training set, as this determines
the significance of larger contexts, and increases
the chance of finding a matching longer con-
text. In smaller collections, large patterns are
a) likely to occur at a low frequency with few
competing labels and b) likely to not exist in
the test set. A larger collection will increase
the number of different contexts, as well as the
significance of picking the best, most frequent,
prediction from a set of (identical) competitors
with different labels.
The presented method does not generalize be-
yond what is recorded in the training set as the
most likely alternative. However, it is expected
to • improve with the size of the training set, as
this makes it feasible to use longer contexts, and
</bodyText>
<listItem confidence="0.956306">
• have a low computational complexity, as the
</listItem>
<page confidence="0.991521">
137
</page>
<table confidence="0.99966675">
test data precision recall Fo=1
ADJP 58.33% 52.74% 55.40
ADVP 67.98% 71.59% 69.74
CONJP 0.00% 0.00% 0.00
INTJ 33.33% 50.00% 40.00
LST 0.00% 0.00% 0.00
NP 88.09% 90.53% 89.30
PP 88.18% 93.39% 90.71
PRT 36.14% 28.30% 31.75
SBAR 54.97% 33.08% 41.31
VP 88.27% 91.28% 89.75
all 86.24% 88.25% 87.23
</table>
<tableCaption confidence="0.998621">
Table 1: Results using at most 5-contexts
</tableCaption>
<bodyText confidence="0.9994052">
process is always limited to use a low number of
hash table look-ups (determined by the largest
size of context). Training is limited to detecting
the most likely outcome of each context (i.e., a
sorting operation).
</bodyText>
<sectionHeader confidence="0.996698" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981872777777778">
Antal van den Bosch and Walter Daele mans. 1998.
Do not forget: Full memory in memory-based
learning of word pronunciation. In D.M.W. Pow-
ers, editor, proceedings of NeMLap3/CoNLL98,
pages 195-204, Sydney, Australia.
Eric Brill. 1994. Some advances in rule-based part
of speech tagging. In proceedings of AAAI.
Christer Johansson. 1999. Noise resistance in pro-
cessing center-embedded clauses: A question of
representation? In proceedings of ICCS&apos;99, pages
253-258, Tokyo. Waseda University.
Risto Miikkulainen and Michael G. Dyer. 1991.
Natural language processing with modular PDP
networks and distributed lexicon. Cognitive Sci-
ence, 15(3):343-399.
Thomas J. Palmeri. 1998. Formal models and fea-
ture creation. Behavioral and Brain Sciences,
21:33-34.
Royal Skousen. 1989. Analogical Modeling of Lan-
guage. Kluwer Academic, Dordrecht, the Nether-
lands.
Jakub Zavrel and Walter Daelemans. 1997. Memory
based learning: using similarity for smoothing.
In proceedings of the 35th annual meeting of the
Association of Computational Linguistics (ACL)
and the 8th conference of the European Chapter
of the ACL, pages 436-443, Madrid, Spain.
</reference>
<page confidence="0.997241">
138
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865705">
<note confidence="0.891305">of CoNLL-2000 and LLL-2000, 136-138, Lisbon, Portugal, 2000.</note>
<title confidence="0.993436">A Context Sensitive Maximum Likelihood Approach to Chunking</title>
<author confidence="0.993998">Christer Johansson</author>
<affiliation confidence="0.988133">Electrotechnical Laboratories Machine Understanding</affiliation>
<address confidence="0.992867">1-1-4 Umez ono, Tsukuba. 305 Ibaraki, JAPAN</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
<author>Walter Daele mans</author>
</authors>
<title>Do not forget: Full memory in memory-based learning of word pronunciation.</title>
<date>1998</date>
<booktitle>proceedings of NeMLap3/CoNLL98,</booktitle>
<pages>195--204</pages>
<editor>In D.M.W. Powers, editor,</editor>
<location>Sydney, Australia.</location>
<marker>van den Bosch, mans, 1998</marker>
<rawString>Antal van den Bosch and Walter Daele mans. 1998. Do not forget: Full memory in memory-based learning of word pronunciation. In D.M.W. Powers, editor, proceedings of NeMLap3/CoNLL98, pages 195-204, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advances in rule-based part of speech tagging.</title>
<date>1994</date>
<booktitle>In proceedings of AAAI.</booktitle>
<marker>Brill, 1994</marker>
<rawString>Eric Brill. 1994. Some advances in rule-based part of speech tagging. In proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Johansson</author>
</authors>
<title>Noise resistance in processing center-embedded clauses: A question of representation?</title>
<date>1999</date>
<booktitle>In proceedings of ICCS&apos;99,</booktitle>
<pages>253--258</pages>
<institution>Tokyo. Waseda University.</institution>
<contexts>
<context position="5996" citStr="Johansson, 1999" startWordPosition="1030" endWordPosition="1031">l be used). Good performance can be obtained within a maximum of 2 look-ups for each label (i.e., using only 1- and 3-contexts) and the best results were obtained with a maximum of 3 look-ups per label. 4 Discussion The memory-based approach seemingly postulates innate tags in the processing machinery. The author has found very little discussion on how the tags are thought to correspond to reality, a fact that was also pointed out, not so long ago, by Palmeri (1998). However, a few papers aiming towards automatic &apos;label&apos;, &apos;feature&apos; or &apos;tag&apos; creation are available (Miikkulainen and Dyer, 1991; Johansson, 1999). It is undeniable that, from a practical perspective, it is possible to reach very high performance on tasks, such as tagging, that demand a choice from a known set of alternatives by estimating statistical properties (e.g., the most likely label) from a large enough training set. This makes the method extremely useful for quick development of tools, which can be used in practical applications such as text retrieval and machine translation; but also in linguistic research; e.g., finding examples of specific grammatical constructions in large collections of data. A challenge for future researc</context>
</contexts>
<marker>Johansson, 1999</marker>
<rawString>Christer Johansson. 1999. Noise resistance in processing center-embedded clauses: A question of representation? In proceedings of ICCS&apos;99, pages 253-258, Tokyo. Waseda University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risto Miikkulainen</author>
<author>Michael G Dyer</author>
</authors>
<title>Natural language processing with modular PDP networks and distributed lexicon.</title>
<date>1991</date>
<journal>Cognitive Science,</journal>
<pages>15--3</pages>
<contexts>
<context position="5978" citStr="Miikkulainen and Dyer, 1991" startWordPosition="1026" endWordPosition="1029"> label of the current tag will be used). Good performance can be obtained within a maximum of 2 look-ups for each label (i.e., using only 1- and 3-contexts) and the best results were obtained with a maximum of 3 look-ups per label. 4 Discussion The memory-based approach seemingly postulates innate tags in the processing machinery. The author has found very little discussion on how the tags are thought to correspond to reality, a fact that was also pointed out, not so long ago, by Palmeri (1998). However, a few papers aiming towards automatic &apos;label&apos;, &apos;feature&apos; or &apos;tag&apos; creation are available (Miikkulainen and Dyer, 1991; Johansson, 1999). It is undeniable that, from a practical perspective, it is possible to reach very high performance on tasks, such as tagging, that demand a choice from a known set of alternatives by estimating statistical properties (e.g., the most likely label) from a large enough training set. This makes the method extremely useful for quick development of tools, which can be used in practical applications such as text retrieval and machine translation; but also in linguistic research; e.g., finding examples of specific grammatical constructions in large collections of data. A challenge </context>
</contexts>
<marker>Miikkulainen, Dyer, 1991</marker>
<rawString>Risto Miikkulainen and Michael G. Dyer. 1991. Natural language processing with modular PDP networks and distributed lexicon. Cognitive Science, 15(3):343-399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas J Palmeri</author>
</authors>
<title>Formal models and feature creation.</title>
<date>1998</date>
<booktitle>Behavioral and Brain Sciences,</booktitle>
<pages>21--33</pages>
<contexts>
<context position="5850" citStr="Palmeri (1998)" startWordPosition="1010" endWordPosition="1011">h-table lookups for each tag (i.e., when the 7-, 5-, and 3- contexts does not exist in the database the most likely label of the current tag will be used). Good performance can be obtained within a maximum of 2 look-ups for each label (i.e., using only 1- and 3-contexts) and the best results were obtained with a maximum of 3 look-ups per label. 4 Discussion The memory-based approach seemingly postulates innate tags in the processing machinery. The author has found very little discussion on how the tags are thought to correspond to reality, a fact that was also pointed out, not so long ago, by Palmeri (1998). However, a few papers aiming towards automatic &apos;label&apos;, &apos;feature&apos; or &apos;tag&apos; creation are available (Miikkulainen and Dyer, 1991; Johansson, 1999). It is undeniable that, from a practical perspective, it is possible to reach very high performance on tasks, such as tagging, that demand a choice from a known set of alternatives by estimating statistical properties (e.g., the most likely label) from a large enough training set. This makes the method extremely useful for quick development of tools, which can be used in practical applications such as text retrieval and machine translation; but also</context>
</contexts>
<marker>Palmeri, 1998</marker>
<rawString>Thomas J. Palmeri. 1998. Formal models and feature creation. Behavioral and Brain Sciences, 21:33-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Royal Skousen</author>
</authors>
<title>Analogical Modeling of Language.</title>
<date>1989</date>
<publisher>Kluwer Academic,</publisher>
<location>Dordrecht, the Netherlands.</location>
<contexts>
<context position="1295" citStr="Skousen, 1989" startWordPosition="208" endWordPosition="209">he performance of simply picking the most likely tag for a given context, under the condition that a larger context is allowed to override the most likely label of a smaller context. The results could be extended by secondary error correction as in Brill&apos;s tagger, but this exercise is left to the reader to allow us to concentrate on the performance based on storing and retrieving the most likely examples only. More sophisticated methods may use more than one stored context to determine the label that best fits the current context (Van den Bosch and Daelemans, 1998; Zavrel and Daelemans, 1997; Skousen, 1989, inter al.). The method of this paper uses only one context to determine the best label, but may decrease the size of the context until a full match is found. 2 Outline of the procedure 2.1 &amp;quot;Training&amp;quot; The training of this mechanism is to determine which patterns in the training set are the most likely. Only tag information is used. A filter to convert a tag with a context into a chunk-label is constructed as follows: 0) Construct symmetric n-contexts from the training corpus. A 1-context is simply the most common chunk-label for each tag. A 3-context is the tag followed by the tag before and </context>
</contexts>
<marker>Skousen, 1989</marker>
<rawString>Royal Skousen. 1989. Analogical Modeling of Language. Kluwer Academic, Dordrecht, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory based learning: using similarity for smoothing.</title>
<date>1997</date>
<booktitle>In proceedings of the 35th annual meeting of the Association of Computational Linguistics (ACL) and the 8th conference of the European Chapter of the ACL,</booktitle>
<pages>436--443</pages>
<location>Madrid,</location>
<contexts>
<context position="1280" citStr="Zavrel and Daelemans, 1997" startWordPosition="203" endWordPosition="207">his paper will investigate the performance of simply picking the most likely tag for a given context, under the condition that a larger context is allowed to override the most likely label of a smaller context. The results could be extended by secondary error correction as in Brill&apos;s tagger, but this exercise is left to the reader to allow us to concentrate on the performance based on storing and retrieving the most likely examples only. More sophisticated methods may use more than one stored context to determine the label that best fits the current context (Van den Bosch and Daelemans, 1998; Zavrel and Daelemans, 1997; Skousen, 1989, inter al.). The method of this paper uses only one context to determine the best label, but may decrease the size of the context until a full match is found. 2 Outline of the procedure 2.1 &amp;quot;Training&amp;quot; The training of this mechanism is to determine which patterns in the training set are the most likely. Only tag information is used. A filter to convert a tag with a context into a chunk-label is constructed as follows: 0) Construct symmetric n-contexts from the training corpus. A 1-context is simply the most common chunk-label for each tag. A 3-context is the tag followed by the </context>
</contexts>
<marker>Zavrel, Daelemans, 1997</marker>
<rawString>Jakub Zavrel and Walter Daelemans. 1997. Memory based learning: using similarity for smoothing. In proceedings of the 35th annual meeting of the Association of Computational Linguistics (ACL) and the 8th conference of the European Chapter of the ACL, pages 436-443, Madrid, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>