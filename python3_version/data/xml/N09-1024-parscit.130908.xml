<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002849">
<title confidence="0.990878">
Unsupervised Morphological Segmentation with Log-Linear Models
</title>
<author confidence="0.979006">
Hoifung Poon∗ Colin Cherry Kristina Toutanova
</author>
<affiliation confidence="0.9916135">
Dept. of Computer Sci. &amp; Eng. Microsoft Research Microsoft Research
University of Washington Redmond, WA 98052 Redmond, WA 98052
</affiliation>
<address confidence="0.895986">
Seattle, WA 98195 colinc@microsoft.com kristout@microsoft.com
</address>
<email confidence="0.979343">
hoifung@cs.washington.edu
</email>
<sectionHeader confidence="0.995412" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998027285714286">
Morphological segmentation breaks words
into morphemes (the basic semantic units). It
is a key component for natural language pro-
cessing systems. Unsupervised morphologi-
cal segmentation is attractive, because in ev-
ery language there are virtually unlimited sup-
plies of text, but very few labeled resources.
However, most existing model-based systems
for unsupervised morphological segmentation
use directed generative models, making it dif-
ficult to leverage arbitrary overlapping fea-
tures that are potentially helpful to learning.
In this paper, we present the first log-linear
model for unsupervised morphological seg-
mentation. Our model uses overlapping fea-
tures such as morphemes and their contexts,
and incorporates exponential priors inspired
by the minimum description length (MDL)
principle. We present efficient algorithms
for learning and inference by combining con-
trastive estimation with sampling. Our sys-
tem, based on monolingual features only, out-
performs a state-of-the-art system by a large
margin, even when the latter uses bilingual in-
formation such as phrasal alignment and pho-
netic correspondence. On the Arabic Penn
Treebank, our system reduces F1 error by 11%
compared to Morfessor.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992967487179488">
The goal of morphological segmentation is to seg-
ment words into morphemes, the basic syntac-
tic/semantic units. This is a key subtask in many
∗ This research was conducted during the author’s intern-
ship at Microsoft Research.
NLP applications, including machine translation,
speech recognition and question answering. Past
approaches include rule-based morphological an-
alyzers (Buckwalter, 2004) and supervised learn-
ing (Habash and Rambow, 2005). While successful,
these require deep language expertise and a long and
laborious process in system building or labeling.
Unsupervised approaches are attractive due to the
the availability of large quantities of unlabeled text,
and unsupervised morphological segmentation has
been extensively studied for a number of languages
(Brent et al., 1995; Goldsmith, 2001; Dasgupta and
Ng, 2007; Creutz and Lagus, 2007). The lack
of supervised labels makes it even more important
to leverage rich features and global dependencies.
However, existing systems use directed generative
models (Creutz and Lagus, 2007; Snyder and Barzi-
lay, 2008b), making it difficult to extend them with
arbitrary overlapping dependencies that are poten-
tially helpful to segmentation.
In this paper, we present the first log-linear model
for unsupervised morphological segmentation. Our
model incorporates simple priors inspired by the
minimum description length (MDL) principle, as
well as overlapping features such as morphemes and
their contexts (e.g., in Arabic, the string Al is likely
a morpheme, as is any string between Al and a word
boundary). We develop efficient learning and infer-
ence algorithms using a novel combination of two
ideas from previous work on unsupervised learn-
ing with log-linear models: contrastive estimation
(Smith and Eisner, 2005) and sampling (Poon and
Domingos, 2008).
We focus on inflectional morphology and test our
</bodyText>
<page confidence="0.984304">
209
</page>
<note confidence="0.890302">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209–217,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999684153846154">
approach on datasets in Arabic and Hebrew. Our
system, using monolingual features only, outper-
forms Snyder &amp; Barzilay (2008b) by a large mar-
gin, even when their system uses bilingual informa-
tion such as phrasal alignment and phonetic corre-
spondence. On the Arabic Penn Treebank, our sys-
tem reduces F1 error by 11% compared to Mor-
fessor Categories-MAP (Creutz and Lagus, 2007).
Our system can be readily applied to supervised
and semi-supervised learning. Using a fraction of
the labeled data, it already outperforms Snyder &amp;
Barzilay’s supervised results (2008a), which further
demonstrates the benefit of using a log-linear model.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999805461538462">
There is a large body of work on the unsupervised
learning of morphology. In addition to morpholog-
ical segmentation, there has been work on unsuper-
vised morpheme analysis, where one needs to deter-
mine features of word forms (Kurimo et al., 2007)
or identify words with the same lemma by model-
ing stem changes (Schone and Jurafsky, 2001; Gold-
smith, 2001). However, we focus our review specif-
ically on morphological segmentation.
In the absence of labels, unsupervised learning
must incorporate a strong learning bias that reflects
prior knowledge about the task. In morphological
segmentation, an often-used bias is the minimum
description length (MDL) principle, which favors
compact representations of the lexicon and corpus
(Brent et al., 1995; Goldsmith, 2001; Creutz and La-
gus, 2007). Other approaches use statistics on mor-
pheme context, such as conditional entropy between
adjacent n-grams, to identify morpheme candidates
(Harris, 1955; Keshava and Pitler, 2006). In this pa-
per, we incorporate both intuitions into a simple yet
powerful model, and show that each contributes sig-
nificantly to performance.
Unsupervised morphological segmentation sys-
tems also differ from the engineering perspective.
Some adopt a pipeline approach (Schone and Ju-
rafsky, 2001; Dasgupta and Ng, 2007; Demberg,
2007), which works by first extracting candidate
affixes and stems, and then segmenting the words
based on the candidates. Others model segmenta-
tion using a joint probabilistic distribution (Goldwa-
ter et al., 2006; Creutz and Lagus, 2007; Snyder and
Barzilay, 2008b); they learn the model parameters
from unlabeled data and produce the most proba-
ble segmentation as the final output. The latter ap-
proach is arguably more appealing from the mod-
eling standpoint and avoids error propagation along
the pipeline. However, most existing systems use
directed generative models; Creutz &amp; Lagus (2007)
used an HMM, while Goldwater et al. (2006) and
Snyder &amp; Barzilay (2008b) used Bayesian models
based on Pitman-Yor or Dirichlet processes. These
models are difficult to extend with arbitrary overlap-
ping features that can help improve accuracy.
In this work we incorporate novel overlapping
contextual features and show that they greatly im-
prove performance. Non-overlapping contextual
features previously have been used in directed gen-
erative models (in the form of Markov models) for
unsupervised morphological segmentation (Creutz
and Lagus, 2007) or word segmentation (Goldwater
et al., 2007). In terms of feature sets, our model is
most closely related to the constituent-context model
proposed by Klein and Manning (2001) for grammar
induction. If we exclude the priors, our model can
also be seen as a semi-Markov conditional random
field (CRF) model (Sarawagi and Cohen, 2004).
Semi-Markov CRFs previously have been used for
supervised word segmentation (Andrew, 2006), but
not for unsupervised morphological segmentation.
Unsupervised learning with log-linear models has
received little attention in the past. Two notable ex-
ceptions are Smith &amp; Eisner (2005) for POS tagging,
and Poon &amp; Domingos (2008) for coreference res-
olution. Learning with log-linear models requires
computing the normalization constant (a.k.a. the
partition function) Z. This is already challenging in
supervised learning. In unsupervised learning, the
difficulty is further compounded by the absence of
supervised labels. Smith &amp; Eisner (2005) proposed
contrastive estimation, which uses a small neighbor-
hood to compute Z. The neighborhood is carefully
designed so that it not only makes computation eas-
ier but also offers sufficient contrastive information
to aid unsupervised learning. Poon &amp; Domingos
(2008), on the other hand, used sampling to approx-
imate Z.1 In this work, we benefit from both tech-
niques: contrastive estimation creates a manageable,
</bodyText>
<footnote confidence="0.986406">
1Rosenfeld (1997) also did this for language modeling.
</footnote>
<page confidence="0.996704">
210
</page>
<figure confidence="0.8875505">
wvlAvwn
(##__##)
w vlAv wn
(##__vl) (#w__wn) (Av__##)
</figure>
<figureCaption confidence="0.9477535">
Figure 1: The morpheme and context (in parentheses)
features for the segmented word w-vlAv-wn.
</figureCaption>
<bodyText confidence="0.8927835">
informative Z, while sampling enables the use of
powerful global features.
</bodyText>
<sectionHeader confidence="0.996299" genericHeader="method">
3 Log-Linear Model for Unsupervised
</sectionHeader>
<subsectionHeader confidence="0.667286">
Morphological Segmentation
</subsectionHeader>
<bodyText confidence="0.993739866666667">
Central to our approach is a log-linear model that
defines the joint probability distribution for a cor-
pus (i.e., the words) and a segmentation on the cor-
pus. The core of this model is a morpheme-context
model, with one feature for each morpheme,2 and
one feature for each morpheme context. We rep-
resent contexts using the n-grams before and after
the morpheme, for some constant n. To illustrate
this, a segmented Arabic corpus is shown below
along with its features, assuming we are tracking bi-
gram contexts. The segmentation is indicated with
hyphens, while the hash symbol (#) represents the
word boundary.
Segmented Corpus hnAk w-vlAv-wn bn-w
Al-ywm Al-jmAEp
</bodyText>
<equation confidence="0.572210142857143">
Morpheme Feature:Value hnAk:1 w:2 vlAv:1
wn:1 bn:1 Al:2 ywm:1 jmAEp:1
hnAk:1 wvlAvwn:1 bnw:1 Alywm:1 Alj-
mAEp:1
Bigram Context Feature:Value ## vl:1
#w wn:1 Av ##:1 ## w#:1 bn ##:1
## yw:1 Al ##:2 ## jm:1 ## ##:5
</equation>
<bodyText confidence="0.9995871">
Furthermore, the corresponding features for the seg-
mented word w-vlAv-wn are shown in Figure 1.
Each feature is associated with a weight, which
correlates with the likelihood that the correspond-
ing morpheme or context marks a valid morpholog-
ical segment. Such overlapping features allow us to
capture rich segmentation regularities. For example,
given the Arabic word Alywm, to derive its correct
segmentation Al-ywm, it helps to know that Al and
ywm are likely morphemes whereas Aly or lyw are
</bodyText>
<footnote confidence="0.590261">
2The word as a whole is also treated as a morpheme in itself.
</footnote>
<bodyText confidence="0.992436166666667">
not; it also helps to know that Al ## or ## yw are
likely morpheme contexts whereas ly ## or ## wm
are not. Ablation tests verify the importance of these
overlapping features (see Section 7.2).
Our morpheme-context model is inspired by
the constituent-context model (CCM) proposed by
Klein and Manning (2001) for grammar induction.
The morphological segmentation of a word can be
viewed as a flat tree, where the root node corre-
sponds to the word and the leaves correspond to
morphemes (see Figure 1). The CCM uses uni-
grams for context features. For this task, however,
we found that bigrams and trigrams lead to much
better accuracy. We use trigrams in our full model.
For learning, one can either view the corpus as
a collection of word types (unique words) or tokens
(word occurrences). Some systems (e.g., Morfessor)
use token frequency for parameter estimation. Our
system, however, performs much better using word
types. This has also been observed for other mor-
phological learners (Goldwater et al., 2006). Thus
we use types in learning and inference, and effec-
tively enforce the constraint that words can have
only one segmentation per type. Evaluation is still
based on tokens to reflect the performance in real
applications.
In addition to the features of the morpheme-
context model, we incorporate two priors which cap-
ture additional intuitions about morphological seg-
mentations. First, we observe that the number of
distinct morphemes used to segment a corpus should
be small. This is achieved when the same mor-
phemes are re-used across many different words.
Our model incorporates this intuition by imposing
a lexicon prior: an exponential prior with nega-
tive weight on the length of the morpheme lexi-
con. We define the lexicon to be the set of unique
morphemes identified by a complete segmentation
of the corpus, and the lexicon length to be the to-
tal number of characters in the lexicon. In this
way, we can simultaneously emphasize that a lexi-
con should contain few unique morphemes, and that
those morphemes should be short. However, the lex-
icon prior alone incorrectly favors the trivial seg-
mentation that shatters each word into characters,
which results in the smallest lexicon possible (sin-
gle characters). Therefore, we also impose a corpus
prior: an exponential prior on the number of mor-
</bodyText>
<page confidence="0.990424">
211
</page>
<bodyText confidence="0.999468681818182">
phemes used to segment each word in the corpus,
which penalizes over-segmentation. We notice that
longer words tend to have more morphemes. There-
fore, each word’s contribution to this prior is nor-
malized by the word’s length in characters (e.g., the
segmented word w-vLAv-wn contributes 3/7 to the to-
tal corpus size). Notice that it is straightforward to
incorporate such a prior in a log-linear model, but
much more challenging to do so in a directed gen-
erative model. These two priors are inspired by the
minimum description length (MDL) length princi-
ple; the lexicon prior favors fewer morpheme types,
whereas the corpus prior favors fewer morpheme to-
kens. They are vital to the success of our model,
providing it with the initial inductive bias.
We also notice that often a word is decomposed
into a stem and some prefixes and suffixes. This is
particularly true for languages with predominantly
inflectional morphology, such as Arabic, Hebrew,
and English. Thus our model uses separate lexicons
for prefixes, stems, and suffixes. This results in a
small but non-negligible accuracy gain in our exper-
iments. We require that a stem contain at least two
characters and no fewer characters than any affixes
in the same word.3 In a given word, when a mor-
pheme is identified as the stem, any preceding mor-
pheme is identified as a prefix, whereas any follow-
ing morpheme as a suffix. The sample segmented
corpus mentioned earlier induces the following lex-
icons:
Prefix w Al
Stem hnAk vlAv bn ywm jmAEp
Suffix wn w
Before presenting our formal model, we first in-
troduce some notation. Let W be a corpus (i.e., a set
of words), and S be a segmentation that breaks each
word in W into prefixes, a stem, and suffixes. Let a
be a string (character sequence). Each occurrence of
a will be in the form of 01a02, where 01, 02 are the
adjacent character n-grams, and c = (01, 02) is the
context of a in this occurrence. Thus a segmentation
can be viewed as a set of morpheme strings and their
contexts. For a string x, L(x) denotes the number of
characters in x; for a word w, MS(w) denotes the
</bodyText>
<footnote confidence="0.979804666666667">
3In a segmentation where several morphemes have the max-
imum length, any of them can be identified as the stem, each
resulting in a distinct segmentation.
</footnote>
<bodyText confidence="0.992222">
number of morphemes in w given the segmentation
S; Pref(W, S), Stem(W, S), Suff(W, S) denote
the lexicons of prefixes, stems, and suffixes induced
by S for W. Then, our model defines a joint proba-
bility distribution over a restricted set of W and S:
</bodyText>
<equation confidence="0.9931035">
1
Pθ(W, S) = Z · uθ(W, S)
</equation>
<bodyText confidence="0.700404">
where
</bodyText>
<equation confidence="0.9992495">
uθ(W, S) = exp( X Aσfσ(S) + X Acfc(S)
σ c
X+ α · L(a)
σEPref(W,S)
+ α · X L(a)
σEStem(W,S)
X+ α · L(a)
σESuff(W,S)
+ Q · X MS(w)/L(w) )
wEW
</equation>
<bodyText confidence="0.9998315">
Here, fσ(S) and fc(S) are respectively the occur-
rence counts of morphemes and contexts under S,
and θ = (Aσ, Ac : a, c) are their feature weights.
α, Q are the weights for the priors. Z is the nor-
malization constant, which sums over a set of cor-
pora and segmentations. In the next section, we will
define this set for our model and show how to effi-
ciently perform learning and inference.
</bodyText>
<sectionHeader confidence="0.995417" genericHeader="method">
4 Unsupervised Learning
</sectionHeader>
<bodyText confidence="0.9995432">
As mentioned in Smith &amp; Eisner (2005), learning
with probabilistic models can be viewed as moving
probability mass to the observed data. The question
is from where to take this mass. For log-linear mod-
els, the answer amounts to defining the set that Z
sums over. We use contrastive estimation and define
the set to be a neighborhood of the observed data.
The instances in the neighborhood can be viewed
as pseudo-negative examples, and learning seeks to
discriminate them from the observed instances.
Formally, let W* be the observed corpus, and let
N(·) be a function that maps a string to a set of
strings; let N(W *) denote the set of all corpora that
can be derived from W* by replacing every word
w ∈ W* with one in N(w). Then,
</bodyText>
<equation confidence="0.990508">
Z = X X u(W, S).
WEN(W∗) S
</equation>
<page confidence="0.985778">
212
</page>
<bodyText confidence="0.752266">
ing now maximizes Lθ(W∗, S∗); the partial deriva-
tives become
Unsupervised learning maximizes the log-likelihood
of observing W∗
</bodyText>
<equation confidence="0.765165">
�
Lθ(W ∗) = log
S
</equation>
<bodyText confidence="0.9978445">
We use gradient descent for this optimization; the
partial derivatives for feature weights are
</bodyText>
<equation confidence="0.8317145">
∂ Lθ(W∗) = ES|W*[fi] − ES,W [fi]
∂λi
</equation>
<bodyText confidence="0.999972586206897">
where i is either a string σ or a context c. The first
expected count ranges over all possible segmenta-
tions while the words are fixed to those observed in
W∗. For the second expected count, the words also
range over the neighborhood.
Smith &amp; Eisner (2005) considered various neigh-
borhoods for unsupervised POS tagging, and
showed that the best neighborhoods are TRANS1
(transposing any pair of adjacent words) and
DELORTRANS1 (deleting any word or transposing
any pair of adjacent words). We can obtain their
counterparts for morphological segmentation by
simply replacing “words” with “characters”. As
mentioned earlier, the instances in the neighbor-
hood serve as pseudo-negative examples from which
probability mass can be taken away. In this regard,
DELORTRANS1 is suitable for POS tagging since
deleting a word often results in an ungrammatical
sentence. However, in morphology, a word less a
character is often a legitimate word too. For exam-
ple, deleting l from the Hebrew word lyhwh (to the
lord) results in yhwh (the lord). Thus DELORTRANS1
forces legal words to compete against each other for
probability mass, which seems like a misguided ob-
jective. Therefore, in our model we use TRANS1. It
is suited for our task because transposing a pair of
adjacent characters usually results in a non-word.
To combat overfitting in learning, we impose a
Gaussian prior (L2 regularization) on all weights.
</bodyText>
<sectionHeader confidence="0.994547" genericHeader="method">
5 Supervised Learning
</sectionHeader>
<bodyText confidence="0.977042357142857">
Our learning algorithm can be readily applied to su-
pervised or semi-supervised learning. Suppose that
gold segmentation is available for some words, de-
noted as S∗. If S∗ contains gold segmentations
for all words in W, we are doing supervised learn-
ing; otherwise, learning is semi-supervised. Train-
∂ Lθ(W ∗, S∗) = ES|W*,S*[fi] − ES,W [fi]
∂λi
The only difference in comparison with unsuper-
vised learning is that we fix the known segmenta-
tion when computing the first expected counts. In
Section 7.3, we show that when labels are available,
our model also learns much more effectively than a
directed graphical model.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="method">
6 Inference
</sectionHeader>
<bodyText confidence="0.999176476190476">
In Smith &amp; Eisner (2005), the objects (sentences) are
independent from each other, and exact inference is
tractable. In our model, however, the lexicon prior
renders all objects (words) interdependent in terms
of segmentation decisions. Consider the simple cor-
pus with just two words: Alrb, lAlrb. If lAlrb is seg-
mented into l-Al-rb, Alrb can be segmented into Al-
rb without paying the penalty imposed by the lexi-
con prior. If, however, lAlrb remains a single mor-
pheme, and we still segment Alrb into Al-rb, then
we introduce two new morphemes into the lexicons,
and we will be penalized by the lexicon prior ac-
cordingly. As a result, we must segment the whole
corpus jointly, making exact inference intractable.
Therefore, we resort to approximate inference. To
compute ES|W*[fi], we use Gibbs sampling. To de-
rive a sample, the procedure goes through each word
and samples the next segmentation conditioned on
the segmentation of all other words. With m sam-
ples S1, · · · , Sm, the expected count can be approx-
imated as
</bodyText>
<equation confidence="0.974723666666667">
1 �
ES|W *[fi] ^ m
j
</equation>
<bodyText confidence="0.9341614">
There are 2n−1 ways to segment a word of n char-
acters. To sample a new segmentation for a partic-
ular word, we need to compute conditional proba-
bility for each of these segmentations. We currently
do this by explicit enumeration.4 When n is large,
4These segmentations could be enumerated implicitly us-
ing the dynamic programming framework employed by semi-
Markov CRFs (Sarawagi and Cohen, 2004). However, in such a
setting, our lexicon prior would likely need to be approximated.
We intend to investigate this in future work.
</bodyText>
<equation confidence="0.9786475">
P(W∗, S)
fi(Sj)
</equation>
<page confidence="0.997375">
213
</page>
<bodyText confidence="0.999974730769231">
this is very expensive. However, we observe that
the maximum number of morphemes that a word
contains is usually a small constant for many lan-
guages; in the Arabic Penn Treebank, the longest
word contains 14 characters, but the maximum num-
ber of morphemes in a word is only 5. Therefore,
we impose the constraint that a word can be seg-
mented into no more than k morphemes, where k
is a language-specific constant. We can determine
k from prior knowledge or use a development set.
This constraint substantially reduces the number of
segmentation candidates to consider; with k = 5, it
reduces the number of segmentations to consider by
almost 90% for a word of 14 characters.
ES,W [fi] can be computed by Gibbs sampling in
the same way, except that in each step we also sam-
ple the next word from the neighborhood, in addition
to the next segmentation.
To compute the most probable segmentation, we
use deterministic annealing. It works just like a sam-
pling algorithm except that the weights are divided
by a temperature, which starts with a large value and
gradually drops to a value close to zero. To make
burn-in faster, when computing the expected counts,
we initialize the sampler with the most probable seg-
mentation output by annealing.
</bodyText>
<sectionHeader confidence="0.999216" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999972575">
We evaluated our system on two datasets. Our main
evaluation is on a multi-lingual dataset constructed
by Snyder &amp; Barzilay (2008a; 2008b). It consists of
6192 short parallel phrases in Hebrew, Arabic, Ara-
maic (a dialect of Arabic), and English. The paral-
lel phrases were extracted from the Hebrew Bible
and its translations via word alignment and post-
processing. For Arabic, the gold segmentation was
obtained using a highly accurate Arabic morpholog-
ical analyzer (Habash and Rambow, 2005); for He-
brew, from a Bible edition distributed by Westmin-
ster Hebrew Institute (Groves and Lowery, 2006).
There is no gold segmentation for English and Ara-
maic. Like Snyder &amp; Barzilay, we evaluate on the
Arabic and Hebrew portions only; unlike their ap-
proach, our system does not use any bilingual in-
formation. We refer to this dataset as S&amp;B . We
also report our results on the Arabic Penn Treebank
(ATB), which provides gold segmentations for an
Arabic corpus with about 120,000 Arabic words.
As in previous work, we report recall, precision,
and F1 over segmentation points. We used 500
phrases from the S&amp;B dataset for feature develop-
ment, and also tuned our model hyperparameters
there. The weights for the lexicon and corpus pri-
ors were set to α = −1, Q = −20. The feature
weights were initialized to zero and were penalized
by a Gaussian prior with u2 = 100. The learning
rate was set to 0.02 for all experiments, except the
full Arabic Penn Treebank, for which it was set to
0.005.5 We used 30 iterations for learning. In each
iteration, 200 samples were collected to compute
each of the two expected counts. The sampler was
initialized by running annealing for 2000 samples,
with the temperature dropping from 10 to 0.1 at 0.1
decrements. The most probable segmentation was
obtained by running annealing for 10000 samples,
using the same temperature schedule. We restricted
the segmentation candidates to those with no greater
than five segments in all experiments.
</bodyText>
<subsectionHeader confidence="0.995008">
7.1 Unsupervised Segmentation on S&amp;B
</subsectionHeader>
<bodyText confidence="0.999407631578947">
We followed the experimental set-up of Snyder &amp;
Barzilay (2008b) to enable a direct comparison. The
dataset is split into a training set with 4/5 of the
phrases, and a test set with the remaining 1/5. First,
we carried out unsupervised learning on the training
data, and computed the most probable segmentation
for it. Then we fixed the learned weights and the seg-
mentation for training, and computed the most prob-
able segmentation for the test set, on which we eval-
uated.6 Snyder &amp; Barzilay (2008b) compared sev-
eral versions of their systems, differing in how much
bilingual information was used. Using monolingual
information only, their system (S&amp;B-MONO) trails
the state-of-the-art system Morfessor; however, their
best system (S&amp;B-BEST), which uses bilingual in-
formation that includes phrasal alignment and pho-
netic correspondence between Arabic and Hebrew,
outperforms Morfessor and achieves the state-of-
the-art results on this dataset.
</bodyText>
<footnote confidence="0.998737833333333">
5The ATB set is more than an order of magnitude larger and
requires a smaller rate.
6With unsupervised learning, we can use the entire dataset
for training since no labels are provided. However, this set-
up is necessary for S&amp;B’s system because they used bilingual
information in training, which is not available at test time.
</footnote>
<page confidence="0.991223">
214
</page>
<table confidence="0.999827625">
ARABIC Prec. Rec. F1
S&amp;B-MONO 53.0 78.5 63.2
S&amp;B-BEST 67.8 77.3 72.2
FULL 76.0 80.2 78.1
HEBREW Prec. Rec. F1
S&amp;B-MONO 55.8 64.4 59.8
S&amp;B-BEST 64.9 62.9 63.9
FULL 67.6 66.1 66.9
</table>
<tableCaption confidence="0.969491">
Table 1: Comparison of segmentation results on the S&amp;B
dataset.
</tableCaption>
<bodyText confidence="0.93790675">
Table 1 compares our system with theirs. Our sys-
tem outperforms both S&amp;B-MONO and S&amp;B-BEST
by a large margin. For example, on Arabic, our sys-
tem reduces F1 error by 21% compared to S&amp;B-
BEST, and by 40% compared to S&amp;B-MONO. This
suggests that the use of monolingual morpheme con-
text, enabled by our log-linear model, is more help-
ful than their bilingual cues.
</bodyText>
<subsectionHeader confidence="0.999324">
7.2 Ablation Tests
</subsectionHeader>
<bodyText confidence="0.99949804">
To evaluate the contributions of the major compo-
nents in our model, we conducted seven ablation
tests on the S&amp;B dataset, each using a model that
differed from our full model in one aspect. The first
three tests evaluate the effect of priors, whereas the
next three test the effect of context features. The
last evaluates the impact of using separate lexicons
for affixes and stems.
NO-PRIOR The priors are not used.
NO-COR-PR The corpus prior is not used.
NO-LEX-PR The lexicon prior is not used.
NO-CONTEXT Context features are not used.
UNIGRAM Unigrams are used in context.
BIGRAM Bigrams are used in context.
SG-LEXICON A single lexicon is used, rather than
three distinct ones for the affixes and stems.
Table 2 presents the ablation results in compari-
son with the results of the full model. When some or
all priors are excluded, the F1 score drops substan-
tially (over 10 points in all cases, and over 40 points
in some). In particular, excluding the corpus prior,
as in NO-PRIOR and NO-COR-PR, results in over-
segmentation, as is evident from the high recalls and
low precisions. When the corpus prior is enacted
but not the lexicon priors (NO-LEX-PR), precision
</bodyText>
<table confidence="0.999962388888889">
ARABIC Prec. Rec. F1
FULL 76.0 80.2 78.1
NO-PRIOR 24.6 89.3 38.6
NO-COR-PR 23.7 87.4 37.2
NO-LEX-PR 79.1 51.3 62.3
NO-CONTEXT 71.2 62.1 66.3
UNIGRAM 71.3 76.5 73.8
BIGRAM 73.1 78.4 75.7
SG-LEXICON 72.8 82.0 77.1
HEBREW Prec. Rec. F1
FULL 67.6 66.1 66.9
NO-PRIOR 34.0 89.9 49.4
NO-COR-PR 35.6 90.6 51.1
NO-LEX-PR 65.9 49.2 56.4
NO-CONTEXT 63.0 47.6 54.3
UNIGRAM 63.0 63.7 63.3
BIGRAM 69.5 66.1 67.8
SG-LEXICON 67.4 65.7 66.6
</table>
<tableCaption confidence="0.999966">
Table 2: Ablation test results on the S&amp;B dataset.
</tableCaption>
<bodyText confidence="0.99997768">
is much higher, but recall is low; the system now errs
on under-segmentation because recurring strings are
often not identified as morphemes.
A large accuracy drop (over 10 points in F1
score) also occurs when the context features are
excluded (NO-CONTEXT), which underscores the
importance of these overlapping features. We also
notice that the NO-CONTEXT model is compara-
ble to the S&amp;B-MONO model; they use the same
feature types, but different priors. The accuracies of
the two systems are comparable, which suggests that
we did not sacrifice accuracy by trading the more
complex and restrictive Dirichlet process prior for
exponential priors. A priori, it is unclear whether us-
ing contexts larger than unigrams would help. While
potentially beneficial, they also risk aggravating the
data sparsity and making our model more prone to
overfitting. For this problem, however, enlarging the
context (using higher n-grams up to trigrams) helps
substantially. For Arabic, the highest accuracy is at-
tained by using trigrams, which reduces F1 error by
16% compared to unigrams; for Hebrew, by using
bigrams, which reduces F1 error by 17%. Finally, it
helps to use separate lexicons for affixes and stems,
although the difference is small.
</bodyText>
<page confidence="0.995598">
215
</page>
<table confidence="0.999903">
ARABIC %Lbl. Prec. Rec. F1
S&amp;B-MONO-S 100 73.2 92.4 81.7
S&amp;B-BEST-S 200 77.8 92.3 84.4
FULL-S 25 84.9 85.5 85.2
50 88.2 86.8 87.5
75 89.6 86.4 87.9
100 91.7 88.5 90.0
HEBREW %Lbl. Prec. Rec. F1
S&amp;B-MONO-S 100 71.4 79.1 75.1
S&amp;B-BEST-S 200 76.8 79.2 78.0
FULL-S 25 78.7 73.3 75.9
50 82.8 74.6 78.4
75 83.1 77.3 80.1
100 83.0 78.9 80.9
</table>
<tableCaption confidence="0.998523">
Table 3: Comparison of segmentation results with super-
vised and semi-supervised learning on the S&amp;B dataset.
</tableCaption>
<subsectionHeader confidence="0.941557">
7.3 Supervised and Semi-Supervised Learning
</subsectionHeader>
<bodyText confidence="0.999996058823529">
To evaluate our system in the supervised and semi-
supervised learning settings, we report the perfor-
mance when various amounts of labeled data are
made available during learning, and compare them
to the results of Snyder &amp; Barzilay (2008a). They
reported results for supervised learning using mono-
lingual features only (S&amp;B-MONO-S), and for su-
pervised bilingual learning with labels for both lan-
guages (S&amp;B-BEST-S). On both languages, our sys-
tem substantially outperforms both S&amp;B-MONO-S
and S&amp;B-BEST-S. E.g., on Arabic, our system re-
duces F1 errors by 46% compared to S&amp;B-MONO-
S, and by 36% compared to S&amp;B-BEST-S. More-
over, with only one-fourth of the labeled data, our
system already outperforms S&amp;B-MONO-S. This
demonstrates that our log-linear model is better
suited to take advantage of supervised labels.
</bodyText>
<subsectionHeader confidence="0.969269">
7.4 Arabic Penn Treebank
</subsectionHeader>
<bodyText confidence="0.999631875">
We also evaluated our system on the Arabic Penn
Treebank (ATB). As is common in unsupervised
learning, we trained and evaluated on the entire set.
We compare our system with Morfessor (Creutz and
Lagus, 2007).7 In addition, we compare with Mor-
fessor Categories-MAP, which builds on Morfessor
and conducts an additional greedy search specifi-
cally tailored to segmentation. We found that it per-
</bodyText>
<footnote confidence="0.831036">
7We cannot compare with Snyder &amp; Barzilay’s system as its
strongest results require bilingual data, which is not available.
</footnote>
<table confidence="0.99906">
ATB-7000 Prec. Rec. F1
MORFESSOR-1.0 70.6 34.3 46.1
MORFESSOR-MAP 86.9 46.4 60.5
FULL 83.4 77.3 80.2
ATB Prec. Rec. F1
MORFESSOR-1.0 80.7 20.4 32.6
MORFESSOR-MAP 77.4 72.6 74.9
FULL 88.5 69.2 77.7
</table>
<tableCaption confidence="0.9722285">
Table 4: Comparison of segmentation results on the Ara-
bic Penn Treebank.
</tableCaption>
<bodyText confidence="0.999935285714286">
forms much better than Morfessor on Arabic but
worse on Hebrew. To test each system in a low-
data setting, we also ran experiments on the set con-
taining the first 7,000 words in ATB with at least
two characters (ATB-7000). Table 4 shows the re-
sults. Morfessor performs rather poorly on ATB-
7000. Morfessor Categories-MAP does much bet-
ter, but its performance is dwarfed by our system,
which further cuts F1 error by half. On the full ATB
dataset, Morfessor performs even worse, whereas
Morfessor Categories-MAP benefits from the larger
dataset and achieves an F1 of 74.9. Still, our system
substantially outperforms it, further reducing F1 er-
ror by 11%.8
</bodyText>
<sectionHeader confidence="0.997373" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999438">
This paper introduces the first log-linear model for
unsupervised morphological segmentation. It lever-
ages overlapping features such as morphemes and
their contexts, and enables easy extension to incor-
porate additional features and linguistic knowledge.
For Arabic and Hebrew, it outperforms the state-
of-the-art systems by a large margin. It can also
be readily applied to supervised or semi-supervised
learning when labeled data is available. Future di-
rections include applying our model to other in-
flectional and agglutinative languages, modeling in-
ternal variations of morphemes, leveraging parallel
data in multiple languages, and combining morpho-
logical segmentation with other NLP tasks, such as
machine translation.
</bodyText>
<footnote confidence="0.9817185">
8Note that the ATB and ATB-7000 experiments each mea-
sure accuracy on their entire training set. This difference in
testing conditions explains why some full ATB results are lower
than ATB-7000.
</footnote>
<page confidence="0.998811">
216
</page>
<sectionHeader confidence="0.995878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999756493975903">
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Michael R. Brent, Sreerama K. Murthy, and Andrew
Lundberg. 1995. Discovering morphemic suffixes: A
case study in minimum description length induction.
In Proceedings of the 15th Annual Conference of the
Cognitive Science Society.
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer version 2.0.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In Proceedings of Human Language
Technology (NAACL).
Vera Demberg. 2007. A language-independent unsuper-
vised model for morphological segmentation. In Pro-
ceedings ofthe 45th Annual Meeting ofthe Association
for Computational Linguistics, Prague, Czech Repub-
lic.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–198.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. In Advances in Neu-
ral Information Processing Systems 18.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word segmenta-
tion: Context is important. In Proceedings of the 31st
Boston University Conference on Language Develop-
ment.
Alan Groves and Kirk Lowery, editors. 2006. The West-
minster Hebrew Bible Morphology Database. West-
minster Hebrew Institute, Philadelphia, PA, USA.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190–222.
Samarth Keshava and Emily Pitler. 2006. A simple, intu-
itive approach to morpheme induction. In Proceedings
of 2nd Pascal Challenges Workshop, Venice, Italy.
Dan Klein and Christopher D. Manning. 2001. Natu-
ral language grammar induction using a constituent-
context model. In Advances in Neural Information
Processing Systems 14.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of Morpho Challenge in CLEF 2007.
In Working Notes of the CLEF 2007 Workshop.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 649–
658, Honolulu, HI. ACL.
Ronald Rosenfeld. 1997. A whole sentence maximum
entropy language model. In IEEE workshop on Auto-
matic Speech Recognition and Understanding.
Sunita Sarawagi and William Cohen. 2004. Semimarkov
conditional random fields for information extraction.
In Proceedings of the Twenty First International Con-
ference on Machine Learning.
Patrick Schone and Daniel Jurafsky. 2001. Knowlege-
free induction of inflectional morphologies. In Pro-
ceedings ofHuman Language Technology (NAACL).
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
Proceedings of the Twenty Third National Conference
on Artificial Intelligence.
Benjamin Snyder and Regina Barzilay. 2008b. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics.
</reference>
<page confidence="0.998397">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.297556">
<title confidence="0.828219">Unsupervised Morphological Segmentation with Log-Linear Models</title>
<affiliation confidence="0.942542">Dept. of Computer Sci. &amp; Colin Kristina University of Microsoft Microsoft</affiliation>
<address confidence="0.999927">Seattle, WA 98195 Redmond, WA 98052 Redmond, WA 98052</address>
<email confidence="0.996968">hoifung@cs.washington.educolinc@microsoft.comkristout@microsoft.com</email>
<abstract confidence="0.980793148148148">Morphological segmentation breaks words into morphemes (the basic semantic units). It is a key component for natural language processing systems. Unsupervised morphological segmentation is attractive, because in every language there are virtually unlimited supplies of text, but very few labeled resources. However, most existing model-based systems for unsupervised morphological segmentation use directed generative models, making it difficult to leverage arbitrary overlapping features that are potentially helpful to learning. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model uses overlapping features such as morphemes and their contexts, and incorporates exponential priors inspired by the minimum description length (MDL) principle. We present efficient algorithms for learning and inference by combining contrastive estimation with sampling. Our system, based on monolingual features only, outperforms a state-of-the-art system by a large margin, even when the latter uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn</abstract>
<note confidence="0.9151015">Treebank, our system reduces F1 error by 11% compared to Morfessor.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
</authors>
<title>A hybrid markov/semi-markov conditional random field for sequence segmentation.</title>
<date>2006</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="7128" citStr="Andrew, 2006" startWordPosition="1055" endWordPosition="1056">overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for unsupervised morphological segmentation (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith &amp; Eisner (2005) for POS tagging, and Poon &amp; Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith &amp; Eisner (2005) proposed contrastive estimation, which uses a sma</context>
</contexts>
<marker>Andrew, 2006</marker>
<rawString>Galen Andrew. 2006. A hybrid markov/semi-markov conditional random field for sequence segmentation. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Sreerama K Murthy</author>
<author>Andrew Lundberg</author>
</authors>
<title>Discovering morphemic suffixes: A case study in minimum description length induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the 15th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2345" citStr="Brent et al., 1995" startWordPosition="325" endWordPosition="328">ring the author’s internship at Microsoft Research. NLP applications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) princip</context>
<context position="5009" citStr="Brent et al., 1995" startWordPosition="733" endWordPosition="736">been work on unsupervised morpheme analysis, where one needs to determine features of word forms (Kurimo et al., 2007) or identify words with the same lemma by modeling stem changes (Schone and Jurafsky, 2001; Goldsmith, 2001). However, we focus our review specifically on morphological segmentation. In the absence of labels, unsupervised learning must incorporate a strong learning bias that reflects prior knowledge about the task. In morphological segmentation, an often-used bias is the minimum description length (MDL) principle, which favors compact representations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate </context>
</contexts>
<marker>Brent, Murthy, Lundberg, 1995</marker>
<rawString>Michael R. Brent, Sreerama K. Murthy, and Andrew Lundberg. 1995. Discovering morphemic suffixes: A case study in minimum description length induction. In Proceedings of the 15th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Buckwalter</author>
</authors>
<title>Buckwalter Arabic morphological analyzer version 2.0.</title>
<date>2004</date>
<contexts>
<context position="1948" citStr="Buckwalter, 2004" startWordPosition="269" endWordPosition="270">y a large margin, even when the latter uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor. 1 Introduction The goal of morphological segmentation is to segment words into morphemes, the basic syntactic/semantic units. This is a key subtask in many ∗ This research was conducted during the author’s internship at Microsoft Research. NLP applications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems </context>
</contexts>
<marker>Buckwalter, 2004</marker>
<rawString>Tim Buckwalter. 2004. Buckwalter Arabic morphological analyzer version 2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="2410" citStr="Creutz and Lagus, 2007" startWordPosition="335" endWordPosition="338">ications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and their c</context>
<context position="3992" citStr="Creutz and Lagus, 2007" startWordPosition="576" endWordPosition="579">). We focus on inflectional morphology and test our 209 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209–217, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics approach on datasets in Arabic and Hebrew. Our system, using monolingual features only, outperforms Snyder &amp; Barzilay (2008b) by a large margin, even when their system uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007). Our system can be readily applied to supervised and semi-supervised learning. Using a fraction of the labeled data, it already outperforms Snyder &amp; Barzilay’s supervised results (2008a), which further demonstrates the benefit of using a log-linear model. 2 Related Work There is a large body of work on the unsupervised learning of morphology. In addition to morphological segmentation, there has been work on unsupervised morpheme analysis, where one needs to determine features of word forms (Kurimo et al., 2007) or identify words with the same lemma by modeling stem changes (Schone and Jurafsk</context>
<context position="5797" citStr="Creutz and Lagus, 2007" startWordPosition="851" endWordPosition="854">pheme candidates (Harris, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz &amp; Lagus (2007) used an HMM, while Goldwater et al. (2006) and Snyder &amp; Barzilay (2008b) used Bayesian models based on Pitman-Yor or Dirichlet processes. These models are difficult to extend with arbitrary overlapping features that can help improve accuracy. </context>
<context position="29502" citStr="Creutz and Lagus, 2007" startWordPosition="4807" endWordPosition="4810">th languages, our system substantially outperforms both S&amp;B-MONO-S and S&amp;B-BEST-S. E.g., on Arabic, our system reduces F1 errors by 46% compared to S&amp;B-MONOS, and by 36% compared to S&amp;B-BEST-S. Moreover, with only one-fourth of the labeled data, our system already outperforms S&amp;B-MONO-S. This demonstrates that our log-linear model is better suited to take advantage of supervised labels. 7.4 Arabic Penn Treebank We also evaluated our system on the Arabic Penn Treebank (ATB). As is common in unsupervised learning, we trained and evaluated on the entire set. We compare our system with Morfessor (Creutz and Lagus, 2007).7 In addition, we compare with Morfessor Categories-MAP, which builds on Morfessor and conducts an additional greedy search specifically tailored to segmentation. We found that it per7We cannot compare with Snyder &amp; Barzilay’s system as its strongest results require bilingual data, which is not available. ATB-7000 Prec. Rec. F1 MORFESSOR-1.0 70.6 34.3 46.1 MORFESSOR-MAP 86.9 46.4 60.5 FULL 83.4 77.3 80.2 ATB Prec. Rec. F1 MORFESSOR-1.0 80.7 20.4 32.6 MORFESSOR-MAP 77.4 72.6 74.9 FULL 88.5 69.2 77.7 Table 4: Comparison of segmentation results on the Arabic Penn Treebank. forms much better than</context>
<context position="6153" citStr="Creutz &amp; Lagus (2007)" startWordPosition="906" endWordPosition="909">upta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz &amp; Lagus (2007) used an HMM, while Goldwater et al. (2006) and Snyder &amp; Barzilay (2008b) used Bayesian models based on Pitman-Yor or Dirichlet processes. These models are difficult to extend with arbitrary overlapping features that can help improve accuracy. In this work we incorporate novel overlapping contextual features and show that they greatly improve performance. Non-overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for unsupervised morphological segmentation (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007). In </context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Highperformance, language-independent morphological segmentation.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technology (NAACL).</booktitle>
<contexts>
<context position="2385" citStr="Dasgupta and Ng, 2007" startWordPosition="331" endWordPosition="334">soft Research. NLP applications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such</context>
<context position="5549" citStr="Dasgupta and Ng, 2007" startWordPosition="813" endWordPosition="816">hich favors compact representations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz &amp; Lagus (2</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng. 2007. Highperformance, language-independent morphological segmentation. In Proceedings of Human Language Technology (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
</authors>
<title>A language-independent unsupervised model for morphological segmentation.</title>
<date>2007</date>
<booktitle>In Proceedings ofthe 45th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5565" citStr="Demberg, 2007" startWordPosition="817" endWordPosition="818">resentations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz &amp; Lagus (2007) used an HMM</context>
</contexts>
<marker>Demberg, 2007</marker>
<rawString>Vera Demberg. 2007. A language-independent unsupervised model for morphological segmentation. In Proceedings ofthe 45th Annual Meeting ofthe Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="2362" citStr="Goldsmith, 2001" startWordPosition="329" endWordPosition="330">ternship at Microsoft Research. NLP applications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as ov</context>
<context position="4617" citStr="Goldsmith, 2001" startWordPosition="678" endWordPosition="680">tem can be readily applied to supervised and semi-supervised learning. Using a fraction of the labeled data, it already outperforms Snyder &amp; Barzilay’s supervised results (2008a), which further demonstrates the benefit of using a log-linear model. 2 Related Work There is a large body of work on the unsupervised learning of morphology. In addition to morphological segmentation, there has been work on unsupervised morpheme analysis, where one needs to determine features of word forms (Kurimo et al., 2007) or identify words with the same lemma by modeling stem changes (Schone and Jurafsky, 2001; Goldsmith, 2001). However, we focus our review specifically on morphological segmentation. In the absence of labels, unsupervised learning must incorporate a strong learning bias that reflects prior knowledge about the task. In morphological segmentation, an often-used bias is the minimum description length (MDL) principle, which favors compact representations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris, 1955; Keshava an</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18.</booktitle>
<contexts>
<context position="5773" citStr="Goldwater et al., 2006" startWordPosition="846" endWordPosition="850">n-grams, to identify morpheme candidates (Harris, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz &amp; Lagus (2007) used an HMM, while Goldwater et al. (2006) and Snyder &amp; Barzilay (2008b) used Bayesian models based on Pitman-Yor or Dirichlet processes. These models are difficult to extend with arbitrary overlapping features that can</context>
<context position="10882" citStr="Goldwater et al., 2006" startWordPosition="1651" endWordPosition="1654"> a flat tree, where the root node corresponds to the word and the leaves correspond to morphemes (see Figure 1). The CCM uses unigrams for context features. For this task, however, we found that bigrams and trigrams lead to much better accuracy. We use trigrams in our full model. For learning, one can either view the corpus as a collection of word types (unique words) or tokens (word occurrences). Some systems (e.g., Morfessor) use token frequency for parameter estimation. Our system, however, performs much better using word types. This has also been observed for other morphological learners (Goldwater et al., 2006). Thus we use types in learning and inference, and effectively enforce the constraint that words can have only one segmentation per type. Evaluation is still based on tokens to reflect the performance in real applications. In addition to the features of the morphemecontext model, we incorporate two priors which capture additional intuitions about morphological segmentations. First, we observe that the number of distinct morphemes used to segment a corpus should be small. This is achieved when the same morphemes are re-used across many different words. Our model incorporates this intuition by i</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Advances in Neural Information Processing Systems 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Distributional cues to word segmentation: Context is important.</title>
<date>2007</date>
<booktitle>In Proceedings of the 31st Boston University Conference on Language Development.</booktitle>
<contexts>
<context position="6748" citStr="Goldwater et al., 2007" startWordPosition="994" endWordPosition="997">odels; Creutz &amp; Lagus (2007) used an HMM, while Goldwater et al. (2006) and Snyder &amp; Barzilay (2008b) used Bayesian models based on Pitman-Yor or Dirichlet processes. These models are difficult to extend with arbitrary overlapping features that can help improve accuracy. In this work we incorporate novel overlapping contextual features and show that they greatly improve performance. Non-overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for unsupervised morphological segmentation (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith &amp; Eisner (2005) for POS tagging, and Poon &amp; </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2007</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2007. Distributional cues to word segmentation: Context is important. In Proceedings of the 31st Boston University Conference on Language Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Groves</author>
<author>Kirk Lowery</author>
<author>editors</author>
</authors>
<title>The Westminster Hebrew Bible Morphology Database. Westminster Hebrew Institute,</title>
<date>2006</date>
<location>Philadelphia, PA, USA.</location>
<marker>Groves, Lowery, editors, 2006</marker>
<rawString>Alan Groves and Kirk Lowery, editors. 2006. The Westminster Hebrew Bible Morphology Database. Westminster Hebrew Institute, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1998" citStr="Habash and Rambow, 2005" startWordPosition="275" endWordPosition="278"> bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor. 1 Introduction The goal of morphological segmentation is to segment words into morphemes, the basic syntactic/semantic units. This is a key subtask in many ∗ This research was conducted during the author’s internship at Microsoft Research. NLP applications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, </context>
<context position="21684" citStr="Habash and Rambow, 2005" startWordPosition="3514" endWordPosition="3517"> when computing the expected counts, we initialize the sampler with the most probable segmentation output by annealing. 7 Experiments We evaluated our system on two datasets. Our main evaluation is on a multi-lingual dataset constructed by Snyder &amp; Barzilay (2008a; 2008b). It consists of 6192 short parallel phrases in Hebrew, Arabic, Aramaic (a dialect of Arabic), and English. The parallel phrases were extracted from the Hebrew Bible and its translations via word alignment and postprocessing. For Arabic, the gold segmentation was obtained using a highly accurate Arabic morphological analyzer (Habash and Rambow, 2005); for Hebrew, from a Bible edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006). There is no gold segmentation for English and Aramaic. Like Snyder &amp; Barzilay, we evaluate on the Arabic and Hebrew portions only; unlike their approach, our system does not use any bilingual information. We refer to this dataset as S&amp;B . We also report our results on the Arabic Penn Treebank (ATB), which provides gold segmentations for an Arabic corpus with about 120,000 Arabic words. As in previous work, we report recall, precision, and F1 over segmentation points. We used 500 phrases fr</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>From phoneme to morpheme.</title>
<date>1955</date>
<journal>Language,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="5205" citStr="Harris, 1955" startWordPosition="763" endWordPosition="764">, 2001; Goldsmith, 2001). However, we focus our review specifically on morphological segmentation. In the absence of labels, unsupervised learning must incorporate a strong learning bias that reflects prior knowledge about the task. In morphological segmentation, an often-used bias is the minimum description length (MDL) principle, which favors compact representations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder</context>
</contexts>
<marker>Harris, 1955</marker>
<rawString>Zellig S. Harris. 1955. From phoneme to morpheme. Language, 31(2):190–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samarth Keshava</author>
<author>Emily Pitler</author>
</authors>
<title>A simple, intuitive approach to morpheme induction.</title>
<date>2006</date>
<booktitle>In Proceedings of 2nd Pascal Challenges Workshop,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="5232" citStr="Keshava and Pitler, 2006" startWordPosition="765" endWordPosition="768">ith, 2001). However, we focus our review specifically on morphological segmentation. In the absence of labels, unsupervised learning must incorporate a strong learning bias that reflects prior knowledge about the task. In morphological segmentation, an often-used bias is the minimum description length (MDL) principle, which favors compact representations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they</context>
</contexts>
<marker>Keshava, Pitler, 2006</marker>
<rawString>Samarth Keshava and Emily Pitler. 2006. A simple, intuitive approach to morpheme induction. In Proceedings of 2nd Pascal Challenges Workshop, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Natural language grammar induction using a constituentcontext model.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14.</booktitle>
<contexts>
<context position="6879" citStr="Klein and Manning (2001)" startWordPosition="1015" endWordPosition="1018">n Pitman-Yor or Dirichlet processes. These models are difficult to extend with arbitrary overlapping features that can help improve accuracy. In this work we incorporate novel overlapping contextual features and show that they greatly improve performance. Non-overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for unsupervised morphological segmentation (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith &amp; Eisner (2005) for POS tagging, and Poon &amp; Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. t</context>
<context position="10178" citStr="Klein and Manning (2001)" startWordPosition="1534" endWordPosition="1537">ical segment. Such overlapping features allow us to capture rich segmentation regularities. For example, given the Arabic word Alywm, to derive its correct segmentation Al-ywm, it helps to know that Al and ywm are likely morphemes whereas Aly or lyw are 2The word as a whole is also treated as a morpheme in itself. not; it also helps to know that Al ## or ## yw are likely morpheme contexts whereas ly ## or ## wm are not. Ablation tests verify the importance of these overlapping features (see Section 7.2). Our morpheme-context model is inspired by the constituent-context model (CCM) proposed by Klein and Manning (2001) for grammar induction. The morphological segmentation of a word can be viewed as a flat tree, where the root node corresponds to the word and the leaves correspond to morphemes (see Figure 1). The CCM uses unigrams for context features. For this task, however, we found that bigrams and trigrams lead to much better accuracy. We use trigrams in our full model. For learning, one can either view the corpus as a collection of word types (unique words) or tokens (word occurrences). Some systems (e.g., Morfessor) use token frequency for parameter estimation. Our system, however, performs much better</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Natural language grammar induction using a constituentcontext model. In Advances in Neural Information Processing Systems 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Mathias Creutz</author>
<author>Ville Turunen</author>
</authors>
<date>2007</date>
<booktitle>Overview of Morpho Challenge in CLEF 2007. In Working Notes of the CLEF</booktitle>
<note>Workshop.</note>
<contexts>
<context position="4509" citStr="Kurimo et al., 2007" startWordPosition="658" endWordPosition="661">ebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007). Our system can be readily applied to supervised and semi-supervised learning. Using a fraction of the labeled data, it already outperforms Snyder &amp; Barzilay’s supervised results (2008a), which further demonstrates the benefit of using a log-linear model. 2 Related Work There is a large body of work on the unsupervised learning of morphology. In addition to morphological segmentation, there has been work on unsupervised morpheme analysis, where one needs to determine features of word forms (Kurimo et al., 2007) or identify words with the same lemma by modeling stem changes (Schone and Jurafsky, 2001; Goldsmith, 2001). However, we focus our review specifically on morphological segmentation. In the absence of labels, unsupervised learning must incorporate a strong learning bias that reflects prior knowledge about the task. In morphological segmentation, an often-used bias is the minimum description length (MDL) principle, which favors compact representations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, suc</context>
</contexts>
<marker>Kurimo, Creutz, Turunen, 2007</marker>
<rawString>Mikko Kurimo, Mathias Creutz, and Ville Turunen. 2007. Overview of Morpho Challenge in CLEF 2007. In Working Notes of the CLEF 2007 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>649--658</pages>
<publisher>ACL.</publisher>
<location>Honolulu, HI.</location>
<contexts>
<context position="3370" citStr="Poon and Domingos, 2008" startWordPosition="479" endWordPosition="482">tation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and their contexts (e.g., in Arabic, the string Al is likely a morpheme, as is any string between Al and a word boundary). We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). We focus on inflectional morphology and test our 209 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209–217, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics approach on datasets in Arabic and Hebrew. Our system, using monolingual features only, outperforms Snyder &amp; Barzilay (2008b) by a large margin, even when their system uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (C</context>
<context position="7363" citStr="Poon &amp; Domingos (2008)" startWordPosition="1088" endWordPosition="1091">, 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith &amp; Eisner (2005) for POS tagging, and Poon &amp; Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith &amp; Eisner (2005) proposed contrastive estimation, which uses a small neighborhood to compute Z. The neighborhood is carefully designed so that it not only makes computation easier but also offers sufficient contrastive information to aid unsupervised learning. Poon &amp; Domingos (2008), on the other han</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with markov logic. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 649– 658, Honolulu, HI. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A whole sentence maximum entropy language model.</title>
<date>1997</date>
<booktitle>In IEEE workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="8108" citStr="Rosenfeld (1997)" startWordPosition="1199" endWordPosition="1200">function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith &amp; Eisner (2005) proposed contrastive estimation, which uses a small neighborhood to compute Z. The neighborhood is carefully designed so that it not only makes computation easier but also offers sufficient contrastive information to aid unsupervised learning. Poon &amp; Domingos (2008), on the other hand, used sampling to approximate Z.1 In this work, we benefit from both techniques: contrastive estimation creates a manageable, 1Rosenfeld (1997) also did this for language modeling. 210 wvlAvwn (##__##) w vlAv wn (##__vl) (#w__wn) (Av__##) Figure 1: The morpheme and context (in parentheses) features for the segmented word w-vlAv-wn. informative Z, while sampling enables the use of powerful global features. 3 Log-Linear Model for Unsupervised Morphological Segmentation Central to our approach is a log-linear model that defines the joint probability distribution for a corpus (i.e., the words) and a segmentation on the corpus. The core of this model is a morpheme-context model, with one feature for each morpheme,2 and one feature for eac</context>
</contexts>
<marker>Rosenfeld, 1997</marker>
<rawString>Ronald Rosenfeld. 1997. A whole sentence maximum entropy language model. In IEEE workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the Twenty First International Conference on Machine Learning.</booktitle>
<contexts>
<context position="7036" citStr="Sarawagi and Cohen, 2004" startWordPosition="1041" endWordPosition="1044">e incorporate novel overlapping contextual features and show that they greatly improve performance. Non-overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for unsupervised morphological segmentation (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith &amp; Eisner (2005) for POS tagging, and Poon &amp; Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence o</context>
<context position="19787" citStr="Sarawagi and Cohen, 2004" startWordPosition="3193" endWordPosition="3196">rive a sample, the procedure goes through each word and samples the next segmentation conditioned on the segmentation of all other words. With m samples S1, · · · , Sm, the expected count can be approximated as 1 � ES|W *[fi] ^ m j There are 2n−1 ways to segment a word of n characters. To sample a new segmentation for a particular word, we need to compute conditional probability for each of these segmentations. We currently do this by explicit enumeration.4 When n is large, 4These segmentations could be enumerated implicitly using the dynamic programming framework employed by semiMarkov CRFs (Sarawagi and Cohen, 2004). However, in such a setting, our lexicon prior would likely need to be approximated. We intend to investigate this in future work. P(W∗, S) fi(Sj) 213 this is very expensive. However, we observe that the maximum number of morphemes that a word contains is usually a small constant for many languages; in the Arabic Penn Treebank, the longest word contains 14 characters, but the maximum number of morphemes in a word is only 5. Therefore, we impose the constraint that a word can be segmented into no more than k morphemes, where k is a language-specific constant. We can determine k from prior know</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William Cohen. 2004. Semimarkov conditional random fields for information extraction. In Proceedings of the Twenty First International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Knowlegefree induction of inflectional morphologies.</title>
<date>2001</date>
<booktitle>In Proceedings ofHuman Language Technology (NAACL).</booktitle>
<contexts>
<context position="4599" citStr="Schone and Jurafsky, 2001" startWordPosition="674" endWordPosition="677">z and Lagus, 2007). Our system can be readily applied to supervised and semi-supervised learning. Using a fraction of the labeled data, it already outperforms Snyder &amp; Barzilay’s supervised results (2008a), which further demonstrates the benefit of using a log-linear model. 2 Related Work There is a large body of work on the unsupervised learning of morphology. In addition to morphological segmentation, there has been work on unsupervised morpheme analysis, where one needs to determine features of word forms (Kurimo et al., 2007) or identify words with the same lemma by modeling stem changes (Schone and Jurafsky, 2001; Goldsmith, 2001). However, we focus our review specifically on morphological segmentation. In the absence of labels, unsupervised learning must incorporate a strong learning bias that reflects prior knowledge about the task. In morphological segmentation, an often-used bias is the minimum description length (MDL) principle, which favors compact representations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2001. Knowlegefree induction of inflectional morphologies. In Proceedings ofHuman Language Technology (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3331" citStr="Smith and Eisner, 2005" startWordPosition="473" endWordPosition="476">that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and their contexts (e.g., in Arabic, the string Al is likely a morpheme, as is any string between Al and a word boundary). We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). We focus on inflectional morphology and test our 209 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209–217, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics approach on datasets in Arabic and Hebrew. Our system, using monolingual features only, outperforms Snyder &amp; Barzilay (2008b) by a large margin, even when their system uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% </context>
<context position="7319" citStr="Smith &amp; Eisner (2005)" startWordPosition="1080" endWordPosition="1083">007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith &amp; Eisner (2005) for POS tagging, and Poon &amp; Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith &amp; Eisner (2005) proposed contrastive estimation, which uses a small neighborhood to compute Z. The neighborhood is carefully designed so that it not only makes computation easier but also offers sufficient contrastive information to aid unsupervised learni</context>
<context position="15293" citStr="Smith &amp; Eisner (2005)" startWordPosition="2432" endWordPosition="2435">1 Pθ(W, S) = Z · uθ(W, S) where uθ(W, S) = exp( X Aσfσ(S) + X Acfc(S) σ c X+ α · L(a) σEPref(W,S) + α · X L(a) σEStem(W,S) X+ α · L(a) σESuff(W,S) + Q · X MS(w)/L(w) ) wEW Here, fσ(S) and fc(S) are respectively the occurrence counts of morphemes and contexts under S, and θ = (Aσ, Ac : a, c) are their feature weights. α, Q are the weights for the priors. Z is the normalization constant, which sums over a set of corpora and segmentations. In the next section, we will define this set for our model and show how to efficiently perform learning and inference. 4 Unsupervised Learning As mentioned in Smith &amp; Eisner (2005), learning with probabilistic models can be viewed as moving probability mass to the observed data. The question is from where to take this mass. For log-linear models, the answer amounts to defining the set that Z sums over. We use contrastive estimation and define the set to be a neighborhood of the observed data. The instances in the neighborhood can be viewed as pseudo-negative examples, and learning seeks to discriminate them from the observed instances. Formally, let W* be the observed corpus, and let N(·) be a function that maps a string to a set of strings; let N(W *) denote the set of</context>
<context position="16554" citStr="Smith &amp; Eisner (2005)" startWordPosition="2659" endWordPosition="2662"> by replacing every word w ∈ W* with one in N(w). Then, Z = X X u(W, S). WEN(W∗) S 212 ing now maximizes Lθ(W∗, S∗); the partial derivatives become Unsupervised learning maximizes the log-likelihood of observing W∗ � Lθ(W ∗) = log S We use gradient descent for this optimization; the partial derivatives for feature weights are ∂ Lθ(W∗) = ES|W*[fi] − ES,W [fi] ∂λi where i is either a string σ or a context c. The first expected count ranges over all possible segmentations while the words are fixed to those observed in W∗. For the second expected count, the words also range over the neighborhood. Smith &amp; Eisner (2005) considered various neighborhoods for unsupervised POS tagging, and showed that the best neighborhoods are TRANS1 (transposing any pair of adjacent words) and DELORTRANS1 (deleting any word or transposing any pair of adjacent words). We can obtain their counterparts for morphological segmentation by simply replacing “words” with “characters”. As mentioned earlier, the instances in the neighborhood serve as pseudo-negative examples from which probability mass can be taken away. In this regard, DELORTRANS1 is suitable for POS tagging since deleting a word often results in an ungrammatical senten</context>
<context position="18377" citStr="Smith &amp; Eisner (2005)" startWordPosition="2951" endWordPosition="2954">eadily applied to supervised or semi-supervised learning. Suppose that gold segmentation is available for some words, denoted as S∗. If S∗ contains gold segmentations for all words in W, we are doing supervised learning; otherwise, learning is semi-supervised. Train∂ Lθ(W ∗, S∗) = ES|W*,S*[fi] − ES,W [fi] ∂λi The only difference in comparison with unsupervised learning is that we fix the known segmentation when computing the first expected counts. In Section 7.3, we show that when labels are available, our model also learns much more effectively than a directed graphical model. 6 Inference In Smith &amp; Eisner (2005), the objects (sentences) are independent from each other, and exact inference is tractable. In our model, however, the lexicon prior renders all objects (words) interdependent in terms of segmentation decisions. Consider the simple corpus with just two words: Alrb, lAlrb. If lAlrb is segmented into l-Al-rb, Alrb can be segmented into Alrb without paying the penalty imposed by the lexicon prior. If, however, lAlrb remains a single morpheme, and we still segment Alrb into Al-rb, then we introduce two new morphemes into the lexicons, and we will be penalized by the lexicon prior accordingly. As </context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Crosslingual propagation for morphological analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twenty Third National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2629" citStr="Snyder and Barzilay, 2008" startWordPosition="367" endWordPosition="371">e successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and their contexts (e.g., in Arabic, the string Al is likely a morpheme, as is any string between Al and a word boundary). We develop efficient learning and inference algorithms using a novel combination of two ideas from previous</context>
<context position="5824" citStr="Snyder and Barzilay, 2008" startWordPosition="855" endWordPosition="858">, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz &amp; Lagus (2007) used an HMM, while Goldwater et al. (2006) and Snyder &amp; Barzilay (2008b) used Bayesian models based on Pitman-Yor or Dirichlet processes. These models are difficult to extend with arbitrary overlapping features that can help improve accuracy. In this work we incorporate</context>
<context position="3740" citStr="Snyder &amp; Barzilay (2008" startWordPosition="533" endWordPosition="536">ry). We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). We focus on inflectional morphology and test our 209 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209–217, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics approach on datasets in Arabic and Hebrew. Our system, using monolingual features only, outperforms Snyder &amp; Barzilay (2008b) by a large margin, even when their system uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007). Our system can be readily applied to supervised and semi-supervised learning. Using a fraction of the labeled data, it already outperforms Snyder &amp; Barzilay’s supervised results (2008a), which further demonstrates the benefit of using a log-linear model. 2 Related Work There is a large body of work on the unsupervised learning of morphology. In</context>
<context position="6224" citStr="Snyder &amp; Barzilay (2008" startWordPosition="919" endWordPosition="922">ndidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz &amp; Lagus (2007) used an HMM, while Goldwater et al. (2006) and Snyder &amp; Barzilay (2008b) used Bayesian models based on Pitman-Yor or Dirichlet processes. These models are difficult to extend with arbitrary overlapping features that can help improve accuracy. In this work we incorporate novel overlapping contextual features and show that they greatly improve performance. Non-overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for unsupervised morphological segmentation (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constit</context>
<context position="21323" citStr="Snyder &amp; Barzilay (2008" startWordPosition="3457" endWordPosition="3460">we also sample the next word from the neighborhood, in addition to the next segmentation. To compute the most probable segmentation, we use deterministic annealing. It works just like a sampling algorithm except that the weights are divided by a temperature, which starts with a large value and gradually drops to a value close to zero. To make burn-in faster, when computing the expected counts, we initialize the sampler with the most probable segmentation output by annealing. 7 Experiments We evaluated our system on two datasets. Our main evaluation is on a multi-lingual dataset constructed by Snyder &amp; Barzilay (2008a; 2008b). It consists of 6192 short parallel phrases in Hebrew, Arabic, Aramaic (a dialect of Arabic), and English. The parallel phrases were extracted from the Hebrew Bible and its translations via word alignment and postprocessing. For Arabic, the gold segmentation was obtained using a highly accurate Arabic morphological analyzer (Habash and Rambow, 2005); for Hebrew, from a Bible edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006). There is no gold segmentation for English and Aramaic. Like Snyder &amp; Barzilay, we evaluate on the Arabic and Hebrew portions only; un</context>
<context position="23258" citStr="Snyder &amp; Barzilay (2008" startWordPosition="3780" endWordPosition="3783">eebank, for which it was set to 0.005.5 We used 30 iterations for learning. In each iteration, 200 samples were collected to compute each of the two expected counts. The sampler was initialized by running annealing for 2000 samples, with the temperature dropping from 10 to 0.1 at 0.1 decrements. The most probable segmentation was obtained by running annealing for 10000 samples, using the same temperature schedule. We restricted the segmentation candidates to those with no greater than five segments in all experiments. 7.1 Unsupervised Segmentation on S&amp;B We followed the experimental set-up of Snyder &amp; Barzilay (2008b) to enable a direct comparison. The dataset is split into a training set with 4/5 of the phrases, and a test set with the remaining 1/5. First, we carried out unsupervised learning on the training data, and computed the most probable segmentation for it. Then we fixed the learned weights and the segmentation for training, and computed the most probable segmentation for the test set, on which we evaluated.6 Snyder &amp; Barzilay (2008b) compared several versions of their systems, differing in how much bilingual information was used. Using monolingual information only, their system (S&amp;B-MONO) trai</context>
<context position="28695" citStr="Snyder &amp; Barzilay (2008" startWordPosition="4681" endWordPosition="4684">9 85.5 85.2 50 88.2 86.8 87.5 75 89.6 86.4 87.9 100 91.7 88.5 90.0 HEBREW %Lbl. Prec. Rec. F1 S&amp;B-MONO-S 100 71.4 79.1 75.1 S&amp;B-BEST-S 200 76.8 79.2 78.0 FULL-S 25 78.7 73.3 75.9 50 82.8 74.6 78.4 75 83.1 77.3 80.1 100 83.0 78.9 80.9 Table 3: Comparison of segmentation results with supervised and semi-supervised learning on the S&amp;B dataset. 7.3 Supervised and Semi-Supervised Learning To evaluate our system in the supervised and semisupervised learning settings, we report the performance when various amounts of labeled data are made available during learning, and compare them to the results of Snyder &amp; Barzilay (2008a). They reported results for supervised learning using monolingual features only (S&amp;B-MONO-S), and for supervised bilingual learning with labels for both languages (S&amp;B-BEST-S). On both languages, our system substantially outperforms both S&amp;B-MONO-S and S&amp;B-BEST-S. E.g., on Arabic, our system reduces F1 errors by 46% compared to S&amp;B-MONOS, and by 36% compared to S&amp;B-BEST-S. Moreover, with only one-fourth of the labeled data, our system already outperforms S&amp;B-MONO-S. This demonstrates that our log-linear model is better suited to take advantage of supervised labels. 7.4 Arabic Penn Treebank W</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008a. Crosslingual propagation for morphological analysis. In Proceedings of the Twenty Third National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2629" citStr="Snyder and Barzilay, 2008" startWordPosition="367" endWordPosition="371">e successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and their contexts (e.g., in Arabic, the string Al is likely a morpheme, as is any string between Al and a word boundary). We develop efficient learning and inference algorithms using a novel combination of two ideas from previous</context>
<context position="5824" citStr="Snyder and Barzilay, 2008" startWordPosition="855" endWordPosition="858">, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz &amp; Lagus (2007) used an HMM, while Goldwater et al. (2006) and Snyder &amp; Barzilay (2008b) used Bayesian models based on Pitman-Yor or Dirichlet processes. These models are difficult to extend with arbitrary overlapping features that can help improve accuracy. In this work we incorporate</context>
<context position="3740" citStr="Snyder &amp; Barzilay (2008" startWordPosition="533" endWordPosition="536">ry). We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). We focus on inflectional morphology and test our 209 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209–217, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics approach on datasets in Arabic and Hebrew. Our system, using monolingual features only, outperforms Snyder &amp; Barzilay (2008b) by a large margin, even when their system uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007). Our system can be readily applied to supervised and semi-supervised learning. Using a fraction of the labeled data, it already outperforms Snyder &amp; Barzilay’s supervised results (2008a), which further demonstrates the benefit of using a log-linear model. 2 Related Work There is a large body of work on the unsupervised learning of morphology. In</context>
<context position="6224" citStr="Snyder &amp; Barzilay (2008" startWordPosition="919" endWordPosition="922">ndidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz &amp; Lagus (2007) used an HMM, while Goldwater et al. (2006) and Snyder &amp; Barzilay (2008b) used Bayesian models based on Pitman-Yor or Dirichlet processes. These models are difficult to extend with arbitrary overlapping features that can help improve accuracy. In this work we incorporate novel overlapping contextual features and show that they greatly improve performance. Non-overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for unsupervised morphological segmentation (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constit</context>
<context position="21323" citStr="Snyder &amp; Barzilay (2008" startWordPosition="3457" endWordPosition="3460">we also sample the next word from the neighborhood, in addition to the next segmentation. To compute the most probable segmentation, we use deterministic annealing. It works just like a sampling algorithm except that the weights are divided by a temperature, which starts with a large value and gradually drops to a value close to zero. To make burn-in faster, when computing the expected counts, we initialize the sampler with the most probable segmentation output by annealing. 7 Experiments We evaluated our system on two datasets. Our main evaluation is on a multi-lingual dataset constructed by Snyder &amp; Barzilay (2008a; 2008b). It consists of 6192 short parallel phrases in Hebrew, Arabic, Aramaic (a dialect of Arabic), and English. The parallel phrases were extracted from the Hebrew Bible and its translations via word alignment and postprocessing. For Arabic, the gold segmentation was obtained using a highly accurate Arabic morphological analyzer (Habash and Rambow, 2005); for Hebrew, from a Bible edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006). There is no gold segmentation for English and Aramaic. Like Snyder &amp; Barzilay, we evaluate on the Arabic and Hebrew portions only; un</context>
<context position="23258" citStr="Snyder &amp; Barzilay (2008" startWordPosition="3780" endWordPosition="3783">eebank, for which it was set to 0.005.5 We used 30 iterations for learning. In each iteration, 200 samples were collected to compute each of the two expected counts. The sampler was initialized by running annealing for 2000 samples, with the temperature dropping from 10 to 0.1 at 0.1 decrements. The most probable segmentation was obtained by running annealing for 10000 samples, using the same temperature schedule. We restricted the segmentation candidates to those with no greater than five segments in all experiments. 7.1 Unsupervised Segmentation on S&amp;B We followed the experimental set-up of Snyder &amp; Barzilay (2008b) to enable a direct comparison. The dataset is split into a training set with 4/5 of the phrases, and a test set with the remaining 1/5. First, we carried out unsupervised learning on the training data, and computed the most probable segmentation for it. Then we fixed the learned weights and the segmentation for training, and computed the most probable segmentation for the test set, on which we evaluated.6 Snyder &amp; Barzilay (2008b) compared several versions of their systems, differing in how much bilingual information was used. Using monolingual information only, their system (S&amp;B-MONO) trai</context>
<context position="28695" citStr="Snyder &amp; Barzilay (2008" startWordPosition="4681" endWordPosition="4684">9 85.5 85.2 50 88.2 86.8 87.5 75 89.6 86.4 87.9 100 91.7 88.5 90.0 HEBREW %Lbl. Prec. Rec. F1 S&amp;B-MONO-S 100 71.4 79.1 75.1 S&amp;B-BEST-S 200 76.8 79.2 78.0 FULL-S 25 78.7 73.3 75.9 50 82.8 74.6 78.4 75 83.1 77.3 80.1 100 83.0 78.9 80.9 Table 3: Comparison of segmentation results with supervised and semi-supervised learning on the S&amp;B dataset. 7.3 Supervised and Semi-Supervised Learning To evaluate our system in the supervised and semisupervised learning settings, we report the performance when various amounts of labeled data are made available during learning, and compare them to the results of Snyder &amp; Barzilay (2008a). They reported results for supervised learning using monolingual features only (S&amp;B-MONO-S), and for supervised bilingual learning with labels for both languages (S&amp;B-BEST-S). On both languages, our system substantially outperforms both S&amp;B-MONO-S and S&amp;B-BEST-S. E.g., on Arabic, our system reduces F1 errors by 46% compared to S&amp;B-MONOS, and by 36% compared to S&amp;B-BEST-S. Moreover, with only one-fourth of the labeled data, our system already outperforms S&amp;B-MONO-S. This demonstrates that our log-linear model is better suited to take advantage of supervised labels. 7.4 Arabic Penn Treebank W</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008b. Unsupervised multilingual learning for morphological segmentation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>