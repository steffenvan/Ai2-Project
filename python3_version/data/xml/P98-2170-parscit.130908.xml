<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000234">
<title confidence="0.995271">
A Procedure for Multi-Class Discrimination and some Linguistic
Applications
</title>
<author confidence="0.995728">
Vladimir Pericliev
</author>
<affiliation confidence="0.9048075">
Institute of Mathematics Sz Informatics
Acad. G. Bonchev Str., hi. 8,
</affiliation>
<address confidence="0.879561">
1113 Sofia, Bulgaria
</address>
<email confidence="0.91127">
periOmath.acad.bg
</email>
<sectionHeader confidence="0.993972" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885214285714">
The paper describes a novel computa-
tional tool for multiple concept learn-
ing. Unlike previous approaches, whose
major goal is prediction on unseen in-
stances rather than the legibility of the
output, our MPD (Maximally Parsimo-
nious Discrimination) program empha-
sizes the conciseness and intelligibility
of the resultant class descriptions, using
three intuitive simplicity criteria to this
end. We illustrate MPD with applica-
tions in componential analysis (in lexicol-
ogy and phonology), language typology,
and speech pathology.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.96880492">
A common task of knowledge discovery is multi-
ple concept learning, in which from multiple given
classes (i.e. a typology) the profiles of these classes
are inferred, such that every class is contrasted from
every other class by feature values. Ideally, good
profiles, besides making good predictions on future
instances, should be concise, intelligible, and com-
prehensive (i.e. yielding all alternatives).
Previous approaches like ID3 (Quinlan, 1983) or
C4.5 (Quinlan, 1993), which use variations on greedy
search, i.e. localized best-next-step search (typi-
cally based on information-gain heuristics), have as
their major goal prediction on unseen instances, and
therefore do not have as an explicit concern the
conciseness, intelligibility, and comprehensiveness of
the output. In contrast to virtually all previous
approaches to multi-class discrimination, the MPD
(Maximally Parsimonious Discrimination) program
we describe here aims at the legibility of the resul-
tant class profiles. To do so, it (1) uses a minimal
number of features by carrying out a global opti-
mization, rather than heuristic greedy search; (2)
produces conjunctive, or nearly conjunctive, profiles
for the sake of intelligibility; and (3) gives all alterna-
tive solutions. The first goal stems from the familiar
</bodyText>
<note confidence="0.703447">
Rai11 E. Valdes-Perez
</note>
<affiliation confidence="0.936176">
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.674286">
valdesOcs.cmu.edu
</email>
<bodyText confidence="0.982451806451613">
requirement that classes be distinguished by jointly
necessary and sufficient descriptions. The second ac-
cords with the also familiar thesis that conjunctive
descriptions are more comprehensible (they are the
norm for typological classification (Hempel, 1965),
and they are more readily acquired by experimen-
tal subjects than disjunctive ones (Bruner et. al.,
1956)), and the third expresses the usefulness, for a
diversity of reasons, of having all alternatives. Lin-
guists would generally subscribe to all three require-
ments, hence the need for a computational tool with
such focus.&apos;
In this paper, we briefly describe the MPD system
(details may be found in Valdes-Perez and Pericliev,
1997; submitted) and focus on some linguistic appli-
cations, including componential analysis of kinship
terms, distinctive feature analysis in phonology, lan-
guage typology, and discrimination of aphasic syn-
dromes from coded texts in the CHILDES database.
For further interesting application areas of similar
algorithms, cf. Daelemans et. al., 1996 and Tanaka,
1996.
2 Overview of the MPD program
The Maximally Parsimonious Discrimination pro-
gram (MPD) is a general computational tool for
inferring, given multiple classes (or, a typology),
with attendant instances of these classes, the pro-
files (=descriptions) of these classes such that every
class is contrasted from all remaining classes on the
basis of feature values. Below is a brief description
of the program.
</bodyText>
<subsectionHeader confidence="0.96879">
2.1 Expressing contrasts
</subsectionHeader>
<bodyText confidence="0.9715945">
The MPD program uses Boolean, nominal and nu-
meric features to express contrasts, as follows:
</bodyText>
<footnote confidence="0.987667666666667">
&apos;The profiling of multiple types, in actual fact, is a
generic task of knowledge discovery, and the program
we describe has found substantial applications in areas
outside of linguistics, as e.g., in criminology, audiology,
and datasets from the UC Irvine repository. However,
we shall not discuss these applications here.
</footnote>
<page confidence="0.992681">
1034
</page>
<listItem confidence="0.9735405">
• Two classes Cl and C2 are contrasted by a
Boolean or nominal feature if the instances of
Cl and the instances of C2 do not share a value.
• Two classes Cl and C2 are contrasted by a nu-
meric feature if the ranges of the instances of
Cl and of C2 do not overlap.2
</listItem>
<bodyText confidence="0.996666125">
MPD distinguishes two types of contrasts: (1) ab-
solute contrasts when all the classes can be cleanly
distinguished, and (2) partial contrasts when no ab-
solute contrasts are possible between some pairwise
classes, but absolute contrasts can nevertheless be
achieved by deleting up to N per cent of the in-
stances, where N is specified by the user.
The program can also invent derived features—in
the case when no successful (absolute) contrasts are
so far achieved—the key idea of which is to express
interactions between the given primitive features.
Currently we have implemented inventing novel de-
rived features via combining two primitive features
(combining three or more primitive features is also
possible, but has not so far been done owing to the
likelihood of a combinatorial explosion):
</bodyText>
<listItem confidence="0.9960869">
• Two Boolean features P and Q are combined
into a set of two-place functions, none of which
is reducible to a one-place function or to the
negation of another two-place function in the
set. The resulting set consists of P-and-Q, P-
or-Q, P-iff-Q, P-implies-Q, and Q-implies-P.
• Two nominal features M and N are combined
into a single two-place nominal function MxN.
• Two numeric features X and Y are combined
by forming their product and their quotient.3
</listItem>
<bodyText confidence="0.998814733333333">
Both primitive and derived features are treated
analogously in deciding whether two classes are con-
trasted by a feature, since derived features are legit-
imate Boolean, nominal or numeric features.
It will be observed that contrasts by a nominal
or numeric feature may (but will not necessarily)
introduce a slight degree of disjunctiveness, which is
to a somewhat greater extent the case in contrasts
accomplished by derived features.
Missing values do not present much problem,
since they can be ignored without any need to es-
timate a value nor to discard the remaining infor-
mative features values of the instance. In the case
of nominal features, missing values can be treated as
just another legitimate feature value.
</bodyText>
<subsectionHeader confidence="0.999412">
2.2 The simplicity criteria
</subsectionHeader>
<bodyText confidence="0.903869333333333">
MPD uses three intuitive criteria to guarantee the
uncovering of the most parsimonious discrimination
among classes:
</bodyText>
<footnote confidence="0.984726">
2Besides these atomic feature values we may also sup-
port (hierarchically) structured values, but this will be
of no concern here.
3Analogously to the Bacon program&apos;s invention of
theoretical terms Langley et. al., 1987.
</footnote>
<listItem confidence="0.8902138">
1. Minimize overall features. A set of classes may
be demarcated using a number of overall fea-
ture sets of different cardinality; this criterion
chooses those overall feature sets which have
the smallest cardinality (i.e. are the shortest).
2. Minimize profiles. Given some overall feature
set, one class may be demarcated—using only
features from this set—by a number of profiles
of different cardinality; this criterion chooses
those profiles having the smallest cardinality.
3. Maximize coordination. This criterion maxi-
mizes the coherence between class profiles in
one discrimination mode1,4 in the case when
alternative profiles remain even after the appli-
cation of the two previous simplicity criteria.3
</listItem>
<bodyText confidence="0.9997153">
Due to space limitations, we cannot enter into the
implementation details of these global optimization
criteria, in fact the most expensive mechanism of
MPD. Suffice it to say here that they are imple-
mented in a uniform way (in all three cases by con-
verting a logic formula – either CNF or something
more complicated – into a DNF formula), and all can
use both sound and unsound (but good) heuristics
to deal successfully with the potentially explosive
combinatorics inherent in the conversion to DNF.
</bodyText>
<subsectionHeader confidence="0.999427">
2.3 An illustration
</subsectionHeader>
<bodyText confidence="0.990446387096774">
By way of (a simplified) illustration, let us consider
the learning of the Bulgarian translational equiva-
lents of the English verb feed on the basis of the
case frames of the latter. Assume the following fea-
tures/values, corresponding to the verbal slots: (1)
NP1={hum,beast,phys-obj}, (2) VTR (binary fea-
ture denoting whether the verb is transitive or not),
(3) NP2 (same values as NP1), (4) PP (binary fea-
ture expressing the obligatory presence of a prepo-
sitional phrase). An illustrative input to MPD is
given in Table 1 (the sentences in the third column
of the table are not a part of the input, and are only
given for the sake of clarity, though, of course, would
normally serve to deriving the instances by parsing).
The output of the program is given in Table 2.
MPD needs to find 10 pairwise contrasts between the
5 classes (i.e. N-choose-2, calculable by the formula
N(N-1)/2 ), and it has successfully discriminated all
4In a &amp;quot;discrimination model&amp;quot; each class is described
with a unique profile.
5By way of an abstract example, denote features by
Fl...Fn, and let Class 1 have the profiles: (1) Fl F2,
(2) Fl F3, and Class 2: (1) F4 F2, (2) F4 F5, (3) F4
F6. Combining freely all alternative profiles with one
another, we should get 6 discrimination models. How-
ever, in Class 1 we have a choice between [F2 F3] (F1
must be used), and in Class 2 between [F2 F5 F6] (F4
must be used); this criterion, quite analogously to the
previous two, will minimize this choice, selecting F2 in
both cases, and hence yield the unique model Class 1:
Fl F2, and Class 2: F4 F2.
</bodyText>
<page confidence="0.997344">
1035
</page>
<tableCaption confidence="0.998656">
Table 1: Classes and Instances
</tableCaption>
<figure confidence="0.993591914285714">
Classes
Instances
Illustrations
1. NP1=hum VTR NP2=beast
2. NP1=hum VTR NP2=beast-.PP
1. NP1=hum VTR NP2=hum-,PP
2. NP1=beast VTR NP2=beast -.PP
1. NP1=beast -.VTR PP
2. NP1=beast -.VTR PP
1. NP1=hum VTR NP2=phys-obj PP
2. NP1=hum VTR NP2=phys-obj PP
1. NP1=phys-obj VTR NP2=phys-obj PP
2. NP1=phys-obj VTR NP2=phys-obj PP
3. NP1=hum VTR NP2=phys-obj PP
1.11e feeds pigs
2.Jane feeds cattle
1.Nurses feed invalids
2.Wild animals feed their
cubs regularly
1.Horses feed on grass
2.Cows feed on hay
1.Farmers feed corn to fowls
2.This family feeds meat
to their dog
1.The production line feeds
cloth in the machine
2.The trace feeds paper
to the printer
3.Jim feeds coal to a
furnace
1.otglezdam
2.xranja
3.xranja-se
4.zaxranvam
5.podavam
</figure>
<tableCaption confidence="0.960367">
Table 2: Classes and their Profiles
</tableCaption>
<bodyText confidence="0.999809">
classes. This is done by the overall feature set {NP1,
PP, NP1xNP2}, whose first two features are primi-
tive, and the third is a derived nominal feature. Not
all classes are absolutely discriminated: Class 4 (za-
xrarivam) and Class 5 (podavam) are only partially
contrasted by the feature NP1. Thus, Class 5 is
66.6% NP1=phys-obj since we need to retract 1/3
of its instances (particularly, sentence (3) from Ta-
ble 1 whose NP1=hum) in order to get a clean con-
trast by that feature. Class 1 (otglezdam) and Class
2 (zranja) use in their profiles the derived nominal
feature NP lxNP2; they actually contrast because all
instances of Class 1 have the value &apos;hum&apos; for NP1
and the value &apos;beast&apos; for NP2, and hence the &amp;quot;de-
rived value&amp;quot; [hum beast], whereas neither of the in-
stances of Class 2 has an identical derived value (in-
deed, referring to Table 1, the first instance of Class
2 has NP lxNP2=[hum hum] and the second instance
NP1xNP2=[beast beast]). The resulting profiles in
Table 2 is the simplest in the sense that there are
no more concise overall feature sets that discrimi-
nate the classes, and the profiles-using only fea-
tures from the overall feature set-are the shortest.
</bodyText>
<sectionHeader confidence="0.869801" genericHeader="method">
3 Componential analysis
</sectionHeader>
<subsectionHeader confidence="0.998267">
3.1 In lexicology
</subsectionHeader>
<bodyText confidence="0.999439673469388">
One of the tasks we addressed with MPD is se-
mantic componential analysis, which has well-known
linguistic implications, e.g., for (machine) trans-
lation (for a familiar early reference, cf. Nida,
1971). More specifically, we were concerned with
the componential analysis of kinship terminologies,
a common area of study within this trend. KIN-
SHIP is a specialized computer program, having as
input the kinterms (=classes) of a language, and
their attendant kintypes (=instances).6 It com-
putes the feature values of the kintypes, and then
feeds the result to the MPD component to make
the discrimination between the kinterms of the lan-
guage. Currently, KINSHIP uses about 30 features,
of all types: binary (e.g., male={+/-}), nominal
(e.g., lineal={1ineal, co-lineal, ablineal}), and nu-
meric (e.g., generation={1,2,..,n}).
In the long history of this area of study, prac-
titioners of the art have come up with explicit re-
quirements as regards the adequacy of analysis: (1)
Parsimony, including both overall features and kin-
term descriptions (=profiles). (2) Conjunctiveness
of kinterm descriptions. (3) Comprehensiveness in
displaying all alternative componential models.
As seen, these requirements fit nicely with most
of the capabilities of MPD. This is not accidental,
since, historically, we started our investigations by
automating the important discovery task of com-
ponential analysis, and then, realizing the generic
nature of the discrimination subtask, isolated this
part of the program, which was later extended with
the mechanisms for derived features and partial con-
trasts.
Some of the results of KINSHIP are worth sum-
marizing. The program has so far been applied to
more than 20 languages of different language fami-
lies. In some cases, the datasets were partial (only
consanguineal, or blood) kin systems, but in oth-
ers they were complete systems comprising 40-50
classes with several hundreds of instances. The pro-
gram has re-discovered some classical analyses (of
the Amerindian language Seneca by Lounsbury),
has successfully analyzed previously unanalyzed lan-
guages (e.g., Bulgarian), and has improved on pre-
vious analyses of English. For English, the most
parsimonious model has been found, and the only
one giving conjunctive class profiles for all kinterms,
which sounds impressive considering the massive ef-
forts concentrated on analyzing the English kinship
</bodyText>
<footnote confidence="0.60817">
6Examples of English kinterms are father, uncle, and
of their respective kintypes are: Fa (father); FaBr (fa-
ther&apos;s brother) MoBr (mother&apos;s brother) FaFaSo (fa-
ther&apos;s father&apos;s son) and a dozen of others.
</footnote>
<figure confidence="0.98017675">
Classes
Profiles
1.otglezdam
2.xranja
3.xranja-se
4.zaxranvam
5.podavam
-.PP NP1xNP2=([hum beast])
NP1xNP2=ahum hum] v (beast beast])
NP1=beast PP
NP1=hum PP
66.6% NP1=phys-obj PP
</figure>
<page confidence="0.952266">
1036
</page>
<bodyText confidence="0.992483769230769">
system .7
Most importantly, MPD has shown that the huge
number of potential componential (=discrimination)
models—a menace to the very foundations of the
approach, which has made some linguists propose
alternative analytic tools— are in fact reduced to
(nearly) unique analyses by our 3 simplicity crite-
ria. Our 3rd criterion, ensuring the coordination be-
tween equally simple alternative profiles, and with
no precedence in the linguistic literature, proved es-
sential in the pruning of solutions (details of KIN-
SHIP are reported in Pericliev and Valdes-Perez,
1997; Pericliev and Valdes-Perez, forthcoming).
</bodyText>
<subsectionHeader confidence="0.999381">
3.2 In phonology
</subsectionHeader>
<bodyText confidence="0.999684757575757">
Componential analysis in phonology amounts to
finding the distinctive features of a phonemic sys-
tem, differentiating any phoneme from all the rest.
The adequacy requirements are the same as in the
above subsection, and indeed they have been bor-
rowed in lexicology (and morphology for that mat-
ter) from phonological work which chronologically
preceded the former. We applied MPD to the Rus-
sian phonemic system, the data coming from a paper
by Cherry et. al., 1953, who also explicitly state as
one of their goals the finding of minimal phoneme
descriptions.
The data consisted of 42 Russian phonemes, i.e.
the transfer of feature values from instances (=allo-
phones) to their respective classes (=phonemes) has
been previously performed. The phonemes were de-
scribed in terms of the following 11 binary features:
(1) vocalic, (2) consonantal, (3) compact, (4) dif-
fuse, (5) grave, (6) nasal, (7) continuant, (8) voiced,
(9) sharp, (10) strident, (11) stressed. MPD con-
firmed that the 11 primitive overall features are in-
deed needed, but it found 11 simpler phoneme pro-
files than those proposed in this classic article (cf.
Table 3). Thus, the average phoneme profile turns
out to comprise 6.14, rather than 6.5, components
as suggested by Cherry et. al.
The capability of MPD to treat not just binary,
but also non-binary (nominal) features, it should be
noted, makes it applicable to datasets of a newer
trend in phonology which are not limited to us-
ing binary features, and instead exploit multivalued
symbolic features as legitimate phonological build-
ing blocks.
</bodyText>
<sectionHeader confidence="0.984553" genericHeader="method">
4 Language typology
</sectionHeader>
<bodyText confidence="0.99981375">
We have used MPD for discovery of linguistic ty-
pologies, where the classes to be contrasted are in-
dividual languages or groups of languages (language
families).
</bodyText>
<footnote confidence="0.969345666666667">
7We also found errors in analyses performed by lin-
guists, which is understandable for a computationally
complex task like this.
</footnote>
<tableCaption confidence="0.996704">
Table 3: Russian phonemes and their profiles
</tableCaption>
<bodyText confidence="0.997821576923077">
In one application, MPD was run on the dataset
from the seminal paper by Greenberg (1966) on word
order universals. This corpus has previously been
used to uncover linguistic universals, or similarities;
we now show its feasibility for the second fundamen-
tal typological task of expressing the differences be-
tween languages. The data consist of a sample of 30
languages with a wide genetic and areal coverage.
The 30 classes to be differentiated are described in
terms of 15 features, 4 of which are nominal, and the
remaining 11 binary. Running MPD on this dataset
showed that from 435 (30-Choose-2) pairwise dis-
criminations to be made, just 12 turned out to be
impossible, viz, the pairs:
(berber ,z apotec) , (berber , welsh)
(berber,hebrew), (fulani,swahili)
(greek,serbian), (greek,maya)
(hebrew,zapotec), (japanese,turkish)
(japanese,kannada), (kannada,turkish)
(malay,yoruba), (maya,serbian)
The contrasts (uniquely) were made with a minimal
set of 8 features: {SubjVerbObj-order, Adj &lt; N,
Genitive &lt; N, Demonstrative &lt; N, Numeral &lt; N,
Aux &lt; V, Adv &lt; Adj, affixation}.
In the processed dataset, for a number of lan-
guages there were missing values, esp. for features
</bodyText>
<figure confidence="0.997534571428571">
Classes I 2 3 4 5 6 7 8 9 10 11
&apos;U
&apos;o
&apos;e
&apos;a
a
1
</figure>
<page confidence="0.990682">
1037
</page>
<bodyText confidence="0.998656538461539">
(12) through (14). The linguistic reasons for this
were two-fold: (i) lack of reliable information; or (ii)
non-applicability of the feature for a specific lan-
guage (e.g., many languages lack particles for ex-
pressing yes-no questions, i.e. feature (12)). The
above results reflect our default treatment of miss-
ing values as making no contribution to the contrast
of language pairs. Following the other alternative
path, and allowing &apos;missing&apos; as a distinct value, will
result in the successful discrimination of most lan-
guage pairs. Greek and Serbian would remain in-
discriminable, which is no surprise given their areal
and genetic affinity.
</bodyText>
<sectionHeader confidence="0.867659" genericHeader="method">
5 Speech production in aphasics
</sectionHeader>
<bodyText confidence="0.999974875">
This application concerns the discrimination of dif-
ferent forms of aphasia on the basis of their language
behaviour.8
We addressed the profiling of aphasic patients, us-
ing the CAP dataset from the CHILDES database
(MacWhinney, 1995), containing (among others) 22
English subjects; 5 are control and the others suffer
from anomia (3 patients), Broca&apos;s disorder (6), Wer-
nicke&apos;s disorder (5), and nonfluents (3). The patients
are grouped into classes according to their fit to a
prototype used by neurologists and speech pathol-
ogists. The patients&apos; records—verbal responses to
pictorial stimuli—are transcribed in the CHILDES
database and are coded with linguistic errors from
an available set that pertains to phonology, morphol-
ogy, syntax and semantics.
As a first step in our study, we attempted to pro-
file the classes using just the errors as they were
coded in the transcripts, which consisted of a set of
26 binary features, based on the occurrence or non-
occurrence of an error (feature) in the transcript of
each patient. We ran MPD with primitive features
and absolute contrasts and found that from a total of
10 pairwise contrasts to be made between 5 classes, 7
were impossible, and only 3 possible. We then used
derived features and absolute contrasts, but still one
pair (Broca&apos;s and Wernicke&apos;s patients) remained un-
contrasted. We obtained 80 simplest models with 5
features (two primitive and three derived) discrimi-
nating the four remaining classes.
We found this profiling unsatisfactory from a do-
main point of view for several reasons9 which led us
</bodyText>
<footnote confidence="0.475797909090909">
8We are grateful to Prof. Brian MacWhinney from
the Psychology Dpt. of CMU for helpful discussions on
this application of MPD.
9First, one pair remained uncontrasted. Second, only
3 pairwise contrasts were made with absolute primitive
features, which are as a rule most intuitively acceptable
as regards the comprehensibility of the demarcations (in
this specific case they correspond to &amp;quot;standard&amp;quot; errors,
priorly and independently identified from the task under
consideration). And, third, some of the derived features
necessary for the profiling lacked the necessary plausibil-
</footnote>
<tableCaption confidence="0.8034495">
Table 4: Profiles of Aphasic Patients with Absolute
Features and Partial Contrasts
</tableCaption>
<bodyText confidence="0.99870775">
to re-examining the transcripts (amounting roughly
to 80 pages of written text) and adding manually
some new features that could eventually result in
more intelligible profiling. These included:
</bodyText>
<listItem confidence="0.992991071428571">
(1) Prolixity. This feature is intended to simu-
late an aspect of the Grice&apos;s maxim of manner, viz.
&amp;quot;Avoid unnecessary prolixity&amp;quot;. We try to model
it by computing the average number of words pro-
nounced per individual pictorial stimulus, so each
patient is assigned a number (at present, each word-
like speech segment is taken into account). Wer-
nicke&apos;s patients seem most prolix, in general.
(2) Truthfulness. This feature attempts to sim-
ulate Grices&apos; Maxim of Quality: &amp;quot;Be truthful. Do
not say that for which you lack adequate evidence&amp;quot;.
Wernicke&apos;s patients are most persistent in violating
this maxim by fabricating things not seen in the pic-
torial stimuli. All other patients seem to conform to
the maxim, except the nonfluents whose speech is
difficult to characterize either way (so this feature is
considered irrelevant for contrasting).
(3) Fluency. By this we mean general fluency, nor-
mal intonation contour, absence of many and long
pauses, etc. The Broca&apos;s and non-fluent patients
have negative value for this feature, in contrast to
all others.
(4) Average number of errors. This is the sec-
ond numerical feature, besides prolixity. It counts
the average number of errors per individual stimu-
lus (picture). Included are all coder&apos;s markings in
the patient&apos;s text, some explicitly marked as errors,
others being pauses, retracings, etc.
</listItem>
<bodyText confidence="0.978225928571429">
Re-running MPD with absolute primitive features
on the new data, now having more than 30 fea-
tures, resulted in 9 successful demarcations out of 10.
Two sets of primitive features were used to this end:
{average errors, fluency, prolixity} and {average er-
rors, fluency, truthfulness). The Broca&apos;s patients
and the nonfluent ones, which still resisted discrim-
ination, could be successfully handled with nine al-
ternative derived Boolean features, formed from dif-
ferent combinations of the coded errors (a handful
of which are also plausible). We also ran MPD with
primitive features and partial contrasts (cf. Table 4).
Retracting one of the six Broca&apos;s subjects allows all
ity for domain scientists.
</bodyText>
<figure confidence="0.997187045454546">
Classes
Control
Subjects
Anomic
Subjects
Broca&apos;s
Subjects
Wernicke&apos;s
Subjects
Nonfluent
Subjects
Profiles
average errors.(0, 1.3]
average errors=[1.7, 4.6]
prolixity =17, 7.5]
fluency
-,fluency
87% ,semi-intelligible
prolixity=[12, 30.11
fluency
-,fluency
semi-intelligible
</figure>
<page confidence="0.986446">
1038
</page>
<bodyText confidence="0.996265636363636">
classes to be completely discriminated.
These results may be considered satisfactory from
the point of view of aphasiology. First of all, now
all disorders are successfully discriminated, most
cleanly, and this is done with the primitive features,
which, furthermore, make good sense to domain spe-
cialists: control subjects are singled out by the least
number of mistakes they make, Wernicke&apos;s patients
are contrasted from anomic ones by their greater
prolixity, anomics contrast Broca&apos;s and nonfluent
patients by their fluent speech, etc.
</bodyText>
<sectionHeader confidence="0.6699095" genericHeader="method">
6 MPD in the context of diverse
application types
</sectionHeader>
<bodyText confidence="0.978629584158416">
A learning program can profitably be viewed along
two dimensions: (1) according to whether the output
of the program is addressed to a human or serves
as input to another program; and (2) according to
whether the program is used for prediction of future
instances or not. This yields four alternatives:
type (i) (+human/-prediction),
type (ii) (+human/+prediction),
type (iii) (-human/+prediction), and
type (iv) (-human/-prediction).
We may now summarize MPD&apos;s mechanisms in
the context of the diverse application types. These
observations will clear up some of the discussion in
the previous sections, and may also serve as guide-
lines in further specific applications of the program.
Componential analysis falls under type (i):
a componential model is addressed to a lin-
guist/anthropologist, and there is no prediction of
unseen instances, since all instances (e.g., kintypes
in kinship analysis) are as a rule available at the
outset. 10
The aphasics discrimination task can be classed
as type (ii): the discrimination model aims to make
sense to a speech pathologist, but it should also have
good predictive power in assigning future patients to
the proper class of disorder.
Learning translational equivalents from verbal
case frames belongs to type (iii) since the output of
the learner will normally be fed to other subroutines
and this output model should make good predictions
as to word selection in the target language, encoun-
tering future sentences in the source language.
We did not discuss here a case of type (iv), so we
just mention an example. Given a grammar G, the
learner should find &amp;quot;look-aheads&amp;quot; , specifying which
of the rules of G should be fired first.11 In this task,
&apos;`&apos;We note that componential analysis in phonology
can alternatively be viewed of type (iii) if its ultimate
goal is speech recognition.
&amp;quot;A trivial example is G, having rules: (i) sl—&gt;np, VP,
; (ii) s2—&gt;vp, [T] ; (iii) s3-4aux, np, v, [&apos;?1, where
the classes are the LHS, the instances are the RHS, and
the profiling should decide which of the 3 rules to use
the output of the learner can be automatically in-
corporated as an additional rule in G (an hence be
of no direct human use), and it should make no pre-
dictions since it applies to the specific G, and not to
any other grammar.
For tasks of types (i) and (ii), a typical scenario
of using MPD would be:
Using all 3 simplicity criteria, and find-
ing all alternative models, follow the fea-
ture/contrast hierarchy: primitive fea-
tures &amp; absolute contrasts &gt; derived &amp;
absolute &gt; primitive &amp; partial &gt; derived
&amp; partial
which reflects the desiderata of conciseness, compre-
hensiveness, and intelligibility (as far as the latter
is concerned, the primitive features (normally user-
supplied) are preferable to the computer-invented,
possibly disjunctive, derived features).
However, in some specific tasks, another hierarchy
seems preferable, which the user is free to follow.
E.g., in kinship under type (i), the inability of MPD
to completely discriminate the kinterms may very
well be due to noise in the instances, a situation
by no means infrequent, esp. in data for &amp;quot;exotic&amp;quot;
languages. In a type (ii) task, an analogous situation
may hold (e.g., a patient may be erroneously classed
under some impairment), all this leading to trying
first the primitive &amp; partial heuristic. There may be
other reasons to change the order of heuristics in the
hierarchy as well.
We see no clear difference between types (i)-(ii)
tasks, placing the emphasis in (ii) on the human ad-
dressee subtask rather than on prediction subtask,
because it is not unreasonable to suppose that a con-
cise and intelligible model has good chances of rea-
sonably high predictive power.12
We have less experience in applying MPD on tasks
of types (iii) and (iv) and would therefore refrain
from suggesting typical scenarios for these types. We
offer instead some observations on the role of MPD&apos;s
mechanisms in the context of such tasks, showing at
some places their different meaning/implication in
comparison with the previous two tasks:
(1) Parsimony, conceived as a minimality of class
profiles, is essential in that it generally contributes to
reducing the cost of assigning an incoming instance
to a class. (In contrast to tasks of types (i)-(ii), the
Maximize-Coordination criterion has no clear mean-
ing here, and the Minimize-Features may well be
having as input say Come here!.
12b&apos;y way of a (non-linguistic) illustration, we have
turned the MPD profiles into classification rules and have
carried out an initial experiment on the LED-24 dataset
from the UC Irvine repository. MPD classified 1000 un-
seen instances at 73 per cent, using five features, which
compares well with a seven features classifier reported
in the literature, as well as with other citations in the
repository entry.
</bodyText>
<page confidence="0.99042">
1039
</page>
<bodyText confidence="0.958636571428572">
sacrificed in order to get shorter profiles).13
(2) Conjunctiveness is of less importance here
than in tasks of type (i)-(ii), but a better legibil-
ity of profiles is in any case preferable. The derived
features mechanism can be essential in achieving in-
tuitive contrasts, as in verbal case frame learning,
where the interaction between features nicely fits the
task of learning &amp;quot;slot dependencies&amp;quot; (Li and Abe,
1996).
(3) All alternative profiles of equal simplicity are
not always a necessity as in tasks of type (i)-(ii), but
are most essential in many tasks where there are dif-
ferent costs of finding the feature values of unseen
instances (e.g., computing a syntactic feature, gen-
erally, would be much less expensive than computing
say a pragmatic one).
The important point to emphasize here is that
MPD generally leaves these mechanisms as program
parameters to be set by the user, and thus, by chang-
ing its inductive bias, it may be tailored to the spe-
cific needs that arise within the 4 types of tasks.
</bodyText>
<sectionHeader confidence="0.998828" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9996306">
The basic contributions of this paper are: (1) to in-
troduce a novel flexible multi-class learning program,
MPD, that emphasizes the conciseness and intelligi-
bility of the class descriptions; (2) to show some uses
of MPD in diverse linguistic fields, at the same time
indicating some prospective modes of using the pro-
gram in the different application types; and (3) to
describe substantial results that employed the pro-
gram.
A basic limitation of MPD is of course its inability
to handle inherently disjunctive concepts, and there
are indeed various tasks of this sort. Also, despite
its efficient implementation, the user may sometimes
be forced to sacrifice conciseness (e.g., choose two
primitive features instead of just one derived that
can validly replace them) in order to evade combi-
natorial problems. Nevertheless in our experience
with linguistic (and not only linguistic) tasks MPD
has proved a successful tool for solving significant
practical problems. As far as our ongoing research
is concerned, we basically are focussing on finding
novel application areas.
Acknowledgments. This work was supported by a
grant #IRI-9421656 from the (USA) National Sci-
ence Foundation and by the NSF Division of Inter-
national Programs.
13E.g., instead of the profile [xranja-se: NP1=beast
PP] in Table 2, one may choose the valid shorter profile
[xranja-se: -VTR], even though that would increase the
number of overall features used.
</bodyText>
<sectionHeader confidence="0.996183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986886377358491">
C. Cherry, M. Halle, and R, Jakobson. 1953. To-
ward the logical description of languages in their
phonemic aspects. Language 29:34-47.
W. Daelemans, P. Berck, and S. Gillis. 1996. Un-
supervised discovery of phonological categories
through supervised learning of morphological
rules. COLING96, Copenhagen, pages 95-100.
J. Bruner, J. Goodnow, and G. Austin. 1956. A
Study of Thinking. John Wiley, New York.
J. Greenberg. 1966. Some universals of grammar
with particular reference to the order of meaning-
ful elements. In J. Greenberg, ed. Universals of
Language, MIT Press, Cambridge, Mass.
C. Hempel. 1965. Aspects of Scientific Explanation.
The Free Press, New York.
P. Langley, H. Simon, G. Bradshaw, and J, Zytkow.
1987. Scientific Discovery: Computational Explo-
rations of the Creative Process. The MIT Press,
Cambridge, Mass.
Hang Li and Naoki Abe. 1996. Learning depen-
dencies between case frame slots. COLING96,
Copenhagen, pages 10-15.
B. MacWhinney. 1995. The CHILDES Project:
Tools for Analyzing Talk. Lawrence Erlbaum, N.J.
E. Nida. 1971. Semantic components in translation
theory. In G. Perren and J. Trim (eds.) Appli-
cations of Linguistics, pages 341-348. Cambridge
University Press, Cambridge, England.
V. Pericliev and R. E. Valdes-Perez. 1997. A dis-
covery system for componential analysis of kin-
ship terminologies. In B. Caron (ed.) 16th Inter-
national Congress of Linguists, Paris, July 1997,
Elsevier.
V. Pericliev and R. E. Valdes-Perez. forthcoming.
Automatic componential analysis of kinship se-
mantics with a proposed structural solution to the
problem of multiple models. Anthropological Lin-
guistics.
J. R. Quinlan. 1986. Induction of decision trees.
Machine Learning, 1:81-106.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
H.Tanaka. 1996. Decision tree learning algorithm
with structured attributes: Application to verbal
case frame acquisition. COLING96, Copenhagen,
pages 943-948.
R. E. Valdes-Perez and V. Pericliev. 1997. Maxi-
mally parsimonious discrimination: a task from
linguistic discovery. A A AI97, Providence, RI,
pages 515-520.
R. E. Valdes-Perez and V. Pericliev. 1998. Concise,
intelligible, and approximate profiling of numer-
ous classes. Submitted for publication.
</reference>
<page confidence="0.987337">
1040
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.430594">
<title confidence="0.9988535">A Procedure for Multi-Class Discrimination and some Linguistic Applications</title>
<author confidence="0.980571">Vladimir Pericliev</author>
<affiliation confidence="0.699602">of Mathematics Informatics</affiliation>
<address confidence="0.726573">Acad. G. Bonchev Str., hi. 8, 1113 Sofia, Bulgaria</address>
<email confidence="0.990898">periOmath.acad.bg</email>
<abstract confidence="0.998144533333333">The paper describes a novel computational tool for multiple concept learning. Unlike previous approaches, whose major goal is prediction on unseen instances rather than the legibility of the our Parsimonious Discrimination) program emphasizes the conciseness and intelligibility of the resultant class descriptions, using three intuitive simplicity criteria to this end. We illustrate MPD with applications in componential analysis (in lexicology and phonology), language typology, and speech pathology.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>M Halle</author>
<author>R Jakobson</author>
</authors>
<title>Toward the logical description of languages in their phonemic aspects.</title>
<date>1953</date>
<journal>Language</journal>
<pages>29--34</pages>
<marker>Cherry, Halle, Jakobson, 1953</marker>
<rawString>C. Cherry, M. Halle, and R, Jakobson. 1953. Toward the logical description of languages in their phonemic aspects. Language 29:34-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>Unsupervised discovery of phonological categories through supervised learning of morphological rules. COLING96,</title>
<date>1996</date>
<pages>95--100</pages>
<location>Copenhagen,</location>
<marker>Daelemans, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, P. Berck, and S. Gillis. 1996. Unsupervised discovery of phonological categories through supervised learning of morphological rules. COLING96, Copenhagen, pages 95-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bruner</author>
<author>J Goodnow</author>
<author>G Austin</author>
</authors>
<title>A Study of Thinking.</title>
<date>1956</date>
<publisher>John Wiley,</publisher>
<location>New York.</location>
<marker>Bruner, Goodnow, Austin, 1956</marker>
<rawString>J. Bruner, J. Goodnow, and G. Austin. 1956. A Study of Thinking. John Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Greenberg</author>
</authors>
<title>Some universals of grammar with particular reference to the order of meaningful elements.</title>
<date>1966</date>
<booktitle>Universals of Language,</booktitle>
<editor>In J. Greenberg, ed.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="16894" citStr="Greenberg (1966)" startWordPosition="2674" endWordPosition="2675"> of a newer trend in phonology which are not limited to using binary features, and instead exploit multivalued symbolic features as legitimate phonological building blocks. 4 Language typology We have used MPD for discovery of linguistic typologies, where the classes to be contrasted are individual languages or groups of languages (language families). 7We also found errors in analyses performed by linguists, which is understandable for a computationally complex task like this. Table 3: Russian phonemes and their profiles In one application, MPD was run on the dataset from the seminal paper by Greenberg (1966) on word order universals. This corpus has previously been used to uncover linguistic universals, or similarities; we now show its feasibility for the second fundamental typological task of expressing the differences between languages. The data consist of a sample of 30 languages with a wide genetic and areal coverage. The 30 classes to be differentiated are described in terms of 15 features, 4 of which are nominal, and the remaining 11 binary. Running MPD on this dataset showed that from 435 (30-Choose-2) pairwise discriminations to be made, just 12 turned out to be impossible, viz, the pairs</context>
</contexts>
<marker>Greenberg, 1966</marker>
<rawString>J. Greenberg. 1966. Some universals of grammar with particular reference to the order of meaningful elements. In J. Greenberg, ed. Universals of Language, MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hempel</author>
</authors>
<title>Aspects of Scientific Explanation.</title>
<date>1965</date>
<publisher>The Free Press,</publisher>
<location>New York.</location>
<contexts>
<context position="2421" citStr="Hempel, 1965" startWordPosition="344" endWordPosition="345">t a global optimization, rather than heuristic greedy search; (2) produces conjunctive, or nearly conjunctive, profiles for the sake of intelligibility; and (3) gives all alternative solutions. The first goal stems from the familiar Rai11 E. Valdes-Perez Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213, USA valdesOcs.cmu.edu requirement that classes be distinguished by jointly necessary and sufficient descriptions. The second accords with the also familiar thesis that conjunctive descriptions are more comprehensible (they are the norm for typological classification (Hempel, 1965), and they are more readily acquired by experimental subjects than disjunctive ones (Bruner et. al., 1956)), and the third expresses the usefulness, for a diversity of reasons, of having all alternatives. Linguists would generally subscribe to all three requirements, hence the need for a computational tool with such focus.&apos; In this paper, we briefly describe the MPD system (details may be found in Valdes-Perez and Pericliev, 1997; submitted) and focus on some linguistic applications, including componential analysis of kinship terms, distinctive feature analysis in phonology, language typology,</context>
</contexts>
<marker>Hempel, 1965</marker>
<rawString>C. Hempel. 1965. Aspects of Scientific Explanation. The Free Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langley</author>
<author>H Simon</author>
<author>G Bradshaw</author>
<author>J Zytkow</author>
</authors>
<date>1987</date>
<booktitle>Scientific Discovery: Computational Explorations of the Creative Process. The</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>Langley, Simon, Bradshaw, Zytkow, 1987</marker>
<rawString>P. Langley, H. Simon, G. Bradshaw, and J, Zytkow. 1987. Scientific Discovery: Computational Explorations of the Creative Process. The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Learning dependencies between case frame slots. COLING96,</title>
<date>1996</date>
<pages>10--15</pages>
<location>Copenhagen,</location>
<contexts>
<context position="29261" citStr="Li and Abe, 1996" startWordPosition="4642" endWordPosition="4645">000 unseen instances at 73 per cent, using five features, which compares well with a seven features classifier reported in the literature, as well as with other citations in the repository entry. 1039 sacrificed in order to get shorter profiles).13 (2) Conjunctiveness is of less importance here than in tasks of type (i)-(ii), but a better legibility of profiles is in any case preferable. The derived features mechanism can be essential in achieving intuitive contrasts, as in verbal case frame learning, where the interaction between features nicely fits the task of learning &amp;quot;slot dependencies&amp;quot; (Li and Abe, 1996). (3) All alternative profiles of equal simplicity are not always a necessity as in tasks of type (i)-(ii), but are most essential in many tasks where there are different costs of finding the feature values of unseen instances (e.g., computing a syntactic feature, generally, would be much less expensive than computing say a pragmatic one). The important point to emphasize here is that MPD generally leaves these mechanisms as program parameters to be set by the user, and thus, by changing its inductive bias, it may be tailored to the specific needs that arise within the 4 types of tasks. 7 Conc</context>
</contexts>
<marker>Li, Abe, 1996</marker>
<rawString>Hang Li and Naoki Abe. 1996. Learning dependencies between case frame slots. COLING96, Copenhagen, pages 10-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum,</title>
<date>1995</date>
<location>N.J.</location>
<contexts>
<context position="18937" citStr="MacWhinney, 1995" startWordPosition="3000" endWordPosition="3001">ult treatment of missing values as making no contribution to the contrast of language pairs. Following the other alternative path, and allowing &apos;missing&apos; as a distinct value, will result in the successful discrimination of most language pairs. Greek and Serbian would remain indiscriminable, which is no surprise given their areal and genetic affinity. 5 Speech production in aphasics This application concerns the discrimination of different forms of aphasia on the basis of their language behaviour.8 We addressed the profiling of aphasic patients, using the CAP dataset from the CHILDES database (MacWhinney, 1995), containing (among others) 22 English subjects; 5 are control and the others suffer from anomia (3 patients), Broca&apos;s disorder (6), Wernicke&apos;s disorder (5), and nonfluents (3). The patients are grouped into classes according to their fit to a prototype used by neurologists and speech pathologists. The patients&apos; records—verbal responses to pictorial stimuli—are transcribed in the CHILDES database and are coded with linguistic errors from an available set that pertains to phonology, morphology, syntax and semantics. As a first step in our study, we attempted to profile the classes using just th</context>
</contexts>
<marker>MacWhinney, 1995</marker>
<rawString>B. MacWhinney. 1995. The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nida</author>
</authors>
<title>Semantic components in translation theory.</title>
<date>1971</date>
<booktitle>Applications of Linguistics,</booktitle>
<pages>341--348</pages>
<editor>In G. Perren and J. Trim (eds.)</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="11699" citStr="Nida, 1971" startWordPosition="1871" endWordPosition="1872"> value (indeed, referring to Table 1, the first instance of Class 2 has NP lxNP2=[hum hum] and the second instance NP1xNP2=[beast beast]). The resulting profiles in Table 2 is the simplest in the sense that there are no more concise overall feature sets that discriminate the classes, and the profiles-using only features from the overall feature set-are the shortest. 3 Componential analysis 3.1 In lexicology One of the tasks we addressed with MPD is semantic componential analysis, which has well-known linguistic implications, e.g., for (machine) translation (for a familiar early reference, cf. Nida, 1971). More specifically, we were concerned with the componential analysis of kinship terminologies, a common area of study within this trend. KINSHIP is a specialized computer program, having as input the kinterms (=classes) of a language, and their attendant kintypes (=instances).6 It computes the feature values of the kintypes, and then feeds the result to the MPD component to make the discrimination between the kinterms of the language. Currently, KINSHIP uses about 30 features, of all types: binary (e.g., male={+/-}), nominal (e.g., lineal={1ineal, co-lineal, ablineal}), and numeric (e.g., gen</context>
</contexts>
<marker>Nida, 1971</marker>
<rawString>E. Nida. 1971. Semantic components in translation theory. In G. Perren and J. Trim (eds.) Applications of Linguistics, pages 341-348. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Pericliev</author>
<author>R E Valdes-Perez</author>
</authors>
<title>A discovery system for componential analysis of kinship terminologies.</title>
<date>1997</date>
<booktitle>16th International Congress of Linguists,</booktitle>
<editor>In B. Caron (ed.)</editor>
<publisher>Elsevier.</publisher>
<location>Paris,</location>
<contexts>
<context position="14828" citStr="Pericliev and Valdes-Perez, 1997" startWordPosition="2339" endWordPosition="2342">v (beast beast]) NP1=beast PP NP1=hum PP 66.6% NP1=phys-obj PP 1036 system .7 Most importantly, MPD has shown that the huge number of potential componential (=discrimination) models—a menace to the very foundations of the approach, which has made some linguists propose alternative analytic tools— are in fact reduced to (nearly) unique analyses by our 3 simplicity criteria. Our 3rd criterion, ensuring the coordination between equally simple alternative profiles, and with no precedence in the linguistic literature, proved essential in the pruning of solutions (details of KINSHIP are reported in Pericliev and Valdes-Perez, 1997; Pericliev and Valdes-Perez, forthcoming). 3.2 In phonology Componential analysis in phonology amounts to finding the distinctive features of a phonemic system, differentiating any phoneme from all the rest. The adequacy requirements are the same as in the above subsection, and indeed they have been borrowed in lexicology (and morphology for that matter) from phonological work which chronologically preceded the former. We applied MPD to the Russian phonemic system, the data coming from a paper by Cherry et. al., 1953, who also explicitly state as one of their goals the finding of minimal phon</context>
</contexts>
<marker>Pericliev, Valdes-Perez, 1997</marker>
<rawString>V. Pericliev and R. E. Valdes-Perez. 1997. A discovery system for componential analysis of kinship terminologies. In B. Caron (ed.) 16th International Congress of Linguists, Paris, July 1997, Elsevier.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Automatic componential analysis of kinship semantics with a proposed structural solution to the problem of multiple models. Anthropological Linguistics.</title>
<marker>forthcoming, </marker>
<rawString>V. Pericliev and R. E. Valdes-Perez. forthcoming. Automatic componential analysis of kinship semantics with a proposed structural solution to the problem of multiple models. Anthropological Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--81</pages>
<marker>Quinlan, 1986</marker>
<rawString>J. R. Quinlan. 1986. Induction of decision trees. Machine Learning, 1:81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="1229" citStr="Quinlan, 1993" startWordPosition="175" endWordPosition="176">trate MPD with applications in componential analysis (in lexicology and phonology), language typology, and speech pathology. 1 Introduction A common task of knowledge discovery is multiple concept learning, in which from multiple given classes (i.e. a typology) the profiles of these classes are inferred, such that every class is contrasted from every other class by feature values. Ideally, good profiles, besides making good predictions on future instances, should be concise, intelligible, and comprehensive (i.e. yielding all alternatives). Previous approaches like ID3 (Quinlan, 1983) or C4.5 (Quinlan, 1993), which use variations on greedy search, i.e. localized best-next-step search (typically based on information-gain heuristics), have as their major goal prediction on unseen instances, and therefore do not have as an explicit concern the conciseness, intelligibility, and comprehensiveness of the output. In contrast to virtually all previous approaches to multi-class discrimination, the MPD (Maximally Parsimonious Discrimination) program we describe here aims at the legibility of the resultant class profiles. To do so, it (1) uses a minimal number of features by carrying out a global optimizati</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tanaka</author>
</authors>
<title>Decision tree learning algorithm with structured attributes: Application to verbal case frame acquisition. COLING96,</title>
<date>1996</date>
<pages>943--948</pages>
<location>Copenhagen,</location>
<contexts>
<context position="3213" citStr="Tanaka, 1996" startWordPosition="465" endWordPosition="466">having all alternatives. Linguists would generally subscribe to all three requirements, hence the need for a computational tool with such focus.&apos; In this paper, we briefly describe the MPD system (details may be found in Valdes-Perez and Pericliev, 1997; submitted) and focus on some linguistic applications, including componential analysis of kinship terms, distinctive feature analysis in phonology, language typology, and discrimination of aphasic syndromes from coded texts in the CHILDES database. For further interesting application areas of similar algorithms, cf. Daelemans et. al., 1996 and Tanaka, 1996. 2 Overview of the MPD program The Maximally Parsimonious Discrimination program (MPD) is a general computational tool for inferring, given multiple classes (or, a typology), with attendant instances of these classes, the profiles (=descriptions) of these classes such that every class is contrasted from all remaining classes on the basis of feature values. Below is a brief description of the program. 2.1 Expressing contrasts The MPD program uses Boolean, nominal and numeric features to express contrasts, as follows: &apos;The profiling of multiple types, in actual fact, is a generic task of knowle</context>
</contexts>
<marker>Tanaka, 1996</marker>
<rawString>H.Tanaka. 1996. Decision tree learning algorithm with structured attributes: Application to verbal case frame acquisition. COLING96, Copenhagen, pages 943-948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Valdes-Perez</author>
<author>V Pericliev</author>
</authors>
<title>Maximally parsimonious discrimination: a task from linguistic discovery. A A AI97,</title>
<date>1997</date>
<pages>515--520</pages>
<location>Providence, RI,</location>
<contexts>
<context position="2854" citStr="Valdes-Perez and Pericliev, 1997" startWordPosition="412" endWordPosition="415">ry and sufficient descriptions. The second accords with the also familiar thesis that conjunctive descriptions are more comprehensible (they are the norm for typological classification (Hempel, 1965), and they are more readily acquired by experimental subjects than disjunctive ones (Bruner et. al., 1956)), and the third expresses the usefulness, for a diversity of reasons, of having all alternatives. Linguists would generally subscribe to all three requirements, hence the need for a computational tool with such focus.&apos; In this paper, we briefly describe the MPD system (details may be found in Valdes-Perez and Pericliev, 1997; submitted) and focus on some linguistic applications, including componential analysis of kinship terms, distinctive feature analysis in phonology, language typology, and discrimination of aphasic syndromes from coded texts in the CHILDES database. For further interesting application areas of similar algorithms, cf. Daelemans et. al., 1996 and Tanaka, 1996. 2 Overview of the MPD program The Maximally Parsimonious Discrimination program (MPD) is a general computational tool for inferring, given multiple classes (or, a typology), with attendant instances of these classes, the profiles (=descrip</context>
</contexts>
<marker>Valdes-Perez, Pericliev, 1997</marker>
<rawString>R. E. Valdes-Perez and V. Pericliev. 1997. Maximally parsimonious discrimination: a task from linguistic discovery. A A AI97, Providence, RI, pages 515-520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Valdes-Perez</author>
<author>V Pericliev</author>
</authors>
<title>Concise, intelligible, and approximate profiling of numerous classes.</title>
<date>1998</date>
<note>Submitted for publication.</note>
<marker>Valdes-Perez, Pericliev, 1998</marker>
<rawString>R. E. Valdes-Perez and V. Pericliev. 1998. Concise, intelligible, and approximate profiling of numerous classes. Submitted for publication.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>