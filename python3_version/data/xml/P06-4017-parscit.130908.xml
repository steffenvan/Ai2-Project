<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017495">
<title confidence="0.994324">
An Implemented Description of Japanese:
The Lexeed Dictionary and the Hinoki Treebank
</title>
<note confidence="0.582636">
Sanae Fujita, Takaaki Tanaka, Francis Bond, Hiromi Nakaiwa
NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
</note>
<email confidence="0.928428">
{sanae, takaaki, bond, nakaiwa}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.993893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9886745">
In this paper we describe the current state
of a new Japanese lexical resource: the
Hinoki treebank. The treebank is built
from dictionary definition sentences, and
uses an HPSG based Japanese grammar to
encode both syntactic and semantic infor-
mation. It is combined with an ontology
based on the definition sentences to give a
detailed sense level description of the most
familiar 28,000 words of Japanese.
</bodyText>
<sectionHeader confidence="0.998847" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998269205128205">
In this paper we describe the current state of a
new lexical resource: the Hinoki treebank. The
ultimate goal of our research is natural language
understanding — we aim to create a system that
can parse text into some useful semantic represen-
tation. This is an ambitious goal, and this pre-
sentation does not present a complete solution,
but rather a road-map to the solution, with some
progress along the way.
The first phase of the project, which we present
here, is to construct a syntactically and semanti-
cally annotated corpus based on the machine read-
able dictionary Lexeed (Kasahara et al., 2004).
This is a hand built self-contained lexicon: it con-
sists of headwords and their definitions for the
most familiar 28,000 words of Japanese. Each
definition and example sentence has been parsed,
and the most appropriate analysis selected. Each
content word in the sentences has been marked
with the appropriate Lexeed sense. The syntac-
tic model is embodied in a grammar, while the se-
mantic model is linked by an ontology. This makes
it possible to test the use of similarity and/or se-
mantic class based back-offs for parsing and gen-
eration with both symbolic grammars and statisti-
cal models.
In order to make the system self sustaining we
base the first growth of our treebank on the dic-
tionary definition sentences themselves. We then
train a statistical model on the treebank and parse
the entire lexicon. From this we induce a the-
saurus. We are currently tagging other genres with
the same information. We will then use this infor-
mation and the thesaurus to build a parsing model
that combines syntactic and semantic information.
We will also produce a richer ontology — for ex-
ample extracting selectional preferences. In the
last phase, we will look at ways of extending our
lexicon and ontology to less familiar words.
</bodyText>
<sectionHeader confidence="0.8404475" genericHeader="method">
2 The Lexeed Semantic Database of
Japanese
</sectionHeader>
<bodyText confidence="0.9985214375">
The Lexeed Semantic Database of Japanese con-
sists of all Japanese words with a familiarity
greater than or equal to five on a seven point
scale (Kasahara et al., 2004). This gives 28,000
words in all, with 46,000 different senses. Defini-
tion sentences for these sentences were rewritten
to use only the 28,000 familiar words (and some
function words). The defining vocabulary is ac-
tually 16,900 different words (60% of all possi-
ble words). A simplified example entry for the
last two senses of the word F ラ イバ ー doraib¯a
“driver” is given in Figure 1, with English glosses
added, but omitting the example sentences. Lex-
eed itself consists of just the definitions, familiar-
ity and part of speech, all the underlined features
are those added by the Hinoki project.
</bodyText>
<sectionHeader confidence="0.991267" genericHeader="method">
3 The Hinoki Treebank
</sectionHeader>
<bodyText confidence="0.998194142857143">
The structure of our treebank is inspired by the
Redwoods treebank of English (Oepen et al.,
2002) in which utterances are parsed and the anno-
tator selects the best parse from the full analyses
derived by the grammar. We had four main rea-
sons for selecting this approach. The first was that
we wanted to develop a precise broad-coverage
</bodyText>
<page confidence="0.997612">
65
</page>
<bodyText confidence="0.6546635">
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 65–68,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<equation confidence="0.943631153846154">
�
� � � � � � � � � � � � � � � � � � �
� � � �
� �
� �
� �
� �
� �
� �
� �
� �
� �
�INDEX ド ラ イバ ー doraib¯a
</equation>
<figure confidence="0.953277">
POS noun Lexical-Type noun-lex
Entropy 0.79
FAMILIARITY 6.5 [1–7] (&gt; 5) Frequency 37
SENSE 1 ...
� �
DEFINITION 自動車1/を/運転1/す る /人1/。
SENSE 2 �
Someone who drives a car. �
�
P(S2) = 0.84 HYPERNYM 人1 hito “person” �
�
�SEM. CLASS (292:chauffeur/driver) (C (5:person)) �
WORDNET driver1
�DEFINITION ゴ ルフ 1/で/ 、 /遠 距離1/用/の /クラ ブ 3/。 一番/ウ ッ ド /。
SENSE 3 In golf, a long-distance club. A number one wood.
P(S2) = 0.05 HYPERNYM ク ラ ブ 3 kurabu “club”
SEM. CLASS (921:leisure equipment) (C 921)
� �WORDNET drivers
DOMAIN ゴ ルフ 1 gorufu “golf”
</figure>
<figureCaption confidence="0.99996">
Figure 1: Entry for the Word doraib¯a “driver” (with English glosses)
</figureCaption>
<bodyText confidence="0.999505459459459">
grammar in tandem with the treebank, as part of
our research into natural language understanding.
Treebanking the output of the parser allows us
to immediately identify problems in the grammar,
and improving the grammar directly improves the
quality of the treebank in a mutually beneficial
feedback loop.
The second reason is that we wanted to annotate
to a high level of detail, marking not only depen-
dency and constituent structure but also detailed
semantic relations. By using a Japanese gram-
mar (JACY: Siegel (2000)) based on a monostratal
theory of grammar (Head Driven Phrase Structure
Grammar) we could simultaneously annotate syn-
tactic and semantic structure without overburden-
ing the annotator. The treebank records the com-
plete syntacto-semantic analysis provided by the
HPSG grammar, along with an annotator’s choice
of the most appropriate parse. From this record,
all kinds of information can be extracted at various
levels of granularity: A simplified example of the
labeled tree, minimal recursion semantics repre-
sentation (MRS) and semantic dependency views
for the definition of F ラ イバ ー 2 doraib¯a “driver”
is given in Figure 2.
The third reason was that use of the grammar as
a base enforces consistency — all sentences anno-
tated are guaranteed to have well-formed parses.
The last reason was the availability of a reason-
ably robust existing HPSG of Japanese (JACY),
and a wide range of open source tools for de-
veloping the grammars. We made extensive use
of tools from the the Deep Linguistic Process-
ing with HPSG Initiative (DELPH-IN: http://
www.delph-in.net/) These existing resources
enabled us to rapidly develop and test our ap-
proach.
</bodyText>
<subsectionHeader confidence="0.996085">
3.1 Syntactic Annotation
</subsectionHeader>
<bodyText confidence="0.999973206896552">
The construction of the treebank is a two stage
process. First, the corpus is parsed (in our case
using JACY), and then the annotator selects the
correct analysis (or occasionally rejects all anal-
yses). Selection is done through a choice of dis-
criminants. The system selects features that distin-
guish between different parses, and the annotator
selects or rejects the features until only one parse
is left. The number of decisions for each sentence
is proportional to log2 in the length of the sentence
(Tanaka et al., 2005). Because the disambiguat-
ing choices made by the annotators are saved, it
is possible to semi-automatically update the tree-
bank when the grammar changes. Re-annotation
is only necessary in cases where the parse has be-
come more ambiguous or, more rarely, existing
rules or lexical items have changed so much that
the system cannot reconstruct the parse.
The Lexeed definition sentences were already
POS tagged. We experimented with using the POS
tags to mark trees as good or bad (Tanaka et al.,
2005). This enabled us to reduce the number of
annotator decisions by 20%.
One concern with Redwoods style treebanking
is that it is only possible to annotate those trees
that the grammar can parse. Sentences for which
no analysis had been implemented in the grammar
or which fail to parse due to processing constraints
are left unannotated. This makes grammar cov-
</bodyText>
<page confidence="0.917769">
66
</page>
<table confidence="0.915139">
UTTERANCE
NP
VP N
PP V
N CASE-P V V
自動車 を 運転 す る 人
jid¯osha o unten suru hito
car ACC drive do person
</table>
<figure confidence="0.973721">
Parse Tree
(h0,x1{h0 :proposition m(h1)
h1 :hito n(x1) “person”
h2:udef q(x1,h1,h6)
h3 :jidosha n(x2) “car”
h4:udef q(x2,h3,h7)
h5 :unten s(e1,x1,x2)})“drive”
MRS
{x1 :
e1 :unten s(ARG1 x1 : hito n,ARG2 x2 : jidosha n)
r1 :proposition m(MARG e1 : unten s)}
Semantic Dependency
</figure>
<figureCaption confidence="0.999629">
Figure 2: Parse Tree, Simplified MRS and Dependency Views for F ラ イバ ー 2 doraib¯a “driver”
</figureCaption>
<bodyText confidence="0.999844545454545">
erage a significant issue. We extended JACY by
adding the defining vocabulary, and added some
new rules and lexical-types (more detail is given
in Bond et al. (2004)). None of the rules are spe-
cific to the dictionary domain. The grammatical
coverage over all sentences is now 86%. Around
12% of the parsed sentences were rejected by the
treebankers due to an incomplete semantic repre-
sentation. The total size of the treebank is cur-
rently 53,600 definition sentences and 36,000 ex-
ample sentences: 89,600 sentences in total.
</bodyText>
<subsectionHeader confidence="0.999165">
3.2 Sense Annotation
</subsectionHeader>
<bodyText confidence="0.999968">
All open class words were annotated with their
sense by five annotators. Inter-annotator agree-
ment ranges from 0.79 to 0.83. For example, the
word tlラ ブ kurabu “club” is tagged as sense 3 in
the definition sentence for driver3, with the mean-
ing “golf-club”. For each sense, we calculate the
entropy and per sense probabilities over four cor-
pora: the Lexeed definition and example sentences
and Newspaper text from the Kyoto University and
Senseval 2 corpora (Tanaka et al., 2006).
</bodyText>
<sectionHeader confidence="0.999599" genericHeader="method">
4 Applications
</sectionHeader>
<subsectionHeader confidence="0.998404">
4.1 Stochastic Parse Ranking
</subsectionHeader>
<bodyText confidence="0.999832545454545">
Using the treebanked data, we built a stochastic
parse ranking model. The ranker uses a maximum
entropy learner to train a PCFG over the parse
derivation trees, with the current node, two grand-
parents and several other conditioning features. A
preliminary experiment showed the correct parse
is ranked first 69% of the time (10-fold cross val-
idation on 13,000 sentences; evaluated per sen-
tence). We are now experimenting with extensions
based on constituent weight, hypernym, semantic
class and selectional preferences.
</bodyText>
<subsectionHeader confidence="0.947772">
4.2 Ontology Acquisition
</subsectionHeader>
<bodyText confidence="0.999843657142857">
To extract hypernyms, we parse the first defini-
tion sentence for each sense (Nichols et al., 2005).
The parser uses the stochastic parse ranking model
learned from the Hinoki treebank, and returns the
semantic representation (MRS) of the first ranked
parse. In cases where JACY fails to return a parse,
we use a dependency parser instead. The highest
scoping real predicate is generally the hypernym.
For example, for doraib¯a2 the hypernym is 人 hito
“person” and for doraib¯a3 the hypernym is tlラ
ブ kurabu “club” (see Figure 1). We also extract
other relationships, such as synonym and domain.
Because the words are sense tags, we can special-
ize the relations to relations between senses, rather
than just words: (hypernym: doraib¯a3, kurabu3).
Once we have synonym/hypernym relations, we
can link the lexicon to other lexical resources. For
example, for the manually constructed Japanese
ontology Goi-Taikei (Ikehara et al., 1997) we link
to its semantic classes by the following heuristic:
look up the semantic classes C for both the head-
word (wi) and hypernym(s) (wg). If at least one of
the index word’s classes is subsumed by at least
one of the genus’ classes, then we consider the re-
lationship confirmed. To link cross-linguistically,
we look up the headwords and hypernym(s) in a
translation lexicon and compare the set of trans-
lations ci C C(T(wi)) with WordNet (Fellbaum,
1998)). Although looking up the translation adds
noise, the additional filter of the relationship triple
effectively filters it out again.
Adding the ontology to the dictionary interface
makes a far more flexible resource. For example,
by clicking on the (hypernym: doraib¯a3, gorufu1)
link, it is possible to see a list of all the senses re-
</bodyText>
<page confidence="0.998777">
67
</page>
<bodyText confidence="0.966558">
lated to golf, a link that is inaccessible in the paper
dictionary.
</bodyText>
<subsectionHeader confidence="0.989956">
4.3 Semi-Automatic Grammar
</subsectionHeader>
<bodyText confidence="0.992023">
Documentation
A detailed grammar is a fundamental component
for precise natural language processing. It pro-
vides not only detailed syntactic and morphologi-
cal information on linguistic expressions but also
precise and usually language-independent seman-
tic structures of them. To simplify grammar de-
velopment, we take a snapshot of the grammar
used to treebank in each development cycle. From
this we extract information about lexical items
and their types from both the grammar and tree-
bank and convert it into an electronically accesi-
ble structured database (the lexical-type database:
Hashimoto et al., 2005). This allows grammar de-
velopers and treebankers to see comprehensive up-
to-date information about lexical types, including
documentation, syntactic properties (super types,
valence, category and so on), usage examples from
the treebank and links to other dictionaries.
</bodyText>
<sectionHeader confidence="0.99972" genericHeader="method">
5 Further Work
</sectionHeader>
<bodyText confidence="0.999538590909091">
We are currently concentrating on three tasks. The
first is improving the coverage of the grammar,
so that we can parse more sentences to a cor-
rect parse. The second is improving the knowl-
edge acquisition, in particular learning other in-
formation from the parsed defining sentences —
such as lexical-types, semantic association scores,
meronyms, and antonyms. The third task is adding
the knowledge of hypernyms into the stochastic
model.
The Hinoki project is being extended in several
ways. For Japanese, we are treebanking other gen-
res, starting with Newspaper text, and increasing
the vocabulary, initially by parsing other machine
readable dictionaries. We are also extending the
approach multilingually with other grammars in
the DELPH-IN group. We have started with the
English Resource Grammar and the Gnu Contem-
porary International Dictionary of English and are
investigating Korean and Norwegian through co-
operation with the Korean Research Grammar and
NorSource.
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99579625">
In this paper we have described the current state of
the Hinoki treebank. We have further showed how
it is being used to develop a language-independent
system for acquiring thesauruses from machine-
readable dictionaries.
With the improved the grammar and ontology,
we will use the knowledge learned to extend our
model to words not in Lexeed, using definition
sentences from machine-readable dictionaries or
where they appear within normal text. In this way,
we can grow an extensible lexicon and thesaurus
from Lexeed.
</bodyText>
<sectionHeader confidence="0.99469" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999808333333333">
We thank the treebankers, Takayuki Kurib-
ayashi, Tomoko Hirata and Koji Yamashita, for
their hard work and attention to detail.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987752173913">
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki
treebank: A treebank for text understanding. In Proceed-
ings of the First International Joint Conference on Natural
Language Processing (IJCNLP-04). Springer Verlag. (in
press).
Christine Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Chikara Hashimoto, Francis Bond, Takaaki Tanaka, and
Melanie Siegel. 2005. Integration of a lexical type
database with a linguistically interpreted corpus. In 6th
International Workshop on Linguistically Integrated Cor-
pora (LINC-2005), pages 31–40. Cheju, Korea.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei —
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lex-
icon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictio-
naries. In Proceedings of the International Joint Confer-
ence on Artificial Intelligence IJCAI-2005, pages 1111–
1116. Edinburgh.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christoper D. Manning, Dan Flickinger, and Thorsten
Brant. 2002. The LinGO redwoods treebank: Motivation
and preliminary applications. In 19th International Con-
ference on Computational Linguistics: COLING-2002,
pages 1253–7. Taipei, Taiwan.
Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolf-
gang Wahlster, editor, Verbmobil: Foundations of Speech-
to-Speech Translation, pages 265–280. Springer, Berlin,
Germany.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The
Hinoki sensebank — a large-scale word sense tagged cor-
pus of Japanese —. In Frontiers in Linguistically Anno-
tated Corpora 2006. Sydney. (ACL Workshop).
Takaaki Tanaka, Francis Bond, Stephan Oepen, and Sanae
Fujita. 2005. High precision treebanking – blazing useful
trees using POS information. In ACL-2005, pages 330–
337.
</reference>
<page confidence="0.999446">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.356331">
<title confidence="0.988133">An Implemented Description of Japanese: The Lexeed Dictionary and the Hinoki Treebank</title>
<author confidence="0.990546">Sanae Fujita</author>
<author confidence="0.990546">Takaaki Tanaka</author>
<author confidence="0.990546">Francis Bond</author>
<author confidence="0.990546">Hiromi Nakaiwa</author>
<affiliation confidence="0.969873">NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation</affiliation>
<address confidence="0.404037">takaaki, bond,</address>
<abstract confidence="0.993190454545454">In this paper we describe the current state of a new Japanese lexical resource: the Hinoki treebank. The treebank is built from dictionary definition sentences, and uses an HPSG based Japanese grammar to encode both syntactic and semantic information. It is combined with an ontology based on the definition sentences to give a detailed sense level description of the most familiar 28,000 words of Japanese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Sanae Fujita</author>
</authors>
<title>Chikara Hashimoto, Kaname Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani, Takaaki Tanaka, and Shigeaki Amano.</title>
<date>2004</date>
<booktitle>In Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP-04).</booktitle>
<publisher>Springer Verlag. (in press).</publisher>
<marker>Bond, Fujita, 2004</marker>
<rawString>Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani, Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki treebank: A treebank for text understanding. In Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP-04). Springer Verlag. (in press).</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christine Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christine Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Francis Bond</author>
<author>Takaaki Tanaka</author>
<author>Melanie Siegel</author>
</authors>
<title>Integration of a lexical type database with a linguistically interpreted corpus.</title>
<date>2005</date>
<booktitle>In 6th International Workshop on Linguistically Integrated Corpora (LINC-2005),</booktitle>
<pages>31--40</pages>
<location>Cheju,</location>
<contexts>
<context position="12162" citStr="Hashimoto et al., 2005" startWordPosition="2022" endWordPosition="2025">atic Grammar Documentation A detailed grammar is a fundamental component for precise natural language processing. It provides not only detailed syntactic and morphological information on linguistic expressions but also precise and usually language-independent semantic structures of them. To simplify grammar development, we take a snapshot of the grammar used to treebank in each development cycle. From this we extract information about lexical items and their types from both the grammar and treebank and convert it into an electronically accesible structured database (the lexical-type database: Hashimoto et al., 2005). This allows grammar developers and treebankers to see comprehensive upto-date information about lexical types, including documentation, syntactic properties (super types, valence, category and so on), usage examples from the treebank and links to other dictionaries. 5 Further Work We are currently concentrating on three tasks. The first is improving the coverage of the grammar, so that we can parse more sentences to a correct parse. The second is improving the knowledge acquisition, in particular learning other information from the parsed defining sentences — such as lexical-types, semantic </context>
</contexts>
<marker>Hashimoto, Bond, Tanaka, Siegel, 2005</marker>
<rawString>Chikara Hashimoto, Francis Bond, Takaaki Tanaka, and Melanie Siegel. 2005. Integration of a lexical type database with a linguistically interpreted corpus. In 6th International Workshop on Linguistically Integrated Corpora (LINC-2005), pages 31–40. Cheju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<booktitle>Goi-Taikei — A Japanese Lexicon. Iwanami Shoten,</booktitle>
<volume>5</volume>
<pages>volumes/CDROM.</pages>
<location>Tokyo.</location>
<contexts>
<context position="10662" citStr="Ikehara et al., 1997" startWordPosition="1781" endWordPosition="1784"> dependency parser instead. The highest scoping real predicate is generally the hypernym. For example, for doraib¯a2 the hypernym is 人 hito “person” and for doraib¯a3 the hypernym is tlラ ブ kurabu “club” (see Figure 1). We also extract other relationships, such as synonym and domain. Because the words are sense tags, we can specialize the relations to relations between senses, rather than just words: (hypernym: doraib¯a3, kurabu3). Once we have synonym/hypernym relations, we can link the lexicon to other lexical resources. For example, for the manually constructed Japanese ontology Goi-Taikei (Ikehara et al., 1997) we link to its semantic classes by the following heuristic: look up the semantic classes C for both the headword (wi) and hypernym(s) (wg). If at least one of the index word’s classes is subsumed by at least one of the genus’ classes, then we consider the relationship confirmed. To link cross-linguistically, we look up the headwords and hypernym(s) in a translation lexicon and compare the set of translations ci C C(T(wi)) with WordNet (Fellbaum, 1998)). Although looking up the translation adds noise, the additional filter of the relationship triple effectively filters it out again. Adding the</context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei — A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 volumes/CDROM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaname Kasahara</author>
<author>Hiroshi Sato</author>
<author>Francis Bond</author>
</authors>
<title>Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki Amano.</title>
<date>2004</date>
<booktitle>Construction of a Japanese semantic lexicon: Lexeed. SIG NLC-159, IPSJ,</booktitle>
<location>Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="1315" citStr="Kasahara et al., 2004" startWordPosition="201" endWordPosition="204">ese. 1 Introduction In this paper we describe the current state of a new lexical resource: the Hinoki treebank. The ultimate goal of our research is natural language understanding — we aim to create a system that can parse text into some useful semantic representation. This is an ambitious goal, and this presentation does not present a complete solution, but rather a road-map to the solution, with some progress along the way. The first phase of the project, which we present here, is to construct a syntactically and semantically annotated corpus based on the machine readable dictionary Lexeed (Kasahara et al., 2004). This is a hand built self-contained lexicon: it consists of headwords and their definitions for the most familiar 28,000 words of Japanese. Each definition and example sentence has been parsed, and the most appropriate analysis selected. Each content word in the sentences has been marked with the appropriate Lexeed sense. The syntactic model is embodied in a grammar, while the semantic model is linked by an ontology. This makes it possible to test the use of similarity and/or semantic class based back-offs for parsing and generation with both symbolic grammars and statistical models. In orde</context>
<context position="2750" citStr="Kasahara et al., 2004" startWordPosition="445" endWordPosition="448"> this we induce a thesaurus. We are currently tagging other genres with the same information. We will then use this information and the thesaurus to build a parsing model that combines syntactic and semantic information. We will also produce a richer ontology — for example extracting selectional preferences. In the last phase, we will look at ways of extending our lexicon and ontology to less familiar words. 2 The Lexeed Semantic Database of Japanese The Lexeed Semantic Database of Japanese consists of all Japanese words with a familiarity greater than or equal to five on a seven point scale (Kasahara et al., 2004). This gives 28,000 words in all, with 46,000 different senses. Definition sentences for these sentences were rewritten to use only the 28,000 familiar words (and some function words). The defining vocabulary is actually 16,900 different words (60% of all possible words). A simplified example entry for the last two senses of the word F ラ イバ ー doraib¯a “driver” is given in Figure 1, with English glosses added, but omitting the example sentences. Lexeed itself consists of just the definitions, familiarity and part of speech, all the underlined features are those added by the Hinoki project. 3 Th</context>
</contexts>
<marker>Kasahara, Sato, Bond, 2004</marker>
<rawString>Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki Amano. 2004. Construction of a Japanese semantic lexicon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nichols</author>
<author>Francis Bond</author>
<author>Daniel Flickinger</author>
</authors>
<title>Robust ontology acquisition from machine-readable dictionaries.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence IJCAI-2005,</booktitle>
<pages>1111--1116</pages>
<location>Edinburgh.</location>
<contexts>
<context position="9828" citStr="Nichols et al., 2005" startWordPosition="1650" endWordPosition="1653">ata, we built a stochastic parse ranking model. The ranker uses a maximum entropy learner to train a PCFG over the parse derivation trees, with the current node, two grandparents and several other conditioning features. A preliminary experiment showed the correct parse is ranked first 69% of the time (10-fold cross validation on 13,000 sentences; evaluated per sentence). We are now experimenting with extensions based on constituent weight, hypernym, semantic class and selectional preferences. 4.2 Ontology Acquisition To extract hypernyms, we parse the first definition sentence for each sense (Nichols et al., 2005). The parser uses the stochastic parse ranking model learned from the Hinoki treebank, and returns the semantic representation (MRS) of the first ranked parse. In cases where JACY fails to return a parse, we use a dependency parser instead. The highest scoping real predicate is generally the hypernym. For example, for doraib¯a2 the hypernym is 人 hito “person” and for doraib¯a3 the hypernym is tlラ ブ kurabu “club” (see Figure 1). We also extract other relationships, such as synonym and domain. Because the words are sense tags, we can specialize the relations to relations between senses, rather t</context>
</contexts>
<marker>Nichols, Bond, Flickinger, 2005</marker>
<rawString>Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Robust ontology acquisition from machine-readable dictionaries. In Proceedings of the International Joint Conference on Artificial Intelligence IJCAI-2005, pages 1111– 1116. Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Christoper D Manning</author>
<author>Dan Flickinger</author>
<author>Thorsten Brant</author>
</authors>
<title>The LinGO redwoods treebank: Motivation and preliminary applications.</title>
<date>2002</date>
<booktitle>In 19th International Conference on Computational Linguistics: COLING-2002,</booktitle>
<pages>1253--7</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3466" citStr="Oepen et al., 2002" startWordPosition="567" endWordPosition="570">ntences were rewritten to use only the 28,000 familiar words (and some function words). The defining vocabulary is actually 16,900 different words (60% of all possible words). A simplified example entry for the last two senses of the word F ラ イバ ー doraib¯a “driver” is given in Figure 1, with English glosses added, but omitting the example sentences. Lexeed itself consists of just the definitions, familiarity and part of speech, all the underlined features are those added by the Hinoki project. 3 The Hinoki Treebank The structure of our treebank is inspired by the Redwoods treebank of English (Oepen et al., 2002) in which utterances are parsed and the annotator selects the best parse from the full analyses derived by the grammar. We had four main reasons for selecting this approach. The first was that we wanted to develop a precise broad-coverage 65 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 65–68, Sydney, July 2006. c�2006 Association for Computational Linguistics � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � �INDEX ド ラ イバ ー doraib¯a POS noun Lexical-Type noun-lex Entropy 0.79 FAMILIARITY 6.5 [1–7] (&gt; 5) Frequency 37 SENSE 1 ... �</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brant, 2002</marker>
<rawString>Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christoper D. Manning, Dan Flickinger, and Thorsten Brant. 2002. The LinGO redwoods treebank: Motivation and preliminary applications. In 19th International Conference on Computational Linguistics: COLING-2002, pages 1253–7. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
</authors>
<title>HPSG analysis of Japanese.</title>
<date>2000</date>
<booktitle>Verbmobil: Foundations of Speechto-Speech Translation,</booktitle>
<pages>265--280</pages>
<editor>In Wolfgang Wahlster, editor,</editor>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="5092" citStr="Siegel (2000)" startWordPosition="872" endWordPosition="873"> ゴ ルフ 1 gorufu “golf” Figure 1: Entry for the Word doraib¯a “driver” (with English glosses) grammar in tandem with the treebank, as part of our research into natural language understanding. Treebanking the output of the parser allows us to immediately identify problems in the grammar, and improving the grammar directly improves the quality of the treebank in a mutually beneficial feedback loop. The second reason is that we wanted to annotate to a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations. By using a Japanese grammar (JACY: Siegel (2000)) based on a monostratal theory of grammar (Head Driven Phrase Structure Grammar) we could simultaneously annotate syntactic and semantic structure without overburdening the annotator. The treebank records the complete syntacto-semantic analysis provided by the HPSG grammar, along with an annotator’s choice of the most appropriate parse. From this record, all kinds of information can be extracted at various levels of granularity: A simplified example of the labeled tree, minimal recursion semantics representation (MRS) and semantic dependency views for the definition of F ラ イバ ー 2 doraib¯a “dr</context>
</contexts>
<marker>Siegel, 2000</marker>
<rawString>Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolfgang Wahlster, editor, Verbmobil: Foundations of Speechto-Speech Translation, pages 265–280. Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
<author>Francis Bond</author>
<author>Sanae Fujita</author>
</authors>
<title>The Hinoki sensebank — a large-scale word sense tagged corpus of Japanese —.</title>
<date>2006</date>
<booktitle>In Frontiers in Linguistically Annotated Corpora 2006. Sydney. (ACL Workshop).</booktitle>
<contexts>
<context position="9139" citStr="Tanaka et al., 2006" startWordPosition="1544" endWordPosition="1547"> size of the treebank is currently 53,600 definition sentences and 36,000 example sentences: 89,600 sentences in total. 3.2 Sense Annotation All open class words were annotated with their sense by five annotators. Inter-annotator agreement ranges from 0.79 to 0.83. For example, the word tlラ ブ kurabu “club” is tagged as sense 3 in the definition sentence for driver3, with the meaning “golf-club”. For each sense, we calculate the entropy and per sense probabilities over four corpora: the Lexeed definition and example sentences and Newspaper text from the Kyoto University and Senseval 2 corpora (Tanaka et al., 2006). 4 Applications 4.1 Stochastic Parse Ranking Using the treebanked data, we built a stochastic parse ranking model. The ranker uses a maximum entropy learner to train a PCFG over the parse derivation trees, with the current node, two grandparents and several other conditioning features. A preliminary experiment showed the correct parse is ranked first 69% of the time (10-fold cross validation on 13,000 sentences; evaluated per sentence). We are now experimenting with extensions based on constituent weight, hypernym, semantic class and selectional preferences. 4.2 Ontology Acquisition To extrac</context>
</contexts>
<marker>Tanaka, Bond, Fujita, 2006</marker>
<rawString>Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The Hinoki sensebank — a large-scale word sense tagged corpus of Japanese —. In Frontiers in Linguistically Annotated Corpora 2006. Sydney. (ACL Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
<author>Francis Bond</author>
<author>Stephan Oepen</author>
<author>Sanae Fujita</author>
</authors>
<title>High precision treebanking – blazing useful trees using POS information.</title>
<date>2005</date>
<booktitle>In ACL-2005,</booktitle>
<pages>330--337</pages>
<contexts>
<context position="6783" citStr="Tanaka et al., 2005" startWordPosition="1144" endWordPosition="1147">xisting resources enabled us to rapidly develop and test our approach. 3.1 Syntactic Annotation The construction of the treebank is a two stage process. First, the corpus is parsed (in our case using JACY), and then the annotator selects the correct analysis (or occasionally rejects all analyses). Selection is done through a choice of discriminants. The system selects features that distinguish between different parses, and the annotator selects or rejects the features until only one parse is left. The number of decisions for each sentence is proportional to log2 in the length of the sentence (Tanaka et al., 2005). Because the disambiguating choices made by the annotators are saved, it is possible to semi-automatically update the treebank when the grammar changes. Re-annotation is only necessary in cases where the parse has become more ambiguous or, more rarely, existing rules or lexical items have changed so much that the system cannot reconstruct the parse. The Lexeed definition sentences were already POS tagged. We experimented with using the POS tags to mark trees as good or bad (Tanaka et al., 2005). This enabled us to reduce the number of annotator decisions by 20%. One concern with Redwoods styl</context>
</contexts>
<marker>Tanaka, Bond, Oepen, Fujita, 2005</marker>
<rawString>Takaaki Tanaka, Francis Bond, Stephan Oepen, and Sanae Fujita. 2005. High precision treebanking – blazing useful trees using POS information. In ACL-2005, pages 330– 337.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>