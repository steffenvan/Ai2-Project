<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.054458">
<title confidence="0.992894">
Statistical Machine Translation for Query Expansion in Answer Retrieval
</title>
<author confidence="0.971654">
Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal and Yi Liu
</author>
<affiliation confidence="0.836647">
Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA 94043
</affiliation>
<email confidence="0.998331">
{riezler|avasserm|ioannis|vibhu|yliu}@google.com
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949444444445">
We present an approach to query expan-
sion in answer retrieval that uses Statisti-
cal Machine Translation (SMT) techniques
to bridge the lexical gap between ques-
tions and answers. SMT-based query ex-
pansion is done by i) using a full-sentence
paraphraser to introduce synonyms in con-
text of the entire query, and ii) by trans-
lating query terms into answer terms us-
ing a full-sentence SMT model trained on
question-answer pairs. We evaluate these
global, context-aware query expansion tech-
niques on tfidf retrieval from 10 million
question-answer pairs extracted from FAQ
pages. Experimental results show that SMT-
based expansion improves retrieval perfor-
mance over local expansion and over re-
trieval without expansion.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998940644444444">
One of the fundamental problems in Question An-
swering (QA) has been recognized to be the “lexi-
cal chasm” (Berger et al., 2000) between question
strings and answer strings. This problem is mani-
fested in a mismatch between question and answer
vocabularies, and is aggravated by the inherent am-
biguity of natural language. Several approaches have
been presented that apply natural language process-
ing technology to close this gap. For example, syn-
tactic information has been deployed to reformu-
late questions (Hermjakob et al., 2002) or to re-
place questions by syntactically similar ones (Lin
and Pantel, 2001); lexical ontologies such as Word-
net1 have been used to find synonyms for question
words (Burke et al., 1997; Hovy et al., 2000; Prager
et al., 2001; Harabagiu et al., 2001), and statisti-
cal machine translation (SMT) models trained on
question-answer pairs have been used to rank can-
didate answers according to their translation prob-
abilities (Berger et al., 2000; Echihabi and Marcu,
2003; Soricut and Brill, 2006). Information retrieval
(IR) is faced by a similar fundamental problem of
“term mismatch” between queries and documents.
A standard IR solution, query expansion, attempts to
increase the chances of matching words in relevant
documents by adding terms with similar statistical
properties to those in the original query (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
In this paper we will concentrate on the task of
answer retrieval from FAQ pages, i.e., an IR prob-
lem where user queries are matched against docu-
ments consisting of question-answer pairs found in
FAQ pages. Equivalently, this is a QA problem that
concentrates on finding answers given FAQ docu-
ments that are known to contain the answers. Our
approach to close the lexical gap in this setting at-
tempts to marry QA and IR technology by deploy-
ing SMT methods for query expansion in answer
retrieval. We present two approaches to SMT-based
query expansion, both of which are implemented in
the framework of phrase-based SMT (Och and Ney,
2004; Koehn et al., 2003).
Our first query expansion model trains an end-
to-end phrase-based SMT model on 10 million
question-answer pairs extracted from FAQ pages.
</bodyText>
<footnote confidence="0.982221">
1http://wordnet.princeton.edu
</footnote>
<page confidence="0.972021">
464
</page>
<note confidence="0.925962">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464–471,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999923780487805">
The goal of this system is to learn lexical correla-
tions between words and phrases in questions and
answers, for example by allowing for multiple un-
aligned words in automatic word alignment, and dis-
regarding issues such as word order. The ability to
translate phrases instead of words and the use of a
large language model serve as rich context to make
precise decisions in the case of ambiguous transla-
tions. Query expansion is performed by adding con-
tent words that have not been seen in the original
query from the n-best translations of the query.
Our second query expansion model is based on
the use of SMT technology for full-sentence para-
phrasing. A phrase table of paraphrases is extracted
from bilingual phrase tables (Bannard and Callison-
Burch, 2005), and paraphrasing quality is improved
by additional discriminative training on manually
created paraphrases. This approach utilizes large
bilingual phrase tables as information source to ex-
tract a table of para-phrases. Synonyms for query
expansion are read off from the n-best paraphrases
of full queries instead of from paraphrases of sep-
arate words or phrases. This allows the model to
take advantage of the rich context of a large n-gram
language model when adding terms from the n-best
paraphrases to the original query.
In our experimental evaluation we deploy a
database of question-answer pairs extracted from
FAQ pages for both training a question-answer
translation model, and for a comparative evalua-
tion of different systems on the task of answer re-
trieval. Retrieval is based on the tfidf framework
of Jijkoun and de Rijke (2005), and query expan-
sion is done straightforwardly by adding expansion
terms to the query for a second retrieval cycle. We
compare our global, context-aware query expansion
techniques with Jijkoun and de Rijke’s (2005) tfidf
model for answer retrieval and a local query expan-
sion technique (Xu and Croft, 1996). Experimen-
tal results show a significant improvement of SMT-
based query expansion over both baselines.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999919903846154">
QA has approached the problem of the lexical gap
by various techniques for question reformulation,
including rule-based syntactic and semantic refor-
mulation patterns (Hermjakob et al., 2002), refor-
mulations based on shared dependency parses (Lin
and Pantel, 2001), or various uses of the Word-
Net ontology to close the lexical gap word-by-word
(Hovy et al., 2000; Prager et al., 2001; Harabagiu
et al., 2001). Another use of natural language pro-
cessing has been the deployment of SMT models on
question-answer pairs for (re)ranking candidate an-
swers which were either assumed to be contained
in FAQ pages (Berger et al., 2000) or retrieved by
baseline systems (Echihabi and Marcu, 2003; Sori-
cut and Brill, 2006).
IR has approached the term mismatch problem by
various approaches to query expansion (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
Inconclusive results have been reported for tech-
niques that expand query terms separately by adding
strongly related terms from an external thesaurus
such as WordNet (Voorhees, 1994). Significant
improvements in retrieval performance could be
achieved by global expansion techniques that com-
pute corpus-wide statistics and take the entire query,
or query concept (Qiu and Frei, 1993), into account,
or by local expansion techniques that select expan-
sion terms from the top ranked documents retrieved
by the original query (Xu and Croft, 1996).
A similar picture emerges for query expansion
in QA: Mixed results have been reported for word-
by-word expansion based on WordNet (Burke et
al., 1997; Hovy et al., 2000; Prager et al., 2001;
Harabagiu et al., 2001). Considerable improvements
have been reported for the use of the local context
analysis model of Xu and Croft (1996) in the QA
system of Ittycheriah et al. (2001), or for the sys-
tems of Agichtein et al. (2004) or Harabagiu and
Lacatusu (2004) that use FAQ data to learn how to
expand query terms by answer terms.
The SMT-based approaches presented in this pa-
per can be seen as global query expansion tech-
niques in that our question-answer translation model
uses the whole question-answer corpus as informa-
tion source, and our approach to paraphrasing de-
ploys large amounts of bilingual phrases as high-
coverage information source for synonym finding.
Furthermore, both approaches take the entire query
context into account when proposing to add new
terms to the original query. The approaches that
are closest to our models are the SMT approach of
Radev et al. (2001) and the paraphrasing approach
</bodyText>
<page confidence="0.999254">
465
</page>
<table confidence="0.7697">
web pages FAQ pages QA pairs
count 4 billion 795,483 10,568,160
</table>
<tableCaption confidence="0.999806">
Table 1: Corpus statistics of QA pair data
</tableCaption>
<bodyText confidence="0.999829230769231">
of Duboue and Chu-Carroll (2006). None of these
approaches defines the problem of the lexical gap
as a query expansion problem, and both approaches
use much simpler SMT models than our systems,
e.g., Radev et al. (2001) neglect to use a language
model to aid disambiguation of translation choices,
and Duboue and Chu-Carroll (2006) use SMT as
black box altogether.
In sum, our approach differs from previous work
in QA and IR in the use SMT technology for query
expansion, and should be applicable in both areas
even though experimental results are only given for
the restricted domain of retrieval from FAQ pages.
</bodyText>
<sectionHeader confidence="0.984567" genericHeader="method">
3 Question-Answer Pairs from FAQ Pages
</sectionHeader>
<bodyText confidence="0.99934468">
Large-scale collection of question-answer pairs has
been hampered in previous work by the small sizes
of publicly available FAQ collections or by restricted
access to retrieval results via public APIs of search
engines. Jijkoun and de Rijke (2005) nevertheless
managed to extract around 300,000 FAQ pages
and 2.8 million question-answer pairs by repeatedly
querying search engines with “intitle:faq”
and “inurl:faq”. Soricut and Brill (2006) could
deploy a proprietary URL collection of 1 billion
URLs to extract 2.3 million FAQ pages contain-
ing the uncased string “faq” in the url string. The
extraction of question-answer pairs amounted to a
database of 1 million pairs in their experiment.
However, inspection of the publicly available Web-
FAQ collection provided by Jijkoun and de Rijke2
showed a great amount of noise in the retrieved
FAQ pages and question-answer pairs, and yet the
indexed question-answer pairs showed a serious re-
call problem in that no answer could be retrieved for
many well-formed queries. For our experiment, we
decided to prefer precision over recall and to attempt
a precision-oriented FAQ and question-answer pair
extraction that benefits the training of question-
answer translation models.
</bodyText>
<footnote confidence="0.737582">
2http://ilps.science.uva.nl/Resources/WazDah/
</footnote>
<bodyText confidence="0.99993225">
As shown in Table 1, the FAQ pages used in our
experiment were extracted from a 4 billion page
subset of the web using the queries “inurl:faq”
and “inurl:faqs” to match the tokens “faq” or
“faqs” in the urls. This extraction resulted in 2.6
million web pages (0.07% of the crawl). Since not
all those pages are actually FAQs, we manually la-
beled 1,000 of those pages to train an online passive-
aggressive classificier (Crammer et al., 2006) in a
10-fold cross validation setup. Training was done
using 20 feature functions on occurrences question
marks and key words in different fields of web
pages, and resulted in an F1 score of around 90%
for FAQ classification. Application of the classifier
to the extracted web pages resulted in a classification
of 795,483 pages as FAQ pages.
The extraction of question-answer pairs from this
database of FAQ pages was performed again in a
precision-oriented manner. The goal of this step
was to extract url, title, question, and answers fields
from the question-answer pairs in FAQ pages. This
was achieved by using feature functions on punc-
tuations, HTML tags (e.g., &lt;p&gt;, &lt;BR&gt;), listing
markers (e.g., Q:, (1)), and lexical cues (e.g.,
What, How), and an algorithm similar to Joachims
(2003) to propagate initial labels across similar text
pieces. The result of this extraction step is a database
of about 10 million question answer pairs (13.3
pairs per FAQ page). A manual evaluation of 100
documents, containing 1,303 question-answer pairs,
achieved a precision of 98% and a recall of 82% for
extracting question-answer pairs.
</bodyText>
<sectionHeader confidence="0.963191" genericHeader="method">
4 SMT-Based Query Expansion
</sectionHeader>
<bodyText confidence="0.989637230769231">
Our SMT-based query expansion techniques are
based on a recent implementation of the phrase-
based SMT framework (Koehn et al., 2003; Och and
Ney, 2004). The probability of translating a foreign
sentence f into English e is defined in the noisy chan-
nel model as
arg max p(elf) = argmax p(fle)p(e) (1)
e e
This allows for a separation of a language model
p(e), and a translation model p(fle). Translation
probabilities are calculated from relative frequencies
of phrases, which are extracted via various heuris-
tics as larger blocks of aligned words from best word
</bodyText>
<page confidence="0.998315">
466
</page>
<bodyText confidence="0.99343675">
alignments. Word alignments are estimated by mod-
els similar to Brown et al. (1993). For a sequence of
I phrases, the translation probability in equation (1)
can be decomposed into
</bodyText>
<equation confidence="0.996751">
I
p(fIi |eIi ) = p(fi|ei) (2)
i=1
</equation>
<bodyText confidence="0.997034166666667">
Recent SMT models have shown significant im-
provements in translation quality by improved mod-
eling of local word order and idiomatic expressions
through the use of phrases, and by the deployment
of large n-gram language models to model fluency
and lexical choice.
</bodyText>
<subsectionHeader confidence="0.881888">
4.1 Question-Answer Translation
</subsectionHeader>
<bodyText confidence="0.99998575">
Our first approach to query expansion treats the
questions and answers in the question-answer cor-
pus as two distinct languages. That is, the 10 million
question-answer pairs extracted from FAQ pages are
fed as parallel training data into an SMT training
pipeline. This training procedure includes various
standard procedures such as preprocessing, sentence
and chunk alignment, word alignment, and phrase
extraction. The goal of question-answer translation
is to learn associations between question words and
synonymous answer words, rather than the trans-
lation of questions into fluent answers. Thus we
did not conduct discriminative training of feature
weights for translation probabilities or language
model probabilities, but we held out 4,000 question-
answer pairs for manual development and testing of
the system. For example, the system was adjusted
to account for the difference in sentence length be-
tween questions and answers by setting the null-
word probability parameter in word alignment to
0.9. This allowed us to concentrate the word align-
ments to a small number of key words. Furthermore,
extraction of phrases was based on the intersection
of alignments from both translation directions, thus
favoring precision over recall also in phrase align-
ment.
Table 2 shows unique translations of the query
“how to live with cat allergies” on the phrase-level,
with corresponding source and target phrases shown
in brackets. Expansion terms are taken from phrase
terms that have not been seen in the original query,
and are highlighted in bold face.
</bodyText>
<subsectionHeader confidence="0.947283">
4.2 SMT-Based Paraphrasing
</subsectionHeader>
<bodyText confidence="0.999988263157895">
Our SMT-based paraphrasing system is based on the
approach presented in Bannard and Callison-Burch
(2005). The central idea in this approach is to iden-
tify paraphrases or synonyms at the phrase level by
pivoting on another language. For example, given
a table of Chinese-to-English phrase translations,
phrasal synonyms in the target language are defined
as those English phrases that are aligned to the same
Chinese source phrases. Translation probabilities for
extracted para-phrases can be inferred from bilin-
gual translation probabilities as follows: Given an
English para-phrase pair (trg, syn), the probability
p(syn|trg) that trg translates into syn is defined
as the joint probability that the English phrase trg
translates into the foreign phrase src, and that the
foreign phrase src translates into the English phrase
syn. Under an independence assumption of those
two events, this probability and the reverse transla-
tion direction p(trg|syn) can be defined as follows:
</bodyText>
<equation confidence="0.98661625">
p(syn|trg) = max p(src|trg)p(syn|src) (3)
src p(src|syn)p(trg|src)
p(trg|syn) = max
src
</equation>
<bodyText confidence="0.9999285">
Since the same para-phrase pair can be obtained
by pivoting on multiple foreign language phrases, a
summation or maximization over foreign language
phrases is necessary. In order not to put too much
probability mass onto para-phrase translations that
can be obtained from multiple foreign language
phrases, we maximize instead of summing over src.
In our experiments, we employed equation (3)
to infer for each para-phrase pair translation model
probabilities pφ(syn|trg) and pφ,(trg|syn) from
relative frequencies of phrases in bilingual tables.
In contrast to Bannard and Callison-Burch (2005),
we applied the same inference step to infer also
lexical translation probabilities pw(syn|trg) and
pw,(trg|syn) as defined in Koehn et al. (2003) for
para-phrases. Furthermore, we deployed features for
the number of words lw, number of phrases cφ , a
reordering score pd , and a score for a 6-gram lan-
guage model pLM trained on English web data. The
final model combines these features in a log-linear
model that defines the probability of paraphrasing a
full sentence, consisting of a sequence of I phrases
</bodyText>
<page confidence="0.998586">
467
</page>
<bodyText confidence="0.999470166666667">
qa-translation (how, how) (to, to) (live, live) (with, with) (cat, pet) (allergies, allergies)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, allergy)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, food)
(how, how) (to, to) (live, live) (with, with) (cat, cats) (allergies, allergies)
paraphrasing (how, how) (to live, to live) (with cat, with cat) (allergies, allergy)
(how, ways) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live with, to live with) (cat, feline) (allergies, allergies)
(how to, how to) (live, living) (with cat, with cat) (allergies, allergies)
(how to, how to) (live, life) (with cat, with cat) (allergies, allergies)
(how, way) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergens)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergen)
</bodyText>
<tableCaption confidence="0.967441">
Table 2: Unique n-best phrase-level translations of query “how to live with cat allergies”.
</tableCaption>
<bodyText confidence="0.59137">
as follows:
</bodyText>
<equation confidence="0.9936909">
I
p(synI1|trgI1) _ ( pφ(syni|trgi)λo (4)
i=1
× pφ&apos;(trgi|syni)λo&apos;
× pw(syni|trgi)λ-
× pw&apos;(trgi|syni)λ-&apos;
× pd(syni,trgi)λd)
× lw(synI1)λ�
× cφ(synI1)λ�
× pLM(synI1)λLM
</equation>
<bodyText confidence="0.99945755">
For estimation of the feature weights A defined
in equation (4) we employed minimum error rate
(MER) training under the BLEU measure (Och,
2003). Training data for MER training were taken
from multiple manual English translations of Chi-
nese sources from the NIST 2006 evaluation data.
The first of four reference translations for each Chi-
nese sentence was taken as source paraphrase, the
rest as reference paraphrases. Discriminative train-
ing was conducted on 1,820 sentences; final evalua-
tion on 2,390 sentences. A baseline paraphrase table
consisting of 33 million English para-phrase pairs
was extracted from 1 billion phrase pairs from three
different languages, at a cutoff of para-phrase prob-
abilities of 0.0025.
Query expansion is done by adding terms intro-
duced in n-best paraphrases of the query. Table 2
shows example paraphrases for the query “how to
live with cat allergies” with newly introduced terms
highlighted in bold face.
</bodyText>
<sectionHeader confidence="0.99777" genericHeader="evaluation">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999984875">
Our baseline answer retrieval system is modeled af-
ter the tfidf retrieval model of Jijkoun and de Ri-
jke (2005). Their model calculates a linear com-
bination of vector similarity scores between the
user query and several fields in the question-answer
pair. We used the cosine similarity metric with
logarithmically weighted term and document fre-
quency weights in order to reproduce the Lucene3
model used in Jijkoun and de Rijke (2005). For
indexing of fields, we adopted the settings that
were reported to be optimal in Jijkoun and de
Rijke (2005). These settings comprise the use of
8 question-answer pair fields, and a weight vec-
tor h0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3i for fields or-
dered as follows: (1) full FAQ document text, (2)
question text, (3) answer text, (4) title text, (5)-(8)
each of the above without stopwords. The second
field thus takes takes wh-words, which would typ-
ically be filtered out, into account. All other fields
are matched without stopwords, with higher weight
assigned to document and question than to answer
and title fields. We did not use phrase-matching or
stemming in our experiments, similar to Jijkoun and
de Rijke (2005), who could not find positive effects
for these features in their experiments.
Expansion terms are taken from those terms
in the n-best translations of the query that have
not been seen in the original query string. For
paraphrasing-based query expansion, a 50-best list
of paraphrases of the original query was used.
For the noisier question-answer translation, expan-
sion terms and phrases were extracted from a 10-
</bodyText>
<footnote confidence="0.987999">
3http://lucene.apache.org
</footnote>
<page confidence="0.995255">
468
</page>
<table confidence="0.99855725">
S2010 S2020 S1,2010 S1,2020
baseline tfidf 27 35 58 65
local expansion 30 (+ 11.1) 40 (+ 14.2) 57 (- 1) 63 (- 3)
SMT-based expansion 38 (+ 40.7) 43 (+ 22.8) 58 65
</table>
<tableCaption confidence="0.910409">
Table 3: Success rate at 10 or 20 results for retrieval of adequate (2) or material (1) answers; relative change
in brackets.
</tableCaption>
<bodyText confidence="0.996331657894737">
best list of query translations. Terms taken from
query paraphrases were matched with the same field
weight vector (0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3) as
above. Terms taken from question-answer trans-
lation were matched with the weight vector
(0.0,1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3), preferring an-
swer fields over question fields. After stopword
removal, the average number of expansion terms
produced was 7.8 for paraphrasing, and 3.1 for
question-answer translation.
The local expansion technique used in our exper-
iments follows Xu and Croft (1996) in taking ex-
pansion terms from the top n answers that were re-
trieved by the baseline tfidf system, and by incorpo-
rating cooccurrence information with query terms.
This is done by calculating term frequencies for ex-
pansion terms by summing up the tfidf weights of
the answers in which they occur, thus giving higher
weight to terms that occur in answers that receive
a higher similarity score to the original query. In
our experiments, expansion terms are ranked accord-
ing to this modified tfidf calculation over the top 20
answers retrieved by the baseline retrieval run, and
matched a second time with the field weight vector
(0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3) that prefers an-
swer fields over question fields. After stopword re-
moval, the average number of expansion terms pro-
duced by the local expansion technique was 9.25.
The test queries we used for retrieval are taken
from query logs of the MetaCrawler search en-
gine4 and were provided to us by Valentin Jijk-
oun. In order to maximize recall for the comparative
evaluation of systems, we selected 60 queries that
were well-formed natural language questions with-
out metacharacters and spelling errors. However, for
one third of these well-formed queries none of the
five compared systems could retrieve an answer. Ex-
amples are “how do you make a cornhusk doll”,
</bodyText>
<footnote confidence="0.836748666666667">
4http://www.metacrawler.com
“what is the idea of materialization”, or “what does
8x certified mean”, pointing to a severe recall prob-
</footnote>
<bodyText confidence="0.9901595625">
lem of the question-answer database.
Evaluation was performed by manual labeling of
top 20 answers retrieved for each of 60 queries for
each system by two independent judges. For the sake
of consistency, we chose not to use the assessments
provided by Jijkoun and de Rijke. Instead, the judges
were asked to find agreement on the examples on
which they disagreed after each evaluation round.
The ratings together with the question-answer pair
id were stored and merged into the retrieval results
for the next system evaluation. In this way consis-
tency across system evaluations could be ensured,
and the effort of manual labeling could be substan-
tially reduced. The quality of retrieval results was
assessed according to Jijkoun and de Rijke’s (2005)
three point scale:
</bodyText>
<listItem confidence="0.9955144">
• adequate (2): answer is contained
• material (1): no exact answer, but important in-
formation given
• unsatisfactory (0): user’s information need is
not addressed
</listItem>
<bodyText confidence="0.999972357142857">
The evaluation measure used in Jijkoun and de
Rijke (2005) is the success rate at 10 or 20 an-
swers, i.e., S20n is the percentage of queries with
at least one adequate answer in the top n retrieved
question-answer pairs, and S1,2On is the percentage
of queries with at least one adequate or material an-
swer in the top n results. This evaluation measure ac-
counts for improvements in coverage, i.e., it rewards
cases where answers are found for queries that did
not have an adequate or material answer before. In
contrast, the mean reciprocal rank (MRR) measure
standardly used in QA can have the effect of prefer-
ring systems that find answers only for a small set
of queries, but rank them higher than systems with
</bodyText>
<page confidence="0.998788">
469
</page>
<table confidence="0.996342363636364">
query: how to live with cat allergies
local expansion (-): allergens allergic infections filter plasmacluster rhinitis introduction effective replacement
qa-translation (+): allergy cats pet food
paraphrasing (+): way allergens life allergy feline ways living allergen
query: how to design model rockets
local expansion (-): models represented orientation drawings analysis element environment different structure
qa-translation (+): models rocket
paraphrasing (+): missiles missile rocket grenades arrow designing prototype models ways paradigm
query: what is dna hybridization
local expansion (-): instructions individual blueprint characteristics chromosomes deoxyribonucleic information biological
qa-translation (+): genetic molecule
paraphrasing (+): slides clone cdna sitting sequences
hibridization hybrids hybridation anything hibridacion hybridising adn hybridisation nothing
query: how to enhance competitiveness of indian industries
local expansion (+): resources production quality processing established investment development facilities institutional
qa-translation (+): increase industry
paraphrasing (+): promote raise improve increase industry strengthen
query: how to induce labour
local expansion (-): experience induction practice imagination concentration information consciousness different meditation
qa-translation (-): relaxation
paraphrasing (-): birth industrial induced induces
way workers inducing employment ways labor working child work job action unions
</table>
<tableCaption confidence="0.9782075">
Table 4: Examples for queries and expansion terms yielding improved (+), decreased (-), or unchanged (0)
retrieval performance compared to retrieval without expansion.
</tableCaption>
<bodyText confidence="0.999894833333333">
higher coverage. This makes MRR less adequate for
the low-recall setup of FAQ retrieval.
Table 3 shows success rates at 10 and 20 retrieved
question-answer pairs for five different systems. The
results for the baseline tfidf system, following Jijk-
oun and de Rijke (2005), are shown in row 2. Row
3 presents results for our variant of local expansion
by pseudo-relevance feedback (Xu and Croft, 1996).
Results for SMT-based expansion are given in row 4.
A comparison of success rates for retrieving at least
one adequate answer in the top 10 results shows rel-
ative improvements over the baseline of 11.1% for
local query expansion, and of 40.7% for combined
SMT-based expansion. Success rates at top 20 re-
sults show similar relative improvements of 14.2%
for local query expansion, and of 22.8% for com-
bined SMT-based expansion. On the easier task of
retrieving a material or adequate answer, success
rates drop by a small amount for local expansion,
and stay unchanged for SMT-based expansion.
These results can be explained by inspecting a few
sample query expansions. Examples (1)-(3) in Ta-
ble 4 illustrate cases where SMT-based query expan-
sion improves results over baseline performance, but
local expansion decreases performance by introduc-
ing irrelevant terms. In (4) retrieval performance is
improved over the baseline for both expansion tech-
niques. In (5) both local and SMT-based expansion
introduce terms that decrease retrieval performance
compared to retrieval without expansion.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999936727272727">
We presented two techniques for query expansion in
answer retrieval that are based on SMT technology.
Our method for question-answer translation uses a
large corpus of question-answer pairs extracted from
FAQ pages to learn a translation model from ques-
tions to answers. SMT-based paraphrasing utilizes
large amounts of bilingual data as a new informa-
tion source to extract phrase-level synonyms. Both
SMT-based techniques take the entire query context
into account when adding new terms to the orig-
inal query. In an experimental comparison with a
baseline tfidf approach and a local query expansion
technique on the task of answer retrieval from FAQ
pages, we showed a significant improvement of both
SMT-based query expansion over both baselines.
Despite the small-scale nature of our current ex-
perimental results, we hope to apply the presented
techniques to general web retrieval in future work.
Another task for future work is to scale up the ex-
traction of question-answer pair data in order to
provide an improved resource for question-answer
translation.
</bodyText>
<page confidence="0.99704">
470
</page>
<sectionHeader confidence="0.996336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999910347826087">
Eugene Agichtein, Steve Lawrence, and Luis Gravano.
2004. Learning to find answers to questions on
the web. ACM Transactions on Internet Technology,
4(2):129–162.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of (ACL’05), Ann Arbor, MI.
Adam L. Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexical
chasm: Statistical approaches to answer-finding. In
Proceedings of SIGIR’00, Athens, Greece.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311.
Robin B. Burke, Kristian J. Hammond, and Vladimir A.
Kulyukin. 1997. Question answering from
frequently-asked question files: Experiences with the
FAQ finder system. AI Magazine, 18(2):57–66.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yo-ram Singer. 2006. Online passive-
agressive algorithms. Machine Learning, 7:551–585.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. An-
swering the question you wish they had asked: The im-
pact of paraphrasing for question answering. In Pro-
ceedings of (HLT-NAACL’06), New York, NY.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of (ACL’03), Sapporo, Japan.
Sanda Harabagiu and Finley Lacatusu. 2004. Strategies
for advanced question answering. In Proceedings of
the HLT-NAACL’04 Workshop on Pragmatics of Ques-
tion Answering, Boston, MA.
Sanda Harabagiu, Dan Moldovan, Marius Pas¸ca, Rada
Mihalcea, Mihai Surdeanu, R˘azvan Bunescu, Roxana
Girju, Vasile Rus, and Paul Mor˘arescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of (ACL’01),
Toulouse, France.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answering.
In Proceedings of TREC-11, Gaithersburg, MD.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2000. Question answering
in webclopedia. In Proceedings of TREC 9, Gaithers-
burg, MD.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM’s statistical question answering system. In
Proceedings of TREC 10, Gaithersburg, MD.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management (CIKM’05),
Bremen, Germany.
Thorsten Joachims. 2003. Transductive learning
via spectral graph partitioning. In Proceedings of
ICML’03, Washington, DC.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of (HLT-NAACL’03), Edmonton, Cananda.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Journal ofNatural
Language Engineering, 7(3):343–360.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
(HLT-NAACL’03), Edmonton, Cananda.
John Prager, Jennifer Chu-Carroll, and Krysztof Czuba.
2001. Use of wordnet hypernyms for answering what-
is questions. In Proceedings of TREC 10, Gaithers-
burg, MD.
Yonggang Qiu and H. P. Frei. 1993. Concept based query
expansion. In Proceedings of SIGIR’93, Pittsburgh,
PA.
Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha
Blair-Goldensohn, Zhu Zhang, Weigo Fan, and John
Prager. 2001. Mining the web for answers to natu-
ral language questions. In Proceedings of (CIKM’01),
Atlanta, GA.
Radu Soricut and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9:191–206.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SI-
GIR’94, Dublin, Ireland.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of SIGIR’96, Zurich, Switzerland.
</reference>
<page confidence="0.998732">
471
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.545516">
<title confidence="0.90759">Statistical Machine Translation for Query Expansion in Answer Retrieval</title>
<author confidence="0.598635">Alexander Vasserman Riezler</author>
<author confidence="0.598635">Ioannis Tsochantaridis</author>
<author confidence="0.598635">Vibhu Mittal Liu</author>
<address confidence="0.957798">Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA 94043</address>
<abstract confidence="0.998454421052632">We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers. SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs. We evaluate these global, context-aware query expansion techon from 10 million question-answer pairs extracted from FAQ pages. Experimental results show that SMTbased expansion improves retrieval performance over local expansion and over retrieval without expansion.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Steve Lawrence</author>
<author>Luis Gravano</author>
</authors>
<title>Learning to find answers to questions on the web.</title>
<date>2004</date>
<journal>ACM Transactions on Internet Technology,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="7276" citStr="Agichtein et al. (2004)" startWordPosition="1148" endWordPosition="1151">, or query concept (Qiu and Frei, 1993), into account, or by local expansion techniques that select expansion terms from the top ranked documents retrieved by the original query (Xu and Croft, 1996). A similar picture emerges for query expansion in QA: Mixed results have been reported for wordby-word expansion based on WordNet (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Considerable improvements have been reported for the use of the local context analysis model of Xu and Croft (1996) in the QA system of Ittycheriah et al. (2001), or for the systems of Agichtein et al. (2004) or Harabagiu and Lacatusu (2004) that use FAQ data to learn how to expand query terms by answer terms. The SMT-based approaches presented in this paper can be seen as global query expansion techniques in that our question-answer translation model uses the whole question-answer corpus as information source, and our approach to paraphrasing deploys large amounts of bilingual phrases as highcoverage information source for synonym finding. Furthermore, both approaches take the entire query context into account when proposing to add new terms to the original query. The approaches that are closest </context>
</contexts>
<marker>Agichtein, Lawrence, Gravano, 2004</marker>
<rawString>Eugene Agichtein, Steve Lawrence, and Luis Gravano. 2004. Learning to find answers to questions on the web. ACM Transactions on Internet Technology, 4(2):129–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of (ACL’05),</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="14357" citStr="Bannard and Callison-Burch (2005)" startWordPosition="2277" endWordPosition="2280">nts to a small number of key words. Furthermore, extraction of phrases was based on the intersection of alignments from both translation directions, thus favoring precision over recall also in phrase alignment. Table 2 shows unique translations of the query “how to live with cat allergies” on the phrase-level, with corresponding source and target phrases shown in brackets. Expansion terms are taken from phrase terms that have not been seen in the original query, and are highlighted in bold face. 4.2 SMT-Based Paraphrasing Our SMT-based paraphrasing system is based on the approach presented in Bannard and Callison-Burch (2005). The central idea in this approach is to identify paraphrases or synonyms at the phrase level by pivoting on another language. For example, given a table of Chinese-to-English phrase translations, phrasal synonyms in the target language are defined as those English phrases that are aligned to the same Chinese source phrases. Translation probabilities for extracted para-phrases can be inferred from bilingual translation probabilities as follows: Given an English para-phrase pair (trg, syn), the probability p(syn|trg) that trg translates into syn is defined as the joint probability that the Eng</context>
<context position="15915" citStr="Bannard and Callison-Burch (2005)" startWordPosition="2508" endWordPosition="2511">c|syn)p(trg|src) p(trg|syn) = max src Since the same para-phrase pair can be obtained by pivoting on multiple foreign language phrases, a summation or maximization over foreign language phrases is necessary. In order not to put too much probability mass onto para-phrase translations that can be obtained from multiple foreign language phrases, we maximize instead of summing over src. In our experiments, we employed equation (3) to infer for each para-phrase pair translation model probabilities pφ(syn|trg) and pφ,(trg|syn) from relative frequencies of phrases in bilingual tables. In contrast to Bannard and Callison-Burch (2005), we applied the same inference step to infer also lexical translation probabilities pw(syn|trg) and pw,(trg|syn) as defined in Koehn et al. (2003) for para-phrases. Furthermore, we deployed features for the number of words lw, number of phrases cφ , a reordering score pd , and a score for a 6-gram language model pLM trained on English web data. The final model combines these features in a log-linear model that defines the probability of paraphrasing a full sentence, consisting of a sequence of I phrases 467 qa-translation (how, how) (to, to) (live, live) (with, with) (cat, pet) (allergies, al</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of (ACL’05), Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Rich Caruana</author>
<author>David Cohn</author>
<author>Dayne Freitag</author>
<author>Vibhu Mittal</author>
</authors>
<title>Bridging the lexical chasm: Statistical approaches to answer-finding.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR’00,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="1135" citStr="Berger et al., 2000" startWordPosition="164" endWordPosition="167">nce paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs. We evaluate these global, context-aware query expansion techniques on tfidf retrieval from 10 million question-answer pairs extracted from FAQ pages. Experimental results show that SMTbased expansion improves retrieval performance over local expansion and over retrieval without expansion. 1 Introduction One of the fundamental problems in Question Answering (QA) has been recognized to be the “lexical chasm” (Berger et al., 2000) between question strings and answer strings. This problem is manifested in a mismatch between question and answer vocabularies, and is aggravated by the inherent ambiguity of natural language. Several approaches have been presented that apply natural language processing technology to close this gap. For example, syntactic information has been deployed to reformulate questions (Hermjakob et al., 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al., 1997; Hovy et a</context>
<context position="6079" citStr="Berger et al., 2000" startWordPosition="954" endWordPosition="957">s approached the problem of the lexical gap by various techniques for question reformulation, including rule-based syntactic and semantic reformulation patterns (Hermjakob et al., 2002), reformulations based on shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al., 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). IR has approached the term mismatch problem by various approaches to query expansion (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). Inconclusive results have been reported for techniques that expand query terms separately by adding strongly related terms from an external thesaurus such as WordNet (Voorhees, 1994). Significant improvements in retrieval performance could be achieved by global expansion techniques that compute corpus-wide statistics and take the entire query, or query concept (Qiu an</context>
</contexts>
<marker>Berger, Caruana, Cohn, Freitag, Mittal, 2000</marker>
<rawString>Adam L. Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu Mittal. 2000. Bridging the lexical chasm: Statistical approaches to answer-finding. In Proceedings of SIGIR’00, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="12243" citStr="Brown et al. (1993)" startWordPosition="1950" endWordPosition="1953">n techniques are based on a recent implementation of the phrasebased SMT framework (Koehn et al., 2003; Och and Ney, 2004). The probability of translating a foreign sentence f into English e is defined in the noisy channel model as arg max p(elf) = argmax p(fle)p(e) (1) e e This allows for a separation of a language model p(e), and a translation model p(fle). Translation probabilities are calculated from relative frequencies of phrases, which are extracted via various heuristics as larger blocks of aligned words from best word 466 alignments. Word alignments are estimated by models similar to Brown et al. (1993). For a sequence of I phrases, the translation probability in equation (1) can be decomposed into I p(fIi |eIi ) = p(fi|ei) (2) i=1 Recent SMT models have shown significant improvements in translation quality by improved modeling of local word order and idiomatic expressions through the use of phrases, and by the deployment of large n-gram language models to model fluency and lexical choice. 4.1 Question-Answer Translation Our first approach to query expansion treats the questions and answers in the question-answer corpus as two distinct languages. That is, the 10 million question-answer pairs</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robin B Burke</author>
<author>Kristian J Hammond</author>
<author>A Vladimir</author>
</authors>
<marker>Burke, Hammond, Vladimir, </marker>
<rawString>Robin B. Burke, Kristian J. Hammond, and Vladimir A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kulyukin</author>
</authors>
<title>Question answering from frequently-asked question files: Experiences with the FAQ finder system.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>2</issue>
<marker>Kulyukin, 1997</marker>
<rawString>Kulyukin. 1997. Question answering from frequently-asked question files: Experiences with the FAQ finder system. AI Magazine, 18(2):57–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yo-ram Singer</author>
</authors>
<title>Online passiveagressive algorithms.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>7--551</pages>
<contexts>
<context position="10435" citStr="Crammer et al., 2006" startWordPosition="1655" endWordPosition="1658">o attempt a precision-oriented FAQ and question-answer pair extraction that benefits the training of questionanswer translation models. 2http://ilps.science.uva.nl/Resources/WazDah/ As shown in Table 1, the FAQ pages used in our experiment were extracted from a 4 billion page subset of the web using the queries “inurl:faq” and “inurl:faqs” to match the tokens “faq” or “faqs” in the urls. This extraction resulted in 2.6 million web pages (0.07% of the crawl). Since not all those pages are actually FAQs, we manually labeled 1,000 of those pages to train an online passiveaggressive classificier (Crammer et al., 2006) in a 10-fold cross validation setup. Training was done using 20 feature functions on occurrences question marks and key words in different fields of web pages, and resulted in an F1 score of around 90% for FAQ classification. Application of the classifier to the extracted web pages resulted in a classification of 795,483 pages as FAQ pages. The extraction of question-answer pairs from this database of FAQ pages was performed again in a precision-oriented manner. The goal of this step was to extract url, title, question, and answers fields from the question-answer pairs in FAQ pages. This was </context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yo-ram Singer. 2006. Online passiveagressive algorithms. Machine Learning, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Ariel Duboue</author>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>Answering the question you wish they had asked: The impact of paraphrasing for question answering.</title>
<date>2006</date>
<booktitle>In Proceedings of (HLT-NAACL’06),</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="8107" citStr="Duboue and Chu-Carroll (2006)" startWordPosition="1285" endWordPosition="1288">in that our question-answer translation model uses the whole question-answer corpus as information source, and our approach to paraphrasing deploys large amounts of bilingual phrases as highcoverage information source for synonym finding. Furthermore, both approaches take the entire query context into account when proposing to add new terms to the original query. The approaches that are closest to our models are the SMT approach of Radev et al. (2001) and the paraphrasing approach 465 web pages FAQ pages QA pairs count 4 billion 795,483 10,568,160 Table 1: Corpus statistics of QA pair data of Duboue and Chu-Carroll (2006). None of these approaches defines the problem of the lexical gap as a query expansion problem, and both approaches use much simpler SMT models than our systems, e.g., Radev et al. (2001) neglect to use a language model to aid disambiguation of translation choices, and Duboue and Chu-Carroll (2006) use SMT as black box altogether. In sum, our approach differs from previous work in QA and IR in the use SMT technology for query expansion, and should be applicable in both areas even though experimental results are only given for the restricted domain of retrieval from FAQ pages. 3 Question-Answer</context>
</contexts>
<marker>Duboue, Chu-Carroll, 2006</marker>
<rawString>Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. Answering the question you wish they had asked: The impact of paraphrasing for question answering. In Proceedings of (HLT-NAACL’06), New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of (ACL’03),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2005" citStr="Echihabi and Marcu, 2003" startWordPosition="303" endWordPosition="306">tural language processing technology to close this gap. For example, syntactic information has been deployed to reformulate questions (Hermjakob et al., 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). In this paper we will concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FA</context>
<context position="6138" citStr="Echihabi and Marcu, 2003" startWordPosition="963" endWordPosition="966">techniques for question reformulation, including rule-based syntactic and semantic reformulation patterns (Hermjakob et al., 2002), reformulations based on shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al., 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). IR has approached the term mismatch problem by various approaches to query expansion (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). Inconclusive results have been reported for techniques that expand query terms separately by adding strongly related terms from an external thesaurus such as WordNet (Voorhees, 1994). Significant improvements in retrieval performance could be achieved by global expansion techniques that compute corpus-wide statistics and take the entire query, or query concept (Qiu and Frei, 1993), into account, or by local expansion techniqu</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>Abdessamad Echihabi and Daniel Marcu. 2003. A noisy-channel approach to question answering. In Proceedings of (ACL’03), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley Lacatusu</author>
</authors>
<title>Strategies for advanced question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL’04 Workshop on Pragmatics of Question Answering,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="7309" citStr="Harabagiu and Lacatusu (2004)" startWordPosition="1153" endWordPosition="1156"> Frei, 1993), into account, or by local expansion techniques that select expansion terms from the top ranked documents retrieved by the original query (Xu and Croft, 1996). A similar picture emerges for query expansion in QA: Mixed results have been reported for wordby-word expansion based on WordNet (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Considerable improvements have been reported for the use of the local context analysis model of Xu and Croft (1996) in the QA system of Ittycheriah et al. (2001), or for the systems of Agichtein et al. (2004) or Harabagiu and Lacatusu (2004) that use FAQ data to learn how to expand query terms by answer terms. The SMT-based approaches presented in this paper can be seen as global query expansion techniques in that our question-answer translation model uses the whole question-answer corpus as information source, and our approach to paraphrasing deploys large amounts of bilingual phrases as highcoverage information source for synonym finding. Furthermore, both approaches take the entire query context into account when proposing to add new terms to the original query. The approaches that are closest to our models are the SMT approac</context>
</contexts>
<marker>Harabagiu, Lacatusu, 2004</marker>
<rawString>Sanda Harabagiu and Finley Lacatusu. 2004. Strategies for advanced question answering. In Proceedings of the HLT-NAACL’04 Workshop on Pragmatics of Question Answering, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pas¸ca</author>
<author>Rada Mihalcea</author>
<author>Mihai Surdeanu</author>
<author>R˘azvan Bunescu</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
<author>Paul Mor˘arescu</author>
</authors>
<title>The role of lexico-semantic feedback in open-domain textual question-answering.</title>
<date>2001</date>
<booktitle>In Proceedings of (ACL’01),</booktitle>
<location>Toulouse, France.</location>
<marker>Harabagiu, Moldovan, Pas¸ca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, Mor˘arescu, 2001</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Marius Pas¸ca, Rada Mihalcea, Mihai Surdeanu, R˘azvan Bunescu, Roxana Girju, Vasile Rus, and Paul Mor˘arescu. 2001. The role of lexico-semantic feedback in open-domain textual question-answering. In Proceedings of (ACL’01), Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>Natural language based reformulation resource and web exploitation for question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of TREC-11,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="1539" citStr="Hermjakob et al., 2002" startWordPosition="226" endWordPosition="229">rieval performance over local expansion and over retrieval without expansion. 1 Introduction One of the fundamental problems in Question Answering (QA) has been recognized to be the “lexical chasm” (Berger et al., 2000) between question strings and answer strings. This problem is manifested in a mismatch between question and answer vocabularies, and is aggravated by the inherent ambiguity of natural language. Several approaches have been presented that apply natural language processing technology to close this gap. For example, syntactic information has been deployed to reformulate questions (Hermjakob et al., 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries an</context>
<context position="5644" citStr="Hermjakob et al., 2002" startWordPosition="881" endWordPosition="884">e (2005), and query expansion is done straightforwardly by adding expansion terms to the query for a second retrieval cycle. We compare our global, context-aware query expansion techniques with Jijkoun and de Rijke’s (2005) tfidf model for answer retrieval and a local query expansion technique (Xu and Croft, 1996). Experimental results show a significant improvement of SMTbased query expansion over both baselines. 2 Related Work QA has approached the problem of the lexical gap by various techniques for question reformulation, including rule-based syntactic and semantic reformulation patterns (Hermjakob et al., 2002), reformulations based on shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al., 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). IR has approached the term mismatch problem by various approaches to query exp</context>
</contexts>
<marker>Hermjakob, Echihabi, Marcu, 2002</marker>
<rawString>Ulf Hermjakob, Abdessamad Echihabi, and Daniel Marcu. 2002. Natural language based reformulation resource and web exploitation for question answering. In Proceedings of TREC-11, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Laurie Gerber</author>
<author>Ulf Hermjakob</author>
<author>Michael Junk</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Question answering in webclopedia.</title>
<date>2000</date>
<booktitle>In Proceedings of TREC 9,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="1743" citStr="Hovy et al., 2000" startWordPosition="262" endWordPosition="265">l., 2000) between question strings and answer strings. This problem is manifested in a mismatch between question and answer vocabularies, and is aggravated by the inherent ambiguity of natural language. Several approaches have been presented that apply natural language processing technology to close this gap. For example, syntactic information has been deployed to reformulate questions (Hermjakob et al., 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original </context>
<context position="5815" citStr="Hovy et al., 2000" startWordPosition="910" endWordPosition="913"> techniques with Jijkoun and de Rijke’s (2005) tfidf model for answer retrieval and a local query expansion technique (Xu and Croft, 1996). Experimental results show a significant improvement of SMTbased query expansion over both baselines. 2 Related Work QA has approached the problem of the lexical gap by various techniques for question reformulation, including rule-based syntactic and semantic reformulation patterns (Hermjakob et al., 2002), reformulations based on shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al., 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). IR has approached the term mismatch problem by various approaches to query expansion (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). Inconclusive results have been reported for techniques that expand query terms separately by adding strong</context>
</contexts>
<marker>Hovy, Gerber, Hermjakob, Junk, Lin, 2000</marker>
<rawString>Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael Junk, and Chin-Yew Lin. 2000. Question answering in webclopedia. In Proceedings of TREC 9, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Martin Franz</author>
<author>Salim Roukos</author>
</authors>
<title>IBM’s statistical question answering system.</title>
<date>2001</date>
<booktitle>In Proceedings of TREC 10,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="7229" citStr="Ittycheriah et al. (2001)" startWordPosition="1138" endWordPosition="1141"> corpus-wide statistics and take the entire query, or query concept (Qiu and Frei, 1993), into account, or by local expansion techniques that select expansion terms from the top ranked documents retrieved by the original query (Xu and Croft, 1996). A similar picture emerges for query expansion in QA: Mixed results have been reported for wordby-word expansion based on WordNet (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Considerable improvements have been reported for the use of the local context analysis model of Xu and Croft (1996) in the QA system of Ittycheriah et al. (2001), or for the systems of Agichtein et al. (2004) or Harabagiu and Lacatusu (2004) that use FAQ data to learn how to expand query terms by answer terms. The SMT-based approaches presented in this paper can be seen as global query expansion techniques in that our question-answer translation model uses the whole question-answer corpus as information source, and our approach to paraphrasing deploys large amounts of bilingual phrases as highcoverage information source for synonym finding. Furthermore, both approaches take the entire query context into account when proposing to add new terms to the o</context>
</contexts>
<marker>Ittycheriah, Franz, Roukos, 2001</marker>
<rawString>Abraham Ittycheriah, Martin Franz, and Salim Roukos. 2001. IBM’s statistical question answering system. In Proceedings of TREC 10, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
</authors>
<title>Retrieving answers from frequently asked questions pages on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth ACM Conference on Information and Knowledge Management (CIKM’05),</booktitle>
<location>Bremen, Germany.</location>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving answers from frequently asked questions pages on the web. In Proceedings of the Tenth ACM Conference on Information and Knowledge Management (CIKM’05), Bremen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive learning via spectral graph partitioning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML’03,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="11229" citStr="Joachims (2003)" startWordPosition="1786" endWordPosition="1787"> F1 score of around 90% for FAQ classification. Application of the classifier to the extracted web pages resulted in a classification of 795,483 pages as FAQ pages. The extraction of question-answer pairs from this database of FAQ pages was performed again in a precision-oriented manner. The goal of this step was to extract url, title, question, and answers fields from the question-answer pairs in FAQ pages. This was achieved by using feature functions on punctuations, HTML tags (e.g., &lt;p&gt;, &lt;BR&gt;), listing markers (e.g., Q:, (1)), and lexical cues (e.g., What, How), and an algorithm similar to Joachims (2003) to propagate initial labels across similar text pieces. The result of this extraction step is a database of about 10 million question answer pairs (13.3 pairs per FAQ page). A manual evaluation of 100 documents, containing 1,303 question-answer pairs, achieved a precision of 98% and a recall of 82% for extracting question-answer pairs. 4 SMT-Based Query Expansion Our SMT-based query expansion techniques are based on a recent implementation of the phrasebased SMT framework (Koehn et al., 2003; Och and Ney, 2004). The probability of translating a foreign sentence f into English e is defined in </context>
</contexts>
<marker>Joachims, 2003</marker>
<rawString>Thorsten Joachims. 2003. Transductive learning via spectral graph partitioning. In Proceedings of ICML’03, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of (HLT-NAACL’03),</booktitle>
<location>Edmonton, Cananda.</location>
<contexts>
<context position="3066" citStr="Koehn et al., 2003" startWordPosition="478" endWordPosition="481"> task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages. Equivalently, this is a QA problem that concentrates on finding answers given FAQ documents that are known to contain the answers. Our approach to close the lexical gap in this setting attempts to marry QA and IR technology by deploying SMT methods for query expansion in answer retrieval. We present two approaches to SMT-based query expansion, both of which are implemented in the framework of phrase-based SMT (Och and Ney, 2004; Koehn et al., 2003). Our first query expansion model trains an endto-end phrase-based SMT model on 10 million question-answer pairs extracted from FAQ pages. 1http://wordnet.princeton.edu 464 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464–471, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics The goal of this system is to learn lexical correlations between words and phrases in questions and answers, for example by allowing for multiple unaligned words in automatic word alignment, and disregarding issues such as word order. The a</context>
<context position="11726" citStr="Koehn et al., 2003" startWordPosition="1862" endWordPosition="1865">, &lt;BR&gt;), listing markers (e.g., Q:, (1)), and lexical cues (e.g., What, How), and an algorithm similar to Joachims (2003) to propagate initial labels across similar text pieces. The result of this extraction step is a database of about 10 million question answer pairs (13.3 pairs per FAQ page). A manual evaluation of 100 documents, containing 1,303 question-answer pairs, achieved a precision of 98% and a recall of 82% for extracting question-answer pairs. 4 SMT-Based Query Expansion Our SMT-based query expansion techniques are based on a recent implementation of the phrasebased SMT framework (Koehn et al., 2003; Och and Ney, 2004). The probability of translating a foreign sentence f into English e is defined in the noisy channel model as arg max p(elf) = argmax p(fle)p(e) (1) e e This allows for a separation of a language model p(e), and a translation model p(fle). Translation probabilities are calculated from relative frequencies of phrases, which are extracted via various heuristics as larger blocks of aligned words from best word 466 alignments. Word alignments are estimated by models similar to Brown et al. (1993). For a sequence of I phrases, the translation probability in equation (1) can be d</context>
<context position="16062" citStr="Koehn et al. (2003)" startWordPosition="2530" endWordPosition="2533">on over foreign language phrases is necessary. In order not to put too much probability mass onto para-phrase translations that can be obtained from multiple foreign language phrases, we maximize instead of summing over src. In our experiments, we employed equation (3) to infer for each para-phrase pair translation model probabilities pφ(syn|trg) and pφ,(trg|syn) from relative frequencies of phrases in bilingual tables. In contrast to Bannard and Callison-Burch (2005), we applied the same inference step to infer also lexical translation probabilities pw(syn|trg) and pw,(trg|syn) as defined in Koehn et al. (2003) for para-phrases. Furthermore, we deployed features for the number of words lw, number of phrases cφ , a reordering score pd , and a score for a 6-gram language model pLM trained on English web data. The final model combines these features in a log-linear model that defines the probability of paraphrasing a full sentence, consisting of a sequence of I phrases 467 qa-translation (how, how) (to, to) (live, live) (with, with) (cat, pet) (allergies, allergies) (how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, allergy) (how, how) (to, to) (live, live) (with, with) (cat, cat) (al</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of (HLT-NAACL’03), Edmonton, Cananda.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Journal ofNatural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="1616" citStr="Lin and Pantel, 2001" startWordPosition="239" endWordPosition="242"> Introduction One of the fundamental problems in Question Answering (QA) has been recognized to be the “lexical chasm” (Berger et al., 2000) between question strings and answer strings. This problem is manifested in a mismatch between question and answer vocabularies, and is aggravated by the inherent ambiguity of natural language. Several approaches have been presented that apply natural language processing technology to close this gap. For example, syntactic information has been deployed to reformulate questions (Hermjakob et al., 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase th</context>
<context position="5717" citStr="Lin and Pantel, 2001" startWordPosition="892" endWordPosition="895"> terms to the query for a second retrieval cycle. We compare our global, context-aware query expansion techniques with Jijkoun and de Rijke’s (2005) tfidf model for answer retrieval and a local query expansion technique (Xu and Croft, 1996). Experimental results show a significant improvement of SMTbased query expansion over both baselines. 2 Related Work QA has approached the problem of the lexical gap by various techniques for question reformulation, including rule-based syntactic and semantic reformulation patterns (Hermjakob et al., 2002), reformulations based on shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al., 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). IR has approached the term mismatch problem by various approaches to query expansion (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). Inconclu</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Journal ofNatural Language Engineering, 7(3):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="3045" citStr="Och and Ney, 2004" startWordPosition="474" endWordPosition="477"> concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages. Equivalently, this is a QA problem that concentrates on finding answers given FAQ documents that are known to contain the answers. Our approach to close the lexical gap in this setting attempts to marry QA and IR technology by deploying SMT methods for query expansion in answer retrieval. We present two approaches to SMT-based query expansion, both of which are implemented in the framework of phrase-based SMT (Och and Ney, 2004; Koehn et al., 2003). Our first query expansion model trains an endto-end phrase-based SMT model on 10 million question-answer pairs extracted from FAQ pages. 1http://wordnet.princeton.edu 464 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464–471, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics The goal of this system is to learn lexical correlations between words and phrases in questions and answers, for example by allowing for multiple unaligned words in automatic word alignment, and disregarding issues such</context>
<context position="11746" citStr="Och and Ney, 2004" startWordPosition="1866" endWordPosition="1869">kers (e.g., Q:, (1)), and lexical cues (e.g., What, How), and an algorithm similar to Joachims (2003) to propagate initial labels across similar text pieces. The result of this extraction step is a database of about 10 million question answer pairs (13.3 pairs per FAQ page). A manual evaluation of 100 documents, containing 1,303 question-answer pairs, achieved a precision of 98% and a recall of 82% for extracting question-answer pairs. 4 SMT-Based Query Expansion Our SMT-based query expansion techniques are based on a recent implementation of the phrasebased SMT framework (Koehn et al., 2003; Och and Ney, 2004). The probability of translating a foreign sentence f into English e is defined in the noisy channel model as arg max p(elf) = argmax p(fle)p(e) (1) e e This allows for a separation of a language model p(e), and a translation model p(fle). Translation probabilities are calculated from relative frequencies of phrases, which are extracted via various heuristics as larger blocks of aligned words from best word 466 alignments. Word alignments are estimated by models similar to Brown et al. (1993). For a sequence of I phrases, the translation probability in equation (1) can be decomposed into I p(f</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of (HLT-NAACL’03),</booktitle>
<location>Edmonton, Cananda.</location>
<contexts>
<context position="17780" citStr="Och, 2003" startWordPosition="2803" endWordPosition="2804">ive) (with cat, with cat) (allergies, allergies) (how, how) (to live, to live) (with cat, with cat) (allergies, allergens) (how, how) (to live, to live) (with cat, with cat) (allergies, allergen) Table 2: Unique n-best phrase-level translations of query “how to live with cat allergies”. as follows: I p(synI1|trgI1) _ ( pφ(syni|trgi)λo (4) i=1 × pφ&apos;(trgi|syni)λo&apos; × pw(syni|trgi)λ- × pw&apos;(trgi|syni)λ-&apos; × pd(syni,trgi)λd) × lw(synI1)λ� × cφ(synI1)λ� × pLM(synI1)λLM For estimation of the feature weights A defined in equation (4) we employed minimum error rate (MER) training under the BLEU measure (Och, 2003). Training data for MER training were taken from multiple manual English translations of Chinese sources from the NIST 2006 evaluation data. The first of four reference translations for each Chinese sentence was taken as source paraphrase, the rest as reference paraphrases. Discriminative training was conducted on 1,820 sentences; final evaluation on 2,390 sentences. A baseline paraphrase table consisting of 33 million English para-phrase pairs was extracted from 1 billion phrase pairs from three different languages, at a cutoff of para-phrase probabilities of 0.0025. Query expansion is done b</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of (HLT-NAACL’03), Edmonton, Cananda.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Jennifer Chu-Carroll</author>
<author>Krysztof Czuba</author>
</authors>
<title>Use of wordnet hypernyms for answering whatis questions.</title>
<date>2001</date>
<booktitle>In Proceedings of TREC 10,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="1764" citStr="Prager et al., 2001" startWordPosition="266" endWordPosition="269">uestion strings and answer strings. This problem is manifested in a mismatch between question and answer vocabularies, and is aggravated by the inherent ambiguity of natural language. Several approaches have been presented that apply natural language processing technology to close this gap. For example, syntactic information has been deployed to reformulate questions (Hermjakob et al., 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query (Voorhees, 1994</context>
<context position="5836" citStr="Prager et al., 2001" startWordPosition="914" endWordPosition="917">jkoun and de Rijke’s (2005) tfidf model for answer retrieval and a local query expansion technique (Xu and Croft, 1996). Experimental results show a significant improvement of SMTbased query expansion over both baselines. 2 Related Work QA has approached the problem of the lexical gap by various techniques for question reformulation, including rule-based syntactic and semantic reformulation patterns (Hermjakob et al., 2002), reformulations based on shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al., 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). IR has approached the term mismatch problem by various approaches to query expansion (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). Inconclusive results have been reported for techniques that expand query terms separately by adding strongly related terms from</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2001</marker>
<rawString>John Prager, Jennifer Chu-Carroll, and Krysztof Czuba. 2001. Use of wordnet hypernyms for answering whatis questions. In Proceedings of TREC 10, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Qiu</author>
<author>H P Frei</author>
</authors>
<title>Concept based query expansion.</title>
<date>1993</date>
<booktitle>In Proceedings of SIGIR’93,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2384" citStr="Qiu and Frei, 1993" startWordPosition="360" endWordPosition="363">Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). In this paper we will concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages. Equivalently, this is a QA problem that concentrates on finding answers given FAQ documents that are known to contain the answers. Our approach to close the lexical gap in this setting attempts to marry QA and IR technology by deploying SMT methods for query expansion in answer retrieval. We present two approaches to SMT-based query expansion, both of which are implem</context>
<context position="6286" citStr="Qiu and Frei, 1993" startWordPosition="987" endWordPosition="990">n shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al., 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). IR has approached the term mismatch problem by various approaches to query expansion (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). Inconclusive results have been reported for techniques that expand query terms separately by adding strongly related terms from an external thesaurus such as WordNet (Voorhees, 1994). Significant improvements in retrieval performance could be achieved by global expansion techniques that compute corpus-wide statistics and take the entire query, or query concept (Qiu and Frei, 1993), into account, or by local expansion techniques that select expansion terms from the top ranked documents retrieved by the original query (Xu and Croft, 1996). A similar picture emerges for que</context>
</contexts>
<marker>Qiu, Frei, 1993</marker>
<rawString>Yonggang Qiu and H. P. Frei. 1993. Concept based query expansion. In Proceedings of SIGIR’93, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hong Qi</author>
<author>Zhiping Zheng</author>
<author>Sasha Blair-Goldensohn</author>
<author>Zhu Zhang</author>
<author>Weigo Fan</author>
<author>John Prager</author>
</authors>
<title>Mining the web for answers to natural language questions.</title>
<date>2001</date>
<booktitle>In Proceedings of (CIKM’01),</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="7933" citStr="Radev et al. (2001)" startWordPosition="1256" endWordPosition="1259"> use FAQ data to learn how to expand query terms by answer terms. The SMT-based approaches presented in this paper can be seen as global query expansion techniques in that our question-answer translation model uses the whole question-answer corpus as information source, and our approach to paraphrasing deploys large amounts of bilingual phrases as highcoverage information source for synonym finding. Furthermore, both approaches take the entire query context into account when proposing to add new terms to the original query. The approaches that are closest to our models are the SMT approach of Radev et al. (2001) and the paraphrasing approach 465 web pages FAQ pages QA pairs count 4 billion 795,483 10,568,160 Table 1: Corpus statistics of QA pair data of Duboue and Chu-Carroll (2006). None of these approaches defines the problem of the lexical gap as a query expansion problem, and both approaches use much simpler SMT models than our systems, e.g., Radev et al. (2001) neglect to use a language model to aid disambiguation of translation choices, and Duboue and Chu-Carroll (2006) use SMT as black box altogether. In sum, our approach differs from previous work in QA and IR in the use SMT technology for qu</context>
</contexts>
<marker>Radev, Qi, Zheng, Blair-Goldensohn, Zhang, Fan, Prager, 2001</marker>
<rawString>Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha Blair-Goldensohn, Zhu Zhang, Weigo Fan, and John Prager. 2001. Mining the web for answers to natural language questions. In Proceedings of (CIKM’01), Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Eric Brill</author>
</authors>
<title>Automatic question answering using the web: Beyond the factoid.</title>
<date>2006</date>
<journal>Journal of Information Retrieval - Special Issue on Web Information Retrieval,</journal>
<pages>9--191</pages>
<contexts>
<context position="2031" citStr="Soricut and Brill, 2006" startWordPosition="307" endWordPosition="310">technology to close this gap. For example, syntactic information has been deployed to reformulate questions (Hermjakob et al., 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). In this paper we will concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages. Equivalently, thi</context>
<context position="6164" citStr="Soricut and Brill, 2006" startWordPosition="967" endWordPosition="971">formulation, including rule-based syntactic and semantic reformulation patterns (Hermjakob et al., 2002), reformulations based on shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al., 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). IR has approached the term mismatch problem by various approaches to query expansion (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). Inconclusive results have been reported for techniques that expand query terms separately by adding strongly related terms from an external thesaurus such as WordNet (Voorhees, 1994). Significant improvements in retrieval performance could be achieved by global expansion techniques that compute corpus-wide statistics and take the entire query, or query concept (Qiu and Frei, 1993), into account, or by local expansion techniques that select expansion t</context>
<context position="9170" citStr="Soricut and Brill (2006)" startWordPosition="1453" endWordPosition="1456">, and should be applicable in both areas even though experimental results are only given for the restricted domain of retrieval from FAQ pages. 3 Question-Answer Pairs from FAQ Pages Large-scale collection of question-answer pairs has been hampered in previous work by the small sizes of publicly available FAQ collections or by restricted access to retrieval results via public APIs of search engines. Jijkoun and de Rijke (2005) nevertheless managed to extract around 300,000 FAQ pages and 2.8 million question-answer pairs by repeatedly querying search engines with “intitle:faq” and “inurl:faq”. Soricut and Brill (2006) could deploy a proprietary URL collection of 1 billion URLs to extract 2.3 million FAQ pages containing the uncased string “faq” in the url string. The extraction of question-answer pairs amounted to a database of 1 million pairs in their experiment. However, inspection of the publicly available WebFAQ collection provided by Jijkoun and de Rijke2 showed a great amount of noise in the retrieved FAQ pages and question-answer pairs, and yet the indexed question-answer pairs showed a serious recall problem in that no answer could be retrieved for many well-formed queries. For our experiment, we d</context>
</contexts>
<marker>Soricut, Brill, 2006</marker>
<rawString>Radu Soricut and Eric Brill. 2006. Automatic question answering using the web: Beyond the factoid. Journal of Information Retrieval - Special Issue on Web Information Retrieval, 9:191–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Query expansion using lexical-semantic relations.</title>
<date>1994</date>
<booktitle>In Proceedings of SIGIR’94,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2364" citStr="Voorhees, 1994" startWordPosition="358" endWordPosition="359">r et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). In this paper we will concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages. Equivalently, this is a QA problem that concentrates on finding answers given FAQ documents that are known to contain the answers. Our approach to close the lexical gap in this setting attempts to marry QA and IR technology by deploying SMT methods for query expansion in answer retrieval. We present two approaches to SMT-based query expansion, both</context>
<context position="6266" citStr="Voorhees, 1994" startWordPosition="985" endWordPosition="986">ulations based on shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al., 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). IR has approached the term mismatch problem by various approaches to query expansion (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). Inconclusive results have been reported for techniques that expand query terms separately by adding strongly related terms from an external thesaurus such as WordNet (Voorhees, 1994). Significant improvements in retrieval performance could be achieved by global expansion techniques that compute corpus-wide statistics and take the entire query, or query concept (Qiu and Frei, 1993), into account, or by local expansion techniques that select expansion terms from the top ranked documents retrieved by the original query (Xu and Croft, 1996). A similar pic</context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>Ellen M. Voorhees. 1994. Query expansion using lexical-semantic relations. In Proceedings of SIGIR’94, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>W Bruce Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of SIGIR’96,</booktitle>
<location>Zurich, Switzerland.</location>
<contexts>
<context position="2405" citStr="Xu and Croft, 1996" startWordPosition="364" endWordPosition="367">01), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). In this paper we will concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages. Equivalently, this is a QA problem that concentrates on finding answers given FAQ documents that are known to contain the answers. Our approach to close the lexical gap in this setting attempts to marry QA and IR technology by deploying SMT methods for query expansion in answer retrieval. We present two approaches to SMT-based query expansion, both of which are implemented in the framewor</context>
<context position="5336" citStr="Xu and Croft, 1996" startWordPosition="836" endWordPosition="839">ur experimental evaluation we deploy a database of question-answer pairs extracted from FAQ pages for both training a question-answer translation model, and for a comparative evaluation of different systems on the task of answer retrieval. Retrieval is based on the tfidf framework of Jijkoun and de Rijke (2005), and query expansion is done straightforwardly by adding expansion terms to the query for a second retrieval cycle. We compare our global, context-aware query expansion techniques with Jijkoun and de Rijke’s (2005) tfidf model for answer retrieval and a local query expansion technique (Xu and Croft, 1996). Experimental results show a significant improvement of SMTbased query expansion over both baselines. 2 Related Work QA has approached the problem of the lexical gap by various techniques for question reformulation, including rule-based syntactic and semantic reformulation patterns (Hermjakob et al., 2002), reformulations based on shared dependency parses (Lin and Pantel, 2001), or various uses of the WordNet ontology to close the lexical gap word-by-word (Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Another use of natural language processing has been the deployment of SMT</context>
<context position="6851" citStr="Xu and Croft, 1996" startWordPosition="1073" endWordPosition="1076">to query expansion (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). Inconclusive results have been reported for techniques that expand query terms separately by adding strongly related terms from an external thesaurus such as WordNet (Voorhees, 1994). Significant improvements in retrieval performance could be achieved by global expansion techniques that compute corpus-wide statistics and take the entire query, or query concept (Qiu and Frei, 1993), into account, or by local expansion techniques that select expansion terms from the top ranked documents retrieved by the original query (Xu and Croft, 1996). A similar picture emerges for query expansion in QA: Mixed results have been reported for wordby-word expansion based on WordNet (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Considerable improvements have been reported for the use of the local context analysis model of Xu and Croft (1996) in the QA system of Ittycheriah et al. (2001), or for the systems of Agichtein et al. (2004) or Harabagiu and Lacatusu (2004) that use FAQ data to learn how to expand query terms by answer terms. The SMT-based approaches presented in this paper can be seen as global </context>
<context position="21056" citStr="Xu and Croft (1996)" startWordPosition="3335" endWordPosition="3338">equate (2) or material (1) answers; relative change in brackets. best list of query translations. Terms taken from query paraphrases were matched with the same field weight vector (0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3) as above. Terms taken from question-answer translation were matched with the weight vector (0.0,1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3), preferring answer fields over question fields. After stopword removal, the average number of expansion terms produced was 7.8 for paraphrasing, and 3.1 for question-answer translation. The local expansion technique used in our experiments follows Xu and Croft (1996) in taking expansion terms from the top n answers that were retrieved by the baseline tfidf system, and by incorporating cooccurrence information with query terms. This is done by calculating term frequencies for expansion terms by summing up the tfidf weights of the answers in which they occur, thus giving higher weight to terms that occur in answers that receive a higher similarity score to the original query. In our experiments, expansion terms are ranked according to this modified tfidf calculation over the top 20 answers retrieved by the baseline retrieval run, and matched a second time w</context>
<context position="26217" citStr="Xu and Croft, 1996" startWordPosition="4121" endWordPosition="4124">ent ways labor working child work job action unions Table 4: Examples for queries and expansion terms yielding improved (+), decreased (-), or unchanged (0) retrieval performance compared to retrieval without expansion. higher coverage. This makes MRR less adequate for the low-recall setup of FAQ retrieval. Table 3 shows success rates at 10 and 20 retrieved question-answer pairs for five different systems. The results for the baseline tfidf system, following Jijkoun and de Rijke (2005), are shown in row 2. Row 3 presents results for our variant of local expansion by pseudo-relevance feedback (Xu and Croft, 1996). Results for SMT-based expansion are given in row 4. A comparison of success rates for retrieving at least one adequate answer in the top 10 results shows relative improvements over the baseline of 11.1% for local query expansion, and of 40.7% for combined SMT-based expansion. Success rates at top 20 results show similar relative improvements of 14.2% for local query expansion, and of 22.8% for combined SMT-based expansion. On the easier task of retrieving a material or adequate answer, success rates drop by a small amount for local expansion, and stay unchanged for SMT-based expansion. These</context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>Jinxi Xu and W. Bruce Croft. 1996. Query expansion using local and global document analysis. In Proceedings of SIGIR’96, Zurich, Switzerland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>