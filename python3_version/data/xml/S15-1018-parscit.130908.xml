<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008033">
<title confidence="0.990445">
Event Extraction as Frame-Semantic Parsing
</title>
<author confidence="0.99731">
Alex Judea and Michael Strube
</author>
<affiliation confidence="0.988125">
Heidelberg Institute for Theoretical Studies gGmbH
</affiliation>
<address confidence="0.9518085">
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
</address>
<email confidence="0.921927">
(alex.judea|michael.strube)@h-its.org
</email>
<sectionHeader confidence="0.994384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931636363636">
Based on the hypothesis that frame-semantic
parsing and event extraction are structurally
identical tasks, we retrain SEMAFOR, a state-
of-the-art frame-semantic parsing system to
predict event triggers and arguments. We de-
scribe how we change SEMAFOR to be better
suited for the new task and show that it per-
forms comparable to one of the best systems
in event extraction. We also describe a bias in
one of its models and propose a feature factor-
ization which is better suited for this model.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987726923077">
Event Extraction is a task in information extraction
where mentions of predefined events are extracted
from texts. We follow the task definition of the Au-
tomatic Content Extraction (ACE) program of 2005.
It defines 33 event types, organized in eight cate-
gories. Each event type has associated roles, e.g.,
ATTACK has the roles attacker, target, and instru-
ment, whereas DIE has the roles agent, victim, and
instrument. The roles place and time are shared by
all event types.
ACE events occur only within sentences. Each
event is indicated by a word, the trigger. The roles
associated with the respective event type are filled
by zero or more arguments. Most arguments are
mentions of entities, e.g. persons, locations, or or-
ganizations. Some arguments are mentions of points
in time, amounts of money, etc. Arguments may
be shared by multiple events and may play different
roles in each of them.
Figure 1 illustrates an example. The sentence
contains two events, DIE and ATTACK, triggered by
“died” and “fired”, respectively. For DIE, the roles
victim, instrument, and place are filled with the argu-
ments “cameraman”, “American tank”, and “Bagh-
dad”, respectively. For ATTACK, the role target has
two arguments, namely “cameraman” and “Palestine
hotel”, the roles instrument, and place have the ar-
guments, “American tank”, and “Baghdad”, respec-
tively. Three arguments are shared. One of them,
“cameraman”, plays different roles in the events,
namely victim of DIE and target of ATTACK.
Frame-semantic parsing is the task of extracting
semantic predicate-argument structures from texts.
It is built on the theory of frame semantics and
FrameNet (Fillmore et al., 2003; Das et al., 2014).
As in event extraction, frames occur within sen-
tences and have triggers and roles (called lexical
units and frame elements).
Our hypothesis is that the two tasks are struc-
turally identical. From a computational point of
view, they differ only in feature types. We can
use the same approach and infrastructure to tackle
both. Based on this hypothesis, we retrain a frame-
semantic parsing system, SEMAFOR, for event ex-
traction.
We describe differences between frame-semantic
parsing and event extraction and the adaptions
needed to better prepare SEMAFOR for the new
task. We also describe a bias in the trigger classifi-
cation model which affects frame-semantic parsing
as well as event extraction and propose a new fac-
torization of features which is better suited for this
</bodyText>
<page confidence="0.986004">
159
</page>
<note confidence="0.561608">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 159–164,
Denver, Colorado, June 4–5, 2015.
</note>
<figure confidence="0.995945333333333">
place
target
place
instrument
In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.
DIE ATTACK
</figure>
<figureCaption confidence="0.996183">
Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event
triggered by “fired”. Three arguments are shared by both events.
</figureCaption>
<figure confidence="0.495025">
victim
instrument
target
</figure>
<bodyText confidence="0.977562">
model. Finally, we evaluate the retrained system on
the ACE 2005 data (Walker et al., 2006).
</bodyText>
<sectionHeader confidence="0.999711" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999626615384616">
Many approaches to event extraction do not cross
sentence boundaries, e.g. Grishman et al. (2005),
Ahn (2006), Lu and Roth (2012), Li et al. (2013)
and Li et al. (2014). Only few approaches, like Ji
and Grishman (2008) and Liao and Grishman (2010)
go beyond sentences and even beyond documents in
order to exploit richer context for the extraction of
events.
While early systems usually predict triggers and
arguments independently, more recent work em-
ploys joint inference, i.e., predicts triggers and argu-
ments (or only arguments) jointly, e.g., Lu and Roth
(2012), Li et al. (2013), and Li et al. (2014).
</bodyText>
<sectionHeader confidence="0.99557" genericHeader="conclusions">
3 Approach
</sectionHeader>
<bodyText confidence="0.999999166666667">
We make use of SEMAFOR, a state-of-the-art
frame-semantic parsing system (Das et al., 2010)1.
We retrain it to predict ACE events, i.e., triggers
with event types and arguments for their roles, and
make adaptions to better prepare it for event extrac-
tion. We call the new system SEMAFORE.
</bodyText>
<subsectionHeader confidence="0.9962">
3.1 Trigger Classification
</subsectionHeader>
<bodyText confidence="0.999977428571428">
In order to classify triggers (single or multiple to-
kens), the original SEMAFOR uses a log-linear
model. To cope with unknown triggers the model
includes a latent variable iterating over triggers seen
in training (called hidden units). At inference time,
hidden units serve as prototypes for unknown words.
The model is defined as
</bodyText>
<equation confidence="0.78402">
1: ei = argmax pθ(e, l  |ti, x). (1)
eEEi lELe
</equation>
<bodyText confidence="0.995866">
ei is the best event type for trigger ti according to
the model. Ei is the set of observed event types for
</bodyText>
<footnote confidence="0.905438666666667">
1http://www.ark.cs.cmu.edu/SEMAFOR/; we
use version 2.1, without semi-supervised extensions or dual
decomposition.
</footnote>
<bodyText confidence="0.9418714">
ti. Le is the set of triggers observed during train-
ing for event e. All l E Le are called hidden units.
pθ(e, l  |ti, x) gives the probability of e and a hid-
den unit l given the trigger ti and a sentence x. This
probability is modeled as
</bodyText>
<equation confidence="0.973018">
1
pθ(e, l  |ti, x) = Z exp θTg(e,l, ti, x). (2)
</equation>
<bodyText confidence="0.999712333333333">
This is a conditional log-linear model with a normal-
ization constant Z, weights θ, and a vector-valued
feature function g.
The model is biased towards classes with many
hidden units. In order to illustrate this, imagine there
is only one feature which does not depend on hid-
den units, e.g., if there is a named entity in the sen-
tence. During inference, the sum in Equation 1 is
computed. As a constant, Z is ignored during infer-
ence. The named entity feature would be active for
every hidden unit, having the same weight in every
iteration, because features are always evaluated in-
side the sum. Then, the sum is not meaningful any-
more, because the event with the most hidden units
wins. This bias affects both, frame-semantic parsing
and event extraction.
In order to weaken the bias we propose to separate
features which actually depend on hidden units, e.g.,
because they capture lexical similarity to some of
them, from features which do not, like the named
entity feature. Then, inference is performed as
</bodyText>
<equation confidence="0.909642666666667">
1: ei = argmax exp θTg&apos;(e,l, ti, x)
eEEi lELe
+ exp θTg*(e, ti, x).
</equation>
<bodyText confidence="0.999447444444444">
g&apos; is a function for features depending on hidden
units, g* is a function for the remaining features.
In this way, activation frequencies of features be-
come meaningful. However, the model is still bi-
ased towards events with many hidden units. This
is problematic, because the distribution of triggers
over events is diverse and arbitrary. The number
of hidden units does not necessarily correlate with
occurrence probabilities of events. On the other
</bodyText>
<equation confidence="0.50468">
(3)
</equation>
<page confidence="0.886113">
160
</page>
<bodyText confidence="0.987064527472527">
hand, the idea of known triggers being prototypes be to introduce a ‘null version’ of every event type,
for events is appealing, therefore we did not change having the same triggers. However, we would have
this part of the model. to predict twice as much classes (66 instead of 33).
3.2 Argument Classification Having only one null class better exploits the limited
The argument model predicts the best argument Ai training data. Furthermore, biasing SEMAFORE to-
for every role rk of an event ei given a set of spans S. wards null events is acceptable because there are
In our experiments, spans are extents of gold men- considerably more null events than events.
tions, including the empty span. The argument-role Allowing all triggers from the training data in
mapping is defined as prediction hurts performance, mainly due to trig-
Ai(rk) = argmax pψ(s I rk, ei, ti, x). (4) gers which coincide with high-frequency words like
sES “be”. In order to prune the trigger set we com-
Again, a conditional log-linear model with weights pute a score for each trigger, catching its distribution
ψ, a normalization constant Z, and a feature func- among events and non-events: s(t) = fe/(fe+fn)d.
tion h is used to model pψ: fe is the frequency of t as an event trigger, fn is the
pψ = 1 Z exp ψTh(s, rk, ei, ti, x). (5) frequency of t in non-events, and d is the number of
3.3 Adaptions events t is a trigger of. The measure prefers triggers
Based on our hypothesis that event extraction is which are frequently triggers for only a few events.
structurally identical to frame-semantic parsing, we We filter all triggers with s &lt; 0.012.
retrain SEMAFOR to predict ACE events. While the Finally, we changed the learning algorithm from
structure of the tasks may be identical, their behav- the maximum entropy to the perceptron framework.
ior is not. It does not suffice to convert the ACE data This was done because the perceptron gives better
to the right format and retrain the model. performance for SEMAFORE and is considerably
There are two important differences between faster, e.g., the argument model can be trained in
frame-semantic parsing and event extraction. First, a few seconds instead of several hours. The new
in frame-semantic parsing, there is no ‘null class’ models have a simpler form because we do not have
for triggers. A trigger may indicate multiple frames, to compute probabilities anymore. The new trigger
but it always invokes one of them. In event extrac- model is defined as
tion, we have potential triggers, which may or may 1: ei = argmax θTg1(e,l, ti, x)
not invoke events. Second, most event arguments are eEEi lELe (6)
defined based on entity types. ACE distinguishes be- + θTg?(e, ti, x).
tween the entity types person, organization, geopo- The new argument model is defined as
litical entity, location, facility, vehicle, and weapon. Ai(rk) = argmax ψTh(s, rk, ei, ti, x). (7)
For frame-semantic parsing, no such restriction in sES
entity type exists. Thus, we need to introduce en- Weights θ and ψ are learned using a variant of
tity type features to tackle argument classification the averaged perceptron (Collins, 2002), where we
for event extraction. Such features are also useful store feature vectors only after each pass through the
for the trigger model. training data.
One way to allow potential triggers to be classi- 3.4 Features
fied as non-triggers is to introduce a null class to the For the trigger model, SEMAFOR’s features include
event types. Each trigger in the training data also be- lemmas (of trigger tokens and of the head governor),
comes a trigger of the null class (or null event). If a dependencies of the head, if the head is equal to or
null event is triggered, we filter it out. Note that hav- has semantic relations with any hidden unit, as well
ing a class with so many triggers biases our model
towards it (Section 3.1). A less biased way would
161
2The threshold was determined on development data.
as the type of these relations3. Additionally, we in- of the same type: From “said [president [Obama]]”,
clude unigrams and bigrams around the trigger in a the inner span would be excluded.
window of two. Following Li et al. (2013), we also SEMAFORE’s recall is comparable to Li et al.
look at the mention nearest to the trigger. We in- (2013). However, their system gives a higher pre-
clude its entity type and its string representation as cision for both subtasks. We believe that the higher
features. precision of their argument model comes from the
Potential triggers are compared to hidden units by higher precision of their trigger model. Simi-
semantic relations. We extend this by incorporating larly, the lower precision of SEMAFORE’s argument
measures of semantic similarity. We compare tokens model is due to the lower precision of its trigger
in the actual sentence with tokens of all sentences model. Because of this, SEMAFORE is a few F1
the actual hidden unit appeared in (in the training points below Li et al. (2013).
data) and with tokens of all sentences all hidden We note that there is only a minor drop in per-
units of the actual frame appeared in. The compari- formance when comparing numbers for the develop-
son is made in terms of cosine similarity. ment and test sets. This indicates that SEMAFORE’s
SEMAFOR’s features for the argument model performance is robust.
characterize the actual span (its length, tokens, and The biggest error source for trigger classification
head dependencies), the voice and string represen- is missing triggers. The second biggest error source
tation of the trigger, and the dependency path be- is confusion of events with null events. Consider
tween span and trigger heads. Additionally, we in- the following example: “Saba hasn’t delivered yet”.
clude the token before the argument and its part-of- SEMAFORE predicted a null event for the trigger
speech, and all tokens and parts-of-speech between “delivered” instead of the right BE-BORN event. The
argument and trigger as features. Following Li et al. context it had to analyze did not suffice to overcome
(2013), we also use as features the type of the entity its bias towards null events. Even for humans it
the actual span represents, if it is the only mention seems hard to infer the right event type here. One
of its entity type, or the nearest to the trigger. would need to know that “Saba” refers to a preg-
4 Experiments nant woman, which could be inferred from the docu-
We trained SEMAFORE on the English ACE 2005 ment. However, the sentence alone does not provide
data. We followed Li et al. (2014) and removed enough information.
the two smallest and most informal parts of the The biggest error source for argument classifica-
data, namely ‘conversational telephone speech’ and tion is error propagation from the trigger model. The
‘Usenet newsgroups’. From the remaining 511 doc- second major error source is the local prediction of
uments, 351 are used for training, 80 for develop- arguments. It seems better to predict triggers and
ment, and 80 for testing. arguments jointly in order to weaken error propaga-
We follow standard evaluation procedures for tion (Li et al., 2013; Li et al., 2014). For example,
triggers and events (Ji and Grishman, 2008). A trig- SEMAFORE finds a START-ORG event for the trig-
ger is correct, if its span and event type match a ref- ger “set up” in the following sentence: “At the site,
erence trigger. An argument is correct, if its span, equipment has been set up to test conventional ex-
event type, and role match a reference argument. plosives [... ]”. In such cases, the model would need
Table 1 summarizes results for SEMAFORE and to know that the argument “equipment” cannot fill
a state-of-the-art system for event extraction (Li et the org role of START-ORG because it is no organiza-
al., 2013). To make a fair comparison, we report tion. Inferring triggers and arguments jointly would
the numbers of their pipeline version, i.e., predicting enable SEMAFORE to better prevent such errors.
trigger and arguments sequentially, as we do. Both 5 Conclusions and Future Work
systems use gold mentions and gold entity types. Based on the hypothesis that frame-semantic pars-
For SEMAFORE, we excluded all nested mentions ing and event extraction are structurally identical,
</bodyText>
<table confidence="0.98013775">
we retrained a state-of-the-art frame-semantic pars-
3Semantic relations come from WordNet (Fellbaum, 1998)
162
Triggers Arguments
P R F1 P R F1
SEMAFORE dev 65.8 57.8 61.6 57.0 32.4 41.3
SEMAFORE test 62.6 56.8 60.0 53.5 33.3 41.0
Li et al. (2013) 74.5 59.1 65.9 65.4 33.1 43.9
</table>
<tableCaption confidence="0.945052">
Table 1: Evaluation results for SEMAFORE on the development and test sets compared to a state-of-the-art
system.
</tableCaption>
<bodyText confidence="0.985604666666667">
ing system for event extraction. We presented the
adaptions in prediction classes and features needed
to make the system better suited for the more restric-
tive task of event extraction. We also described a
bias in the trigger classification model and proposed
a feature factorization which is better suited for this
model. As the evaluation shows, the retrained sys-
tem can rival the state-of-the-art in event extraction.
For future work, we plan to incorporate men-
tion detection into SEMAFORE . SEMAFOR’s seg-
mentation approach is not suited for event extrac-
tion because it produces too many argument candi-
dates. Furthermore, error analysis and evaluation
suggest that we need to predict triggers and argu-
ments jointly. We also plan to go beyond sentences
and search for larger contexts which may be rele-
vant for event extraction. These changes may also
be beneficial for frame-semantic parsing.
</bodyText>
<sectionHeader confidence="0.997638" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999949714285714">
This work has been funded by the Klaus Tschira
Foundation, Heidelberg, Germany. The first author
has been supported by a HITS Ph.D. scholarship.
We would like to thank our colleagues Sebastian
Martschat, Daraksha Parveen, Nafise Moosavi, Yu-
fang Hou and Mohsen Mesgar who commented on
earlier drafts of this paper.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999589038461538">
David Ahn. 2006. The stages of event extraction. In
Proceedings of the Workshop on Annotating and Rea-
soning about Time and Events, Sydney, Australia, 23
July 2006, pages 1–8.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing, Philadelphia, Penn., 6–7 July
2002, pages 1–8.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In Proceedings of Human Language Tech-
nologies 2010: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Los Angeles, Cal., 2–4 June 2010, pages 948–
956.
Dipanjan Das, Desai Chen, Andr´e F. T. Martins, Nathan
Schneider, and Noah A. Smith. 2014. Frame-semantic
parsing. Computational Linguistics, 40(1):9–56.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Charles J. Fillmore, Christopher R. Johnson, and Miriam
R. L. Petruck. 2003. Background to FrameNet. Inter-
national Journal of Lexicography, 16(3):235–250.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. NYU’s English ACE 2005 system description.
Technical report, Department of Computer Science,
New York University, New York, N.Y.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Columbus, Ohio, 15–20 June
2008, pages 254–262.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), Sofia, Bulgaria, 4–9 August
2013, pages 73–82.
Qi Li, Heng Ji, Yu Heng, and Sujian Li. 2014. Construct-
ing information networks using one single model.
In Proceedings of the 2014 Conference on Empiri-
cal Methods in Natural Language Processing, Doha,
Qatar, 25–29 October 2014, pages 1846–1851.
Shasha Liao and Ralph Grishman. 2010. Using docu-
ment level cross-event inference to improve event ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, Up-
psala, Sweden, 11–16 July 2010, pages 789–797.
</reference>
<page confidence="0.988624">
163
</page>
<reference confidence="0.996294111111111">
Wei Lu and Dan Roth. 2012. Automatic event extrac-
tion with structured preference modeling. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
Jeju Island, Korea, 8–14 July 2012, pages 835–844.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Maeda Kazuaki. 2006. ACE 2005 multilingual
training corpus. LDC2006T06, Philadelphia, Penn.:
Linguistic Data Consortium.
</reference>
<page confidence="0.99852">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.426947">
<title confidence="0.999052">Event Extraction as Frame-Semantic Parsing</title>
<author confidence="0.752223">Judea</author>
<affiliation confidence="0.713448">Heidelberg Institute for Theoretical Studies Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.999532">69118 Heidelberg, Germany</address>
<email confidence="0.999102">(alex.judea|michael.strube)@h-its.org</email>
<abstract confidence="0.997748333333333">Based on the hypothesis that frame-semantic parsing and event extraction are structurally identical tasks, we retrain SEMAFOR, a stateof-the-art frame-semantic parsing system to predict event triggers and arguments. We describe how we change SEMAFOR to be better suited for the new task and show that it performs comparable to one of the best systems in event extraction. We also describe a bias in one of its models and propose a feature factorization which is better suited for this model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Ahn</author>
</authors>
<title>The stages of event extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Annotating and Reasoning about Time and Events,</booktitle>
<pages>1--8</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="3873" citStr="Ahn (2006)" startWordPosition="610" endWordPosition="611">Semantics (*SEM 2015), pages 159–164, Denver, Colorado, June 4–5, 2015. place target place instrument In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. victim instrument target model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010</context>
</contexts>
<marker>Ahn, 2006</marker>
<rawString>David Ahn. 2006. The stages of event extraction. In Proceedings of the Workshop on Annotating and Reasoning about Time and Events, Sydney, Australia, 23 July 2006, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, Penn.,</location>
<contexts>
<context position="10263" citStr="Collins, 2002" startWordPosition="1702" endWordPosition="1703">may or may 1: ei = argmax θTg1(e,l, ti, x) not invoke events. Second, most event arguments are eEEi lELe (6) defined based on entity types. ACE distinguishes be- + θTg?(e, ti, x). tween the entity types person, organization, geopo- The new argument model is defined as litical entity, location, facility, vehicle, and weapon. Ai(rk) = argmax ψTh(s, rk, ei, ti, x). (7) For frame-semantic parsing, no such restriction in sES entity type exists. Thus, we need to introduce en- Weights θ and ψ are learned using a variant of tity type features to tackle argument classification the averaged perceptron (Collins, 2002), where we for event extraction. Such features are also useful store feature vectors only after each pass through the for the trigger model. training data. One way to allow potential triggers to be classi- 3.4 Features fied as non-triggers is to introduce a null class to the For the trigger model, SEMAFOR’s features include event types. Each trigger in the training data also be- lemmas (of trigger tokens and of the head governor), comes a trigger of the null class (or null event). If a dependencies of the head, if the head is equal to or null event is triggered, we filter it out. Note that hav</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, Philadelphia, Penn., 6–7 July 2002, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic frame-semantic parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies 2010: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>948--956</pages>
<location>Los Angeles, Cal.,</location>
<contexts>
<context position="4474" citStr="Das et al., 2010" startWordPosition="708" endWordPosition="711">005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1. We retrain it to predict ACE events, i.e., triggers with event types and arguments for their roles, and make adaptions to better prepare it for event extraction. We call the new system SEMAFORE. 3.1 Trigger Classification In order to classify triggers (single or multiple tokens), the original SEMAFOR uses a log-linear model. To cope with unknown triggers the model includes a latent variable iterating over triggers seen in training (called hidden units). At inference time, hidden units serve as prototypes for unknown words. The model is defined as 1: ei = argmax pθ(e, l |ti, x). (1) eEEi lEL</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic frame-semantic parsing. In Proceedings of Human Language Technologies 2010: The Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, Cal., 2–4 June 2010, pages 948– 956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Desai Chen</author>
<author>Andr´e F T Martins</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Frame-semantic parsing.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="2414" citStr="Das et al., 2014" startWordPosition="373" endWordPosition="376">d place are filled with the arguments “cameraman”, “American tank”, and “Baghdad”, respectively. For ATTACK, the role target has two arguments, namely “cameraman” and “Palestine hotel”, the roles instrument, and place have the arguments, “American tank”, and “Baghdad”, respectively. Three arguments are shared. One of them, “cameraman”, plays different roles in the events, namely victim of DIE and target of ATTACK. Frame-semantic parsing is the task of extracting semantic predicate-argument structures from texts. It is built on the theory of frame semantics and FrameNet (Fillmore et al., 2003; Das et al., 2014). As in event extraction, frames occur within sentences and have triggers and roles (called lexical units and frame elements). Our hypothesis is that the two tasks are structurally identical. From a computational point of view, they differ only in feature types. We can use the same approach and infrastructure to tackle both. Based on this hypothesis, we retrain a framesemantic parsing system, SEMAFOR, for event extraction. We describe differences between frame-semantic parsing and event extraction and the adaptions needed to better prepare SEMAFOR for the new task. We also describe a bias in t</context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2014</marker>
<rawString>Dipanjan Das, Desai Chen, Andr´e F. T. Martins, Nathan Schneider, and Noah A. Smith. 2014. Frame-semantic parsing. Computational Linguistics, 40(1):9–56.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="2395" citStr="Fillmore et al., 2003" startWordPosition="369" endWordPosition="372"> victim, instrument, and place are filled with the arguments “cameraman”, “American tank”, and “Baghdad”, respectively. For ATTACK, the role target has two arguments, namely “cameraman” and “Palestine hotel”, the roles instrument, and place have the arguments, “American tank”, and “Baghdad”, respectively. Three arguments are shared. One of them, “cameraman”, plays different roles in the events, namely victim of DIE and target of ATTACK. Frame-semantic parsing is the task of extracting semantic predicate-argument structures from texts. It is built on the theory of frame semantics and FrameNet (Fillmore et al., 2003; Das et al., 2014). As in event extraction, frames occur within sentences and have triggers and roles (called lexical units and frame elements). Our hypothesis is that the two tasks are structurally identical. From a computational point of view, they differ only in feature types. We can use the same approach and infrastructure to tackle both. Based on this hypothesis, we retrain a framesemantic parsing system, SEMAFOR, for event extraction. We describe differences between frame-semantic parsing and event extraction and the adaptions needed to better prepare SEMAFOR for the new task. We also d</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Charles J. Fillmore, Christopher R. Johnson, and Miriam R. L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16(3):235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>David Westbrook</author>
<author>Adam Meyers</author>
</authors>
<title>NYU’s English ACE</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, New York University,</institution>
<location>New York, N.Y.</location>
<contexts>
<context position="3861" citStr="Grishman et al. (2005)" startWordPosition="606" endWordPosition="609">xical and Computational Semantics (*SEM 2015), pages 159–164, Denver, Colorado, June 4–5, 2015. place target place instrument In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. victim instrument target model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das </context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>Ralph Grishman, David Westbrook, and Adam Meyers. 2005. NYU’s English ACE 2005 system description. Technical report, Department of Computer Science, New York University, New York, N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through cross-document inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>254--262</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3982" citStr="Ji and Grishman (2008)" startWordPosition="629" endWordPosition="632">ment In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. victim instrument target model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1. We retrain it to predict ACE events, i.e., triggers with event types and arguments for their roles, and m</context>
<context position="14341" citStr="Ji and Grishman, 2008" startWordPosition="2390" endWordPosition="2393">he two smallest and most informal parts of the The biggest error source for argument classificadata, namely ‘conversational telephone speech’ and tion is error propagation from the trigger model. The ‘Usenet newsgroups’. From the remaining 511 doc- second major error source is the local prediction of uments, 351 are used for training, 80 for develop- arguments. It seems better to predict triggers and ment, and 80 for testing. arguments jointly in order to weaken error propagaWe follow standard evaluation procedures for tion (Li et al., 2013; Li et al., 2014). For example, triggers and events (Ji and Grishman, 2008). A trig- SEMAFORE finds a START-ORG event for the trigger is correct, if its span and event type match a ref- ger “set up” in the following sentence: “At the site, erence trigger. An argument is correct, if its span, equipment has been set up to test conventional exevent type, and role match a reference argument. plosives [... ]”. In such cases, the model would need Table 1 summarizes results for SEMAFORE and to know that the argument “equipment” cannot fill a state-of-the-art system for event extraction (Li et the org role of START-ORG because it is no organizaal., 2013). To make a fair comp</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Columbus, Ohio, 15–20 June 2008, pages 254–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>73--82</pages>
<location>Sofia, Bulgaria, 4–9</location>
<contexts>
<context position="3911" citStr="Li et al. (2013)" startWordPosition="616" endWordPosition="619">–164, Denver, Colorado, June 4–5, 2015. place target place instrument In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. victim instrument target model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1. We retrain it to predict ACE event</context>
<context position="11305" citStr="Li et al. (2013)" startWordPosition="1883" endWordPosition="1886">vernor), comes a trigger of the null class (or null event). If a dependencies of the head, if the head is equal to or null event is triggered, we filter it out. Note that hav- has semantic relations with any hidden unit, as well ing a class with so many triggers biases our model towards it (Section 3.1). A less biased way would 161 2The threshold was determined on development data. as the type of these relations3. Additionally, we in- of the same type: From “said [president [Obama]]”, clude unigrams and bigrams around the trigger in a the inner span would be excluded. window of two. Following Li et al. (2013), we also SEMAFORE’s recall is comparable to Li et al. look at the mention nearest to the trigger. We in- (2013). However, their system gives a higher preclude its entity type and its string representation as cision for both subtasks. We believe that the higher features. precision of their argument model comes from the Potential triggers are compared to hidden units by higher precision of their trigger model. Simisemantic relations. We extend this by incorporating larly, the lower precision of SEMAFORE’s argument measures of semantic similarity. We compare tokens model is due to the lower prec</context>
<context position="14265" citStr="Li et al., 2013" startWordPosition="2377" endWordPosition="2380"> data. We followed Li et al. (2014) and removed enough information. the two smallest and most informal parts of the The biggest error source for argument classificadata, namely ‘conversational telephone speech’ and tion is error propagation from the trigger model. The ‘Usenet newsgroups’. From the remaining 511 doc- second major error source is the local prediction of uments, 351 are used for training, 80 for develop- arguments. It seems better to predict triggers and ment, and 80 for testing. arguments jointly in order to weaken error propagaWe follow standard evaluation procedures for tion (Li et al., 2013; Li et al., 2014). For example, triggers and events (Ji and Grishman, 2008). A trig- SEMAFORE finds a START-ORG event for the trigger is correct, if its span and event type match a ref- ger “set up” in the following sentence: “At the site, erence trigger. An argument is correct, if its span, equipment has been set up to test conventional exevent type, and role match a reference argument. plosives [... ]”. In such cases, the model would need Table 1 summarizes results for SEMAFORE and to know that the argument “equipment” cannot fill a state-of-the-art system for event extraction (Li et the or</context>
<context position="15638" citStr="Li et al. (2013)" startWordPosition="2606" endWordPosition="2609">s of their pipeline version, i.e., predicting enable SEMAFORE to better prevent such errors. trigger and arguments sequentially, as we do. Both 5 Conclusions and Future Work systems use gold mentions and gold entity types. Based on the hypothesis that frame-semantic parsFor SEMAFORE, we excluded all nested mentions ing and event extraction are structurally identical, we retrained a state-of-the-art frame-semantic pars3Semantic relations come from WordNet (Fellbaum, 1998) 162 Triggers Arguments P R F1 P R F1 SEMAFORE dev 65.8 57.8 61.6 57.0 32.4 41.3 SEMAFORE test 62.6 56.8 60.0 53.5 33.3 41.0 Li et al. (2013) 74.5 59.1 65.9 65.4 33.1 43.9 Table 1: Evaluation results for SEMAFORE on the development and test sets compared to a state-of-the-art system. ing system for event extraction. We presented the adaptions in prediction classes and features needed to make the system better suited for the more restrictive task of event extraction. We also described a bias in the trigger classification model and proposed a feature factorization which is better suited for this model. As the evaluation shows, the retrained system can rival the state-of-the-art in event extraction. For future work, we plan to incorpo</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Sofia, Bulgaria, 4–9 August 2013, pages 73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Yu Heng</author>
<author>Sujian Li</author>
</authors>
<title>Constructing information networks using one single model.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1846--1851</pages>
<location>Doha,</location>
<contexts>
<context position="3932" citStr="Li et al. (2014)" startWordPosition="621" endWordPosition="624">o, June 4–5, 2015. place target place instrument In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. victim instrument target model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1. We retrain it to predict ACE events, i.e., triggers wit</context>
<context position="13685" citStr="Li et al. (2014)" startWordPosition="2285" endWordPosition="2288">ent. The argument and trigger as features. Following Li et al. context it had to analyze did not suffice to overcome (2013), we also use as features the type of the entity its bias towards null events. Even for humans it the actual span represents, if it is the only mention seems hard to infer the right event type here. One of its entity type, or the nearest to the trigger. would need to know that “Saba” refers to a preg4 Experiments nant woman, which could be inferred from the docuWe trained SEMAFORE on the English ACE 2005 ment. However, the sentence alone does not provide data. We followed Li et al. (2014) and removed enough information. the two smallest and most informal parts of the The biggest error source for argument classificadata, namely ‘conversational telephone speech’ and tion is error propagation from the trigger model. The ‘Usenet newsgroups’. From the remaining 511 doc- second major error source is the local prediction of uments, 351 are used for training, 80 for develop- arguments. It seems better to predict triggers and ment, and 80 for testing. arguments jointly in order to weaken error propagaWe follow standard evaluation procedures for tion (Li et al., 2013; Li et al., 2014). </context>
</contexts>
<marker>Li, Ji, Heng, Li, 2014</marker>
<rawString>Qi Li, Heng Ji, Yu Heng, and Sujian Li. 2014. Constructing information networks using one single model. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, 25–29 October 2014, pages 1846–1851.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Using document level cross-event inference to improve event extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>789--797</pages>
<location>Uppsala,</location>
<contexts>
<context position="4011" citStr="Liao and Grishman (2010)" startWordPosition="634" endWordPosition="637">n died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. victim instrument target model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1. We retrain it to predict ACE events, i.e., triggers with event types and arguments for their roles, and make adaptions to better prepa</context>
</contexts>
<marker>Liao, Grishman, 2010</marker>
<rawString>Shasha Liao and Ralph Grishman. 2010. Using document level cross-event inference to improve event extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11–16 July 2010, pages 789–797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Dan Roth</author>
</authors>
<title>Automatic event extraction with structured preference modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jeju Island, Korea, 8–14</booktitle>
<pages>835--844</pages>
<contexts>
<context position="3893" citStr="Lu and Roth (2012)" startWordPosition="612" endWordPosition="615">SEM 2015), pages 159–164, Denver, Colorado, June 4–5, 2015. place target place instrument In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. victim instrument target model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1. We retrain it to</context>
</contexts>
<marker>Lu, Roth, 2012</marker>
<rawString>Wei Lu and Dan Roth. 2012. Automatic event extraction with structured preference modeling. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jeju Island, Korea, 8–14 July 2012, pages 835–844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Walker</author>
<author>Stephanie Strassel</author>
<author>Julie Medero</author>
<author>Maeda Kazuaki</author>
</authors>
<title>multilingual training corpus. LDC2006T06,</title>
<date>2006</date>
<publisher>ACE</publisher>
<institution>Linguistic Data Consortium.</institution>
<location>Philadelphia, Penn.:</location>
<contexts>
<context position="3747" citStr="Walker et al., 2006" startWordPosition="588" endWordPosition="591">w factorization of features which is better suited for this 159 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 159–164, Denver, Colorado, June 4–5, 2015. place target place instrument In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. victim instrument target model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013</context>
</contexts>
<marker>Walker, Strassel, Medero, Kazuaki, 2006</marker>
<rawString>Christopher Walker, Stephanie Strassel, Julie Medero, and Maeda Kazuaki. 2006. ACE 2005 multilingual training corpus. LDC2006T06, Philadelphia, Penn.: Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>