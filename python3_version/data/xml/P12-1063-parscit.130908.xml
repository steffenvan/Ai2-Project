<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000536">
<title confidence="0.982309">
Computational Approaches to Sentence Completion
</title>
<author confidence="0.938889333333333">
Geoffrey Zweig, John C. Platt
Christopher Meek
Christopher J.C. Burges
</author>
<affiliation confidence="0.852297">
Microsoft Research
</affiliation>
<address confidence="0.713635">
Redmond, WA 98052
</address>
<author confidence="0.878034">
Ainur Yessenalina
</author>
<affiliation confidence="0.782554333333333">
Cornell University
Computer Science Dept.
Ithaca, NY 14853
</affiliation>
<author confidence="0.995802">
Qiang Liu
</author>
<affiliation confidence="0.799966">
Univ. of California, Irvine
Info. &amp; Comp. Sci.
Irvine, California 92697
</affiliation>
<sectionHeader confidence="0.980173" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999091555555556">
This paper studies the problem of sentence-
level semantic coherence by answering SAT-
style sentence completion questions. These
questions test the ability of algorithms to dis-
tinguish sense from nonsense based on a vari-
ety of sentence-level phenomena. We tackle
the problem with two approaches: methods
that use local lexical information, such as the
n-grams of a classical language model; and
methods that evaluate global coherence, such
as latent semantic analysis. We evaluate these
methods on a suite of practice SAT questions,
and on a recently released sentence comple-
tion task based on data taken from five Conan
Doyle novels. We find that by fusing local
and global information, we can exceed 50%
on this task (chance baseline is 20%), and we
suggest some avenues for further research.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998590021276596">
In recent years, standardized examinations have
proved a fertile source of evaluation data for lan-
guage processing tasks. They are valuable for many
reasons: they represent facets of language under-
standing recognized as important by educational ex-
perts; they are organized in various formats designed
to evaluate specific capabilities; they are yardsticks
by which society measures educational progress;
and they affect a large number of people.
Previous researchers have taken advantage of this
material to test both narrow and general language
processing capabilities. Among the narrower tasks,
the identification of synonyms and antonyms has
been studied by (Landauer and Dumais, 1997; Mo-
hammed et al., 2008; Mohammed et al., 2011; Tur-
ney et al., 2003; Turney, 2008), who used ques-
tions from the Test of English as a Foreign Lan-
guage (TOEFL), Graduate Record Exams (GRE)
and English as a Second Language (ESL) exams.
Tasks requiring broader competencies include logic
puzzles and reading comprehension. Logic puzzles
drawn from the Law School Administration Test
(LSAT) and the GRE were studied in (Lev et al.,
2004), which combined an extensive array of tech-
niques to solve the problems. The DeepRead sys-
tem (Hirschman et al., 1999) initiated a long line of
research into reading comprehension based on test
prep material (Charniak et al., 2000; Riloff and The-
len, 2000; Wang et al., 2000; Ng et al., 2000).
In this paper, we study a new class of problems
intermediate in difficulty between the extremes of
synonym detection and general question answer-
ing - the sentence completion questions found on
the Scholastic Aptitude Test (SAT). These questions
present a sentence with one or two blanks that need
to be filled in. Five possible words (or short phrases)
are given as options for each blank. All possible an-
swers except one result in a nonsense sentence. Two
examples are shown in Figure 1.
The questions are highly constrained in the sense
that all the information necessary is present in the
sentence itself without any other context. Neverthe-
less, they vary widely in difficulty. The first of these
examples is relatively simple: the second half of the
sentence is a clear description of the type of behavior
characterized by the desired adjective. The second
example is more sophisticated; one must infer from
</bodyText>
<page confidence="0.96709">
601
</page>
<note confidence="0.9829075">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 601–610,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.911942846153846">
1. One of the characters in Milton Murayama’s
novel is considered because he deliber-
ately defies an oppressive hierarchical society.
(A) rebellious (B) impulsive (C) artistic (D)
industrious (E) tyrannical
2. Whether substances are medicines or poisons
often depends on dosage, for substances that are
in small doses can be in large.
(A) useless.. effective
(B) mild.. benign
(C) curative.. toxic
(D) harmful .. fatal
(E) beneficial.. miraculous
</figure>
<figureCaption confidence="0.9970405">
Figure 1: Sample sentence completion questions
(Educational-Testing-Service, 2011).
</figureCaption>
<bodyText confidence="0.9999363">
the contrast between medicine and poison that the
correct answer involves a contrast, either useless vs.
effective or curative vs. toxic. Moreover, the first, in-
correct, possibility is perfectly acceptable in the con-
text of the second clause alone; only irrelevance to
the contrast between medicine and poison eliminates
it. In general, the questions require a combination of
semantic and world knowledge as well as occasional
logical reasoning. We study the sentence comple-
tion task because we believe it is complex enough to
pose a significant challenge, yet structured enough
that progress may be possible.
As a first step, we have approached the prob-
lem from two points-of-view: first by exploiting lo-
cal sentence structure, and secondly by measuring
a novel form of global sentence coherence based
on latent semantic analysis. To investigate the use-
fulness of local information, we evaluated n-gram
language model scores, from both a conventional
model with Good-Turing smoothing, and with a re-
cently proposed maximum-entropy class-based n-
gram model (Chen, 2009a; Chen, 2009b). Also
in the language modeling vein, but with potentially
global context, we evaluate the use of a recurrent
neural network language model. In all the language
modeling approaches, a model is used to compute a
sentence probability with each of the potential com-
pletions. To measure global coherence, we propose
a novel method based on latent semantic analysis
(LSA). We find that the LSA based method performs
best, and that both local and global information can
be combined to exceed 50% accuracy. We report re-
sults on a set of questions taken from a collection
of SAT practice exams (Princeton-Review, 2010),
and further validate the methods with the recently
proposed MSR Sentence Completion Challenge set
(Zweig and Burges, 2011).
Our paper thus makes the following contributions:
First, we present the first published results on the
SAT sentence completion task. Secondly, we eval-
uate the effectiveness of both local n-gram informa-
tion, and global coherence in the form of a novel
LSA-based metric. Finally, we illustrate that the lo-
cal and global information can be effectively fused.
The remainder of this paper is organized as fol-
lows. In Section 2 we discuss related work. Section
3 describes the language modeling methods we have
evaluated. Section 4 outlines the LSA-based meth-
ods. Section 5 presents our experimental results. We
conclude with a discussion in Section 6.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999989958333333">
The past work which is most similar to ours is de-
rived from the lexical substitution track of SemEval-
2007 (McCarthy and Navigli, 2007). In this task,
the challenge is to find a replacement for a word or
phrase removed from a sentence. In contrast to our
SAT-inspired task, the original answer is indicated.
For example, one might be asked to find alternates
for match in “After the match, replace any remain-
ing fluid deficit to prevent problems of chronic de-
hydration throughout the tournament.” Two consis-
tently high-performing systems for this task are the
KU (Yuret, 2007) and UNT (Hassan et al., 2007)
systems. These operate in two phases: first they find
a set of potential replacement words, and then they
rank them. The KU system uses just an N-gram lan-
guage model to do this ranking. The UNT system
uses a large variety of information sources, and a
language model score receives the highest weight.
N-gram statistics were also very effective in (Giu-
liano et al., 2007). That paper also explores the use
of Latent Semantic Analysis to measure the degree
of similarity between a potential replacement and its
context, but the results are poorer than others. Since
the original word provides a strong hint as to the pos-
</bodyText>
<page confidence="0.99777">
602
</page>
<bodyText confidence="0.999831863636364">
sible meanings of the replacements, we hypothesize
that N-gram statistics are largely able to resolve the
remaining ambiguities. The SAT sentence comple-
tion sentences do not have this property and thus are
more challenging.
Related to, but predating the Semeval lexical sub-
stitution task are the ESL synonym questions pro-
posed by Turney (2001), and subsequently consid-
ered by numerous research groups including Terra
and Clarke (2003) and Pado and Lapata (2007).
These questions are similar to the SemEval task, but
in addition to the original word and the sentence
context, the list of options is provided. Jarmasz and
Szpakowicz (2003) used a sophisticated thesaurus-
based method and achieved state-of-the art perfor-
mance, which is 82%.
Other work on standardized tests includes the syn-
onym and antonym tasks mentioned in Section 1,
and more recent work on a SAT analogy task in-
troduced by (Turney et al., 2003) and extensively
used by other researchers (Veale, 2004; Turney and
Littman, 2005; D. et al., 2009).
</bodyText>
<sectionHeader confidence="0.8872375" genericHeader="method">
3 Sentence Completion via Language
Modeling
</sectionHeader>
<bodyText confidence="0.999972033333333">
Perhaps the most straightforward approach to solv-
ing the sentence completion task is to form the com-
plete sentence with each option in turn, and to eval-
uate its likelihood under a language model. As
discussed in Section 2, this was found be be very
effective in the ranking phase of several SemEval
systems. In this section, we describe the suite of
state-of-the-art language modeling techniques for
which we will present results. We begin with n-
gram models; first a classical n-gram backoff model
(Chen and Goodman, 1999), and then a recently pro-
posed class-based maximum-entropy n-gram model
(Chen, 2009a; Chen, 2009b). N-gram models have
the obvious disadvantage of using a very limited
context in predicting word probabilities. There-
fore we evaluate the recurrent neural net model of
(Mikolov et al., 2010; Mikolov et al., 2011b). This
model has produced record-breaking perplexity re-
sults in several tasks (Mikolov et al., 2011a), and has
the potential to encode sentence-span information in
the network hidden-layer activations. We have also
evaluated the use of parse scores, using an off-the-
shelf stochastic context free grammar parser. How-
ever, the grammatical structure of the alternatives is
often identical. With scores differing only in the fi-
nal non-terminal/terminal rewrites, this did little bet-
ter than chance. The use of other syntactically de-
rived features, for example based on a dependency
parse, are likely to be more effective, but we leave
this for future work.
</bodyText>
<subsectionHeader confidence="0.999697">
3.1 Backoff N-gram Language Model
</subsectionHeader>
<bodyText confidence="0.999982375">
Our baseline model is a Good-Turing smoothed
model trained with the CMU language modeling
toolkit (Clarkson and Rosenfeld, 1997). For the SAT
task, we used a trigram language model trained on
1.1B words of newspaper data, described in Section
5.1. All bigrams occurring at least twice were re-
tained in the model, along with all trigrams occur-
ring at least three times. The vocabulary consisted
of all words occurring at least 100 times in the data,
along with every word in the development or test
sets. This resulted in a 124k word vocabulary and
59M n-grams. For the Conan Doyle data, which we
henceforth refer to as the Holmes data (see Section
5.1), the smaller amount of training data allowed us
to use 4-grams and a vocabulary cutoff of 3. This re-
sulted in 26M n-grams and a 126k word vocabulary.
</bodyText>
<subsectionHeader confidence="0.9998775">
3.2 Maximum Entropy Class-Based N-gram
Language Model
</subsectionHeader>
<bodyText confidence="0.9999846">
Word-class information provides a level of abstrac-
tion which is not available in a word-level lan-
guage model; therefore we evaluated a state-of-the-
art class based language model. Model M (Chen,
2009a; Chen, 2009b) is a recently proposed class
based exponential n-gram language model which
has shown improvements across a variety of tasks
(Chen, 2009b; Chen et al., 2009; Emami et al.,
2010). The key ideas are the modeling of word n-
gram probabilities with a maximum entropy model,
and the use of word-class information in the defini-
tion of the features. In particular, each word w is
assigned deterministically to a class c, allowing the
n-gram probabilities to be estimated as the product
of class and word parts
</bodyText>
<equation confidence="0.997856">
P(wi|wi−n+1 ... wi−2wi−1) _
P(ci|ci−n+1 ... ci−2ci−17 wi−n+1 ... wi−2wi−1)
P(wi|wi−n+1 ... wi−2wi−17 ci).
</equation>
<page confidence="0.976222">
603
</page>
<bodyText confidence="0.93613525">
Both components are themselves maximum entropy
n-gram models in which the probability of a word
or class label l given history h is determined by
� exp(�
1 k fk(h, l)). The features fk(h, l) used are
the presence of various patterns in the concatena-
tion of hl, for example whether a particular suffix
is present in hl.
</bodyText>
<subsectionHeader confidence="0.99989">
3.3 Recurrent Neural Net Language Model
</subsectionHeader>
<bodyText confidence="0.999373421052632">
Many of the questions involve long-range depen-
dencies between words. While n-gram models have
no ability to explicitly maintain long-span context,
the recently proposed recurrent neural-net model of
(Mikolov et al., 2010) does. Related approaches
have been proposed by (Sutskever et al., 2011;
Socher et al., 2011). In this model, a set of neu-
ral net activations s(t) is maintained and updated at
each sentence position t. These activations encapsu-
late the sentence history up to the tth word in a real-
valued vector which typically has several hundred
dimensions. The word at position t is represented as
a binary vector w(t) whose length is the vocabulary
size, and with a “1” in a position uniquely associated
with the word, and “0” elsewhere. w(t) and s(t) are
concatenated to predict an output distribution over
words, y(t). Updating is done with two weight ma-
trices u and v and nonlinear functions f() and g()
(Mikolov et al., 2011b):
</bodyText>
<equation confidence="0.998546">
X(t) = [w(t)Ts(t − 1)T]T
sj(t) = f( � xi(t)uji)
i
yk(t) = g( � sj(t)vkj)
j
fwith f() being a sigmoid and g() a softmax:
( ) — 1 exp(z.)
W 1 + exp(−z), g(z.) = Ek exp(zk)
</equation>
<bodyText confidence="0.999903454545454">
The output y(t) is a probability distribution over
words, and the parameters u and v are trained with
back-propagation to minimize the Kullback-Leibler
(KL) divergence between the predicted and observed
distributions. Because of the recurrent connections,
this model is similar to a nonlinear infinite impulse
response (IIR) filter, and has the potential to model
long span dependencies. Theoretical considerations
(Bengio et al., 1994) indicate that for many prob-
lems, this may not be possible, but in practice it is
an empirical question.
</bodyText>
<sectionHeader confidence="0.864094" genericHeader="method">
4 Sentence Completion via Latent
</sectionHeader>
<subsectionHeader confidence="0.937792">
Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999835818181818">
Latent Semantic Analysis (LSA) (Deerwester et al.,
1990) is a widely used method for representing
words and documents in a low dimensional vector
space. The method is based on applying singular
value decomposition (SVD) to a matrix W repre-
senting the occurrence of words in documents. SVD
results in an approximation of W by the product
of three matrices, one in which each word is rep-
resented as a low-dimensional vector, one in which
each document is represented as a low dimensional
vector, and a diagonal scaling matrix. The simi-
larity between two words can then be quantified as
the cosine-similarity between their respective scaled
vectors, and document similarity can be measured
likewise. It has been used in numerous tasks, rang-
ing from information retrieval (Deerwester et al.,
1990) to speech recognition (Bellegarda, 2000; Coc-
caro and Jurafsky, 1998).
To perform LSA, one proceeds as follows. The
input is a collection of n documents which are ex-
pressed in terms of words from a vocabulary of size
m. These documents may be actual documents such
as newspaper articles, or simply as in our case no-
tional documents such as sentences. Next, a m x n
matrix W is formed. At its simplest, the ij&amp;quot; entry
contains the number of times word i has occurred in
document j - its term frequency or TF value. More
conventionally, the entry is weighted by some no-
tion of the importance of word i, for example the
negative logarithm of the fraction of documents that
contain it, resulting in a TF-IDF weighting (Salton
et al., 1975). Finally, to obtain a subspace represen-
tation of dimension d, W is decomposed as
</bodyText>
<equation confidence="0.401623">
W &apos;,Z� USVT
</equation>
<bodyText confidence="0.9993229">
where U is m x d, VT is d x n, and S is a d x d diag-
onal matrix. In applications, d &lt;&lt; n and d &lt;&lt; m;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the rows
of US - which represents the words - behave sim-
ilarly to the original rows of W, in the sense that
the cosine similarity between two rows in US ap-
proximates the cosine similarity between the corre-
</bodyText>
<page confidence="0.978105">
604
</page>
<bodyText confidence="0.5749435">
sponding rows in W. Cosine similarity is defined as
sim(x, y) = IIXII IIYII.
</bodyText>
<subsectionHeader confidence="0.973704">
4.1 Total Word Similarity
</subsectionHeader>
<bodyText confidence="0.999973">
Perhaps the simplest way of doing sentence comple-
tion with LSA is to compute the total similarity of a
potential answer a with the rest of the words in the
sentence S, and to choose the most related option.
We define the total similarity as:
</bodyText>
<equation confidence="0.996549">
totsim(a, S) = � sim(a, w)
wES
</equation>
<bodyText confidence="0.99973325">
When the completion requires two words, total sim-
ilarity is the sum of the contributions for both words.
This is our baseline method for using LSA, and one
of the best methods we have found.
</bodyText>
<subsectionHeader confidence="0.992662">
4.2 Sentence Reconstruction
</subsectionHeader>
<bodyText confidence="0.999905125">
Recall that LSA approximates a weighted word-
document matrix W as the product of low rank
matrices U and V along with a scaling matrix S:
W ≈ USVT . Using singular value decomposition,
this is done so as to minimize the mean square re-
construction error Eij Q2 ijwhere Q = W −USVT .
From the basic definition of LSA, each column of W
(representing a document) is represented as
</bodyText>
<equation confidence="0.98639">
Wj = USVjT, (1)
</equation>
<bodyText confidence="0.997877066666667">
that is, as a linear combination of the set of basis
functions formed by the columns of US, with the
combination weights specified in VjT. When a new
document is presented, it is also possible to repre-
sent it in terms of the same basis vectors. Moreover,
we may take the reconstruction error induced by this
representation to be a measure of how consistent the
new document is with the original set of documents
used to determine U S and V (Bellegarda, 2000).
It remains to represent a new document in terms
of the LSA bases. This is done as follows (Deer-
wester et al., 1990; Bellegarda, 2000), again with
the objective of minimizing the reconstruction error.
First, note that since U is column-orthonormal, (1)
implies that
</bodyText>
<equation confidence="0.967222">
Vj = WjT US−1 (2)
</equation>
<bodyText confidence="0.97584575">
Thus, if we notionally index a new document by p,
we proceed by forming a new column (document)
vector Wp using the standard term-weighting, and
then find its LSA-space representation Vp using (2).
We can evaluate the reconstruction quality by insert-
ing the result in (1). The reconstruction error is then
||(UUT − I)Wp||2
Note that if all the dimensions are retained, the re-
construction error is zero; in the case that only the
highest singular vectors are used, however, it is not.
Due to the fact that the sentences vary in length we
choose the number of retained singular vectors as a
fraction f of the sentence length. If the answer has
n words we use the top nf components. In practice,
a f of 1.2 was selected on the basis of development
set results.
</bodyText>
<subsectionHeader confidence="0.996474">
4.3 A LSA N-gram Language Model
</subsectionHeader>
<bodyText confidence="0.999945214285714">
In the context of speech recognition, LSA has been
combined with classical n-gram language models
in (Coccaro and Jurafsky, 1998; Bellegarda, 2000).
The crux of this idea is to interpolate an n-gram lan-
guage model probability with one based on LSA,
with the intuition that the standard n-gram model
will do a good job predicting function words, and
the LSA model will do a good job on words pre-
dicted by their long-span context. This logic makes
sense for the sentence completion task as well, mo-
tivating us to evaluate it.
To do this, we adopt the procedure of (Coccaro
and Jurafsky, 1998), using linear interpolation be-
tween the n-gram and LSA probabilities:
</bodyText>
<equation confidence="0.9997645">
p(w|history) =
αpng(w|history) + (1 − α)plsa(w|history)
</equation>
<bodyText confidence="0.999968571428572">
The probability of a word given its history is com-
puted by the LSA model in the following way. Let h
be the sum of all the LSA word vectors in the his-
tory. Let m be the smallest cosine similarity be-
tween h and any word in the vocabulary V : m =
minwEV sim(h, w). The probability of a word w in
the context of history h is given by
</bodyText>
<equation confidence="0.995376333333333">
sim(h, w) − m
Plsa(w|h) =
EqEV (sim(h, q) − m)
</equation>
<bodyText confidence="0.990754666666667">
Since similarity can be negative, subtracting the
minimum (m) ensures that all the estimated prob-
abilities are between 0 and 1.
</bodyText>
<page confidence="0.994784">
605
</page>
<subsectionHeader confidence="0.999634">
4.4 Improving Efficiency and Expressiveness
</subsectionHeader>
<bodyText confidence="0.999982047619047">
Given the basic framework described above, a num-
ber of enhancements are possible. In terms of ef-
ficiency, recall that it is necessary to perform SVD
on a term-document matrix. The data we used was
grouped into paragraph “documents,” of which there
were over 27 million, with 2.6 million unique words.
While the resulting matrix is highly sparse, it is nev-
ertheless impractical to perform SVD. We overcome
this difficulty in two ways. First, we restrict the set
of documents used to those which are “relevant” to
a given test set. This is done by requiring that a doc-
ument contain at least one of the potential answer-
words. Secondly, we restrict the vocabulary to the
set of words present in the test set. For the sentence-
reconstruction method of Section 4.2, we have found
it convenient to do data selection per-sentence.
To enhance the expressive power of LSA, the term
vocabulary can be expanded from unigrams to bi-
grams or trigrams of words, thus adding information
about word ordering. This was also used in the re-
construction technique.
</bodyText>
<sectionHeader confidence="0.994576" genericHeader="method">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.991701">
5.1 Data Resources
</subsectionHeader>
<bodyText confidence="0.999803727272727">
We present results with two datasets. The first is
taken from 11 Practice Tests for the SAT &amp; PSAT
2011 Edition (Princeton-Review, 2010). This book
contains eleven practice tests, and we used all the
sentence completion questions in the first five tests
as a development set, and all the questions in the last
six tests as the test set. This resulted in sets with 95
and 108 questions respectively. Additionally, we re-
port results on the recently released MSR Sentence
Completion Challenge (Zweig and Burges, 2011).
This consists of a set of 1, 040 sentence completion
questions based on sentences occurring in five Co-
nan Doyle Sherlock Holmes novels, and is identical
in format to the SAT questions. Due to the source of
this data, we refer to it as the Holmes data.
To train models, we have experimented with a
variety of data sources. Since there is no publi-
cally available collection of SAT questions suitable
to training, our methods have all relied on unsu-
pervised data. Early on, we ran a set of experi-
ments to determine the relevance of different types
of data. Thinking that data from an encyclopedia
</bodyText>
<table confidence="0.9987045">
Data Dev % Correct Test % Correct
Encarta 26 33
Wikipedia 32 31
LA Times 39 42
</table>
<tableCaption confidence="0.999925">
Table 1: Effectiveness of different types of training data.
</tableCaption>
<bodyText confidence="0.99979785">
might be useful, we evaluated an electronic version
of the 2003 Encarta encyclopedia, which has ap-
proximately 29M words. Along similar lines, we
used a collection of Wikipedia articles consisting of
709M words. This data is the entire Wikipedia as of
January 2011, broken down into sentences, with fil-
tering to remove sentences consisting of URLs and
Wiki author comments. Finally, we used a com-
mercial newspaper dataset consisting of all the Los
Angeles Times data from 1985 to 2002, containing
about 1.1B words. These data sources were evalu-
ated using the baseline n-gram LM approach of Sec-
tion 3.1. Initial experiments indicated that that the
Los Angeles Times data is best suited to this task
(see Table 1), and our SAT experiments use this
source. For the MSR Sentence Completion data,
we obtained the training data specified in (Zweig
and Burges, 2011), consisting of approximately 500
19th-century novels available from Project Guten-
berg, and comprising 48M words.
</bodyText>
<subsectionHeader confidence="0.9995">
5.2 Human Performance
</subsectionHeader>
<bodyText confidence="0.9999817">
To provide human benchmark performance, we
asked six native speaking high school students and
five graduate students to answer the questions on the
development set. The high-schoolers attained 87%
accuracy and the graduate students 95%. Zweig and
Burges (2011) cite a human performance of 91%
on the Holmes data. Statistics from a large cross-
section of the population are not available. As a fur-
ther point of comparison, we note that chance per-
formance is 20%.
</bodyText>
<subsectionHeader confidence="0.997481">
5.3 Language Modeling Results
</subsectionHeader>
<bodyText confidence="0.999731285714286">
Table 2 summarizes our language modeling results
on the SAT data. With the exception of the base-
line backoff n-gram model, these techniques were
too computationally expensive to utilize the full Los
Angeles Times corpus. Instead, as with LSA, a “rel-
evant” corpus was selected of the sentences which
contain at least one answer option from either the
</bodyText>
<page confidence="0.997923">
606
</page>
<table confidence="0.99422">
Method Data (Dev / Test) Dev Test
3-gram GT 1.1B / 1.1B 39% 42%
Model M 193M / 236M 35 41
RNN 36M / 44M 37 42
LSA-LM 293M / 358 M 48 44
Method Dev Test
Total Word Similarity 46% 46%
Reconstruction Error 53 41
</table>
<tableCaption confidence="0.985878666666667">
Table 4: SAT performance of LSA based methods.
Table 2: Performance of language modeling methods on
SAT questions.
</tableCaption>
<table confidence="0.999174">
Method Dev ppl Dev Test ppl Test
3-gram GT 195 36% 190 44%
Model M 178 36 175 42
RNN 147 37 144 42
</table>
<tableCaption confidence="0.996805">
Table 3: Performance of language modeling methods us-
ing identical training data and vocabularies.
</tableCaption>
<bodyText confidence="0.99958432">
development or test set. Separate subsets were made
for development and test data. This data was further
sub-sampled to obtain the training set sizes indicated
in the second column. For the LSA-LM, an interpo-
lation weight of 0.1 was used for the LSA score, de-
termined through optimization on the development
set. We see from this table that the language models
perform similarly and achieve just above 40% on the
test set.
To make a more controlled comparison that nor-
malizes for the amount of training data, we have
trained Model M, and the Good-Turing model on
the same data subset as the RNN, and with the same
vocabulary. In Table 3, we present perplexity re-
sults on a held-out set of dev/test-relevant Los Ange-
les Times data, and performance on the actual SAT
questions. Two things are notable. First, the re-
current neural net has dramatically lower perplexity
than the other methods. This is consistent with re-
sults in (Mikolov et al., 2011a). Secondly, despite
the differences in perplexity, the methods show little
difference on SAT performance. Because Model M
was not better, only uses n-gram context, and was
used in the construction of the Holmes data (Zweig
and Burges, 2011), we do not consider it further.
</bodyText>
<subsectionHeader confidence="0.997643">
5.4 LSA Results
</subsectionHeader>
<bodyText confidence="0.997377">
Table 4 presents results for the methods of Sections
4.1 and 4.2. Of all the methods in isolation, the sim-
ple approach of Section 4.1 - to use the total cosine
similarity between a potential answer and the other
words in the sentence - has performed best. The ap-
</bodyText>
<table confidence="0.99706325">
Method Test
3-input LSA 46%
LSA + Good-Turing LM 53
LSA + Good-Turing LM + RNN 52
</table>
<tableCaption confidence="0.999969">
Table 5: SAT test set accuracy with combined methods.
</tableCaption>
<bodyText confidence="0.986233666666667">
proach of using reconstruction error performed very
well on the development set, but unremarkably on
the test set.
</bodyText>
<subsectionHeader confidence="0.997646">
5.5 Combination Results
</subsectionHeader>
<bodyText confidence="0.9999188">
A well-known trick for obtaining best results from
a machine learning system is to combine a set of
diverse methods into a single ensemble (Dietterich,
2000). We use ensembles to get the highest accuracy
on both of our data sets.
We use a simple linear combination of the out-
puts of the other models discussed in this paper. For
the LSA model, the linear combination has three in-
puts: the total word similarity, the cosine similarity
between the sum of the answer word vectors and the
sum of the rest of sentence’s word vectors, and the
number of out-of-vocabulary terms in the answer.
Each additional language model beyond LSA con-
tributes an additional input: the probability of the
sentence under that language model.
We train the parameters of the linear combination
on the SAT development set. The training minimizes
a loss function of pairs of answers: one correct and
one incorrect fill-in from the same question. We use
the RankNet loss function (Burges et al., 2005):
</bodyText>
<equation confidence="0.531853">
f(�w · (�� − ��)) + A||�w||2
</equation>
<bodyText confidence="0.998518625">
where x are the input features for the incorrect an-
swer, y are the features for the correct answer, w�
are the weights for the combination, and f(z) _
log(1 + exp(z)). We tune the regularizer via 5-
fold cross validation, and minimize the loss using
L-BFGS (Nocedal and Wright, 2006). The results
on the SAT test set for combining various models
are shown in Table 5.
</bodyText>
<equation confidence="0.632924">
min
w
</equation>
<page confidence="0.989748">
607
</page>
<subsectionHeader confidence="0.992604">
5.6 Holmes Data Results
</subsectionHeader>
<bodyText confidence="0.9995128">
To measure the robustness of our approaches, we
have applied them to the MSR Sentence Completion
set (Zweig and Burges, 2011), termed the Holmes
data. In Table 6, we present the results on this set,
along with the comparable SAT results. Note that
the latter are derived from models trained with the
Los Angeles Times data, while the Holmes results
are derived from models trained with 19th-century
novels. We see from this table that the results are
similar across the two tasks. The best performing
single model is LSA total word similarity.
For the Holmes data, combining the models out-
performs any single model. We train the linear com-
bination function via 5-fold cross-validation: the
model is trained five times, each time on 3/5 of the
data, the regularization tuned on 1/5 of the data, and
tested on 1/5. The test results are pooled across all
5 folds and are shown in Table 6. In this case, the
best combination is to blend LSA, the Good-Turing
language model, and the recurrent neural network.
</bodyText>
<sectionHeader confidence="0.998516" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999899291666667">
To verify that the differences in accuracy between
the different algorithms are not statistical flukes, we
perform a statistical significance test on the out-
puts of each algorithm. We use McNemar’s test,
which is a matched test between two classifiers (Di-
etterich, 1998). We use the False Discovery Rate
method (Benjamini and Hochberg, 1995) to control
the false positive rate caused by multiple tests. If
we allow 2% of our tests to yield incorrectly false
results, then for the SAT data, the combination of
the Good-Turing smoothed language model with an
LSA-based global similarity model (52% accuracy)
is better that the baseline alone (42% accuracy).
Secondly, for the Holmes data, we can state that
LSA total similarity beats the recurrent neural net-
work, which in turn is better than the baseline n-
gram model. The combination of all three is sig-
nificantly better than any of the individual models.
To better understand the system performance and
gain insight into ways of improving it, we have ex-
amined the system’s errors. Encouragingly, one-
third of the errors involve single-word questions
which test the dictionary definition of a word. This
is done either by stating the definition, or provid-
</bodyText>
<table confidence="0.991657111111111">
Method SAT Holmes
Chance 20% 20%
GT N-gram LM 42 39
RNN 42 45
LSA Total Similarity 46 49
Reconstruction Error 41 41
LSA-LM 44 42
Combination 53 52
Human 87 to 95 91
</table>
<tableCaption confidence="0.997106">
Table 6: Performance of methods on the MSR Sentence
Completion Challenge, contrasted with SAT test set.
</tableCaption>
<bodyText confidence="0.999962166666667">
ing a stereotypical use of the word. An example of
the first case is: “Great artists are often prophetic
(visual): they perceive what we cannot and antici-
pate the future long before we do.” (The system’s
incorrect answer is in parentheses.) An example
of the second is: “One cannot help but be moved
by Theresa’s heartrending (therapeutic) struggle to
overcome a devastating and debilitating accident.”
At the other end of the difficulty spectrum are
questions involving world knowledge and/or logical
implications. An example requiring both is, “Many
fear that the ratification (withdrawal) of more le-
nient tobacco advertising could be detrimental to
public health.” About 40% of the errors require this
sort of general knowledge to resolve. Based on our
analysis, we believe that future research could prof-
itably exploit the structured information present in
a dictionary. However, the ability to identify and
manipulate logical relationships and embed world
knowledge in a manner amenable to logical manip-
ulation may be necessary for a full solution. It is
an interesting research question if this could be done
implicitly with a machine learning technique, for ex-
ample recurrent or recursive neural networks.
</bodyText>
<sectionHeader confidence="0.998501" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998468666666667">
In this paper we have investigated methods for
answering sentence-completion questions. These
questions are intriguing because they probe the abil-
ity to distinguish semantically coherent sentences
from incoherent ones, and yet involve no more con-
text than the single sentence. We find that both local
n-gram information and an LSA-based global coher-
ence model do significantly better than chance, and
that they can be effectively combined.
</bodyText>
<page confidence="0.998007">
608
</page>
<sectionHeader confidence="0.989729" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9919215">
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neural
Networks, 5(2):157 –166.
Y. Benjamini and Y. Hochberg. 1995. Controlling the
fase discovery rate: a practical and powerful approach
to multiple testing. J. Royal Statistical Society B,
53(1):289–300.
C. Burges, T. Shaked., E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
rank using gradient descent. In Proc. ICML, pages 89–
96.
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo
Braz, Benjamin Garrett, Margaret Kosmala, Tomer
Moscovich, Lixin Pang, Changhee Pyo, Ye Sun,
Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa
Zorn. 2000. Reading comprehension programs in
a statistical-language-processing class. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ’00, pages 1–5. Asso-
ciation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1999. An empirical
study of smoothing techniques for language modeling.
Computer Speech and Language, 13(4):359–393.
S. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, and
A. Sethy. 2009. Scaling shrinkage-based language
models. In ASRU.
S. Chen. 2009a. Performance prediction for exponential
language models. In NAACL-HLT.
S. Chen. 2009b. Shrinking exponential language models.
In NAACL-HLT.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
Toolkit. In Proceedings ESCA Eurospeech,
http://www.speech.cs.cmu.edu/SLM/toolkit.html.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, ICSLP.
Bollegala D., Matsuo Y., and Ishizuka M. 2009. Measur-
ing the similarity between implicit semantic relations
from the web. In World Wide Web Conference (WWW).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
T.G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895–1923.
T.G. Dietterich. 2000. Ensemble methods in machine
learning. In International Workshop on Multiple Clas-
sifier Systems, pages 1–15. Springer-Verlag.
Educational-Testing-Service. 2011.
https://satonlinecourse.collegeboard.com/sr/digital assets/
assessment/pdf/0833a611-0a43-10c2-0148-
cc8c0087fb06-f.pdf.
A. Emami, S. Chen, A. Ittycheriah, H. Soltau, and
B. Zhao. 2010. Decoding with shrinkage-based lan-
guage models. In Interspeech.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ’07, pages 145–148, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. Unt: Subfinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations, SemEval ’07,
pages 410–413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lynette Hirschman, Mark Light, Eric Breck, and John D.
Burger. 1999. Deep read: A reading comprehension
system. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato’s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211–
240.
Iddo Lev, Bill MacCartney, Christopher D. Manning, and
Roger Levy. 2004. Solving logic puzzles: from ro-
bust processing to precise semantics. In Proceedings
of the 2nd Workshop on Text Meaning and Interpreta-
tion, pages 9–16. Association for Computational Lin-
guistics.
Jarmasz M. and Szpakowicz S. 2003. Roget’s thesaurus
and semantic similarity. In Recent Advances in Natu-
ral Language Processing (RANLP).
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48–53.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
</reference>
<page confidence="0.986549">
609
</page>
<reference confidence="0.999465333333333">
Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas
Burget, and Jan Cernocky. 2011a. Empirical evalua-
tion and combination of advanced language modeling
techniques. In Proceedings of Interspeech 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai Pheng
Kwan. 2000. A machine learning approach to answer-
ing questions for reading comprehension tests. In Pro-
ceedings of the 2000 Joint SIGDAT conference on Em-
pirical methods in natural language processing and
very large corpora: held in conjunction with the 38th
Annual Meeting of the Association for Computational
Linguistics - Volume 13, EMNLP ’00, pages 124–132.
J. Nocedal and S. Wright. 2006. Numerical Optimiza-
tion. Springer-Verlag.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33 (2), pages 161–199.
Princeton-Review. 2010. 11 Practice Tests for the SAT
&amp; PSAT, 2011 Edition. The Princeton Review.
Ellen Riloff and Michael Thelen. 2000. A rule-based
question answering system for reading comprehension
tests. In Proceedings of the 2000ANLP/NAACL Work-
shop on Reading comprehension tests as evaluation for
computer-based language understanding sytems - Vol-
ume 6, ANLP/NAACL-ReadingComp ’00, pages 13–
19.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In Proceedings of the 2011 International Con-
ference on Machine Learning (ICML-2011).
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural networks.
In Proceedings of the 2011 International Conference
on Machine Learning (ICML-2011).
E. Terra and C. Clarke. 2003. Frequency estimates for
statistical word similarity measures. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251–278.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning (ECML).
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
T. Veale. 2004. Wordnet sits the sat: A knowledge-based
approach to lexical analogy. In European Conference
on Artificial Intelligence (ECAI).
W. Wang, J. Auer, R. Parasuraman, I. Zubarev,
D. Brandyberry, and M. P. Harper. 2000. A ques-
tion answering system developed as a project in a
natural language processing course. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ’00, pages 28–35.
Deniz Yuret. 2007. Ku: word sense disambiguation
by substitution. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
’07, pages 207–213, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Geoffrey Zweig and Christopher J.C. Burges. 2011. The
Microsoft Research sentence completion challenge.
Technical Report MSR-TR-2011-129, Microsoft.
</reference>
<page confidence="0.997042">
610
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.102429">
<title confidence="0.983554">Computational Approaches to Sentence Completion</title>
<author confidence="0.935189333333333">Geoffrey Zweig</author>
<author confidence="0.935189333333333">John C Christopher Christopher J C</author>
<affiliation confidence="0.947315">Microsoft</affiliation>
<address confidence="0.999182">Redmond, WA 98052</address>
<email confidence="0.541057">Ainur</email>
<affiliation confidence="0.728914">Cornell Computer Science</affiliation>
<address confidence="0.995546">Ithaca, NY 14853</address>
<email confidence="0.426964">Qiang</email>
<affiliation confidence="0.961336">Univ. of California, Info. &amp; Comp.</affiliation>
<address confidence="0.999087">Irvine, California 92697</address>
<abstract confidence="0.999341631578948">This paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels. We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>88</volume>
<issue>8</issue>
<contexts>
<context position="15043" citStr="Bellegarda, 2000" startWordPosition="2436" endWordPosition="2437">D) to a matrix W representing the occurrence of words in documents. SVD results in an approximation of W by the product of three matrices, one in which each word is represented as a low-dimensional vector, one in which each document is represented as a low dimensional vector, and a diagonal scaling matrix. The similarity between two words can then be quantified as the cosine-similarity between their respective scaled vectors, and document similarity can be measured likewise. It has been used in numerous tasks, ranging from information retrieval (Deerwester et al., 1990) to speech recognition (Bellegarda, 2000; Coccaro and Jurafsky, 1998). To perform LSA, one proceeds as follows. The input is a collection of n documents which are expressed in terms of words from a vocabulary of size m. These documents may be actual documents such as newspaper articles, or simply as in our case notional documents such as sentences. Next, a m x n matrix W is formed. At its simplest, the ij&amp;quot; entry contains the number of times word i has occurred in document j - its term frequency or TF value. More conventionally, the entry is weighted by some notion of the importance of word i, for example the negative logarithm of th</context>
<context position="17753" citStr="Bellegarda, 2000" startWordPosition="2948" endWordPosition="2949">construction error Eij Q2 ijwhere Q = W −USVT . From the basic definition of LSA, each column of W (representing a document) is represented as Wj = USVjT, (1) that is, as a linear combination of the set of basis functions formed by the columns of US, with the combination weights specified in VjT. When a new document is presented, it is also possible to represent it in terms of the same basis vectors. Moreover, we may take the reconstruction error induced by this representation to be a measure of how consistent the new document is with the original set of documents used to determine U S and V (Bellegarda, 2000). It remains to represent a new document in terms of the LSA bases. This is done as follows (Deerwester et al., 1990; Bellegarda, 2000), again with the objective of minimizing the reconstruction error. First, note that since U is column-orthonormal, (1) implies that Vj = WjT US−1 (2) Thus, if we notionally index a new document by p, we proceed by forming a new column (document) vector Wp using the standard term-weighting, and then find its LSA-space representation Vp using (2). We can evaluate the reconstruction quality by inserting the result in (1). The reconstruction error is then ||(UUT − </context>
<context position="18975" citStr="Bellegarda, 2000" startWordPosition="3161" endWordPosition="3162">|2 Note that if all the dimensions are retained, the reconstruction error is zero; in the case that only the highest singular vectors are used, however, it is not. Due to the fact that the sentences vary in length we choose the number of retained singular vectors as a fraction f of the sentence length. If the answer has n words we use the top nf components. In practice, a f of 1.2 was selected on the basis of development set results. 4.3 A LSA N-gram Language Model In the context of speech recognition, LSA has been combined with classical n-gram language models in (Coccaro and Jurafsky, 1998; Bellegarda, 2000). The crux of this idea is to interpolate an n-gram language model probability with one based on LSA, with the intuition that the standard n-gram model will do a good job predicting function words, and the LSA model will do a good job on words predicted by their long-span context. This logic makes sense for the sentence completion task as well, motivating us to evaluate it. To do this, we adopt the procedure of (Coccaro and Jurafsky, 1998), using linear interpolation between the n-gram and LSA probabilities: p(w|history) = αpng(w|history) + (1 − α)plsa(w|history) The probability of a word give</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>J. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the IEEE, 88(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult.</title>
<date>1994</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>166</pages>
<contexts>
<context position="14054" citStr="Bengio et al., 1994" startWordPosition="2274" endWordPosition="2277"> al., 2011b): X(t) = [w(t)Ts(t − 1)T]T sj(t) = f( � xi(t)uji) i yk(t) = g( � sj(t)vkj) j fwith f() being a sigmoid and g() a softmax: ( ) — 1 exp(z.) W 1 + exp(−z), g(z.) = Ek exp(zk) The output y(t) is a probability distribution over words, and the parameters u and v are trained with back-propagation to minimize the Kullback-Leibler (KL) divergence between the predicted and observed distributions. Because of the recurrent connections, this model is similar to a nonlinear infinite impulse response (IIR) filter, and has the potential to model long span dependencies. Theoretical considerations (Bengio et al., 1994) indicate that for many problems, this may not be possible, but in practice it is an empirical question. 4 Sentence Completion via Latent Semantic Analysis Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is a widely used method for representing words and documents in a low dimensional vector space. The method is based on applying singular value decomposition (SVD) to a matrix W representing the occurrence of words in documents. SVD results in an approximation of W by the product of three matrices, one in which each word is represented as a low-dimensional vector, one in which each doc</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157 –166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benjamini</author>
<author>Y Hochberg</author>
</authors>
<title>Controlling the fase discovery rate: a practical and powerful approach to multiple testing.</title>
<date>1995</date>
<journal>J. Royal Statistical Society B,</journal>
<volume>53</volume>
<issue>1</issue>
<contexts>
<context position="29331" citStr="Benjamini and Hochberg, 1995" startWordPosition="4955" endWordPosition="4958">on 3/5 of the data, the regularization tuned on 1/5 of the data, and tested on 1/5. The test results are pooled across all 5 folds and are shown in Table 6. In this case, the best combination is to blend LSA, the Good-Turing language model, and the recurrent neural network. 6 Discussion To verify that the differences in accuracy between the different algorithms are not statistical flukes, we perform a statistical significance test on the outputs of each algorithm. We use McNemar’s test, which is a matched test between two classifiers (Dietterich, 1998). We use the False Discovery Rate method (Benjamini and Hochberg, 1995) to control the false positive rate caused by multiple tests. If we allow 2% of our tests to yield incorrectly false results, then for the SAT data, the combination of the Good-Turing smoothed language model with an LSA-based global similarity model (52% accuracy) is better that the baseline alone (42% accuracy). Secondly, for the Holmes data, we can state that LSA total similarity beats the recurrent neural network, which in turn is better than the baseline ngram model. The combination of all three is significantly better than any of the individual models. To better understand the system perf</context>
</contexts>
<marker>Benjamini, Hochberg, 1995</marker>
<rawString>Y. Benjamini and Y. Hochberg. 1995. Controlling the fase discovery rate: a practical and powerful approach to multiple testing. J. Royal Statistical Society B, 53(1):289–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burges</author>
<author>T Shaked</author>
<author>E Renshaw</author>
<author>A Lazier</author>
<author>M Deeds</author>
<author>N Hamilton</author>
<author>G Hullender</author>
</authors>
<title>Learning to rank using gradient descent.</title>
<date>2005</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="27540" citStr="Burges et al., 2005" startWordPosition="4645" endWordPosition="4648"> combination has three inputs: the total word similarity, the cosine similarity between the sum of the answer word vectors and the sum of the rest of sentence’s word vectors, and the number of out-of-vocabulary terms in the answer. Each additional language model beyond LSA contributes an additional input: the probability of the sentence under that language model. We train the parameters of the linear combination on the SAT development set. The training minimizes a loss function of pairs of answers: one correct and one incorrect fill-in from the same question. We use the RankNet loss function (Burges et al., 2005): f(�w · (�� − ��)) + A||�w||2 where x are the input features for the incorrect answer, y are the features for the correct answer, w� are the weights for the combination, and f(z) _ log(1 + exp(z)). We tune the regularizer via 5- fold cross validation, and minimize the loss using L-BFGS (Nocedal and Wright, 2006). The results on the SAT test set for combining various models are shown in Table 5. min w 607 5.6 Holmes Data Results To measure the robustness of our approaches, we have applied them to the MSR Sentence Completion set (Zweig and Burges, 2011), termed the Holmes data. In Table 6, we p</context>
</contexts>
<marker>Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton, Hullender, 2005</marker>
<rawString>C. Burges, T. Shaked., E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. 2005. Learning to rank using gradient descent. In Proc. ICML, pages 89– 96.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eugene Charniak</author>
<author>Yasemin Altun</author>
<author>Rodrigo de Salvo Braz</author>
<author>Benjamin Garrett</author>
<author>Margaret Kosmala</author>
<author>Tomer Moscovich</author>
<author>Lixin Pang</author>
<author>Changhee Pyo</author>
<author>Ye Sun</author>
<author>Wei Wy</author>
<author>Zhongfa Yang</author>
<author>Shawn Zeller</author>
<author>Lisa Zorn</author>
</authors>
<title>Reading comprehension programs in a statistical-language-processing class.</title>
<date>2000</date>
<booktitle>In Proceedings of the</booktitle>
<contexts>
<context position="2479" citStr="Charniak et al., 2000" startWordPosition="382" endWordPosition="385">, 2011; Turney et al., 2003; Turney, 2008), who used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on test prep material (Charniak et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000; Ng et al., 2000). In this paper, we study a new class of problems intermediate in difficulty between the extremes of synonym detection and general question answering - the sentence completion questions found on the Scholastic Aptitude Test (SAT). These questions present a sentence with one or two blanks that need to be filled in. Five possible words (or short phrases) are given as options for each blank. All possible answers except one result in a nonsense sentence. Two examples are shown in Figure 1. The questions are highly constrained in the sen</context>
</contexts>
<marker>Charniak, Altun, Braz, Garrett, Kosmala, Moscovich, Pang, Pyo, Sun, Wy, Yang, Zeller, Zorn, 2000</marker>
<rawString>Eugene Charniak, Yasemin Altun, Rodrigo de Salvo Braz, Benjamin Garrett, Margaret Kosmala, Tomer Moscovich, Lixin Pang, Changhee Pyo, Ye Sun, Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa Zorn. 2000. Reading comprehension programs in a statistical-language-processing class. In Proceedings of the 2000 ANLP/NAACL Workshop on Reading comprehension tests as evaluation for computerbased language understanding sytems - Volume 6, ANLP/NAACL-ReadingComp ’00, pages 1–5. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="9470" citStr="Chen and Goodman, 1999" startWordPosition="1514" endWordPosition="1517">Veale, 2004; Turney and Littman, 2005; D. et al., 2009). 3 Sentence Completion via Language Modeling Perhaps the most straightforward approach to solving the sentence completion task is to form the complete sentence with each option in turn, and to evaluate its likelihood under a language model. As discussed in Section 2, this was found be be very effective in the ranking phase of several SemEval systems. In this section, we describe the suite of state-of-the-art language modeling techniques for which we will present results. We begin with ngram models; first a classical n-gram backoff model (Chen and Goodman, 1999), and then a recently proposed class-based maximum-entropy n-gram model (Chen, 2009a; Chen, 2009b). N-gram models have the obvious disadvantage of using a very limited context in predicting word probabilities. Therefore we evaluate the recurrent neural net model of (Mikolov et al., 2010; Mikolov et al., 2011b). This model has produced record-breaking perplexity results in several tasks (Mikolov et al., 2011a), and has the potential to encode sentence-span information in the network hidden-layer activations. We have also evaluated the use of parse scores, using an off-theshelf stochastic contex</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13(4):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>L Mangu</author>
<author>B Ramabhadran</author>
<author>R Sarikaya</author>
<author>A Sethy</author>
</authors>
<title>Scaling shrinkage-based language models.</title>
<date>2009</date>
<booktitle>In ASRU.</booktitle>
<contexts>
<context position="11694" citStr="Chen et al., 2009" startWordPosition="1876" endWordPosition="1879">fer to as the Holmes data (see Section 5.1), the smaller amount of training data allowed us to use 4-grams and a vocabulary cutoff of 3. This resulted in 26M n-grams and a 126k word vocabulary. 3.2 Maximum Entropy Class-Based N-gram Language Model Word-class information provides a level of abstraction which is not available in a word-level language model; therefore we evaluated a state-of-theart class based language model. Model M (Chen, 2009a; Chen, 2009b) is a recently proposed class based exponential n-gram language model which has shown improvements across a variety of tasks (Chen, 2009b; Chen et al., 2009; Emami et al., 2010). The key ideas are the modeling of word ngram probabilities with a maximum entropy model, and the use of word-class information in the definition of the features. In particular, each word w is assigned deterministically to a class c, allowing the n-gram probabilities to be estimated as the product of class and word parts P(wi|wi−n+1 ... wi−2wi−1) _ P(ci|ci−n+1 ... ci−2ci−17 wi−n+1 ... wi−2wi−1) P(wi|wi−n+1 ... wi−2wi−17 ci). 603 Both components are themselves maximum entropy n-gram models in which the probability of a word or class label l given history h is determined by</context>
</contexts>
<marker>Chen, Mangu, Ramabhadran, Sarikaya, Sethy, 2009</marker>
<rawString>S. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, and A. Sethy. 2009. Scaling shrinkage-based language models. In ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
</authors>
<title>Performance prediction for exponential language models.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="5249" citStr="Chen, 2009" startWordPosition="817" endWordPosition="818">he sentence completion task because we believe it is complex enough to pose a significant challenge, yet structured enough that progress may be possible. As a first step, we have approached the problem from two points-of-view: first by exploiting local sentence structure, and secondly by measuring a novel form of global sentence coherence based on latent semantic analysis. To investigate the usefulness of local information, we evaluated n-gram language model scores, from both a conventional model with Good-Turing smoothing, and with a recently proposed maximum-entropy class-based ngram model (Chen, 2009a; Chen, 2009b). Also in the language modeling vein, but with potentially global context, we evaluate the use of a recurrent neural network language model. In all the language modeling approaches, a model is used to compute a sentence probability with each of the potential completions. To measure global coherence, we propose a novel method based on latent semantic analysis (LSA). We find that the LSA based method performs best, and that both local and global information can be combined to exceed 50% accuracy. We report results on a set of questions taken from a collection of SAT practice exams</context>
<context position="9553" citStr="Chen, 2009" startWordPosition="1528" endWordPosition="1529">ling Perhaps the most straightforward approach to solving the sentence completion task is to form the complete sentence with each option in turn, and to evaluate its likelihood under a language model. As discussed in Section 2, this was found be be very effective in the ranking phase of several SemEval systems. In this section, we describe the suite of state-of-the-art language modeling techniques for which we will present results. We begin with ngram models; first a classical n-gram backoff model (Chen and Goodman, 1999), and then a recently proposed class-based maximum-entropy n-gram model (Chen, 2009a; Chen, 2009b). N-gram models have the obvious disadvantage of using a very limited context in predicting word probabilities. Therefore we evaluate the recurrent neural net model of (Mikolov et al., 2010; Mikolov et al., 2011b). This model has produced record-breaking perplexity results in several tasks (Mikolov et al., 2011a), and has the potential to encode sentence-span information in the network hidden-layer activations. We have also evaluated the use of parse scores, using an off-theshelf stochastic context free grammar parser. However, the grammatical structure of the alternatives is of</context>
<context position="11523" citStr="Chen, 2009" startWordPosition="1851" endWordPosition="1852">ta, along with every word in the development or test sets. This resulted in a 124k word vocabulary and 59M n-grams. For the Conan Doyle data, which we henceforth refer to as the Holmes data (see Section 5.1), the smaller amount of training data allowed us to use 4-grams and a vocabulary cutoff of 3. This resulted in 26M n-grams and a 126k word vocabulary. 3.2 Maximum Entropy Class-Based N-gram Language Model Word-class information provides a level of abstraction which is not available in a word-level language model; therefore we evaluated a state-of-theart class based language model. Model M (Chen, 2009a; Chen, 2009b) is a recently proposed class based exponential n-gram language model which has shown improvements across a variety of tasks (Chen, 2009b; Chen et al., 2009; Emami et al., 2010). The key ideas are the modeling of word ngram probabilities with a maximum entropy model, and the use of word-class information in the definition of the features. In particular, each word w is assigned deterministically to a class c, allowing the n-gram probabilities to be estimated as the product of class and word parts P(wi|wi−n+1 ... wi−2wi−1) _ P(ci|ci−n+1 ... ci−2ci−17 wi−n+1 ... wi−2wi−1) P(wi|wi−n</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>S. Chen. 2009a. Performance prediction for exponential language models. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
</authors>
<title>Shrinking exponential language models.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="5249" citStr="Chen, 2009" startWordPosition="817" endWordPosition="818">he sentence completion task because we believe it is complex enough to pose a significant challenge, yet structured enough that progress may be possible. As a first step, we have approached the problem from two points-of-view: first by exploiting local sentence structure, and secondly by measuring a novel form of global sentence coherence based on latent semantic analysis. To investigate the usefulness of local information, we evaluated n-gram language model scores, from both a conventional model with Good-Turing smoothing, and with a recently proposed maximum-entropy class-based ngram model (Chen, 2009a; Chen, 2009b). Also in the language modeling vein, but with potentially global context, we evaluate the use of a recurrent neural network language model. In all the language modeling approaches, a model is used to compute a sentence probability with each of the potential completions. To measure global coherence, we propose a novel method based on latent semantic analysis (LSA). We find that the LSA based method performs best, and that both local and global information can be combined to exceed 50% accuracy. We report results on a set of questions taken from a collection of SAT practice exams</context>
<context position="9553" citStr="Chen, 2009" startWordPosition="1528" endWordPosition="1529">ling Perhaps the most straightforward approach to solving the sentence completion task is to form the complete sentence with each option in turn, and to evaluate its likelihood under a language model. As discussed in Section 2, this was found be be very effective in the ranking phase of several SemEval systems. In this section, we describe the suite of state-of-the-art language modeling techniques for which we will present results. We begin with ngram models; first a classical n-gram backoff model (Chen and Goodman, 1999), and then a recently proposed class-based maximum-entropy n-gram model (Chen, 2009a; Chen, 2009b). N-gram models have the obvious disadvantage of using a very limited context in predicting word probabilities. Therefore we evaluate the recurrent neural net model of (Mikolov et al., 2010; Mikolov et al., 2011b). This model has produced record-breaking perplexity results in several tasks (Mikolov et al., 2011a), and has the potential to encode sentence-span information in the network hidden-layer activations. We have also evaluated the use of parse scores, using an off-theshelf stochastic context free grammar parser. However, the grammatical structure of the alternatives is of</context>
<context position="11523" citStr="Chen, 2009" startWordPosition="1851" endWordPosition="1852">ta, along with every word in the development or test sets. This resulted in a 124k word vocabulary and 59M n-grams. For the Conan Doyle data, which we henceforth refer to as the Holmes data (see Section 5.1), the smaller amount of training data allowed us to use 4-grams and a vocabulary cutoff of 3. This resulted in 26M n-grams and a 126k word vocabulary. 3.2 Maximum Entropy Class-Based N-gram Language Model Word-class information provides a level of abstraction which is not available in a word-level language model; therefore we evaluated a state-of-theart class based language model. Model M (Chen, 2009a; Chen, 2009b) is a recently proposed class based exponential n-gram language model which has shown improvements across a variety of tasks (Chen, 2009b; Chen et al., 2009; Emami et al., 2010). The key ideas are the modeling of word ngram probabilities with a maximum entropy model, and the use of word-class information in the definition of the features. In particular, each word w is assigned deterministically to a class c, allowing the n-gram probabilities to be estimated as the product of class and word parts P(wi|wi−n+1 ... wi−2wi−1) _ P(ci|ci−n+1 ... ci−2ci−17 wi−n+1 ... wi−2wi−1) P(wi|wi−n</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>S. Chen. 2009b. Shrinking exponential language models. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge Toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings ESCA Eurospeech,</booktitle>
<location>http://www.speech.cs.cmu.edu/SLM/toolkit.html.</location>
<contexts>
<context position="10596" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="1688" endWordPosition="1691">er activations. We have also evaluated the use of parse scores, using an off-theshelf stochastic context free grammar parser. However, the grammatical structure of the alternatives is often identical. With scores differing only in the final non-terminal/terminal rewrites, this did little better than chance. The use of other syntactically derived features, for example based on a dependency parse, are likely to be more effective, but we leave this for future work. 3.1 Backoff N-gram Language Model Our baseline model is a Good-Turing smoothed model trained with the CMU language modeling toolkit (Clarkson and Rosenfeld, 1997). For the SAT task, we used a trigram language model trained on 1.1B words of newspaper data, described in Section 5.1. All bigrams occurring at least twice were retained in the model, along with all trigrams occurring at least three times. The vocabulary consisted of all words occurring at least 100 times in the data, along with every word in the development or test sets. This resulted in a 124k word vocabulary and 59M n-grams. For the Conan Doyle data, which we henceforth refer to as the Holmes data (see Section 5.1), the smaller amount of training data allowed us to use 4-grams and a vocabu</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>P.R. Clarkson and R. Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge Toolkit. In Proceedings ESCA Eurospeech, http://www.speech.cs.cmu.edu/SLM/toolkit.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Coccaro</author>
<author>D Jurafsky</author>
</authors>
<title>Towards better integration of semantic predictors in statistical language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings, ICSLP.</booktitle>
<contexts>
<context position="15072" citStr="Coccaro and Jurafsky, 1998" startWordPosition="2438" endWordPosition="2442">epresenting the occurrence of words in documents. SVD results in an approximation of W by the product of three matrices, one in which each word is represented as a low-dimensional vector, one in which each document is represented as a low dimensional vector, and a diagonal scaling matrix. The similarity between two words can then be quantified as the cosine-similarity between their respective scaled vectors, and document similarity can be measured likewise. It has been used in numerous tasks, ranging from information retrieval (Deerwester et al., 1990) to speech recognition (Bellegarda, 2000; Coccaro and Jurafsky, 1998). To perform LSA, one proceeds as follows. The input is a collection of n documents which are expressed in terms of words from a vocabulary of size m. These documents may be actual documents such as newspaper articles, or simply as in our case notional documents such as sentences. Next, a m x n matrix W is formed. At its simplest, the ij&amp;quot; entry contains the number of times word i has occurred in document j - its term frequency or TF value. More conventionally, the entry is weighted by some notion of the importance of word i, for example the negative logarithm of the fraction of documents that </context>
<context position="18956" citStr="Coccaro and Jurafsky, 1998" startWordPosition="3157" endWordPosition="3160">error is then ||(UUT − I)Wp||2 Note that if all the dimensions are retained, the reconstruction error is zero; in the case that only the highest singular vectors are used, however, it is not. Due to the fact that the sentences vary in length we choose the number of retained singular vectors as a fraction f of the sentence length. If the answer has n words we use the top nf components. In practice, a f of 1.2 was selected on the basis of development set results. 4.3 A LSA N-gram Language Model In the context of speech recognition, LSA has been combined with classical n-gram language models in (Coccaro and Jurafsky, 1998; Bellegarda, 2000). The crux of this idea is to interpolate an n-gram language model probability with one based on LSA, with the intuition that the standard n-gram model will do a good job predicting function words, and the LSA model will do a good job on words predicted by their long-span context. This logic makes sense for the sentence completion task as well, motivating us to evaluate it. To do this, we adopt the procedure of (Coccaro and Jurafsky, 1998), using linear interpolation between the n-gram and LSA probabilities: p(w|history) = αpng(w|history) + (1 − α)plsa(w|history) The probabi</context>
</contexts>
<marker>Coccaro, Jurafsky, 1998</marker>
<rawString>N. Coccaro and D. Jurafsky. 1998. Towards better integration of semantic predictors in statistical language modeling. In Proceedings, ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Measuring the similarity between implicit semantic relations from the web. In World Wide Web Conference (WWW).</title>
<date>2009</date>
<marker>Bollegala, Matsuo, Ishizuka, 2009</marker>
<rawString>Bollegala D., Matsuo Y., and Ishizuka M. 2009. Measuring the similarity between implicit semantic relations from the web. In World Wide Web Conference (WWW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>96</issue>
<contexts>
<context position="14266" citStr="Deerwester et al., 1990" startWordPosition="2308" endWordPosition="2311">bability distribution over words, and the parameters u and v are trained with back-propagation to minimize the Kullback-Leibler (KL) divergence between the predicted and observed distributions. Because of the recurrent connections, this model is similar to a nonlinear infinite impulse response (IIR) filter, and has the potential to model long span dependencies. Theoretical considerations (Bengio et al., 1994) indicate that for many problems, this may not be possible, but in practice it is an empirical question. 4 Sentence Completion via Latent Semantic Analysis Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is a widely used method for representing words and documents in a low dimensional vector space. The method is based on applying singular value decomposition (SVD) to a matrix W representing the occurrence of words in documents. SVD results in an approximation of W by the product of three matrices, one in which each word is represented as a low-dimensional vector, one in which each document is represented as a low dimensional vector, and a diagonal scaling matrix. The similarity between two words can then be quantified as the cosine-similarity between their respective scaled vectors, and docum</context>
<context position="17869" citStr="Deerwester et al., 1990" startWordPosition="2968" endWordPosition="2972">ng a document) is represented as Wj = USVjT, (1) that is, as a linear combination of the set of basis functions formed by the columns of US, with the combination weights specified in VjT. When a new document is presented, it is also possible to represent it in terms of the same basis vectors. Moreover, we may take the reconstruction error induced by this representation to be a measure of how consistent the new document is with the original set of documents used to determine U S and V (Bellegarda, 2000). It remains to represent a new document in terms of the LSA bases. This is done as follows (Deerwester et al., 1990; Bellegarda, 2000), again with the objective of minimizing the reconstruction error. First, note that since U is column-orthonormal, (1) implies that Vj = WjT US−1 (2) Thus, if we notionally index a new document by p, we proceed by forming a new column (document) vector Wp using the standard term-weighting, and then find its LSA-space representation Vp using (2). We can evaluate the reconstruction quality by inserting the result in (1). The reconstruction error is then ||(UUT − I)Wp||2 Note that if all the dimensions are retained, the reconstruction error is zero; in the case that only the hi</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<pages>10--1895</pages>
<contexts>
<context position="29260" citStr="Dietterich, 1998" startWordPosition="4945" endWordPosition="4947">oss-validation: the model is trained five times, each time on 3/5 of the data, the regularization tuned on 1/5 of the data, and tested on 1/5. The test results are pooled across all 5 folds and are shown in Table 6. In this case, the best combination is to blend LSA, the Good-Turing language model, and the recurrent neural network. 6 Discussion To verify that the differences in accuracy between the different algorithms are not statistical flukes, we perform a statistical significance test on the outputs of each algorithm. We use McNemar’s test, which is a matched test between two classifiers (Dietterich, 1998). We use the False Discovery Rate method (Benjamini and Hochberg, 1995) to control the false positive rate caused by multiple tests. If we allow 2% of our tests to yield incorrectly false results, then for the SAT data, the combination of the Good-Turing smoothed language model with an LSA-based global similarity model (52% accuracy) is better that the baseline alone (42% accuracy). Secondly, for the Holmes data, we can state that LSA total similarity beats the recurrent neural network, which in turn is better than the baseline ngram model. The combination of all three is significantly better </context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>T.G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10:1895–1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Ensemble methods in machine learning.</title>
<date>2000</date>
<booktitle>In International Workshop on Multiple Classifier Systems,</booktitle>
<pages>1--15</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="26723" citStr="Dietterich, 2000" startWordPosition="4507" endWordPosition="4508">methods in isolation, the simple approach of Section 4.1 - to use the total cosine similarity between a potential answer and the other words in the sentence - has performed best. The apMethod Test 3-input LSA 46% LSA + Good-Turing LM 53 LSA + Good-Turing LM + RNN 52 Table 5: SAT test set accuracy with combined methods. proach of using reconstruction error performed very well on the development set, but unremarkably on the test set. 5.5 Combination Results A well-known trick for obtaining best results from a machine learning system is to combine a set of diverse methods into a single ensemble (Dietterich, 2000). We use ensembles to get the highest accuracy on both of our data sets. We use a simple linear combination of the outputs of the other models discussed in this paper. For the LSA model, the linear combination has three inputs: the total word similarity, the cosine similarity between the sum of the answer word vectors and the sum of the rest of sentence’s word vectors, and the number of out-of-vocabulary terms in the answer. Each additional language model beyond LSA contributes an additional input: the probability of the sentence under that language model. We train the parameters of the linear</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>T.G. Dietterich. 2000. Ensemble methods in machine learning. In International Workshop on Multiple Classifier Systems, pages 1–15. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Educational-Testing-Service</author>
</authors>
<date>2011</date>
<pages>0833--611</pages>
<note>https://satonlinecourse.collegeboard.com/sr/digital assets/</note>
<contexts>
<context position="4181" citStr="Educational-Testing-Service, 2011" startWordPosition="651" endWordPosition="652"> pages 601–610, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 1. One of the characters in Milton Murayama’s novel is considered because he deliberately defies an oppressive hierarchical society. (A) rebellious (B) impulsive (C) artistic (D) industrious (E) tyrannical 2. Whether substances are medicines or poisons often depends on dosage, for substances that are in small doses can be in large. (A) useless.. effective (B) mild.. benign (C) curative.. toxic (D) harmful .. fatal (E) beneficial.. miraculous Figure 1: Sample sentence completion questions (Educational-Testing-Service, 2011). the contrast between medicine and poison that the correct answer involves a contrast, either useless vs. effective or curative vs. toxic. Moreover, the first, incorrect, possibility is perfectly acceptable in the context of the second clause alone; only irrelevance to the contrast between medicine and poison eliminates it. In general, the questions require a combination of semantic and world knowledge as well as occasional logical reasoning. We study the sentence completion task because we believe it is complex enough to pose a significant challenge, yet structured enough that progress may b</context>
</contexts>
<marker>Educational-Testing-Service, 2011</marker>
<rawString>Educational-Testing-Service. 2011. https://satonlinecourse.collegeboard.com/sr/digital assets/ assessment/pdf/0833a611-0a43-10c2-0148-cc8c0087fb06-f.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Emami</author>
<author>S Chen</author>
<author>A Ittycheriah</author>
<author>H Soltau</author>
<author>B Zhao</author>
</authors>
<title>Decoding with shrinkage-based language models.</title>
<date>2010</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="11715" citStr="Emami et al., 2010" startWordPosition="1880" endWordPosition="1883">s data (see Section 5.1), the smaller amount of training data allowed us to use 4-grams and a vocabulary cutoff of 3. This resulted in 26M n-grams and a 126k word vocabulary. 3.2 Maximum Entropy Class-Based N-gram Language Model Word-class information provides a level of abstraction which is not available in a word-level language model; therefore we evaluated a state-of-theart class based language model. Model M (Chen, 2009a; Chen, 2009b) is a recently proposed class based exponential n-gram language model which has shown improvements across a variety of tasks (Chen, 2009b; Chen et al., 2009; Emami et al., 2010). The key ideas are the modeling of word ngram probabilities with a maximum entropy model, and the use of word-class information in the definition of the features. In particular, each word w is assigned deterministically to a class c, allowing the n-gram probabilities to be estimated as the product of class and word parts P(wi|wi−n+1 ... wi−2wi−1) _ P(ci|ci−n+1 ... ci−2ci−17 wi−n+1 ... wi−2wi−1) P(wi|wi−n+1 ... wi−2wi−17 ci). 603 Both components are themselves maximum entropy n-gram models in which the probability of a word or class label l given history h is determined by � exp(� 1 k fk(h, l)</context>
</contexts>
<marker>Emami, Chen, Ittycheriah, Soltau, Zhao, 2010</marker>
<rawString>A. Emami, S. Chen, A. Ittycheriah, H. Soltau, and B. Zhao. 2010. Decoding with shrinkage-based language models. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Fbk-irst: Lexical substitution task exploiting domain and syntagmatic coherence.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval ’07,</booktitle>
<pages>145--148</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7639" citStr="Giuliano et al., 2007" startWordPosition="1212" endWordPosition="1216">es for match in “After the match, replace any remaining fluid deficit to prevent problems of chronic dehydration throughout the tournament.” Two consistently high-performing systems for this task are the KU (Yuret, 2007) and UNT (Hassan et al., 2007) systems. These operate in two phases: first they find a set of potential replacement words, and then they rank them. The KU system uses just an N-gram language model to do this ranking. The UNT system uses a large variety of information sources, and a language model score receives the highest weight. N-gram statistics were also very effective in (Giuliano et al., 2007). That paper also explores the use of Latent Semantic Analysis to measure the degree of similarity between a potential replacement and its context, but the results are poorer than others. Since the original word provides a strong hint as to the pos602 sible meanings of the replacements, we hypothesize that N-gram statistics are largely able to resolve the remaining ambiguities. The SAT sentence completion sentences do not have this property and thus are more challenging. Related to, but predating the Semeval lexical substitution task are the ESL synonym questions proposed by Turney (2001), and</context>
</contexts>
<marker>Giuliano, Gliozzo, Strapparava, 2007</marker>
<rawString>Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava. 2007. Fbk-irst: Lexical substitution task exploiting domain and syntagmatic coherence. In Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval ’07, pages 145–148, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Andras Csomai</author>
<author>Carmen Banea</author>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unt: Subfinder: Combining knowledge sources for automatic lexical substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval ’07,</booktitle>
<pages>410--413</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7267" citStr="Hassan et al., 2007" startWordPosition="1148" endWordPosition="1151">2 Related Work The past work which is most similar to ours is derived from the lexical substitution track of SemEval2007 (McCarthy and Navigli, 2007). In this task, the challenge is to find a replacement for a word or phrase removed from a sentence. In contrast to our SAT-inspired task, the original answer is indicated. For example, one might be asked to find alternates for match in “After the match, replace any remaining fluid deficit to prevent problems of chronic dehydration throughout the tournament.” Two consistently high-performing systems for this task are the KU (Yuret, 2007) and UNT (Hassan et al., 2007) systems. These operate in two phases: first they find a set of potential replacement words, and then they rank them. The KU system uses just an N-gram language model to do this ranking. The UNT system uses a large variety of information sources, and a language model score receives the highest weight. N-gram statistics were also very effective in (Giuliano et al., 2007). That paper also explores the use of Latent Semantic Analysis to measure the degree of similarity between a potential replacement and its context, but the results are poorer than others. Since the original word provides a stron</context>
</contexts>
<marker>Hassan, Csomai, Banea, Sinha, Mihalcea, 2007</marker>
<rawString>Samer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, and Rada Mihalcea. 2007. Unt: Subfinder: Combining knowledge sources for automatic lexical substitution. In Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval ’07, pages 410–413, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Mark Light</author>
<author>Eric Breck</author>
<author>John D Burger</author>
</authors>
<title>Deep read: A reading comprehension system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2367" citStr="Hirschman et al., 1999" startWordPosition="364" endWordPosition="367">on of synonyms and antonyms has been studied by (Landauer and Dumais, 1997; Mohammed et al., 2008; Mohammed et al., 2011; Turney et al., 2003; Turney, 2008), who used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on test prep material (Charniak et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000; Ng et al., 2000). In this paper, we study a new class of problems intermediate in difficulty between the extremes of synonym detection and general question answering - the sentence completion questions found on the Scholastic Aptitude Test (SAT). These questions present a sentence with one or two blanks that need to be filled in. Five possible words (or short phrases) are given as options for each blank. All possible answers except one re</context>
</contexts>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>Lynette Hirschman, Mark Light, Eric Breck, and John D. Burger. 1999. Deep read: A reading comprehension system. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>211--240</pages>
<contexts>
<context position="1818" citStr="Landauer and Dumais, 1997" startWordPosition="271" endWordPosition="274">le source of evaluation data for language processing tasks. They are valuable for many reasons: they represent facets of language understanding recognized as important by educational experts; they are organized in various formats designed to evaluate specific capabilities; they are yardsticks by which society measures educational progress; and they affect a large number of people. Previous researchers have taken advantage of this material to test both narrow and general language processing capabilities. Among the narrower tasks, the identification of synonyms and antonyms has been studied by (Landauer and Dumais, 1997; Mohammed et al., 2008; Mohammed et al., 2011; Turney et al., 2003; Turney, 2008), who used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading com</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104(2), pages 211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iddo Lev</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
<author>Roger Levy</author>
</authors>
<title>Solving logic puzzles: from robust processing to precise semantics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2nd Workshop on Text Meaning and Interpretation,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2250" citStr="Lev et al., 2004" startWordPosition="344" endWordPosition="347">al to test both narrow and general language processing capabilities. Among the narrower tasks, the identification of synonyms and antonyms has been studied by (Landauer and Dumais, 1997; Mohammed et al., 2008; Mohammed et al., 2011; Turney et al., 2003; Turney, 2008), who used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on test prep material (Charniak et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000; Ng et al., 2000). In this paper, we study a new class of problems intermediate in difficulty between the extremes of synonym detection and general question answering - the sentence completion questions found on the Scholastic Aptitude Test (SAT). These questions present a sentence with one or two blanks that need to be fill</context>
</contexts>
<marker>Lev, MacCartney, Manning, Levy, 2004</marker>
<rawString>Iddo Lev, Bill MacCartney, Christopher D. Manning, and Roger Levy. 2004. Solving logic puzzles: from robust processing to precise semantics. In Proceedings of the 2nd Workshop on Text Meaning and Interpretation, pages 9–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jarmasz</author>
<author>S Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In Recent Advances in Natural Language Processing (RANLP).</booktitle>
<contexts>
<context position="8528" citStr="Jarmasz and Szpakowicz (2003)" startWordPosition="1357" endWordPosition="1360">anings of the replacements, we hypothesize that N-gram statistics are largely able to resolve the remaining ambiguities. The SAT sentence completion sentences do not have this property and thus are more challenging. Related to, but predating the Semeval lexical substitution task are the ESL synonym questions proposed by Turney (2001), and subsequently considered by numerous research groups including Terra and Clarke (2003) and Pado and Lapata (2007). These questions are similar to the SemEval task, but in addition to the original word and the sentence context, the list of options is provided. Jarmasz and Szpakowicz (2003) used a sophisticated thesaurusbased method and achieved state-of-the art performance, which is 82%. Other work on standardized tests includes the synonym and antonym tasks mentioned in Section 1, and more recent work on a SAT analogy task introduced by (Turney et al., 2003) and extensively used by other researchers (Veale, 2004; Turney and Littman, 2005; D. et al., 2009). 3 Sentence Completion via Language Modeling Perhaps the most straightforward approach to solving the sentence completion task is to form the complete sentence with each option in turn, and to evaluate its likelihood under a </context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Jarmasz M. and Szpakowicz S. 2003. Roget’s thesaurus and semantic similarity. In Recent Advances in Natural Language Processing (RANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<contexts>
<context position="6796" citStr="McCarthy and Navigli, 2007" startWordPosition="1068" endWordPosition="1071">ess of both local n-gram information, and global coherence in the form of a novel LSA-based metric. Finally, we illustrate that the local and global information can be effectively fused. The remainder of this paper is organized as follows. In Section 2 we discuss related work. Section 3 describes the language modeling methods we have evaluated. Section 4 outlines the LSA-based methods. Section 5 presents our experimental results. We conclude with a discussion in Section 6. 2 Related Work The past work which is most similar to ours is derived from the lexical substitution track of SemEval2007 (McCarthy and Navigli, 2007). In this task, the challenge is to find a replacement for a word or phrase removed from a sentence. In contrast to our SAT-inspired task, the original answer is indicated. For example, one might be asked to find alternates for match in “After the match, replace any remaining fluid deficit to prevent problems of chronic dehydration throughout the tournament.” Two consistently high-performing systems for this task are the KU (Yuret, 2007) and UNT (Hassan et al., 2007) systems. These operate in two phases: first they find a set of potential replacement words, and then they rank them. The KU syst</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. Semeval2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafiat</author>
<author>Jan Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<contexts>
<context position="9757" citStr="Mikolov et al., 2010" startWordPosition="1558" endWordPosition="1561">model. As discussed in Section 2, this was found be be very effective in the ranking phase of several SemEval systems. In this section, we describe the suite of state-of-the-art language modeling techniques for which we will present results. We begin with ngram models; first a classical n-gram backoff model (Chen and Goodman, 1999), and then a recently proposed class-based maximum-entropy n-gram model (Chen, 2009a; Chen, 2009b). N-gram models have the obvious disadvantage of using a very limited context in predicting word probabilities. Therefore we evaluate the recurrent neural net model of (Mikolov et al., 2010; Mikolov et al., 2011b). This model has produced record-breaking perplexity results in several tasks (Mikolov et al., 2011a), and has the potential to encode sentence-span information in the network hidden-layer activations. We have also evaluated the use of parse scores, using an off-theshelf stochastic context free grammar parser. However, the grammatical structure of the alternatives is often identical. With scores differing only in the final non-terminal/terminal rewrites, this did little better than chance. The use of other syntactically derived features, for example based on a dependenc</context>
<context position="12729" citStr="Mikolov et al., 2010" startWordPosition="2047" endWordPosition="2050">wi−1) P(wi|wi−n+1 ... wi−2wi−17 ci). 603 Both components are themselves maximum entropy n-gram models in which the probability of a word or class label l given history h is determined by � exp(� 1 k fk(h, l)). The features fk(h, l) used are the presence of various patterns in the concatenation of hl, for example whether a particular suffix is present in hl. 3.3 Recurrent Neural Net Language Model Many of the questions involve long-range dependencies between words. While n-gram models have no ability to explicitly maintain long-span context, the recently proposed recurrent neural-net model of (Mikolov et al., 2010) does. Related approaches have been proposed by (Sutskever et al., 2011; Socher et al., 2011). In this model, a set of neural net activations s(t) is maintained and updated at each sentence position t. These activations encapsulate the sentence history up to the tth word in a realvalued vector which typically has several hundred dimensions. The word at position t is represented as a binary vector w(t) whose length is the vocabulary size, and with a “1” in a position uniquely associated with the word, and “0” elsewhere. w(t) and s(t) are concatenated to predict an output distribution over words</context>
</contexts>
<marker>Mikolov, Karafiat, Cernocky, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafiat, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of Interspeech 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Anoop Deoras</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
</authors>
<title>Empirical evaluation and combination of advanced language modeling techniques.</title>
<date>2011</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<contexts>
<context position="9779" citStr="Mikolov et al., 2011" startWordPosition="1562" endWordPosition="1565"> Section 2, this was found be be very effective in the ranking phase of several SemEval systems. In this section, we describe the suite of state-of-the-art language modeling techniques for which we will present results. We begin with ngram models; first a classical n-gram backoff model (Chen and Goodman, 1999), and then a recently proposed class-based maximum-entropy n-gram model (Chen, 2009a; Chen, 2009b). N-gram models have the obvious disadvantage of using a very limited context in predicting word probabilities. Therefore we evaluate the recurrent neural net model of (Mikolov et al., 2010; Mikolov et al., 2011b). This model has produced record-breaking perplexity results in several tasks (Mikolov et al., 2011a), and has the potential to encode sentence-span information in the network hidden-layer activations. We have also evaluated the use of parse scores, using an off-theshelf stochastic context free grammar parser. However, the grammatical structure of the alternatives is often identical. With scores differing only in the final non-terminal/terminal rewrites, this did little better than chance. The use of other syntactically derived features, for example based on a dependency parse, are likely to</context>
<context position="13444" citStr="Mikolov et al., 2011" startWordPosition="2172" endWordPosition="2175">In this model, a set of neural net activations s(t) is maintained and updated at each sentence position t. These activations encapsulate the sentence history up to the tth word in a realvalued vector which typically has several hundred dimensions. The word at position t is represented as a binary vector w(t) whose length is the vocabulary size, and with a “1” in a position uniquely associated with the word, and “0” elsewhere. w(t) and s(t) are concatenated to predict an output distribution over words, y(t). Updating is done with two weight matrices u and v and nonlinear functions f() and g() (Mikolov et al., 2011b): X(t) = [w(t)Ts(t − 1)T]T sj(t) = f( � xi(t)uji) i yk(t) = g( � sj(t)vkj) j fwith f() being a sigmoid and g() a softmax: ( ) — 1 exp(z.) W 1 + exp(−z), g(z.) = Ek exp(zk) The output y(t) is a probability distribution over words, and the parameters u and v are trained with back-propagation to minimize the Kullback-Leibler (KL) divergence between the predicted and observed distributions. Because of the recurrent connections, this model is similar to a nonlinear infinite impulse response (IIR) filter, and has the potential to model long span dependencies. Theoretical considerations (Bengio et </context>
<context position="25738" citStr="Mikolov et al., 2011" startWordPosition="4336" endWordPosition="4339">his table that the language models perform similarly and achieve just above 40% on the test set. To make a more controlled comparison that normalizes for the amount of training data, we have trained Model M, and the Good-Turing model on the same data subset as the RNN, and with the same vocabulary. In Table 3, we present perplexity results on a held-out set of dev/test-relevant Los Angeles Times data, and performance on the actual SAT questions. Two things are notable. First, the recurrent neural net has dramatically lower perplexity than the other methods. This is consistent with results in (Mikolov et al., 2011a). Secondly, despite the differences in perplexity, the methods show little difference on SAT performance. Because Model M was not better, only uses n-gram context, and was used in the construction of the Holmes data (Zweig and Burges, 2011), we do not consider it further. 5.4 LSA Results Table 4 presents results for the methods of Sections 4.1 and 4.2. Of all the methods in isolation, the simple approach of Section 4.1 - to use the total cosine similarity between a potential answer and the other words in the sentence - has performed best. The apMethod Test 3-input LSA 46% LSA + Good-Turing L</context>
</contexts>
<marker>Mikolov, Deoras, Kombrink, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas Burget, and Jan Cernocky. 2011a. Empirical evaluation and combination of advanced language modeling techniques. In Proceedings of Interspeech 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network based language model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<contexts>
<context position="9779" citStr="Mikolov et al., 2011" startWordPosition="1562" endWordPosition="1565"> Section 2, this was found be be very effective in the ranking phase of several SemEval systems. In this section, we describe the suite of state-of-the-art language modeling techniques for which we will present results. We begin with ngram models; first a classical n-gram backoff model (Chen and Goodman, 1999), and then a recently proposed class-based maximum-entropy n-gram model (Chen, 2009a; Chen, 2009b). N-gram models have the obvious disadvantage of using a very limited context in predicting word probabilities. Therefore we evaluate the recurrent neural net model of (Mikolov et al., 2010; Mikolov et al., 2011b). This model has produced record-breaking perplexity results in several tasks (Mikolov et al., 2011a), and has the potential to encode sentence-span information in the network hidden-layer activations. We have also evaluated the use of parse scores, using an off-theshelf stochastic context free grammar parser. However, the grammatical structure of the alternatives is often identical. With scores differing only in the final non-terminal/terminal rewrites, this did little better than chance. The use of other syntactically derived features, for example based on a dependency parse, are likely to</context>
<context position="13444" citStr="Mikolov et al., 2011" startWordPosition="2172" endWordPosition="2175">In this model, a set of neural net activations s(t) is maintained and updated at each sentence position t. These activations encapsulate the sentence history up to the tth word in a realvalued vector which typically has several hundred dimensions. The word at position t is represented as a binary vector w(t) whose length is the vocabulary size, and with a “1” in a position uniquely associated with the word, and “0” elsewhere. w(t) and s(t) are concatenated to predict an output distribution over words, y(t). Updating is done with two weight matrices u and v and nonlinear functions f() and g() (Mikolov et al., 2011b): X(t) = [w(t)Ts(t − 1)T]T sj(t) = f( � xi(t)uji) i yk(t) = g( � sj(t)vkj) j fwith f() being a sigmoid and g() a softmax: ( ) — 1 exp(z.) W 1 + exp(−z), g(z.) = Ek exp(zk) The output y(t) is a probability distribution over words, and the parameters u and v are trained with back-propagation to minimize the Kullback-Leibler (KL) divergence between the predicted and observed distributions. Because of the recurrent connections, this model is similar to a nonlinear infinite impulse response (IIR) filter, and has the potential to model long span dependencies. Theoretical considerations (Bengio et </context>
<context position="25738" citStr="Mikolov et al., 2011" startWordPosition="4336" endWordPosition="4339">his table that the language models perform similarly and achieve just above 40% on the test set. To make a more controlled comparison that normalizes for the amount of training data, we have trained Model M, and the Good-Turing model on the same data subset as the RNN, and with the same vocabulary. In Table 3, we present perplexity results on a held-out set of dev/test-relevant Los Angeles Times data, and performance on the actual SAT questions. Two things are notable. First, the recurrent neural net has dramatically lower perplexity than the other methods. This is consistent with results in (Mikolov et al., 2011a). Secondly, despite the differences in perplexity, the methods show little difference on SAT performance. Because Model M was not better, only uses n-gram context, and was used in the construction of the Holmes data (Zweig and Burges, 2011), we do not consider it further. 5.4 LSA Results Table 4 presents results for the methods of Sections 4.1 and 4.2. Of all the methods in isolation, the simple approach of Section 4.1 - to use the total cosine similarity between a potential answer and the other words in the sentence - has performed best. The apMethod Test 3-input LSA 46% LSA + Good-Turing L</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2011b. Extensions of recurrent neural network based language model. In Proceedings of ICASSP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammed</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
</authors>
<title>Computing word pair antonymy.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1841" citStr="Mohammed et al., 2008" startWordPosition="275" endWordPosition="279">a for language processing tasks. They are valuable for many reasons: they represent facets of language understanding recognized as important by educational experts; they are organized in various formats designed to evaluate specific capabilities; they are yardsticks by which society measures educational progress; and they affect a large number of people. Previous researchers have taken advantage of this material to test both narrow and general language processing capabilities. Among the narrower tasks, the identification of synonyms and antonyms has been studied by (Landauer and Dumais, 1997; Mohammed et al., 2008; Mohammed et al., 2011; Turney et al., 2003; Turney, 2008), who used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on tes</context>
</contexts>
<marker>Mohammed, Dorr, Hirst, 2008</marker>
<rawString>Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008. Computing word pair antonymy. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammed</author>
<author>Bonnie J Dorr</author>
<author>Graeme Hirst</author>
<author>Peter D Turney</author>
</authors>
<title>Measuring degrees of semantic opposition.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>National Research Council Canada.</institution>
<contexts>
<context position="1864" citStr="Mohammed et al., 2011" startWordPosition="280" endWordPosition="283">ng tasks. They are valuable for many reasons: they represent facets of language understanding recognized as important by educational experts; they are organized in various formats designed to evaluate specific capabilities; they are yardsticks by which society measures educational progress; and they affect a large number of people. Previous researchers have taken advantage of this material to test both narrow and general language processing capabilities. Among the narrower tasks, the identification of synonyms and antonyms has been studied by (Landauer and Dumais, 1997; Mohammed et al., 2008; Mohammed et al., 2011; Turney et al., 2003; Turney, 2008), who used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on test prep material (Charni</context>
</contexts>
<marker>Mohammed, Dorr, Hirst, Turney, 2011</marker>
<rawString>Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and Peter D. Turney. 2011. Measuring degrees of semantic opposition. Technical report, National Research Council Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hwee Tou Ng</author>
<author>Leong Hwee Teo</author>
<author>Jennifer Lai Pheng Kwan</author>
</authors>
<title>A machine learning approach to answering questions for reading comprehension tests.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics - Volume 13, EMNLP ’00,</booktitle>
<pages>124--132</pages>
<contexts>
<context position="2541" citStr="Ng et al., 2000" startWordPosition="395" endWordPosition="398"> the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on test prep material (Charniak et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000; Ng et al., 2000). In this paper, we study a new class of problems intermediate in difficulty between the extremes of synonym detection and general question answering - the sentence completion questions found on the Scholastic Aptitude Test (SAT). These questions present a sentence with one or two blanks that need to be filled in. Five possible words (or short phrases) are given as options for each blank. All possible answers except one result in a nonsense sentence. Two examples are shown in Figure 1. The questions are highly constrained in the sense that all the information necessary is present in the senten</context>
</contexts>
<marker>Ng, Teo, Kwan, 2000</marker>
<rawString>Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai Pheng Kwan. 2000. A machine learning approach to answering questions for reading comprehension tests. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics - Volume 13, EMNLP ’00, pages 124–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
<author>S Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>2006</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="27854" citStr="Nocedal and Wright, 2006" startWordPosition="4703" endWordPosition="4706">e probability of the sentence under that language model. We train the parameters of the linear combination on the SAT development set. The training minimizes a loss function of pairs of answers: one correct and one incorrect fill-in from the same question. We use the RankNet loss function (Burges et al., 2005): f(�w · (�� − ��)) + A||�w||2 where x are the input features for the incorrect answer, y are the features for the correct answer, w� are the weights for the combination, and f(z) _ log(1 + exp(z)). We tune the regularizer via 5- fold cross validation, and minimize the loss using L-BFGS (Nocedal and Wright, 2006). The results on the SAT test set for combining various models are shown in Table 5. min w 607 5.6 Holmes Data Results To measure the robustness of our approaches, we have applied them to the MSR Sentence Completion set (Zweig and Burges, 2011), termed the Holmes data. In Table 6, we present the results on this set, along with the comparable SAT results. Note that the latter are derived from models trained with the Los Angeles Times data, while the Holmes results are derived from models trained with 19th-century novels. We see from this table that the results are similar across the two tasks. </context>
</contexts>
<marker>Nocedal, Wright, 2006</marker>
<rawString>J. Nocedal and S. Wright. 2006. Numerical Optimization. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>161--199</pages>
<contexts>
<context position="8352" citStr="Pado and Lapata (2007)" startWordPosition="1328" endWordPosition="1331">ilarity between a potential replacement and its context, but the results are poorer than others. Since the original word provides a strong hint as to the pos602 sible meanings of the replacements, we hypothesize that N-gram statistics are largely able to resolve the remaining ambiguities. The SAT sentence completion sentences do not have this property and thus are more challenging. Related to, but predating the Semeval lexical substitution task are the ESL synonym questions proposed by Turney (2001), and subsequently considered by numerous research groups including Terra and Clarke (2003) and Pado and Lapata (2007). These questions are similar to the SemEval task, but in addition to the original word and the sentence context, the list of options is provided. Jarmasz and Szpakowicz (2003) used a sophisticated thesaurusbased method and achieved state-of-the art performance, which is 82%. Other work on standardized tests includes the synonym and antonym tasks mentioned in Section 1, and more recent work on a SAT analogy task introduced by (Turney et al., 2003) and extensively used by other researchers (Veale, 2004; Turney and Littman, 2005; D. et al., 2009). 3 Sentence Completion via Language Modeling Perh</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33 (2), pages 161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Princeton-Review</author>
</authors>
<title>11 Practice Tests for the SAT &amp; PSAT,</title>
<date>2010</date>
<contexts>
<context position="5874" citStr="Princeton-Review, 2010" startWordPosition="920" endWordPosition="921"> Chen, 2009b). Also in the language modeling vein, but with potentially global context, we evaluate the use of a recurrent neural network language model. In all the language modeling approaches, a model is used to compute a sentence probability with each of the potential completions. To measure global coherence, we propose a novel method based on latent semantic analysis (LSA). We find that the LSA based method performs best, and that both local and global information can be combined to exceed 50% accuracy. We report results on a set of questions taken from a collection of SAT practice exams (Princeton-Review, 2010), and further validate the methods with the recently proposed MSR Sentence Completion Challenge set (Zweig and Burges, 2011). Our paper thus makes the following contributions: First, we present the first published results on the SAT sentence completion task. Secondly, we evaluate the effectiveness of both local n-gram information, and global coherence in the form of a novel LSA-based metric. Finally, we illustrate that the local and global information can be effectively fused. The remainder of this paper is organized as follows. In Section 2 we discuss related work. Section 3 describes the lan</context>
<context position="21319" citStr="Princeton-Review, 2010" startWordPosition="3577" endWordPosition="3578">he potential answerwords. Secondly, we restrict the vocabulary to the set of words present in the test set. For the sentencereconstruction method of Section 4.2, we have found it convenient to do data selection per-sentence. To enhance the expressive power of LSA, the term vocabulary can be expanded from unigrams to bigrams or trigrams of words, thus adding information about word ordering. This was also used in the reconstruction technique. 5 Experimental Results 5.1 Data Resources We present results with two datasets. The first is taken from 11 Practice Tests for the SAT &amp; PSAT 2011 Edition (Princeton-Review, 2010). This book contains eleven practice tests, and we used all the sentence completion questions in the first five tests as a development set, and all the questions in the last six tests as the test set. This resulted in sets with 95 and 108 questions respectively. Additionally, we report results on the recently released MSR Sentence Completion Challenge (Zweig and Burges, 2011). This consists of a set of 1, 040 sentence completion questions based on sentences occurring in five Conan Doyle Sherlock Holmes novels, and is identical in format to the SAT questions. Due to the source of this data, we </context>
</contexts>
<marker>Princeton-Review, 2010</marker>
<rawString>Princeton-Review. 2010. 11 Practice Tests for the SAT &amp; PSAT, 2011 Edition. The Princeton Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Michael Thelen</author>
</authors>
<title>A rule-based question answering system for reading comprehension tests.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000ANLP/NAACL Workshop on Reading comprehension</booktitle>
<pages>13--19</pages>
<contexts>
<context position="2504" citStr="Riloff and Thelen, 2000" startWordPosition="386" endWordPosition="390">2003; Turney, 2008), who used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on test prep material (Charniak et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000; Ng et al., 2000). In this paper, we study a new class of problems intermediate in difficulty between the extremes of synonym detection and general question answering - the sentence completion questions found on the Scholastic Aptitude Test (SAT). These questions present a sentence with one or two blanks that need to be filled in. Five possible words (or short phrases) are given as options for each blank. All possible answers except one result in a nonsense sentence. Two examples are shown in Figure 1. The questions are highly constrained in the sense that all the informati</context>
</contexts>
<marker>Riloff, Thelen, 2000</marker>
<rawString>Ellen Riloff and Michael Thelen. 2000. A rule-based question answering system for reading comprehension tests. In Proceedings of the 2000ANLP/NAACL Workshop on Reading comprehension tests as evaluation for computer-based language understanding sytems - Volume 6, ANLP/NAACL-ReadingComp ’00, pages 13– 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A Vector Space Model for Automatic Indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="15737" citStr="Salton et al., 1975" startWordPosition="2562" endWordPosition="2565">e input is a collection of n documents which are expressed in terms of words from a vocabulary of size m. These documents may be actual documents such as newspaper articles, or simply as in our case notional documents such as sentences. Next, a m x n matrix W is formed. At its simplest, the ij&amp;quot; entry contains the number of times word i has occurred in document j - its term frequency or TF value. More conventionally, the entry is weighted by some notion of the importance of word i, for example the negative logarithm of the fraction of documents that contain it, resulting in a TF-IDF weighting (Salton et al., 1975). Finally, to obtain a subspace representation of dimension d, W is decomposed as W &apos;,Z� USVT where U is m x d, VT is d x n, and S is a d x d diagonal matrix. In applications, d &lt;&lt; n and d &lt;&lt; m; for example one might have a 50, 000 word vocabulary and 1, 000, 000 documents and use a 300 dimensional subspace representation. An important property of SVD is that the rows of US - which represents the words - behave similarly to the original rows of W, in the sense that the cosine similarity between two rows in US approximates the cosine similarity between the corre604 sponding rows in W. Cosine si</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A Vector Space Model for Automatic Indexing. Communications of the ACM, 18(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff Chiung-Yu Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 International Conference on Machine Learning (ICML-2011).</booktitle>
<contexts>
<context position="12822" citStr="Socher et al., 2011" startWordPosition="2062" endWordPosition="2065"> models in which the probability of a word or class label l given history h is determined by � exp(� 1 k fk(h, l)). The features fk(h, l) used are the presence of various patterns in the concatenation of hl, for example whether a particular suffix is present in hl. 3.3 Recurrent Neural Net Language Model Many of the questions involve long-range dependencies between words. While n-gram models have no ability to explicitly maintain long-span context, the recently proposed recurrent neural-net model of (Mikolov et al., 2010) does. Related approaches have been proposed by (Sutskever et al., 2011; Socher et al., 2011). In this model, a set of neural net activations s(t) is maintained and updated at each sentence position t. These activations encapsulate the sentence history up to the tth word in a realvalued vector which typically has several hundred dimensions. The word at position t is represented as a binary vector w(t) whose length is the vocabulary size, and with a “1” in a position uniquely associated with the word, and “0” elsewhere. w(t) and s(t) are concatenated to predict an output distribution over words, y(t). Updating is done with two weight matrices u and v and nonlinear functions f() and g()</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 2011 International Conference on Machine Learning (ICML-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 International Conference on Machine Learning (ICML-2011).</booktitle>
<contexts>
<context position="12800" citStr="Sutskever et al., 2011" startWordPosition="2058" endWordPosition="2061">s maximum entropy n-gram models in which the probability of a word or class label l given history h is determined by � exp(� 1 k fk(h, l)). The features fk(h, l) used are the presence of various patterns in the concatenation of hl, for example whether a particular suffix is present in hl. 3.3 Recurrent Neural Net Language Model Many of the questions involve long-range dependencies between words. While n-gram models have no ability to explicitly maintain long-span context, the recently proposed recurrent neural-net model of (Mikolov et al., 2010) does. Related approaches have been proposed by (Sutskever et al., 2011; Socher et al., 2011). In this model, a set of neural net activations s(t) is maintained and updated at each sentence position t. These activations encapsulate the sentence history up to the tth word in a realvalued vector which typically has several hundred dimensions. The word at position t is represented as a binary vector w(t) whose length is the vocabulary size, and with a “1” in a position uniquely associated with the word, and “0” elsewhere. w(t) and s(t) are concatenated to predict an output distribution over words, y(t). Updating is done with two weight matrices u and v and nonlinear</context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 2011 International Conference on Machine Learning (ICML-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Terra</author>
<author>C Clarke</author>
</authors>
<title>Frequency estimates for statistical word similarity measures.</title>
<date>2003</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="8325" citStr="Terra and Clarke (2003)" startWordPosition="1323" endWordPosition="1326">to measure the degree of similarity between a potential replacement and its context, but the results are poorer than others. Since the original word provides a strong hint as to the pos602 sible meanings of the replacements, we hypothesize that N-gram statistics are largely able to resolve the remaining ambiguities. The SAT sentence completion sentences do not have this property and thus are more challenging. Related to, but predating the Semeval lexical substitution task are the ESL synonym questions proposed by Turney (2001), and subsequently considered by numerous research groups including Terra and Clarke (2003) and Pado and Lapata (2007). These questions are similar to the SemEval task, but in addition to the original word and the sentence context, the list of options is provided. Jarmasz and Szpakowicz (2003) used a sophisticated thesaurusbased method and achieved state-of-the art performance, which is 82%. Other work on standardized tests includes the synonym and antonym tasks mentioned in Section 1, and more recent work on a SAT analogy task introduced by (Turney et al., 2003) and extensively used by other researchers (Veale, 2004; Turney and Littman, 2005; D. et al., 2009). 3 Sentence Completion</context>
</contexts>
<marker>Terra, Clarke, 2003</marker>
<rawString>E. Terra and C. Clarke. 2003. Frequency estimates for statistical word similarity measures. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Corpus-based learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<pages>1--3</pages>
<contexts>
<context position="8884" citStr="Turney and Littman, 2005" startWordPosition="1416" endWordPosition="1419">red by numerous research groups including Terra and Clarke (2003) and Pado and Lapata (2007). These questions are similar to the SemEval task, but in addition to the original word and the sentence context, the list of options is provided. Jarmasz and Szpakowicz (2003) used a sophisticated thesaurusbased method and achieved state-of-the art performance, which is 82%. Other work on standardized tests includes the synonym and antonym tasks mentioned in Section 1, and more recent work on a SAT analogy task introduced by (Turney et al., 2003) and extensively used by other researchers (Veale, 2004; Turney and Littman, 2005; D. et al., 2009). 3 Sentence Completion via Language Modeling Perhaps the most straightforward approach to solving the sentence completion task is to form the complete sentence with each option in turn, and to evaluate its likelihood under a language model. As discussed in Section 2, this was found be be very effective in the ranking phase of several SemEval systems. In this section, we describe the suite of state-of-the-art language modeling techniques for which we will present results. We begin with ngram models; first a classical n-gram backoff model (Chen and Goodman, 1999), and then a r</context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter Turney and Michael Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine Learning, 60 (1-3), pages 251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
<author>Jeffrey Bigham</author>
<author>Victor Shnayder</author>
</authors>
<title>Combining independent modules to solve multiple-choice synonym and analogy problems.</title>
<date>2003</date>
<booktitle>In Recent Advances in Natural Language Processing (RANLP).</booktitle>
<contexts>
<context position="1885" citStr="Turney et al., 2003" startWordPosition="284" endWordPosition="288">able for many reasons: they represent facets of language understanding recognized as important by educational experts; they are organized in various formats designed to evaluate specific capabilities; they are yardsticks by which society measures educational progress; and they affect a large number of people. Previous researchers have taken advantage of this material to test both narrow and general language processing capabilities. Among the narrower tasks, the identification of synonyms and antonyms has been studied by (Landauer and Dumais, 1997; Mohammed et al., 2008; Mohammed et al., 2011; Turney et al., 2003; Turney, 2008), who used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on test prep material (Charniak et al., 2000; Rilo</context>
<context position="8803" citStr="Turney et al., 2003" startWordPosition="1404" endWordPosition="1407">the ESL synonym questions proposed by Turney (2001), and subsequently considered by numerous research groups including Terra and Clarke (2003) and Pado and Lapata (2007). These questions are similar to the SemEval task, but in addition to the original word and the sentence context, the list of options is provided. Jarmasz and Szpakowicz (2003) used a sophisticated thesaurusbased method and achieved state-of-the art performance, which is 82%. Other work on standardized tests includes the synonym and antonym tasks mentioned in Section 1, and more recent work on a SAT analogy task introduced by (Turney et al., 2003) and extensively used by other researchers (Veale, 2004; Turney and Littman, 2005; D. et al., 2009). 3 Sentence Completion via Language Modeling Perhaps the most straightforward approach to solving the sentence completion task is to form the complete sentence with each option in turn, and to evaluate its likelihood under a language model. As discussed in Section 2, this was found be be very effective in the ranking phase of several SemEval systems. In this section, we describe the suite of state-of-the-art language modeling techniques for which we will present results. We begin with ngram mode</context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining independent modules to solve multiple-choice synonym and analogy problems. In Recent Advances in Natural Language Processing (RANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In European Conference on Machine Learning (ECML).</booktitle>
<contexts>
<context position="8234" citStr="Turney (2001)" startWordPosition="1312" endWordPosition="1313">iano et al., 2007). That paper also explores the use of Latent Semantic Analysis to measure the degree of similarity between a potential replacement and its context, but the results are poorer than others. Since the original word provides a strong hint as to the pos602 sible meanings of the replacements, we hypothesize that N-gram statistics are largely able to resolve the remaining ambiguities. The SAT sentence completion sentences do not have this property and thus are more challenging. Related to, but predating the Semeval lexical substitution task are the ESL synonym questions proposed by Turney (2001), and subsequently considered by numerous research groups including Terra and Clarke (2003) and Pado and Lapata (2007). These questions are similar to the SemEval task, but in addition to the original word and the sentence context, the list of options is provided. Jarmasz and Szpakowicz (2003) used a sophisticated thesaurusbased method and achieved state-of-the art performance, which is 82%. Other work on standardized tests includes the synonym and antonym tasks mentioned in Section 1, and more recent work on a SAT analogy task introduced by (Turney et al., 2003) and extensively used by other </context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In European Conference on Machine Learning (ECML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="1900" citStr="Turney, 2008" startWordPosition="289" endWordPosition="290">: they represent facets of language understanding recognized as important by educational experts; they are organized in various formats designed to evaluate specific capabilities; they are yardsticks by which society measures educational progress; and they affect a large number of people. Previous researchers have taken advantage of this material to test both narrow and general language processing capabilities. Among the narrower tasks, the identification of synonyms and antonyms has been studied by (Landauer and Dumais, 1997; Mohammed et al., 2008; Mohammed et al., 2011; Turney et al., 2003; Turney, 2008), who used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on test prep material (Charniak et al., 2000; Riloff and Thelen, </context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
</authors>
<title>Wordnet sits the sat: A knowledge-based approach to lexical analogy.</title>
<date>2004</date>
<booktitle>In European Conference on Artificial Intelligence (ECAI).</booktitle>
<contexts>
<context position="8858" citStr="Veale, 2004" startWordPosition="1414" endWordPosition="1415">ently considered by numerous research groups including Terra and Clarke (2003) and Pado and Lapata (2007). These questions are similar to the SemEval task, but in addition to the original word and the sentence context, the list of options is provided. Jarmasz and Szpakowicz (2003) used a sophisticated thesaurusbased method and achieved state-of-the art performance, which is 82%. Other work on standardized tests includes the synonym and antonym tasks mentioned in Section 1, and more recent work on a SAT analogy task introduced by (Turney et al., 2003) and extensively used by other researchers (Veale, 2004; Turney and Littman, 2005; D. et al., 2009). 3 Sentence Completion via Language Modeling Perhaps the most straightforward approach to solving the sentence completion task is to form the complete sentence with each option in turn, and to evaluate its likelihood under a language model. As discussed in Section 2, this was found be be very effective in the ranking phase of several SemEval systems. In this section, we describe the suite of state-of-the-art language modeling techniques for which we will present results. We begin with ngram models; first a classical n-gram backoff model (Chen and Go</context>
</contexts>
<marker>Veale, 2004</marker>
<rawString>T. Veale. 2004. Wordnet sits the sat: A knowledge-based approach to lexical analogy. In European Conference on Artificial Intelligence (ECAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>J Auer</author>
<author>R Parasuraman</author>
<author>I Zubarev</author>
<author>D Brandyberry</author>
<author>M P Harper</author>
</authors>
<title>A question answering system developed as a project in a natural language processing course.</title>
<date>2000</date>
<booktitle>In Proceedings of the</booktitle>
<pages>28--35</pages>
<contexts>
<context position="2523" citStr="Wang et al., 2000" startWordPosition="391" endWordPosition="394">used questions from the Test of English as a Foreign Language (TOEFL), Graduate Record Exams (GRE) and English as a Second Language (ESL) exams. Tasks requiring broader competencies include logic puzzles and reading comprehension. Logic puzzles drawn from the Law School Administration Test (LSAT) and the GRE were studied in (Lev et al., 2004), which combined an extensive array of techniques to solve the problems. The DeepRead system (Hirschman et al., 1999) initiated a long line of research into reading comprehension based on test prep material (Charniak et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000; Ng et al., 2000). In this paper, we study a new class of problems intermediate in difficulty between the extremes of synonym detection and general question answering - the sentence completion questions found on the Scholastic Aptitude Test (SAT). These questions present a sentence with one or two blanks that need to be filled in. Five possible words (or short phrases) are given as options for each blank. All possible answers except one result in a nonsense sentence. Two examples are shown in Figure 1. The questions are highly constrained in the sense that all the information necessary is pre</context>
</contexts>
<marker>Wang, Auer, Parasuraman, Zubarev, Brandyberry, Harper, 2000</marker>
<rawString>W. Wang, J. Auer, R. Parasuraman, I. Zubarev, D. Brandyberry, and M. P. Harper. 2000. A question answering system developed as a project in a natural language processing course. In Proceedings of the 2000 ANLP/NAACL Workshop on Reading comprehension tests as evaluation for computerbased language understanding sytems - Volume 6, ANLP/NAACL-ReadingComp ’00, pages 28–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Ku: word sense disambiguation by substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval ’07,</booktitle>
<pages>207--213</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7237" citStr="Yuret, 2007" startWordPosition="1144" endWordPosition="1145">cussion in Section 6. 2 Related Work The past work which is most similar to ours is derived from the lexical substitution track of SemEval2007 (McCarthy and Navigli, 2007). In this task, the challenge is to find a replacement for a word or phrase removed from a sentence. In contrast to our SAT-inspired task, the original answer is indicated. For example, one might be asked to find alternates for match in “After the match, replace any remaining fluid deficit to prevent problems of chronic dehydration throughout the tournament.” Two consistently high-performing systems for this task are the KU (Yuret, 2007) and UNT (Hassan et al., 2007) systems. These operate in two phases: first they find a set of potential replacement words, and then they rank them. The KU system uses just an N-gram language model to do this ranking. The UNT system uses a large variety of information sources, and a language model score receives the highest weight. N-gram statistics were also very effective in (Giuliano et al., 2007). That paper also explores the use of Latent Semantic Analysis to measure the degree of similarity between a potential replacement and its context, but the results are poorer than others. Since the </context>
</contexts>
<marker>Yuret, 2007</marker>
<rawString>Deniz Yuret. 2007. Ku: word sense disambiguation by substitution. In Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval ’07, pages 207–213, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Zweig</author>
<author>Christopher J C Burges</author>
</authors>
<title>The Microsoft Research sentence completion challenge.</title>
<date>2011</date>
<tech>Technical Report MSR-TR-2011-129, Microsoft.</tech>
<contexts>
<context position="5998" citStr="Zweig and Burges, 2011" startWordPosition="936" endWordPosition="939">eural network language model. In all the language modeling approaches, a model is used to compute a sentence probability with each of the potential completions. To measure global coherence, we propose a novel method based on latent semantic analysis (LSA). We find that the LSA based method performs best, and that both local and global information can be combined to exceed 50% accuracy. We report results on a set of questions taken from a collection of SAT practice exams (Princeton-Review, 2010), and further validate the methods with the recently proposed MSR Sentence Completion Challenge set (Zweig and Burges, 2011). Our paper thus makes the following contributions: First, we present the first published results on the SAT sentence completion task. Secondly, we evaluate the effectiveness of both local n-gram information, and global coherence in the form of a novel LSA-based metric. Finally, we illustrate that the local and global information can be effectively fused. The remainder of this paper is organized as follows. In Section 2 we discuss related work. Section 3 describes the language modeling methods we have evaluated. Section 4 outlines the LSA-based methods. Section 5 presents our experimental resu</context>
<context position="21697" citStr="Zweig and Burges, 2011" startWordPosition="3638" endWordPosition="3641">word ordering. This was also used in the reconstruction technique. 5 Experimental Results 5.1 Data Resources We present results with two datasets. The first is taken from 11 Practice Tests for the SAT &amp; PSAT 2011 Edition (Princeton-Review, 2010). This book contains eleven practice tests, and we used all the sentence completion questions in the first five tests as a development set, and all the questions in the last six tests as the test set. This resulted in sets with 95 and 108 questions respectively. Additionally, we report results on the recently released MSR Sentence Completion Challenge (Zweig and Burges, 2011). This consists of a set of 1, 040 sentence completion questions based on sentences occurring in five Conan Doyle Sherlock Holmes novels, and is identical in format to the SAT questions. Due to the source of this data, we refer to it as the Holmes data. To train models, we have experimented with a variety of data sources. Since there is no publically available collection of SAT questions suitable to training, our methods have all relied on unsupervised data. Early on, we ran a set of experiments to determine the relevance of different types of data. Thinking that data from an encyclopedia Data</context>
<context position="23290" citStr="Zweig and Burges, 2011" startWordPosition="3913" endWordPosition="3916">Wikipedia as of January 2011, broken down into sentences, with filtering to remove sentences consisting of URLs and Wiki author comments. Finally, we used a commercial newspaper dataset consisting of all the Los Angeles Times data from 1985 to 2002, containing about 1.1B words. These data sources were evaluated using the baseline n-gram LM approach of Section 3.1. Initial experiments indicated that that the Los Angeles Times data is best suited to this task (see Table 1), and our SAT experiments use this source. For the MSR Sentence Completion data, we obtained the training data specified in (Zweig and Burges, 2011), consisting of approximately 500 19th-century novels available from Project Gutenberg, and comprising 48M words. 5.2 Human Performance To provide human benchmark performance, we asked six native speaking high school students and five graduate students to answer the questions on the development set. The high-schoolers attained 87% accuracy and the graduate students 95%. Zweig and Burges (2011) cite a human performance of 91% on the Holmes data. Statistics from a large crosssection of the population are not available. As a further point of comparison, we note that chance performance is 20%. 5.3</context>
<context position="25980" citStr="Zweig and Burges, 2011" startWordPosition="4374" endWordPosition="4377">e same data subset as the RNN, and with the same vocabulary. In Table 3, we present perplexity results on a held-out set of dev/test-relevant Los Angeles Times data, and performance on the actual SAT questions. Two things are notable. First, the recurrent neural net has dramatically lower perplexity than the other methods. This is consistent with results in (Mikolov et al., 2011a). Secondly, despite the differences in perplexity, the methods show little difference on SAT performance. Because Model M was not better, only uses n-gram context, and was used in the construction of the Holmes data (Zweig and Burges, 2011), we do not consider it further. 5.4 LSA Results Table 4 presents results for the methods of Sections 4.1 and 4.2. Of all the methods in isolation, the simple approach of Section 4.1 - to use the total cosine similarity between a potential answer and the other words in the sentence - has performed best. The apMethod Test 3-input LSA 46% LSA + Good-Turing LM 53 LSA + Good-Turing LM + RNN 52 Table 5: SAT test set accuracy with combined methods. proach of using reconstruction error performed very well on the development set, but unremarkably on the test set. 5.5 Combination Results A well-known t</context>
<context position="28098" citStr="Zweig and Burges, 2011" startWordPosition="4747" endWordPosition="4750"> question. We use the RankNet loss function (Burges et al., 2005): f(�w · (�� − ��)) + A||�w||2 where x are the input features for the incorrect answer, y are the features for the correct answer, w� are the weights for the combination, and f(z) _ log(1 + exp(z)). We tune the regularizer via 5- fold cross validation, and minimize the loss using L-BFGS (Nocedal and Wright, 2006). The results on the SAT test set for combining various models are shown in Table 5. min w 607 5.6 Holmes Data Results To measure the robustness of our approaches, we have applied them to the MSR Sentence Completion set (Zweig and Burges, 2011), termed the Holmes data. In Table 6, we present the results on this set, along with the comparable SAT results. Note that the latter are derived from models trained with the Los Angeles Times data, while the Holmes results are derived from models trained with 19th-century novels. We see from this table that the results are similar across the two tasks. The best performing single model is LSA total word similarity. For the Holmes data, combining the models outperforms any single model. We train the linear combination function via 5-fold cross-validation: the model is trained five times, each t</context>
</contexts>
<marker>Zweig, Burges, 2011</marker>
<rawString>Geoffrey Zweig and Christopher J.C. Burges. 2011. The Microsoft Research sentence completion challenge. Technical Report MSR-TR-2011-129, Microsoft.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>