<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000789">
<title confidence="0.998202">
Empirically Estimating Order Constraints for
Content Planning in Generation
</title>
<author confidence="0.930506">
Pablo A. Duboue and Kathleen R. McKeown
</author>
<affiliation confidence="0.888596333333333">
Computer Science Department
Columbia University
10027, New York, NY, USA
</affiliation>
<email confidence="0.999103">
{pablo,kathy}@cs.columbia.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970391304348">
In a language generation system, a
content planner embodies one or more
“plans” that are usually hand–crafted,
sometimes through manual analysis of
target text. In this paper, we present a
system that we developed to automati-
cally learn elements of a plan and the
ordering constraints among them. As
training data, we use semantically an-
notated transcripts of domain experts
performing the task our system is de-
signed to mimic. Given the large degree
of variation in the spoken language of
the transcripts, we developed a novel al-
gorithm to find parallels between tran-
scripts based on techniques used in
computational genomics. Our proposed
methodology was evaluated two–fold:
the learning and generalization capabil-
ities were quantitatively evaluated us-
ing cross validation obtaining a level of
accuracy of 89%. A qualitative evalua-
tion is also provided.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903707317073">
In a language generation system, a content plan-
ner typically uses one or more “plans” to rep-
resent the content to be included in the out-
put and the ordering between content elements.
Some researchers rely on generic planners (e.g.,
(Dale, 1988)) for this task, while others use plans
based on Rhetorical Structure Theory (RST) (e.g.,
(Bouayad-Aga et al., 2000; Moore and Paris,
1993; Hovy, 1993)) or schemas (e.g., (McKe-
own, 1985; McKeown et al., 1997)). In all cases,
constraints on application of rules (e.g., plan op-
erators), which determine content and order, are
usually hand-crafted, sometimes through manual
analysis of target text.
In this paper, we present a method for learn-
ing the basic patterns contained within a plan and
the ordering among them. As training data, we
use semantically tagged transcripts of domain ex-
perts performing the task our system is designed
to mimic, an oral briefing of patient status af-
ter undergoing coronary bypass surgery. Given
that our target output is spoken language, there is
some level of variability between individual tran-
scripts. It is difficult for a human to see patterns
in the data and thus supervised learning based on
hand-tagged training sets can not be applied. We
need a learning algorithm that can discover order-
ing patterns in apparently unordered input.
We based our unsupervised learning algorithm
on techniques used in computational genomics
(Durbin et al., 1998), where from large amounts
of seemingly unorganized genetic sequences, pat-
terns representing meaningful biological features
are discovered. In our application, a transcript is
the equivalent of a sequence and we are searching
for patterns that occur repeatedly across multiple
sequences. We can think of these patterns as the
basic elements of a plan, representing small clus-
ters of semantic units that are similar in size, for
example, to the nucleus-satellite pairs of RST.1
By learning ordering constraints over these ele-
</bodyText>
<footnote confidence="0.662786">
&apos;Note, however, that we do not learn or represent inten-
tion.
</footnote>
<note confidence="0.6879455">
age, gender, pmh, pmh, pmh, pmh, med-preop,
med-preop, med-preop, drip-preop, med-preop,
ekg-preop, echo-preop, hct-preop, procedure,
. . .
</note>
<figureCaption confidence="0.9644815">
Figure 2: The semantic sequence obtained from
the transcript shown in Figure 1.
</figureCaption>
<bodyText confidence="0.999982074074074">
ments, we produce a plan that can be expressed
as a constraint-satisfaction problem. In this pa-
per, we focus on learning the plan elements and
the ordering constraints between them. Our sys-
tem uses combinatorial pattern matching (Rigout-
sos and Floratos, 1998) combined with clustering
to learn plan elements. Subsequently, it applies
counting procedures to learn ordering constraints
among these elements.
Our system produced a set of 24 schemata
units, that we call “plan elements”2, and 29 order-
ing constraints between these basic plan elements,
which we compared to the elements contained in
the orginal hand-crafted plan that was constructed
based on hand-analysis of transcripts, input from
domain experts, and experimental evaluation of
the system (McKeown et al., 2000).
The remainder of this article is organized as
follows: first the data used in our experiments
is presented and its overall structure and acqui-
sition methodology are analyzed. In Section 3
our techniques are described, together with their
grounding in computational genomics. The quan-
titative and qualitative evaluation are discussed
in Section 4. Related work is presented in Sec-
tion 5. Conclusions and future work are discussed
in Section 6.
</bodyText>
<sectionHeader confidence="0.92182" genericHeader="general terms">
2 Our data
</sectionHeader>
<bodyText confidence="0.9994793">
Our research is part of MAGIC (Dalal et al., 1996;
McKeown et al., 2000), a system that is designed
to produce a briefing of patient status after un-
dergoing a coronary bypass operation. Currently,
when a patient is brought to the intensive care
unit (ICU) after surgery, one of the residents who
was present in the operating room gives a brief-
ing to the ICU nurses and residents. Several of
these briefings were collected and annotated for
the aforementioned evaluation. The resident was
</bodyText>
<footnote confidence="0.634075">
2These units can be loosely related to the concept ofines-
sages in (Reiter and Dale, 2000).
</footnote>
<bodyText confidence="0.999779833333334">
equipped with a wearable tape recorder to tape
the briefings, which were transcribed to provide
the base of our empirical data. The text was sub-
sequently annotated with semantic tags as shown
in Figure 1. The figure shows that each sentence
is split into several semantically tagged chunks.
The tag-set was developed with the assistance of
a domain expert in order to capture the different
information types that are important for commu-
nication and the tagging process was done by two
non-experts, after measuring acceptable agree-
ment levels with the domain expert (see (McK-
eown et al., 2000)). The tag-set totalled over 200
tags. These 200 tags were then mapped to 29 cat-
egories, which was also done by a domain expert.
These categories are the ones used for our current
research.
From these transcripts, we derive the sequences
of semantic tags for each transcript. These se-
quences constitute the input and working material
of our analysis, they are an average length of 33
tags per transcript (min = 13, max = 66, Q =
11.6). A tag-set distribution analysis showed that
some of the categories dominate the tag counts.
Furthermore, some tags occur fairly regularly to-
wards either the beginning (e.g., date-of-birth) or
the end (e.g., urine-output) of the transcript, while
others (e.g., intraop-problems) are spread more or
less evenly throughout.
Getting these transcripts is a highly expensive
task involving the cooperation and time of nurses
and physicians in the busy ICU. Our corpus con-
tains a total number of 24 transcripts. Therefore,
it is important that we develop techniques that can
detect patterns without requiring large amounts of
data.
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="keywords">
3 Methods
</sectionHeader>
<bodyText confidence="0.976830875">
During the preliminary analysis for this research,
we looked for techniques to deal with analysis of
regularities in sequences of finite items (semantic
tags, in this case). We were interested in devel-
oping techniques that could scale as well as work
with small amounts of highly varied sequences.
Computational biology is another branch of
computer science that has this problem as one
topic of study. We focused on motif detection
techniques as a way to reduce the complexity of
the overall setting of the problem. In biological
He is 58-year-old male. History is significant for Hodgkin’s disease, treated
age gender pmh
with ... to his neck, back and chest. Hyperspadias , BPH , hiatal hernia and
pmh pmh pmh
proliferative lymph edema in his right arm . No IV’s or blood pressure down in the left
</bodyText>
<figure confidence="0.9592122">
pmh
arm. Medications — Inderal , Lopid , Pepcid , nitroglycerine and heparin. EKG has PAC’s.
med-preop med-preop med-preop drip-preop med-preop ekg-preop
His Echo showed AI, MR of 47 cine amps with hypokinetic basal and anterior apical region.
echo-preop
Hematocrit 1.2
hct-preop
2 vessel CABG off pump both mammaries ......
procedure
, otherwise his labs are unremarkable. Went to OR for what was felt to be
</figure>
<figureCaption confidence="0.999933">
Figure 1: An annotated transcription of an ICU briefing (after anonymising).
</figureCaption>
<bodyText confidence="0.999900444444444">
terms, a motif is a small subsequence, highly con-
served through evolution. From the computer sci-
ence standpoint, a motif is a fixed-order pattern,
simply because it is a subsequence. The problem
of detecting such motifs in large databases has
attracted considerable interest in the last decade
(see (Hudak and McClure, 1999) for a recent sur-
vey). Combinatorial pattern discovery, one tech-
nique developed for this problem, promised to
be a good fit for our task because it can be pa-
rameterized to operate successfully without large
amounts of data and it will be able to iden-
tify domain swapped motifs: for example, given
a–b–c in one sequence and c–b–a in another.
This difference is central to our current research,
given that order constraints are our main focus.
TEIRESIAS (Rigoutsos and Floratos, 1998) and
SPLASH (Califano, 1999) are good representa-
tives of this kind of algorithm. We used an adap-
tation of TEIRESIAS.
The algorithm can be sketched as follows: we
apply combinatorial pattern discovery (see Sec-
tion 3.1) to the semantic sequences. The obtained
patterns are refined through clustering (Section
3.2). Counting procedures are then used to es-
timate order constraints between those clusters
(Section 3.3).
</bodyText>
<subsectionHeader confidence="0.997342">
3.1 Pattern detection
</subsectionHeader>
<bodyText confidence="0.999484837837838">
In this section, we provide a brief explanation of
our pattern discovery methodology. The explana-
tion builds on the definitions below:
(L, W) pattern. Given that E represents the se-
mantic tags alphabet, a pattern is a string of
the form E (E|?)* E, where ? represents a
don’t care (wildcard) position. The (L, W)
parameters are used to further control the
amount and placement of the don’t cares:
every subsequence of length W, at least L
positions must be filled (i.e., they are non-
wildcards characters). This definition entails
that L G W and also that a (L, W) pattern
is also a (L, W + 1) pattern, etc.
Support. The support of pattern p given a set of
sequences S is the number of sequences that
contain at least one match of p. It indicates
how useful a pattern is in a certain environ-
ment.
Offset list. The offset list records the matching
locations of a pattern p in a list of sequences.
They are sets of ordered pairs, where the first
position records the sequence number and
the second position records the offset in that
sequence where p matches (see Figure 3).
Specificity. We define a partial order relation on
the pattern space as follows: a pattern p is
said to be more specific than a pattern q
if: (1) p is equal to q in the defined posi-
tions of q but has fewer undefined (i.e., wild-
cards) positions; or (2) q is a substring of p.
Specificity provides a notion of complexity
of a pattern (more specific patterns are more
complex). See Figure 4 for an example.
Using the previous definitions, the algorithm re-
duces to the problem of, given a set of sequences,
L, W, a minimum windowsize, and a support
</bodyText>
<figureCaption confidence="0.9614855">
Figure 5: The process of generalizing an existing
subsequence.
</figureCaption>
<figure confidence="0.999853">
pattern: AB?D
0 1 2 3 4 5 6 7 8 ... offset
seqa:
seqo:
...
offset list: {(a, 0); (a, 6); (Q, 2); ...}
F
A B C D F
C
A B
D D
A A
F F
B
F
D ...
F subsequence
AB?DEF   
 . . . ABCD?F patterns
D E
C
A B
</figure>
<figureCaption confidence="0.999985">
Figure 3: A pattern, a set of sequences and an
offset list.
Figure 4: The specificity relation among patterns.
</figureCaption>
<bodyText confidence="0.993125958333334">
threshold, finding maximal (L, W)-patterns with
at least a support of support threshold. Our im-
plementation can be sketched as follows:
Scanning. For a given window size n, all the pos-
sible subsequences (i.e., n-grams) occurring
in the training set are identified. This process
is repeated for different window sizes.
Generalizing. For each of the identified subse-
quences, patterns are created by replacing
valid positions (i.e., any place but the first
and last positions) with wildcards. Only
(L, W) patterns with support greater than
support threshold are kept. Figure 5 shows
an example.
Filtering. The above process is repeated increas-
ing the window size until no patterns with
enough support are found. The list of iden-
tified patterns is then filtered according to
specificity: given two patterns in the list, one
of them more specific than the other, if both
have offset lists of equal size, the less spe-
cific one is pruned3. This gives us the list
of maximal motifs (i.e. patterns) which are
supported by the training data.
</bodyText>
<footnote confidence="0.964295">
3Since they match in exactly the same positions, we
prune the less specific one, as it adds no new information.
</footnote>
<subsectionHeader confidence="0.998499">
3.2 Clustering
</subsectionHeader>
<bodyText confidence="0.99998905">
After the detection of patterns is finished, the
number of patterns is relatively large. Moreover,
as they have fixed length, they tend to be pretty
similar. In fact, many tend to have their support
from the same subsequences in the corpus. We are
interested in syntactic similarity as well as simi-
larity in context.
A convenient solution was to further cluster the
patterns, according to an approximate matching
distance measure between patterns, defined in an
appendix at the end of the paper.
We use agglomerative clustering with the dis-
tance between clusters defined as the maximum
pairwise distance between elements of the two
clusters. Clustering stops when no inter-cluster
distance falls below a user-defined threshold.
Each of the resulting clusters has a single pat-
tern represented by the centroid of the cluster.
This concept is useful for visualization of the
cluster in qualitative evaluation.
</bodyText>
<subsectionHeader confidence="0.996734">
3.3 Constraints inference
</subsectionHeader>
<bodyText confidence="0.9994668125">
The last step of our algorithm measures the fre-
quencies of all possible order constraints among
pairs of clusters, retaining those that occur of-
ten enough to be considered important, accord-
ing to some relevancy measure. We also discard
any constraint that it is violated in any training
sequence. We do this in order to obtain clear-cut
constraints. Using the number of times a given
constraint is violated as a quality measure is a
straight-forward extension of our framework. The
algorithm proceeds as follows: we build a table
of counts that is updated every time a pair of pat-
terns belonging to particular clusters are matched.
To obtain clear-cut constraints, we do not count
overlapping occurrences of patterns.
From the table of counts we need some rele-
</bodyText>
<figure confidence="0.753655">
ABC??DF
ABCA?DF ABC??DFG
 less specific than
 
</figure>
<bodyText confidence="0.9983464">
vancy measure, as the distribution of the tags is
skewed. We use a simple heuristic to estimate
a relevancy measure over the constraints that are
never contradicted. We are trying to obtain an es-
timate of
</bodyText>
<equation confidence="0.856782666666667">
Pr (A precedes B)
from the counts of
c = A ˜ preceded B
</equation>
<bodyText confidence="0.882035666666667">
We normalize with these counts (where x ranges
over all the patterns that match before/after A or
B):
</bodyText>
<equation confidence="0.913031">
c1 = A ˜ preceded x
and
c2 = x ˜ preceded B
</equation>
<bodyText confidence="0.989241333333333">
The obtained estimates, e1 = c/c1 and e2 = c/c2,
will in general yield different numbers. We use
the arithmetic mean between both, e = (e1+e2),
</bodyText>
<page confidence="0.430878">
2
</page>
<bodyText confidence="0.999857333333333">
as the final estimate for each constraint. It turns
out to be a good estimate, that predicts accuracy
of the generated constraints (see Section 4).
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="introduction">
4 Results
</sectionHeader>
<bodyText confidence="0.999882">
We use cross validation to quantitatively evaluate
our results and a comparison against the plan of
our existing system for qualitative evaluation.
</bodyText>
<subsectionHeader confidence="0.996131">
4.1 Quantitative evaluation
</subsectionHeader>
<bodyText confidence="0.999352470588235">
We evaluated two items: how effective the pat-
terns and constraints learned were in an unseen
test set and how accurate the predicted constraints
were. More precisely:
Pattern Confidence. This figure measures the
percentage of identified patterns that were
able to match a sequence in the test set.
Constraint Confidence. An ordering constraint
between two clusters can only be checkable
on a given sequence if at least one pattern
from each cluster is present. We measure
the percentage of the learned constraints that
are indeed checkable over the set of test se-
quences.
Constraint Accuracy. This is, from our perspec-
tive, the most important judgement. It mea-
sures the percentage of checkable ordering
</bodyText>
<tableCaption confidence="0.999523">
Table 1: Evaluation results.
</tableCaption>
<table confidence="0.99719675">
Test Result
pattern confidence 84.62%
constraint confidence 66.70%
constraint accuracy 89.45%
</table>
<bodyText confidence="0.99763815">
constraints that are correct, i.e., the order
constraint was maintained in any pair of
matching patterns from both clusters in all
the test-set sequences.
Using 3-fold cross-validation for computing these
metrics, we obtained the results shown in Ta-
ble 1 (averaged over 100 executions of the exper-
iment). The different parameter settings were de-
fined as follows: for the motif detection algorithm
(L, W) = (2,3) and support threshold of 3. The
algorithm will normally find around 100 maximal
motifs. The clustering algorithm used a relative
distance threshold of 3.5 that translates to an ac-
tual treshold of 120 for an average inter-cluster
distance of 174. The number of produced clusters
was in the order of the 25 clusters or so. Finally, a
threshold in relevancy of 0.1 was used in the con-
straint learning procedure. Given the amount of
data available for these experiments all these pa-
rameters were hand-tunned.
</bodyText>
<subsectionHeader confidence="0.992311">
4.2 Qualitative evaluation
</subsectionHeader>
<bodyText confidence="0.999425285714286">
The system was executed using all the available
information, with the same parametric settings
used in the quantitative evaluation, yielding a set
of 29 constraints, out of 23 generated clusters.
These constraints were analyzed by hand and
compared to the existing content-planner. We
found that most rules that were learned were val-
idated by our existing plan. Moreover, we gained
placement constraints for two pieces of semantic
information that are currently not represented in
the system’s plan. In addition, we found minor
order variation in relative placement of two differ-
ent pairs of semantic tags. This leads us to believe
that the fixed order on these particular tags can
be relaxed to attain greater degrees of variability
in the generated plans. The process of creation
of the existing content-planner was thorough, in-
formed by multiple domain experts over a three
year period. The fact that the obtained constraints
mostly occur in the existing plan is very encour-
aging.
</bodyText>
<sectionHeader confidence="0.999488" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999991333333333">
As explained in (Hudak and McClure, 1999), mo-
tif detection is usually targeted with alignment
techniques (as in (Durbin et al., 1998)) or with
combinatorial pattern discovery techniques such
as the ones we used here. Combinatorial pattern
discovery is more appropriate for our task because
it allows for matching across patterns with permu-
tations, for representation of wild cards and for
use on smaller data sets.
Similar techniques are used in NLP. Align-
ments are widely used in MT, for example
(Melamed, 1997), but the crossing problem is a
phenomenon that occurs repeatedly and at many
levels in our task and thus, this is not a suitable
approach for us.
Pattern discovery techniques are often used for
information extraction (e.g., (Riloff, 1993; Fisher
et al., 1995)), but most work uses data that con-
tains patterns labelled with the semantic slot the
pattern fills. Given the difficulty for humans in
finding patterns systematically in our data, we
needed unsupervised techniques such as those de-
veloped in computational genomics.
Other stochastic approaches to NLG normally
focus on the problem of sentence generation,
including syntactic and lexical realization (e.g.,
(Langkilde and Knight, 1998; Bangalore and
Rambow, 2000; Knight and Hatzivassiloglou,
1995)). Concurrent work analyzing constraints on
ordering of sentences in summarization found that
a coherence constraint that ensures that blocks of
sentences on the same topic tend to occur together
(Barzilay et al., 2001). This results in a bottom-
up approach for ordering that opportunistically
groups sentences together based on content fea-
tures. In contrast, our work attempts to automati-
cally learn plans for generation based on semantic
types of the input clause, resulting in a top-down
planner for selecting and ordering content.
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="method">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999538576923077">
In this paper we presented a technique for extract-
ing order constraints among plan elements that
performs satisfactorily without the need of large
corpora. Using a conservative set of parameters,
we were able to reconstruct a good portion of a
carefully hand-crafted planner. Moreover, as dis-
cussed in the evaluation, there are several pieces
of information in the transcripts which are not
present in the current system. From our learned
results, we have inferred placement constraints of
the new information in relation to the previous
plan elements without further interviews with ex-
perts.
Furthermore, it seems we have captured order-
sensitive information in the patterns and free-
order information is kept in the don’t care model.
The patterns, and ordering constraints among
them, provide a backbone of relatively fixed struc-
ture, while don’t cares are interspersed among
them. This model, being probabilistic in nature,
means a great deal of variation, but our gener-
ated plans should have variability in the right po-
sitions. This is similar to findings of~oating posi-
tioning of information, together with oportunistic
rendering of the data as used in STREAK (Robin
and McKeown, 1996).
</bodyText>
<subsectionHeader confidence="0.708655">
6.1 Future work
</subsectionHeader>
<bodyText confidence="0.999953875">
We are planning to use these techniques to revise
our current content-planner and incorporate infor-
mation that is learned from the transcripts to in-
crease the possible variation in system output.
The final step in producing a full-fledged
content-planner is to add semantic constraints on
the selection of possible orderings. This can be
generated through clustering of semantic input to
the generator.
We also are interested in further evaluating the
technique in an unrestricted domain such as the
Wall Street Journal (WSJ) with shallow seman-
tics such as the WordNet top-category for each
NP-head. This kind of experiment may show
strengths and limitations of the algorithm in large
corpora.
</bodyText>
<sectionHeader confidence="0.999092" genericHeader="method">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.993730333333333">
This research is supported in part by NLM Con-
tract R01 LM06593-01 and the Columbia Uni-
versity Center for Advanced Technology in In-
formation Management (funded by the New York
State Science and Technology Foundation). The
authors would like to thank Regina Barzilay,
</bodyText>
<table confidence="0.98854075">
intraop-problems intraop-problems I operation 11.11% } drip
drip 33.33%
intraop-problems 33.33%
total-meds-anesthetics 22.22%
intraop-problems � operation 14.29% } drip drip
� drip 14.29%
� intraop-problems 42.86%
total-meds-anesthetics 28.58%
intraop-problems intraop-problems I operation 20.00% } drip drip
drip 20.00%
intraop-problems 20.00%
total-meds-anesthetics 40.00%
</table>
<figureCaption confidence="0.95622625">
Figure 6: Cluster and patterns example. Each line corresponds to a different pattern. The elements
between braces are don’t care positions (three patterns conform this cluster: intraop-problems intraop-problems ? drip,
intraop-problems ? drip drip and intraop-problems intraop-problems drip drip the don’t care model shown in each brace must sum up to
1 but there is a strong overlap between patterns —the main reason for clustering)
</figureCaption>
<bodyText confidence="0.99911425">
Noemie Elhadad and Smaranda Muresan for help-
ful suggestions and comments. The aid of two
anonymous reviewers was also highly appreci-
ated.
</bodyText>
<sectionHeader confidence="0.997986" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.987469241758242">
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In COLING, 2000, Saarbrcken, Germany.
Regina Barzilay, Noemie Elhadad, and Kathleen R.
McKeown. 2001. Sentence ordering in multidoc-
ument summarization. In HLT, 2001, San Diego,
CA.
Nadjet Bouayad-Aga, Richard Power, and Donia
Scott. 2000. Can text structure be incompatible
with rhetorical structure? In Proceedings of the
1st International Conference on Natural Language
Generation (INLG-2000), pages 194–200, Mitzpe
Ramon, Israel.
Andrea Califano. 1999. Splash: Structural pattern lo-
calization analysis by sequential histograms. Bioin-
formatics, 12, February.
Mukesh Dalal, Steven Feiner, , Kathleen McKeown,
ShiMei Pan, Michelle Zhou, Tobias Hollerer, James
Shaw, Yong Feng, and Jeanne Fromer. 1996. Nego-
tiation for automated generation of temporal multi-
media presentations. In Proceedings ofACM Mul-
timedia ’96, Philadelphia.
Robert Dale. 1988. Generating referring expressions
in a domain of objects and processes. Ph.D. thesis,
University of Edinburgh.
Richard Durbin, S. Eddy, A. Krogh, and G. Mitchi-
son. 1998. Biological sequence analysis. Cam-
bridge Univeristy Press.
David Fisher, Stephen Soderland, Joseph McCarthy,
Fangfang Feng, and Wendy Lehnert. 1995. De-
scription of the umass system as used for muc-
6. In Morgan Kaufman, editor, Proceedings of the
Sixth Message Understanding Conference (MUC-
6), pages 127–140, San Francisco.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Arti�cial
Intelligence. (Special Issue on Natural Language
Processing).
J. Hudak and Marcela McClure. 1999. A comparative
analysis of computational motif–detection methods.
In R.B. Altman, A. K. Dunker, L. Hunter, T. E.
Klein, and K. Lauderdale, editors, Paci�c Sympo-
sium on Biocomputing, ’99, pages 138–149, New
Jersey. World Scientific.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics (ACL’95).
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Proceedings
of the Ninth International Natural Language Gen-
eration Workshop (INLG’98).
Kathleen McKeown, ShiMei Pan, James Shaw, Jordan
Desmand, and Barry Allen. 1997. Language gen-
eration for multimedia healthcare briefings. In Pro-
ceedings of the 5th Conference on Applied Natural
Language Processing (ANLP’97), Washington, DC,
April.
Kathleen R. McKeown, Desmond Jordan, Steven
Feiner, James Shaw, Elizabeth Chen, Shabina Ah-
mad, Andre Kushniruk, and Vimla Patel. 2000. A
study of communication in the cardiac surgery in-
tensive care unit and its implications for automated
briefing. In AMIA ’2000.
Kathleen R. McKeown. 1985. Text Generation: Us-
ing Discourse Strategies and Focus Constraints to
Generate Natural Language Text. Cambridge Uni-
versity Press.
I. Dan Melamed. 1997. A portable algorithm for
mapping bitext correspondence. In 35th Confer-
ence of the Association for Computational Linguis-
tics (ACL’97), Madrid, Spain.
Johanna D. Moore and C´ecile L. Paris. 1993. Plan-
ning text for advisory dialogues: Capturing inten-
tional and rhetorical information. Computational
Linguistics, 19(4):651–695.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Isidore Rigoutsos and Aris Floratos. 1998. Combina-
torial pattern discovery in biological sequences: the
teiresias algorithm. Bioinformatics, 14(1):55–67.
Ellen Riloff. 1993. Automatically constructing a dic-
tionary for information extraction. In AAAI Press
/ MIT Press, editor, Proceedings of the Eleventh Na-
tional Conference on Artificial Intelligence, pages
811–816.
Jacques Robin and Kathleen McKeown. 1996. Em-
pirically designing and evaluating a new revision–
based model for summary generation. Artificial In-
telligence, 85(1–2):135–179.
</reference>
<sectionHeader confidence="0.6298795" genericHeader="method">
Appendix - Definition of the distance mea-
sure used for clustering.
</sectionHeader>
<bodyText confidence="0.984937076923077">
An approximate matching measure is de-
fined for a given extended pattern. The ex-
tended pattern is represented as a sequence of
sets; defined positions have a singleton set,
while wildcard positions contain the non-zero
probability elements in their don’t care model
(e.g. given intraop-problems, intraop-problems, {drip 10%,intubation
90%j, drip we model this as [{intraop-problems}; {intraop-
problems}; {drip, intubation}; {drip}}]).
Consider p to be such a pattern, o an offset and
S a sequence, the approximate matching is de-
fined by
E. 0 h(p) match(p[i], S[i + o])
</bodyText>
<equation confidence="0.547405">
m (p, o, S) = length(p)
</equation>
<bodyText confidence="0.999916571428572">
where the match(P, e) function is defined as 0 if
e E P, 1 otherwise, and where P is the set at
position i in the extended pattern p and e is the
element of the sequence S at position i + o.
Our measure is normalized to [0, 1]. Using
this function, we define the approximate match-
ing distance measure (one way) between a pattern
p1 and a pattern p2 as the sum (averaged over the
length of the offset list of p1) of all the approxi-
mate matching measures of p2 over the offset list
of p1. This is, again, a real number in [0, 1]. To
ensure symmetry, we define the distance between
p1 and p2 as the average between the one way dis-
tance between p1 and p2 and between p2 and p1.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985568">
<title confidence="0.999585">Empirically Estimating Order Constraints for Content Planning in Generation</title>
<author confidence="0.99981">A Duboue R McKeown</author>
<affiliation confidence="0.999971">Computer Science Department Columbia University</affiliation>
<address confidence="0.999995">10027, New York, NY, USA</address>
<abstract confidence="0.999431625">In a language generation system, a content planner embodies one or more “plans” that are usually hand–crafted, sometimes through manual analysis of target text. In this paper, we present a system that we developed to automatically learn elements of a plan and the ordering constraints among them. As training data, we use semantically annotated transcripts of domain experts performing the task our system is designed to mimic. Given the large degree of variation in the spoken language of the transcripts, we developed a novel algorithm to find parallels between transcripts based on techniques used in computational genomics. Our proposed methodology was evaluated two–fold: the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89%. A qualitative evaluation is also provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In COLING,</booktitle>
<location>Saarbrcken, Germany.</location>
<contexts>
<context position="19130" citStr="Bangalore and Rambow, 2000" startWordPosition="3150" endWordPosition="3153">k and thus, this is not a suitable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g., (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Knight and Hatzivassiloglou, 1995)). Concurrent work analyzing constraints on ordering of sentences in summarization found that a coherence constraint that ensures that blocks of sentences on the same topic tend to occur together (Barzilay et al., 2001). This results in a bottomup approach for ordering that opportunistically groups sentences together based on content features. In contrast, our work attempts to automatically learn plans for generation based on semantic types of the input clause, resulting in a top-down planner for selecting and ordering content. 6 Conclusions In this paper we</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In COLING, 2000, Saarbrcken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence ordering in multidocument summarization.</title>
<date>2001</date>
<booktitle>In HLT,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="19385" citStr="Barzilay et al., 2001" startWordPosition="3187" endWordPosition="3190">fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g., (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Knight and Hatzivassiloglou, 1995)). Concurrent work analyzing constraints on ordering of sentences in summarization found that a coherence constraint that ensures that blocks of sentences on the same topic tend to occur together (Barzilay et al., 2001). This results in a bottomup approach for ordering that opportunistically groups sentences together based on content features. In contrast, our work attempts to automatically learn plans for generation based on semantic types of the input clause, resulting in a top-down planner for selecting and ordering content. 6 Conclusions In this paper we presented a technique for extracting order constraints among plan elements that performs satisfactorily without the need of large corpora. Using a conservative set of parameters, we were able to reconstruct a good portion of a carefully hand-crafted plan</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2001</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2001. Sentence ordering in multidocument summarization. In HLT, 2001, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadjet Bouayad-Aga</author>
<author>Richard Power</author>
<author>Donia Scott</author>
</authors>
<title>Can text structure be incompatible with rhetorical structure?</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Conference on Natural Language Generation (INLG-2000),</booktitle>
<pages>194--200</pages>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="1458" citStr="Bouayad-Aga et al., 2000" startWordPosition="221" endWordPosition="224">mputational genomics. Our proposed methodology was evaluated two–fold: the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89%. A qualitative evaluation is also provided. 1 Introduction In a language generation system, a content planner typically uses one or more “plans” to represent the content to be included in the output and the ordering between content elements. Some researchers rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g., (Bouayad-Aga et al., 2000; Moore and Paris, 1993; Hovy, 1993)) or schemas (e.g., (McKeown, 1985; McKeown et al., 1997)). In all cases, constraints on application of rules (e.g., plan operators), which determine content and order, are usually hand-crafted, sometimes through manual analysis of target text. In this paper, we present a method for learning the basic patterns contained within a plan and the ordering among them. As training data, we use semantically tagged transcripts of domain experts performing the task our system is designed to mimic, an oral briefing of patient status after undergoing coronary bypass sur</context>
</contexts>
<marker>Bouayad-Aga, Power, Scott, 2000</marker>
<rawString>Nadjet Bouayad-Aga, Richard Power, and Donia Scott. 2000. Can text structure be incompatible with rhetorical structure? In Proceedings of the 1st International Conference on Natural Language Generation (INLG-2000), pages 194–200, Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Califano</author>
</authors>
<title>Splash: Structural pattern localization analysis by sequential histograms.</title>
<date>1999</date>
<journal>Bioinformatics,</journal>
<volume>12</volume>
<contexts>
<context position="8927" citStr="Califano, 1999" startWordPosition="1431" endWordPosition="1432"> in large databases has attracted considerable interest in the last decade (see (Hudak and McClure, 1999) for a recent survey). Combinatorial pattern discovery, one technique developed for this problem, promised to be a good fit for our task because it can be parameterized to operate successfully without large amounts of data and it will be able to identify domain swapped motifs: for example, given a–b–c in one sequence and c–b–a in another. This difference is central to our current research, given that order constraints are our main focus. TEIRESIAS (Rigoutsos and Floratos, 1998) and SPLASH (Califano, 1999) are good representatives of this kind of algorithm. We used an adaptation of TEIRESIAS. The algorithm can be sketched as follows: we apply combinatorial pattern discovery (see Section 3.1) to the semantic sequences. The obtained patterns are refined through clustering (Section 3.2). Counting procedures are then used to estimate order constraints between those clusters (Section 3.3). 3.1 Pattern detection In this section, we provide a brief explanation of our pattern discovery methodology. The explanation builds on the definitions below: (L, W) pattern. Given that E represents the semantic tag</context>
</contexts>
<marker>Califano, 1999</marker>
<rawString>Andrea Califano. 1999. Splash: Structural pattern localization analysis by sequential histograms. Bioinformatics, 12, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>ShiMei Pan</author>
<author>Michelle Zhou</author>
<author>Tobias Hollerer</author>
<author>James Shaw</author>
<author>Yong Feng</author>
<author>Jeanne Fromer</author>
</authors>
<title>Negotiation for automated generation of temporal multimedia presentations.</title>
<date>1996</date>
<booktitle>In Proceedings ofACM Multimedia ’96,</booktitle>
<location>Philadelphia.</location>
<marker>McKeown, Pan, Zhou, Hollerer, Shaw, Feng, Fromer, 1996</marker>
<rawString>Mukesh Dalal, Steven Feiner, , Kathleen McKeown, ShiMei Pan, Michelle Zhou, Tobias Hollerer, James Shaw, Yong Feng, and Jeanne Fromer. 1996. Negotiation for automated generation of temporal multimedia presentations. In Proceedings ofACM Multimedia ’96, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating referring expressions in a domain of objects and processes.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1343" citStr="Dale, 1988" startWordPosition="205" endWordPosition="206">s, we developed a novel algorithm to find parallels between transcripts based on techniques used in computational genomics. Our proposed methodology was evaluated two–fold: the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89%. A qualitative evaluation is also provided. 1 Introduction In a language generation system, a content planner typically uses one or more “plans” to represent the content to be included in the output and the ordering between content elements. Some researchers rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g., (Bouayad-Aga et al., 2000; Moore and Paris, 1993; Hovy, 1993)) or schemas (e.g., (McKeown, 1985; McKeown et al., 1997)). In all cases, constraints on application of rules (e.g., plan operators), which determine content and order, are usually hand-crafted, sometimes through manual analysis of target text. In this paper, we present a method for learning the basic patterns contained within a plan and the ordering among them. As training data, we use semantically tagged transcripts of domain experts performin</context>
</contexts>
<marker>Dale, 1988</marker>
<rawString>Robert Dale. 1988. Generating referring expressions in a domain of objects and processes. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Durbin</author>
<author>S Eddy</author>
<author>A Krogh</author>
<author>G Mitchison</author>
</authors>
<title>Biological sequence analysis.</title>
<date>1998</date>
<publisher>Cambridge Univeristy Press.</publisher>
<contexts>
<context position="2527" citStr="Durbin et al., 1998" startWordPosition="393" endWordPosition="396">nscripts of domain experts performing the task our system is designed to mimic, an oral briefing of patient status after undergoing coronary bypass surgery. Given that our target output is spoken language, there is some level of variability between individual transcripts. It is difficult for a human to see patterns in the data and thus supervised learning based on hand-tagged training sets can not be applied. We need a learning algorithm that can discover ordering patterns in apparently unordered input. We based our unsupervised learning algorithm on techniques used in computational genomics (Durbin et al., 1998), where from large amounts of seemingly unorganized genetic sequences, patterns representing meaningful biological features are discovered. In our application, a transcript is the equivalent of a sequence and we are searching for patterns that occur repeatedly across multiple sequences. We can think of these patterns as the basic elements of a plan, representing small clusters of semantic units that are similar in size, for example, to the nucleus-satellite pairs of RST.1 By learning ordering constraints over these ele&apos;Note, however, that we do not learn or represent intention. age, gender, pm</context>
<context position="18030" citStr="Durbin et al., 1998" startWordPosition="2978" endWordPosition="2981"> we found minor order variation in relative placement of two different pairs of semantic tags. This leads us to believe that the fixed order on these particular tags can be relaxed to attain greater degrees of variability in the generated plans. The process of creation of the existing content-planner was thorough, informed by multiple domain experts over a three year period. The fact that the obtained constraints mostly occur in the existing plan is very encouraging. 5 Related work As explained in (Hudak and McClure, 1999), motif detection is usually targeted with alignment techniques (as in (Durbin et al., 1998)) or with combinatorial pattern discovery techniques such as the ones we used here. Combinatorial pattern discovery is more appropriate for our task because it allows for matching across patterns with permutations, for representation of wild cards and for use on smaller data sets. Similar techniques are used in NLP. Alignments are widely used in MT, for example (Melamed, 1997), but the crossing problem is a phenomenon that occurs repeatedly and at many levels in our task and thus, this is not a suitable approach for us. Pattern discovery techniques are often used for information extraction (e.</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>Richard Durbin, S. Eddy, A. Krogh, and G. Mitchison. 1998. Biological sequence analysis. Cambridge Univeristy Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Fisher</author>
<author>Stephen Soderland</author>
<author>Joseph McCarthy</author>
<author>Fangfang Feng</author>
<author>Wendy Lehnert</author>
</authors>
<title>Description of the umass system as used for muc6.</title>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC6),</booktitle>
<pages>127--140</pages>
<editor>In Morgan Kaufman, editor,</editor>
<location>San Francisco.</location>
<contexts>
<context position="18669" citStr="Fisher et al., 1995" startWordPosition="3082" endWordPosition="3085">rial pattern discovery techniques such as the ones we used here. Combinatorial pattern discovery is more appropriate for our task because it allows for matching across patterns with permutations, for representation of wild cards and for use on smaller data sets. Similar techniques are used in NLP. Alignments are widely used in MT, for example (Melamed, 1997), but the crossing problem is a phenomenon that occurs repeatedly and at many levels in our task and thus, this is not a suitable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g., (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Knight and Hatzivassiloglou, 1995)). Concurrent work analyzing constraints on ordering of sentences in summarization found that a coheren</context>
</contexts>
<marker>Fisher, Soderland, McCarthy, Feng, Lehnert, 1995</marker>
<rawString>David Fisher, Stephen Soderland, Joseph McCarthy, Fangfang Feng, and Wendy Lehnert. 1995. Description of the umass system as used for muc6. In Morgan Kaufman, editor, Proceedings of the Sixth Message Understanding Conference (MUC6), pages 127–140, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Automated discourse generation using discourse structure relations.</title>
<date>1993</date>
<journal>Arti�cial Intelligence. (Special Issue on Natural Language Processing).</journal>
<contexts>
<context position="1494" citStr="Hovy, 1993" startWordPosition="229" endWordPosition="230">s evaluated two–fold: the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89%. A qualitative evaluation is also provided. 1 Introduction In a language generation system, a content planner typically uses one or more “plans” to represent the content to be included in the output and the ordering between content elements. Some researchers rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g., (Bouayad-Aga et al., 2000; Moore and Paris, 1993; Hovy, 1993)) or schemas (e.g., (McKeown, 1985; McKeown et al., 1997)). In all cases, constraints on application of rules (e.g., plan operators), which determine content and order, are usually hand-crafted, sometimes through manual analysis of target text. In this paper, we present a method for learning the basic patterns contained within a plan and the ordering among them. As training data, we use semantically tagged transcripts of domain experts performing the task our system is designed to mimic, an oral briefing of patient status after undergoing coronary bypass surgery. Given that our target output i</context>
</contexts>
<marker>Hovy, 1993</marker>
<rawString>Eduard H. Hovy. 1993. Automated discourse generation using discourse structure relations. Arti�cial Intelligence. (Special Issue on Natural Language Processing).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hudak</author>
<author>Marcela McClure</author>
</authors>
<title>A comparative analysis of computational motif–detection methods.</title>
<date>1999</date>
<booktitle>Paci�c Symposium on Biocomputing, ’99,</booktitle>
<pages>138--149</pages>
<editor>In R.B. Altman, A. K. Dunker, L. Hunter, T. E. Klein, and K. Lauderdale, editors,</editor>
<publisher>World Scientific.</publisher>
<location>New Jersey.</location>
<contexts>
<context position="8417" citStr="Hudak and McClure, 1999" startWordPosition="1344" endWordPosition="1347">e amps with hypokinetic basal and anterior apical region. echo-preop Hematocrit 1.2 hct-preop 2 vessel CABG off pump both mammaries ...... procedure , otherwise his labs are unremarkable. Went to OR for what was felt to be Figure 1: An annotated transcription of an ICU briefing (after anonymising). terms, a motif is a small subsequence, highly conserved through evolution. From the computer science standpoint, a motif is a fixed-order pattern, simply because it is a subsequence. The problem of detecting such motifs in large databases has attracted considerable interest in the last decade (see (Hudak and McClure, 1999) for a recent survey). Combinatorial pattern discovery, one technique developed for this problem, promised to be a good fit for our task because it can be parameterized to operate successfully without large amounts of data and it will be able to identify domain swapped motifs: for example, given a–b–c in one sequence and c–b–a in another. This difference is central to our current research, given that order constraints are our main focus. TEIRESIAS (Rigoutsos and Floratos, 1998) and SPLASH (Califano, 1999) are good representatives of this kind of algorithm. We used an adaptation of TEIRESIAS. T</context>
<context position="17938" citStr="Hudak and McClure, 1999" startWordPosition="2963" endWordPosition="2966">es of semantic information that are currently not represented in the system’s plan. In addition, we found minor order variation in relative placement of two different pairs of semantic tags. This leads us to believe that the fixed order on these particular tags can be relaxed to attain greater degrees of variability in the generated plans. The process of creation of the existing content-planner was thorough, informed by multiple domain experts over a three year period. The fact that the obtained constraints mostly occur in the existing plan is very encouraging. 5 Related work As explained in (Hudak and McClure, 1999), motif detection is usually targeted with alignment techniques (as in (Durbin et al., 1998)) or with combinatorial pattern discovery techniques such as the ones we used here. Combinatorial pattern discovery is more appropriate for our task because it allows for matching across patterns with permutations, for representation of wild cards and for use on smaller data sets. Similar techniques are used in NLP. Alignments are widely used in MT, for example (Melamed, 1997), but the crossing problem is a phenomenon that occurs repeatedly and at many levels in our task and thus, this is not a suitable</context>
</contexts>
<marker>Hudak, McClure, 1999</marker>
<rawString>J. Hudak and Marcela McClure. 1999. A comparative analysis of computational motif–detection methods. In R.B. Altman, A. K. Dunker, L. Hunter, T. E. Klein, and K. Lauderdale, editors, Paci�c Symposium on Biocomputing, ’99, pages 138–149, New Jersey. World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Two-level, many-paths generation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL’95).</booktitle>
<contexts>
<context position="19166" citStr="Knight and Hatzivassiloglou, 1995" startWordPosition="3154" endWordPosition="3157">itable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g., (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Knight and Hatzivassiloglou, 1995)). Concurrent work analyzing constraints on ordering of sentences in summarization found that a coherence constraint that ensures that blocks of sentences on the same topic tend to occur together (Barzilay et al., 2001). This results in a bottomup approach for ordering that opportunistically groups sentences together based on content features. In contrast, our work attempts to automatically learn plans for generation based on semantic types of the input clause, resulting in a top-down planner for selecting and ordering content. 6 Conclusions In this paper we presented a technique for extractin</context>
</contexts>
<marker>Knight, Hatzivassiloglou, 1995</marker>
<rawString>Kevin Knight and Vasileios Hatzivassiloglou. 1995. Two-level, many-paths generation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL’95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>The practical value of n-grams in generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the Ninth International Natural Language Generation Workshop (INLG’98).</booktitle>
<contexts>
<context position="19102" citStr="Langkilde and Knight, 1998" startWordPosition="3146" endWordPosition="3149">nd at many levels in our task and thus, this is not a suitable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g., (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Knight and Hatzivassiloglou, 1995)). Concurrent work analyzing constraints on ordering of sentences in summarization found that a coherence constraint that ensures that blocks of sentences on the same topic tend to occur together (Barzilay et al., 2001). This results in a bottomup approach for ordering that opportunistically groups sentences together based on content features. In contrast, our work attempts to automatically learn plans for generation based on semantic types of the input clause, resulting in a top-down planner for selecting and ordering content. 6 </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. The practical value of n-grams in generation. In Proceedings of the Ninth International Natural Language Generation Workshop (INLG’98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>ShiMei Pan</author>
<author>James Shaw</author>
<author>Jordan Desmand</author>
<author>Barry Allen</author>
</authors>
<title>Language generation for multimedia healthcare briefings.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP’97),</booktitle>
<location>Washington, DC,</location>
<contexts>
<context position="1551" citStr="McKeown et al., 1997" startWordPosition="237" endWordPosition="240">zation capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89%. A qualitative evaluation is also provided. 1 Introduction In a language generation system, a content planner typically uses one or more “plans” to represent the content to be included in the output and the ordering between content elements. Some researchers rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g., (Bouayad-Aga et al., 2000; Moore and Paris, 1993; Hovy, 1993)) or schemas (e.g., (McKeown, 1985; McKeown et al., 1997)). In all cases, constraints on application of rules (e.g., plan operators), which determine content and order, are usually hand-crafted, sometimes through manual analysis of target text. In this paper, we present a method for learning the basic patterns contained within a plan and the ordering among them. As training data, we use semantically tagged transcripts of domain experts performing the task our system is designed to mimic, an oral briefing of patient status after undergoing coronary bypass surgery. Given that our target output is spoken language, there is some level of variability bet</context>
</contexts>
<marker>McKeown, Pan, Shaw, Desmand, Allen, 1997</marker>
<rawString>Kathleen McKeown, ShiMei Pan, James Shaw, Jordan Desmand, and Barry Allen. 1997. Language generation for multimedia healthcare briefings. In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP’97), Washington, DC, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Desmond Jordan</author>
<author>Steven Feiner</author>
<author>James Shaw</author>
<author>Elizabeth Chen</author>
<author>Shabina Ahmad</author>
<author>Andre Kushniruk</author>
<author>Vimla Patel</author>
</authors>
<title>A study of communication in the cardiac surgery intensive care unit and its implications for automated briefing.</title>
<date>2000</date>
<booktitle>In AMIA</booktitle>
<contexts>
<context position="4108" citStr="McKeown et al., 2000" startWordPosition="634" endWordPosition="637"> constraints between them. Our system uses combinatorial pattern matching (Rigoutsos and Floratos, 1998) combined with clustering to learn plan elements. Subsequently, it applies counting procedures to learn ordering constraints among these elements. Our system produced a set of 24 schemata units, that we call “plan elements”2, and 29 ordering constraints between these basic plan elements, which we compared to the elements contained in the orginal hand-crafted plan that was constructed based on hand-analysis of transcripts, input from domain experts, and experimental evaluation of the system (McKeown et al., 2000). The remainder of this article is organized as follows: first the data used in our experiments is presented and its overall structure and acquisition methodology are analyzed. In Section 3 our techniques are described, together with their grounding in computational genomics. The quantitative and qualitative evaluation are discussed in Section 4. Related work is presented in Section 5. Conclusions and future work are discussed in Section 6. 2 Our data Our research is part of MAGIC (Dalal et al., 1996; McKeown et al., 2000), a system that is designed to produce a briefing of patient status afte</context>
<context position="5735" citStr="McKeown et al., 2000" startWordPosition="903" endWordPosition="907"> in (Reiter and Dale, 2000). equipped with a wearable tape recorder to tape the briefings, which were transcribed to provide the base of our empirical data. The text was subsequently annotated with semantic tags as shown in Figure 1. The figure shows that each sentence is split into several semantically tagged chunks. The tag-set was developed with the assistance of a domain expert in order to capture the different information types that are important for communication and the tagging process was done by two non-experts, after measuring acceptable agreement levels with the domain expert (see (McKeown et al., 2000)). The tag-set totalled over 200 tags. These 200 tags were then mapped to 29 categories, which was also done by a domain expert. These categories are the ones used for our current research. From these transcripts, we derive the sequences of semantic tags for each transcript. These sequences constitute the input and working material of our analysis, they are an average length of 33 tags per transcript (min = 13, max = 66, Q = 11.6). A tag-set distribution analysis showed that some of the categories dominate the tag counts. Furthermore, some tags occur fairly regularly towards either the beginni</context>
</contexts>
<marker>McKeown, Jordan, Feiner, Shaw, Chen, Ahmad, Kushniruk, Patel, 2000</marker>
<rawString>Kathleen R. McKeown, Desmond Jordan, Steven Feiner, James Shaw, Elizabeth Chen, Shabina Ahmad, Andre Kushniruk, and Vimla Patel. 2000. A study of communication in the cardiac surgery intensive care unit and its implications for automated briefing. In AMIA ’2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1528" citStr="McKeown, 1985" startWordPosition="234" endWordPosition="236">ng and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89%. A qualitative evaluation is also provided. 1 Introduction In a language generation system, a content planner typically uses one or more “plans” to represent the content to be included in the output and the ordering between content elements. Some researchers rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g., (Bouayad-Aga et al., 2000; Moore and Paris, 1993; Hovy, 1993)) or schemas (e.g., (McKeown, 1985; McKeown et al., 1997)). In all cases, constraints on application of rules (e.g., plan operators), which determine content and order, are usually hand-crafted, sometimes through manual analysis of target text. In this paper, we present a method for learning the basic patterns contained within a plan and the ordering among them. As training data, we use semantically tagged transcripts of domain experts performing the task our system is designed to mimic, an oral briefing of patient status after undergoing coronary bypass surgery. Given that our target output is spoken language, there is some l</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>Kathleen R. McKeown. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>A portable algorithm for mapping bitext correspondence.</title>
<date>1997</date>
<booktitle>In 35th Conference of the Association for Computational Linguistics (ACL’97),</booktitle>
<location>Madrid,</location>
<contexts>
<context position="18409" citStr="Melamed, 1997" startWordPosition="3041" endWordPosition="3042">t that the obtained constraints mostly occur in the existing plan is very encouraging. 5 Related work As explained in (Hudak and McClure, 1999), motif detection is usually targeted with alignment techniques (as in (Durbin et al., 1998)) or with combinatorial pattern discovery techniques such as the ones we used here. Combinatorial pattern discovery is more appropriate for our task because it allows for matching across patterns with permutations, for representation of wild cards and for use on smaller data sets. Similar techniques are used in NLP. Alignments are widely used in MT, for example (Melamed, 1997), but the crossing problem is a phenomenon that occurs repeatedly and at many levels in our task and thus, this is not a suitable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of senten</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. A portable algorithm for mapping bitext correspondence. In 35th Conference of the Association for Computational Linguistics (ACL’97), Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
<author>C´ecile L Paris</author>
</authors>
<title>Planning text for advisory dialogues: Capturing intentional and rhetorical information.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="1481" citStr="Moore and Paris, 1993" startWordPosition="225" endWordPosition="228">proposed methodology was evaluated two–fold: the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89%. A qualitative evaluation is also provided. 1 Introduction In a language generation system, a content planner typically uses one or more “plans” to represent the content to be included in the output and the ordering between content elements. Some researchers rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g., (Bouayad-Aga et al., 2000; Moore and Paris, 1993; Hovy, 1993)) or schemas (e.g., (McKeown, 1985; McKeown et al., 1997)). In all cases, constraints on application of rules (e.g., plan operators), which determine content and order, are usually hand-crafted, sometimes through manual analysis of target text. In this paper, we present a method for learning the basic patterns contained within a plan and the ordering among them. As training data, we use semantically tagged transcripts of domain experts performing the task our system is designed to mimic, an oral briefing of patient status after undergoing coronary bypass surgery. Given that our ta</context>
</contexts>
<marker>Moore, Paris, 1993</marker>
<rawString>Johanna D. Moore and C´ecile L. Paris. 1993. Planning text for advisory dialogues: Capturing intentional and rhetorical information. Computational Linguistics, 19(4):651–695.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5141" citStr="Reiter and Dale, 2000" startWordPosition="806" endWordPosition="809"> work are discussed in Section 6. 2 Our data Our research is part of MAGIC (Dalal et al., 1996; McKeown et al., 2000), a system that is designed to produce a briefing of patient status after undergoing a coronary bypass operation. Currently, when a patient is brought to the intensive care unit (ICU) after surgery, one of the residents who was present in the operating room gives a briefing to the ICU nurses and residents. Several of these briefings were collected and annotated for the aforementioned evaluation. The resident was 2These units can be loosely related to the concept ofinessages in (Reiter and Dale, 2000). equipped with a wearable tape recorder to tape the briefings, which were transcribed to provide the base of our empirical data. The text was subsequently annotated with semantic tags as shown in Figure 1. The figure shows that each sentence is split into several semantically tagged chunks. The tag-set was developed with the assistance of a domain expert in order to capture the different information types that are important for communication and the tagging process was done by two non-experts, after measuring acceptable agreement levels with the domain expert (see (McKeown et al., 2000)). The</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isidore Rigoutsos</author>
<author>Aris Floratos</author>
</authors>
<title>Combinatorial pattern discovery in biological sequences: the teiresias algorithm.</title>
<date>1998</date>
<journal>Bioinformatics,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="3591" citStr="Rigoutsos and Floratos, 1998" startWordPosition="556" endWordPosition="560">, to the nucleus-satellite pairs of RST.1 By learning ordering constraints over these ele&apos;Note, however, that we do not learn or represent intention. age, gender, pmh, pmh, pmh, pmh, med-preop, med-preop, med-preop, drip-preop, med-preop, ekg-preop, echo-preop, hct-preop, procedure, . . . Figure 2: The semantic sequence obtained from the transcript shown in Figure 1. ments, we produce a plan that can be expressed as a constraint-satisfaction problem. In this paper, we focus on learning the plan elements and the ordering constraints between them. Our system uses combinatorial pattern matching (Rigoutsos and Floratos, 1998) combined with clustering to learn plan elements. Subsequently, it applies counting procedures to learn ordering constraints among these elements. Our system produced a set of 24 schemata units, that we call “plan elements”2, and 29 ordering constraints between these basic plan elements, which we compared to the elements contained in the orginal hand-crafted plan that was constructed based on hand-analysis of transcripts, input from domain experts, and experimental evaluation of the system (McKeown et al., 2000). The remainder of this article is organized as follows: first the data used in our</context>
<context position="8899" citStr="Rigoutsos and Floratos, 1998" startWordPosition="1425" endWordPosition="1428">ence. The problem of detecting such motifs in large databases has attracted considerable interest in the last decade (see (Hudak and McClure, 1999) for a recent survey). Combinatorial pattern discovery, one technique developed for this problem, promised to be a good fit for our task because it can be parameterized to operate successfully without large amounts of data and it will be able to identify domain swapped motifs: for example, given a–b–c in one sequence and c–b–a in another. This difference is central to our current research, given that order constraints are our main focus. TEIRESIAS (Rigoutsos and Floratos, 1998) and SPLASH (Califano, 1999) are good representatives of this kind of algorithm. We used an adaptation of TEIRESIAS. The algorithm can be sketched as follows: we apply combinatorial pattern discovery (see Section 3.1) to the semantic sequences. The obtained patterns are refined through clustering (Section 3.2). Counting procedures are then used to estimate order constraints between those clusters (Section 3.3). 3.1 Pattern detection In this section, we provide a brief explanation of our pattern discovery methodology. The explanation builds on the definitions below: (L, W) pattern. Given that E</context>
</contexts>
<marker>Rigoutsos, Floratos, 1998</marker>
<rawString>Isidore Rigoutsos and Aris Floratos. 1998. Combinatorial pattern discovery in biological sequences: the teiresias algorithm. Bioinformatics, 14(1):55–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction.</title>
<date>1993</date>
<booktitle>In AAAI Press /</booktitle>
<pages>811--816</pages>
<editor>editor,</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="18647" citStr="Riloff, 1993" startWordPosition="3080" endWordPosition="3081">with combinatorial pattern discovery techniques such as the ones we used here. Combinatorial pattern discovery is more appropriate for our task because it allows for matching across patterns with permutations, for representation of wild cards and for use on smaller data sets. Similar techniques are used in NLP. Alignments are widely used in MT, for example (Melamed, 1997), but the crossing problem is a phenomenon that occurs repeatedly and at many levels in our task and thus, this is not a suitable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g., (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Knight and Hatzivassiloglou, 1995)). Concurrent work analyzing constraints on ordering of sentences in summarizatio</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction. In AAAI Press / MIT Press, editor, Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
<author>Kathleen McKeown</author>
</authors>
<title>Empirically designing and evaluating a new revision– based model for summary generation.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<pages>85--1</pages>
<contexts>
<context position="20902" citStr="Robin and McKeown, 1996" startWordPosition="3426" endWordPosition="3429">her interviews with experts. Furthermore, it seems we have captured ordersensitive information in the patterns and freeorder information is kept in the don’t care model. The patterns, and ordering constraints among them, provide a backbone of relatively fixed structure, while don’t cares are interspersed among them. This model, being probabilistic in nature, means a great deal of variation, but our generated plans should have variability in the right positions. This is similar to findings of~oating positioning of information, together with oportunistic rendering of the data as used in STREAK (Robin and McKeown, 1996). 6.1 Future work We are planning to use these techniques to revise our current content-planner and incorporate information that is learned from the transcripts to increase the possible variation in system output. The final step in producing a full-fledged content-planner is to add semantic constraints on the selection of possible orderings. This can be generated through clustering of semantic input to the generator. We also are interested in further evaluating the technique in an unrestricted domain such as the Wall Street Journal (WSJ) with shallow semantics such as the WordNet top-category </context>
</contexts>
<marker>Robin, McKeown, 1996</marker>
<rawString>Jacques Robin and Kathleen McKeown. 1996. Empirically designing and evaluating a new revision– based model for summary generation. Artificial Intelligence, 85(1–2):135–179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>