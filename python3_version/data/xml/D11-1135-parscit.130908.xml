<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040746">
<title confidence="0.856782">
Structured Relation Discovery using Generative Models
</title>
<author confidence="0.840221">
Limin Yao* Aria Haghighi+ Sebastian Riedel* Andrew McCallum*
</author>
<affiliation confidence="0.803639">
* Department of Computer Science, University of Massachusetts at Amherst
</affiliation>
<address confidence="0.473468">
+ CSAIL, Massachusetts Institute of Technology
</address>
<email confidence="0.9954135">
{lmyao,riedel,mccallum}@cs.umass.edu
{aria42}@csail.mit.edu
</email>
<sectionHeader confidence="0.996625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997545">
We explore unsupervised approaches to rela-
tion extraction between two named entities;
for instance, the semantic bornIn relation be-
tween a person and location entity. Con-
cretely, we propose a series of generative
probabilistic models, broadly similar to topic
models, each which generates a corpus of ob-
served triples of entity mention pairs and the
surface syntactic dependency path between
them. The output of each model is a cluster-
ing of observed relation tuples and their as-
sociated textual expressions to underlying se-
mantic relation types. Our proposed models
exploit entity type constraints within a relation
as well as features on the dependency path be-
tween entity mentions. We examine effective-
ness of our approach via multiple evaluations
and demonstrate 12% error reduction in preci-
sion over a state-of-the-art weakly supervised
baseline.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959155555556">
Many NLP applications would benefit from large
knowledge bases of relational information about
entities. For instance, knowing that the entity
Steve Balmer bears the leaderOf relation to the
entity Microsoft, would facilitate question answer-
ing (Ravichandran and Hovy, 2002), data mining,
and a host of other end-user applications. Due to
these many potential applications, relation extrac-
tion has gained much attention in information ex-
traction (Kambhatla, 2004; Culotta and Sorensen,
2004; Mintz et al., 2009; Riedel et al., 2010; Yao et
al., 2010). We propose a series of generative prob-
abilistic models, broadly similar to standard topic
models, which generate a corpus of observed triples
of entity mention pairs and the surface syntactic de-
pendency path between them. Our proposed mod-
els exploit entity type constraints within a relation
as well as features on the dependency path between
entity mentions. The output of our approach is a
clustering over observed relation paths (e.g. “X was
born in Y” and “X is from Y”) such that expressions
in the same cluster bear the same semantic relation
type between entities.
Past work has shown that standard supervised
techniques can yield high-performance relation de-
tection when abundant labeled data exists for a
fixed inventory of individual relation types (e.g.
leaderOf) (Kambhatla, 2004; Culotta and Sorensen,
2004; Roth and tau Yih, 2002). However, less ex-
plored are open-domain approaches where the set
of possible relation types are not fixed and little to
no labeled is given for each relation type (Banko et
al., 2007; Banko and Etzioni, 2008). A more re-
lated line of research has explored inducing rela-
tion types via clustering. For example, DIRT (Lin
and Pantel, 2001) aims to discover different repre-
sentations of the same semantic relation using dis-
tributional similarity of dependency paths. Poon
and Domingos (2008) present an Unsupervised se-
mantic parsing (USP) approach to partition depen-
dency trees into meaningful fragments (or “parts”
to use their terminology). The combinatorial nature
of this dependency partition model makes it difficult
for USP to scale to large data sets despite several
necessary approximations during learning and infer-
</bodyText>
<page confidence="0.933571">
1456
</page>
<note confidence="0.9579525">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456–1466,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.998184618181818">
ence. Our work is similar to DIRT and USP in that of relation tuples. Each tuple represents an ob-
we induce relation types from observed dependency served syntactic relationship between two Named
paths, but our approach is a straightforward and Entities (NE) and consists of three components: the
principled generative model which can be efficiently dependency path between two NE mentions, the
learned. As we show empirically, our approach out- source argument NE, and the destination argument
performs these related works when trained with the NE. A dependency path is a concatenation of depen-
same amount of data and further gains are observed dency relations (edges) and words (nodes) along a
when trained with more data. path in a dependency tree. For instance, the sentence
We evaluate our approach using ‘intrinsic’ clus- “John Lennnon was born in Liverpool” would yield
tering evaluation and ‘extrinsic’ evaluation settings.1 the relation tuple (Lennon, [f −nsubjpass, born, ↓
The former evaluation is performed using subset of −in], Liverpool). This relation tuple reflects a se-
induced clusters against Freebase relations, a large mantic bornIn relation between the John Lennon and
manually-built entity and relational database. We Liverpool entities. The dependency path in this ex-
also show some clusters which are not included as ample corresponds to the “X was born in Y” textual
Freebase relations, as well as some entity clusters expression given earlier. Note that for the above ex-
found by our approach. The latter evaluation uses ample, the bornIn relation can only occur between a
the clustering induced by our models as features for person and a location. The relation tuple is the pri-
relation extraction in distant supervision framework. mary observed random variable in our model and we
Empirical results show that we can find coherent construct our models (see Section 3) so that clusters
clusters. In relation extraction, we can achieve 12% consist of textual expressions representing the same
error reduction in precision over a state-of-the-art underlying relation type.
weakly supervised baseline and we show that using 3 Models
features from our proposed models can find more We propose three generative models for modeling
facts for a relation without significant accuracy loss. tuples of entity mention pairs and the syntactic de-
2 Problem and Experimental Setup pendency path between them (see Section 2). The
The task of relation extraction is mapping surface first two models, Rel-LDA and Rel-LDA1 are sim-
textual relations to underlying semantic relations. ple extensions of the standard LDA model (Blei et
For instance, the textual expression “X was born in al., 2003). At the document level, our model is iden-
Y” indicates a semantic relation bornIn between en- tical to standard LDA; a multinomial distribution
tities “X” and “Y”. This relation can be expressed is drawn over a fixed number of relation types R.
textually in several ways: for instance, “X, a native Changes lie in the observations. In standard LDA,
of Y” or “X grew up in Y”. There are several com- the atomic observation is a word drawn from a la-
ponents to a coherent relation type, including a tight tent topic distribution determined by a latent topic
small number of textual expressions as well as con- indicator variable for that word position. In our ap-
straints on the entities involved in the relation. For proach, a document consists of an exchangeable set
instance, in the bornIn relation “X” must be a person of relation tuples. Each relation tuple is drawn from
entity and “Y” a location (typically a city or nation). a relation type ‘topic’ distribution selected by a la-
In this work, we present an unsupervised probabilis- tent relation type indicator variable. Relation tuples
tic generative model for inducing clusters of relation are generated using a collection of independent fea-
types and recognizing their textual expressions. The tures drawn from the underlying relation type distri-
set of relation types is not pre-specified but induced bution. These changes to standard LDA are intended
from observed unlabeled data. See Table 4 for ex- to have the effect that instead of representing seman-
amples of learned semantic relations. tically related words, the ‘topic’ latent variable rep-
Our observed data consists of a corpus of docu- resents a relation type.
ments and each document is represented by a bag Our third model exploits entity type constraints
within a relation and induces clusters of relations
1See Section 4 for a fuller discussion of evaluation.
1457
and entities jointly. For each tuple, a set of rela-
tion level features and two latent entity type indica-
tors are drawn independently from the relation type
distribution; a collection of entity mention features
for each argument is drawn independently from the
entity type distribution selected by the entity type
indicator.
</bodyText>
<table confidence="0.999449">
Path X, made by Y
Source Gamma Knife
Dest Elekta
Trigger make
Lex , made by the Swedish
medical technology firm
POS , VBN IN DT JJ JJ NN NN
NER pair MISC-ORG
Sync pair partmod-pobj
</table>
<tableCaption confidence="0.989762">
Table 1: The features of tuple ‘(Gamma Knife, made
by, Elekta)’ in sentence “Gamma Knife, made by the
Swedish medical technology firm Elekta, focuses low-
dosage gamma radiation ...”
</tableCaption>
<subsectionHeader confidence="0.822487">
3.1 Rel-LDA Model
</subsectionHeader>
<bodyText confidence="0.999947588235294">
This model is an extension to the standard LDA
model. At the document level, a multinomial dis-
tribution over relations Bdoc is drawn from a prior
Dir(α). To generate a relation tuple, we first draw a
relation ‘topic’ r from Multi(B). Then we generate
each feature f of a tuple independently from a multi-
nomial distribution Multi(φrf) selected by r. In this
model, each tuple has three features, i.e. its three
components, shown in the first three rows in Table 1.
Figure 1 shows the graphical representation of Rel-
LDA. Table 2 lists all the notation used in describing
our models.
The learning process of the models is an EM pro-
cess. The procedure is similar to that used by the
standard topic model. In the variational E-step (in-
ference), we sample the relation type indicator for
each tuple using p(r|f):
</bodyText>
<equation confidence="0.993991333333333">
P(r|f(p, s, d)) ∝ p(r) 11f p(f|r)
77�7T Qf+nf|
∝ (αr + nr|d) llf Ef0(βf0+nf0|r)
</equation>
<table confidence="0.999167230769231">
|R |Number of relations
|D |Number of documents
r A relation
doc A document
p, s, d Dep path, source and dest args
f A feature/feature type
T Entity type of one argument
α Dirichlet prior for Bdoc
βx Dirichlet prior for φrx
β Dirichlet prior for φt
Bdoc p(r|doc)
φrx p(x|r)
φt p(fs|T), p(fd|T)
</table>
<tableCaption confidence="0.999282">
Table 2: The notation used in our models
</tableCaption>
<figureCaption confidence="0.992153">
Figure 1: Rel-LDA model. Shaded circles are observa-
tions, and unshaded ones are hidden variables. A docu-
ment consists of N tuples. Each tuple has a set of fea-
tures. Each feature of a tuple is generated independently
from a hidden relation variable r.
</figureCaption>
<equation confidence="0.993460571428572">
p(r) and p(f|r) are estimated in the M-step:
α + nr|doc
E
r0(α + nr0|doc)
βf + nf|r
φrf =
�f0(βf0 + nf0|r)
</equation>
<bodyText confidence="0.999591">
where nf|r indicates the number of times a feature f
is assigned with r.
</bodyText>
<subsectionHeader confidence="0.999696">
3.2 Rel-LDA1
</subsectionHeader>
<bodyText confidence="0.999895125">
Looking at results of Rel-LDA, we find the clus-
ters sometimes are in need of refinement, and we
can address this by adding more features. For in-
stance, adding trigger features can encourage spar-
sity over dependency paths. We define trigger words
as all the words on the dependency path except stop
words. For example, from path “X, based in Y”,
“base” is extracted as a trigger word. The intuition
</bodyText>
<equation confidence="0.984101272727273">
φrf
βf
|R|
r
θ
r
r
N
|D|
α
Bdoc =
</equation>
<page confidence="0.929993">
1458
</page>
<bodyText confidence="0.999972357142857">
for using trigger words is that paths sharing the same
set of trigger words should go to one cluster. Adding
named entity tag pair can refine the cluster too. For
example, a cluster found by Rel-LDA contains “X
was born in Y” and “X lives in Y”; but it also con-
tains “X, a company in Y”. In this scenario, adding
features ‘PER-LOC’ and ‘ORG-LOC’ can push the
model to split the clusters into two and put the third
case into a new cluster.
Hence we propose Rel-LDA1. It is similar to
Rel-LDA, except that each tuple is represented with
more features. Besides p, s, and d, we introduce
trigger words, lexical pattern, POS tag pattern, the
named entity pair and the syntactic category pair fea-
tures for each tuple. Lexical pattern is the word se-
quence between the two arguments of a tuple and
POS tag pattern is the POS tag sequence of the lexi-
cal pattern. See Table 1 as an example.
Following typical EM learning(Charniak and El-
sner, 2009), we start with a much simpler genera-
tive model, expose the model to fewer features first,
and iteratively add more features. First, we train a
Rel-LDA model, i.e. the model only generates the
dependency path, source and destination arguments.
After each interval of 10 iterations, we introduce one
additional feature. We add the features in the order
of trigger, lexical pattern, POS, NER pair, and syn-
tactic pair.
</bodyText>
<subsectionHeader confidence="0.933633">
3.3 Type-LDA model
</subsectionHeader>
<bodyText confidence="0.999947444444444">
We know that relations can only hold between
certain entity types, known as selectional prefer-
ences (Ritter et al., 2010; Seaghdha, 2010; Kozareva
and Hovy, 2010). Hence we propose Type-LDA
model. This model can capture the selectional pref-
erences of relations to their arguments. In the mean
time, it clusters tuples into relational clusters, and
arguments into different entity clusters. The entity
clusters could be interesting in many ways, for ex-
ample, defining fine-grained entity types and finding
new concepts.
We split the features of a tuple into relation level
features and entity level features. Relation level fea-
tures include the dependency path, trigger, lex and
POS features; entity level features include the entity
mention itself and its named entity tag.
The generative storyline is as follows. At the doc-
ument level, a multinomial distribution over rela-
</bodyText>
<figureCaption confidence="0.880915375">
Figure 2: Type-LDA model. Each document consists of
N tuples. Each tuple has a set of features, relation level
features f and entity level features of source argument fs
and destination argument fd. Relation level features and
two hidden entity types T1 and T2 are generated from
hidden relation variable r independently. Source entity
features are generated from T1 and destination features
are generated from T2.
</figureCaption>
<bodyText confidence="0.9999232">
tions Bdo, is drawn from a Dirichlet prior. A doc-
ument consists of N relation tuples. Each tuple is
represented by relation level features (f) and entity
level features of source argument (fs) and destina-
tion argument (fd). For each tuple, a relation r is
drawn from Multi(Bdo,). The relation level features
and two hidden entity types T1 and T2 are indepen-
dently generated from r. Features fs are generated
from T1 and fd from T2. Figure 2 shows the graphi-
cal representation of this model.
At inference time, we sample r, T1 and T2 for
each tuple. For efficient inference, we first initialize
the model without T1 and T2, i.e. all the features are
generated directly from r. Here the model degener-
ates to Rel-LDA1. After some iterations, we intro-
duce T1 and T2. We sample the relation variable (r)
and two mention types variables (T1,T2) iteratively
for each tuple. We can sample them together, but
this is not very efficient. In addition, we found that
it does not improve performance.
</bodyText>
<sectionHeader confidence="0.999377" genericHeader="introduction">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999595333333333">
Our experiments are carried out on New York Times
articles from year 2000 to 2007 (Sandhaus, 2008).
We filter out some noisy documents, for example,
</bodyText>
<figure confidence="0.997833333333333">
β
f
T1 T2
fs
θ
r
φt
|T|
fd
N
|D|
α
φrt2
βt2
|R|
φrt1
φrf
βt1
βf
|R|
|R|
</figure>
<page confidence="0.989591">
1459
</page>
<bodyText confidence="0.999961169811321">
obituary content, lists and so on. Obituary arti-
cles often contain syntax that diverges from stan-
dard newswire text. This leads to parse errors with
WSJ-trained parsers and in turn, makes extraction
harder. We also filter out documents that contain
lists or tables of items (such as books, movies) be-
cause this semi-structured information is not the fo-
cus of our current work. After filtering we are left
with approximately 428K documents. They are pre-
processed in several steps. First we employ Stanford
tools to tokenize, sentence-split and Part-Of-Speech
tag (Toutanova et al., 2003) a document. Next we
recognize named entities (Finkel et al., 2005) by
labelling tokens with PERSON, ORGANIZATION,
LOCATION, MISC and NONE tags. Consecutive
tokens which share the same category are assembled
into entity mentions. They serve as source and des-
tination arguments of the tuples we seek to model.
Finally we parse each sentence of a document using
MaltParser (Nivre et al., 2004) and extract depen-
dency paths for each pair of named entity mentions
in one sentence.
Following DIRT (Lin and Pantel, 2001), we fil-
ter out tuples that do not satisfy the following con-
straints. First, the path needs to be shorter than
10 edges, since longer paths occur less frequently.
Second, the dependency relations in the path should
connect two content words, i.e. nouns, verbs, ad-
jectives and adverbs. For example, in phrase ‘solve
a problem’, ‘obj(solve, problem)’ is kept, while
‘det(problem, a)’ is discarded. Finally, the de-
pendency labels on the path must not be: ‘conj’,
‘ccomp’, ‘parataxis’, ‘xcomp’, ‘pcomp’, ‘advcl’,
‘punct’, and ‘infmod’. This selection is based on the
observation that most of the times the corresponding
dependency relations do not explicitly state a rela-
tion between two candidate arguments.
After all entity mentions are generated and paths
are extracted, we have nearly 2.5M tuples. After
clustering (inference), each of these tuple will be-
long to one cluster/relation and is associated with its
clusterID.
We experimented with the number of clusters and
find that in a range of 50-200 the performance does
not vary significantly with different numbers. In our
experiments, we cluster the tuples into 100 relation
clusters for all three models. For Type-LDA model,
we use 50 entity clusters.
We evaluate our models in two ways. The first
aims at measuring the clustering quality by mapping
clusters to Freebase relations. The second seeks to
assess the utility of our predicted clusters as features
for relation extraction.
</bodyText>
<subsectionHeader confidence="0.982244">
4.1 Relations discovered by different models
</subsectionHeader>
<bodyText confidence="0.999933243902439">
Looking closely at the clusters we predict, we find
that some of them can be mapped to Freebase rela-
tions. We discover clusters that roughly correspond
to the parentCom (parent company relation), filmDi-
rector, authorOf, comBase (base of a company rela-
tion) and dieIn relations in Freebase. We treat Free-
base annotations as ground truth and measure recall.
We count each tuple in a cluster as true positive if
Freebase states the corresponding relation between
its argument pair. We find that precision numbers
against Freebase are low, below 10%. However,
these numbers are not reliable mainly because many
correct instances found by our models are missing
in Freebase. One reason why our predictions are
missing in Freebase is coreference. For example,
we predict parentCom relation between ‘Linksys’
and ‘Cisco’, while Freebase only considers ‘Cisco
Systems, Inc.’ as the parent company of ‘Linksys’.
It does not corefer ‘Cisco’ to ‘Cisco Systems, Inc.’.
Incorporating coreference in our model may fix this
problem and is a focus of future work. Instead of
measuring precision against Freebase, we ask hu-
mans to label 50 instances for each cluster and report
precision according to this annotated data. Table 3
shows the scores.
We can see that in most cases Rel-LDA1 and
Type-LDA substantially outperform the Rel-LDA
model. This is due to the fact that both models can
exploit more features to make clustering decisions.
For example, in Rel-LDA1 model, the NER pair fea-
ture restricts the entity types the two arguments can
take.
In the following, we take parentCom relation as
an example to analyze the behaviors of different
models. Rel-LDA includes spurious instances such
as ‘A is the chief executive of B’, while Rel-LDA1
has fewer such instances due to the NER pair fea-
ture. Similarly, by explicitly modeling entity type
constraints, Type-LDA makes fewer such errors. All
our models make mistakes when sentences have co-
ordination structures on which the parser has failed.
</bodyText>
<page confidence="0.889798">
1460
</page>
<table confidence="0.9999415625">
Rel. Sys. Rec. Prec.
Rel-LDA 51.4 76.0
parentCom Rel-LDA1 49.5 78.0
Type-LDA 55.3 72.0
Rel-LDA 42.5 32.0
filmDirector Rel-LDA1 70.5 40.0
Type-LDA 74.2 26.0
Rel-LDA 31.5 12.0
comBase Rel-LDA1 54.2 22.0
Type-LDA 57.1 30.0
Rel-LDA 25.2 84.0
authorOf Rel-LDA1 46.9 86.0
Type-LDA 20.2 68.0
Rel-LDA 26.5 34.0
dieIn Rel-LDA1 55.9 40.0
Type-LDA 50.2 28.0
</table>
<tableCaption confidence="0.996775">
Table 3: Clustering quality evaluation (%), Rec. is mea-
sured against Freebase, Prec. is measured according to
human annotators
</tableCaption>
<bodyText confidence="0.999969636363636">
For example, when a sentence has the following pat-
tern “The winners are A, a part of B; C, a part of
D; E, a part of F”, our models may predict parent-
Com(A,F), because the parser connects A with F via
the pattern ‘a part of’.
Some clusters found by our models cannot be
mapped to Freebase relations. Consider the Free-
base relation worksFor as one example. This re-
lation subsumes all types of employment relation-
ships, irrespective of the role the employee plays for
the employer. By contrast, our models discover clus-
ters such as leaderOf, editorOf that correspond to
more specific roles an employee can have. We show
some example relations in Table 4. In the table, the
2nd row shows a cluster of employees of news media
companies; the 3rd row shows leaders of companies;
the last one shows birth and death places of persons.
We can see that the last cluster is noisy since we
do not handle antonyms in our models. The argu-
ments of the clusters have noise too. For example,
‘New York’ occurs as a destination argument in the
2nd cluster. This is because ‘New York’ has high
frequency in the corpus and it brings noise to the
clustering results. In Table 5 some entity clusters
found by Type-LDA are shown. We find different
types of companies, such as financial companies and
news companies. We also find subclasses of person,
for example, reviewer and politician, because these
different entity classes participate in different rela-
tions. The last cluster shown in the table is a mix-
ture of news companies and government agencies.
This may be because this entity cluster is affected
by many relations.
</bodyText>
<subsectionHeader confidence="0.9155355">
4.2 Distant Supervision based Relation
Extraction
</subsectionHeader>
<bodyText confidence="0.998141973684211">
Our generative models detect clusters of dependency
paths and their arguments. Such clusters are inter-
esting in their own right, but we claim that they can
also be used to help a supervised relation extractor.
We validate this hypothesis in the context of relation
extraction with distant supervision using predicted
clusters as features.
Following previous work (Mintz et al., 2009), we
use Freebase as our distant supervision source, and
align related entity pairs to the New York Times arti-
cles discussed earlier. Our training and test instances
are pairs of entities for which both arguments appear
in at least one sentence together. Features of each
instance are extracted from all sentences in which
both entities appear together. The gold label for each
instance comes from Freebase. If a pair of entities
is not related according to Freebase, we consider it
a negative example. Note that this tends to create
some amount of noise: some pairs may be related,
but their relationships are not yet covered in Free-
base.
After filtering out relations with fewer than 10 in-
stances we have 65 relations and an additional “O”
label for unrelated pairs of entities. We call related
instances positive examples and unrelated instances
negative examples.
We train supervised classifiers using maximum
entropy. The baseline classifier employs features
that Mintz et al. (2009) used. To extract features
from the generative models we proceed as follows.
For each pair of entities, we collect all tuples asso-
ciated with it. For each of these tuples we extract its
clusterID, and use this ID as a binary feature.
The baseline system without generative model
features is called Distant. The classifiers with ad-
ditional features from generative models are named
after the generative models. Thus we have Rel-LDA,
Rel-LDA1 and Type-LDA classifiers. We compare
</bodyText>
<page confidence="0.969292">
1461
</page>
<table confidence="0.999651">
Source New York, Euro RSCG Worldwide, BBDO Worldwide, American, DDB Worldwide
Path X, a part of Y; X, a unit of Y; X unit of Y; X, a division of Y; X is a part of Y
Dest Omnicom Group, Interpublic Group of Companies, WPP Group, Publicis Groupe
Source Supreme Court, Anna Wintour, William Kristol, Bill Keller, Charles McGrath
Path X, an editor of Y; X, a publisher of Y; X, an editor at Y; X, an editor in chief of Y; X is an editor of Y;
Dest The Times, The New York Times, Vogue, Vanity Fair, New York
Source Kenneth L. Lay, L. Dennis Kozlowski, Bernard J. Ebbers, Thomas R. Suozzi, Bill Gates
Path X, the executive of Y; X, Y’s executive; X, Y executive; X, the chairman of Y; X, Y’s chairman
Dest Enron, Microsoft, WorldCom, Citigroup, Nassau County
Source Paul J. Browne, John McArdle, Tom Cocola, Claire Buchan, Steve Schmidt
Path X, a spokesman for Y; X, a spokeswoman for Y; X, Y spokesman; X, Y spokeswoman; X, a commissioner of Y
Dest White House, Justice Department, Pentagon, United States, State Department
Source United Nations, Microsoft, Intel, Internet, M. D. Anderson
Path X, based in Y; X, which is based in Y; X, a company in Y; X, a company based in Y; X, a consultant in Y
Dest New York, Washington, Manhattan, Chicago, London
Source Army, Shiite, Navy, John, David
Path X was born in Y; X die at home in Y; X die in Y; X, son of Y; X die at Y
Dest Manhattan, World War II, Brooklyn, Los Angeles, New York
</table>
<tableCaption confidence="0.999741">
Table 4: The path, source and destination arguments of some relations found by Rel-LDA1.
</tableCaption>
<table confidence="0.9980656">
Company Microsoft, Enron, NBC, CBS, Disney
FinanceCom Merrill Lynch, Morgan Stanley, Goldman Sachs, Lehman Brothers, Credit Suisse First Boston
News Notebook, New Yorker, Vogue, Vanity Fair, Newsweek
SportsTeam Yankees, Mets, Giants, Knicks, Jets
University University of California, Harvard, Columbia University, New York University, University of Penn.
Art Reviewer Stephen Holden, Ken Johnson, Roberta Smith, Anthony Tommasini, Grace Glueck
Games World Series, Olympic, World Cup, Super Bowl, Olympics
Politician Eliot Spitzer, Ari Fleischer, Kofi Annan, Scott McClellan, Karl Rove
Gov. Agency Congress, European Union, NATO, Federal Reserve, United States Court of Appeals
News/Agency The New York Times, The Times, Supreme Court, Security Council, Book Review
</table>
<tableCaption confidence="0.999887">
Table 5: The entity clusters found by Type-LDA
</tableCaption>
<bodyText confidence="0.999749375">
these against Distant and the DIRT database. For
the latter we parse our data using Minipar (Lin,
1998) and extract dependency paths between pairs
of named entity mentions. For each path, the top 3
similar paths are extracted from DIRT database. The
Minipar path and the similar paths are used as addi-
tional features.
For held-out evaluation, we construct the training
data from half of the positive examples and half of
the negative examples. The remaining examples are
used as test data. Note that the number of negative
instances is more than 10 times larger than the num-
ber of positive instances. At test time, we rank the
predictions by the conditional probabilities obtained
from the Maximum Entropy classifier. We report
precision of top ranked 50 instances for each relation
in table 6. From the table we can see that all systems
using additional features outperform the Distant sys-
tem. In average, our best model achieves 4.1%
improvement over the distant supervision baseline,
12% error reduction. The precision of bornIn is low
because in most cases we predict bornIn instances
as liveIn.
We expect systems using generative model fea-
tures to have higher recall than the baseline. This
is difficult to measure, but precision in the high re-
call area is a signal. We look at top ranked 1000
instances of each system and show the precision in
the last row of the table. We can see that our best
model Type-LDA outperforms the distant supervi-
sion baseline by 4.5%.
Why do generative model features help to im-
</bodyText>
<page confidence="0.974818">
1462
</page>
<table confidence="0.999910142857143">
Relation Dist Rel Rel1 Type DIRT
worksFor 80.0 92.0 86.0 90.0 84.0
authorOf 98.0 98.0 98.0 98.0 98.0
containedBy 92.0 96.0 96.0 92.0 96.0
bornIn 16.0 18.0 22.0 24.0 10.0
dieIn 28.0 30.0 28.0 24.0 24.0
liveIn 50.0 52.0 54.0 54.0 56.0
nationality 92.0 94.0 90.0 90.0 94.0
parentCom 94.0 96.0 96.0 96.0 90.0
founder 65.2 76.3 61.2 64.0 68.3
parent 52.0 54.0 50.0 52.0 52.0
filmDirector 54.0 60.0 60.0 64.0 62.0
Avg 65.6 69.7 67.4 68.0 66.8
Prec@1K 82.8 85.8 85.3 87.3 82.8
</table>
<tableCaption confidence="0.999596">
Table 6: Precision (%) of some frequent relations
</tableCaption>
<bodyText confidence="0.999849341463415">
prove relation extraction? One reason is that gen-
erative models can transfer information from known
patterns to unseen patterns. For example, given
“Sidney Mintz, the great food anthropologist at
Johns Hopkins University”, we want to predict the
relation between ‘Sidney Mintz’ and ‘Johns Hopkins
University’. The distant supervision system incor-
rectly predicts the pair as ‘O’ since it has not seen
the path ‘X, the anthropologist at Y’ in the training
data. By contrast, Rel-LDA can predict this pair cor-
rectly as worksFor because the dependency path of
this pair is in a cluster which contains the path ‘X, a
professor at Y’.
In addition to held-out evaluation we also carry
out manual evaluation. To this end, we use all the
positive examples and randomly select five times
the number of positive examples as negative ex-
amples to train a classifier. The remaining nega-
tive examples are candidate instances. We rank the
predicted instances according to their classification
scores. For each relation, we ask human annotators
to judge its top ranked 50 instances.
Table 7 lists the manual evaluation results for
some frequent relations. We also list how many in-
stances are found for each relation. For almost all
the relations, systems using generative model fea-
tures find more instances. In terms of precision, our
models perform comparatively to the baseline, even
better for some relations.
We also notice that clustering quality is not con-
sistent with distant supervision performance. Rel-
LDA1 can find better clusters than Rel-LDA but it
has lower precision in held-out evaluation. Type-
LDA underperforms Rel-LDA in average precision
but it gets higher precision in a higher recall area, i.e.
precision at 1K. One possible reason for the incon-
sistency is that the baseline distant supervision sys-
tem already employs features that are used in Rel-
LDA1. Another reason may be that the clusters do
not overlap with Freebase relations very well, see
section 4.1.
</bodyText>
<subsectionHeader confidence="0.999813">
4.3 Comparing against USP
</subsectionHeader>
<bodyText confidence="0.999939391304348">
We also try to compare against USP (Poon and
Domingos, 2008). Due to memory requirements of
USP, we are only able to run it on a smaller data
set consisting of 1,000 NYT documents; this is three
times the amount of data Poon and Domingos (2008)
used to train USP.2 For distant supervision based re-
lation extraction, we only match about 500 Freebase
instances to this small data set.
USP provides a parse tree for each sentence and
for each mention pair we can extract a path from
the tree. Since USP provides clusters of words and
phrases, we use the USP clusterID associated with
the words on the path as binary features in the clas-
sifier.
All models are less accurate when trained on this
smaller dataset; we can do as well as USP does,
even a little better. USP achieves 8.6% in F1, Rel-
LDA 8.7%, Rel-LDA1 10.3%, Type-LDA 8.9% and
Distant 10.3%. Of course, given larger datasets,
the performance of Rel-LDA, Rel-LDA1, and Type-
LDA improves considerably. In summary, compar-
ing against USP, our approach scales much more
easily to large data.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999925285714286">
Many approaches have been explored in relation ex-
traction, including bootstrapping, supervised classi-
fication, distant supervision, and unsupervised ap-
proaches.
Bootstrapping employs a few labeled examples
for each relation, iteratively extracts patterns from
the labeled seeds, and uses the patterns to extract
</bodyText>
<footnote confidence="0.77032">
2Using the publicly released USP code, training a model
with 1,000 documents resulted in about 45 gigabytes of heap
space in the JVM.
</footnote>
<page confidence="0.714934">
1463
</page>
<table confidence="0.999945357142857">
Relation Dist Top Type Dist #Instances Type
50 (%) Rel
Rel
worksFor 100.0 100.0 100.0 314 349 349
authorOf 94.0 94.0 96.0 185 208 229
containedBy 98.0 98.0 98.0 670 714 804
bornIn 82.6 88.2 88.0 46 36 56
dieIn 100.0 100.0 100.0 167 176 231
liveIn 98.0 98.0 94.0 77 86 109
nationality 78.0 82.0 76.0 84 92 114
parentCom 79.2 77.4 85.7 24 31 28
founder 80.0 80.0 50.0 5 5 14
parent 97.0 92.3 94.7 33 39 38
filmDirector 92.6 96.9 97.1 27 32 34
</table>
<tableCaption confidence="0.999874">
Table 7: Manual evaluation, Precision and recall of some frequent relations
</tableCaption>
<bodyText confidence="0.99973720967742">
more seeds (Brin, 1998). This approach may suffer
from low recall since the patterns can be too specific.
Supervised learning can discover more general
patterns (Kambhatla, 2004; Culotta and Sorensen,
2004). However, this approach requires labeled data,
and most work only carry out experiments on small
data set.
Distant supervision for relation extraction re-
quires no labeled data. The approach takes some
existing knowledge base as supervision source,
matches its relational instances against the text cor-
pus to build the training data, and extracts new in-
stances using the trained classifiers (Mintz et al.,
2009; Bunescu and Mooney, 2007; Riedel et al.,
2010; Yao et al., 2010).
All these approaches can not discover new rela-
tions and classify instances which do not belong to
any of the predefined relations. Other past work has
explored inducing relations using unsupervised ap-
proaches.
For example, DIRT (Lin and Pantel, 2001) aims
to discover different representations of the same se-
mantic relation, i.e. similar dependency paths. They
employ the distributional similarity based approach
while we use generative models. Both DIRT and our
approach take advantage of the arguments of depen-
dency paths to find semantic relations. Moreover,
our approach can cluster the arguments into differ-
ent types.
Unsupervised semantic parsing (USP) (Poon and
Domingos, 2008) discovers relations by merging
predicates which have similar meanings; it proceeds
to recursively cluster dependency tree fragments (or
“parts”) to best explain the observed sentence. It is
not focused on capturing any particular kind of re-
lation between sentence constituents, but to capture
repeated patterns. Our approach differs in that we
are focused on capturing a narrow range of binary
relations between named entities; some of our mod-
els (see Section 3) utilize entity type information to
constraint relation type induction. Also, our models
are built to be scalable and trained on a very large
corpus. In addition, we use a distant supervision
framework for evaluation.
Relation duality (Bollegala et al., 2010) employs
co-clustering to find clusters of entity pairs and pat-
terns. They identify each cluster of entity pairs as a
relation by selecting representative patterns for that
relation. This approach is related to our models,
however, it does not identify any entity clusters.
Generative probabilistic models are widely em-
ployed in relation extraction. For example, they are
used for in-domain relation discovery while incorpo-
rating constraints via posterior regularization (Chen
et al., 2011). We are focusing on open domain re-
lation discovery. Generative models are also ap-
plied to selectional preference discovery (Ritter et
al., 2010; Seaghdha, 2010). In this scenario, the
authors assume relation labels are given while we
automatically discover relations. Generative models
are also used in unsupervised coreference (Haghighi
and Klein, 2010).
</bodyText>
<page confidence="0.825923">
1464
</page>
<bodyText confidence="0.998667862745098">
Clustering is also employed in relation extraction. References
Hasegawa et al. (2004) cluster pairs of named en- Michele Banko and Oren Etzioni. 2008. The tradeoffs
tities according to the similarity of context words between open and traditional relation extraction. In
intervening between them. Their approach is not Proceedings of ACL-08: HLT.
probabilistic. Researchers also use topic models to Michele Banko, Michael J Cafarella, Stephen Soderland,
perform dimension reduction on features when they Matt Broadhead, and Oren Etzioni. 2007. Open in-
cluster relations (Hachey, 2009). However, they do formation extraction from the web. In Proceedings of
not explicitly model entity types. IJCAI2007.
Open information extraction aims to discover re- David Blei, Andrew Ng, and Michael Jordan. 2003. La-
lations independent of specific domains and rela- tent Dirichlet allocation. Journal of Machine Learning
tions (Banko et al., 2007; Banko and Etzioni, 2008). Research, 3:993–1022, January.
A self-learner is employed to extract relation in- Danushka Bollegala, Yutaka Matsuo, and Mitsuru
stances but the systems do not cluster the instances Ishizuka. 2010. Relational duality: Unsupervised ex-
into relations. Yates and Etzioni (2009) present RE- traction of semantic relations between entities on the
SOLVER for discovering relational synonyms as a web. In Proceedings of WWW.
post processing step. Our approach integrates entity Sergey Brin. 1998. Extracting patterns and relations
and relation discovery in a probabilistic model. from the world wide web. In Proc. of WebDB Work-
6 Conclusion shop at 6th International Conference on Extending
We have presented an unsupervised probabilistic Database Technology.
generative approach to relation extraction between Razvan C. Bunescu and Raymond J. Mooney. 2007.
two named entities. Our proposed models exploit Learning to extract relations from the web using min-
entity type constraints within a relation as well imal supervision. In Proceedings of the 45rd Annual
as features on the dependency path between entity Meeting of the Association for Computational Linguis-
mentions to cluster equivalent textual expressions. tics (ACL ’07).
We demonstrate the effectiveness of this approach Eugene Charniak and Micha Elsner. 2009. Em works for
by comparing induced relation clusters against a pronoun anaphora resolution. In Proceedings of ACL.
large knowledge base. We also show that using clus- Harr Chen, Edward Benson, Tahira Naseem, and Regina
ters of our models as features in distant supervised Barzilay. 2011. In-domain relation discovery with
framework yields 12% error reduction in precision meta-constraints via posterior regularization. In Pro-
over a weakly supervised baseline and outperforms ceedings of ACL.
other state-of-the art relation extraction techniques. Aron Culotta and Jeffery Sorensen. 2004. Dependency
Acknowledgments tree kernels for relation extraction. In 42nd Annual
This work was supported in part by the Center Meeting of the Association for Computational Linguis-
for Intelligent Information Retrieval and the Uni- tics, Barcelona, Spain.
versity of Massachusetts gratefully acknowledges Jenny Rose Finkel, Trond Grenager, and Christopher
the support of Defense Advanced Research Projects Manning. 2005. Incorporating non-local informa-
Agency (DARPA) Machine Reading Program un- tion into information extraction systems by gibbs sam-
der Air Force Research Laboratory (AFRL) prime pling. In Proceedings of the 43rd Annual Meeting of
contract no. FA8750-09-C-0181, ITR#1, and NSF the Association for Computational Linguistics (ACL
MALLET. Any opinions, findings, and conclusion ’05), pages 363–370, June.
or recommendations expressed in this material are Benjamin Hachey. 2009. Towards Generic Relation Ex-
those of the author(s) and do not necessarily reflect traction. Ph.D. thesis, University of Edinburgh.
the view of the DARPA, AFRL, or the US govern- Aria Haghighi and Dan Klein. 2010. Coreference resolu-
ment. Any opinions, findings and conclusions or tion in a modular, entity-centered model. In Proceed-
recommendations expressed in this material are the ings of HLT-NAACL.
authors’ and do not necessarily reflect those of the Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
sponsor. 2004. Discovering relations among named entities
1465 from large corpora. In ACL.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for extracting relations. In Proceedings of ACL.
</bodyText>
<reference confidence="0.99665272">
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of ACL 10.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of
inference rules from text. In Proceedings of KDD.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on the Eval-
uation of Parsing Systems.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49–56.
Hoifung Poon and Pedro Domingos. 2008. Unsuper-
vised semantic parsing. In Proceedings of the Confer-
ence on Empirical methods in natural language pro-
cessing (EMNLP).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ’10).
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet allocation method for selectional preferences.
In Proceedings of ACL10.
Dan Roth and Wen tau Yih. 2002. Probabilistic reason-
ing for entity and relation recognition. In Proceedings
of Coling.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Diarmuid O Seaghdha. 2010. Latent variable models of
selectional preference. In Proceedings ofACL 10.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL, pages 252–259.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1013–1023, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal ofArtificial Intelligence Research,
34:255–296.
</reference>
<page confidence="0.992944">
1466
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.879861">
<title confidence="0.999777">Structured Relation Discovery using Generative Models</title>
<author confidence="0.999796">Aria Sebastian Andrew</author>
<affiliation confidence="0.979329">of Computer Science, University of Massachusetts at Massachusetts Institute of</affiliation>
<abstract confidence="0.995707047619048">We explore unsupervised approaches to relation extraction between two named entities; instance, the semantic between a person and location entity. Concretely, we propose a series of generative probabilistic models, broadly similar to topic models, each which generates a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. The output of each model is a clustering of observed relation tuples and their associated textual expressions to underlying semantic relation types. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. We examine effectiveness of our approach via multiple evaluations and demonstrate 12% error reduction in precision over a state-of-the-art weakly supervised baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning arguments and supertypes of semantic relations using recursive patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 10.</booktitle>
<contexts>
<context position="12510" citStr="Kozareva and Hovy, 2010" startWordPosition="2046" endWordPosition="2049">(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). Hence we propose Type-LDA model. This model can capture the selectional preferences of relations to their arguments. In the mean time, it clusters tuples into relational clusters, and arguments into different entity clusters. The entity clusters could be interesting in many ways, for example, defining fine-grained entity types and finding new concepts. We split the features of a tuple into relation level features and entity level features. Relation level features include the dependency path, trigger, lex and POS features; entity level features include the entity mention itself and its named </context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010. Learning arguments and supertypes of semantic relations using recursive patterns. In Proceedings of ACL 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="2903" citStr="Lin and Pantel, 2001" startWordPosition="439" endWordPosition="442">en entities. Past work has shown that standard supervised techniques can yield high-performance relation detection when abundant labeled data exists for a fixed inventory of individual relation types (e.g. leaderOf) (Kambhatla, 2004; Culotta and Sorensen, 2004; Roth and tau Yih, 2002). However, less explored are open-domain approaches where the set of possible relation types are not fixed and little to no labeled is given for each relation type (Banko et al., 2007; Banko and Etzioni, 2008). A more related line of research has explored inducing relation types via clustering. For example, DIRT (Lin and Pantel, 2001) aims to discover different representations of the same semantic relation using distributional similarity of dependency paths. Poon and Domingos (2008) present an Unsupervised semantic parsing (USP) approach to partition dependency trees into meaningful fragments (or “parts” to use their terminology). The combinatorial nature of this dependency partition model makes it difficult for USP to scale to large data sets despite several necessary approximations during learning and infer1456 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456–1466, Edinbu</context>
<context position="15964" citStr="Lin and Pantel, 2001" startWordPosition="2623" endWordPosition="2626"> First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions. They serve as source and destination arguments of the tuples we seek to model. Finally we parse each sentence of a document using MaltParser (Nivre et al., 2004) and extract dependency paths for each pair of named entity mentions in one sentence. Following DIRT (Lin and Pantel, 2001), we filter out tuples that do not satisfy the following constraints. First, the path needs to be shorter than 10 edges, since longer paths occur less frequently. Second, the dependency relations in the path should connect two content words, i.e. nouns, verbs, adjectives and adverbs. For example, in phrase ‘solve a problem’, ‘obj(solve, problem)’ is kept, while ‘det(problem, a)’ is discarded. Finally, the dependency labels on the path must not be: ‘conj’, ‘ccomp’, ‘parataxis’, ‘xcomp’, ‘pcomp’, ‘advcl’, ‘punct’, and ‘infmod’. This selection is based on the observation that most of the times th</context>
<context position="32732" citStr="Lin and Pantel, 2001" startWordPosition="5407" endWordPosition="5410"> Distant supervision for relation extraction requires no labeled data. The approach takes some existing knowledge base as supervision source, matches its relational instances against the text corpus to build the training data, and extracts new instances using the trained classifiers (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010; Yao et al., 2010). All these approaches can not discover new relations and classify instances which do not belong to any of the predefined relations. Other past work has explored inducing relations using unsupervised approaches. For example, DIRT (Lin and Pantel, 2001) aims to discover different representations of the same semantic relation, i.e. similar dependency paths. They employ the distributional similarity based approach while we use generative models. Both DIRT and our approach take advantage of the arguments of dependency paths to find semantic relations. Moreover, our approach can cluster the arguments into different types. Unsupervised semantic parsing (USP) (Poon and Domingos, 2008) discovers relations by merging predicates which have similar meanings; it proceeds to recursively cluster dependency tree fragments (or “parts”) to best explain the </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="25848" citStr="Lin, 1998" startWordPosition="4253" endWordPosition="4254">versity, New York University, University of Penn. Art Reviewer Stephen Holden, Ken Johnson, Roberta Smith, Anthony Tommasini, Grace Glueck Games World Series, Olympic, World Cup, Super Bowl, Olympics Politician Eliot Spitzer, Ari Fleischer, Kofi Annan, Scott McClellan, Karl Rove Gov. Agency Congress, European Union, NATO, Federal Reserve, United States Court of Appeals News/Agency The New York Times, The Times, Supreme Court, Security Council, Book Review Table 5: The entity clusters found by Type-LDA these against Distant and the DIRT database. For the latter we parse our data using Minipar (Lin, 1998) and extract dependency paths between pairs of named entity mentions. For each path, the top 3 similar paths are extracted from DIRT database. The Minipar path and the similar paths are used as additional features. For held-out evaluation, we construct the training data from half of the positive examples and half of the negative examples. The remaining examples are used as test data. Note that the number of negative instances is more than 10 times larger than the number of positive instances. At test time, we rank the predictions by the conditional probabilities obtained from the Maximum Entro</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Proceedings of the Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="1681" citStr="Mintz et al., 2009" startWordPosition="238" endWordPosition="241">nstrate 12% error reduction in precision over a state-of-the-art weakly supervised baseline. 1 Introduction Many NLP applications would benefit from large knowledge bases of relational information about entities. For instance, knowing that the entity Steve Balmer bears the leaderOf relation to the entity Microsoft, would facilitate question answering (Ravichandran and Hovy, 2002), data mining, and a host of other end-user applications. Due to these many potential applications, relation extraction has gained much attention in information extraction (Kambhatla, 2004; Culotta and Sorensen, 2004; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). We propose a series of generative probabilistic models, broadly similar to standard topic models, which generate a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. The output of our approach is a clustering over observed relation paths (e.g. “X was born in Y” and “X is from Y”) such that expressions in the same cluster bear the same semantic relation type betw</context>
<context position="21945" citStr="Mintz et al., 2009" startWordPosition="3600" endWordPosition="3603">n different relations. The last cluster shown in the table is a mixture of news companies and government agencies. This may be because this entity cluster is affected by many relations. 4.2 Distant Supervision based Relation Extraction Our generative models detect clusters of dependency paths and their arguments. Such clusters are interesting in their own right, but we claim that they can also be used to help a supervised relation extractor. We validate this hypothesis in the context of relation extraction with distant supervision using predicted clusters as features. Following previous work (Mintz et al., 2009), we use Freebase as our distant supervision source, and align related entity pairs to the New York Times articles discussed earlier. Our training and test instances are pairs of entities for which both arguments appear in at least one sentence together. Features of each instance are extracted from all sentences in which both entities appear together. The gold label for each instance comes from Freebase. If a pair of entities is not related according to Freebase, we consider it a negative example. Note that this tends to create some amount of noise: some pairs may be related, but their relatio</context>
<context position="32414" citStr="Mintz et al., 2009" startWordPosition="5355" endWordPosition="5358">s more seeds (Brin, 1998). This approach may suffer from low recall since the patterns can be too specific. Supervised learning can discover more general patterns (Kambhatla, 2004; Culotta and Sorensen, 2004). However, this approach requires labeled data, and most work only carry out experiments on small data set. Distant supervision for relation extraction requires no labeled data. The approach takes some existing knowledge base as supervision source, matches its relational instances against the text corpus to build the training data, and extracts new instances using the trained classifiers (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010; Yao et al., 2010). All these approaches can not discover new relations and classify instances which do not belong to any of the predefined relations. Other past work has explored inducing relations using unsupervised approaches. For example, DIRT (Lin and Pantel, 2001) aims to discover different representations of the same semantic relation, i.e. similar dependency paths. They employ the distributional similarity based approach while we use generative models. Both DIRT and our approach take advantage of the arguments of dependency paths to find </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="15841" citStr="Nivre et al., 2004" startWordPosition="2602" endWordPosition="2605"> our current work. After filtering we are left with approximately 428K documents. They are preprocessed in several steps. First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions. They serve as source and destination arguments of the tuples we seek to model. Finally we parse each sentence of a document using MaltParser (Nivre et al., 2004) and extract dependency paths for each pair of named entity mentions in one sentence. Following DIRT (Lin and Pantel, 2001), we filter out tuples that do not satisfy the following constraints. First, the path needs to be shorter than 10 edges, since longer paths occur less frequently. Second, the dependency relations in the path should connect two content words, i.e. nouns, verbs, adjectives and adverbs. For example, in phrase ‘solve a problem’, ‘obj(solve, problem)’ is kept, while ‘det(problem, a)’ is discarded. Finally, the dependency labels on the path must not be: ‘conj’, ‘ccomp’, ‘paratax</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP).</booktitle>
<contexts>
<context position="3054" citStr="Poon and Domingos (2008)" startWordPosition="461" endWordPosition="464"> for a fixed inventory of individual relation types (e.g. leaderOf) (Kambhatla, 2004; Culotta and Sorensen, 2004; Roth and tau Yih, 2002). However, less explored are open-domain approaches where the set of possible relation types are not fixed and little to no labeled is given for each relation type (Banko et al., 2007; Banko and Etzioni, 2008). A more related line of research has explored inducing relation types via clustering. For example, DIRT (Lin and Pantel, 2001) aims to discover different representations of the same semantic relation using distributional similarity of dependency paths. Poon and Domingos (2008) present an Unsupervised semantic parsing (USP) approach to partition dependency trees into meaningful fragments (or “parts” to use their terminology). The combinatorial nature of this dependency partition model makes it difficult for USP to scale to large data sets despite several necessary approximations during learning and infer1456 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456–1466, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics ence. Our work is similar to DIRT and USP in that of relation tup</context>
<context position="29833" citStr="Poon and Domingos, 2008" startWordPosition="4918" endWordPosition="4921">g quality is not consistent with distant supervision performance. RelLDA1 can find better clusters than Rel-LDA but it has lower precision in held-out evaluation. TypeLDA underperforms Rel-LDA in average precision but it gets higher precision in a higher recall area, i.e. precision at 1K. One possible reason for the inconsistency is that the baseline distant supervision system already employs features that are used in RelLDA1. Another reason may be that the clusters do not overlap with Freebase relations very well, see section 4.1. 4.3 Comparing against USP We also try to compare against USP (Poon and Domingos, 2008). Due to memory requirements of USP, we are only able to run it on a smaller data set consisting of 1,000 NYT documents; this is three times the amount of data Poon and Domingos (2008) used to train USP.2 For distant supervision based relation extraction, we only match about 500 Freebase instances to this small data set. USP provides a parse tree for each sentence and for each mention pair we can extract a path from the tree. Since USP provides clusters of words and phrases, we use the USP clusterID associated with the words on the path as binary features in the classifier. All models are less</context>
<context position="33166" citStr="Poon and Domingos, 2008" startWordPosition="5471" endWordPosition="5474">sify instances which do not belong to any of the predefined relations. Other past work has explored inducing relations using unsupervised approaches. For example, DIRT (Lin and Pantel, 2001) aims to discover different representations of the same semantic relation, i.e. similar dependency paths. They employ the distributional similarity based approach while we use generative models. Both DIRT and our approach take advantage of the arguments of dependency paths to find semantic relations. Moreover, our approach can cluster the arguments into different types. Unsupervised semantic parsing (USP) (Poon and Domingos, 2008) discovers relations by merging predicates which have similar meanings; it proceeds to recursively cluster dependency tree fragments (or “parts”) to best explain the observed sentence. It is not focused on capturing any particular kind of relation between sentence constituents, but to capture repeated patterns. Our approach differs in that we are focused on capturing a narrow range of binary relations between named entities; some of our models (see Section 3) utilize entity type information to constraint relation type induction. Also, our models are built to be scalable and trained on a very l</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Unsupervised semantic parsing. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1445" citStr="Ravichandran and Hovy, 2002" startWordPosition="202" endWordPosition="205">derlying semantic relation types. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. We examine effectiveness of our approach via multiple evaluations and demonstrate 12% error reduction in precision over a state-of-the-art weakly supervised baseline. 1 Introduction Many NLP applications would benefit from large knowledge bases of relational information about entities. For instance, knowing that the entity Steve Balmer bears the leaderOf relation to the entity Microsoft, would facilitate question answering (Ravichandran and Hovy, 2002), data mining, and a host of other end-user applications. Due to these many potential applications, relation extraction has gained much attention in information extraction (Kambhatla, 2004; Culotta and Sorensen, 2004; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). We propose a series of generative probabilistic models, broadly similar to standard topic models, which generate a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. Our proposed models exploit entity type constraints within a relation as well as features on the dep</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</booktitle>
<contexts>
<context position="1702" citStr="Riedel et al., 2010" startWordPosition="242" endWordPosition="245">duction in precision over a state-of-the-art weakly supervised baseline. 1 Introduction Many NLP applications would benefit from large knowledge bases of relational information about entities. For instance, knowing that the entity Steve Balmer bears the leaderOf relation to the entity Microsoft, would facilitate question answering (Ravichandran and Hovy, 2002), data mining, and a host of other end-user applications. Due to these many potential applications, relation extraction has gained much attention in information extraction (Kambhatla, 2004; Culotta and Sorensen, 2004; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). We propose a series of generative probabilistic models, broadly similar to standard topic models, which generate a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. The output of our approach is a clustering over observed relation paths (e.g. “X was born in Y” and “X is from Y”) such that expressions in the same cluster bear the same semantic relation type between entities. Past wo</context>
<context position="32461" citStr="Riedel et al., 2010" startWordPosition="5363" endWordPosition="5366">uffer from low recall since the patterns can be too specific. Supervised learning can discover more general patterns (Kambhatla, 2004; Culotta and Sorensen, 2004). However, this approach requires labeled data, and most work only carry out experiments on small data set. Distant supervision for relation extraction requires no labeled data. The approach takes some existing knowledge base as supervision source, matches its relational instances against the text corpus to build the training data, and extracts new instances using the trained classifiers (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010; Yao et al., 2010). All these approaches can not discover new relations and classify instances which do not belong to any of the predefined relations. Other past work has explored inducing relations using unsupervised approaches. For example, DIRT (Lin and Pantel, 2001) aims to discover different representations of the same semantic relation, i.e. similar dependency paths. They employ the distributional similarity based approach while we use generative models. Both DIRT and our approach take advantage of the arguments of dependency paths to find semantic relations. Moreover, our approach can </context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL10.</booktitle>
<contexts>
<context position="12468" citStr="Ritter et al., 2010" startWordPosition="2040" endWordPosition="2043">xample. Following typical EM learning(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). Hence we propose Type-LDA model. This model can capture the selectional preferences of relations to their arguments. In the mean time, it clusters tuples into relational clusters, and arguments into different entity clusters. The entity clusters could be interesting in many ways, for example, defining fine-grained entity types and finding new concepts. We split the features of a tuple into relation level features and entity level features. Relation level features include the dependency path, trigger, lex and POS features; entity level features includ</context>
<context position="34522" citStr="Ritter et al., 2010" startWordPosition="5679" endWordPosition="5682">stering to find clusters of entity pairs and patterns. They identify each cluster of entity pairs as a relation by selecting representative patterns for that relation. This approach is related to our models, however, it does not identify any entity clusters. Generative probabilistic models are widely employed in relation extraction. For example, they are used for in-domain relation discovery while incorporating constraints via posterior regularization (Chen et al., 2011). We are focusing on open domain relation discovery. Generative models are also applied to selectional preference discovery (Ritter et al., 2010; Seaghdha, 2010). In this scenario, the authors assume relation labels are given while we automatically discover relations. Generative models are also used in unsupervised coreference (Haghighi and Klein, 2010). 1464 Clustering is also employed in relation extraction. References Hasegawa et al. (2004) cluster pairs of named en- Michele Banko and Oren Etzioni. 2008. The tradeoffs tities according to the similarity of context words between open and traditional relation extraction. In intervening between them. Their approach is not Proceedings of ACL-08: HLT. probabilistic. Researchers also use </context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of ACL10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>Probabilistic reasoning for entity and relation recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of Coling.</booktitle>
<marker>Roth, Yih, 2002</marker>
<rawString>Dan Roth and Wen tau Yih. 2002. Probabilistic reasoning for entity and relation recognition. In Proceedings of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times Annotated Corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia.</location>
<contexts>
<context position="14736" citStr="Sandhaus, 2008" startWordPosition="2416" endWordPosition="2417">odel. At inference time, we sample r, T1 and T2 for each tuple. For efficient inference, we first initialize the model without T1 and T2, i.e. all the features are generated directly from r. Here the model degenerates to Rel-LDA1. After some iterations, we introduce T1 and T2. We sample the relation variable (r) and two mention types variables (T1,T2) iteratively for each tuple. We can sample them together, but this is not very efficient. In addition, we found that it does not improve performance. 4 Experiments Our experiments are carried out on New York Times articles from year 2000 to 2007 (Sandhaus, 2008). We filter out some noisy documents, for example, β f T1 T2 fs θ r φt |T| fd N |D| α φrt2 βt2 |R| φrt1 φrf βt1 βf |R| |R| 1459 obituary content, lists and so on. Obituary articles often contain syntax that diverges from standard newswire text. This leads to parse errors with WSJ-trained parsers and in turn, makes extraction harder. We also filter out documents that contain lists or tables of items (such as books, movies) because this semi-structured information is not the focus of our current work. After filtering we are left with approximately 428K documents. They are preprocessed in several</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus, 2008. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O Seaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL 10.</booktitle>
<contexts>
<context position="12484" citStr="Seaghdha, 2010" startWordPosition="2044" endWordPosition="2045">ical EM learning(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). Hence we propose Type-LDA model. This model can capture the selectional preferences of relations to their arguments. In the mean time, it clusters tuples into relational clusters, and arguments into different entity clusters. The entity clusters could be interesting in many ways, for example, defining fine-grained entity types and finding new concepts. We split the features of a tuple into relation level features and entity level features. Relation level features include the dependency path, trigger, lex and POS features; entity level features include the entity men</context>
<context position="34539" citStr="Seaghdha, 2010" startWordPosition="5683" endWordPosition="5684">ers of entity pairs and patterns. They identify each cluster of entity pairs as a relation by selecting representative patterns for that relation. This approach is related to our models, however, it does not identify any entity clusters. Generative probabilistic models are widely employed in relation extraction. For example, they are used for in-domain relation discovery while incorporating constraints via posterior regularization (Chen et al., 2011). We are focusing on open domain relation discovery. Generative models are also applied to selectional preference discovery (Ritter et al., 2010; Seaghdha, 2010). In this scenario, the authors assume relation labels are given while we automatically discover relations. Generative models are also used in unsupervised coreference (Haghighi and Klein, 2010). 1464 Clustering is also employed in relation extraction. References Hasegawa et al. (2004) cluster pairs of named en- Michele Banko and Oren Etzioni. 2008. The tradeoffs tities according to the similarity of context words between open and traditional relation extraction. In intervening between them. Their approach is not Proceedings of ACL-08: HLT. probabilistic. Researchers also use topic models to M</context>
</contexts>
<marker>Seaghdha, 2010</marker>
<rawString>Diarmuid O Seaghdha. 2010. Latent variable models of selectional preference. In Proceedings ofACL 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>HLTNAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="15450" citStr="Toutanova et al., 2003" startWordPosition="2539" endWordPosition="2542">βt2 |R| φrt1 φrf βt1 βf |R| |R| 1459 obituary content, lists and so on. Obituary articles often contain syntax that diverges from standard newswire text. This leads to parse errors with WSJ-trained parsers and in turn, makes extraction harder. We also filter out documents that contain lists or tables of items (such as books, movies) because this semi-structured information is not the focus of our current work. After filtering we are left with approximately 428K documents. They are preprocessed in several steps. First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions. They serve as source and destination arguments of the tuples we seek to model. Finally we parse each sentence of a document using MaltParser (Nivre et al., 2004) and extract dependency paths for each pair of named entity mentions in one sentence. Following DIRT (Lin and Pantel, 2001), we filter out tuples that do not satisfy the following constraints. First, the path </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In HLTNAACL, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective cross-document relation extraction without labelled data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1013--1023</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1721" citStr="Yao et al., 2010" startWordPosition="246" endWordPosition="249">over a state-of-the-art weakly supervised baseline. 1 Introduction Many NLP applications would benefit from large knowledge bases of relational information about entities. For instance, knowing that the entity Steve Balmer bears the leaderOf relation to the entity Microsoft, would facilitate question answering (Ravichandran and Hovy, 2002), data mining, and a host of other end-user applications. Due to these many potential applications, relation extraction has gained much attention in information extraction (Kambhatla, 2004; Culotta and Sorensen, 2004; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). We propose a series of generative probabilistic models, broadly similar to standard topic models, which generate a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. The output of our approach is a clustering over observed relation paths (e.g. “X was born in Y” and “X is from Y”) such that expressions in the same cluster bear the same semantic relation type between entities. Past work has shown that s</context>
<context position="32480" citStr="Yao et al., 2010" startWordPosition="5367" endWordPosition="5370"> since the patterns can be too specific. Supervised learning can discover more general patterns (Kambhatla, 2004; Culotta and Sorensen, 2004). However, this approach requires labeled data, and most work only carry out experiments on small data set. Distant supervision for relation extraction requires no labeled data. The approach takes some existing knowledge base as supervision source, matches its relational instances against the text corpus to build the training data, and extracts new instances using the trained classifiers (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010; Yao et al., 2010). All these approaches can not discover new relations and classify instances which do not belong to any of the predefined relations. Other past work has explored inducing relations using unsupervised approaches. For example, DIRT (Lin and Pantel, 2001) aims to discover different representations of the same semantic relation, i.e. similar dependency paths. They employ the distributional similarity based approach while we use generative models. Both DIRT and our approach take advantage of the arguments of dependency paths to find semantic relations. Moreover, our approach can cluster the argumen</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2010</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction without labelled data. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013–1023, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>34--255</pages>
<contexts>
<context position="35971" citStr="Yates and Etzioni (2009)" startWordPosition="5887" endWordPosition="5890">ion extraction from the web. In Proceedings of not explicitly model entity types. IJCAI2007. Open information extraction aims to discover re- David Blei, Andrew Ng, and Michael Jordan. 2003. Lalations independent of specific domains and rela- tent Dirichlet allocation. Journal of Machine Learning tions (Banko et al., 2007; Banko and Etzioni, 2008). Research, 3:993–1022, January. A self-learner is employed to extract relation in- Danushka Bollegala, Yutaka Matsuo, and Mitsuru stances but the systems do not cluster the instances Ishizuka. 2010. Relational duality: Unsupervised exinto relations. Yates and Etzioni (2009) present RE- traction of semantic relations between entities on the SOLVER for discovering relational synonyms as a web. In Proceedings of WWW. post processing step. Our approach integrates entity Sergey Brin. 1998. Extracting patterns and relations and relation discovery in a probabilistic model. from the world wide web. In Proc. of WebDB Work6 Conclusion shop at 6th International Conference on Extending We have presented an unsupervised probabilistic Database Technology. generative approach to relation extraction between Razvan C. Bunescu and Raymond J. Mooney. 2007. two named entities. Our </context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. Journal ofArtificial Intelligence Research, 34:255–296.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>