<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013030">
<title confidence="0.987593">
Automatically Predicting Peer-Review Helpfulness
</title>
<author confidence="0.998285">
Wenting Xiong
</author>
<affiliation confidence="0.999778">
University of Pittsburgh
Department of Computer Science
</affiliation>
<address confidence="0.879051">
Pittsburgh, PA, 15260
</address>
<email confidence="0.99855">
wex12@cs.pitt.edu
</email>
<author confidence="0.995677">
Diane Litman
</author>
<affiliation confidence="0.902901666666667">
University of Pittsburgh
Department of Computer Science &amp;
Learning Research and Development Center
</affiliation>
<address confidence="0.893028">
Pittsburgh, PA, 15260
</address>
<email confidence="0.999388">
litman@cs.pitt.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999594117647059">
Identifying peer-review helpfulness is an im-
portant task for improving the quality of feed-
back that students receive from their peers. As
a first step towards enhancing existing peer-
review systems with new functionality based
on helpfulness detection, we examine whether
standard product review analysis techniques
also apply to our new context of peer reviews.
In addition, we investigate the utility of in-
corporating additional specialized features tai-
lored to peer review. Our preliminary results
show that the structural features, review uni-
grams and meta-data combined are useful in
modeling the helpfulness of both peer reviews
and product reviews, while peer-review spe-
cific auxiliary features can further improve
helpfulness prediction.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997435">
Peer reviewing of student writing has been widely
used in various academic fields. While existing
web-based peer-review systems largely save instruc-
tors effort in setting up peer-review assignments and
managing document assignment, there still remains
the problem that the quality of peer reviews is of-
ten poor (Nelson and Schunn, 2009). Thus to en-
hance the effectiveness of existing peer-review sys-
tems, we propose to automatically predict the help-
fulness of peer reviews.
In this paper, we examine prior techniques that
have been used to successfully rank helpfulness for
product reviews, and adapt them to the peer-review
domain. In particular, we use an SVM regression al-
gorithm to predict the helpfulness of peer reviews
based on generic linguistic features automatically
mined from peer reviews and students’ papers, plus
specialized features based on existing knowledge
about peer reviews. We not only demonstrate that
prior techniques from product reviews can be suc-
cessfully tailored to peer reviews, but also show the
importance of peer-review specific features.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999878">
Prior studies of peer review in the Natural Lan-
guage Processing field have not focused on help-
fulness prediction, but instead have been concerned
with issues such as highlighting key sentences in pa-
pers (Sandor and Vorndran, 2009), detecting impor-
tant feedback features in reviews (Cho, 2008; Xiong
and Litman, 2010), and adapting peer-review assign-
ment (Garcia, 2010). However, given some simi-
larity between peer reviews and other review types,
we hypothesize that techniques used to predict re-
view helpfulness in other domains can also be ap-
plied to peer reviews. Kim et al. (2006) used re-
gression to predict the helpfulness ranking of prod-
uct reviews based on various classes of linguistic
features. Ghose and Ipeirotis (2010) further exam-
ined the socio-economic impact of product reviews
using a similar approach and suggested the useful-
ness of subjectivity analysis. Another study (Liu
et al., 2008) of movie reviews showed that helpful-
ness depends on reviewers’ expertise, their writing
style, and the timeliness of the review. Tsur and
Rappoport (2009) proposed RevRank to select the
most helpful book reviews in an unsupervised fash-
ion based on review lexicons. However, studies of
Amazon’s product reviews also show that the per-
</bodyText>
<page confidence="0.968838">
502
</page>
<note confidence="0.849119">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 502–507,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.9052661">
Class Label Features
Structural STR review length in terms of tokens, number of sentences, percentage of sentences
that end with question marks, number of exclamatory sentences.
Lexical UGR, BGR tf-idf statistics of review unigrams and bigrams.
Syntactic SYN percentage of tokens that are nouns, verbs, verbs conjugated in the
first person, adjectives / adverbs and open classes, respectively.
Semantic TOP, counts of topic words,
posW, negW counts of positive and negative sentiment words.
Meta-data MET the overall ratings of papers assigned by reviewers, and the absolute
difference between the rating and the average score given by all reviewers.
</table>
<tableCaption confidence="0.998897">
Table 1: Generic features motivated by related work of product reviews (Kim et al., 2006).
</tableCaption>
<bodyText confidence="0.995128285714286">
ceived helpfulness of a review depends not only on
its review content, but also on social effects such as
product qualities, and individual bias in the presence
of mixed opinion distribution (Danescu-Niculescu-
Mizil et al., 2009).
Nonetheless, several properties distinguish our
corpus of peer reviews from other types of reviews:
</bodyText>
<listItem confidence="0.888008928571429">
1) The helpfulness of our peer reviews is directly
rated using a discrete scale from one to five instead
of being defined as a function of binary votes (e.g.
the percentage of “helpful” votes (Kim et al., 2006));
2) Peer reviews frequently refer to the related stu-
dents’ papers, thus review analysis needs to take into
account paper topics; 3) Within the context of edu-
cation, peer-review helpfulness often has a writing
specific semantics, e.g. improving revision likeli-
hood; 4) In general, peer-review corpora collected
from classrooms are of a much smaller size com-
pared to online product reviews. To tailor existing
techniques to peer reviews, we will thus propose
new specialized features to address these issues.
</listItem>
<sectionHeader confidence="0.921171" genericHeader="method">
3 Data and Features
</sectionHeader>
<bodyText confidence="0.948235896551724">
In this study, we use a previously annotated peer-
review corpus (Nelson and Schunn, 2009; Patchan
et al., 2009), collected using a freely available web-
based peer-review system (Cho and Schunn, 2007)
in an introductory college history class. The corpus
consists of 16 papers (about six pages each) and 267
reviews (varying from twenty words to about two
hundred words). Two experts (a writing instructor
and a content instructor) (Patchan et al., 2009) were
asked to rate the helpfulness of each peer review
on a scale from one to five (Pearson correlation
r = 0.425, p &lt; 0.01). For our study, we consider
the average ratings given by the two experts (which
roughly follow a normal distribution) as the gold
standard of review helpfulness. Two example rated
peer reviews (shown verbatim) follow:
A helpful peer review of average-rating 5:
The support and explanation of the ideas could use
some work. broading the explanations to include all
groups could be useful. My concerns come from some
of the claims that are put forth. Page 2 says that the
13th amendment ended the war. is this true? was there
no more fighting or problems once this amendment was
added? ...
The arguments were sorted up into paragraphs,
keeping the area of interest clear, but be careful about
bringing up new things at the end and then simply leaving
them there without elaboration (ie black sterilization at
the end of the paragraph).
</bodyText>
<subsectionHeader confidence="0.660144">
An unhelpful peer review of average-rating 1:
</subsectionHeader>
<bodyText confidence="0.995285076923077">
Your paper and its main points are easy to find and to
follow.
As shown in Table 1, we first mine generic
linguistic features from reviews and papers based
on the results of syntactic analysis of the texts,
aiming to replicate the feature sets used by Kim et
al. (2006). While structural, lexical and syntactic
features are created in the same way as suggested
in their paper, we adapt the semantic and meta-data
features to peer reviews by converting the mentions
of product properties to mentions of the history
topics and by using paper ratings assigned by peers
instead of product scores.1
</bodyText>
<footnote confidence="0.9618455">
1We used MSTParser (McDonald et al., 2005) for syntactic
analysis. Topic words are automatically extracted from all stu-
</footnote>
<page confidence="0.997703">
503
</page>
<bodyText confidence="0.988897222222222">
In addition, the following specialized features are
motivated by an empirical study in cognitive sci-
ence (Nelson and Schunn, 2009), which suggests
that students’ revision likelihood is significantly cor-
related with certain feedback features, and by our
prior work (Xiong and Litman, 2010; Xiong et
al., 2010) for detecting these cognitive science con-
structs automatically:
Cognitive-science features (cogS): For a given
review, cognitive-science constructs that are signifi-
cantly correlated with review implementation likeli-
hood are manually coded for each idea unit (Nel-
son and Schunn, 2009) within the review. Note,
however, that peer-review helpfulness is rated for
the whole review, which can include multiple idea
units.2 Therefore in our study, we calculate the dis-
tribution of feedbackType values (praise, problem,
and summary) (kappa = .92), the percentage of
problems that have problem localization —the pres-
ence of information indicating where the problem is
localized in the related paper— (kappa = .69), and
the percentage of problems that have a solution —
the presence of a solution addressing the problem
mentioned in the review— (kappa = .79) to model
peer-review helpfulness. These kappa values (Nel-
son and Schunn, 2009) were calculated from a sub-
set of the corpus for evaluating the reliability of hu-
man annotations3. Consider the example of the help-
ful review presented in Section 3 which was manu-
ally separated into two idea units (each presented in
a separate paragraph). As both ideas are coded as
problem with the presence of problem localization
and solution, the cognitive-science features of this
review are praise%=0, problem%=1, summary%=0,
localization%=1, and solution%=1.
Lexical category features (LEX2): Ten cate-
gories of keyword lexicons developed for automat-
ically detecting the previously manually annotated
feedback types (Xiong et al., 2010). The categories
are learned in a semi-supervised way based on syn-
tactic and semantic functions, such as suggestion
dents’ papers using topic signature (Lin and Hovy, 2000) soft-
ware kindly provided by Annie Louis. Positive and negative
sentiment words are extracted from the General Inquirer Dic-
tionaries (http://www.wjh.harvard.edu/ inquirer/homecat.htm).
</bodyText>
<footnote confidence="0.6410705">
2Details of different granularity levels of annotation can be
found in (Nelson and Schunn, 2009).
3These annotators are not the same experts who rated the
peer-review helpfulness.
</footnote>
<bodyText confidence="0.999450894736842">
modal verbs (e.g. should, must, might, could, need),
negations (e.g. not, don’t, doesn’t), positive and neg-
ative words, and so on. We first manually created
a list of words that were specified as signal words
for annotating feedbackType and problem localiza-
tion in the coding manual; then we supplemented
the list with words selected by a decision tree model
learned using a Bag-of-Words representation of the
peer reviews. These categories will also be helpful
for reducing the feature space size as discussed be-
low.
Localization features (LOC): Five features de-
veloped in our prior work (Xiong and Litman, 2010)
for automatically identifying the manually coded
problem localization tags, such as the percentage of
problems in reviews that could be matched with a
localization pattern (e.g. “on page 5”, “the section
about”), the percentage of sentences in which topic
words exist between the subject and object, etc.
</bodyText>
<sectionHeader confidence="0.995413" genericHeader="method">
4 Experiment and Results
</sectionHeader>
<bodyText confidence="0.999912714285714">
Following Kim et al. (2006), we train our helpful-
ness model using SVM regression with a radial ba-
sis function kernel provided by SVM&amp;quot;ght (Joachims,
1999). We first evaluate each feature type in iso-
lation to investigate its predictive power of peer-
review helpfulness; we then examine them together
in various combinations to find the most useful fea-
ture set for modeling peer-review helpfulness. Per-
formance is evaluated in 10-fold cross validation
of our 267 peer reviews by predicting the absolute
helpfulness scores (with Pearson correlation coeffi-
cient r) as well as by predicting helpfulness rank-
ing (with Spearman rank correlation coefficient rs).
Although predicted helpfulness ranking could be di-
rectly used to compare the helpfulness of a given set
of reviews, predicting helpfulness rating is desirable
in practice to compare helpfulness between existing
reviews and new written ones without reranking all
previously ranked reviews. Results are presented re-
garding the generic features and the specialized fea-
tures respectively, with 95% confidence bounds.
</bodyText>
<subsectionHeader confidence="0.998275">
4.1 Performance of Generic Features
</subsectionHeader>
<bodyText confidence="0.98418">
Evaluation of the generic features is presented in
Table 2, showing that all classes except syntac-
tic (SYN) and meta-data (MET) features are sig-
</bodyText>
<page confidence="0.9909">
504
</page>
<bodyText confidence="0.9998192">
nificantly correlated with both helpfulness rating
(r) and helpfulness ranking (rs). Structural fea-
tures (bolded) achieve the highest Pearson (0.60)
and Spearman correlation coefficients (0.59) (al-
though within the significant correlations, the dif-
ference among coefficients are insignificant). Note
that in isolation, MET (paper ratings) are not sig-
nificantly correlated with peer-review helpfulness,
which is different from prior findings of product re-
views (Kim et al., 2006) where product scores are
significantly correlated with product-review help-
fulness. However, when combined with other fea-
tures, MET does appear to add value (last row).
When comparing the performance between predict-
ing helpfulness ratings versus ranking, we observe
r pz� rs consistently for our peer reviews, while Kim
et al. (2006) reported r &lt; rs for product reviews.4
Finally, we observed a similar feature redundancy
effect as Kim et al. (2006) did, in that simply com-
bining all features does not improve the model’s per-
formance. Interestingly, our best feature combina-
tion (last row) is the same as theirs. In sum our
results verify our hypothesis that the effectiveness
of generic features can be transferred to our peer-
review domain for predicting review helpfulness.
</bodyText>
<table confidence="0.999731923076923">
Features Pearson r Spearman r$
STR 0.60 f 0.10* 0.59 f 0.10*
UGR 0.53 f 0.09* 0.54 f 0.09*
BGR 0.58 f 0.07* 0.57 f 0.10*
SYN 0.36 f 0.12 0.35 f 0.11
TOP 0.55 f 0.10* 0.54 f 0.10*
posW 0.57 f 0.13* 0.53 f 0.12*
negW 0.49 f 0.11* 0.46 f 0.10*
MET 0.22 f 0.15 0.23 f 0.12
All-combined 0.56 f 0.07* 0.58 f 0.09*
STR+UGR+MET 0.61 f 0.10* 0.61 f 0.10*
+TOP
STR+UGR+MET 0.62 f 0.10* 0.61 f 0.10*
</table>
<tableCaption confidence="0.988853">
Table 2: Performance evaluation of the generic features
for predicting peer-review helpfulness. Significant results
are marked by * (p &lt; 0.05).
</tableCaption>
<subsectionHeader confidence="0.944894">
4.2 Analysis of the Specialized Features
</subsectionHeader>
<bodyText confidence="0.801439">
Evaluation of the specialized features is shown in
Table 3, where all features examined are signifi-
</bodyText>
<footnote confidence="0.8827885">
4The best performing single feature type reported (Kim et
al., 2006) was review unigrams: r = 0.398 and r3 = 0.593.
</footnote>
<bodyText confidence="0.999956725">
cantly correlated with both helpfulness rating and
ranking. When evaluated in isolation, although
specialized features have weaker correlation coeffi-
cients ([0.43, 0.51]) than the best generic features,
these differences are not significant, and the special-
ized features have the potential advantage of being
theory-based. The use of features related to mean-
ingful dimensions of writing has contributed to va-
lidity and greater acceptability in the related area of
automated essay scoring (Attali and Burstein, 2006).
When combined with some generic features, the
specialized features improve the model’s perfor-
mance in terms of both r and rs compared to
the best performance in Section 4.1 (the baseline).
Though the improvement is not significant yet, we
think it still interesting to investigate the potential
trend to understand how specialized features cap-
ture additional information of peer-review helpful-
ness. Therefore, the following analysis is also pre-
sented (based on the absolute mean values), where
we start from the baseline feature set, and gradually
expand it by adding our new specialized features:
1) We first replace the raw lexical unigram features
(UGR) with lexical category features (LEX2), which
slightly improves the performance before rounding
to the significant digits shown in row 5. Note that
the categories not only substantially abstract lexical
information from the reviews, but also carry simple
syntactic and semantic information. 2) We then add
one semantic class – topic words (row 6), which en-
hances the performance further. Semantic features
did not help when working with generic lexical fea-
tures in Section 4.1 (second to last row in Table 2),
but they can be successfully combined with the lexi-
cal category features and further improve the perfor-
mance as indicated here. 3) When cognitive-science
and localization features are introduced, the predic-
tion becomes even more accurate, which reaches a
Pearson correlation of 0.67 and a Spearman correla-
tion of 0.67 (Table 3, last row).
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999835">
Despite the difference between peer reviews and
other types of reviews as discussed in Section 2,
our work demonstrates that many generic linguistic
features are also effective in predicting peer-review
helpfulness. The model’s performance can be alter-
</bodyText>
<page confidence="0.993934">
505
</page>
<table confidence="0.999781692307692">
Features Pearson r Spearman r$
cogS 0.43 f 0.09 0.46 f 0.07
LEX2 0.51 f 0.11 0.50 f 0.10
LOC 0.45 f 0.13 0.47 f 0.11
STR+MET+UGR 0.62 f 0.10 0.61 f 0.10
(Baseline)
STR+MET+LEX2 0.62 f 0.10 0.61 f 0.09
STR+MET+LEX2+ 0.65 f 0.10 0.66 f 0.08
TOP
STR+MET+LEX2+ 0.66 f 0.09 0.66 f 0.08
TOP+cogS
STR+MET+LEX2+ 0.67 f 0.09 0.67 f 0.08
TOP+cogS+LOC
</table>
<tableCaption confidence="0.9859905">
Table 3: Evaluation of the model’s performance (all sig-
nificant) after introducing the specialized features.
</tableCaption>
<bodyText confidence="0.999975392857143">
natively achieved and further improved by adding
auxiliary features tailored to peer reviews. These
specialized features not only introduce domain ex-
pertise, but also capture linguistic information at an
abstracted level, which can help avoid the risk of
over-fitting. Given only 267 peer reviews in our
case compared to more than ten thousand product
reviews (Kim et al., 2006), this is an important con-
sideration.
Though our absolute quantitative results are
not directly comparable to the results of Kim et
al. (2006), we indirectly compared them by ana-
lyzing the utility of features in isolation and com-
bined. While STR+UGR+MET is found as the best
combination of generic features for both types of
reviews, the best individual feature type is differ-
ent (review unigrams work best for product reviews;
structural features work best for peer reviews). More
importantly, meta-data, which are found to signif-
icantly affect the perceived helpfulness of product
reviews (Kim et al., 2006; Danescu-Niculescu-Mizil
et al., 2009), have no predictive power for peer re-
views. Perhaps because the paper grades and other
helpfulness ratings are not visible to the reviewers,
we have less of a social dimension for predicting
the helpfulness of peer reviews. We also found that
SVM regression does not favor ranking over predict-
ing helpfulness as in (Kim et al., 2006).
</bodyText>
<sectionHeader confidence="0.993682" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999993411764706">
The contribution of our work is three-fold: 1) Our
work successfully demonstrates that techniques used
in predicting product review helpfulness ranking can
be effectively adapted to the domain of peer reviews,
with minor modifications to the semantic and meta-
data features. 2) Our qualitative comparison shows
that the utility of generic features (e.g. meta-data
features) in predicting review helpfulness varies be-
tween different review types. 3) We further show
that prediction performance could be improved by
incorporating specialized features that capture help-
fulness information specific to peer reviews.
In the future, we would like to replace the man-
ually coded peer-review specialized features (cogS)
with their automatic predictions, since we have al-
ready shown in our prior work that some impor-
tant cognitive-science constructs can be successfully
identified automatically.5 Also, it is interesting to
observe that the average helpfulness ratings assigned
by experts (used as the gold standard in this study)
differ from those given by students. Prior work on
this corpus has already shown that feedback fea-
tures of review comments differ not only between
students and experts, but also between the writing
and the content experts (Patchan et al., 2009). While
Patchan et al. (2009) focused on the review com-
ments, we hypothesize that there is also a difference
in perceived peer-review helpfulness. Therefore, we
are planning to investigate the impact of these dif-
ferent helpfulness ratings on the utilities of features
used in modeling peer-review helpfulness. Finally,
we would like to integrate our helpfulness model
into a web-based peer-review system to improve the
quality of both peer reviews and paper revisions.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999831875">
This work was supported by the Learning Research
and Development Center at the University of Pitts-
burgh. We thank Melissa Patchan and Christian D.
Schunn for generously providing the manually an-
notated peer-review corpus. We are also grateful to
Christian D. Schunn, Janyce Wiebe, Joanna Drum-
mond, and Michael Lipschultz who kindly gave us
valuable feedback while writing this paper.
</bodyText>
<footnote confidence="0.933787333333333">
5The accuracy rate is 0.79 for predicting feedbackType, 0.78
for problem localization, and 0.81 for solution on the same his-
tory data set.
</footnote>
<page confidence="0.996193">
506
</page>
<sectionHeader confidence="0.985847" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.961710527472528">
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. In Michael Russell, editor,
The Journal of Technology, Learning and Assessment
(JTLA), volume 4, February.
Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded writing and rewriting in the discipline: A web-
based reciprocal peer review system. In Computers
and Education, volume 48, pages 409–426.
Kwangsu Cho. 2008. Machine classification of peer
comments in physics. In Proceedings of the First In-
ternational Conference on Educational Data Mining
(EDM2008), pages 192–196.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinber g, and Lillian Lee. 2009. How opin-
ions are received by online communities: A case study
on Amazon.com helpfulness votes. In Proceedings of
WWW, pages 141–150.
Raquel M. Crespo Garcia. 2010. Exploring document
clustering techniques for personalized peer assessment
in exploratory courses. In Proceedings of Computer-
Supported Peer Review in Education (CSPRED) Work-
shop in the Tenth International Conference on Intelli-
gent Tutoring Systems (ITS 2010).
Anindya Ghose and Panagiotis G. Ipeirotis. 2010. Esti-
mating the helpfulness and economic impact of prod-
uct reviews: Mining text and reviewer characteristics.
In IEEE Transactions on Knowledge and Data Engi-
neering, volume 99, Los Alamitos, CA, USA. IEEE
Computer Society.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Sch¨olkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press, Cambridge, MA,
USA.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco
Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP2006), pages 423–430, Sydney, Australia,
July.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th conference on
Computational linguistics, volume 1 of COLING ’00,
pages 495–501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu.
2008. Modeling and predicting the helpfulness of on-
line reviews. In Proceedings of the Eighth IEEE Inter-
national Conference on Data Mining, pages 443–452,
Los Alamitos, CA, USA.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ’05,
pages 91–98, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. In Instructional Sci-
ence, volume 37, pages 375–401.
Melissa M. Patchan, Davida Charney, and Christian D.
Schunn. 2009. A validation study of students’ end
comments: Comparing comments by students, a writ-
ing instructor, and a content instructor. In Journal of
Writing Research, volume 1, pages 124–152. Univer-
sity of Antwerp.
Agnes Sandor and Angela Vorndran. 2009. Detect-
ing key sentences for automatic assistance in peer-
reviewing research articles in educational sciences. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP), pages 36–44.
Oren Tsur and Ari Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most helpful
book reviews. In Proceedings of the Third Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM2009), pages 36–44.
Wenting Xiong and Diane J. Litman. 2010. Identifying
problem localization in peer-review feedback. In Pro-
ceedings of Tenth International Conference on Intelli-
gent Tutoring Systems (ITS2010), volume 6095, pages
429–431.
Wenting Xiong, Diane J. Litman, and Christian D.
Schunn. 2010. Assessing reviewers performance
based on mining problem localization in peer-review
data. In Proceedings of the Third International Con-
ference on Educational Data Mining (EDM2010),
pages 211–220.
</reference>
<page confidence="0.99711">
507
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.094028">
<title confidence="0.998001">Automatically Predicting Peer-Review Helpfulness</title>
<author confidence="0.72691">Wenting</author>
<affiliation confidence="0.831998666666667">University of Department of Computer Pittsburgh, PA,</affiliation>
<email confidence="0.997513">wex12@cs.pitt.edu</email>
<affiliation confidence="0.8035032">Diane University of Department of Computer Science Learning Research and Development Pittsburgh, PA,</affiliation>
<email confidence="0.999788">litman@cs.pitt.edu</email>
<abstract confidence="0.998763277777778">Identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. As a first step towards enhancing existing peerreview systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. In addition, we investigate the utility of incorporating additional specialized features tailored to peer review. Our preliminary results show that the structural features, review unigrams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e-rater v.2.</title>
<date>2006</date>
<booktitle>The Journal of Technology, Learning and Assessment (JTLA),</booktitle>
<volume>4</volume>
<editor>In Michael Russell, editor,</editor>
<contexts>
<context position="14763" citStr="Attali and Burstein, 2006" startWordPosition="2303" endWordPosition="2306">best performing single feature type reported (Kim et al., 2006) was review unigrams: r = 0.398 and r3 = 0.593. cantly correlated with both helpfulness rating and ranking. When evaluated in isolation, although specialized features have weaker correlation coefficients ([0.43, 0.51]) than the best generic features, these differences are not significant, and the specialized features have the potential advantage of being theory-based. The use of features related to meaningful dimensions of writing has contributed to validity and greater acceptability in the related area of automated essay scoring (Attali and Burstein, 2006). When combined with some generic features, the specialized features improve the model’s performance in terms of both r and rs compared to the best performance in Section 4.1 (the baseline). Though the improvement is not significant yet, we think it still interesting to investigate the potential trend to understand how specialized features capture additional information of peer-review helpfulness. Therefore, the following analysis is also presented (based on the absolute mean values), where we start from the baseline feature set, and gradually expand it by adding our new specialized features: </context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater v.2. In Michael Russell, editor, The Journal of Technology, Learning and Assessment (JTLA), volume 4, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kwangsu Cho</author>
<author>Christian D Schunn</author>
</authors>
<title>Scaffolded writing and rewriting in the discipline: A webbased reciprocal peer review system.</title>
<date>2007</date>
<booktitle>In Computers and Education,</booktitle>
<volume>48</volume>
<pages>409--426</pages>
<contexts>
<context position="5619" citStr="Cho and Schunn, 2007" startWordPosition="851" endWordPosition="854">nto account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews. To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues. 3 Data and Features In this study, we use a previously annotated peerreview corpus (Nelson and Schunn, 2009; Patchan et al., 2009), collected using a freely available webbased peer-review system (Cho and Schunn, 2007) in an introductory college history class. The corpus consists of 16 papers (about six pages each) and 267 reviews (varying from twenty words to about two hundred words). Two experts (a writing instructor and a content instructor) (Patchan et al., 2009) were asked to rate the helpfulness of each peer review on a scale from one to five (Pearson correlation r = 0.425, p &lt; 0.01). For our study, we consider the average ratings given by the two experts (which roughly follow a normal distribution) as the gold standard of review helpfulness. Two example rated peer reviews (shown verbatim) follow: A h</context>
</contexts>
<marker>Cho, Schunn, 2007</marker>
<rawString>Kwangsu Cho and Christian D. Schunn. 2007. Scaffolded writing and rewriting in the discipline: A webbased reciprocal peer review system. In Computers and Education, volume 48, pages 409–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kwangsu Cho</author>
</authors>
<title>Machine classification of peer comments in physics.</title>
<date>2008</date>
<booktitle>In Proceedings of the First International Conference on Educational Data Mining (EDM2008),</booktitle>
<pages>192--196</pages>
<contexts>
<context position="2460" citStr="Cho, 2008" startWordPosition="361" endWordPosition="362">ically mined from peer reviews and students’ papers, plus specialized features based on existing knowledge about peer reviews. We not only demonstrate that prior techniques from product reviews can be successfully tailored to peer reviews, but also show the importance of peer-review specific features. 2 Related Work Prior studies of peer review in the Natural Language Processing field have not focused on helpfulness prediction, but instead have been concerned with issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, given some similarity between peer reviews and other review types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu </context>
</contexts>
<marker>Cho, 2008</marker>
<rawString>Kwangsu Cho. 2008. Machine classification of peer comments in physics. In Proceedings of the First International Conference on Educational Data Mining (EDM2008), pages 192–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Gueorgi Kossinets</author>
<author>Jon Kleinber g</author>
<author>Lillian Lee</author>
</authors>
<title>How opinions are received by online communities: A case study on Amazon.com helpfulness votes.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="18014" citStr="Danescu-Niculescu-Mizil et al., 2009" startWordPosition="2821" endWordPosition="2824">an important consideration. Though our absolute quantitative results are not directly comparable to the results of Kim et al. (2006), we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews (Kim et al., 2006; Danescu-Niculescu-Mizil et al., 2009), have no predictive power for peer reviews. Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predicting helpfulness as in (Kim et al., 2006). 6 Conclusions and Future Work The contribution of our work is three-fold: 1) Our work successfully demonstrates that techniques used in predicting product review helpfulness ranking can be effectively adapted to the domain of peer reviews, with minor modifications</context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Kossinets, g, Lee, 2009</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets, Jon Kleinber g, and Lillian Lee. 2009. How opinions are received by online communities: A case study on Amazon.com helpfulness votes. In Proceedings of WWW, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel M Crespo Garcia</author>
</authors>
<title>Exploring document clustering techniques for personalized peer assessment in exploratory courses.</title>
<date>2010</date>
<booktitle>In Proceedings of ComputerSupported Peer Review in Education (CSPRED) Workshop in the Tenth International Conference on Intelligent Tutoring Systems (ITS</booktitle>
<contexts>
<context position="2537" citStr="Garcia, 2010" startWordPosition="372" endWordPosition="373">tures based on existing knowledge about peer reviews. We not only demonstrate that prior techniques from product reviews can be successfully tailored to peer reviews, but also show the importance of peer-review specific features. 2 Related Work Prior studies of peer review in the Natural Language Processing field have not focused on helpfulness prediction, but instead have been concerned with issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, given some similarity between peer reviews and other review types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ </context>
</contexts>
<marker>Garcia, 2010</marker>
<rawString>Raquel M. Crespo Garcia. 2010. Exploring document clustering techniques for personalized peer assessment in exploratory courses. In Proceedings of ComputerSupported Peer Review in Education (CSPRED) Workshop in the Tenth International Conference on Intelligent Tutoring Systems (ITS 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anindya Ghose</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics.</title>
<date>2010</date>
<journal>In IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>99</volume>
<publisher>IEEE Computer Society.</publisher>
<location>Los Alamitos, CA, USA.</location>
<contexts>
<context position="2898" citStr="Ghose and Ipeirotis (2010)" startWordPosition="430" endWordPosition="433">rediction, but instead have been concerned with issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, given some similarity between peer reviews and other review types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to select the most helpful book reviews in an unsupervised fashion based on review lexicons. However, studies of Amazon’s product reviews also show that the per502 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:</context>
</contexts>
<marker>Ghose, Ipeirotis, 2010</marker>
<rawString>Anindya Ghose and Panagiotis G. Ipeirotis. 2010. Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics. In IEEE Transactions on Knowledge and Data Engineering, volume 99, Los Alamitos, CA, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="11108" citStr="Joachims, 1999" startWordPosition="1725" endWordPosition="1726">ture space size as discussed below. Localization features (LOC): Five features developed in our prior work (Xiong and Litman, 2010) for automatically identifying the manually coded problem localization tags, such as the percentage of problems in reviews that could be matched with a localization pattern (e.g. “on page 5”, “the section about”), the percentage of sentences in which topic words exist between the subject and object, etc. 4 Experiment and Results Following Kim et al. (2006), we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM&amp;quot;ght (Joachims, 1999). We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we then examine them together in various combinations to find the most useful feature set for modeling peer-review helpfulness. Performance is evaluated in 10-fold cross validation of our 267 peer reviews by predicting the absolute helpfulness scores (with Pearson correlation coefficient r) as well as by predicting helpfulness ranking (with Spearman rank correlation coefficient rs). Although predicted helpfulness ranking could be directly used to compare the helpfulness of a given </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Patrick Pantel</author>
<author>Tim Chklovski</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Automatically assessing review helpfulness.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006),</booktitle>
<pages>423--430</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2752" citStr="Kim et al. (2006)" startWordPosition="407" endWordPosition="410">specific features. 2 Related Work Prior studies of peer review in the Natural Language Processing field have not focused on helpfulness prediction, but instead have been concerned with issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, given some similarity between peer reviews and other review types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to select the most helpful book reviews in an unsupervised fashion based on review lexicons. However, stud</context>
<context position="4351" citStr="Kim et al., 2006" startWordPosition="648" endWordPosition="651"> with question marks, number of exclamatory sentences. Lexical UGR, BGR tf-idf statistics of review unigrams and bigrams. Syntactic SYN percentage of tokens that are nouns, verbs, verbs conjugated in the first person, adjectives / adverbs and open classes, respectively. Semantic TOP, counts of topic words, posW, negW counts of positive and negative sentiment words. Meta-data MET the overall ratings of papers assigned by reviewers, and the absolute difference between the rating and the average score given by all reviewers. Table 1: Generic features motivated by related work of product reviews (Kim et al., 2006). ceived helpfulness of a review depends not only on its review content, but also on social effects such as product qualities, and individual bias in the presence of mixed opinion distribution (Danescu-NiculescuMizil et al., 2009). Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of “helpful” votes (Kim et al., 2006)); 2) Peer reviews frequently refer to the related student</context>
<context position="7147" citStr="Kim et al. (2006)" startWordPosition="1115" endWordPosition="1118">fighting or problems once this amendment was added? ... The arguments were sorted up into paragraphs, keeping the area of interest clear, but be careful about bringing up new things at the end and then simply leaving them there without elaboration (ie black sterilization at the end of the paragraph). An unhelpful peer review of average-rating 1: Your paper and its main points are easy to find and to follow. As shown in Table 1, we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by Kim et al. (2006). While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.1 1We used MSTParser (McDonald et al., 2005) for syntactic analysis. Topic words are automatically extracted from all stu503 In addition, the following specialized features are motivated by an empirical study in cognitive science (Nelson and Schunn, 2009), which suggests that </context>
<context position="10982" citStr="Kim et al. (2006)" startWordPosition="1702" endWordPosition="1705">odel learned using a Bag-of-Words representation of the peer reviews. These categories will also be helpful for reducing the feature space size as discussed below. Localization features (LOC): Five features developed in our prior work (Xiong and Litman, 2010) for automatically identifying the manually coded problem localization tags, such as the percentage of problems in reviews that could be matched with a localization pattern (e.g. “on page 5”, “the section about”), the percentage of sentences in which topic words exist between the subject and object, etc. 4 Experiment and Results Following Kim et al. (2006), we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM&amp;quot;ght (Joachims, 1999). We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we then examine them together in various combinations to find the most useful feature set for modeling peer-review helpfulness. Performance is evaluated in 10-fold cross validation of our 267 peer reviews by predicting the absolute helpfulness scores (with Pearson correlation coefficient r) as well as by predicting helpfulness ranking (with Spearman rank cor</context>
<context position="12683" citStr="Kim et al., 2006" startWordPosition="1957" endWordPosition="1960">n of the generic features is presented in Table 2, showing that all classes except syntactic (SYN) and meta-data (MET) features are sig504 nificantly correlated with both helpfulness rating (r) and helpfulness ranking (rs). Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant). Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews (Kim et al., 2006) where product scores are significantly correlated with product-review helpfulness. However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r pz� rs consistently for our peer reviews, while Kim et al. (2006) reported r &lt; rs for product reviews.4 Finally, we observed a similar feature redundancy effect as Kim et al. (2006) did, in that simply combining all features does not improve the model’s performance. Interestingly, our best feature combination (last row) is the sam</context>
<context position="14200" citStr="Kim et al., 2006" startWordPosition="2218" endWordPosition="2221"> 0.36 f 0.12 0.35 f 0.11 TOP 0.55 f 0.10* 0.54 f 0.10* posW 0.57 f 0.13* 0.53 f 0.12* negW 0.49 f 0.11* 0.46 f 0.10* MET 0.22 f 0.15 0.23 f 0.12 All-combined 0.56 f 0.07* 0.58 f 0.09* STR+UGR+MET 0.61 f 0.10* 0.61 f 0.10* +TOP STR+UGR+MET 0.62 f 0.10* 0.61 f 0.10* Table 2: Performance evaluation of the generic features for predicting peer-review helpfulness. Significant results are marked by * (p &lt; 0.05). 4.2 Analysis of the Specialized Features Evaluation of the specialized features is shown in Table 3, where all features examined are signifi4The best performing single feature type reported (Kim et al., 2006) was review unigrams: r = 0.398 and r3 = 0.593. cantly correlated with both helpfulness rating and ranking. When evaluated in isolation, although specialized features have weaker correlation coefficients ([0.43, 0.51]) than the best generic features, these differences are not significant, and the specialized features have the potential advantage of being theory-based. The use of features related to meaningful dimensions of writing has contributed to validity and greater acceptability in the related area of automated essay scoring (Attali and Burstein, 2006). When combined with some generic fea</context>
<context position="17367" citStr="Kim et al., 2006" startWordPosition="2720" endWordPosition="2723">X2+ 0.65 f 0.10 0.66 f 0.08 TOP STR+MET+LEX2+ 0.66 f 0.09 0.66 f 0.08 TOP+cogS STR+MET+LEX2+ 0.67 f 0.09 0.67 f 0.08 TOP+cogS+LOC Table 3: Evaluation of the model’s performance (all significant) after introducing the specialized features. natively achieved and further improved by adding auxiliary features tailored to peer reviews. These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting. Given only 267 peer reviews in our case compared to more than ten thousand product reviews (Kim et al., 2006), this is an important consideration. Though our absolute quantitative results are not directly comparable to the results of Kim et al. (2006), we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews (Kim et a</context>
</contexts>
<marker>Kim, Pantel, Chklovski, Pennacchiotti, 2006</marker>
<rawString>Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pennacchiotti. 2006. Automatically assessing review helpfulness. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006), pages 423–430, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<volume>1</volume>
<pages>495--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9646" citStr="Lin and Hovy, 2000" startWordPosition="1498" endWordPosition="1501">nto two idea units (each presented in a separate paragraph). As both ideas are coded as problem with the presence of problem localization and solution, the cognitive-science features of this review are praise%=0, problem%=1, summary%=0, localization%=1, and solution%=1. Lexical category features (LEX2): Ten categories of keyword lexicons developed for automatically detecting the previously manually annotated feedback types (Xiong et al., 2010). The categories are learned in a semi-supervised way based on syntactic and semantic functions, such as suggestion dents’ papers using topic signature (Lin and Hovy, 2000) software kindly provided by Annie Louis. Positive and negative sentiment words are extracted from the General Inquirer Dictionaries (http://www.wjh.harvard.edu/ inquirer/homecat.htm). 2Details of different granularity levels of annotation can be found in (Nelson and Schunn, 2009). 3These annotators are not the same experts who rated the peer-review helpfulness. modal verbs (e.g. should, must, might, could, need), negations (e.g. not, don’t, doesn’t), positive and negative words, and so on. We first manually created a list of words that were specified as signal words for annotating feedbackTyp</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th conference on Computational linguistics, volume 1 of COLING ’00, pages 495–501, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Xiangji Guang</author>
<author>Aijun An</author>
<author>Xiaohui Yu</author>
</authors>
<title>Modeling and predicting the helpfulness of online reviews.</title>
<date>2008</date>
<booktitle>In Proceedings of the Eighth IEEE International Conference on Data Mining,</booktitle>
<pages>443--452</pages>
<location>Los Alamitos, CA, USA.</location>
<contexts>
<context position="3073" citStr="Liu et al., 2008" startWordPosition="457" endWordPosition="460">2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, given some similarity between peer reviews and other review types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to select the most helpful book reviews in an unsupervised fashion based on review lexicons. However, studies of Amazon’s product reviews also show that the per502 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 502–507, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Class Label Features Structural STR review length in terms of </context>
</contexts>
<marker>Liu, Guang, An, Yu, 2008</marker>
<rawString>Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu. 2008. Modeling and predicting the helpfulness of online reviews. In Proceedings of the Eighth IEEE International Conference on Data Mining, pages 443–452, Los Alamitos, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7514" citStr="McDonald et al., 2005" startWordPosition="1174" endWordPosition="1177"> its main points are easy to find and to follow. As shown in Table 1, we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by Kim et al. (2006). While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.1 1We used MSTParser (McDonald et al., 2005) for syntactic analysis. Topic words are automatically extracted from all stu503 In addition, the following specialized features are motivated by an empirical study in cognitive science (Nelson and Schunn, 2009), which suggests that students’ revision likelihood is significantly correlated with certain feedback features, and by our prior work (Xiong and Litman, 2010; Xiong et al., 2010) for detecting these cognitive science constructs automatically: Cognitive-science features (cogS): For a given review, cognitive-science constructs that are significantly correlated with review implementation l</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 91–98, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melissa M Nelson</author>
<author>Christian D Schunn</author>
</authors>
<title>The nature of feedback: how different types of peer feedback affect writing performance.</title>
<date>2009</date>
<journal>In Instructional Science,</journal>
<volume>37</volume>
<pages>375--401</pages>
<contexts>
<context position="1417" citStr="Nelson and Schunn, 2009" startWordPosition="196" endWordPosition="199">ew. Our preliminary results show that the structural features, review unigrams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction. 1 Introduction Peer reviewing of student writing has been widely used in various academic fields. While existing web-based peer-review systems largely save instructors effort in setting up peer-review assignments and managing document assignment, there still remains the problem that the quality of peer reviews is often poor (Nelson and Schunn, 2009). Thus to enhance the effectiveness of existing peer-review systems, we propose to automatically predict the helpfulness of peer reviews. In this paper, we examine prior techniques that have been used to successfully rank helpfulness for product reviews, and adapt them to the peer-review domain. In particular, we use an SVM regression algorithm to predict the helpfulness of peer reviews based on generic linguistic features automatically mined from peer reviews and students’ papers, plus specialized features based on existing knowledge about peer reviews. We not only demonstrate that prior tech</context>
<context position="5509" citStr="Nelson and Schunn, 2009" startWordPosition="834" endWordPosition="837">, 2006)); 2) Peer reviews frequently refer to the related students’ papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews. To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues. 3 Data and Features In this study, we use a previously annotated peerreview corpus (Nelson and Schunn, 2009; Patchan et al., 2009), collected using a freely available webbased peer-review system (Cho and Schunn, 2007) in an introductory college history class. The corpus consists of 16 papers (about six pages each) and 267 reviews (varying from twenty words to about two hundred words). Two experts (a writing instructor and a content instructor) (Patchan et al., 2009) were asked to rate the helpfulness of each peer review on a scale from one to five (Pearson correlation r = 0.425, p &lt; 0.01). For our study, we consider the average ratings given by the two experts (which roughly follow a normal distrib</context>
<context position="7725" citStr="Nelson and Schunn, 2009" startWordPosition="1206" endWordPosition="1209">ate the feature sets used by Kim et al. (2006). While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.1 1We used MSTParser (McDonald et al., 2005) for syntactic analysis. Topic words are automatically extracted from all stu503 In addition, the following specialized features are motivated by an empirical study in cognitive science (Nelson and Schunn, 2009), which suggests that students’ revision likelihood is significantly correlated with certain feedback features, and by our prior work (Xiong and Litman, 2010; Xiong et al., 2010) for detecting these cognitive science constructs automatically: Cognitive-science features (cogS): For a given review, cognitive-science constructs that are significantly correlated with review implementation likelihood are manually coded for each idea unit (Nelson and Schunn, 2009) within the review. Note, however, that peer-review helpfulness is rated for the whole review, which can include multiple idea units.2 The</context>
<context position="9927" citStr="Nelson and Schunn, 2009" startWordPosition="1535" endWordPosition="1538">al category features (LEX2): Ten categories of keyword lexicons developed for automatically detecting the previously manually annotated feedback types (Xiong et al., 2010). The categories are learned in a semi-supervised way based on syntactic and semantic functions, such as suggestion dents’ papers using topic signature (Lin and Hovy, 2000) software kindly provided by Annie Louis. Positive and negative sentiment words are extracted from the General Inquirer Dictionaries (http://www.wjh.harvard.edu/ inquirer/homecat.htm). 2Details of different granularity levels of annotation can be found in (Nelson and Schunn, 2009). 3These annotators are not the same experts who rated the peer-review helpfulness. modal verbs (e.g. should, must, might, could, need), negations (e.g. not, don’t, doesn’t), positive and negative words, and so on. We first manually created a list of words that were specified as signal words for annotating feedbackType and problem localization in the coding manual; then we supplemented the list with words selected by a decision tree model learned using a Bag-of-Words representation of the peer reviews. These categories will also be helpful for reducing the feature space size as discussed below</context>
</contexts>
<marker>Nelson, Schunn, 2009</marker>
<rawString>Melissa M. Nelson and Christian D. Schunn. 2009. The nature of feedback: how different types of peer feedback affect writing performance. In Instructional Science, volume 37, pages 375–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melissa M Patchan</author>
<author>Davida Charney</author>
<author>Christian D Schunn</author>
</authors>
<title>A validation study of students’ end comments: Comparing comments by students, a writing instructor, and a content instructor.</title>
<date>2009</date>
<journal>In Journal of Writing Research,</journal>
<volume>1</volume>
<pages>124--152</pages>
<institution>University of Antwerp.</institution>
<contexts>
<context position="5532" citStr="Patchan et al., 2009" startWordPosition="838" endWordPosition="841"> frequently refer to the related students’ papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews. To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues. 3 Data and Features In this study, we use a previously annotated peerreview corpus (Nelson and Schunn, 2009; Patchan et al., 2009), collected using a freely available webbased peer-review system (Cho and Schunn, 2007) in an introductory college history class. The corpus consists of 16 papers (about six pages each) and 267 reviews (varying from twenty words to about two hundred words). Two experts (a writing instructor and a content instructor) (Patchan et al., 2009) were asked to rate the helpfulness of each peer review on a scale from one to five (Pearson correlation r = 0.425, p &lt; 0.01). For our study, we consider the average ratings given by the two experts (which roughly follow a normal distribution) as the gold stan</context>
<context position="19644" citStr="Patchan et al., 2009" startWordPosition="3075" endWordPosition="3078">replace the manually coded peer-review specialized features (cogS) with their automatic predictions, since we have already shown in our prior work that some important cognitive-science constructs can be successfully identified automatically.5 Also, it is interesting to observe that the average helpfulness ratings assigned by experts (used as the gold standard in this study) differ from those given by students. Prior work on this corpus has already shown that feedback features of review comments differ not only between students and experts, but also between the writing and the content experts (Patchan et al., 2009). While Patchan et al. (2009) focused on the review comments, we hypothesize that there is also a difference in perceived peer-review helpfulness. Therefore, we are planning to investigate the impact of these different helpfulness ratings on the utilities of features used in modeling peer-review helpfulness. Finally, we would like to integrate our helpfulness model into a web-based peer-review system to improve the quality of both peer reviews and paper revisions. Acknowledgements This work was supported by the Learning Research and Development Center at the University of Pittsburgh. We thank </context>
</contexts>
<marker>Patchan, Charney, Schunn, 2009</marker>
<rawString>Melissa M. Patchan, Davida Charney, and Christian D. Schunn. 2009. A validation study of students’ end comments: Comparing comments by students, a writing instructor, and a content instructor. In Journal of Writing Research, volume 1, pages 124–152. University of Antwerp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agnes Sandor</author>
<author>Angela Vorndran</author>
</authors>
<title>Detecting key sentences for automatic assistance in peerreviewing research articles in educational sciences.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP),</booktitle>
<pages>36--44</pages>
<contexts>
<context position="2399" citStr="Sandor and Vorndran, 2009" startWordPosition="350" endWordPosition="353">t the helpfulness of peer reviews based on generic linguistic features automatically mined from peer reviews and students’ papers, plus specialized features based on existing knowledge about peer reviews. We not only demonstrate that prior techniques from product reviews can be successfully tailored to peer reviews, but also show the importance of peer-review specific features. 2 Related Work Prior studies of peer review in the Natural Language Processing field have not focused on helpfulness prediction, but instead have been concerned with issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, given some similarity between peer reviews and other review types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested</context>
</contexts>
<marker>Sandor, Vorndran, 2009</marker>
<rawString>Agnes Sandor and Angela Vorndran. 2009. Detecting key sentences for automatic assistance in peerreviewing research articles in educational sciences. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP), pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Revrank: A fully unsupervised algorithm for selecting the most helpful book reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third International AAAI Conference on Weblogs and Social Media (ICWSM2009),</booktitle>
<pages>36--44</pages>
<contexts>
<context position="3228" citStr="Tsur and Rappoport (2009)" startWordPosition="481" endWordPosition="484">view types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to select the most helpful book reviews in an unsupervised fashion based on review lexicons. However, studies of Amazon’s product reviews also show that the per502 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 502–507, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Class Label Features Structural STR review length in terms of tokens, number of sentences, percentage of sentences that end with question marks, number of exclamatory sentences. Lexical UGR, BGR tf-idf statistics of r</context>
</contexts>
<marker>Tsur, Rappoport, 2009</marker>
<rawString>Oren Tsur and Ari Rappoport. 2009. Revrank: A fully unsupervised algorithm for selecting the most helpful book reviews. In Proceedings of the Third International AAAI Conference on Weblogs and Social Media (ICWSM2009), pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenting Xiong</author>
<author>Diane J Litman</author>
</authors>
<title>Identifying problem localization in peer-review feedback.</title>
<date>2010</date>
<booktitle>In Proceedings of Tenth International Conference on Intelligent Tutoring Systems (ITS2010),</booktitle>
<volume>6095</volume>
<pages>429--431</pages>
<contexts>
<context position="2485" citStr="Xiong and Litman, 2010" startWordPosition="363" endWordPosition="366">d from peer reviews and students’ papers, plus specialized features based on existing knowledge about peer reviews. We not only demonstrate that prior techniques from product reviews can be successfully tailored to peer reviews, but also show the importance of peer-review specific features. 2 Related Work Prior studies of peer review in the Natural Language Processing field have not focused on helpfulness prediction, but instead have been concerned with issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, given some similarity between peer reviews and other review types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie re</context>
<context position="7882" citStr="Xiong and Litman, 2010" startWordPosition="1229" endWordPosition="1232">apt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.1 1We used MSTParser (McDonald et al., 2005) for syntactic analysis. Topic words are automatically extracted from all stu503 In addition, the following specialized features are motivated by an empirical study in cognitive science (Nelson and Schunn, 2009), which suggests that students’ revision likelihood is significantly correlated with certain feedback features, and by our prior work (Xiong and Litman, 2010; Xiong et al., 2010) for detecting these cognitive science constructs automatically: Cognitive-science features (cogS): For a given review, cognitive-science constructs that are significantly correlated with review implementation likelihood are manually coded for each idea unit (Nelson and Schunn, 2009) within the review. Note, however, that peer-review helpfulness is rated for the whole review, which can include multiple idea units.2 Therefore in our study, we calculate the distribution of feedbackType values (praise, problem, and summary) (kappa = .92), the percentage of problems that have </context>
<context position="10624" citStr="Xiong and Litman, 2010" startWordPosition="1646" endWordPosition="1649">pfulness. modal verbs (e.g. should, must, might, could, need), negations (e.g. not, don’t, doesn’t), positive and negative words, and so on. We first manually created a list of words that were specified as signal words for annotating feedbackType and problem localization in the coding manual; then we supplemented the list with words selected by a decision tree model learned using a Bag-of-Words representation of the peer reviews. These categories will also be helpful for reducing the feature space size as discussed below. Localization features (LOC): Five features developed in our prior work (Xiong and Litman, 2010) for automatically identifying the manually coded problem localization tags, such as the percentage of problems in reviews that could be matched with a localization pattern (e.g. “on page 5”, “the section about”), the percentage of sentences in which topic words exist between the subject and object, etc. 4 Experiment and Results Following Kim et al. (2006), we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM&amp;quot;ght (Joachims, 1999). We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we</context>
</contexts>
<marker>Xiong, Litman, 2010</marker>
<rawString>Wenting Xiong and Diane J. Litman. 2010. Identifying problem localization in peer-review feedback. In Proceedings of Tenth International Conference on Intelligent Tutoring Systems (ITS2010), volume 6095, pages 429–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenting Xiong</author>
<author>Diane J Litman</author>
<author>Christian D Schunn</author>
</authors>
<title>Assessing reviewers performance based on mining problem localization in peer-review data.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third International Conference on Educational Data Mining (EDM2010),</booktitle>
<pages>211--220</pages>
<contexts>
<context position="7903" citStr="Xiong et al., 2010" startWordPosition="1233" endWordPosition="1236">a-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.1 1We used MSTParser (McDonald et al., 2005) for syntactic analysis. Topic words are automatically extracted from all stu503 In addition, the following specialized features are motivated by an empirical study in cognitive science (Nelson and Schunn, 2009), which suggests that students’ revision likelihood is significantly correlated with certain feedback features, and by our prior work (Xiong and Litman, 2010; Xiong et al., 2010) for detecting these cognitive science constructs automatically: Cognitive-science features (cogS): For a given review, cognitive-science constructs that are significantly correlated with review implementation likelihood are manually coded for each idea unit (Nelson and Schunn, 2009) within the review. Note, however, that peer-review helpfulness is rated for the whole review, which can include multiple idea units.2 Therefore in our study, we calculate the distribution of feedbackType values (praise, problem, and summary) (kappa = .92), the percentage of problems that have problem localization </context>
<context position="9474" citStr="Xiong et al., 2010" startWordPosition="1471" endWordPosition="1474"> subset of the corpus for evaluating the reliability of human annotations3. Consider the example of the helpful review presented in Section 3 which was manually separated into two idea units (each presented in a separate paragraph). As both ideas are coded as problem with the presence of problem localization and solution, the cognitive-science features of this review are praise%=0, problem%=1, summary%=0, localization%=1, and solution%=1. Lexical category features (LEX2): Ten categories of keyword lexicons developed for automatically detecting the previously manually annotated feedback types (Xiong et al., 2010). The categories are learned in a semi-supervised way based on syntactic and semantic functions, such as suggestion dents’ papers using topic signature (Lin and Hovy, 2000) software kindly provided by Annie Louis. Positive and negative sentiment words are extracted from the General Inquirer Dictionaries (http://www.wjh.harvard.edu/ inquirer/homecat.htm). 2Details of different granularity levels of annotation can be found in (Nelson and Schunn, 2009). 3These annotators are not the same experts who rated the peer-review helpfulness. modal verbs (e.g. should, must, might, could, need), negations </context>
</contexts>
<marker>Xiong, Litman, Schunn, 2010</marker>
<rawString>Wenting Xiong, Diane J. Litman, and Christian D. Schunn. 2010. Assessing reviewers performance based on mining problem localization in peer-review data. In Proceedings of the Third International Conference on Educational Data Mining (EDM2010), pages 211–220.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>