<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001136">
<title confidence="0.9963775">
When Specialists and Generalists Work Together: Overcoming Domain
Dependence in Sentiment Tagging
</title>
<author confidence="0.992021">
Alina Andreevskaia
</author>
<affiliation confidence="0.8509265">
Concordia University
Montreal, Quebec
</affiliation>
<email confidence="0.989263">
andreev@cs.concordia.ca
</email>
<author confidence="0.990953">
Sabine Bergler
</author>
<affiliation confidence="0.801285">
Concordia University
Montreal, Canada
</affiliation>
<email confidence="0.995727">
bergler@cs.concordia.ca
</email>
<sectionHeader confidence="0.995612" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999779117647059">
This study presents a novel approach to the
problem of system portability across differ-
ent domains: a sentiment annotation system
that integrates a corpus-based classifier trained
on a small set of annotated in-domain data
and a lexicon-based system trained on Word-
Net. The paper explores the challenges of sys-
tem portability across domains and text gen-
res (movie reviews, news, blogs, and product
reviews), highlights the factors affecting sys-
tem performance on out-of-domain and small-
set in-domain data, and presents a new sys-
tem consisting of the ensemble of two classi-
fiers with precision-based vote weighting, that
provides significant gains in accuracy and re-
call over the corpus-based classifier and the
lexicon-based system taken individually.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999680625">
One of the emerging directions in NLP is the de-
velopment of machine learning methods that per-
form well not only on the domain on which they
were trained, but also on other domains, for which
training data is not available or is not sufficient to
ensure adequate machine learning. Many applica-
tions require reliable processing of heterogeneous
corpora, such as the World Wide Web, where the
diversity of genres and domains present in the Inter-
net limits the feasibility of in-domain training. In
this paper, sentiment annotation is defined as the
assignment of positive, negative or neutral senti-
ment values to texts, sentences, and other linguistic
units. Recent experiments assessing system porta-
bility across different domains, conducted by Aue
and Gamon (2005), demonstrated that sentiment an-
notation classifiers trained in one domain do not per-
form well on other domains. A number of methods
has been proposed in order to overcome this system
portability limitation by using out-of-domain data,
unlabelled in-domain corpora or a combination of
in-domain and out-of-domain examples (Aue and
Gamon, 2005; Bai et al., 2005; Drezde et al., 2007;
Tan et al., 2007).
In this paper, we present a novel approach to the
problem of system portability across different do-
mains by developing a sentiment annotation sys-
tem that integrates a corpus-based classifier with
a lexicon-based system trained on WordNet. By
adopting this approach, we sought to develop a
system that relies on both general and domain-
specific knowledge, as humans do when analyzing
a text. The information contained in lexicographi-
cal sources, such as WordNet, reflects a lay person’s
general knowledge about the world, while domain-
specific knowledge can be acquired through classi-
fier training on a small set of in-domain data.
The first part of this paper reviews the extant lit-
erature on domain adaptation in sentiment analy-
sis and highlights promising directions for research.
The second part establishes a baseline for system
evaluation by drawing comparisons of system per-
formance across four different domains/genres -
movie reviews, news, blogs, and product reviews.
The final, third part of the paper presents our sys-
tem, composed of an ensemble of two classifiers –
one trained on WordNet glosses and synsets and the
other trained on a small in-domain training set.
</bodyText>
<page confidence="0.957832">
290
</page>
<note confidence="0.704919">
Proceedings of ACL-08: HLT, pages 290–298,
</note>
<page confidence="0.392755">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<sectionHeader confidence="0.8814275" genericHeader="introduction">
2 Domain Adaptation in Sentiment
Research
</sectionHeader>
<bodyText confidence="0.99998817721519">
Most text-level sentiment classifiers use standard
machine learning techniques to learn and select fea-
tures from labeled corpora. Such approaches work
well in situations where large labeled corpora are
available for training and validation (e.g., movie re-
views), but they do not perform well when training
data is scarce or when it comes from a different do-
main (Aue and Gamon, 2005; Read, 2005), topic
(Read, 2005) or time period (Read, 2005). There are
two alternatives to supervised machine learning that
can be used to get around this problem: on the one
hand, general lists of sentiment clues/features can be
acquired from domain-independent sources such as
dictionaries or the Internet, on the other hand, unsu-
pervised and weakly-supervised approaches can be
used to take advantage of a small number of anno-
tated in-domain examples and/or of unlabelled in-
domain data.
The first approach, using general word lists au-
tomatically acquired from the Internet or from dic-
tionaries, outperforms corpus-based classifiers when
such classifiers use out-of-domain training data or
when the training corpus is not sufficiently large to
accumulate the necessary feature frequency infor-
mation. But such general word lists were shown to
perform worse than statistical models built on suf-
ficiently large in-domain training sets of movie re-
views (Pang et al., 2002). On other domains, such
as product reviews, the performance of systems that
use general word lists is comparable to the perfor-
mance of supervised machine learning approaches
(Gamon and Aue, 2005).
The recognition of major performance deficien-
cies of supervised machine learning methods with
insufficient or out-of-domain training brought about
an increased interest in unsupervised and weakly-
supervised approaches to feature learning. For in-
stance, Aue and Gamon (2005) proposed training
on a samll number of labeled examples and large
quantities of unlabelled in-domain data. This sys-
tem performed well even when compared to sys-
tems trained on a large set of in-domain examples:
on feedback messages from a web survey on knowl-
edge bases, Aue and Gamon report 73.86% accu-
racy using unlabelled data compared to 77.34% for
in-domain and 72.39% for the best out-of-domain
training on a large training set.
Drezde et al. (2007) applied structural corre-
spondence learning (Drezde et al., 2007) to the task
of domain adaptation for sentiment classification of
product reviews. They showed that, depending on
the domain, a small number (e.g., 50) of labeled
examples allows to adapt the model learned on an-
other corpus to a new domain. However, they note
that the success of such adaptation and the num-
ber of necessary in-domain examples depends on
the similarity between the original domain and the
new one. Similarly, Tan et al. (2007) suggested to
combine out-of-domain labeled examples with unla-
belled ones from the target domain in order to solve
the domain-transfer problem. They applied an out-
of-domain-trained SVM classifier to label examples
from the target domain and then retrained the classi-
fier using these new examples. In order to maximize
the utility of the examples from the target domain,
these examples were selected using Similarity Rank-
ing and Relative Similarity Ranking algorithms (Tan
et al., 2007). Depending on the similarity between
domains, this method brought up to 15% gain com-
pared to the baseline SVM.
Overall, the development of semi-supervised ap-
proaches to sentiment tagging is a promising direc-
tion of the research in this area but so far, based
on reported results, the performance of such meth-
ods is inferior to the supervised approaches with in-
domain training and to the methods that use general
word lists. It also strongly depends on the similarity
between the domains as has been shown by (Drezde
et al., 2007; Tan et al., 2007).
</bodyText>
<sectionHeader confidence="0.910087" genericHeader="method">
3 Factors Affecting System Performance
</sectionHeader>
<bodyText confidence="0.999344545454545">
The comparison of system performance across dif-
ferent domains involves a number of factors that can
significantly affect system performance – from train-
ing set size to level of analysis (sentence or entire
document), document domain/genre and many other
factors. In this section we present a series of experi-
ments conducted to assess the effects of different ex-
ternal factors (i.e., factors unrelated to the merits of
the system itself) on system performance in order to
establish the baseline for performance comparisons
across different domains/genres.
</bodyText>
<page confidence="0.994558">
291
</page>
<subsectionHeader confidence="0.999578">
3.1 Level of Analysis
</subsectionHeader>
<bodyText confidence="0.999898666666667">
Research on sentiment annotation is usually con-
ducted at the text (Aue and Gamon, 2005; Pang et
al., 2002; Pang and Lee, 2004; Riloff et al., 2006;
Turney, 2002; Turney and Littman, 2003) or at the
sentence levels (Gamon and Aue, 2005; Hu and Liu,
2004; Kim and Hovy, 2005; Riloff et al., 2006). It
should be noted that each of these levels presents dif-
ferent challenges for sentiment annotation. For ex-
ample, it has been observed that texts often contain
multiple opinions on different topics (Turney, 2002;
Wiebe et al., 2001), which makes assignment of the
overall sentiment to the whole document problem-
atic. On the other hand, each individual sentence
contains a limited number of sentiment clues, which
often negatively affects the accuracy and recall if
that single sentiment clue encountered in the sen-
tence was not learned by the system.
Since the comparison of sentiment annotation
system performance on texts and on sentences
has not been attempted to date, we also sought
to close this gap in the literature by conducting
the first set of our comparative experiments on
data sets of 2,002 movie review texts and 10,662
movie review snippets (5331 with positive and
5331 with negative sentiment) provided by Bo Pang
(http://www.cs.cornell.edu/People/pabo/movie-
review-data/).
</bodyText>
<subsectionHeader confidence="0.999036">
3.2 Domain Effects
</subsectionHeader>
<bodyText confidence="0.9999475">
The second set of our experiments explores system
performance on different domains at sentence level.
For this we used four different data sets of sentences
annotated with sentiment tags:
</bodyText>
<listItem confidence="0.981267583333333">
• A set of movie review snippets (further: movie)
from (Pang and Lee, 2005). This dataset of
10,662 snippets was collected automatically
from www.rottentomatoes.com website. All
sentences in reviews marked “rotten” were con-
sidered negative and snippets from “fresh” re-
views were deemed positive. In order to make
the results obtained on this dataset comparable
to other domains, a randomly selected subset of
1066 snippets was used in the experiments.
• A balanced corpus of 800 manually annotated
sentences extracted from 83 newspaper texts
</listItem>
<bodyText confidence="0.999683153846154">
(further, news). The full set of sentences
was annotated by one judge. 200 sentences
from this corpus (100 positive and 100 neg-
ative) were also randomly selected from the
corpus for an inter-annotator agreement study
and were manually annotated by two indepen-
dent annotators. The pairwise agreement be-
tween annotators was calculated as the percent
of same tags divided by the number of sen-
tences with this tag in the gold standard. The
pair-wise agreement between the three anno-
tators ranged from 92.5 to 95.9% (r.=0.74 and
0.75 respectively) on positive vs. negative tags.
</bodyText>
<listItem confidence="0.5060443125">
• A set of sentences taken from personal
weblogs (further, blogs) posted on Live-
Journal (http://www.livejournal.com) and on
http://www.cyberjournalist.com. This corpus
is composed of 800 sentences (400 sentences
with positive and 400 sentences with negative
sentiment). In order to establish the inter-
annotator agreement, two independent judges
were asked to annotate 200 sentences from this
corpus. The agreement between the two an-
notators on positive vs. negative tags reached
99% (r.=0.97).
• A set of 1200 product review (PR) sentences
extracted from the annotated corpus made
available by Bing Liu (Hu and Liu, 2004)
(http://www.cs.uic.edu/ liub/FBS/FBS.html).
</listItem>
<bodyText confidence="0.527114">
The data set sizes are summarized in Table 1.
</bodyText>
<table confidence="0.96527575">
Movies News Blogs PR
Text level 2002 texts n/a n/a n/a
Sentence level 10662 800 800 1200
snippets sent. sent. sent.
</table>
<tableCaption confidence="0.999165">
Table 1: Datasets
</tableCaption>
<subsectionHeader confidence="0.99897">
3.3 Establishing a Baseline for a Corpus-based
System (CBS)
</subsectionHeader>
<bodyText confidence="0.999907833333333">
Supervised statistical methods have been very suc-
cessful in sentiment tagging of texts: on movie re-
view texts they reach accuracies of 85-90% (Aue
and Gamon, 2005; Pang and Lee, 2004). These
methods perform particularly well when a large vol-
ume of labeled data from the same domain as the
</bodyText>
<page confidence="0.981534">
292
</page>
<bodyText confidence="0.999861413043478">
test set is available for training (Aue and Gamon,
2005). For this reason, most of the research on senti-
ment tagging using statistical classifiers was limited
to product and movie reviews, where review authors
usually indicate their sentiment in a form of a stan-
dardized score that accompanies the texts of their re-
views.
The lack of sufficient data for training appears to
be the main reason for the virtual absence of exper-
iments with statistical classifiers in sentiment tag-
ging at the sentence level. To our knowledge, the
only work that describes the application of statis-
tical classifiers (SVM) to sentence-level sentiment
classification is (Gamon and Aue, 2005)1. The av-
erage performance of the system on ternary clas-
sification (positive, negative, and neutral) was be-
tween 0.50 and 0.52 for both average precision and
recall. The results reported by (Riloff et al., 2006)
for binary classification of sentences in a related
domain of subjectivity tagging (i.e., the separation
of sentiment-laden from neutral sentences) suggest
that statistical classifiers can perform well on this
task: the authors have reached 74.9% accuracy on
the MPQA corpus (Riloff et al., 2006).
In order to explore the performance of dif-
ferent approaches in sentiment annotation at the
text and sentence levels, we used a basic Naive
Bayes classifier. It has been shown that both
Naive Bayes and SVMs perform with similar ac-
curacy on different sentiment tagging tasks (Pang
and Lee, 2004). These observations were con-
firmed with our own experiments with SVMs and
Naive Bayes (Table 3). We used the Weka pack-
age (http://www.cs.waikato.ac.nz/ml/weka/) with
default settings.
In the sections that follow, we describe a set
of comparative experiments with SVMs and Naive
Bayes classifiers (1) on texts and sentences and (2)
on four different domains (movie reviews, news,
blogs, and product reviews). System runs with un-
igrams, bigrams, and trigrams as features and with
different training set sizes are presented.
1Recently, a similar task has been addressed by the Affective
Text Task at SemEval-1 where even shorter units – headlines
– were classified into positive, negative and neutral categories
using a variety of techniques (Strapparava and Mihalcea, 2007).
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999793">
4.1 System Performance on Texts vs. Sentences
</subsectionHeader>
<bodyText confidence="0.962944">
The experiments comparing in-domain trained sys-
tem performance on texts vs. sentences were con-
ducted on 2,002 movie review texts and on 10,662
movie review snippets. The results with 10-fold
cross-validation are reported in Table 22.
</bodyText>
<table confidence="0.9992915">
Trained on Texts Trained on Sent.
Tested on Tested on Tested on Tested on
Texts Sent. Texts Sent.
1gram 81.1 69.0 66.8 77.4
2gram 83.7 68.6 71.2 73.9
3gram 82.5 64.1 70.0 65.4
</table>
<tableCaption confidence="0.999638">
Table 2: Accuracy of Naive Bayes on movie reviews.
</tableCaption>
<bodyText confidence="0.999444086956522">
Consistent with findings in the literature (Cui et
al., 2006; Dave et al., 2003; Gamon and Aue, 2005),
on the large corpus of movie review texts, the in-
domain-trained system based solely on unigrams
had lower accuracy than the similar system trained
on bigrams. But the trigrams fared slightly worse
than bigrams. On sentences, however, we have ob-
served an inverse pattern: unigrams performed bet-
ter than bigrams and trigrams. These results high-
light a special property of sentence-level annota-
tion: greater sensitivity to sparseness of the model:
On texts, classifier error on one particular sentiment
marker is often compensated by a number of cor-
rectly identified other sentiment clues. Since sen-
tences usually contain a much smaller number of
sentiment clues than texts, sentence-level annota-
tion more readily yields errors when a single sen-
timent clue is incorrectly identified or missed by
the system. Due to lower frequency of higher-order
n-grams (as opposed to unigrams), higher-order n-
gram language models are more sparse, which in-
creases the probability of missing a particular sen-
timent marker in a sentence (Table 33). Very large
</bodyText>
<footnote confidence="0.99928">
2All results are statistically significant at α = 0.01 with two
exceptions: the difference between trigrams and bigrams for the
system trained and tested on texts is statistically significant at
alpha=0.1 and for the system trained on sentences and tested on
texts is not statistically significant at α = 0.01.
3The results for movie reviews are lower than those reported
in Table 2 since the dataset is 10 times smaller, which results
in less accurate classification. The statistical significance of the
</footnote>
<page confidence="0.988339">
293
</page>
<table confidence="0.9933376875">
training sets are required to overcome this higher n-
gram sparseness in sentence-level annotation.
Dataset Movie News Blogs PRs
Dataset size 1066 800 800 1200
unigrams
SVM 68.5 61.5 63.85 76.9
NB 60.2 59.5 60.5 74.25
nb features 5410 4544 3615 2832
bigrams
SVM 59.9 63.2 61.5 75.9
NB 57.0 58.4 59.5 67.8
nb features 16286 14633 15182 12951
trigrams
SVM 54.3 55.4 52.7 64.4
NB 53.3 57.0 56.0 69.7
nb features 20837 18738 19847 19132
</table>
<tableCaption confidence="0.996374">
Table 3: Accuracy of unigram, bigram and trigram mod-
els across domains.
</tableCaption>
<subsectionHeader confidence="0.985232">
4.2 System Performance on Different Domains
</subsectionHeader>
<bodyText confidence="0.99945225">
In the second set of experiments we sought to com-
pare system results on sentences using in-domain
and out-of-domain training. Table 4 shows that in-
domain training, as expected, consistently yields su-
perior accuracy than out-of-domain training across
all four datasets: movie reviews (Movies), news,
blogs, and product reviews (PRs). The numbers for
in-domain trained runs are highlighted in bold.
</bodyText>
<table confidence="0.9998135">
Training Data Test Data
Movies News Blogs PRs
Movies 68.5 55.2 53.2 60.7
News 55.0 61.5 56.25 57.4
Blogs 53.7 49.9 63.85 58.8
PRs 55.8 55.9 56.25 76.9
</table>
<tableCaption confidence="0.999852">
Table 4: Accuracy of SVM with unigram model
</tableCaption>
<bodyText confidence="0.99636105882353">
results depends on the genre and size of the n-gram: on prod-
uct reviews, all results are statistically significant at α = 0.025
level; on movie reviews, the difference between Na¨ve Bayes
and SVM is statistically significant at α = 0.01 but the signif-
icance diminishes as the size of the n-gram increases; on news,
only bi-grams produce a statistically significant (α = 0.01) dif-
ference between the two machine learning methods, while on
blogs the difference between SVMs and Na¨ve Bayes is most
pronounced when unigrams are used (α = 0.025).
It is interesting to note that on sentences, regard-
less of the domain used in system training and re-
gardless of the domain used in system testing, un-
igrams tend to perform better than higher-order n-
grams. This observation suggests that, given the
constraints on the size of the available training sets,
unigram-based systems may be better suited for
sentence-level sentiment annotation.
</bodyText>
<sectionHeader confidence="0.996586" genericHeader="method">
5 Lexicon-Based Approach
</sectionHeader>
<bodyText confidence="0.999950405405405">
The search for a base-learner that can produce great-
est synergies with a classifier trained on small-set
in-domain data has turned our attention to lexicon-
based systems. Since the benefits from combining
classifiers that always make similar decisions is min-
imal, the two (or more) base-learners should com-
plement each other (Alpaydin, 2004). Since a sys-
tem based on a fairly different learning approach
is more likely to produce a different decision un-
der a given set of circumstances, the diversity of
approaches integrated in the ensemble of classifiers
was expected to have a beneficial effect on the over-
all system performance.
A lexicon-based approach capitalizes on the
fact that dictionaries, such as WordNet (Fell-
baum, 1998), contain a comprehensive and domain-
independent set of sentiment clues that exist in
general English. A system trained on such gen-
eral data, therefore, should be less sensitive to do-
main changes. This robustness, however is expected
to come at some cost, since some domain-specific
sentiment clues may not be covered in the dictio-
nary. Our hypothesis was, therefore, that a lexicon-
based system will perform worse than an in-domain
trained classifier but possibly better than a classifier
trained on out-of domain data.
One of the limitations of general lexicons and
dictionaries, such as WordNet (Fellbaum, 1998), as
training sets for sentiment tagging systems is that
they contain only definitions of individual words
and, hence, only unigrams could be effectively
learned from dictionary entries. Since the struc-
ture of WordNet glosses is fairly different from
that of other types of corpora, we developed a sys-
tem that used the list of human-annotated adjec-
tives from (Hatzivassiloglou and McKeown, 1997)
as a seed list and then learned additional unigrams
</bodyText>
<page confidence="0.995132">
294
</page>
<bodyText confidence="0.999945972972973">
from WordNet synsets and glosses with up to 88%
accuracy, when evaluated against General Inquirer
(Stone et al., 1966) (GI) on the intersection of our
automatically acquired list with GI. In order to ex-
pand the list coverage for our experiments at the text
and sentence levels, we then augmented the list by
adding to it all the words annotated with “Positiv”
or “Negativ” tags in GI, that were not picked up by
the system. The resulting list of features contained
11,000 unigrams with the degree of membership in
the category of positive or negative sentiment as-
signed to each of them.
In order to assign the membership score to each
word, we did 58 system runs on unique non-
intersecting seed lists drawn from manually anno-
tated list of positive and negative adjectives from
(Hatzivassiloglou and McKeown, 1997). The 58
runs were then collapsed into a single set of 7,813
unique words. For each word we computed a score
by subtracting the total number of runs assigning
this word a negative sentiment from the total of the
runs that consider it positive. The resulting measure,
termed Net Overlap Score (NOS), reflected the num-
ber of ties linking a given word with other sentiment-
laden words in WordNet, and hence, could be used
as a measure of the words’ centrality in the fuzzy
category of sentiment. The NOSs were then normal-
ized into the interval from -1 to +1 using a sigmoid
fuzzy membership function (Zadeh, 1975)4. Only
words with fuzzy membership degree not equal to
zero were retained in the list. The resulting list
contained 10,809 sentiment-bearing words of differ-
ent parts of speech. The sentiment determination at
the sentence and text level was then done by sum-
ming up the scores of all identified positive unigrams
(NOS&gt;0) and all negative unigrams (NOS&lt;0) (An-
dreevskaia and Bergler, 2006).
</bodyText>
<subsectionHeader confidence="0.997333">
5.1 Establishing a Baseline for the
Lexicon-Based System (LBS)
</subsectionHeader>
<bodyText confidence="0.987450833333333">
The baseline performance of the Lexicon-Based
System (LBS) described above is presented in Ta-
ble 5, along with the performance results of the in-
domain- and out-of-domain-trained SVM classifier.
Table 5 confirms the predicted pattern: the
LBS performs with lower accuracy than in-domain-
</bodyText>
<footnote confidence="0.651039">
4With coefficients: α=1, 7=15.
</footnote>
<table confidence="0.99977525">
Movies News Blogs PRs
LBS 57.5 62.3 63.3 59.3
SVM in-dom. 68.5 61.5 63.85 76.9
SVM out-of-dom. 55.8 55.9 56.25 60.7
</table>
<tableCaption confidence="0.999551">
Table 5: System accuracy on best runs on sentences
</tableCaption>
<bodyText confidence="0.9994828">
trained corpus-based classifiers, and with similar
or better accuracy than the corpus-based classifiers
trained on out-of-domain data. Thus, the lexicon-
based approach is characterized by a bounded but
stable performance when the system is ported across
domains. These performance characteristics of
corpus-based and lexicon-based approaches prompt
further investigation into the possibility to combine
the portability of dictionary-trained systems with the
accuracy of in-domain trained systems.
</bodyText>
<sectionHeader confidence="0.987745" genericHeader="method">
6 Integrating the Corpus-based and
Dictionary-based Approaches
</sectionHeader>
<bodyText confidence="0.999972407407407">
The strategy of integration of two or more sys-
tems in a single ensemble of classifiers has been
actively used on different tasks within NLP. In sen-
timent tagging and related areas, Aue and Gamon
(2005) demonstrated that combining classifiers can
be a valuable tool in domain adaptation for senti-
ment analysis. In the ensemble of classifiers, they
used a combination of nine SVM-based classifiers
deployed to learn unigrams, bigrams, and trigrams
on three different domains, while the fourth domain
was used as an evaluation set. Using then an SVM
meta-classifier trained on a small number of target
domain examples to combine the nine base clas-
sifiers, they obtained a statistically significant im-
provement on out-of-domain texts from book re-
views, knowledge-base feedback, and product sup-
port services survey data. No improvement occurred
on movie reviews.
Pang and Lee (2004) applied two different clas-
sifiers to perform sentiment annotation in two se-
quential steps: the first classifier separated subjec-
tive (sentiment-laden) texts from objective (neutral)
ones and then they used the second classifier to clas-
sify the subjective texts into positive and negative.
Das and Chen (2004) used five classifiers to deter-
mine market sentiment on Yahoo! postings. Simple
majority vote was applied to make decisions within
</bodyText>
<page confidence="0.995525">
295
</page>
<bodyText confidence="0.9986705">
the ensemble of classifiers and achieved accuracy of
62% on ternary in-domain classification.
In this study we describe a system that attempts to
combine the portability of a dictionary-trained sys-
tem (LBS) with the accuracy of an in-domain trained
corpus-based system (CBS). The selection of these
two classifiers for this system, thus, was theory-
based. The section that follows describes the classi-
fier integration and presents the performance results
of the system consisting of an ensemble CBS and
LBS classifier and a precision-based vote weighting
procedure.
</bodyText>
<subsectionHeader confidence="0.999326">
6.1 The Classifier Integration Procedure and
System Evaluation
</subsectionHeader>
<bodyText confidence="0.9999559375">
The comparative analysis of the corpus-based and
lexicon-based systems described above revealed that
the errors produced by CBS and LBS were to a
great extent complementary (i.e., where one classi-
fier makes an error, the other tends to give the cor-
rect answer). This provided further justification to
the integration of corpus-based and lexicon-based
approaches in a single system.
Table 6 below illustrates the complementarity of
the performance CBS and LBS classifiers on the
positive and negative categories. In this experiment,
the corpus-based classifier was trained on 400 an-
notated product review sentences5. The two systems
were then evaluated on a test set of another 400 prod-
uct review sentences. The results reported in Table 6
are statistically significant at α = 0.01.
</bodyText>
<table confidence="0.99810475">
CBS LBS
Precision positives 89.3% 69.3%
Precision negatives 55.5% 81.5%
Pos/Neg Precision 58.0% 72.1%
</table>
<tableCaption confidence="0.9978285">
Table 6: Base-learners’ precision and recall on product
reviews on test data.
</tableCaption>
<bodyText confidence="0.7211306">
Table 6 shows that the corpus-based system has a
very good precision on those sentences that it classi-
fies as positive but makes a lot of errors on those sen-
tences that it deems negative. At the same time, the
lexicon-based system has low precision on positives
</bodyText>
<footnote confidence="0.892009">
5The small training set explains relatively low overall per-
formance of the CBS system.
</footnote>
<bodyText confidence="0.999822976190477">
and high precision on negatives6. Such complemen-
tary distribution of errors produced by the two sys-
tems was observed on different data sets from differ-
ent domains, which suggests that the observed dis-
tribution pattern reflects the properties of each of
the classifiers, rather than the specifics of the do-
main/genre.
In order to take advantage of the observed com-
plementarity of the two systems, the following pro-
cedure was used. First, a small set of in-domain
data was used to train the CBS system. Then both
CBS and LBS systems were run separately on the
same training set, and for each classifier, the preci-
sion measures were calculated separately for those
sentences that the classifier considered positive and
those it considered negative. The chance-level per-
formance (50%) was then subtracted from the pre-
cision figures to ensure that the final weights reflect
by how much the classifier’s precision exceeds the
chance level. The resulting chance-adjusted preci-
sion numbers of the two classifiers were then nor-
malized, so that the weights of CBS and LBS clas-
sifiers sum up to 100% on positive and to 100% on
negative sentences. These weights were then used
to adjust the contribution of each classifier to the de-
cision of the ensemble system. The choice of the
weight applied to the classifier decision, thus, varied
depending on whether the classifier scored a given
sentence as positive or as negative. The resulting
system was then tested on a separate test set of sen-
tences7. The small-set training and evaluation exper-
iments with the system were performed on different
domains using 3-fold validation.
The experiments conducted with the Ensemble
system were designed to explore system perfor-
mance under conditions of limited availability of an-
notated data for classifier training. For this reason,
the numbers reported for the corpus-based classifier
do not reflect the full potential of machine learn-
ing approaches when sufficient in-domain training
data is available. Table 7 presents the results of
these experiments by domain/genre. The results
</bodyText>
<footnote confidence="0.998846833333333">
6These results are consistent with an observation in
(Kennedy and Inkpen, 2006), where a lexicon-based system
performed with a better precision on negative than on positive
texts.
7The size of the test set varied in different experiments due
to the availability of annotated data for a particular domain.
</footnote>
<page confidence="0.996994">
296
</page>
<bodyText confidence="0.99244275">
are statistically significant at α = 0.01, except the
runs on movie reviews where the difference between
the LBS and Ensemble classifiers was significant at
α = 0.05.
</bodyText>
<table confidence="0.998426636363636">
LBS CBS Ensemble
News Acc 67.8 53.2 73.3
F 0.82 0.71 0.85
Movies Acc 54.5 53.5 62.1
F 0.73 0.72 0.77
Blogs Acc 61.2 51.1 70.9
F 0.78 0.69 0.83
PRs Acc 59.5 58.9 78.0
F 0.77 0.75 0.88
Average Acc 60.7 54.2 71.1
F 0.77 0.72 0.83
</table>
<tableCaption confidence="0.909329">
Table 7: Performance of the ensemble classifier
Table 7 shows that the combination of two classi-
</tableCaption>
<bodyText confidence="0.991061384615385">
fiers into an ensemble using the weighting technique
described above leads to consistent improvement in
system performance across all domains/genres. In
the ensemble system, the average gain in accuracy
across the four domains was 16.9% relative to CBS
and 10.3% relative to LBS. Moreover, the gain in
accuracy and precision was not offset by decreases
in recall: the net gain in recall was 7.4% relative to
CBS and 13.5% vs. LBS. The ensemble system on
average reached 99.1% recall. The F-measure has
increased from 0.77 and 0.72 for LBS and CBS clas-
sifiers respectively to 0.83 for the whole ensemble
system.
</bodyText>
<sectionHeader confidence="0.999528" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999931703703704">
The development of domain-independent sentiment
determination systems poses a substantial challenge
for researchers in NLP and artificial intelligence.
The results presented in this study suggest that the
integration of two fairly different classifier learning
approaches in a single ensemble of classifiers can
yield substantial gains in system performance on all
measures. The most substantial gains occurred in
recall, accuracy, and F-measure.
This study permits to highlight a set of factors
that enable substantial performance gains with the
ensemble of classifiers approach. Such gains are
most likely when (1) the errors made by the clas-
sifiers are complementary, i.e., where one classifier
makes an error, the other tends to give the correct
answer, (2) the classifier errors are not fully random
and occur more often in a certain segment (or cate-
gory) of classifier results, and (3) there is a way for
a system to identify that low-precision segment and
reduce the weights of that classifier’s results on that
segment accordingly. The two classifiers used in this
study – corpus-based and lexicon-based – provided
an interesting illustration of potential performance
gains associated with these three conditions. The
use of precision of classifier results on the positives
and negatives proved to be an effective technique for
classifier vote weighting within the ensemble.
</bodyText>
<sectionHeader confidence="0.998352" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999955580645161">
This study contributes to the research on sentiment
tagging, domain adaptation, and the development of
ensembles of classifiers (1) by proposing a novel ap-
proach for sentiment determination at sentence level
and delineating the conditions under which great-
est synergies among combined classifiers can be
achieved, (2) by describing a precision-based tech-
nique for assigning differential weights to classifier
results on different categories identified by the clas-
sifier (i.e., categories of positive vs. negative sen-
tences), and (3) by proposing a new method for sen-
timent annotation in situations where the annotated
in-domain data is scarce and insufficient to ensure
adequate performance of the corpus-based classifier,
which still remains the preferred choice when large
volumes of annotated data are available for system
training.
Among the most promising directions for future
research in the direction laid out in this paper is the
deployment of more advanced classifiers and fea-
ture selection techniques that can further enhance
the performance of the ensemble of classifiers. The
precision-based vote weighting technique may prove
to be effective also in situations, where more than
two classifiers are integrated into a single system.
We expect that these more advanced ensemble-of-
classifiers systems would inherit the benefits of mul-
tiple complementary approaches to sentiment anno-
tation and will be able to achieve better and more
stable accuracy on in-domain, as well as on out-of-
domain data.
</bodyText>
<page confidence="0.994552">
297
</page>
<sectionHeader confidence="0.990242" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999413320754717">
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. The MIT Press, Cambridge, MA.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for a fuzzy sentiment: Sentiment tag extrac-
tion from WordNet glosses. In Proceedings the 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, Trento, IT.
Anthony Aue and Michael Gamon. 2005. Customizing
sentiment classifiers to new domains: a case study. In
Proccedings of the International Conference on Recent
Advances in Natural Language Processing, Borovets,
BG.
Xue Bai, Rema Padman, and Edoardo Airoldi. 2005. On
learning parsimonious models for extracting consumer
opinions. In Proceedings of the 38th Annual Hawaii
International Conference on System Sciences, Wash-
ington, DC.
Hang Cui, Vibhu Mittal, and Mayur Datar. 2006. Com-
parative experiments on sentiment classification for
online product reviews. In Proceedings of the 21st
International Conference on Artificial Intelligence,
Boston, MA.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the Peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW03, Budapest, HU.
Mark Drezde, John Blitzer, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, Prague, CZ.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the ACL-05 Workshop on Feature Engineering
for Machine Learning in Natural Language Process-
ing, Ann Arbor, US.
Vasileios Hatzivassiloglou and Kathleen B. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the the 40th Annual Meeting
of the Association of Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD-04, pages 168–177.
Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment Classification of Movie Reviews Using Con-
textual Valence Shifters. Computational Intelligence,
22(2):110–125.
Soo-Min Kim and Eduard Hovy. 2005. Automatic detec-
tion of opinion bearing words and sentences. In Pro-
ceedings of the Second International Joint Conference
on Natural Language Processing, Companion Volume,
Jeju Island, KR.
Bo Pang and Lilian Lee. 2004. A sentiment education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the 43nd
Meeting of the Association for Computational Linguis-
tics, Ann Arbor, US.
Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Conference on Empiri-
cal Methods in Natural Language Processing.
Jonathon Read. 2005. Using emoticons to reduce depen-
dency in machine learning techniques for sentiment
classification. In Proceedings of the ACL-2005 Stu-
dent Research Workshop, Ann Arbor, MI.
Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, Sydney, AUS.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The General Inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics. M.I.T. Press, Cambridge, MA.
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text. In Proceedings of the
4th International Workshop on Semantic Evaluations,
Prague, CZ.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Zueqi
Cheng. 2007. A Novel Scheme for Domain-transfer
Problem in the context of Sentiment Analysis. In Pro-
ceedings of CIKM 2007.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: inference of semantic orientation
from association. ACM Transactions on Information
Systems (TOIS), 21:315–346.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting of the Association of Computational Linguis-
tics.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study of
Evaluative and Speculative Language. In Proceedings
of the 2nd ACL SIGDial Workshop on Discourse and
Dialogue, Aalberg, DK.
Lotfy A. Zadeh. 1975. Calculus of Fuzzy Restrictions.
In L.A. Zadeh et al., editor, Fuzzy Sets and their Ap-
plications to cognitive and decision processes, pages
1–40. Academic Press Inc., New-York.
</reference>
<page confidence="0.996881">
298
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883684">
<title confidence="0.99777">When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging</title>
<author confidence="0.998925">Alina Andreevskaia</author>
<affiliation confidence="0.999991">Concordia University</affiliation>
<address confidence="0.999433">Montreal, Quebec</address>
<email confidence="0.949968">andreev@cs.concordia.ca</email>
<author confidence="0.948858">Sabine Bergler</author>
<affiliation confidence="0.99997">Concordia University</affiliation>
<address confidence="0.998721">Montreal, Canada</address>
<email confidence="0.99857">bergler@cs.concordia.ca</email>
<abstract confidence="0.999206055555556">This study presents a novel approach to the problem of system portability across different domains: a sentiment annotation system that integrates a corpus-based classifier trained on a small set of annotated in-domain data and a lexicon-based system trained on Word- Net. The paper explores the challenges of system portability across domains and text genres (movie reviews, news, blogs, and product reviews), highlights the factors affecting system performance on out-of-domain and smallset in-domain data, and presents a new system consisting of the ensemble of two classifiers with precision-based vote weighting, that provides significant gains in accuracy and recall over the corpus-based classifier and the lexicon-based system taken individually.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ethem Alpaydin</author>
</authors>
<title>Introduction to Machine Learning.</title>
<date>2004</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="18643" citStr="Alpaydin, 2004" startWordPosition="2963" endWordPosition="2964"> testing, unigrams tend to perform better than higher-order ngrams. This observation suggests that, given the constraints on the size of the available training sets, unigram-based systems may be better suited for sentence-level sentiment annotation. 5 Lexicon-Based Approach The search for a base-learner that can produce greatest synergies with a classifier trained on small-set in-domain data has turned our attention to lexiconbased systems. Since the benefits from combining classifiers that always make similar decisions is minimal, the two (or more) base-learners should complement each other (Alpaydin, 2004). Since a system based on a fairly different learning approach is more likely to produce a different decision under a given set of circumstances, the diversity of approaches integrated in the ensemble of classifiers was expected to have a beneficial effect on the overall system performance. A lexicon-based approach capitalizes on the fact that dictionaries, such as WordNet (Fellbaum, 1998), contain a comprehensive and domainindependent set of sentiment clues that exist in general English. A system trained on such general data, therefore, should be less sensitive to domain changes. This robustn</context>
</contexts>
<marker>Alpaydin, 2004</marker>
<rawString>Ethem Alpaydin. 2004. Introduction to Machine Learning. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>Mining WordNet for a fuzzy sentiment: Sentiment tag extraction from WordNet glosses.</title>
<date>2006</date>
<booktitle>In Proceedings the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Trento, IT.</location>
<contexts>
<context position="21906" citStr="Andreevskaia and Bergler, 2006" startWordPosition="3502" endWordPosition="3506">imentladen words in WordNet, and hence, could be used as a measure of the words’ centrality in the fuzzy category of sentiment. The NOSs were then normalized into the interval from -1 to +1 using a sigmoid fuzzy membership function (Zadeh, 1975)4. Only words with fuzzy membership degree not equal to zero were retained in the list. The resulting list contained 10,809 sentiment-bearing words of different parts of speech. The sentiment determination at the sentence and text level was then done by summing up the scores of all identified positive unigrams (NOS&gt;0) and all negative unigrams (NOS&lt;0) (Andreevskaia and Bergler, 2006). 5.1 Establishing a Baseline for the Lexicon-Based System (LBS) The baseline performance of the Lexicon-Based System (LBS) described above is presented in Table 5, along with the performance results of the indomain- and out-of-domain-trained SVM classifier. Table 5 confirms the predicted pattern: the LBS performs with lower accuracy than in-domain4With coefficients: α=1, 7=15. Movies News Blogs PRs LBS 57.5 62.3 63.3 59.3 SVM in-dom. 68.5 61.5 63.85 76.9 SVM out-of-dom. 55.8 55.9 56.25 60.7 Table 5: System accuracy on best runs on sentences trained corpus-based classifiers, and with similar o</context>
</contexts>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2006. Mining WordNet for a fuzzy sentiment: Sentiment tag extraction from WordNet glosses. In Proceedings the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, IT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: a case study.</title>
<date>2005</date>
<booktitle>In Proccedings of the International Conference on Recent Advances in Natural Language Processing,</booktitle>
<location>Borovets, BG.</location>
<contexts>
<context position="1795" citStr="Aue and Gamon (2005)" startWordPosition="263" endWordPosition="266">e trained, but also on other domains, for which training data is not available or is not sufficient to ensure adequate machine learning. Many applications require reliable processing of heterogeneous corpora, such as the World Wide Web, where the diversity of genres and domains present in the Internet limits the feasibility of in-domain training. In this paper, sentiment annotation is defined as the assignment of positive, negative or neutral sentiment values to texts, sentences, and other linguistic units. Recent experiments assessing system portability across different domains, conducted by Aue and Gamon (2005), demonstrated that sentiment annotation classifiers trained in one domain do not perform well on other domains. A number of methods has been proposed in order to overcome this system portability limitation by using out-of-domain data, unlabelled in-domain corpora or a combination of in-domain and out-of-domain examples (Aue and Gamon, 2005; Bai et al., 2005; Drezde et al., 2007; Tan et al., 2007). In this paper, we present a novel approach to the problem of system portability across different domains by developing a sentiment annotation system that integrates a corpus-based classifier with a </context>
<context position="3925" citStr="Aue and Gamon, 2005" startWordPosition="600" endWordPosition="603">d synsets and the other trained on a small in-domain training set. 290 Proceedings of ACL-08: HLT, pages 290–298, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Domain Adaptation in Sentiment Research Most text-level sentiment classifiers use standard machine learning techniques to learn and select features from labeled corpora. Such approaches work well in situations where large labeled corpora are available for training and validation (e.g., movie reviews), but they do not perform well when training data is scarce or when it comes from a different domain (Aue and Gamon, 2005; Read, 2005), topic (Read, 2005) or time period (Read, 2005). There are two alternatives to supervised machine learning that can be used to get around this problem: on the one hand, general lists of sentiment clues/features can be acquired from domain-independent sources such as dictionaries or the Internet, on the other hand, unsupervised and weakly-supervised approaches can be used to take advantage of a small number of annotated in-domain examples and/or of unlabelled indomain data. The first approach, using general word lists automatically acquired from the Internet or from dictionaries, </context>
<context position="5367" citStr="Aue and Gamon (2005)" startWordPosition="820" endWordPosition="823">ord lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews (Pang et al., 2002). On other domains, such as product reviews, the performance of systems that use general word lists is comparable to the performance of supervised machine learning approaches (Gamon and Aue, 2005). The recognition of major performance deficiencies of supervised machine learning methods with insufficient or out-of-domain training brought about an increased interest in unsupervised and weaklysupervised approaches to feature learning. For instance, Aue and Gamon (2005) proposed training on a samll number of labeled examples and large quantities of unlabelled in-domain data. This system performed well even when compared to systems trained on a large set of in-domain examples: on feedback messages from a web survey on knowledge bases, Aue and Gamon report 73.86% accuracy using unlabelled data compared to 77.34% for in-domain and 72.39% for the best out-of-domain training on a large training set. Drezde et al. (2007) applied structural correspondence learning (Drezde et al., 2007) to the task of domain adaptation for sentiment classification of product reviews</context>
<context position="8069" citStr="Aue and Gamon, 2005" startWordPosition="1261" endWordPosition="1264">erent domains involves a number of factors that can significantly affect system performance – from training set size to level of analysis (sentence or entire document), document domain/genre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of </context>
<context position="11660" citStr="Aue and Gamon, 2005" startWordPosition="1829" endWordPosition="1832">ive vs. negative tags reached 99% (r.=0.97). • A set of 1200 product review (PR) sentences extracted from the annotated corpus made available by Bing Liu (Hu and Liu, 2004) (http://www.cs.uic.edu/ liub/FBS/FBS.html). The data set sizes are summarized in Table 1. Movies News Blogs PR Text level 2002 texts n/a n/a n/a Sentence level 10662 800 800 1200 snippets sent. sent. sent. Table 1: Datasets 3.3 Establishing a Baseline for a Corpus-based System (CBS) Supervised statistical methods have been very successful in sentiment tagging of texts: on movie review texts they reach accuracies of 85-90% (Aue and Gamon, 2005; Pang and Lee, 2004). These methods perform particularly well when a large volume of labeled data from the same domain as the 292 test set is available for training (Aue and Gamon, 2005). For this reason, most of the research on sentiment tagging using statistical classifiers was limited to product and movie reviews, where review authors usually indicate their sentiment in a form of a standardized score that accompanies the texts of their reviews. The lack of sufficient data for training appears to be the main reason for the virtual absence of experiments with statistical classifiers in senti</context>
<context position="23214" citStr="Aue and Gamon (2005)" startWordPosition="3697" endWordPosition="3700"> the lexiconbased approach is characterized by a bounded but stable performance when the system is ported across domains. These performance characteristics of corpus-based and lexicon-based approaches prompt further investigation into the possibility to combine the portability of dictionary-trained systems with the accuracy of in-domain trained systems. 6 Integrating the Corpus-based and Dictionary-based Approaches The strategy of integration of two or more systems in a single ensemble of classifiers has been actively used on different tasks within NLP. In sentiment tagging and related areas, Aue and Gamon (2005) demonstrated that combining classifiers can be a valuable tool in domain adaptation for sentiment analysis. In the ensemble of classifiers, they used a combination of nine SVM-based classifiers deployed to learn unigrams, bigrams, and trigrams on three different domains, while the fourth domain was used as an evaluation set. Using then an SVM meta-classifier trained on a small number of target domain examples to combine the nine base classifiers, they obtained a statistically significant improvement on out-of-domain texts from book reviews, knowledge-base feedback, and product support service</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Anthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: a case study. In Proccedings of the International Conference on Recent Advances in Natural Language Processing, Borovets, BG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xue Bai</author>
<author>Rema Padman</author>
<author>Edoardo Airoldi</author>
</authors>
<title>On learning parsimonious models for extracting consumer opinions.</title>
<date>2005</date>
<booktitle>In Proceedings of the 38th Annual Hawaii International Conference on System Sciences,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="2155" citStr="Bai et al., 2005" startWordPosition="319" endWordPosition="322">sentiment annotation is defined as the assignment of positive, negative or neutral sentiment values to texts, sentences, and other linguistic units. Recent experiments assessing system portability across different domains, conducted by Aue and Gamon (2005), demonstrated that sentiment annotation classifiers trained in one domain do not perform well on other domains. A number of methods has been proposed in order to overcome this system portability limitation by using out-of-domain data, unlabelled in-domain corpora or a combination of in-domain and out-of-domain examples (Aue and Gamon, 2005; Bai et al., 2005; Drezde et al., 2007; Tan et al., 2007). In this paper, we present a novel approach to the problem of system portability across different domains by developing a sentiment annotation system that integrates a corpus-based classifier with a lexicon-based system trained on WordNet. By adopting this approach, we sought to develop a system that relies on both general and domainspecific knowledge, as humans do when analyzing a text. The information contained in lexicographical sources, such as WordNet, reflects a lay person’s general knowledge about the world, while domainspecific knowledge can be </context>
</contexts>
<marker>Bai, Padman, Airoldi, 2005</marker>
<rawString>Xue Bai, Rema Padman, and Edoardo Airoldi. 2005. On learning parsimonious models for extracting consumer opinions. In Proceedings of the 38th Annual Hawaii International Conference on System Sciences, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Vibhu Mittal</author>
<author>Mayur Datar</author>
</authors>
<title>Comparative experiments on sentiment classification for online product reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Artificial Intelligence,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="14617" citStr="Cui et al., 2006" startWordPosition="2306" endWordPosition="2309"> (Strapparava and Mihalcea, 2007). 4 Experiments 4.1 System Performance on Texts vs. Sentences The experiments comparing in-domain trained system performance on texts vs. sentences were conducted on 2,002 movie review texts and on 10,662 movie review snippets. The results with 10-fold cross-validation are reported in Table 22. Trained on Texts Trained on Sent. Tested on Tested on Tested on Tested on Texts Sent. Texts Sent. 1gram 81.1 69.0 66.8 77.4 2gram 83.7 68.6 71.2 73.9 3gram 82.5 64.1 70.0 65.4 Table 2: Accuracy of Naive Bayes on movie reviews. Consistent with findings in the literature (Cui et al., 2006; Dave et al., 2003; Gamon and Aue, 2005), on the large corpus of movie review texts, the indomain-trained system based solely on unigrams had lower accuracy than the similar system trained on bigrams. But the trigrams fared slightly worse than bigrams. On sentences, however, we have observed an inverse pattern: unigrams performed better than bigrams and trigrams. These results highlight a special property of sentence-level annotation: greater sensitivity to sparseness of the model: On texts, classifier error on one particular sentiment marker is often compensated by a number of correctly iden</context>
</contexts>
<marker>Cui, Mittal, Datar, 2006</marker>
<rawString>Hang Cui, Vibhu Mittal, and Mayur Datar. 2006. Comparative experiments on sentiment classification for online product reviews. In Proceedings of the 21st International Conference on Artificial Intelligence, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the Peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW03,</booktitle>
<location>Budapest, HU.</location>
<contexts>
<context position="14636" citStr="Dave et al., 2003" startWordPosition="2310" endWordPosition="2313">Mihalcea, 2007). 4 Experiments 4.1 System Performance on Texts vs. Sentences The experiments comparing in-domain trained system performance on texts vs. sentences were conducted on 2,002 movie review texts and on 10,662 movie review snippets. The results with 10-fold cross-validation are reported in Table 22. Trained on Texts Trained on Sent. Tested on Tested on Tested on Tested on Texts Sent. Texts Sent. 1gram 81.1 69.0 66.8 77.4 2gram 83.7 68.6 71.2 73.9 3gram 82.5 64.1 70.0 65.4 Table 2: Accuracy of Naive Bayes on movie reviews. Consistent with findings in the literature (Cui et al., 2006; Dave et al., 2003; Gamon and Aue, 2005), on the large corpus of movie review texts, the indomain-trained system based solely on unigrams had lower accuracy than the similar system trained on bigrams. But the trigrams fared slightly worse than bigrams. On sentences, however, we have observed an inverse pattern: unigrams performed better than bigrams and trigrams. These results highlight a special property of sentence-level annotation: greater sensitivity to sparseness of the model: On texts, classifier error on one particular sentiment marker is often compensated by a number of correctly identified other sentim</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the Peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of WWW03, Budapest, HU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Drezde</author>
<author>John Blitzer</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Prague, CZ.</location>
<contexts>
<context position="2176" citStr="Drezde et al., 2007" startWordPosition="323" endWordPosition="326">on is defined as the assignment of positive, negative or neutral sentiment values to texts, sentences, and other linguistic units. Recent experiments assessing system portability across different domains, conducted by Aue and Gamon (2005), demonstrated that sentiment annotation classifiers trained in one domain do not perform well on other domains. A number of methods has been proposed in order to overcome this system portability limitation by using out-of-domain data, unlabelled in-domain corpora or a combination of in-domain and out-of-domain examples (Aue and Gamon, 2005; Bai et al., 2005; Drezde et al., 2007; Tan et al., 2007). In this paper, we present a novel approach to the problem of system portability across different domains by developing a sentiment annotation system that integrates a corpus-based classifier with a lexicon-based system trained on WordNet. By adopting this approach, we sought to develop a system that relies on both general and domainspecific knowledge, as humans do when analyzing a text. The information contained in lexicographical sources, such as WordNet, reflects a lay person’s general knowledge about the world, while domainspecific knowledge can be acquired through clas</context>
<context position="5821" citStr="Drezde et al. (2007)" startWordPosition="897" endWordPosition="900"> or out-of-domain training brought about an increased interest in unsupervised and weaklysupervised approaches to feature learning. For instance, Aue and Gamon (2005) proposed training on a samll number of labeled examples and large quantities of unlabelled in-domain data. This system performed well even when compared to systems trained on a large set of in-domain examples: on feedback messages from a web survey on knowledge bases, Aue and Gamon report 73.86% accuracy using unlabelled data compared to 77.34% for in-domain and 72.39% for the best out-of-domain training on a large training set. Drezde et al. (2007) applied structural correspondence learning (Drezde et al., 2007) to the task of domain adaptation for sentiment classification of product reviews. They showed that, depending on the domain, a small number (e.g., 50) of labeled examples allows to adapt the model learned on another corpus to a new domain. However, they note that the success of such adaptation and the number of necessary in-domain examples depends on the similarity between the original domain and the new one. Similarly, Tan et al. (2007) suggested to combine out-of-domain labeled examples with unlabelled ones from the target dom</context>
<context position="7342" citStr="Drezde et al., 2007" startWordPosition="1147" endWordPosition="1150">ted using Similarity Ranking and Relative Similarity Ranking algorithms (Tan et al., 2007). Depending on the similarity between domains, this method brought up to 15% gain compared to the baseline SVM. Overall, the development of semi-supervised approaches to sentiment tagging is a promising direction of the research in this area but so far, based on reported results, the performance of such methods is inferior to the supervised approaches with indomain training and to the methods that use general word lists. It also strongly depends on the similarity between the domains as has been shown by (Drezde et al., 2007; Tan et al., 2007). 3 Factors Affecting System Performance The comparison of system performance across different domains involves a number of factors that can significantly affect system performance – from training set size to level of analysis (sentence or entire document), document domain/genre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different d</context>
</contexts>
<marker>Drezde, Blitzer, Pereira, 2007</marker>
<rawString>Mark Drezde, John Blitzer, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, CZ.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
</authors>
<title>Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-05 Workshop on Feature Engineering for Machine Learning in Natural Language Processing,</booktitle>
<location>Ann Arbor, US.</location>
<contexts>
<context position="5093" citStr="Gamon and Aue, 2005" startWordPosition="782" endWordPosition="785">ly acquired from the Internet or from dictionaries, outperforms corpus-based classifiers when such classifiers use out-of-domain training data or when the training corpus is not sufficiently large to accumulate the necessary feature frequency information. But such general word lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews (Pang et al., 2002). On other domains, such as product reviews, the performance of systems that use general word lists is comparable to the performance of supervised machine learning approaches (Gamon and Aue, 2005). The recognition of major performance deficiencies of supervised machine learning methods with insufficient or out-of-domain training brought about an increased interest in unsupervised and weaklysupervised approaches to feature learning. For instance, Aue and Gamon (2005) proposed training on a samll number of labeled examples and large quantities of unlabelled in-domain data. This system performed well even when compared to systems trained on a large set of in-domain examples: on feedback messages from a web survey on knowledge bases, Aue and Gamon report 73.86% accuracy using unlabelled da</context>
<context position="8217" citStr="Gamon and Aue, 2005" startWordPosition="1288" endWordPosition="1291">or entire document), document domain/genre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy and recall if that single sentiment clue encountered in the sentence was not learned by</context>
<context position="12459" citStr="Gamon and Aue, 2005" startWordPosition="1962" endWordPosition="1965">n, 2005). For this reason, most of the research on sentiment tagging using statistical classifiers was limited to product and movie reviews, where review authors usually indicate their sentiment in a form of a standardized score that accompanies the texts of their reviews. The lack of sufficient data for training appears to be the main reason for the virtual absence of experiments with statistical classifiers in sentiment tagging at the sentence level. To our knowledge, the only work that describes the application of statistical classifiers (SVM) to sentence-level sentiment classification is (Gamon and Aue, 2005)1. The average performance of the system on ternary classification (positive, negative, and neutral) was between 0.50 and 0.52 for both average precision and recall. The results reported by (Riloff et al., 2006) for binary classification of sentences in a related domain of subjectivity tagging (i.e., the separation of sentiment-laden from neutral sentences) suggest that statistical classifiers can perform well on this task: the authors have reached 74.9% accuracy on the MPQA corpus (Riloff et al., 2006). In order to explore the performance of different approaches in sentiment annotation at the</context>
<context position="14658" citStr="Gamon and Aue, 2005" startWordPosition="2314" endWordPosition="2317">Experiments 4.1 System Performance on Texts vs. Sentences The experiments comparing in-domain trained system performance on texts vs. sentences were conducted on 2,002 movie review texts and on 10,662 movie review snippets. The results with 10-fold cross-validation are reported in Table 22. Trained on Texts Trained on Sent. Tested on Tested on Tested on Tested on Texts Sent. Texts Sent. 1gram 81.1 69.0 66.8 77.4 2gram 83.7 68.6 71.2 73.9 3gram 82.5 64.1 70.0 65.4 Table 2: Accuracy of Naive Bayes on movie reviews. Consistent with findings in the literature (Cui et al., 2006; Dave et al., 2003; Gamon and Aue, 2005), on the large corpus of movie review texts, the indomain-trained system based solely on unigrams had lower accuracy than the similar system trained on bigrams. But the trigrams fared slightly worse than bigrams. On sentences, however, we have observed an inverse pattern: unigrams performed better than bigrams and trigrams. These results highlight a special property of sentence-level annotation: greater sensitivity to sparseness of the model: On texts, classifier error on one particular sentiment marker is often compensated by a number of correctly identified other sentiment clues. Since sente</context>
</contexts>
<marker>Gamon, Aue, 2005</marker>
<rawString>Michael Gamon and Anthony Aue. 2005. Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms. In Proceedings of the ACL-05 Workshop on Feature Engineering for Machine Learning in Natural Language Processing, Ann Arbor, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen B McKeown</author>
</authors>
<title>Predicting the Semantic Orientation of Adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the the 40th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="20042" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="3184" endWordPosition="3187">a lexiconbased system will perform worse than an in-domain trained classifier but possibly better than a classifier trained on out-of domain data. One of the limitations of general lexicons and dictionaries, such as WordNet (Fellbaum, 1998), as training sets for sentiment tagging systems is that they contain only definitions of individual words and, hence, only unigrams could be effectively learned from dictionary entries. Since the structure of WordNet glosses is fairly different from that of other types of corpora, we developed a system that used the list of human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and then learned additional unigrams 294 from WordNet synsets and glosses with up to 88% accuracy, when evaluated against General Inquirer (Stone et al., 1966) (GI) on the intersection of our automatically acquired list with GI. In order to expand the list coverage for our experiments at the text and sentence levels, we then augmented the list by adding to it all the words annotated with “Positiv” or “Negativ” tags in GI, that were not picked up by the system. The resulting list of features contained 11,000 unigrams with the degree of membership in the category of positive or n</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen B. McKeown. 1997. Predicting the Semantic Orientation of Adjectives. In Proceedings of the the 40th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In KDD-04,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="8235" citStr="Hu and Liu, 2004" startWordPosition="1292" endWordPosition="1295">document domain/genre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy and recall if that single sentiment clue encountered in the sentence was not learned by the system. Since</context>
<context position="11213" citStr="Hu and Liu, 2004" startWordPosition="1758" endWordPosition="1761">set of sentences taken from personal weblogs (further, blogs) posted on LiveJournal (http://www.livejournal.com) and on http://www.cyberjournalist.com. This corpus is composed of 800 sentences (400 sentences with positive and 400 sentences with negative sentiment). In order to establish the interannotator agreement, two independent judges were asked to annotate 200 sentences from this corpus. The agreement between the two annotators on positive vs. negative tags reached 99% (r.=0.97). • A set of 1200 product review (PR) sentences extracted from the annotated corpus made available by Bing Liu (Hu and Liu, 2004) (http://www.cs.uic.edu/ liub/FBS/FBS.html). The data set sizes are summarized in Table 1. Movies News Blogs PR Text level 2002 texts n/a n/a n/a Sentence level 10662 800 800 1200 snippets sent. sent. sent. Table 1: Datasets 3.3 Establishing a Baseline for a Corpus-based System (CBS) Supervised statistical methods have been very successful in sentiment tagging of texts: on movie review texts they reach accuracies of 85-90% (Aue and Gamon, 2005; Pang and Lee, 2004). These methods perform particularly well when a large volume of labeled data from the same domain as the 292 test set is available </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In KDD-04, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment Classification of Movie Reviews Using Contextual Valence Shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="28414" citStr="Kennedy and Inkpen, 2006" startWordPosition="4518" endWordPosition="4521">valuation experiments with the system were performed on different domains using 3-fold validation. The experiments conducted with the Ensemble system were designed to explore system performance under conditions of limited availability of annotated data for classifier training. For this reason, the numbers reported for the corpus-based classifier do not reflect the full potential of machine learning approaches when sufficient in-domain training data is available. Table 7 presents the results of these experiments by domain/genre. The results 6These results are consistent with an observation in (Kennedy and Inkpen, 2006), where a lexicon-based system performed with a better precision on negative than on positive texts. 7The size of the test set varied in different experiments due to the availability of annotated data for a particular domain. 296 are statistically significant at α = 0.01, except the runs on movie reviews where the difference between the LBS and Ensemble classifiers was significant at α = 0.05. LBS CBS Ensemble News Acc 67.8 53.2 73.3 F 0.82 0.71 0.85 Movies Acc 54.5 53.5 62.1 F 0.73 0.72 0.77 Blogs Acc 61.2 51.1 70.9 F 0.78 0.69 0.83 PRs Acc 59.5 58.9 78.0 F 0.77 0.75 0.88 Average Acc 60.7 54.</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Alistair Kennedy and Diana Inkpen. 2006. Sentiment Classification of Movie Reviews Using Contextual Valence Shifters. Computational Intelligence, 22(2):110–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic detection of opinion bearing words and sentences.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second International Joint Conference on Natural Language Processing, Companion Volume,</booktitle>
<location>Jeju Island, KR.</location>
<contexts>
<context position="8255" citStr="Kim and Hovy, 2005" startWordPosition="1296" endWordPosition="1299">nre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy and recall if that single sentiment clue encountered in the sentence was not learned by the system. Since the comparison of s</context>
</contexts>
<marker>Kim, Hovy, 2005</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2005. Automatic detection of opinion bearing words and sentences. In Proceedings of the Second International Joint Conference on Natural Language Processing, Companion Volume, Jeju Island, KR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lilian Lee</author>
</authors>
<title>A sentiment education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8108" citStr="Pang and Lee, 2004" startWordPosition="1269" endWordPosition="1272">rs that can significantly affect system performance – from training set size to level of analysis (sentence or entire document), document domain/genre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively</context>
<context position="11681" citStr="Pang and Lee, 2004" startWordPosition="1833" endWordPosition="1836"> reached 99% (r.=0.97). • A set of 1200 product review (PR) sentences extracted from the annotated corpus made available by Bing Liu (Hu and Liu, 2004) (http://www.cs.uic.edu/ liub/FBS/FBS.html). The data set sizes are summarized in Table 1. Movies News Blogs PR Text level 2002 texts n/a n/a n/a Sentence level 10662 800 800 1200 snippets sent. sent. sent. Table 1: Datasets 3.3 Establishing a Baseline for a Corpus-based System (CBS) Supervised statistical methods have been very successful in sentiment tagging of texts: on movie review texts they reach accuracies of 85-90% (Aue and Gamon, 2005; Pang and Lee, 2004). These methods perform particularly well when a large volume of labeled data from the same domain as the 292 test set is available for training (Aue and Gamon, 2005). For this reason, most of the research on sentiment tagging using statistical classifiers was limited to product and movie reviews, where review authors usually indicate their sentiment in a form of a standardized score that accompanies the texts of their reviews. The lack of sufficient data for training appears to be the main reason for the virtual absence of experiments with statistical classifiers in sentiment tagging at the s</context>
<context position="13262" citStr="Pang and Lee, 2004" startWordPosition="2091" endWordPosition="2094">ed by (Riloff et al., 2006) for binary classification of sentences in a related domain of subjectivity tagging (i.e., the separation of sentiment-laden from neutral sentences) suggest that statistical classifiers can perform well on this task: the authors have reached 74.9% accuracy on the MPQA corpus (Riloff et al., 2006). In order to explore the performance of different approaches in sentiment annotation at the text and sentence levels, we used a basic Naive Bayes classifier. It has been shown that both Naive Bayes and SVMs perform with similar accuracy on different sentiment tagging tasks (Pang and Lee, 2004). These observations were confirmed with our own experiments with SVMs and Naive Bayes (Table 3). We used the Weka package (http://www.cs.waikato.ac.nz/ml/weka/) with default settings. In the sections that follow, we describe a set of comparative experiments with SVMs and Naive Bayes classifiers (1) on texts and sentences and (2) on four different domains (movie reviews, news, blogs, and product reviews). System runs with unigrams, bigrams, and trigrams as features and with different training set sizes are presented. 1Recently, a similar task has been addressed by the Affective Text Task at Se</context>
<context position="23890" citStr="Pang and Lee (2004)" startWordPosition="3801" endWordPosition="3804">e tool in domain adaptation for sentiment analysis. In the ensemble of classifiers, they used a combination of nine SVM-based classifiers deployed to learn unigrams, bigrams, and trigrams on three different domains, while the fourth domain was used as an evaluation set. Using then an SVM meta-classifier trained on a small number of target domain examples to combine the nine base classifiers, they obtained a statistically significant improvement on out-of-domain texts from book reviews, knowledge-base feedback, and product support services survey data. No improvement occurred on movie reviews. Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative. Das and Chen (2004) used five classifiers to determine market sentiment on Yahoo! postings. Simple majority vote was applied to make decisions within 295 the ensemble of classifiers and achieved accuracy of 62% on ternary in-domain classification. In this study we describe a system that attempts to combine th</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lilian Lee. 2004. A sentiment education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Meeting of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, US.</location>
<contexts>
<context position="9551" citStr="Pang and Lee, 2005" startWordPosition="1500" endWordPosition="1503"> attempted to date, we also sought to close this gap in the literature by conducting the first set of our comparative experiments on data sets of 2,002 movie review texts and 10,662 movie review snippets (5331 with positive and 5331 with negative sentiment) provided by Bo Pang (http://www.cs.cornell.edu/People/pabo/moviereview-data/). 3.2 Domain Effects The second set of our experiments explores system performance on different domains at sentence level. For this we used four different data sets of sentences annotated with sentiment tags: • A set of movie review snippets (further: movie) from (Pang and Lee, 2005). This dataset of 10,662 snippets was collected automatically from www.rottentomatoes.com website. All sentences in reviews marked “rotten” were considered negative and snippets from “fresh” reviews were deemed positive. In order to make the results obtained on this dataset comparable to other domains, a randomly selected subset of 1066 snippets was used in the experiments. • A balanced corpus of 800 manually annotated sentences extracted from 83 newspaper texts (further, news). The full set of sentences was annotated by one judge. 200 sentences from this corpus (100 positive and 100 negative)</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43nd Meeting of the Association for Computational Linguistics, Ann Arbor, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lilian Lee</author>
<author>Shrivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4897" citStr="Pang et al., 2002" startWordPosition="751" endWordPosition="754">supervised approaches can be used to take advantage of a small number of annotated in-domain examples and/or of unlabelled indomain data. The first approach, using general word lists automatically acquired from the Internet or from dictionaries, outperforms corpus-based classifiers when such classifiers use out-of-domain training data or when the training corpus is not sufficiently large to accumulate the necessary feature frequency information. But such general word lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews (Pang et al., 2002). On other domains, such as product reviews, the performance of systems that use general word lists is comparable to the performance of supervised machine learning approaches (Gamon and Aue, 2005). The recognition of major performance deficiencies of supervised machine learning methods with insufficient or out-of-domain training brought about an increased interest in unsupervised and weaklysupervised approaches to feature learning. For instance, Aue and Gamon (2005) proposed training on a samll number of labeled examples and large quantities of unlabelled in-domain data. This system performed </context>
<context position="8088" citStr="Pang et al., 2002" startWordPosition="1265" endWordPosition="1268">s a number of factors that can significantly affect system performance – from training set size to level of analysis (sentence or entire document), document domain/genre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of sentiment clues, wh</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Read</author>
</authors>
<title>Using emoticons to reduce dependency in machine learning techniques for sentiment classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-2005 Student Research Workshop,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="3938" citStr="Read, 2005" startWordPosition="604" endWordPosition="605">er trained on a small in-domain training set. 290 Proceedings of ACL-08: HLT, pages 290–298, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Domain Adaptation in Sentiment Research Most text-level sentiment classifiers use standard machine learning techniques to learn and select features from labeled corpora. Such approaches work well in situations where large labeled corpora are available for training and validation (e.g., movie reviews), but they do not perform well when training data is scarce or when it comes from a different domain (Aue and Gamon, 2005; Read, 2005), topic (Read, 2005) or time period (Read, 2005). There are two alternatives to supervised machine learning that can be used to get around this problem: on the one hand, general lists of sentiment clues/features can be acquired from domain-independent sources such as dictionaries or the Internet, on the other hand, unsupervised and weakly-supervised approaches can be used to take advantage of a small number of annotated in-domain examples and/or of unlabelled indomain data. The first approach, using general word lists automatically acquired from the Internet or from dictionaries, outperforms c</context>
</contexts>
<marker>Read, 2005</marker>
<rawString>Jonathon Read. 2005. Using emoticons to reduce dependency in machine learning techniques for sentiment classification. In Proceedings of the ACL-2005 Student Research Workshop, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
<author>Janyce Wiebe</author>
</authors>
<title>Feature subsumption for opinion analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, AUS.</location>
<contexts>
<context position="8129" citStr="Riloff et al., 2006" startWordPosition="1273" endWordPosition="1276">antly affect system performance – from training set size to level of analysis (sentence or entire document), document domain/genre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy</context>
<context position="12670" citStr="Riloff et al., 2006" startWordPosition="1997" endWordPosition="2000">tandardized score that accompanies the texts of their reviews. The lack of sufficient data for training appears to be the main reason for the virtual absence of experiments with statistical classifiers in sentiment tagging at the sentence level. To our knowledge, the only work that describes the application of statistical classifiers (SVM) to sentence-level sentiment classification is (Gamon and Aue, 2005)1. The average performance of the system on ternary classification (positive, negative, and neutral) was between 0.50 and 0.52 for both average precision and recall. The results reported by (Riloff et al., 2006) for binary classification of sentences in a related domain of subjectivity tagging (i.e., the separation of sentiment-laden from neutral sentences) suggest that statistical classifiers can perform well on this task: the authors have reached 74.9% accuracy on the MPQA corpus (Riloff et al., 2006). In order to explore the performance of different approaches in sentiment annotation at the text and sentence levels, we used a basic Naive Bayes classifier. It has been shown that both Naive Bayes and SVMs perform with similar accuracy on different sentiment tagging tasks (Pang and Lee, 2004). These </context>
</contexts>
<marker>Riloff, Patwardhan, Wiebe, 2006</marker>
<rawString>Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe. 2006. Feature subsumption for opinion analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Sydney, AUS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
<author>D C Dumphy</author>
<author>M S Smith</author>
<author>D M Ogilvie</author>
</authors>
<title>The General Inquirer: a computer approach to content analysis. M.I.T. studies in comparative politics.</title>
<date>1966</date>
<publisher>M.I.T. Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="20217" citStr="Stone et al., 1966" startWordPosition="3213" endWordPosition="3216">nd dictionaries, such as WordNet (Fellbaum, 1998), as training sets for sentiment tagging systems is that they contain only definitions of individual words and, hence, only unigrams could be effectively learned from dictionary entries. Since the structure of WordNet glosses is fairly different from that of other types of corpora, we developed a system that used the list of human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and then learned additional unigrams 294 from WordNet synsets and glosses with up to 88% accuracy, when evaluated against General Inquirer (Stone et al., 1966) (GI) on the intersection of our automatically acquired list with GI. In order to expand the list coverage for our experiments at the text and sentence levels, we then augmented the list by adding to it all the words annotated with “Positiv” or “Negativ” tags in GI, that were not picked up by the system. The resulting list of features contained 11,000 unigrams with the degree of membership in the category of positive or negative sentiment assigned to each of them. In order to assign the membership score to each word, we did 58 system runs on unique nonintersecting seed lists drawn from manuall</context>
</contexts>
<marker>Stone, Dumphy, Smith, Ogilvie, 1966</marker>
<rawString>P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie. 1966. The General Inquirer: a computer approach to content analysis. M.I.T. studies in comparative politics. M.I.T. Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Rada Mihalcea</author>
</authors>
<title>SemEval2007 Task 14: Affective Text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<location>Prague, CZ.</location>
<contexts>
<context position="14034" citStr="Strapparava and Mihalcea, 2007" startWordPosition="2209" endWordPosition="2212">kato.ac.nz/ml/weka/) with default settings. In the sections that follow, we describe a set of comparative experiments with SVMs and Naive Bayes classifiers (1) on texts and sentences and (2) on four different domains (movie reviews, news, blogs, and product reviews). System runs with unigrams, bigrams, and trigrams as features and with different training set sizes are presented. 1Recently, a similar task has been addressed by the Affective Text Task at SemEval-1 where even shorter units – headlines – were classified into positive, negative and neutral categories using a variety of techniques (Strapparava and Mihalcea, 2007). 4 Experiments 4.1 System Performance on Texts vs. Sentences The experiments comparing in-domain trained system performance on texts vs. sentences were conducted on 2,002 movie review texts and on 10,662 movie review snippets. The results with 10-fold cross-validation are reported in Table 22. Trained on Texts Trained on Sent. Tested on Tested on Tested on Tested on Texts Sent. Texts Sent. 1gram 81.1 69.0 66.8 77.4 2gram 83.7 68.6 71.2 73.9 3gram 82.5 64.1 70.0 65.4 Table 2: Accuracy of Naive Bayes on movie reviews. Consistent with findings in the literature (Cui et al., 2006; Dave et al., 20</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>Carlo Strapparava and Rada Mihalcea. 2007. SemEval2007 Task 14: Affective Text. In Proceedings of the 4th International Workshop on Semantic Evaluations, Prague, CZ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songbo Tan</author>
<author>Gaowei Wu</author>
<author>Huifeng Tang</author>
<author>Zueqi Cheng</author>
</authors>
<title>A Novel Scheme for Domain-transfer Problem in the context of Sentiment Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM</booktitle>
<contexts>
<context position="2195" citStr="Tan et al., 2007" startWordPosition="327" endWordPosition="330">assignment of positive, negative or neutral sentiment values to texts, sentences, and other linguistic units. Recent experiments assessing system portability across different domains, conducted by Aue and Gamon (2005), demonstrated that sentiment annotation classifiers trained in one domain do not perform well on other domains. A number of methods has been proposed in order to overcome this system portability limitation by using out-of-domain data, unlabelled in-domain corpora or a combination of in-domain and out-of-domain examples (Aue and Gamon, 2005; Bai et al., 2005; Drezde et al., 2007; Tan et al., 2007). In this paper, we present a novel approach to the problem of system portability across different domains by developing a sentiment annotation system that integrates a corpus-based classifier with a lexicon-based system trained on WordNet. By adopting this approach, we sought to develop a system that relies on both general and domainspecific knowledge, as humans do when analyzing a text. The information contained in lexicographical sources, such as WordNet, reflects a lay person’s general knowledge about the world, while domainspecific knowledge can be acquired through classifier training on </context>
<context position="6328" citStr="Tan et al. (2007)" startWordPosition="981" endWordPosition="984">34% for in-domain and 72.39% for the best out-of-domain training on a large training set. Drezde et al. (2007) applied structural correspondence learning (Drezde et al., 2007) to the task of domain adaptation for sentiment classification of product reviews. They showed that, depending on the domain, a small number (e.g., 50) of labeled examples allows to adapt the model learned on another corpus to a new domain. However, they note that the success of such adaptation and the number of necessary in-domain examples depends on the similarity between the original domain and the new one. Similarly, Tan et al. (2007) suggested to combine out-of-domain labeled examples with unlabelled ones from the target domain in order to solve the domain-transfer problem. They applied an outof-domain-trained SVM classifier to label examples from the target domain and then retrained the classifier using these new examples. In order to maximize the utility of the examples from the target domain, these examples were selected using Similarity Ranking and Relative Similarity Ranking algorithms (Tan et al., 2007). Depending on the similarity between domains, this method brought up to 15% gain compared to the baseline SVM. Ove</context>
</contexts>
<marker>Tan, Wu, Tang, Cheng, 2007</marker>
<rawString>Songbo Tan, Gaowei Wu, Huifeng Tang, and Zueqi Cheng. 2007. A Novel Scheme for Domain-transfer Problem in the context of Sentiment Analysis. In Proceedings of CIKM 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring praise and criticism: inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<pages>21--315</pages>
<contexts>
<context position="8170" citStr="Turney and Littman, 2003" startWordPosition="1279" endWordPosition="1282">rom training set size to level of analysis (sentence or entire document), document domain/genre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy and recall if that single sentiment clue</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael Littman. 2003. Measuring praise and criticism: inference of semantic orientation from association. ACM Transactions on Information Systems (TOIS), 21:315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="8143" citStr="Turney, 2002" startWordPosition="1277" endWordPosition="1278">erformance – from training set size to level of analysis (sentence or entire document), document domain/genre and many other factors. In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy and recall if</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
<author>Theresa Wilson</author>
</authors>
<title>A corpus study of Evaluative and Speculative Language.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd ACL SIGDial Workshop on Discourse and Dialogue,</booktitle>
<location>Aalberg, DK.</location>
<contexts>
<context position="8511" citStr="Wiebe et al., 2001" startWordPosition="1338" endWordPosition="1341">ine for performance comparisons across different domains/genres. 291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006). It should be noted that each of these levels presents different challenges for sentiment annotation. For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic. On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy and recall if that single sentiment clue encountered in the sentence was not learned by the system. Since the comparison of sentiment annotation system performance on texts and on sentences has not been attempted to date, we also sought to close this gap in the literature by conducting the first set of our comparative experiments on data sets of 2,002 movie review texts and 10,6</context>
</contexts>
<marker>Wiebe, Bruce, Bell, Martin, Wilson, 2001</marker>
<rawString>Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie Martin, and Theresa Wilson. 2001. A corpus study of Evaluative and Speculative Language. In Proceedings of the 2nd ACL SIGDial Workshop on Discourse and Dialogue, Aalberg, DK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lotfy A Zadeh</author>
</authors>
<title>Calculus of Fuzzy Restrictions.</title>
<date>1975</date>
<pages>1--40</pages>
<editor>In L.A. Zadeh et al., editor,</editor>
<publisher>Academic Press Inc.,</publisher>
<location>New-York.</location>
<contexts>
<context position="21520" citStr="Zadeh, 1975" startWordPosition="3443" endWordPosition="3444">97). The 58 runs were then collapsed into a single set of 7,813 unique words. For each word we computed a score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive. The resulting measure, termed Net Overlap Score (NOS), reflected the number of ties linking a given word with other sentimentladen words in WordNet, and hence, could be used as a measure of the words’ centrality in the fuzzy category of sentiment. The NOSs were then normalized into the interval from -1 to +1 using a sigmoid fuzzy membership function (Zadeh, 1975)4. Only words with fuzzy membership degree not equal to zero were retained in the list. The resulting list contained 10,809 sentiment-bearing words of different parts of speech. The sentiment determination at the sentence and text level was then done by summing up the scores of all identified positive unigrams (NOS&gt;0) and all negative unigrams (NOS&lt;0) (Andreevskaia and Bergler, 2006). 5.1 Establishing a Baseline for the Lexicon-Based System (LBS) The baseline performance of the Lexicon-Based System (LBS) described above is presented in Table 5, along with the performance results of the indomai</context>
</contexts>
<marker>Zadeh, 1975</marker>
<rawString>Lotfy A. Zadeh. 1975. Calculus of Fuzzy Restrictions. In L.A. Zadeh et al., editor, Fuzzy Sets and their Applications to cognitive and decision processes, pages 1–40. Academic Press Inc., New-York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>