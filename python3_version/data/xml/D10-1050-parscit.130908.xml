<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000275">
<title confidence="0.997207">
Title Generation with Quasi-Synchronous Grammar
</title>
<author confidence="0.999366">
Kristian Woodsend, Yansong Feng and Mirella Lapata
</author>
<affiliation confidence="0.999914">
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.902207">
Edinburgh EH8 9AB, United Kingdom
</address>
<email confidence="0.996572">
k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.995666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999181636363636">
The task of selecting information and render-
ing it appropriately appears in multiple con-
texts in summarization. In this paper we
present a model that simultaneously optimizes
selection and rendering preferences. The
model operates over a phrase-based represen-
tation of the source document which we ob-
tain by merging PCFG parse trees and depen-
dency graphs. Selection preferences for in-
dividual phrases are learned discriminatively,
while a quasi-synchronous grammar (Smith
and Eisner, 2006) captures rendering prefer-
ences such as paraphrases and compressions.
Based on an integer linear programming for-
mulation, the model learns to generate sum-
maries that satisfy both types of preferences,
while ensuring that length, topic coverage and
grammar constraints are met. Experiments on
headline and image caption generation show
that our method obtains state-of-the-art per-
formance using essentially the same model for
both tasks without any major modifications.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971">
Summarization is the process of condensing a source
text into a shorter version while preserving its infor-
mation content. Humans summarize on a daily ba-
sis and effortlessly, yet the automatic production of
high-quality summaries remains a challenge.
Most work today focuses on extractive summa-
rization, where a summary is created by identifying
and subsequently concatenating the most important
sentences in a document. The advantage of this ap-
proach is that it does not require a great deal of lin-
guistic analysis to generate grammatical sentences,
assuming the source document was well written.
Unfortunately, extracts generated this way are often
documents of low readability and text quality, and
contain much redundant information. The concise-
ness can be improved when sentence extraction is
interfaced with sentence compression, where words
and clauses are deleted based on rules typically op-
erating over parsed input (Jing, 2000; Daum´e III
and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic
et al., 2007; Martins and Smith, 2009).
An alternative abstractive or “bottom-up” ap-
proach involves identifying high-interest words and
phrases in the source text, and combining them into
new sentences guided by a language model (Banko
et al., 2000; Soricut and Marcu, 2007). This ap-
proach has the potential to work well, breaking out
of the single-sentence paradigm. Unfortunately, the
resulting summaries are not always coherent — indi-
vidual constituent phrases are often combined with-
out any semantic constraints — or grammatical be-
yond the n-gram horizon imposed by the language
model.
Constituent deletion and recombination are
merely two of the many rewrite operations profes-
sional editors and abstractors employ when creating
summaries (Jing, 2002). Additional operations in-
clude truncating sentences, aggregating them, and
paraphrasing at word or syntax level. Furthermore,
professionals write summaries in a task-specific
style. News headlines for example are typically
short (three to six words), written in the present
tense and active voice, and often leave out forms of
the verb be. There are also different ways of writing
a headline either directly by stating what the docu-
</bodyText>
<page confidence="0.9806">
513
</page>
<note confidence="0.817726">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 513–523,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999840846153847">
ment is about or indirectly by raising a question in
the reader’s mind, which the document answers.
The automatic generation of summaries similar to
those produced by human abstractors is challenging
because of the many constraints imposed by the task:
the summary must be maximally informative and
minimally redundant, grammatical, coherent, adhere
to a pre-specified length and stylistic conventions.
Importantly, these constraints are conflicting; the
deletion of certain phrases may avoid redundancy
but result in ungrammatical output and information
loss.
In this paper we propose a model for summariza-
tion that attempts to capture and optimize these con-
straints jointly. We learn both how to select the
most important information (the content), and how
to render it appropriately (the style). Selection pref-
erences are learned discriminatively, while a quasi-
synchronous grammar (QG, Smith and Eisner 2006)
captures rendering preferences such as paraphrases
and compressions. The entire solution space of
possible extractions and QG-generated paraphrases
is searched efficiently through use of integer lin-
ear programming. The ILP framework allows us to
model naturally as constraints, additional require-
ments such as sentence length, overall summary
length, topic coverage and, importantly, grammati-
cality.
We argue that QG is attractive for describ-
ing rewrite operations common in summarization.
Rather than assuming a strictly synchronous struc-
ture over the source and target sentences, QG iden-
tifies a “sloppy” alignment of parse trees assuming
that the target tree is in some way “inspired by” the
source tree. A key insight in our approach is to
formulate the summarization problem at the phrase
level: both QG rules and information extraction op-
erate over individual phrases rather than (as is the
norm) sentences. At this smaller unit level, QG
rules become more widely applicable and compres-
sion falls naturally because only phrases deemed im-
portant should appear in the summary.
We evaluate the proposed model on headline gen-
eration and the related task of image caption gen-
eration. However, there is nothing inherent in our
formulation that is specific to those two tasks; it is
possible for the model to generate longer or shorter
summaries, for a single or multiple documents. Ex-
perimental results show that our method obtains
state-of-the-art performance, both in terms of gram-
maticality and informativeness for both tasks using
the same summarization model.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999867763157895">
Much effort in automatic summarization has been
devoted to sentence extraction which is often for-
malized as a classification task (Kupiec et al., 1995).
Given appropriately annotated training data, a bi-
nary classifier learns to predict for each document
sentence if it is worth extracting. A few previ-
ous approaches have attempted to interface sentence
compression with summarization. A straightforward
way to achieve this is by adopting a two-stage ar-
chitecture (e.g., Lin 2003) where the sentences are
first extracted and then compressed or the other way
round.
Other work implements a joint model where
words are deleted and sentences selected from a doc-
ument simultaneously (Daum´e III and Marcu, 2002;
Martins and Smith, 2009; Woodsend and Lapata,
2010). ILP models have also been developed for
sentence rather than document compression (Clarke
and Lapata, 2008). Dras (1999) discusses the appli-
cation of ILP to reluctant paraphrasing, i.e., the task
of choosing between paraphrases while conforming
to length, readability, or style constraints. Again,
the aim is to rewrite text without, however, con-
tent selection. Rewrite operations other than dele-
tion tend to be hand-crafted and domain specific
(Jing and McKeown, 2000). Notable exceptions are
Cohn and Lapata (2008) and Zhao et al. (2009) who
present a model that can both compress and para-
phrase individual sentences without however gener-
ating document-level summaries.
Headline generation is a well-studied task within
single-document summarization, due to its promi-
nence in the DUC-03 and DUC-04 evaluation com-
petitions.1 Many approaches identify the most infor-
mative sentence in a given document (typically the
first sentence for the news genre) and subsequently
apply a form of sentence compression such that
the headline meets some length requirement (Dorr
</bodyText>
<footnote confidence="0.997035333333333">
1Approaches to headline generation are too numerous to list
in detail; see the proceedings of DUC-03 and DUC-04 for an
overview.
</footnote>
<page confidence="0.997245">
514
</page>
<bodyText confidence="0.999873522727273">
et al., 2003). The compressed sentence may also be
“padded” with important content words or phrases
to ensure that the topic of the document is covered
(Zajic et al., 2004). Other work generates headlines
in a bottom-up fashion starting from important, indi-
vidual words and phrases, that are glued together to
create a fluent sentence. For example, Banko et al.
(2000) draw inspiration from Machine Translation
and generate headlines using statistical models for
content selection and sentence realization.
Relatively little work has focused on caption gen-
eration, a task related to headline generation. The
aim here is to create a short, title-like description of
an image embedded in a news article. Like head-
lines, captions have to be short and informative. In
addition, a good caption must clearly identify the
subject of the picture and establish its relevance to
the article. Feng and Lapata (2010a) develop ex-
tractive and abstractive caption generation models
that operate over the output of a probabilistic im-
age annotation model that preprocesses the pictures
and suggests keywords to describe their content.
Their best model is an extension of Banko et al.’s
(2000) word-based model for headline generation to
phrases.
Our own work develops an ILP-based summariza-
tion model with rewrite operations that are not lim-
ited to deletion, are defined over phrases, and en-
coded in quasi-synchronous grammar. The QG for-
malism has been previously applied to parser adap-
tation and projection (Smith and Eisner, 2009), para-
phrase identification (Das and Smith, 2009), and
question answering (Wang et al., 2007); however
the use of QG in summarization is novel to our
knowledge. Unlike most synchronous grammar for-
malisms, QG does not posit a strict isomorphism be-
tween a source sentence and its target translation; it
only loosely links the syntactic structure of the two,
and is therefore well suited to describing the rela-
tionship between a document and its abstract. We
propose an ILP formulation which not only allows
to efficiently search through the space of many QG
rules but also to incorporate constraints relating to
content, style, and the task at hand.
</bodyText>
<sectionHeader confidence="0.981654" genericHeader="method">
3 Modeling
</sectionHeader>
<bodyText confidence="0.9998781">
There are three components to our model. Content
selection is performed discriminatively; an SVM
learns which information in the source document
should be in the summary, and gives a real-valued
salience score for each phrase. QG rules are used
to generate compressions and paraphrases of the
source sentences. An ILP model combines the out-
put of these two components into an output sum-
mary, while optimizing content selection and surface
realization preferences jointly.
</bodyText>
<subsectionHeader confidence="0.999658">
3.1 Document Representation
</subsectionHeader>
<bodyText confidence="0.999947733333333">
Our model operates on documents annotated with
syntactic information which we obtain by parsing
every sentence twice, once with a phrase structure
parser and once with a dependency parser. The out-
put from the two representations is combined into a
single data structure, by mapping the dependencies
to the edges of the phrase structure tree. The proce-
dure is described in detail in Woodsend and Lapata
(2010). However, we do not merge the leaf nodes
into phrases here, but keep the full tree structure,
as we will apply compression to phrases through
the QG. In our experiments, we obtain this com-
bined representation from the output of the Stan-
ford parser (Klein and Manning, 2003) but any other
broadly similar parser could be used instead.
</bodyText>
<subsectionHeader confidence="0.999521">
3.2 Quasi-synchronous grammar
</subsectionHeader>
<bodyText confidence="0.999982176470588">
Given an input sentence S1 or its parse tree T1, the
QG constructs a monolingual grammar for parsing,
or generating, the possible translation (or here, para-
phrase) trees T2. A grammar node in the target tree
T2 is modeled on a subset of nodes in the source tree,
with a rather loose alignment between the trees.
In our approach, the process of learning the gram-
mar is unsupervised. Each sentence of the source
document is compared to each sentence in the target
document — headline or caption, depending on the
task. Using the combined PCFG-dependency tree
representation described above, we build up a list of
leaf node alignments based on lexical identity, after
stemming and removing stop words. We align direct
parent nodes where more than one child node aligns.
A grammar rule is created if the all the nodes in the
target tree can be explained using nodes from the
</bodyText>
<page confidence="0.993482">
515
</page>
<figure confidence="0.999969576271187">
(a) NP
NP
CHOICE/–
(b) PP/prep in
PP/prep in
of
East
Timor
NNP/nn
JJ/amod
NNP/nn
NNP/–
NNP/–
NNP/nn
NNP/nn
PP/prep in
PP/prep in
Saudi
dissident
Osama
bin
Laden
bin
Laden
IN/–
PP/prep of
IN/–
NNP/nn
NNP/–
DT/det
NN/–
in
the
IN/–
DT/det
JJ/amod
NNP/nn
NNP/–
NN/–
IN/–
disputed
territory
IN/–
NNP/nn
in
NNP/–
Timor
East
PP/prep of
in
the
in
East
disputed
territory
Timor
IN/–
NNP/nn
NNP/–
</figure>
<figureCaption confidence="0.634144">
Figure 2: Alternative paraphrases are represented as
</figureCaption>
<figure confidence="0.871707">
a CHOICE sub-tree.
JJ/amod
</figure>
<figureCaption confidence="0.998806">
Figure 1: Examples of QG alignments between
</figureCaption>
<bodyText confidence="0.994334035087719">
source node (left) and target node (right). (a) align-
ment of child nodes, involving compression through
deletion; (b) rewriting involving child and grand-
child nodes; (c) reordering of child nodes (with fur-
ther compression through applying other QG rules
on children). Nodes bear phrase and dependency la-
bels. Dotted lines show alignments in the grammar
between source and target child nodes. Examples
are taken from the QG rules discovered in the DUC-
03 data set of headlines.
source; this helps to improve the quality in what is
inherently a noisy process. Finally, QG rules are cre-
ated from aligned nodes above the leaf node level,
recording the phrase and dependency label of nodes,
and the alignment of child nodes.
Unlike previous work involving QG which has
used dependency graphs exclusively (e.g., Wang
et al. 2007; Das and Smith 2009), our approach op-
erates over a combined PCFG-dependency represen-
tation. As a result, some configurations in Smith and
Eisner (2006) are not so relevant here — instead,
we found that deletions, reorderings, flattening of
nodes, and the addition of text elements were im-
portant operations for the grammar.
Figure 1 shows some example alignments that are
captured by the QG, with the source node on the
left and the target node on the right. Leaf nodes
have their original text, while other nodes have a
combined phrase and dependency label that they ob-
tain in the merged representation described in Sec-
tion 3.1 above (e.g., NP/dobj is a noun phrase and a
direct object, NNP/nn is a proper noun and a nomi-
nal modifier, whereas NN/– is a head noun). Align-
ments between the children are shown by dotted
lines. In Figure 1(a), some child nodes are aligned
while others are not present in the target tree. This
type of rule is common in our training data, and typ-
ically arises from the compression of names in noun
phrases. Another frequent compression, shown in
Figure 1(b), is flattening the tree structure by in-
corporating grand-child elements at the child level.
Figure 1(c) shows a rule involving the reordering
of child nodes, and where additional rules are ap-
plied recursively to achieve further compression and
a transformation in the phrase constituency.
Paraphrases are created from source sentence
parse trees by applying suitable rules recursively.
Suitable rules have matching structure in terms of
phrase and dependency label, for both the parent and
child nodes. Additionally, the proposed paraphrase
sub-tree must be suitable for the target tree being
created (i.e., the root node of the paraphrase must
match the phrase and dependency label of the corre-
sponding node in the target tree). Where more than
one paraphrase is possible, the alternatives are incor-
porated into the target parse tree under a CHOICE
node, as is shown in Figure 2. Note that unlike pre-
</bodyText>
<figure confidence="0.996638545454545">
of
East
Timor
(c) NP/dobj
NP/dobj
the
extradition
of
Kurdish
leader
Ocalan
IN/–
NNP/–
Ocalan’s
extradition
NN/nn
NN/–
DT/det
NP/poss
NN/–
PP/prep of
NNP/nn
</figure>
<page confidence="0.991909">
516
</page>
<bodyText confidence="0.9997132">
vious QG approaches, we do not use the probability
model proposed by Smith and Eisner (2006); instead
the QG is used to represent rewrite operations, and
we simply record a frequency count for how often
each rule is encountered in the training data.
</bodyText>
<subsectionHeader confidence="0.9526">
3.3 ILP model
</subsectionHeader>
<bodyText confidence="0.997408">
The objective of our model is to create the most in-
formative text possible, subject to constraints which
can be tailored to the specific task. These relate to
sentence length, overall summary length, the inclu-
sion of specific topics, and grammaticality. These
constraints are global in their scope, and cannot be
adequately satisfied by optimizing each one of them
individually. Our approach therefore uses an ILP
formulation which will provide a globally optimal
solution, and which can be efficiently solved using
standard optimization tools. Specifically, the model
selects phrases and paraphrases from which to form
the output sentence. Here, we focus on a single
sentence as this is most appropriate for title gener-
ation. However, multi-sentence output can be easily
generated by setting a summary length constraint.
The model operates over the merged phrase struc-
ture trees described in Section 3.1, augmented with
paraphrase choice nodes such as shown in Figure 2
rather than raw text.
Let S be the set of sentences in a document, P be
the set of phrases, and Ps ⊂ P be the set of phrases
in each sentence s ∈ S. Let the sets Di ⊂ P, ∀i ∈ P
capture the phrase dependency information for each
phrase i, where each set Di contains the phrases that
depend on the presence of i. In a similar fashion,
C ⊂ P is the set of choice nodes throughout the doc-
ument, which represent nodes in the tree where more
than one QG rule can be applied; Ci ⊂ P,i ∈ C are
the sets of phrases that are direct children of each
choice node, in other words they are the individual
alternative paraphrases. Let li be the length of each
phrase i, in tokens.
For caption generation, the model has as addi-
tional input a list of tags (keywords drawn from the
source document) that correspond to the image, and
we refer to this set of tags as T . Pt ⊂ P is the set of
phrases containing the tag t ∈ T . We use the proba-
bilistic image annotation model of Feng and Lapata
(2010a) to generate the list of keywords. The lat-
ter highlight the objects depicted in the image and
should be in all likelihood included in the caption.
The model is cast as an integer linear program:
</bodyText>
<table confidence="0.999283">
max ∑ ( fi + λgi)xi
x i∈P
s.t. ∑ lixi ≤ Lmax
i∈P
∑ lixi ≥ Lmin
i∈P
∑ xi ≥ Tmin
i∈Pt,t∈T
xj → xi ∀i ∈ P, j ∈ Di
∑ xj = xi ∀i ∈ C, j ∈ Ci
j∈Ci
xi → ys ∀s ∈ S,i ∈ Ps
∑ ys ≤ NS
s∈S
xi ∈ {0,1} ∀i ∈ P
ys ∈ {0,1} ∀s ∈ S.
</table>
<bodyText confidence="0.998846833333333">
A vector of binary variables x ∈ {0,1}|P |indicates
if each phrase is to be part of the output. The vector
of auxiliary binary variables y ∈ {0,1}|S |indicates
from which sentences the chosen phrases come, see
Equation (1g).
Our objective function (1a) is the weighted sum of
two components for each phrase: a salience score,
and a measure of how frequently the QG rule was
seen in the training data. Let fi denote the salience
score for phrase i, determined by the machine learn-
ing algorithm. We apply a paraphrase penalty gi to
each phrase,
</bodyText>
<equation confidence="0.968976333333333">
nr
gi = log ,
Nr
</equation>
<bodyText confidence="0.999943846153846">
where nr is a count of the number of times this par-
ticular QG rule r was seen in the training data, and
Nr is the number of times all suitable rules for this
phrase node were seen. If no suitable rules exist,
we set gi = 0. The intuition here is that common
paraphrases should be more trustworthy, and thus
are given a smaller penalty than rare ones. Para-
phrase penalties are weighted by the constant param-
eter λ. which controls the amount of paraphrasing
we allow in the output. The objective function is
the sum of the salience scores and paraphrase penal-
ties of all the phrases chosen to form the output of a
given document, subject to the constraints in Equa-
</bodyText>
<page confidence="0.981212">
517
</page>
<bodyText confidence="0.991913388888889">
tions (1b)–(1j). The latter provide a natural way of
describing the requirements the output must meet.
Constraints (1b) and (1c) ensure that the gener-
ated output stays within the acceptable length range
of (Lmin,Lmax) tokens. Equation (1d) is a set-
covering constraint, requiring that at least Tmin words
in T appear in the output. This is important where
we want to focus on some aspect of the source doc-
ument, for instance on the subject of an image.
Constraint (1e) ensures that the phrase dependen-
cies are respected and thus enforces grammatical
correctness. Phrases that depend on phrase i are con-
tained in the set Di. Variable xi is true, and therefore
phrase i will be included, if any of its dependents
xj E Di are true. The phrase dependency constraints,
contained in the set Di and enforced by (1e), are the
result of three principles based on the typed depen-
dency information:
</bodyText>
<listItem confidence="0.9924552">
1. Where the QG provides alternative para-
phrases, it makes sense of course to select only
one. This is controlled by constraint (1f), and
by placing all paraphrases in the set Di for the
choice node i.
2. Where there are no applicable QG rules to
guide the model, in general we require all child
nodes j of the current node i to be included in
the summary if node i is included. As excep-
tions, we allow the subtree represented by node
j to be deleted if the dependency label for the
connecting edge i —* j is of type advcl (adver-
bial clause) or some form of conj (conjunction).
3. In general, we force the parent node p of the
current node i to be included in the output if i
is, resulting in all ancestors up to the root node
being included. We allow a break, and the sub-
tree at i to be used as a stand-alone sentence, if
the PCFG parser has marked i with an S (sen-
tence) label.
</listItem>
<bodyText confidence="0.953875333333333">
Constraint (1g) tells the ILP to output a sentence if
one of its constituent phrases is chosen. Finally, (1h)
limits the output to a maximum of NS sentences.
</bodyText>
<sectionHeader confidence="0.999683" genericHeader="method">
4 Experimental Set-up
</sectionHeader>
<bodyText confidence="0.999968770833333">
As mentioned earlier we evaluated the performance
of our model on two title generation tasks, namely
headline and caption generation. In this section we
give details on the corpora and grammars we used,
model parameters and features. We also describe the
baselines used for comparison with our approach,
and explain how system output was evaluated.
Training We obtained phrase-based salience
scores using a supervised machine learning algo-
rithm. For the headline generation task, the full
DUC-03 (Task 1) corpus was used for training;
it contains 500 documents and 4 headline-style
summaries per document. For the captions, training
data was gathered from the CNN news website.2
We used 200 documents and their corresponding
captions. Sentences were first tokenized to separate
words and punctuation, and then parsed to obtain
phrases and dependencies as described in Section 3
using the Stanford parser (Klein and Manning,
2003). Document phrases were marked as positive
or negative automatically. If there was a unigram
overlap (excluding stop words) between the phrase
and any of the original title or caption, we marked
this phrase with a positive label. Non-overlapping
phrases were given negative labels.
Our feature set comprised surface features such as
sentence and paragraph position information, POS
tags, and whether high-scoring tf.idf words were
present in the phrase. Additionally, the caption train-
ing set contained features for unigram and bigram
overlap with the title. We learned the feature weights
with a linear SVM, using the software SVM-OOPS
(Woodsend and Gondzio, 2009). This tool gave us
directly the feature weights as well as support vec-
tor values, and it allowed different penalties to be
applied to positive and negative misclassifications,
enabling us to compensate for the unbalanced data
set. The penalty hyper-parameters chosen were the
ones that gave the best F-scores, using 10-fold vali-
dation.
For each of the two tasks, QG rules were extracted
from the same data used to train the SVM, resulting
in 2,910 distinct rules for headlines and 2,757 rules
for the captions. Table 1 shows that for both tasks,
the majority of rules apply to PP and NP phrases.
Both tasks involve considerable compression, but
the proportions of the rewrite operations involved in-
dicate differences in style between them. Compared
</bodyText>
<footnote confidence="0.996179">
2See http://edition.cnn.com/.
</footnote>
<page confidence="0.982741">
518
</page>
<table confidence="0.978131214285714">
Label Prop’n Proportion for Label
of set Unmod Del Ins Re-ord
PP 40% 5% 93% 12% 6%
NP 31% 5% 87% 14% 7%
S 20% 1% 96% 15% 7%
SBAR 6% 4% 95% 28% 6%
(a) Headlines
Label Prop’n Proportion for Label
of set Unmod Del Ins Re-ord
PP 30% 17% 81% 7% 4%
NP 29% 17% 76% 11% 3%
S 27% 10% 84% 16% 6%
SBAR 10% 13% 80% 16% 3%
(b) Captions
</table>
<tableCaption confidence="0.7322585">
Table 1: QG rules generated for (a) headline and
(b) caption tasks (top 4 labels shown). The columns
</tableCaption>
<bodyText confidence="0.99894996">
show label of root node, proportion of the full rule-
set, then the proportions of rules for this label in-
volving no modification, deletions, insertions and
re-orderings.
to headlines, captions involve slightly less deletion
and a higher proportion of the phrases are unmod-
ified. The QG learning mechanism also discovers
more alignments between source sentences and cap-
tions than it does for the headline task.
Title generation For the headline generation task,
we evaluated our model on a testing partition from
the DUC-04 corpus (75 documents, Task 1). For the
caption task, we used the test set (240 documents)
described in Feng and Lapata (2010a). Their corpus
was downloaded from the BBC news site and con-
tains documents, images, and their captions.3
We created and solved an ILP for each docu-
ment. For each phrase, features were extracted and
salience scores calculated from the feature weights
determined through SVM training. The distance
from the SVM hyperplane represents the salience
score. Parameters for the ILP models for the two
tasks are shown in Table 2. The X parameter was
set to 0.2 to ensure that paraphrases were included;
other parameters were chosen to capture the prop-
</bodyText>
<footnote confidence="0.9920495">
3Available from http://homepages.inf.ed.ac.uk/
s0677528/data.html.
</footnote>
<table confidence="0.999504833333333">
Parameter Headlines Captions
Min length Lmin 8 8
Max length Lmax 16 20
Min keywords Tmin 0 2
Max sentences NS 5 1
Paraphrase X 0.2 0.1
</table>
<tableCaption confidence="0.999788">
Table 2: ILP model parameters for the two tasks.
</tableCaption>
<bodyText confidence="0.996787918918919">
erties seen in the majority of the training set. Note
the maximum number of sentences allowed to form
a headline is set to 5 as some of the headlines in the
DUC dataset contained multiple sentences.
To solve the ILP model we used the ZIB Opti-
mization Suite software (Achterberg, 2007; Koch,
2004). The solution was converted into a sentence
by removing nodes not chosen from the tree rep-
resentation, then concatenating the remaining leaf
nodes in order.
Model Comparison For the headline task, we
compared our model to the DUC-04 standard base-
line of the first sentence, truncated at the first word
boundary after 75 characters; and the output of the
Topiary system (Zajic et al., 2004), which came top
in almost all measures in the DUC-04 evaluation.
In order to generate a headline, Topiary first com-
presses the lead sentence using linguistically moti-
vated heuristics and then enhances it with topic key-
words. For the captions, we compared our model
against the highest-scoring document sentence ac-
cording to the SVM and against the probabilistic
model presented in Feng and Lapata (2010a). The
latter estimates the probability of a phrase appear-
ing in the caption given the same phrase appearing
in the corresponding document and uses a language
model to select among many different surface real-
izations. The language model is adapted with prob-
abilities from an image annotation model (Feng and
Lapata, 2010b).
Evaluation We evaluated the quality of the head-
lines using ROUGE (Lin and Hovy, 2003). The
DUC-04 dataset provides four reference head-
lines per document. We report unigram overlap
(ROUGE-1) and bigram overlap (ROUGE-2) as a
means of assessing informativeness, and the longest
common subsequence (ROUGE-L) as a means of as-
</bodyText>
<page confidence="0.995903">
519
</page>
<bodyText confidence="0.999982222222222">
sessing fluency. Original DUC-04 ROUGE parame-
ters were used. We also use ROUGE to evaluate the
automatic captions with the original BBC captions
as reference.
In addition, we evaluated the generated headlines
by eliciting human judgments. Participants were
presented with a news article and its correspond-
ing headline and were asked to rate the latter along
two dimensions: informativeness (does the headline
capture the article’s most important information?),
and grammaticality (is it fluent and easy to under-
stand?). The subjects used a seven point rating scale;
an ideal system would receive high numbers for
both measures. We randomly selected twelve docu-
ments from the test set and generated headlines with
our model. We also included the output of Topiary
and the human written DUC-04 headlines as a gold
standard. We thus obtained ratings for 48 (12 x 4)
document-highlights pairs.
We elicited judgments for the generated captions
in a similar fashion. Participants were presented
with a document, an associated image, and its cap-
tion, and asked to rate the latter (using a 1–7 rating
scale) with respect to grammaticality and informa-
tiveness (does it describe succinctly the content of
the image and document?). Again, we randomly se-
lected 12 document-image pairs from the test set and
generated captions for them using the highest scor-
ing document sentence according to the SVM, our
ILP-based model, and the output of Feng and Lap-
ata’s (2010a) system. We also included the original
BBC captions as an upper bound. Both studies were
conducted over the Internet using WebExp (Keller
et al., 2009). 80 unpaid volunteers rated the head-
lines and 65 the captions, all self reported native En-
glish speakers.
</bodyText>
<sectionHeader confidence="0.999862" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999021444444445">
We report results on the headline generation task in
Figure 3, with ROUGE-1, ROUGE-2 and ROUGE-
L. In ROUGE-1 and ROUGE-L measures, the best
scores are obtained by the Topiary system, slightly
better than the lead sentence baseline, while for
ROUGE-2 the ordering is reversed. Our model does
not outperform the lead sentence or Topiary. Note
that the 95% confidence level intervals reported by
ROUGE are so large that no results are statistically
</bodyText>
<table confidence="0.959274677419355">
Lead The chances for a new, strictly secular government in
Turkey faded Wednesday.
Topiary TURKEY YILMAZ PARTY ECEVIT chances strictly
secular government faded.
ILP Bulent Ecevit needs Turkey’s two-center right parties to
hammer together secular coalition.
DUC Chance for new, secular, Turkish government fades; what
will Ecevit do now?
Source Premier-designate Bulent Ecevit needs Turkey’s two-
center right parties to hammer together a secular coali-
tion, but Tansu Ciller, the ex-premier who commands 99
votes in parliament, rebuffed him Wednesday.
Lead U.S. President Bill Clinton won South Korea’s support
Saturday for confronting.
Topiary NUCLEAR U.S. President Bill Clinton won for con-
fronting North Korea.
ILP North Koreans have denied construction site has nuclear
purpose.
DUC U.S. warns N. Korea not to waste chance for peace over
alleged nuclear site.
Source The North Koreans have denied the underground con-
struction site has any nuclear purpose, and it has de-
manded a dlrs 300 million payment for proving that.
Lead By only one vote, the center-left prime minister of Italy,
Romano Prodi.
Topiary PRODI By only one vote center left prime minister and
toppled from power.
ILP Political system changes, Italy is condemned to political
instability.
DUC Prodi loses confidence vote; will stay as caretaker until
new government.
</table>
<tableCaption confidence="0.631306846153846">
Source “Unless the Italian political system changes, Italy is con-
demned to political instability,” said Sergio Romano, a
former diplomat and political science professor.
Table 3: Example headline output.
F&amp;L The former paramedic training officer stood at the next
general election.
ILP The majority are now believing that war in Iraq was
wrong.
BBC L/Cpl Thomas Keys was shot 18 times, his inquest heard.
Source The majority of people in this country are now believing
that the war in Iraq was wrong, and I do believe we will
get support.
F&amp;L The state government of Victoria take as those tests for
</tableCaption>
<table confidence="0.961888">
cannabis.
ILP Police in Victoria have begun randomly testing drivers for
the drug ecstasy.
BBC Police say drugs like Ecstasy can be as dangerous as al-
cohol for drivers.
Source Police in the Australian state of Victoria have begun ran-
domly testing drivers for the drug ecstasy.
F&amp;L The US Government Professor Holdren called for more
than a year.
</table>
<tableCaption confidence="0.870249625">
ILP “We are experiencing dangerous human disruption of
global climate,” Professor Holdren said.
BBC Sea levels could rise by 4m over the coming century, he
warns.
Source “We are experiencing dangerous human disruption of the
global climate and we’re going to experience more,” Pro-
fessor Holdren said.
Table 4: Example caption output.
</tableCaption>
<page confidence="0.967183">
520
</page>
<figure confidence="0.800813">
Lead-1 Topiary ILP
</figure>
<figureCaption confidence="0.917669666666667">
Figure 3: ROUGE-1, ROUGE-2 and ROUGE-L re-
sults on the DUC-04 headlines for our ILP model,
the lead sentence baseline and Topiary.
</figureCaption>
<figure confidence="0.637882">
SVM F&amp;L ILP
</figure>
<figureCaption confidence="0.65566475">
Figure 4: ROUGE-1, ROUGE-2 and ROUGE-L re-
sults on the BBC captions for our ILP model, the
sentence baseline chosen by the SVM, and Feng and
Lapata’s (2010) model.
</figureCaption>
<bodyText confidence="0.999829210526316">
significant. We also investigated using an ILP model
with just the QG rules or just dependency label in-
formation (see constraint (1e) in Section 3.3). Both
settings gave less compressed output, and the result-
ing ROUGE scores were lower on all measures. The
ROUGE results for the caption generation task fol-
low a similar pattern (see Figure 4). Our model is
slightly better than the best sentence baseline but
performs worse than Feng and Lapata (2010a). Ta-
bles 3 and 4 show example output for the ILP model
and the baselines on the headline and caption tasks
respectively. In the tables, Source refers to the sen-
tence chosen by the ILP, but before any paraphrasing
is applied. We can see that deletion rules dominate,
and a more compressive style of paraphrasing has
been learned for the headline task.
The results of our human evaluation study for
the DUC-04 headlines are summarized in Table 5.
Means differences were compared using a Post-hoc
</bodyText>
<table confidence="0.996895">
Model Grammaticality Importance
Lead-1 4.95 3.30
Topiary 3.03 3.43
ILP 5.36 4.94
Reference 5.12 5.17
</table>
<tableCaption confidence="0.98437075">
Table 5: Average human ratings of DUC-04 head-
lines, for our ILP model, the lead sentence baseline,
the output of Topiary and the human-written refer-
ence.
</tableCaption>
<table confidence="0.999845">
Model Grammaticality Importance
SVM 5.24 5.01
F&amp;L 4.42 4.74
ILP 5.49 5.25
Reference 5.61 5.18
</table>
<tableCaption confidence="0.985058">
Table 6: Average human ratings of captions, for
</tableCaption>
<bodyText confidence="0.997288310344827">
our ILP model, the sentence baseline chosen by the
SVM, Feng and Lapata’s (2010) model and the ref-
erence BBC caption.
Tukey test. The headlines created by our model
were considered significantly more important and
more grammatical than those of the Topiary sys-
tem (a &lt; 0.01), despite the better overlap of Topi-
ary with the reference headlines as indicated in the
Rouge results above. Compared to the lead sentence
of the article (the DUC-04 baseline), our model was
also rated significantly higher in terms of importance
(a &lt; 0.01) but not grammaticality.
Table 6 summarizes the results of our second
judgment elicitation study. The captions generated
by our model are significantly more grammatical
than those of Feng and Lapata (2010a) (a &lt; 0.01).
The SVM, ILP model and reference captions do not
differ significantly in terms of grammaticality. In
terms of importance, the ILP model is significantly
better than the SVM (a &lt; 0.01) and Feng and Lap-
ata (a &lt; 0.01) and comparable to the reference.
The human ratings are more favorable to our
model than ROUGE for both tasks. There are two
reasons for this. Firstly, the model is not bi-
ased towards selecting the lead sentence as a head-
line/caption and is disadvantaged in ROUGE evalua-
tions as professional abstractors often reuse the lead
or parts of it to create a title. Secondly, the model
often generates an appropriate title that is lexically
</bodyText>
<figure confidence="0.999729631578947">
Rouge-1
Rouge-L
Rouge-2
Score
0.25
0.15
0.05
0.3
0.2
0.1
0
Rouge-1
Rouge-L
Rouge-2
Score 0.2
0.15
0.1
0.05
0
</figure>
<page confidence="0.990824">
521
</page>
<bodyText confidence="0.992602">
distinct from the reference even though it expresses
similar meaning.
</bodyText>
<sectionHeader confidence="0.999254" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99972962745098">
In this paper we proposed a joint content se-
lection and surface realization model for single-
document summarization. The model operates over
a syntax-rich representation of the source docu-
ment and learns which phrases should be in the
summary. Content selection preferences are cou-
pled with a quasi-synchronous grammar whose rules
encode surface realization preferences (e.g., para-
phrases and compressions). Both types of prefer-
ences are optimized simultaneously in an integer lin-
ear program subject to grammaticality, length and
coverage constraints. Importantly, the QG allows
the model to adapt to the writing and stylistic con-
ventions of different tasks. The results of our hu-
man studies show that our system creates grammati-
cal and informative summaries whilst outperforming
several competitive baselines.
The model itself is relatively simple and achieves
good performance without any task-specific modifi-
cation. One potential stumbling block may be the
availability of parallel data for acquiring the QG.
The Internet provides a large repository of news
documents with headlines, images and captions. In
some cases news articles are even accompanied with
“story highlights” which could be used as training
data for longer summaries.4 For other domains ob-
taining such data may be more difficult. However,
our experiments have shown that relatively small
parallel corpora (in the range of 200–500 pairs) suf-
fice to learn many of the writing conventions for a
given task.
In the future, we plan to explore how to inte-
grate more sophisticated QG rules in the generation
process. Currently we consider deletions, reorder-
ings and insertions. Ideally, we would also like to
model arbitrary substitutions between words but also
larger constituents (e.g., subclauses, sentence aggre-
gation). Beyond summarization, we would also like
to apply our model to other generation tasks, such as
paraphrasing and text simplification.
4On-line CNN news articles are prefaced by story
highlights—three or four short sentences that are written by hu-
mans and give a brief overview of the article.
Acknowledgments We are grateful to David Chi-
ang and Noah Smith for their input on earlier ver-
sions of this work. We would also like to thank
Andreas Grothey and members of ICCS at the
School of Informatics for valuable discussions and
comments. We acknowledge the support of EP-
SRC through project grants EP/F055765/1 and
GR/T04540/01.
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999655825">
Achterberg, Tobias. 2007. Constraint Integer Program-
ming. Ph.D. thesis, Technische Universit¨at Berlin.
Banko, Michele, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statisti-
cal translation. In Proceedings of the 38th ACL. Hong
Kong, pages 318–325.
Clarke, James and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research 31:399–429.
Cohn, Trevor and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd COLING. Manchester, UK, pages 137–144.
Das, Dipanjan and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the ACL-IJCNLP.
Suntec, Singapore, pages 468–476.
Daum´e III, Hal. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California.
Daum´e III, Hal and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th ACL. Philadelphia, PA, pages 449–456.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 2003 Text Summarization Workshop and Doc-
ument Understanding Conference. Edmondon, Al-
berta, pages 1–8.
Dras, Mark. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text.. Ph.D. thesis, Macquarie
University.
Feng, Yansong and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics, Uppsala, Sweden, pages 1239–1249.
Feng, Yansong and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In Pro-
</reference>
<page confidence="0.971261">
522
</page>
<reference confidence="0.999846405063291">
ceedings of the NAACL HLT. Association for Com-
putational Linguistics, Los Angeles, California, pages
831–839.
Jing, Hongyan. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the 6th ANLP.
Seattle, WA, pages 310–315.
Jing, Hongyan. 2002. Using hidden Markov modeling to
decompose human-written summaries. Computational
Linguistics 28(4):527–544.
Jing, Hongyan and Kathleen McKeown. 2000. Cut
and paste summarization. In Proceedings of the 1st
NAACL. Seattle, WA, pages 178–185.
Keller, Frank, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of web experi-
ments: A case study using the WebExp software pack-
age. Behavior Research Methods 41(1):1–12.
Klein, Dan and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st ACL.
Sapporo, Japan, pages 423–430.
Koch, Thorsten. 2004. Rapid Mathematical Prototyping.
Ph.D. thesis, Technische Universit¨at Berlin.
Kupiec, Julian, Jan O. Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Proceed-
ings of SIGIR-95. Seattle, WA, pages 68–73.
Lin, Chin-Yew. 2003. Improving summarization perfor-
mance by sentence compression — a pilot study. In
Proceedings of the 6th International Workshop on In-
formation Retrieval with Asian Languages. Sapporo,
Japan, pages 1–8.
Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT NAACL. Edmonton,
Canada, pages 71–78.
Martins, Andr´e and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing. Boulder, Colorado, pages 1–9.
Smith, David and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings on the Workshop on Sta-
tistical Machine Translation. Association for Compu-
tational Linguistics, New York City, pages 23–30.
Smith, David A. and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the EMNLP. Suntec, Sin-
gapore, pages 822–831.
Soricut, R. and D. Marcu. 2007. Abstractive head-
line generation using WIDL-expressions. Information
Processing and Management 43(6):1536–1548. Text
Summarization.
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
EMNLP-CoNLL. Prague, Czech Republic, pages 22–
32.
Woodsend, Kristian and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector ma-
chine training. Computational Optimization and Ap-
plications Published online.
Woodsend, Kristian and Mirella Lapata. 2010. Automatic
generation of story highlights. In Sandra Carberry and
Stephen Clark, editors, Proceedings of the 48th ACL.
Uppsala, Sweden, pages 565–574.
Zajic, David, Bonnie Dorr, and Richard Schwartz. 2004.
BBN/UMD at DUC-2004: Topiary. In Proceedings
of the NAACL Workshop on Document Understanding.
Boston, MA, pages 112–119.
Zajic, David, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. Information Processing Management Special Is-
sue on Summarization 43(6):1549–1570.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP. Suntec, Singapore, pages 834–842.
</reference>
<page confidence="0.998929">
523
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942677">
<title confidence="0.999817">Generation with Quasi-Synchronous Grammar</title>
<author confidence="0.992444">Kristian Woodsend</author>
<author confidence="0.992444">Yansong Feng</author>
<author confidence="0.992444">Mirella</author>
<affiliation confidence="0.998874">School of Informatics, University of</affiliation>
<address confidence="0.989509">Edinburgh EH8 9AB, United Kingdom</address>
<email confidence="0.970017">k.woodsend@ed.ac.uk,Y.Feng-4@sms.ed.ac.uk,mlap@inf.ed.ac.uk</email>
<abstract confidence="0.999529869565218">The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions. Based on an integer linear programming formulation, the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met. Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tobias Achterberg</author>
</authors>
<title>Constraint Integer Programming.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Technische Universit¨at Berlin.</institution>
<contexts>
<context position="26190" citStr="Achterberg, 2007" startWordPosition="4336" endWordPosition="4337">e that paraphrases were included; other parameters were chosen to capture the prop3Available from http://homepages.inf.ed.ac.uk/ s0677528/data.html. Parameter Headlines Captions Min length Lmin 8 8 Max length Lmax 16 20 Min keywords Tmin 0 2 Max sentences NS 5 1 Paraphrase X 0.2 0.1 Table 2: ILP model parameters for the two tasks. erties seen in the majority of the training set. Note the maximum number of sentences allowed to form a headline is set to 5 as some of the headlines in the DUC dataset contained multiple sentences. To solve the ILP model we used the ZIB Optimization Suite software (Achterberg, 2007; Koch, 2004). The solution was converted into a sentence by removing nodes not chosen from the tree representation, then concatenating the remaining leaf nodes in order. Model Comparison For the headline task, we compared our model to the DUC-04 standard baseline of the first sentence, truncated at the first word boundary after 75 characters; and the output of the Topiary system (Zajic et al., 2004), which came top in almost all measures in the DUC-04 evaluation. In order to generate a headline, Topiary first compresses the lead sentence using linguistically motivated heuristics and then enha</context>
</contexts>
<marker>Achterberg, 2007</marker>
<rawString>Achterberg, Tobias. 2007. Constraint Integer Programming. Ph.D. thesis, Technische Universit¨at Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu O Mittal</author>
<author>Michael J Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th ACL. Hong Kong,</booktitle>
<pages>318--325</pages>
<contexts>
<context position="2472" citStr="Banko et al., 2000" startWordPosition="359" endWordPosition="362"> this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and combining them into new sentences guided by a language model (Banko et al., 2000; Soricut and Marcu, 2007). This approach has the potential to work well, breaking out of the single-sentence paradigm. Unfortunately, the resulting summaries are not always coherent — individual constituent phrases are often combined without any semantic constraints — or grammatical beyond the n-gram horizon imposed by the language model. Constituent deletion and recombination are merely two of the many rewrite operations professional editors and abstractors employ when creating summaries (Jing, 2002). Additional operations include truncating sentences, aggregating them, and paraphrasing at w</context>
<context position="8429" citStr="Banko et al. (2000)" startWordPosition="1273" endWordPosition="1276">nre) and subsequently apply a form of sentence compression such that the headline meets some length requirement (Dorr 1Approaches to headline generation are too numerous to list in detail; see the proceedings of DUC-03 and DUC-04 for an overview. 514 et al., 2003). The compressed sentence may also be “padded” with important content words or phrases to ensure that the topic of the document is covered (Zajic et al., 2004). Other work generates headlines in a bottom-up fashion starting from important, individual words and phrases, that are glued together to create a fluent sentence. For example, Banko et al. (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Relatively little work has focused on caption generation, a task related to headline generation. The aim here is to create a short, title-like description of an image embedded in a news article. Like headlines, captions have to be short and informative. In addition, a good caption must clearly identify the subject of the picture and establish its relevance to the article. Feng and Lapata (2010a) develop extractive and abstractive caption generation models t</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Banko, Michele, Vibhu O. Mittal, and Michael J. Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th ACL. Hong Kong, pages 318–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research</journal>
<pages>31--399</pages>
<contexts>
<context position="6971" citStr="Clarke and Lapata, 2008" startWordPosition="1045" endWordPosition="1048">h document sentence if it is worth extracting. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words are deleted and sentences selected from a document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Headline generation is a well-studi</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>Clarke, James and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal of Artificial Intelligence Research 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd COLING.</booktitle>
<pages>137--144</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="7380" citStr="Cohn and Lapata (2008)" startWordPosition="1107" endWordPosition="1110">document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Headline generation is a well-studied task within single-document summarization, due to its prominence in the DUC-03 and DUC-04 evaluation competitions.1 Many approaches identify the most informative sentence in a given document (typically the first sentence for the news genre) and subsequently apply a form of sentence compression such that the headline meets some length requirement (Dorr 1Approaches to headline generation are too numerous </context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Cohn, Trevor and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of the 22nd COLING. Manchester, UK, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP. Suntec, Singapore,</booktitle>
<pages>468--476</pages>
<contexts>
<context position="9625" citStr="Das and Smith, 2009" startWordPosition="1460" endWordPosition="1463">tion generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), and question answering (Wang et al., 2007); however the use of QG in summarization is novel to our knowledge. Unlike most synchronous grammar formalisms, QG does not posit a strict isomorphism between a source sentence and its target translation; it only loosely links the syntactic structure of the two, and is therefore well suited to describing the relationship between a document and its abstract. We propose an ILP formulation which not only allows to efficiently search through the space of many QG rules but also to incorporate constraints relating to content, style, and the task at hand. 3</context>
<context position="13736" citStr="Das and Smith 2009" startWordPosition="2142" endWordPosition="2145">hrough applying other QG rules on children). Nodes bear phrase and dependency labels. Dotted lines show alignments in the grammar between source and target child nodes. Examples are taken from the QG rules discovered in the DUC03 data set of headlines. source; this helps to improve the quality in what is inherently a noisy process. Finally, QG rules are created from aligned nodes above the leaf node level, recording the phrase and dependency label of nodes, and the alignment of child nodes. Unlike previous work involving QG which has used dependency graphs exclusively (e.g., Wang et al. 2007; Das and Smith 2009), our approach operates over a combined PCFG-dependency representation. As a result, some configurations in Smith and Eisner (2006) are not so relevant here — instead, we found that deletions, reorderings, flattening of nodes, and the addition of text elements were important operations for the grammar. Figure 1 shows some example alignments that are captured by the QG, with the source node on the left and the target node on the right. Leaf nodes have their original text, while other nodes have a combined phrase and dependency label that they obtain in the merged representation described in Sec</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Das, Dipanjan and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the ACL-IJCNLP. Suntec, Singapore, pages 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
</authors>
<title>Practical Structured Learning Techniques for Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<marker>Hal, 2006</marker>
<rawString>Daum´e III, Hal. 2006. Practical Structured Learning Techniques for Natural Language Processing. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL.</booktitle>
<pages>449--456</pages>
<location>Philadelphia, PA,</location>
<marker>Hal, Marcu, 2002</marker>
<rawString>Daum´e III, Hal and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of the 40th ACL. Philadelphia, PA, pages 449–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: A parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLTNAACL 2003 Text Summarization Workshop and Document Understanding Conference. Edmondon,</booktitle>
<pages>1--8</pages>
<location>Alberta,</location>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Dorr, Bonnie, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLTNAACL 2003 Text Summarization Workshop and Document Understanding Conference. Edmondon, Alberta, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>Tree Adjoining Grammar and the Reluctant Paraphrasing of Text..</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University.</institution>
<contexts>
<context position="6984" citStr="Dras (1999)" startWordPosition="1049" endWordPosition="1050">is worth extracting. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words are deleted and sentences selected from a document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Headline generation is a well-studied task withi</context>
</contexts>
<marker>Dras, 1999</marker>
<rawString>Dras, Mark. 1999. Tree Adjoining Grammar and the Reluctant Paraphrasing of Text.. Ph.D. thesis, Macquarie University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? Automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>1239--1249</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="8964" citStr="Feng and Lapata (2010" startWordPosition="1357" endWordPosition="1360"> that are glued together to create a fluent sentence. For example, Banko et al. (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Relatively little work has focused on caption generation, a task related to headline generation. The aim here is to create a short, title-like description of an image embedded in a news article. Like headlines, captions have to be short and informative. In addition, a good caption must clearly identify the subject of the picture and establish its relevance to the article. Feng and Lapata (2010a) develop extractive and abstractive caption generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and E</context>
<context position="18079" citStr="Feng and Lapata (2010" startWordPosition="2896" endWordPosition="2899">e nodes throughout the document, which represent nodes in the tree where more than one QG rule can be applied; Ci ⊂ P,i ∈ C are the sets of phrases that are direct children of each choice node, in other words they are the individual alternative paraphrases. Let li be the length of each phrase i, in tokens. For caption generation, the model has as additional input a list of tags (keywords drawn from the source document) that correspond to the image, and we refer to this set of tags as T . Pt ⊂ P is the set of phrases containing the tag t ∈ T . We use the probabilistic image annotation model of Feng and Lapata (2010a) to generate the list of keywords. The latter highlight the objects depicted in the image and should be in all likelihood included in the caption. The model is cast as an integer linear program: max ∑ ( fi + λgi)xi x i∈P s.t. ∑ lixi ≤ Lmax i∈P ∑ lixi ≥ Lmin i∈P ∑ xi ≥ Tmin i∈Pt,t∈T xj → xi ∀i ∈ P, j ∈ Di ∑ xj = xi ∀i ∈ C, j ∈ Ci j∈Ci xi → ys ∀s ∈ S,i ∈ Ps ∑ ys ≤ NS s∈S xi ∈ {0,1} ∀i ∈ P ys ∈ {0,1} ∀s ∈ S. A vector of binary variables x ∈ {0,1}|P |indicates if each phrase is to be part of the output. The vector of auxiliary binary variables y ∈ {0,1}|S |indicates from which sentences the chos</context>
<context position="25111" citStr="Feng and Lapata (2010" startWordPosition="4153" endWordPosition="4156">proportion of the full ruleset, then the proportions of rules for this label involving no modification, deletions, insertions and re-orderings. to headlines, captions involve slightly less deletion and a higher proportion of the phrases are unmodified. The QG learning mechanism also discovers more alignments between source sentences and captions than it does for the headline task. Title generation For the headline generation task, we evaluated our model on a testing partition from the DUC-04 corpus (75 documents, Task 1). For the caption task, we used the test set (240 documents) described in Feng and Lapata (2010a). Their corpus was downloaded from the BBC news site and contains documents, images, and their captions.3 We created and solved an ILP for each document. For each phrase, features were extracted and salience scores calculated from the feature weights determined through SVM training. The distance from the SVM hyperplane represents the salience score. Parameters for the ILP models for the two tasks are shown in Table 2. The X parameter was set to 0.2 to ensure that paraphrases were included; other parameters were chosen to capture the prop3Available from http://homepages.inf.ed.ac.uk/ s0677528</context>
<context position="26996" citStr="Feng and Lapata (2010" startWordPosition="4466" endWordPosition="4469">on For the headline task, we compared our model to the DUC-04 standard baseline of the first sentence, truncated at the first word boundary after 75 characters; and the output of the Topiary system (Zajic et al., 2004), which came top in almost all measures in the DUC-04 evaluation. In order to generate a headline, Topiary first compresses the lead sentence using linguistically motivated heuristics and then enhances it with topic keywords. For the captions, we compared our model against the highest-scoring document sentence according to the SVM and against the probabilistic model presented in Feng and Lapata (2010a). The latter estimates the probability of a phrase appearing in the caption given the same phrase appearing in the corresponding document and uses a language model to select among many different surface realizations. The language model is adapted with probabilities from an image annotation model (Feng and Lapata, 2010b). Evaluation We evaluated the quality of the headlines using ROUGE (Lin and Hovy, 2003). The DUC-04 dataset provides four reference headlines per document. We report unigram overlap (ROUGE-1) and bigram overlap (ROUGE-2) as a means of assessing informativeness, and the longest</context>
<context position="33205" citStr="Feng and Lapata (2010" startWordPosition="5473" endWordPosition="5476">F&amp;L ILP Figure 4: ROUGE-1, ROUGE-2 and ROUGE-L results on the BBC captions for our ILP model, the sentence baseline chosen by the SVM, and Feng and Lapata’s (2010) model. significant. We also investigated using an ILP model with just the QG rules or just dependency label information (see constraint (1e) in Section 3.3). Both settings gave less compressed output, and the resulting ROUGE scores were lower on all measures. The ROUGE results for the caption generation task follow a similar pattern (see Figure 4). Our model is slightly better than the best sentence baseline but performs worse than Feng and Lapata (2010a). Tables 3 and 4 show example output for the ILP model and the baselines on the headline and caption tasks respectively. In the tables, Source refers to the sentence chosen by the ILP, but before any paraphrasing is applied. We can see that deletion rules dominate, and a more compressive style of paraphrasing has been learned for the headline task. The results of our human evaluation study for the DUC-04 headlines are summarized in Table 5. Means differences were compared using a Post-hoc Model Grammaticality Importance Lead-1 4.95 3.30 Topiary 3.03 3.43 ILP 5.36 4.94 Reference 5.12 5.17 Tab</context>
<context position="34833" citStr="Feng and Lapata (2010" startWordPosition="5743" endWordPosition="5746">on. Tukey test. The headlines created by our model were considered significantly more important and more grammatical than those of the Topiary system (a &lt; 0.01), despite the better overlap of Topiary with the reference headlines as indicated in the Rouge results above. Compared to the lead sentence of the article (the DUC-04 baseline), our model was also rated significantly higher in terms of importance (a &lt; 0.01) but not grammaticality. Table 6 summarizes the results of our second judgment elicitation study. The captions generated by our model are significantly more grammatical than those of Feng and Lapata (2010a) (a &lt; 0.01). The SVM, ILP model and reference captions do not differ significantly in terms of grammaticality. In terms of importance, the ILP model is significantly better than the SVM (a &lt; 0.01) and Feng and Lapata (a &lt; 0.01) and comparable to the reference. The human ratings are more favorable to our model than ROUGE for both tasks. There are two reasons for this. Firstly, the model is not biased towards selecting the lead sentence as a headline/caption and is disadvantaged in ROUGE evaluations as professional abstractors often reuse the lead or parts of it to create a title. Secondly, th</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Feng, Yansong and Mirella Lapata. 2010a. How many words is a picture worth? Automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Uppsala, Sweden, pages 1239–1249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT. Association for Computational Linguistics,</booktitle>
<pages>831--839</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="8964" citStr="Feng and Lapata (2010" startWordPosition="1357" endWordPosition="1360"> that are glued together to create a fluent sentence. For example, Banko et al. (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Relatively little work has focused on caption generation, a task related to headline generation. The aim here is to create a short, title-like description of an image embedded in a news article. Like headlines, captions have to be short and informative. In addition, a good caption must clearly identify the subject of the picture and establish its relevance to the article. Feng and Lapata (2010a) develop extractive and abstractive caption generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and E</context>
<context position="18079" citStr="Feng and Lapata (2010" startWordPosition="2896" endWordPosition="2899">e nodes throughout the document, which represent nodes in the tree where more than one QG rule can be applied; Ci ⊂ P,i ∈ C are the sets of phrases that are direct children of each choice node, in other words they are the individual alternative paraphrases. Let li be the length of each phrase i, in tokens. For caption generation, the model has as additional input a list of tags (keywords drawn from the source document) that correspond to the image, and we refer to this set of tags as T . Pt ⊂ P is the set of phrases containing the tag t ∈ T . We use the probabilistic image annotation model of Feng and Lapata (2010a) to generate the list of keywords. The latter highlight the objects depicted in the image and should be in all likelihood included in the caption. The model is cast as an integer linear program: max ∑ ( fi + λgi)xi x i∈P s.t. ∑ lixi ≤ Lmax i∈P ∑ lixi ≥ Lmin i∈P ∑ xi ≥ Tmin i∈Pt,t∈T xj → xi ∀i ∈ P, j ∈ Di ∑ xj = xi ∀i ∈ C, j ∈ Ci j∈Ci xi → ys ∀s ∈ S,i ∈ Ps ∑ ys ≤ NS s∈S xi ∈ {0,1} ∀i ∈ P ys ∈ {0,1} ∀s ∈ S. A vector of binary variables x ∈ {0,1}|P |indicates if each phrase is to be part of the output. The vector of auxiliary binary variables y ∈ {0,1}|S |indicates from which sentences the chos</context>
<context position="25111" citStr="Feng and Lapata (2010" startWordPosition="4153" endWordPosition="4156">proportion of the full ruleset, then the proportions of rules for this label involving no modification, deletions, insertions and re-orderings. to headlines, captions involve slightly less deletion and a higher proportion of the phrases are unmodified. The QG learning mechanism also discovers more alignments between source sentences and captions than it does for the headline task. Title generation For the headline generation task, we evaluated our model on a testing partition from the DUC-04 corpus (75 documents, Task 1). For the caption task, we used the test set (240 documents) described in Feng and Lapata (2010a). Their corpus was downloaded from the BBC news site and contains documents, images, and their captions.3 We created and solved an ILP for each document. For each phrase, features were extracted and salience scores calculated from the feature weights determined through SVM training. The distance from the SVM hyperplane represents the salience score. Parameters for the ILP models for the two tasks are shown in Table 2. The X parameter was set to 0.2 to ensure that paraphrases were included; other parameters were chosen to capture the prop3Available from http://homepages.inf.ed.ac.uk/ s0677528</context>
<context position="26996" citStr="Feng and Lapata (2010" startWordPosition="4466" endWordPosition="4469">on For the headline task, we compared our model to the DUC-04 standard baseline of the first sentence, truncated at the first word boundary after 75 characters; and the output of the Topiary system (Zajic et al., 2004), which came top in almost all measures in the DUC-04 evaluation. In order to generate a headline, Topiary first compresses the lead sentence using linguistically motivated heuristics and then enhances it with topic keywords. For the captions, we compared our model against the highest-scoring document sentence according to the SVM and against the probabilistic model presented in Feng and Lapata (2010a). The latter estimates the probability of a phrase appearing in the caption given the same phrase appearing in the corresponding document and uses a language model to select among many different surface realizations. The language model is adapted with probabilities from an image annotation model (Feng and Lapata, 2010b). Evaluation We evaluated the quality of the headlines using ROUGE (Lin and Hovy, 2003). The DUC-04 dataset provides four reference headlines per document. We report unigram overlap (ROUGE-1) and bigram overlap (ROUGE-2) as a means of assessing informativeness, and the longest</context>
<context position="33205" citStr="Feng and Lapata (2010" startWordPosition="5473" endWordPosition="5476">F&amp;L ILP Figure 4: ROUGE-1, ROUGE-2 and ROUGE-L results on the BBC captions for our ILP model, the sentence baseline chosen by the SVM, and Feng and Lapata’s (2010) model. significant. We also investigated using an ILP model with just the QG rules or just dependency label information (see constraint (1e) in Section 3.3). Both settings gave less compressed output, and the resulting ROUGE scores were lower on all measures. The ROUGE results for the caption generation task follow a similar pattern (see Figure 4). Our model is slightly better than the best sentence baseline but performs worse than Feng and Lapata (2010a). Tables 3 and 4 show example output for the ILP model and the baselines on the headline and caption tasks respectively. In the tables, Source refers to the sentence chosen by the ILP, but before any paraphrasing is applied. We can see that deletion rules dominate, and a more compressive style of paraphrasing has been learned for the headline task. The results of our human evaluation study for the DUC-04 headlines are summarized in Table 5. Means differences were compared using a Post-hoc Model Grammaticality Importance Lead-1 4.95 3.30 Topiary 3.03 3.43 ILP 5.36 4.94 Reference 5.12 5.17 Tab</context>
<context position="34833" citStr="Feng and Lapata (2010" startWordPosition="5743" endWordPosition="5746">on. Tukey test. The headlines created by our model were considered significantly more important and more grammatical than those of the Topiary system (a &lt; 0.01), despite the better overlap of Topiary with the reference headlines as indicated in the Rouge results above. Compared to the lead sentence of the article (the DUC-04 baseline), our model was also rated significantly higher in terms of importance (a &lt; 0.01) but not grammaticality. Table 6 summarizes the results of our second judgment elicitation study. The captions generated by our model are significantly more grammatical than those of Feng and Lapata (2010a) (a &lt; 0.01). The SVM, ILP model and reference captions do not differ significantly in terms of grammaticality. In terms of importance, the ILP model is significantly better than the SVM (a &lt; 0.01) and Feng and Lapata (a &lt; 0.01) and comparable to the reference. The human ratings are more favorable to our model than ROUGE for both tasks. There are two reasons for this. Firstly, the model is not biased towards selecting the lead sentence as a headline/caption and is disadvantaged in ROUGE evaluations as professional abstractors often reuse the lead or parts of it to create a title. Secondly, th</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Feng, Yansong and Mirella Lapata. 2010b. Topic models for image annotation and text illustration. In Proceedings of the NAACL HLT. Association for Computational Linguistics, Los Angeles, California, pages 831–839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th ANLP.</booktitle>
<pages>310--315</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="2159" citStr="Jing, 2000" startWordPosition="311" endWordPosition="312">ated by identifying and subsequently concatenating the most important sentences in a document. The advantage of this approach is that it does not require a great deal of linguistic analysis to generate grammatical sentences, assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and combining them into new sentences guided by a language model (Banko et al., 2000; Soricut and Marcu, 2007). This approach has the potential to work well, breaking out of the single-sentence paradigm. Unfortunately, the resulting summaries are not always coherent — individual constituent phrases are often combined without any semantic constraints — or grammatical be</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Jing, Hongyan. 2000. Sentence reduction for automatic text summarization. In Proceedings of the 6th ANLP. Seattle, WA, pages 310–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden Markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="2979" citStr="Jing, 2002" startWordPosition="437" endWordPosition="438">in the source text, and combining them into new sentences guided by a language model (Banko et al., 2000; Soricut and Marcu, 2007). This approach has the potential to work well, breaking out of the single-sentence paradigm. Unfortunately, the resulting summaries are not always coherent — individual constituent phrases are often combined without any semantic constraints — or grammatical beyond the n-gram horizon imposed by the language model. Constituent deletion and recombination are merely two of the many rewrite operations professional editors and abstractors employ when creating summaries (Jing, 2002). Additional operations include truncating sentences, aggregating them, and paraphrasing at word or syntax level. Furthermore, professionals write summaries in a task-specific style. News headlines for example are typically short (three to six words), written in the present tense and active voice, and often leave out forms of the verb be. There are also different ways of writing a headline either directly by stating what the docu513 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 513–523, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Associati</context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Jing, Hongyan. 2002. Using hidden Markov modeling to decompose human-written summaries. Computational Linguistics 28(4):527–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
</authors>
<title>Cut and paste summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st NAACL.</booktitle>
<pages>178--185</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="7333" citStr="Jing and McKeown, 2000" startWordPosition="1100" endWordPosition="1103">words are deleted and sentences selected from a document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Headline generation is a well-studied task within single-document summarization, due to its prominence in the DUC-03 and DUC-04 evaluation competitions.1 Many approaches identify the most informative sentence in a given document (typically the first sentence for the news genre) and subsequently apply a form of sentence compression such that the headline meets some length requirement (Dorr 1Appr</context>
</contexts>
<marker>Jing, McKeown, 2000</marker>
<rawString>Jing, Hongyan and Kathleen McKeown. 2000. Cut and paste summarization. In Proceedings of the 1st NAACL. Seattle, WA, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Subahshini Gunasekharan</author>
<author>Neil Mayo</author>
<author>Martin Corley</author>
</authors>
<title>Timing accuracy of web experiments: A case study using the WebExp software package.</title>
<date>2009</date>
<journal>Behavior Research Methods</journal>
<volume>41</volume>
<issue>1</issue>
<contexts>
<context position="29250" citStr="Keller et al., 2009" startWordPosition="4828" endWordPosition="4831">nted with a document, an associated image, and its caption, and asked to rate the latter (using a 1–7 rating scale) with respect to grammaticality and informativeness (does it describe succinctly the content of the image and document?). Again, we randomly selected 12 document-image pairs from the test set and generated captions for them using the highest scoring document sentence according to the SVM, our ILP-based model, and the output of Feng and Lapata’s (2010a) system. We also included the original BBC captions as an upper bound. Both studies were conducted over the Internet using WebExp (Keller et al., 2009). 80 unpaid volunteers rated the headlines and 65 the captions, all self reported native English speakers. 5 Results We report results on the headline generation task in Figure 3, with ROUGE-1, ROUGE-2 and ROUGEL. In ROUGE-1 and ROUGE-L measures, the best scores are obtained by the Topiary system, slightly better than the lead sentence baseline, while for ROUGE-2 the ordering is reversed. Our model does not outperform the lead sentence or Topiary. Note that the 95% confidence level intervals reported by ROUGE are so large that no results are statistically Lead The chances for a new, strictly s</context>
</contexts>
<marker>Keller, Gunasekharan, Mayo, Corley, 2009</marker>
<rawString>Keller, Frank, Subahshini Gunasekharan, Neil Mayo, and Martin Corley. 2009. Timing accuracy of web experiments: A case study using the WebExp software package. Behavior Research Methods 41(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st ACL.</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="11417" citStr="Klein and Manning, 2003" startWordPosition="1753" endWordPosition="1756">nformation which we obtain by parsing every sentence twice, once with a phrase structure parser and once with a dependency parser. The output from the two representations is combined into a single data structure, by mapping the dependencies to the edges of the phrase structure tree. The procedure is described in detail in Woodsend and Lapata (2010). However, we do not merge the leaf nodes into phrases here, but keep the full tree structure, as we will apply compression to phrases through the QG. In our experiments, we obtain this combined representation from the output of the Stanford parser (Klein and Manning, 2003) but any other broadly similar parser could be used instead. 3.2 Quasi-synchronous grammar Given an input sentence S1 or its parse tree T1, the QG constructs a monolingual grammar for parsing, or generating, the possible translation (or here, paraphrase) trees T2. A grammar node in the target tree T2 is modeled on a subset of nodes in the source tree, with a rather loose alignment between the trees. In our approach, the process of learning the grammar is unsupervised. Each sentence of the source document is compared to each sentence in the target document — headline or caption, depending on th</context>
<context position="22592" citStr="Klein and Manning, 2003" startWordPosition="3729" endWordPosition="3732">nd explain how system output was evaluated. Training We obtained phrase-based salience scores using a supervised machine learning algorithm. For the headline generation task, the full DUC-03 (Task 1) corpus was used for training; it contains 500 documents and 4 headline-style summaries per document. For the captions, training data was gathered from the CNN news website.2 We used 200 documents and their corresponding captions. Sentences were first tokenized to separate words and punctuation, and then parsed to obtain phrases and dependencies as described in Section 3 using the Stanford parser (Klein and Manning, 2003). Document phrases were marked as positive or negative automatically. If there was a unigram overlap (excluding stop words) between the phrase and any of the original title or caption, we marked this phrase with a positive label. Non-overlapping phrases were given negative labels. Our feature set comprised surface features such as sentence and paragraph position information, POS tags, and whether high-scoring tf.idf words were present in the phrase. Additionally, the caption training set contained features for unigram and bigram overlap with the title. We learned the feature weights with a lin</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st ACL. Sapporo, Japan, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Koch</author>
</authors>
<date>2004</date>
<booktitle>Rapid Mathematical Prototyping. Ph.D. thesis,</booktitle>
<institution>Technische Universit¨at Berlin.</institution>
<contexts>
<context position="26203" citStr="Koch, 2004" startWordPosition="4338" endWordPosition="4339"> were included; other parameters were chosen to capture the prop3Available from http://homepages.inf.ed.ac.uk/ s0677528/data.html. Parameter Headlines Captions Min length Lmin 8 8 Max length Lmax 16 20 Min keywords Tmin 0 2 Max sentences NS 5 1 Paraphrase X 0.2 0.1 Table 2: ILP model parameters for the two tasks. erties seen in the majority of the training set. Note the maximum number of sentences allowed to form a headline is set to 5 as some of the headlines in the DUC dataset contained multiple sentences. To solve the ILP model we used the ZIB Optimization Suite software (Achterberg, 2007; Koch, 2004). The solution was converted into a sentence by removing nodes not chosen from the tree representation, then concatenating the remaining leaf nodes in order. Model Comparison For the headline task, we compared our model to the DUC-04 standard baseline of the first sentence, truncated at the first word boundary after 75 characters; and the output of the Topiary system (Zajic et al., 2004), which came top in almost all measures in the DUC-04 evaluation. In order to generate a headline, Topiary first compresses the lead sentence using linguistically motivated heuristics and then enhances it with </context>
</contexts>
<marker>Koch, 2004</marker>
<rawString>Koch, Thorsten. 2004. Rapid Mathematical Prototyping. Ph.D. thesis, Technische Universit¨at Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan O Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of SIGIR-95.</booktitle>
<pages>68--73</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="6255" citStr="Kupiec et al., 1995" startWordPosition="934" endWordPosition="937"> on headline generation and the related task of image caption generation. However, there is nothing inherent in our formulation that is specific to those two tasks; it is possible for the model to generate longer or shorter summaries, for a single or multiple documents. Experimental results show that our method obtains state-of-the-art performance, both in terms of grammaticality and informativeness for both tasks using the same summarization model. 2 Related work Much effort in automatic summarization has been devoted to sentence extraction which is often formalized as a classification task (Kupiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words are deleted and sentences selected from a document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Kupiec, Julian, Jan O. Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of SIGIR-95. Seattle, WA, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Improving summarization performance by sentence compression — a pilot study.</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th International Workshop on Information Retrieval with Asian Languages.</booktitle>
<pages>1--8</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="2198" citStr="Lin, 2003" startWordPosition="318" endWordPosition="319">catenating the most important sentences in a document. The advantage of this approach is that it does not require a great deal of linguistic analysis to generate grammatical sentences, assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and combining them into new sentences guided by a language model (Banko et al., 2000; Soricut and Marcu, 2007). This approach has the potential to work well, breaking out of the single-sentence paradigm. Unfortunately, the resulting summaries are not always coherent — individual constituent phrases are often combined without any semantic constraints — or grammatical beyond the n-gram horizon imposed by the </context>
<context position="6583" citStr="Lin 2003" startWordPosition="986" endWordPosition="987">erformance, both in terms of grammaticality and informativeness for both tasks using the same summarization model. 2 Related work Much effort in automatic summarization has been devoted to sentence extraction which is often formalized as a classification task (Kupiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words are deleted and sentences selected from a document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Lin, Chin-Yew. 2003. Improving summarization performance by sentence compression — a pilot study. In Proceedings of the 6th International Workshop on Information Retrieval with Asian Languages. Sapporo, Japan, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT NAACL.</booktitle>
<pages>71--78</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="27406" citStr="Lin and Hovy, 2003" startWordPosition="4533" endWordPosition="4536">nhances it with topic keywords. For the captions, we compared our model against the highest-scoring document sentence according to the SVM and against the probabilistic model presented in Feng and Lapata (2010a). The latter estimates the probability of a phrase appearing in the caption given the same phrase appearing in the corresponding document and uses a language model to select among many different surface realizations. The language model is adapted with probabilities from an image annotation model (Feng and Lapata, 2010b). Evaluation We evaluated the quality of the headlines using ROUGE (Lin and Hovy, 2003). The DUC-04 dataset provides four reference headlines per document. We report unigram overlap (ROUGE-1) and bigram overlap (ROUGE-2) as a means of assessing informativeness, and the longest common subsequence (ROUGE-L) as a means of as519 sessing fluency. Original DUC-04 ROUGE parameters were used. We also use ROUGE to evaluate the automatic captions with the original BBC captions as reference. In addition, we evaluated the generated headlines by eliciting human judgments. Participants were presented with a news article and its corresponding headline and were asked to rate the latter along tw</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of HLT NAACL. Edmonton, Canada, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing.</booktitle>
<pages>1--9</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="2262" citStr="Martins and Smith, 2009" startWordPosition="327" endWordPosition="330">ment. The advantage of this approach is that it does not require a great deal of linguistic analysis to generate grammatical sentences, assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and combining them into new sentences guided by a language model (Banko et al., 2000; Soricut and Marcu, 2007). This approach has the potential to work well, breaking out of the single-sentence paradigm. Unfortunately, the resulting summaries are not always coherent — individual constituent phrases are often combined without any semantic constraints — or grammatical beyond the n-gram horizon imposed by the language model. Constituent deletion and recombination are merel</context>
<context position="6834" citStr="Martins and Smith, 2009" startWordPosition="1025" endWordPosition="1028"> a classification task (Kupiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words are deleted and sentences selected from a document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can b</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Martins, Andr´e and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing. Boulder, Colorado, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation. Association for Computational Linguistics,</booktitle>
<pages>23--30</pages>
<location>New York City,</location>
<contexts>
<context position="740" citStr="Smith and Eisner, 2006" startWordPosition="96" endWordPosition="99">ics, University of Edinburgh Edinburgh EH8 9AB, United Kingdom k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk Abstract The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions. Based on an integer linear programming formulation, the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met. Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications. 1 Introduction Summarization is the process of condensing a source text into a shorter version while preserving its information con</context>
<context position="4523" citStr="Smith and Eisner 2006" startWordPosition="664" endWordPosition="667">ative and minimally redundant, grammatical, coherent, adhere to a pre-specified length and stylistic conventions. Importantly, these constraints are conflicting; the deletion of certain phrases may avoid redundancy but result in ungrammatical output and information loss. In this paper we propose a model for summarization that attempts to capture and optimize these constraints jointly. We learn both how to select the most important information (the content), and how to render it appropriately (the style). Selection preferences are learned discriminatively, while a quasisynchronous grammar (QG, Smith and Eisner 2006) captures rendering preferences such as paraphrases and compressions. The entire solution space of possible extractions and QG-generated paraphrases is searched efficiently through use of integer linear programming. The ILP framework allows us to model naturally as constraints, additional requirements such as sentence length, overall summary length, topic coverage and, importantly, grammaticality. We argue that QG is attractive for describing rewrite operations common in summarization. Rather than assuming a strictly synchronous structure over the source and target sentences, QG identifies a “</context>
<context position="13867" citStr="Smith and Eisner (2006)" startWordPosition="2162" endWordPosition="2165">ar between source and target child nodes. Examples are taken from the QG rules discovered in the DUC03 data set of headlines. source; this helps to improve the quality in what is inherently a noisy process. Finally, QG rules are created from aligned nodes above the leaf node level, recording the phrase and dependency label of nodes, and the alignment of child nodes. Unlike previous work involving QG which has used dependency graphs exclusively (e.g., Wang et al. 2007; Das and Smith 2009), our approach operates over a combined PCFG-dependency representation. As a result, some configurations in Smith and Eisner (2006) are not so relevant here — instead, we found that deletions, reorderings, flattening of nodes, and the addition of text elements were important operations for the grammar. Figure 1 shows some example alignments that are captured by the QG, with the source node on the left and the target node on the right. Leaf nodes have their original text, while other nodes have a combined phrase and dependency label that they obtain in the merged representation described in Section 3.1 above (e.g., NP/dobj is a noun phrase and a direct object, NNP/nn is a proper noun and a nominal modifier, whereas NN/– is</context>
<context position="15957" citStr="Smith and Eisner (2006)" startWordPosition="2513" endWordPosition="2516">paraphrase sub-tree must be suitable for the target tree being created (i.e., the root node of the paraphrase must match the phrase and dependency label of the corresponding node in the target tree). Where more than one paraphrase is possible, the alternatives are incorporated into the target parse tree under a CHOICE node, as is shown in Figure 2. Note that unlike preof East Timor (c) NP/dobj NP/dobj the extradition of Kurdish leader Ocalan IN/– NNP/– Ocalan’s extradition NN/nn NN/– DT/det NP/poss NN/– PP/prep of NNP/nn 516 vious QG approaches, we do not use the probability model proposed by Smith and Eisner (2006); instead the QG is used to represent rewrite operations, and we simply record a frequency count for how often each rule is encountered in the training data. 3.3 ILP model The objective of our model is to create the most informative text possible, subject to constraints which can be tailored to the specific task. These relate to sentence length, overall summary length, the inclusion of specific topics, and grammaticality. These constraints are global in their scope, and cannot be adequately satisfied by optimizing each one of them individually. Our approach therefore uses an ILP formulation wh</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>Smith, David and Jason Eisner. 2006. Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings on the Workshop on Statistical Machine Translation. Association for Computational Linguistics, New York City, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP. Suntec, Singapore,</booktitle>
<pages>822--831</pages>
<contexts>
<context position="9576" citStr="Smith and Eisner, 2009" startWordPosition="1453" endWordPosition="1456">apata (2010a) develop extractive and abstractive caption generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), and question answering (Wang et al., 2007); however the use of QG in summarization is novel to our knowledge. Unlike most synchronous grammar formalisms, QG does not posit a strict isomorphism between a source sentence and its target translation; it only loosely links the syntactic structure of the two, and is therefore well suited to describing the relationship between a document and its abstract. We propose an ILP formulation which not only allows to efficiently search through the space of many QG rules but also to incorporate constraints re</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>Smith, David A. and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of the EMNLP. Suntec, Singapore, pages 822–831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Abstractive headline generation using WIDL-expressions. Information Processing and Management 43(6):1536–1548. Text Summarization.</title>
<date>2007</date>
<contexts>
<context position="2498" citStr="Soricut and Marcu, 2007" startWordPosition="363" endWordPosition="366">documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and combining them into new sentences guided by a language model (Banko et al., 2000; Soricut and Marcu, 2007). This approach has the potential to work well, breaking out of the single-sentence paradigm. Unfortunately, the resulting summaries are not always coherent — individual constituent phrases are often combined without any semantic constraints — or grammatical beyond the n-gram horizon imposed by the language model. Constituent deletion and recombination are merely two of the many rewrite operations professional editors and abstractors employ when creating summaries (Jing, 2002). Additional operations include truncating sentences, aggregating them, and paraphrasing at word or syntax level. Furth</context>
</contexts>
<marker>Soricut, Marcu, 2007</marker>
<rawString>Soricut, R. and D. Marcu. 2007. Abstractive headline generation using WIDL-expressions. Information Processing and Management 43(6):1536–1548. Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy model? a quasisynchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP-CoNLL.</booktitle>
<pages>22--32</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="9669" citStr="Wang et al., 2007" startWordPosition="1467" endWordPosition="1470">utput of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), and question answering (Wang et al., 2007); however the use of QG in summarization is novel to our knowledge. Unlike most synchronous grammar formalisms, QG does not posit a strict isomorphism between a source sentence and its target translation; it only loosely links the syntactic structure of the two, and is therefore well suited to describing the relationship between a document and its abstract. We propose an ILP formulation which not only allows to efficiently search through the space of many QG rules but also to incorporate constraints relating to content, style, and the task at hand. 3 Modeling There are three components to our </context>
<context position="13715" citStr="Wang et al. 2007" startWordPosition="2138" endWordPosition="2141">ther compression through applying other QG rules on children). Nodes bear phrase and dependency labels. Dotted lines show alignments in the grammar between source and target child nodes. Examples are taken from the QG rules discovered in the DUC03 data set of headlines. source; this helps to improve the quality in what is inherently a noisy process. Finally, QG rules are created from aligned nodes above the leaf node level, recording the phrase and dependency label of nodes, and the alignment of child nodes. Unlike previous work involving QG which has used dependency graphs exclusively (e.g., Wang et al. 2007; Das and Smith 2009), our approach operates over a combined PCFG-dependency representation. As a result, some configurations in Smith and Eisner (2006) are not so relevant here — instead, we found that deletions, reorderings, flattening of nodes, and the addition of text elements were important operations for the grammar. Figure 1 shows some example alignments that are captured by the QG, with the source node on the left and the target node on the right. Leaf nodes have their original text, while other nodes have a combined phrase and dependency label that they obtain in the merged representa</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Wang, Mengqiu, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? a quasisynchronous grammar for QA. In Proceedings of the EMNLP-CoNLL. Prague, Czech Republic, pages 22– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Jacek Gondzio</author>
</authors>
<title>Exploiting separability in large-scale linear support vector machine training.</title>
<date>2009</date>
<journal>Computational Optimization and Applications</journal>
<note>Published online.</note>
<contexts>
<context position="23257" citStr="Woodsend and Gondzio, 2009" startWordPosition="3830" endWordPosition="3833">tive or negative automatically. If there was a unigram overlap (excluding stop words) between the phrase and any of the original title or caption, we marked this phrase with a positive label. Non-overlapping phrases were given negative labels. Our feature set comprised surface features such as sentence and paragraph position information, POS tags, and whether high-scoring tf.idf words were present in the phrase. Additionally, the caption training set contained features for unigram and bigram overlap with the title. We learned the feature weights with a linear SVM, using the software SVM-OOPS (Woodsend and Gondzio, 2009). This tool gave us directly the feature weights as well as support vector values, and it allowed different penalties to be applied to positive and negative misclassifications, enabling us to compensate for the unbalanced data set. The penalty hyper-parameters chosen were the ones that gave the best F-scores, using 10-fold validation. For each of the two tasks, QG rules were extracted from the same data used to train the SVM, resulting in 2,910 distinct rules for headlines and 2,757 rules for the captions. Table 1 shows that for both tasks, the majority of rules apply to PP and NP phrases. Bot</context>
</contexts>
<marker>Woodsend, Gondzio, 2009</marker>
<rawString>Woodsend, Kristian and Jacek Gondzio. 2009. Exploiting separability in large-scale linear support vector machine training. Computational Optimization and Applications Published online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic generation of story highlights.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th ACL. Uppsala, Sweden,</booktitle>
<pages>565--574</pages>
<editor>In Sandra Carberry and Stephen Clark, editors,</editor>
<contexts>
<context position="6862" citStr="Woodsend and Lapata, 2010" startWordPosition="1029" endWordPosition="1032">upiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words are deleted and sentences selected from a document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase </context>
<context position="11143" citStr="Woodsend and Lapata (2010)" startWordPosition="1705" endWordPosition="1708">ses of the source sentences. An ILP model combines the output of these two components into an output summary, while optimizing content selection and surface realization preferences jointly. 3.1 Document Representation Our model operates on documents annotated with syntactic information which we obtain by parsing every sentence twice, once with a phrase structure parser and once with a dependency parser. The output from the two representations is combined into a single data structure, by mapping the dependencies to the edges of the phrase structure tree. The procedure is described in detail in Woodsend and Lapata (2010). However, we do not merge the leaf nodes into phrases here, but keep the full tree structure, as we will apply compression to phrases through the QG. In our experiments, we obtain this combined representation from the output of the Stanford parser (Klein and Manning, 2003) but any other broadly similar parser could be used instead. 3.2 Quasi-synchronous grammar Given an input sentence S1 or its parse tree T1, the QG constructs a monolingual grammar for parsing, or generating, the possible translation (or here, paraphrase) trees T2. A grammar node in the target tree T2 is modeled on a subset o</context>
</contexts>
<marker>Woodsend, Lapata, 2010</marker>
<rawString>Woodsend, Kristian and Mirella Lapata. 2010. Automatic generation of story highlights. In Sandra Carberry and Stephen Clark, editors, Proceedings of the 48th ACL. Uppsala, Sweden, pages 565–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>BBN/UMD at DUC-2004: Topiary.</title>
<date>2004</date>
<booktitle>In Proceedings of the NAACL Workshop on Document Understanding.</booktitle>
<pages>112--119</pages>
<location>Boston, MA,</location>
<contexts>
<context position="8233" citStr="Zajic et al., 2004" startWordPosition="1242" endWordPosition="1245">n, due to its prominence in the DUC-03 and DUC-04 evaluation competitions.1 Many approaches identify the most informative sentence in a given document (typically the first sentence for the news genre) and subsequently apply a form of sentence compression such that the headline meets some length requirement (Dorr 1Approaches to headline generation are too numerous to list in detail; see the proceedings of DUC-03 and DUC-04 for an overview. 514 et al., 2003). The compressed sentence may also be “padded” with important content words or phrases to ensure that the topic of the document is covered (Zajic et al., 2004). Other work generates headlines in a bottom-up fashion starting from important, individual words and phrases, that are glued together to create a fluent sentence. For example, Banko et al. (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Relatively little work has focused on caption generation, a task related to headline generation. The aim here is to create a short, title-like description of an image embedded in a news article. Like headlines, captions have to be short and informative. In addition</context>
<context position="26593" citStr="Zajic et al., 2004" startWordPosition="4401" endWordPosition="4404">umber of sentences allowed to form a headline is set to 5 as some of the headlines in the DUC dataset contained multiple sentences. To solve the ILP model we used the ZIB Optimization Suite software (Achterberg, 2007; Koch, 2004). The solution was converted into a sentence by removing nodes not chosen from the tree representation, then concatenating the remaining leaf nodes in order. Model Comparison For the headline task, we compared our model to the DUC-04 standard baseline of the first sentence, truncated at the first word boundary after 75 characters; and the output of the Topiary system (Zajic et al., 2004), which came top in almost all measures in the DUC-04 evaluation. In order to generate a headline, Topiary first compresses the lead sentence using linguistically motivated heuristics and then enhances it with topic keywords. For the captions, we compared our model against the highest-scoring document sentence according to the SVM and against the probabilistic model presented in Feng and Lapata (2010a). The latter estimates the probability of a phrase appearing in the caption given the same phrase appearing in the corresponding document and uses a language model to select among many different </context>
</contexts>
<marker>Zajic, Dorr, Schwartz, 2004</marker>
<rawString>Zajic, David, Bonnie Dorr, and Richard Schwartz. 2004. BBN/UMD at DUC-2004: Topiary. In Proceedings of the NAACL Workshop on Document Understanding. Boston, MA, pages 112–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie J Dorr</author>
<author>Jimmy Lin</author>
<author>Richard Schwartz</author>
</authors>
<title>Multi-candidate reduction: Sentence compression as a tool for document summarization tasks.</title>
<date>2007</date>
<journal>Information Processing Management Special Issue on Summarization</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="2236" citStr="Zajic et al., 2007" startWordPosition="323" endWordPosition="326"> sentences in a document. The advantage of this approach is that it does not require a great deal of linguistic analysis to generate grammatical sentences, assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and combining them into new sentences guided by a language model (Banko et al., 2000; Soricut and Marcu, 2007). This approach has the potential to work well, breaking out of the single-sentence paradigm. Unfortunately, the resulting summaries are not always coherent — individual constituent phrases are often combined without any semantic constraints — or grammatical beyond the n-gram horizon imposed by the language model. Constituent deletion a</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2007</marker>
<rawString>Zajic, David, Bonnie J. Dorr, Jimmy Lin, and Richard Schwartz. 2007. Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing Management Special Issue on Summarization 43(6):1549–1570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Xiang Lan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Application-driven statistical paraphrase generation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Suntec, Singapore,</booktitle>
<pages>834--842</pages>
<contexts>
<context position="7403" citStr="Zhao et al. (2009)" startWordPosition="1112" endWordPosition="1115">um´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Headline generation is a well-studied task within single-document summarization, due to its prominence in the DUC-03 and DUC-04 evaluation competitions.1 Many approaches identify the most informative sentence in a given document (typically the first sentence for the news genre) and subsequently apply a form of sentence compression such that the headline meets some length requirement (Dorr 1Approaches to headline generation are too numerous to list in detail; see </context>
</contexts>
<marker>Zhao, Lan, Liu, Li, 2009</marker>
<rawString>Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li. 2009. Application-driven statistical paraphrase generation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Suntec, Singapore, pages 834–842.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>