<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9986235">
Domain Adaptation of Rule-Based Annotators
for Named-Entity Recognition Tasks
</title>
<author confidence="0.91742">
Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li Frederick Reiss Shivakumar Vaithyanathan
</author>
<affiliation confidence="0.809099">
IBM Research – Almaden
</affiliation>
<address confidence="0.981492">
650 Harry Road, San Jose, CA 95120, USA
</address>
<email confidence="0.976773">
{chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com
</email>
<sectionHeader confidence="0.982175" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996175555555555">
Named-entity recognition (NER) is an impor-
tant task required in a wide variety of ap-
plications. While rule-based systems are ap-
pealing due to their well-known “explainabil-
ity,” most, if not all, state-of-the-art results
for NER tasks are based on machine learning
techniques. Motivated by these results, we ex-
plore the following natural question in this pa-
per: Are rule-based systems still a viable ap-
proach to named-entity recognition? Specif-
ically, we have designed and implemented
a high-level language NERL on top of Sys-
temT, a general-purpose algebraic informa-
tion extraction system. NERL is tuned to the
needs of NER tasks and simplifies the pro-
cess of building, understanding, and customiz-
ing complex rule-based named-entity annota-
tors. We show that these customized annota-
tors match or outperform the best published
results achieved with machine learning tech-
niques. These results confirm that we can
reap the benefits of rule-based extractors’ ex-
plainability without sacrificing accuracy. We
conclude by discussing lessons learned while
building and customizing complex rule-based
annotators and outlining several research di-
rections towards facilitating rule development.
</bodyText>
<sectionHeader confidence="0.995143" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989931222222222">
Named-entity recognition (NER) is the task of iden-
tifying mentions of rigid designators from text be-
longing to named-entity types such as persons, orga-
nizations and locations (Nadeau and Sekine, 2007).
While NER over formal text such as news articles
and webpages is a well-studied problem (Bikel et
al., 1999; McCallum and Li, 2003; Etzioni et al.,
2005), there has been recent work on NER over in-
formal text such as emails and blogs (Huang et al.,
</bodyText>
<note confidence="0.660621333333333">
2001; Poibeau and Kosseim, 2001; Jansche and Ab-
ney, 2002; Minkov et al., 2005; Gruhl et al., 2009).
The techniques proposed in the literature fall under
three categories: rule-based (Krupka and Hausman,
2001; Sekine and Nobata, 2004), machine learning-
based (O. Bender and Ney, 2003; Florian et al.,
2003; McCallum and Li, 2003; Finkel and Manning,
2009; Singh et al., 2010) and hybrid solutions (Sri-
hari et al., 2001; Jansche and Abney, 2002).
</note>
<subsectionHeader confidence="0.987729">
1.1 Motivation
</subsectionHeader>
<bodyText confidence="0.99982955">
Although there are well-established rule-based sys-
tems to perform NER tasks, most, if not all, state-of-
the-art results for NER tasks are based on machine
learning techniques. However, the rule-based ap-
proach is still extremely appealing due to the associ-
ated transparency of the internal system state, which
leads to better explainability of errors (Siniakov,
2010). Ideally, one would like to benefit from the
transparency and explainability of rule-based tech-
niques, while achieving state-of-the-art accuracy.
A particularly challenging aspect of rule-based
NER in practice is domain customization — cus-
tomizing existing annotators to produce accurate re-
sults in new domains. In machine learning-based
systems, adapting to a new domain has tradition-
ally involved acquiring additional labeled data and
learning a new model from scratch. However, recent
work has proposed more sophisticated approaches
that learn a domain-independent base model, which
can later be adapted to specific domains (Florian et
</bodyText>
<page confidence="0.418852">
1002
</page>
<note confidence="0.969125">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1002–1012,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.78407629787234">
Document d1 Document d2 // Core rules identify Organization and Location candidates
BASEBALL - MAJOR LEAGUE
STANDINGS AFTER TUESDAY &apos;S GAMES
NEW YORK 1996-08-28...
...
AMERICAN LEAGUE
EASTERN DIVISION
W L PCT GB
NEW YORK 74 57 .565 -
BALTIMORE 70 61 .534 4
BOSTON 68 65 .511 7
...
TEXAS AT KANSAS CITY
BOSTON AT CALIFORNIA
NEW YORK AT SEATTLE
...
BASEBALL - ORIOLES WIN , YANKEES
LOSE .
BALTIMORE 1996-08-27
...
In Seattle , Jay Buhner&apos;s eighth-inning single
snapped a tie as the Seattle Mariners edged
the New York Yankees 2-1 in the opener of a
three-game series .
New York starter Jimmy Key left the game in
the first inning after Seattle shortstop Alex
Rodriguez lined a shot off his left elbow .
...
Organization Location
// Begin customization
// Identify articles covering sports event from article title
CR1 &lt;SportsArticle&gt; E Evaluate Regular Expressions &lt;R1&gt;
// Identify locations in sports articles
CR2 Retain &lt;Location&gt; As &lt;LocationMaybeOrg&gt; If ContainedWithin &lt;SportsArticle&gt;
// City/County/State references (e.g., New York) may refer to the sports team in that city
CR3 Retain &lt;LocationMaybeOrg&gt; If Matches Dictionaries
&lt;‘cities.dict’,’counties.dict’,’states.dict’&gt;
// Some city references in sports articles may refer to the city (e.g., In Seattle )
// These references should not be reclassified as Organization
CR4 Discard &lt;LocationMaybeOrg&gt; If Matches Regular Expression &lt;R2&gt;
on Left Context 2 Tokens
Customization Requirement: City, County or State names within sports articles
may refer to a sports team or to the location itself.
Customization Solution (CS) :
Within sports articles,
Identify all occurrences of city/county/state as Organizations,
Except when a contextual clue indicates that the reference is to the location
</figure>
<figureCaption confidence="0.999088">
Figure 1: Example Customization Requirement
</figureCaption>
<note confidence="0.968757">
al., 2004; Blitzer et al., 2006; Jiang and Zhai, 2006;
Arnold et al., 2008; Wu et al., 2009). Implement-
</note>
<bodyText confidence="0.95488825">
ing a similar approach for rule-based NER typically
requires a significant amount of manual effort to (a)
identify the explicit semantic changes required for
the new domain (e.g., differences in entity type def-
inition), (b) identify the portions of the (complex)
core annotator that should be modified for each dif-
ference and (c) implement the required customiza-
tion rules without compromising the extraction qual-
ity of the core annotator. Domain customization of
rule-based NER has not received much attention in
the recent literature with a few exceptions (Petasis et
al., 2001; Maynard et al., 2003; Zhu et al., 2005).
</bodyText>
<subsectionHeader confidence="0.996589">
1.2 Problem Statement
</subsectionHeader>
<bodyText confidence="0.99516325">
In this paper, we explore the following natural ques-
tion: Are rule-based systems still a viable approach
to named-entity recognition? Specifically, (a) Is it
possible to build, maintain and customize rule-based
NER annotators that match the state-of-the-art re-
sults obtained using machine-learning techniques?
and (b) Can this be achieved with a reasonable
amount of manual effort?
</bodyText>
<subsectionHeader confidence="0.993198">
1.3 Contributions
</subsectionHeader>
<bodyText confidence="0.9934485">
In this paper, we address the challenges mentioned
above by (i) defining a taxonomy of the different
types of customizations that a rule developer may
perform when adapting to a new domain (Sec. 2), (ii)
identifying a set of high-level operations required
for building and customizing NER annotators, and
(iii) exposing these operations in a domain-specific
NER rule language, NERL, developed on top of Sys-
</bodyText>
<figure confidence="0.918489833333333">
// City references to sports teams are added to Organization and removed from Location
CR5 Augment &lt;Organization&gt; With &lt;LocationMaybeOrg&gt;
// End customization
// Continuation of core rules
// Remove Locations that overlap with Organizations
Discard &lt;Location&gt; If Overlaps Concepts &lt;Organization&gt;
</figure>
<figureCaption confidence="0.993031">
Figure 2: Example Customization Rules in NERL
</figureCaption>
<bodyText confidence="0.964609212121212">
temT (Chiticariu et al., 2010), a general-purpose
algebraic information extraction system (Sec. 3).
NERL is specifically geared towards building and
customizing complex NER annotators and makes it
easy to understand a complex annotator that may
comprise hundreds of rules. It simplifies the iden-
tification of what portions need to be modified for
a given customization requirement. It also makes
individual customizations easier to implement, as il-
lustrated by the following example.
Suppose we have to customize a domain-
independent rule-based NER annotator for the
CoNLL corpus (Tjong et al., 2003). Consider the
two sports-related news articles in Fig. 1 from the
corpus, where city names such as ‘New York’ or
‘Seattle’ can refer to either a Location or an Orga-
nization (the sports team based in that city). In the
domain-independent annotator, city names were al-
ways identified as Location, as this subtle require-
ment was not considered during rule development.
A customization to address this issue is shown in
Fig. 1, which can be implemented in NERL with five
rules (Fig. 2). This customization (explained in de-
tail in Sec. 3) improved the Fp=1 score for Organi-
zation and Location by approximately 9% and 3%,
respectively (Sec. 4).
We used NERL to customize a domain-
independent rule-based NER annotator for three
different domains – CoNLL03 (Tjong et al., 2003),
Enron (Minkov et al., 2005) and ACE05 (NIST,
2005). Our experimental results (Sec. 4.3) demon-
strate that the customized annotators have extraction
quality better than the best-known results for
</bodyText>
<table confidence="0.8794028">
1003
Affects Single Affects Multiple
Entity Type Entity Types
Identify New Instances CS, CDD, CDSD BR
Modify Existing instances CEB, CDD CATA, CG
</table>
<tableCaption confidence="0.999664">
Table 1: Categorizing NER Customizations
</tableCaption>
<bodyText confidence="0.999926307692308">
individual domains, which were achieved with
machine learning techniques. The fact that we are
able to achieve such results across multiple domains
answers our earlier question and confirms that
we can reap the benefits of rule-based extractors’
explainability without sacrificing accuracy.
However, we found that even using NERL, the
amount of manual effort and expertise required in
rule-based NER may still be significant. In Sec. 5,
we report on the lessons learned and outline several
interesting research directions towards simplifying
rule development and facilitating the adoption of the
rule-based approach towards NER.
</bodyText>
<sectionHeader confidence="0.858726" genericHeader="method">
2 Domain Customization for NER
</sectionHeader>
<bodyText confidence="0.998602472972973">
We consider NER tasks following the broad defini-
tion put forth by (Nadeau and Sekine, 2007), for-
mally defined as follows:
Definition 1 Named entity recognition is the task of
identifying and classifying mentions of entities with
one or more rigid designators, as defined by (Kripke,
1982).
For instance, the identification of proper nouns
representing persons, organizations, locations, prod-
uct names, proteins, drugs and chemicals are consid-
ered as NER tasks.
Based on our experience of customizing NER an-
notators for multiple domains, we categorize the
customizations involved into two main categories as
listed below. This categorization motivates the de-
sign of NERL (Sec. 3).
Data-driven (CDD): The most common NER cus-
tomization is data-driven, where the customizations
mostly involve the addition of new patterns and
dictionary entries, driven by observations from the
training data in the new domain. An example is
the addition of a new rule to identify locations from
the beginning of news articles (e.g., “BALTIMORE
1995-08-27” and “MURCIA , Spain 1996-09-10”).
Application-driven: What is considered a valid
named entity and its corresponding type can vary
across application domains. The most common di-
mensions on which the definition of a named entity
can vary are:
Entity Boundary (CEB): Different application do-
mains may have different definitions of where the
same entity starts or ends. For example, a Person
may (CoNLL03) or may not (Enron) include gener-
ational markers (e.g. “Jr.” in “Bush Jr.” or “IV” in
“Henry IV”).
Ambiguous Type Assignment (CATA): The exact
type of a given named entity can be ambiguous.
Different applications may assign different types
for the same named entity. For instance, all in-
stances of “White House” may be considered as Lo-
cation (CoNLL03), or be assigned as Facility or Or-
ganization based on their context (ACE05). In fact,
even within the same application domain, entities
typically considered as of the same type may be as-
signed differently. For example, given “New York
beat Seattle” and “Ethiopia beat Uganda”, both
‘New York’ and ‘Ethiopia’ are teams referred by their
locations. However, (Tjong et al., 2003) considers
the former, which corresponds to a city, as an Orga-
nization, and the latter, which corresponds to a coun-
try, as a Location.
Domain-Specific Definition (CDSD): Whether a
given term is even considered a named entity may
depend on the specific domain. As an example, con-
sider the text “Commercialization Meeting - SBeck,
BHall, BSuperty, TBusby, SGandhi-Gupta”. Infor-
mal names such as ‘SBeck’ and ‘BHall’ may be con-
sidered as valid person names (Enron).
Scope(CS): Each type of named entity usually con-
tains several subtypes. For the same named en-
tity task, different applications may choose to in-
clude different sets of subtypes. For instance,
roads and buildings are considered part of Location
in CoNLL03, while they are not included in ACE05.
Granularity(CG): Name entity types are hierarchi-
cal. Different applications may define NER tasks
at different granularities. For instance, in ACE05,
Organization and Location entity types were split
into four entity types (Organization, Location, Geo-
Political Entity and Facility).
The different customizations are summarized as
shown in Tab. 1, based on the following criteria: (i)
whether the customization identifies new instances
or modifies existing instances; and (ii) whether the
</bodyText>
<page confidence="0.468991">
1004
</page>
<bodyText confidence="0.999869">
customization affects single or multiple entities. For
instance, CS identifies new instances for a single en-
tity type, as it adds instances of a new subtype for an
existing entity type. Note that BR in the table de-
notes the rules used to build the core annotator.
</bodyText>
<sectionHeader confidence="0.976372" genericHeader="method">
3 Named Entity Rule Language
</sectionHeader>
<subsectionHeader confidence="0.998434">
3.1 Grammar vs. Algebraic NER
</subsectionHeader>
<bodyText confidence="0.999986987179487">
Traditionally, rule-based NER systems were based
on the popular CPSL cascading grammar specifi-
cation (Appelt and Onyshkevych, 1998). CPSL is
designed so that rules that adhere to the standard
can be executed efficiently with finite state transduc-
ers. Accordingly, the standard defines a rigid left-to-
right execution model where a region of text can be
matched by at most one rule according to a fixed rule
priority, and where overlapping annotations are dis-
allowed in the output of each grammar phase.
While it simplifies the design of CPSL engines,
the rigidity of the rule matching semantics makes
it difficult to express operations frequently used in
rule-based information extraction. These limitations
have been recognized in the literature, and several
extensions have been proposed to allow more flex-
ible matching semantics, and to allow overlapping
annotations (Cunningham et al., 2000; Boguraev,
2003; Drozdzynski et al., 2004). However, even
with these extensions, common operations such as
filtering annotations (e.g. CR4 in Fig. 2), are dif-
ficult to express in grammars and often require an
escape to custom procedural code.
Recently, several declarative algebraic languages
have been proposed for rule-based IE systems, no-
tably AQL (Chiticariu et al., 2010) and Xlog (Shen
et al., 2007). These languages are not constrained
by the requirement that all rules map onto finite state
transducers, and therefore can express a significantly
richer semantics than grammar-based languages. In
particular, the AQL rule language as implemented in
SystemT (Chiticariu et al., 2010) can express many
common operations used in rule-based information
extraction without requiring custom code. In addi-
tion, the separation of extraction semantics from ex-
ecution enables SystemT’s rule optimizer and effi-
cient runtime engine. Indeed, as shown in (Chiti-
cariu et al., 2010), SystemT can deliver an order of
magitude higher annotation throughput compared to
a state-of-the-art CPSL-based IE system.
Since AQL is a general purpose information ex-
traction rule language, similar to CPSL and JAPE,
it exposes an expressive set of capabilities that go
beyond what is required for NER tasks. These ad-
ditional capabilities can make AQL rules more ver-
bose than is necessary for implementing rules in the
NER domain. For example, Fig. 3 shows how the
same customization rule CR4 from Fig. 2 can be
implemented in JAPE or in AQL. Notice how im-
plementing even a single customization may lead to
defining complex rules (e.g. JAPE-R1, AQL-R1)
and sometimes even using custom code (e.g. JAPE-
R2). As illustrated by this example, the rules in AQL
and JAPE tend to be complex since some operations
— e.g., filtering the outputs of one rule based on the
outputs of another rule — that are common in NER
rule sets require multiple rules in AQL or multiple
grammar phases in JAPE.
To make NER rules easier to develop and to
understand, we designed and implemented Named
Entity Rule Language (NERL) on top of SystemT.
NERL is a declarative rule language designed specif-
ically for named entity recognition. The design of
NERL draws on our experience with building and
customizing multiple complex NER annotators. In
particular, we have identified the operations required
in practice for such tasks, and expose these opera-
tions as built-in constructs in NERL. In doing so, we
ensure that frequently performed operations can be
expressed succinctly, so as not to complicate the rule
set unnecessarily. As a result, NERL rules for named
entity recognition tasks are significantly more com-
pact and easy to understand than the equivalent AQL
rules. At the same time, NERL rules can easily be
compiled to AQL, allowing our NER rule develop-
ment framework to take advantage of the capabilities
of the SystemT rule optimizer and efficient runtime
execution engine.
</bodyText>
<subsectionHeader confidence="0.974888">
3.2 NERL
</subsectionHeader>
<bodyText confidence="0.9877175">
For the rest of this section, we focus on describ-
ing the types of rules supported in NERL. In Sec. 4,
we shall demonstrate empirically that NERL can be
successfully employed in building and customizing
complex NER annotators.
A NERL rule has the following form:
</bodyText>
<table confidence="0.912078166666667">
IntConcept +— RuleBody(IntConcept1, IntConcept2, ...)
1005
Rule in NERL
// Some city references in sports articles may refer to the city (e.g., In Seattle )
// These references should not be reclassified as Organization
CR4 Discard &lt;LocationMaybeOrg&gt; If Matches Regular Expression &lt;R2&gt; on Left Context 2 Tokens
JAPE-R1 JAPE-R2 AQL-R1
JAPE Phase 1 Rule : RetainValidLocation create view LocationMaybeOrgInvalid as
Rule : AmbiguousLocationContext ({Token}[2]):context({AmbiguousLoc}):loc --&gt; select LMO.value as value
({Token}[2]):context({AmbiguousLoc}): annot { // Action part in Java to test R2 on left context from LocationMaybeOrg LMO
:annot.AmbiguousLoc = {lc = context.string} // and delete annotation where MatchesRegex(/R2/,
AnnotationSet loc = bindings.get(“loc&amp;quot;); LeftContextTok(LMO.value,2));
AnnotationSet context = bindings.get(“context&amp;quot;);
int begOffset = context.firstNode().getOffset().intValue();
int endOffset = context.lastNode().getOffset().intValue();
String mydocContent = doc.getContent().toString();
String contextString =
mydocContent.substring(begOffset, endOffset);
if (Pattern.matches(“R2”, contextString)) {
outputAS.removeAll(loc);
}
}
JAPE Phase 2 create view LocationMaybeOrgValid as
Rule : RetainValidLocation (select LMO.value as value
({AmbiguousLoc.lc =~ R2}):ambiguousloc --&gt; from LocationMaybeOrg LMO)
{ // rule to discard ambiguous locations minus
AnnotationSet loc = bindings.get(“ambiguousloc&amp;quot;); (select LMOI.value as value
outputAS.removeAll(loc); from LocationMaybeOrgInvalid LMOI);
}
Two Alternative Rule sets in JAPE Equivalent Rule set in AQL
</table>
<figureCaption confidence="0.95677">
Figure 3: Single Customization Rule expressed in NERL, JAPE and AQL
</figureCaption>
<bodyText confidence="0.999467592592593">
Intuitively, a NERL rule creates an intermediate con-
cept or named entity (IntConcept for short) by ap-
plying a NERL rule on the input text and zero or
more previously defined intermediate concepts.
NERL Rule Types The types of rules supported in
NERL are summarized in Tab. 2. In what follows,
we illustrate these types by means of examples.
Feature definition (FD): FD rules identify basic
features from text (e.g., FirstName, LastName and
CapsWord features for identifying person names).
Candidate definition (CD): CD rules identify com-
plete occurrences of the target entity. For instance,
the Sequence rule “LastName followed by ‘,’ fol-
lowed by FirstName” identifies person annotations
as a sequence of three tokens, where the first and
third tokens occur in dictionaries containing last and
first names.
Candidate Refinement (CR): CR rules are used to
refine candidates generated for different annotation
types. E.g., the Filter rule CR3 in Fig. 2 retains Loca-
tionMaybeOrg annotations that appear in one of sev-
eral dictionaries.
Consolidation (CO): CO rules are used to resolve
overlapping candidates generated by multiple CD
rules. For instance, consider the text “Please see
the following request from Dr. Kenneth Lim of the
BAAQMD.”. A CD rule may identify ‘Dr. Kenneth
Lim’ as a person, while another CD rule may identify
‘Kenneth Lim’ as a candidate person. A consolidation
rule is then used to merge these two annotations to
produce a single annotation for ‘Dr. Kenneth Lim’.
NERL Examples Within these categories, three
types of rules deserve special attention, as they cor-
respond to frequently used operations and are specif-
ically designed to ensure compactness of the rule-
set. In contrast, as discussed earlier (Fig. 3), each of
these operations require several rules and possibly
custom code in existing rule-based IE systems.
DynamicDict: The DynamicDict rule is used to create
customized gazetteers on the fly. The following ex-
ample shows the need for such a rule: While ‘Clin-
ton’ does not always refer to a person’s last name
(Clinton is the name of several cities in USA), in
documents containing a full person name with ‘Clin-
ton’ as a last name (e.g., ‘Hillary Clinton’) it is rea-
sonable to annotate all references to the (possibly)
ambiguous word ‘Clinton’ as a person. This goal
can be accomplished using the rule &lt;Create Dynamic
Dictionary using Person with length 1 to 2 tokens&gt;,
which creates a gazetteer on a per-document basis.
Filter: The Filter rule is used to discard/retain cer-
tain intermediate annotations based on predicates on
the annotation text and its local context. Example
filtering predicates include
</bodyText>
<listItem confidence="0.999978">
• Discard C If Matches Regular Expression R
• Retain C If Contains Dictionary D on Local Context LC
• Discard C If Overlaps Concepts C1, C2, .. .
</listItem>
<bodyText confidence="0.8022952">
ModifySpan: The ModifySpan rule is used to expand
or trim the span of a candidate annotation. For
instance, an Entity Boundary customization to in-
clude generational markers as part of a Person anno-
tation can be implemented using a ModifySpan rule
</bodyText>
<table confidence="0.94542325">
&lt;Expand Person Using Dictionary ‘generation.dict’ on
RightContext 2 Tokens&gt;.
Using NERL Tab. 2 shows how different types of
rules are used during rule building and customiza-
tions. Since BR and CS involve identifying one
1006
Rule Category Syntax BR CDD CEB CG
CS CDSD CATA
Dictionary FD Evaluate Dictionaries &lt; D1, D2,... &gt; with flags? X X
Regex FD Evaluate Regular Expressions &lt; R1, R2,... &gt; with flags? X X
PoS FD Evaluate Part of Speech &lt; P1, P2,... &gt; with language &lt; L &gt;? X X
DynamicDict FD Create Dynamic Dictionary using IntConcept with flags? X X
Sequence CD IntConceptorString multiplicity? X X
(followed by IntConceptorString multiplicity?)+
Filter CR Discard/Retain IntConcept(As IntConcept)? X X X
If SatisfiesPredicate on LocalContext
ModifySpan CR Trim/Expand IntConcept Using Dictionary &lt; D &gt; X X
on LocalContext
Augment CO Augment IntConcept With IntConcept X X
Consolidate CO Consolidate IntConcept using ConsolidationPolicy X X
</table>
<tableCaption confidence="0.999101">
Table 2: Description of rules supported in NERL
</tableCaption>
<bodyText confidence="0.999827857142857">
or more entity (sub)types from scratch, all types
of rules are used. CDD and CDSD identify addi-
tional instances for an existing type and therefore
mainly rely on FD and CD rules. On the other hand,
the customizations that modify existing instances
(CEB,CATA,CG) require CR and CO rules.
Revisiting the example in Fig. 2, CR rules were
used to implement a fairly sophisticated customiza-
tion in a compact fashion, as follows. Rule CR1
first identifies sports articles using a regular expres-
sion based on the article title. Rule CR2 marks
Locations within these articles as LocationMaybeOrg
and Rule CR3 only retains those occurrences that
match a city, county or state name (e.g., ‘Seattle’).
Rule CR4 identifies occurrences that have a contex-
tual clue confirming that the mention was to a lo-
cation (e.g., ‘In’ or ‘At’). These occurrences are al-
ready classified correctly as Location and do not need
to be changed. Finally, CR5 adds the remaining am-
biguous mentions to Organization, which would be
deleted from Location by a subsequent core rule.
</bodyText>
<sectionHeader confidence="0.899122" genericHeader="method">
4 Development and Customization of NER
extractors with NERL
</sectionHeader>
<bodyText confidence="0.999919095238095">
Using NERL, we have developed CoreNER, a
domain-independent generic library for multiple
NER extraction tasks commonly encountered in
practice, including Person, Organization, Location,
EmailAddress, PhoneNumber, URL, and DateTime, but
we shall focus the discussion on the first three tasks
(see Tab. 3 for entity definitions), since they are the
most challenging. In this section, we first overview
the process of developing CoreNER (Sec. 4.1). We
then describe how we have customized CoreNER
for three different domains (Sec. 4.2), and present
a quality comparison with best published results ob-
tained with state-of-the-art machine learning tech-
niques (Sec. 4.3). The tasks we consider are not re-
stricted to documents in a particular language, but
due to limited availability of non-English corpora
and extractors for comparison, our evaluation uses
English-language text. In Sec. 5 we shall elaborate
on the difficulties encountered while building and
customizing CoreNER using NERL and the lessons
we learned in the process.
</bodyText>
<subsectionHeader confidence="0.989509">
4.1 Developing CoreNER
</subsectionHeader>
<bodyText confidence="0.999861611111111">
We have built our domain independent CoreNER li-
brary using a variety of formal and informal text
(e.g. web pages, emails, blogs, etc.), and informa-
tion from public data sources such as the US Census
Bureau (Census, 2007) and Wikipedia.
The development process proceeded as follows.
We first collected dictionaries for each entity
type from different resources, followed by man-
ual cleanup when needed to categorize entries col-
lected into “strong” and “weak” dictionaries. For
instance, we used US Census data to create several
name dictionaries, placing ambiguous entries such
as ‘White’ and ‘Price’ in a dictionary of ambigu-
ous last names, while unambiguous entries such as
‘Johnson’ and ‘Williams’ went to the dictionary for
strict last names. Second, we developed FD and
CD rules to identify candidate entities based on the
way named entities generally occur in text. E.g.,
</bodyText>
<table confidence="0.9782788">
&lt;Salutation CapsWord CapsWord&gt; and &lt;FirstName
1007
Type Subtypes
PER individual
LOC Address, Boundary, Land-Region-Natural, Region-General,
Region-International, Airport, Buildings-Grounds, Path, Plant,
Subarea-Facility, Continent, Country-or-District, Nation,
Population-Center, State-or-Province
ORG Commercial, Educational, Government, Media, Medical-Science
Non-Governmental
</table>
<tableCaption confidence="0.999207">
Table 3: NER Task Types and Subtypes
</tableCaption>
<note confidence="0.746034">
LastName&gt; for Person, and &lt;CapsWord{1,3} OrgSuf-
fix&gt; and &lt;CapsWord{1,2} Industry&gt; for Organization.
</note>
<bodyText confidence="0.999561714285714">
We then added CR and CO rules to account for
contextual clues and overlapping annotations (e.g.,
Delete Person annotations appearing within an Orga-
nization annotation).
The final CoreNER library consists of 104 FD (in-
volving 68 dictionaries, 33 regexes and 3 dynamic
dictionaries), 74 CD, 123 CR and 102 CO rules.
</bodyText>
<subsectionHeader confidence="0.987975">
4.2 Customizing CoreNER
</subsectionHeader>
<bodyText confidence="0.999984333333333">
In this section we describe the process of customiz-
ing our domain-independent CoreNER library for
several different datasets. We start by discussing our
choice of datasets to use for customization.
Datasets For a rigorous evaluation of CoreNER’s
customizability, we require multiple datasets satis-
fying the following criteria: First, the datasets must
cover diverse sources and styles of text. Second,
the set of the most challenging NER tasks Person,
Organization and Location (see Tab. 3) considered
in CoreNER should be applicable to them. Finally,
they should be publicly available and preferably
have associated published results, against which we
can compare our experimental results. Towards this
end, we chose the following public datasets.
</bodyText>
<listItem confidence="0.997955222222222">
• CoNLL03 (Tjong et al., 2003): a collection of
Reuters news stories. Consists of formal text.
• Enron (Minkov et al., 2005): a collection of
emails with meeting information from the Enron
dataset. Contains predominantly informal text.
• ACE05 (NIST, 2005)1 a collection of broadcast
news, broadcast conversations and newswire re-
ports. Consists of both formal and informal text.
Customization Process The goal of customization
</listItem>
<bodyText confidence="0.988973730769231">
1The evaluation test set is not publicly available. Thus, fol-
lowing the example of (Florian et al., 2006), the publicly avail-
able set is split into a 80%/20% data split, with the last 20% of
the data in chronological order selected as test data.
is to refine the original CoreNER (hence referred
to as CoreNERoTig) in order to improve its extrac-
tion quality on the training set (in terms of F0=1)
for each dataset individually. In addition, a devel-
opment set is available for CoNLL03 (referred to as
CoNLL03de7), therefore we seek to improve F0=1 on
CoNLL03de7 as well.
The customization process for each dataset pro-
ceeded as follows. First, we studied the entity defini-
tions and identified their differences when compared
with the definitions used for CoreNERorig (Tab. 3).
We then added rules to account for the differences.
For example, the definition of Organization in the
CoNLL03 dataset contained a sports organization
subtype, which was not considered when develop-
ing CoreNER. Therefore, we have used public data
sources (e.g., Wikipedia) to collect and curate dic-
tionaries of major sports associations and sport clubs
from around the world. The new dictionaries, along
with regular expressions identifying sports teams in
sports articles were used for defining FD and CD
rules such as CR1(Fig. 2). Finally, CR and CO rules
were added to filter invalid candidates and augment
the Organization type with the new sports subtype
(similar in spirit to rules CR4 and CR5 in Fig. 2).
In addition to the train and development sets, the
customization process for CoNLL03 also involved
unlabeled data from the corpus as follows. 1) Since
data-driven rules (CDD) are often created based on a
few instances from the training data, testing them on
the unlabeled data helped fine tune the rules for pre-
cision. 2) CoNLL03 is largely dominated by sports
news, but only a subset of all sports were represented
in the train dataset. Using the unlabeled data, we
were able to add CDD rules for five additional types
of sports, resulting in 0.31% improvement in F0=1
score on CoNLL03de7. 3) Unlabeled data was also
useful in identifying domain-specific gazetteers by
using simple extraction rules followed by a man-
ual cleanup phase. For instance, for CoNLL03 we
collected five gazetteers of organization and person
names from the unlabeled data, resulting in 0.45%
improvement in recall for CoNLL03de7.
The quality of the customization on the train col-
lections is shown in Tab. 5. The total number of
rules added during customization for each of the
three domains is listed in Tab. 4. Notice how rules
of all four types are used both in the development
</bodyText>
<table confidence="0.9828524">
1008
FD CD CR CO
CoreNERorig 104 74 123 102
CoreNERconll 179 56 284 71
CoreNERenron 13 10 9 1
CoreNERace 83 35 117 26
Precision Recall Fβ=1
CoreNERorig 83.81 61.77 71.12
CoNLL03dev CoreNERconll 96.49 93.76 95.11
Improvement 12.68 31.99 13.99
</table>
<tableCaption confidence="0.99184">
Table 4: Rules added during customization CoreNERorig 77.21 54.87 64.15
</tableCaption>
<table confidence="0.9974706">
CoNLL03test CoreNERconll 93.89 89.75 91.77
Precision Recall Fβ=1
CoreNERconll 97.64 95.60 96.61
CoreNERenron 91.15 92.58 91.86
CoreNERace 92.32 91.22 91.77
</table>
<tableCaption confidence="0.999673">
Table 5: Quality of customization on train datasets (%)
</tableCaption>
<bodyText confidence="0.999961894736842">
of the domain independent NER annotator, and dur-
ing customizations for different domains. A total of
8 person weeks were spent on customizations, and
we believe this effort is quite reasonable by rule-
based extraction standards. For example, (Maynard
et al., 2003) reports that customizing the ANNIE
domain independent NER annotator developed us-
ing the JAPE grammar-based rule language for the
ACE05 dataset required 6 weeks (and subsequent
tuning over the next 6 months), resulting in im-
proving the quality to 82% for this dataset. As we
shall discuss shortly, with similar manual effort, we
were able to achieve results outperforming state-of-
art published results on three different datasets, in-
cluding ACE05. However, one may rightfully ar-
gue that the process is still too lengthy impeding the
widespread deployment of rule-based NER extrac-
tion. We elaborate on the effort involved and the
lessons learned in the process in Sec. 5.
</bodyText>
<subsectionHeader confidence="0.999757">
4.3 Evaluation of Customization
</subsectionHeader>
<bodyText confidence="0.999666153846154">
We now present an experimental evaluation of the
customizability of CoreNER. The main goals are
to investigate: (i) the feasibility of CoreNER cus-
tomization for different application domains; (ii)
the effectiveness of such customization compared to
state-of-the-art results; (iii) the impact of different
types of customization (Tab. 1); and (iv) how often
different categories of NERL rules (Tab. 2) are used
during customization.
We measured the effectiveness of customization
using the improvement in extraction quality of the
customized CoreNER over CoreNERorig. As shown
in Tab. 6, customization significantly improved
</bodyText>
<table confidence="0.994338">
Improvement 15.68 34.88 27.62
CoreNERorig 85.06 69.55 76.53
Enron CoreNERenron 88.41 82.39 85.29
Improvement 3.35 12.84 8.76
CoreNERorig 57.23 57.41 57.32
ACE2005 CoreNERace 90.11 87.82 88.95
Improvement 32.88 30.41 31.63
</table>
<tableCaption confidence="0.918267">
Table 6: Overall Improvement due to Customization (%)
</tableCaption>
<table confidence="0.9992044375">
Precision Recall Fβ=1
LOC CoreNERconll 97.17 95.37 96.26
Florian 96.59 95.65 96.12
ORG CoreNERconll 93.70 88.67 91.11
Florian 90.85 89.63 90.24
PER CoreNERconll 97.79 95.87 96.82
Florian 96.08 97.12 96.60
LOC CoreNERconll 93.11 91.61 92.35
Florian 90.59 91.73 91.15
ORG CoreNERconll 92.25 85.31 88.65
Florian 85.93 83.44 84.67
PER CoreNERconll 96.32 92.39 94.32
Florian 92.49 95.24 93.85
PER CoreNERenron 87.27 81.82 84.46
Enron
Minkov 81.1 74.9 77.9
</table>
<tableCaption confidence="0.999944">
Table 7: Comparison with state-of-the-art results(%)
</tableCaption>
<bodyText confidence="0.984314714285714">
Fa=1 score for CoreNERorig across all datasets. 2
We note that the extraction quality of
CoreNERorig was low on CoNLL03 and ACE05
mainly due to differences in entity type definitions.
In particular, sports organizations, which occurred
frequently in the CoNLL03 collection, were not
considered during the development of CoreNERorig,
while in ACE05, ORG and LOC entity types were
split into four entity types (Organization, Location,
Geo-Political Entity and Facility). Customizations
such as CS and CG address the above changes
in named-entity type definition and substantially
improve the extraction quality of CoreNERorig.
Next, we compare the extraction quality of the
</bodyText>
<tableCaption confidence="0.4476855">
2CoNLL03dev and CoNLL03test correspond to the develop-
ment and test sets for CoNLL03 respectively.
</tableCaption>
<figure confidence="0.757754666666667">
CoNLL03dev
CoNLL03test
1009
</figure>
<bodyText confidence="0.9981710625">
customized CoreNER for CoNLL03 and Enron3 with
the corresponding best published results by (Florian
et al., 2003) and (Minkov et al., 2005). Tab. 7 shows
that our customized CoreNER outperforms the cor-
responding state-of-the-art numbers for all the NER
tasks on both CoNLL03 and Enron. 4 These results
demonstrate that high-quality annotators can be built
by customizing CoreNERoTZ9 using NERL, with the
final extraction quality matching that of state-of-the-
art machine learning-based extractors.
It is worthwhile noting that the best pub-
lished results for CoNLL03 (Florian et al., 2003)
were obtained by using four different classifiers
(Robust Risk Minimization, Maximum Entropy,
Transformation-based learning, and Hidden Markov
Model) and trying six different classifier combi-
nation methods. Compared to the best published
result obtained by combining the four classifiers,
the individual classifiers performed between 2.5-
7.6% worse for Location, 5.6-15.2% for Organiza-
tion and 3.9-14.0% for Person5. Taking this into
account, the extraction quality advantage of cus-
tomized CoreNER is significant when compared
with the individual state-of-the-art classifiers.
Impact of Customizations by Type. While cus-
tomizing CoreNER for the three datasets, all types
of changes described in Sec. 2 were performed. We
measured the impact of each type of customization
by comparing the extraction quality of CoreNERoTZ9
with CoreNERoTZ9 enhanced with all the customiza-
tions of that type. From the results for CoNLL03
(Tab. 8), we make the following observations.
</bodyText>
<listItem confidence="0.9294012">
• Customizations that identify additional subtypes
of entities (CS) or modify existing instances for
multiple types (CATA) have significant impact.
This effect can be especially high when the miss-
ing subtype appears very often in the new do-
main (E.g., over 50% of the organizations in
CoNLL03 are sports teams).
• Data-driven customizations (CDD) rely on the
aggregated impact of many rules. While individ-
ual rules may have considerable impact on their
</listItem>
<footnote confidence="0.9417616">
3We cannot meaningfully compare our results against previ-
ously published results for ACE05, which is originally used for
mention detection while CoreNER considers only NER tasks.
4For Enron the comparison is reported only for Person, as
labeled data is available only for that type.
</footnote>
<note confidence="0.604361">
5Extended version obtained via private communication.
</note>
<table confidence="0.999984794117647">
# rules added Precision Recall Fp=1
CEB 3 CoNLL03dev LOC T0.21 T0.22 T0.22
ORG T1.35 T0.38 T0.59
PER - - -
CoNLL03test LOC T0.30 T0.36 T0.33
ORG T0.54 T0.12 T0.20
PER - - -
LOC T7.18 10.87 T3.19
CoNLL03dev ORG T1.37 T10.67 T9.04
PER 10.04 - 10.01
CATA 5
LOC T7.73 11.20 T3.77
CoNLL03test ORG T1.37 T11.62 T14.18
PER - - -
LOC T0.85 - T0.45
CoNLL03dev ORG T1.00 10.07 T0.01
PER - - -
CDSD 2
LOC T0.04 10.12 10.12
CoNLL03test ORG T0.64 - T0.04
PER - - -
LOC T1.63 10.21 T0.85
CoNLL03dev ORG T11.44 T40.79 T39.73
CS 149 PER T0.13 - T0.05
LOC T3.71 10.18 T2.05
CoNLL03test ORG T9.2 T36.24 T37.96
PER T0.58 - T0.2
LOC 10.94 T10.18 T3.99
CoNLL03dev ORG T9.63 T11.93 T14.71
PER T6.12 T28.5 T18.84
CDD 431
LOC 11.66 T6.72 T1.64
CoNLL03test ORG T8.84 T12.40 T15.90
PER T9.15 T31.48 T22.21
</table>
<tableCaption confidence="0.99964">
Table 8: Impact by customization type on CoNLL03(%)
</tableCaption>
<bodyText confidence="0.999097">
own (e.g., identifying all names that appear as
part of a player list increases the recall of PER by
over 6% on both CoNLL03de7 and CoNLL03test),
the overall impact relies on the accumulative ef-
fect of many small improvements.
</bodyText>
<listItem confidence="0.647476333333333">
• Certain customizations (CEB and CDSD) pro-
vide smaller quality improvements, both per rule
and in aggregate.
</listItem>
<sectionHeader confidence="0.975691" genericHeader="method">
5 Lessons Learned
</sectionHeader>
<bodyText confidence="0.984717142857143">
Our experimental evaluation shows that rule-based
annotators can achieve quality comparable to that of
state-of-the-art machine learning techniques. In this
section we discuss three important lessons learned
regarding the human effort involved in developing
such rule-based extractors.
Usefulness of NERL We found NERL very helpful
in that it provided a higher-level abstraction catered
specifically towards NER tasks, thus hiding the com-
plexity inherent in a general-purpose IE rule lan-
guage. In doing so, NERL restricts the large space
of operations possible within a general-purpose lan-
guage to the small number of predefined “templates”
1010
listed in Tab. 2. (We have shown empirically that our
choice of NERL rules is sufficient to achieve high
accuracy for NER tasks.) Therefore, NERL simpli-
fies development and maintenance of complex NER
extractors, since one does not need to worry about
multiple AQL statements or JAPE grammar phases
for implementing a single conceptual operation such
as filtering (see Fig. 3).
Is NERL Sufficient? Even using NERL, building
and customizing NER rules remains a labor inten-
sive process. Consider the example of designing the
filter rule CR4 from Fig. 3. First, one must exam-
ine multiple false positive Location entities to even
decide that a filter rule is appropriate. Second, one
must understand how those false positives were pro-
duced, and decide accordingly on the particular con-
cept to be used as filter (LocationMaybeOrg in this
case). Finally, one needs to decide how to build the
filter. Tab. 9 lists all the attributes that need to be
specified for a Filter rule, along with examples of
the search space for each rule attribute.
</bodyText>
<table confidence="0.998889333333333">
Rule Attributes Examples of Search Space
Location Intermediate Concept to filter
Predicate Type Matches Regex, Contains Dictionary, .. .
Predicate Parameter Regular Expressions, Dictionary Entries, .. .
Context Type Entity text, Left or Right context
Context Parameter k tokens, l characters
</table>
<tableCaption confidence="0.999733">
Table 9: Search space explored while adding a Filter rule
</tableCaption>
<bodyText confidence="0.999602825">
This search space problem is not unique to filter
rules. In fact, most rules in Tab. 2 have two or more
rule attributes. Therefore, designing an individual
NERL rule remains a time-consuming “trial and er-
ror” process, in which multiple “promising” combi-
nations are implemented and evaluated individually
before deciding on a satisfactory final rule.
Tooling for NERL The fact that NERL is a high-
level language exposing a restricted set of operators
can be exploited to reduce the human effort involved
in building NER annotators by enabling the follow-
ing tools:
Annotation Provenance Tools tracking prove-
nance (Cheney et al., 2009) for NERL rules can
help in explaining exactly which sequence of rules
is responsible for producing a given false positive,
thereby enabling one to quickly identify “misbe-
haved” rules. For instance, one can quickly narrow
down the choices for the location where the filter
rule CR4 (Fig. 2) should be applied based on the
provenance of the false positives. Similarly, tools
for explaining false positives in the spirit of (Huang
et al., 2008), are also conceivable.
Automatic Parameter Learning The most time-
consuming part in building a rule often is to decide
the value of its parameters, especially for FD and
CR rules. For instance, while defining a CR rule,
one has to choose values for the Predicate parame-
ter and the Context parameter (see Tab. 9). Some
parameter values can be learned – for example, dic-
tionaries (Riloff, 1993) and regular expressions (Li
et al., 2008).
Automatic Rule Refinement Tools automatically
suggesting entire customization rules to a complex
NERL program in the spirit of (Liu et al., 2010) can
further reduce human effort in building NER anno-
tators. With the help of such tools, one only needs
to consider good candidate NERL rules suggested
by the system without having to go through the
conventional manual “trial and error” process.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999987">
In this paper, we described NERL, a high-level rule
language for building and customizing NER annota-
tors. We demonstrated that a complex NER annota-
tor built using NERL can be effectively customized
for different domains, achieving extraction quality
superior to the state-of-the-art numbers. However,
our experience also indicates that the process of de-
signing the rules themselves is still manual and time-
consuming. Finally, we discuss how NERL opens
up several interesting research directions towards the
development of sophisticated tooling for automating
some of the rule development tasks.
</bodyText>
<sectionHeader confidence="0.996415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998869931623932">
D. E. Appelt and B. Onyshkevych. 1998. The common
pattern specification language. In TIPSTER workshop.
A. Arnold, R. Nallapati, and W. W. Cohen. 2008.
Exploiting feature hierarchy for transfer learning in
named entity recognition. In ACL.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel.
1999. An algorithm that learns what’s in a name. In
Machine Learning, volume 34, pages 211–231.
J. Blitzer, R. Mcdonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP.
1011
B. Boguraev. 2003. Annotation-based finite state pro-
cessing in a large-scale nlp arhitecture. In RANLP.
Census. 2007. U.S. Census Bureau.
http://www.census.gov.
J. Cheney, L. Chiticariu, and W. Tan. 2009. Provenance
in databases: Why, how, and where. Foundations and
Trends in Databases, 1(4):379–474.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010. SystemT: An
algebraic approach to declarative information extrac-
tion. In ACL.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Second Edi-
tion). Research Memorandum CS–00–10, Department
of Computer Science, University of Sheffield.
W. Drozdzynski, H. Krieger, J. Piskorski, U. Sch¨afer, and
F. Xu. 2004. Shallow processing with unification and
typed feature structures — foundations and applica-
tions. K¨unstliche Intelligenz, 1:17–23.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005.
Unsupervised named-entity extraction from the web:
an experimental study. Artif. Intell., 165(1):91–134.
J. R. Finkel and C. D. Manning. 2009. Nested named
entity recognition. In EMNLP.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In CoNLL.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In NAACL-HLT.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006.
Factorizing complex models: A case study in mention
detection. In ACL.
D. Gruhl, M. Nagarajan, J. Pieper, C. Robson, and
A. Sheth. 2009. Context and domain knowledge en-
hanced entity spotting in informal text. In ISWC.
J. Huang, G. Zweig, and M. Padmanabhan. 2001. Infor-
mation extraction from voicemail. In ACL.
Jiansheng Huang, Ting Chen, AnHai Doan, and Jeffrey F.
Naughton. 2008. On the Provenance of Non-Answers
to Queries over Extracted Data. PVLDB, 1(1):736–
747.
M. Jansche and S. Abney. 2002. Information extraction
from voicemail transcripts. In EMNLP.
J. Jiang and C. Zhai. 2006. Exploiting domain structure
for named entity recognition. In NAACL-HLT.
Saul Kripke. 1982. Naming and Necessity. Harvard Uni-
versity Press.
G. R. Krupka and K. Hausman. 2001. IsoQuest Inc.: De-
scription of the NetOwlTM extractor system as used
for MUC-7. In MUC-7.
Y. Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan,
and H. V. Jagadish. 2008. Regular expression learning
for information extraction. In EMNLP.
B. Liu, L. Chiticariu, V. Chu, H. V. Jagadish, and F. Reiss.
2010. Automatic Rule Refinement for Information
Extraction. PVLDB, 3.
D. Maynard, K. Bontcheva, and H. Cunningham. 2003.
Towards a semantic extraction of named entities. In
RANLP.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. In CoNLL.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting personal names from emails: Applying named
entity recognition to informal text. In HLT/EMNLP.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Linguisticae In-
vestigationes, 30(1):3–26.
NIST. 2005. The ace evaluation plan.
F. J. Och O. Bender and H. Ney. 2003. Maximum en-
tropy models for named entity recognition. In CoNLL.
G. Petasis, F. Vichot, F. Wolinski, G. Paliouras,
V. Karkaletsis, and C. Spyropoulos. 2001. Using
machine learning to maintain rule-based named-entity
recognition and classification systems. In ACL.
T. Poibeau and L. Kosseim. 2001. Proper name ex-
traction from non-journalistic texts. In Computational
Linguistics in the Netherlands, pages 144–157.
E. Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In KDD.
S. Sekine and C. Nobata. 2004. Definition, dictionaries
and tagger for extended named entity hierarchy. In
Conference on Language Resources and Evaluation.
W. Shen, A. Doan, J. F. Naughton, and R. Ramakrishnan.
2007. Declarative information extraction using data-
log with embedded extraction predicates. In VLDB.
S. Singh, D. Hillard, and C. Leggeteer. 2010. Minimally-
supervised extraction of entities from text advertise-
ments. In NAACL-HLT.
P. Siniakov. 2010. GROPUS - an adaptive rule-based al-
gorithm for information extraction. Ph.D. thesis, Freie
Universitat Berlin.
R. Srihari, C. Niu, and W. Li. 2001. A hybrid approach
for named entity and sub-type tagging. In ANLP.
E. F. Tjong, K. Sang, and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In CoNLL.
D. Wu, W. S. Lee, N. Ye, and H. L. Chieu. 2009. Domain
adaptive bootstrapping for named entity recognition.
In EMNLP.
J. Zhu, V. Uren, and E. Motta. 2005. Espotter: Adaptive
named entity recognition for web browsing. In WM.
</reference>
<page confidence="0.831942">
1012
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938608">
<title confidence="0.999017">Domain Adaptation of Rule-Based for Named-Entity Recognition Tasks</title>
<author confidence="0.999464">Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li Frederick Reiss Shivakumar</author>
<affiliation confidence="0.999837">IBM Research –</affiliation>
<address confidence="0.999922">650 Harry Road, San Jose, CA 95120,</address>
<email confidence="0.998458">rajase,yunyaoli,shiv@almaden.ibm.com</email>
<abstract confidence="0.997935464285714">Named-entity recognition (NER) is an important task required in a wide variety of applications. While rule-based systems are appealing due to their well-known “explainability,” most, if not all, state-of-the-art results for NER tasks are based on machine learning techniques. Motivated by these results, we explore the following natural question in this parule-based systems still a viable apto named-entity recognition? Specifically, we have designed and implemented high-level language top of SystemT, a general-purpose algebraic informaextraction system. tuned to the needs of NER tasks and simplifies the process of building, understanding, and customizing complex rule-based named-entity annotators. We show that these customized annotators match or outperform the best published results achieved with machine learning techniques. These results confirm that we can reap the benefits of rule-based extractors’ explainability without sacrificing accuracy. We conclude by discussing lessons learned while building and customizing complex rule-based annotators and outlining several research directions towards facilitating rule development.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D E Appelt</author>
<author>B Onyshkevych</author>
</authors>
<title>The common pattern specification language. In</title>
<date>1998</date>
<booktitle>TIPSTER workshop.</booktitle>
<contexts>
<context position="13585" citStr="Appelt and Onyshkevych, 1998" startWordPosition="2088" endWordPosition="2091">mmarized as shown in Tab. 1, based on the following criteria: (i) whether the customization identifies new instances or modifies existing instances; and (ii) whether the 1004 customization affects single or multiple entities. For instance, CS identifies new instances for a single entity type, as it adds instances of a new subtype for an existing entity type. Note that BR in the table denotes the rules used to build the core annotator. 3 Named Entity Rule Language 3.1 Grammar vs. Algebraic NER Traditionally, rule-based NER systems were based on the popular CPSL cascading grammar specification (Appelt and Onyshkevych, 1998). CPSL is designed so that rules that adhere to the standard can be executed efficiently with finite state transducers. Accordingly, the standard defines a rigid left-toright execution model where a region of text can be matched by at most one rule according to a fixed rule priority, and where overlapping annotations are disallowed in the output of each grammar phase. While it simplifies the design of CPSL engines, the rigidity of the rule matching semantics makes it difficult to express operations frequently used in rule-based information extraction. These limitations have been recognized in </context>
</contexts>
<marker>Appelt, Onyshkevych, 1998</marker>
<rawString>D. E. Appelt and B. Onyshkevych. 1998. The common pattern specification language. In TIPSTER workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Arnold</author>
<author>R Nallapati</author>
<author>W W Cohen</author>
</authors>
<title>Exploiting feature hierarchy for transfer learning in named entity recognition.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5488" citStr="Arnold et al., 2008" startWordPosition="827" endWordPosition="830">e.g., In Seattle ) // These references should not be reclassified as Organization CR4 Discard &lt;LocationMaybeOrg&gt; If Matches Regular Expression &lt;R2&gt; on Left Context 2 Tokens Customization Requirement: City, County or State names within sports articles may refer to a sports team or to the location itself. Customization Solution (CS) : Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location Figure 1: Example Customization Requirement al., 2004; Blitzer et al., 2006; Jiang and Zhai, 2006; Arnold et al., 2008; Wu et al., 2009). Implementing a similar approach for rule-based NER typically requires a significant amount of manual effort to (a) identify the explicit semantic changes required for the new domain (e.g., differences in entity type definition), (b) identify the portions of the (complex) core annotator that should be modified for each difference and (c) implement the required customization rules without compromising the extraction quality of the core annotator. Domain customization of rule-based NER has not received much attention in the recent literature with a few exceptions (Petasis et a</context>
</contexts>
<marker>Arnold, Nallapati, Cohen, 2008</marker>
<rawString>A. Arnold, R. Nallapati, and W. W. Cohen. 2008. Exploiting feature hierarchy for transfer learning in named entity recognition. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>R L Schwartz</author>
<author>R M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>In Machine Learning,</booktitle>
<volume>34</volume>
<pages>211--231</pages>
<contexts>
<context position="1816" citStr="Bikel et al., 1999" startWordPosition="262" endWordPosition="265"> confirm that we can reap the benefits of rule-based extractors’ explainability without sacrificing accuracy. We conclude by discussing lessons learned while building and customizing complex rule-based annotators and outlining several research directions towards facilitating rule development. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation </context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. 1999. An algorithm that learns what’s in a name. In Machine Learning, volume 34, pages 211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R Mcdonald</author>
<author>F Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5445" citStr="Blitzer et al., 2006" startWordPosition="819" endWordPosition="822">s in sports articles may refer to the city (e.g., In Seattle ) // These references should not be reclassified as Organization CR4 Discard &lt;LocationMaybeOrg&gt; If Matches Regular Expression &lt;R2&gt; on Left Context 2 Tokens Customization Requirement: City, County or State names within sports articles may refer to a sports team or to the location itself. Customization Solution (CS) : Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location Figure 1: Example Customization Requirement al., 2004; Blitzer et al., 2006; Jiang and Zhai, 2006; Arnold et al., 2008; Wu et al., 2009). Implementing a similar approach for rule-based NER typically requires a significant amount of manual effort to (a) identify the explicit semantic changes required for the new domain (e.g., differences in entity type definition), (b) identify the portions of the (complex) core annotator that should be modified for each difference and (c) implement the required customization rules without compromising the extraction quality of the core annotator. Domain customization of rule-based NER has not received much attention in the recent lit</context>
</contexts>
<marker>Blitzer, Mcdonald, Pereira, 2006</marker>
<rawString>J. Blitzer, R. Mcdonald, and F. Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
</authors>
<title>Annotation-based finite state processing in a large-scale nlp arhitecture.</title>
<date>2003</date>
<booktitle>In RANLP.</booktitle>
<contexts>
<context position="14363" citStr="Boguraev, 2003" startWordPosition="2212" endWordPosition="2213">ft-toright execution model where a region of text can be matched by at most one rule according to a fixed rule priority, and where overlapping annotations are disallowed in the output of each grammar phase. While it simplifies the design of CPSL engines, the rigidity of the rule matching semantics makes it difficult to express operations frequently used in rule-based information extraction. These limitations have been recognized in the literature, and several extensions have been proposed to allow more flexible matching semantics, and to allow overlapping annotations (Cunningham et al., 2000; Boguraev, 2003; Drozdzynski et al., 2004). However, even with these extensions, common operations such as filtering annotations (e.g. CR4 in Fig. 2), are difficult to express in grammars and often require an escape to custom procedural code. Recently, several declarative algebraic languages have been proposed for rule-based IE systems, notably AQL (Chiticariu et al., 2010) and Xlog (Shen et al., 2007). These languages are not constrained by the requirement that all rules map onto finite state transducers, and therefore can express a significantly richer semantics than grammar-based languages. In particular,</context>
</contexts>
<marker>Boguraev, 2003</marker>
<rawString>B. Boguraev. 2003. Annotation-based finite state processing in a large-scale nlp arhitecture. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Census</author>
</authors>
<date>2007</date>
<note>U.S. Census Bureau. http://www.census.gov.</note>
<contexts>
<context position="25609" citStr="Census, 2007" startWordPosition="3953" endWordPosition="3954"> 4.3). The tasks we consider are not restricted to documents in a particular language, but due to limited availability of non-English corpora and extractors for comparison, our evaluation uses English-language text. In Sec. 5 we shall elaborate on the difficulties encountered while building and customizing CoreNER using NERL and the lessons we learned in the process. 4.1 Developing CoreNER We have built our domain independent CoreNER library using a variety of formal and informal text (e.g. web pages, emails, blogs, etc.), and information from public data sources such as the US Census Bureau (Census, 2007) and Wikipedia. The development process proceeded as follows. We first collected dictionaries for each entity type from different resources, followed by manual cleanup when needed to categorize entries collected into “strong” and “weak” dictionaries. For instance, we used US Census data to create several name dictionaries, placing ambiguous entries such as ‘White’ and ‘Price’ in a dictionary of ambiguous last names, while unambiguous entries such as ‘Johnson’ and ‘Williams’ went to the dictionary for strict last names. Second, we developed FD and CD rules to identify candidate entities based o</context>
</contexts>
<marker>Census, 2007</marker>
<rawString>Census. 2007. U.S. Census Bureau. http://www.census.gov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cheney</author>
<author>L Chiticariu</author>
<author>W Tan</author>
</authors>
<date>2009</date>
<booktitle>Provenance in databases: Why, how, and where. Foundations and Trends in Databases,</booktitle>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="40823" citStr="Cheney et al., 2009" startWordPosition="6307" endWordPosition="6310"> space problem is not unique to filter rules. In fact, most rules in Tab. 2 have two or more rule attributes. Therefore, designing an individual NERL rule remains a time-consuming “trial and error” process, in which multiple “promising” combinations are implemented and evaluated individually before deciding on a satisfactory final rule. Tooling for NERL The fact that NERL is a highlevel language exposing a restricted set of operators can be exploited to reduce the human effort involved in building NER annotators by enabling the following tools: Annotation Provenance Tools tracking provenance (Cheney et al., 2009) for NERL rules can help in explaining exactly which sequence of rules is responsible for producing a given false positive, thereby enabling one to quickly identify “misbehaved” rules. For instance, one can quickly narrow down the choices for the location where the filter rule CR4 (Fig. 2) should be applied based on the provenance of the false positives. Similarly, tools for explaining false positives in the spirit of (Huang et al., 2008), are also conceivable. Automatic Parameter Learning The most timeconsuming part in building a rule often is to decide the value of its parameters, especially</context>
</contexts>
<marker>Cheney, Chiticariu, Tan, 2009</marker>
<rawString>J. Cheney, L. Chiticariu, and W. Tan. 2009. Provenance in databases: Why, how, and where. Foundations and Trends in Databases, 1(4):379–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chiticariu</author>
<author>R Krishnamurthy</author>
<author>Y Li</author>
<author>S Raghavan</author>
<author>F Reiss</author>
<author>S Vaithyanathan</author>
</authors>
<title>SystemT: An algebraic approach to declarative information extraction.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7339" citStr="Chiticariu et al., 2010" startWordPosition="1109" endWordPosition="1112">form when adapting to a new domain (Sec. 2), (ii) identifying a set of high-level operations required for building and customizing NER annotators, and (iii) exposing these operations in a domain-specific NER rule language, NERL, developed on top of Sys// City references to sports teams are added to Organization and removed from Location CR5 Augment &lt;Organization&gt; With &lt;LocationMaybeOrg&gt; // End customization // Continuation of core rules // Remove Locations that overlap with Organizations Discard &lt;Location&gt; If Overlaps Concepts &lt;Organization&gt; Figure 2: Example Customization Rules in NERL temT (Chiticariu et al., 2010), a general-purpose algebraic information extraction system (Sec. 3). NERL is specifically geared towards building and customizing complex NER annotators and makes it easy to understand a complex annotator that may comprise hundreds of rules. It simplifies the identification of what portions need to be modified for a given customization requirement. It also makes individual customizations easier to implement, as illustrated by the following example. Suppose we have to customize a domainindependent rule-based NER annotator for the CoNLL corpus (Tjong et al., 2003). Consider the two sports-relat</context>
<context position="14724" citStr="Chiticariu et al., 2010" startWordPosition="2265" endWordPosition="2268">used in rule-based information extraction. These limitations have been recognized in the literature, and several extensions have been proposed to allow more flexible matching semantics, and to allow overlapping annotations (Cunningham et al., 2000; Boguraev, 2003; Drozdzynski et al., 2004). However, even with these extensions, common operations such as filtering annotations (e.g. CR4 in Fig. 2), are difficult to express in grammars and often require an escape to custom procedural code. Recently, several declarative algebraic languages have been proposed for rule-based IE systems, notably AQL (Chiticariu et al., 2010) and Xlog (Shen et al., 2007). These languages are not constrained by the requirement that all rules map onto finite state transducers, and therefore can express a significantly richer semantics than grammar-based languages. In particular, the AQL rule language as implemented in SystemT (Chiticariu et al., 2010) can express many common operations used in rule-based information extraction without requiring custom code. In addition, the separation of extraction semantics from execution enables SystemT’s rule optimizer and efficient runtime engine. Indeed, as shown in (Chiticariu et al., 2010), S</context>
</contexts>
<marker>Chiticariu, Krishnamurthy, Li, Raghavan, Reiss, Vaithyanathan, 2010</marker>
<rawString>L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan, F. Reiss, and S. Vaithyanathan. 2010. SystemT: An algebraic approach to declarative information extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>V Tablan</author>
</authors>
<title>JAPE: a Java Annotation Patterns Engine (Second Edition). Research Memorandum CS–00–10,</title>
<date>2000</date>
<institution>Department of Computer Science, University of Sheffield.</institution>
<contexts>
<context position="14347" citStr="Cunningham et al., 2000" startWordPosition="2208" endWordPosition="2211">andard defines a rigid left-toright execution model where a region of text can be matched by at most one rule according to a fixed rule priority, and where overlapping annotations are disallowed in the output of each grammar phase. While it simplifies the design of CPSL engines, the rigidity of the rule matching semantics makes it difficult to express operations frequently used in rule-based information extraction. These limitations have been recognized in the literature, and several extensions have been proposed to allow more flexible matching semantics, and to allow overlapping annotations (Cunningham et al., 2000; Boguraev, 2003; Drozdzynski et al., 2004). However, even with these extensions, common operations such as filtering annotations (e.g. CR4 in Fig. 2), are difficult to express in grammars and often require an escape to custom procedural code. Recently, several declarative algebraic languages have been proposed for rule-based IE systems, notably AQL (Chiticariu et al., 2010) and Xlog (Shen et al., 2007). These languages are not constrained by the requirement that all rules map onto finite state transducers, and therefore can express a significantly richer semantics than grammar-based languages</context>
</contexts>
<marker>Cunningham, Maynard, Tablan, 2000</marker>
<rawString>H. Cunningham, D. Maynard, and V. Tablan. 2000. JAPE: a Java Annotation Patterns Engine (Second Edition). Research Memorandum CS–00–10, Department of Computer Science, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Drozdzynski</author>
<author>H Krieger</author>
<author>J Piskorski</author>
<author>U Sch¨afer</author>
<author>F Xu</author>
</authors>
<title>Shallow processing with unification and typed feature structures — foundations and applications. K¨unstliche Intelligenz,</title>
<date>2004</date>
<pages>1--17</pages>
<marker>Drozdzynski, Krieger, Piskorski, Sch¨afer, Xu, 2004</marker>
<rawString>W. Drozdzynski, H. Krieger, J. Piskorski, U. Sch¨afer, and F. Xu. 2004. Shallow processing with unification and typed feature structures — foundations and applications. K¨unstliche Intelligenz, 1:17–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artif. Intell.,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="1862" citStr="Etzioni et al., 2005" startWordPosition="270" endWordPosition="273">ule-based extractors’ explainability without sacrificing accuracy. We conclude by discussing lessons learned while building and customizing complex rule-based annotators and outlining several research directions towards facilitating rule development. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artif. Intell., 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Nested named entity recognition.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2309" citStr="Finkel and Manning, 2009" startWordPosition="346" endWordPosition="349">deau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-based approach is still extremely appealing due to the associated transparency of the internal system state, which leads to better explainability of errors (Siniakov, 2010). Ideally, one would like to benefit from the transparency and explainability of rule-based techniques, while achieving state-of</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. R. Finkel and C. D. Manning. 2009. Nested named entity recognition. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>T Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="2260" citStr="Florian et al., 2003" startWordPosition="338" endWordPosition="341">h as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-based approach is still extremely appealing due to the associated transparency of the internal system state, which leads to better explainability of errors (Siniakov, 2010). Ideally, one would like to benefit from the transparency and explainability o</context>
<context position="34743" citStr="Florian et al., 2003" startWordPosition="5341" endWordPosition="5344">red during the development of CoreNERorig, while in ACE05, ORG and LOC entity types were split into four entity types (Organization, Location, Geo-Political Entity and Facility). Customizations such as CS and CG address the above changes in named-entity type definition and substantially improve the extraction quality of CoreNERorig. Next, we compare the extraction quality of the 2CoNLL03dev and CoNLL03test correspond to the development and test sets for CoNLL03 respectively. CoNLL03dev CoNLL03test 1009 customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al., 2003) and (Minkov et al., 2005). Tab. 7 shows that our customized CoreNER outperforms the corresponding state-of-the-art numbers for all the NER tasks on both CoNLL03 and Enron. 4 These results demonstrate that high-quality annotators can be built by customizing CoreNERoTZ9 using NERL, with the final extraction quality matching that of state-of-theart machine learning-based extractors. It is worthwhile noting that the best published results for CoNLL03 (Florian et al., 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, </context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003. Named entity recognition through classifier combination. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In NAACL-HLT.</booktitle>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A statistical model for multilingual entity detection and tracking. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>I Zitouni</author>
</authors>
<title>Factorizing complex models: A case study in mention detection.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28399" citStr="Florian et al., 2006" startWordPosition="4353" endWordPosition="4356">pare our experimental results. Towards this end, we chose the following public datasets. • CoNLL03 (Tjong et al., 2003): a collection of Reuters news stories. Consists of formal text. • Enron (Minkov et al., 2005): a collection of emails with meeting information from the Enron dataset. Contains predominantly informal text. • ACE05 (NIST, 2005)1 a collection of broadcast news, broadcast conversations and newswire reports. Consists of both formal and informal text. Customization Process The goal of customization 1The evaluation test set is not publicly available. Thus, following the example of (Florian et al., 2006), the publicly available set is split into a 80%/20% data split, with the last 20% of the data in chronological order selected as test data. is to refine the original CoreNER (hence referred to as CoreNERoTig) in order to improve its extraction quality on the training set (in terms of F0=1) for each dataset individually. In addition, a development set is available for CoNLL03 (referred to as CoNLL03de7), therefore we seek to improve F0=1 on CoNLL03de7 as well. The customization process for each dataset proceeded as follows. First, we studied the entity definitions and identified their differen</context>
</contexts>
<marker>Florian, Jing, Kambhatla, Zitouni, 2006</marker>
<rawString>R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006. Factorizing complex models: A case study in mention detection. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gruhl</author>
<author>M Nagarajan</author>
<author>J Pieper</author>
<author>C Robson</author>
<author>A Sheth</author>
</authors>
<title>Context and domain knowledge enhanced entity spotting in informal text.</title>
<date>2009</date>
<booktitle>In ISWC.</booktitle>
<contexts>
<context position="2055" citStr="Gruhl et al., 2009" startWordPosition="307" endWordPosition="310">earch directions towards facilitating rule development. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-based approach is still extremely appealing du</context>
</contexts>
<marker>Gruhl, Nagarajan, Pieper, Robson, Sheth, 2009</marker>
<rawString>D. Gruhl, M. Nagarajan, J. Pieper, C. Robson, and A. Sheth. 2009. Context and domain knowledge enhanced entity spotting in informal text. In ISWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>G Zweig</author>
<author>M Padmanabhan</author>
</authors>
<title>Information extraction from voicemail.</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1961" citStr="Huang et al., 2001" startWordPosition="290" endWordPosition="293">earned while building and customizing complex rule-based annotators and outlining several research directions towards facilitating rule development. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on</context>
</contexts>
<marker>Huang, Zweig, Padmanabhan, 2001</marker>
<rawString>J. Huang, G. Zweig, and M. Padmanabhan. 2001. Information extraction from voicemail. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiansheng Huang</author>
<author>Ting Chen</author>
<author>AnHai Doan</author>
<author>Jeffrey F Naughton</author>
</authors>
<date>2008</date>
<booktitle>On the Provenance of Non-Answers to Queries over Extracted Data. PVLDB,</booktitle>
<volume>1</volume>
<issue>1</issue>
<pages>747</pages>
<contexts>
<context position="41265" citStr="Huang et al., 2008" startWordPosition="6380" endWordPosition="6383">e exploited to reduce the human effort involved in building NER annotators by enabling the following tools: Annotation Provenance Tools tracking provenance (Cheney et al., 2009) for NERL rules can help in explaining exactly which sequence of rules is responsible for producing a given false positive, thereby enabling one to quickly identify “misbehaved” rules. For instance, one can quickly narrow down the choices for the location where the filter rule CR4 (Fig. 2) should be applied based on the provenance of the false positives. Similarly, tools for explaining false positives in the spirit of (Huang et al., 2008), are also conceivable. Automatic Parameter Learning The most timeconsuming part in building a rule often is to decide the value of its parameters, especially for FD and CR rules. For instance, while defining a CR rule, one has to choose values for the Predicate parameter and the Context parameter (see Tab. 9). Some parameter values can be learned – for example, dictionaries (Riloff, 1993) and regular expressions (Li et al., 2008). Automatic Rule Refinement Tools automatically suggesting entire customization rules to a complex NERL program in the spirit of (Liu et al., 2010) can further reduce</context>
</contexts>
<marker>Huang, Chen, Doan, Naughton, 2008</marker>
<rawString>Jiansheng Huang, Ting Chen, AnHai Doan, and Jeffrey F. Naughton. 2008. On the Provenance of Non-Answers to Queries over Extracted Data. PVLDB, 1(1):736– 747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jansche</author>
<author>S Abney</author>
</authors>
<title>Information extraction from voicemail transcripts.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2013" citStr="Jansche and Abney, 2002" startWordPosition="298" endWordPosition="302">ule-based annotators and outlining several research directions towards facilitating rule development. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-base</context>
</contexts>
<marker>Jansche, Abney, 2002</marker>
<rawString>M. Jansche and S. Abney. 2002. Information extraction from voicemail transcripts. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>C Zhai</author>
</authors>
<title>Exploiting domain structure for named entity recognition.</title>
<date>2006</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="5467" citStr="Jiang and Zhai, 2006" startWordPosition="823" endWordPosition="826">ay refer to the city (e.g., In Seattle ) // These references should not be reclassified as Organization CR4 Discard &lt;LocationMaybeOrg&gt; If Matches Regular Expression &lt;R2&gt; on Left Context 2 Tokens Customization Requirement: City, County or State names within sports articles may refer to a sports team or to the location itself. Customization Solution (CS) : Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location Figure 1: Example Customization Requirement al., 2004; Blitzer et al., 2006; Jiang and Zhai, 2006; Arnold et al., 2008; Wu et al., 2009). Implementing a similar approach for rule-based NER typically requires a significant amount of manual effort to (a) identify the explicit semantic changes required for the new domain (e.g., differences in entity type definition), (b) identify the portions of the (complex) core annotator that should be modified for each difference and (c) implement the required customization rules without compromising the extraction quality of the core annotator. Domain customization of rule-based NER has not received much attention in the recent literature with a few exc</context>
</contexts>
<marker>Jiang, Zhai, 2006</marker>
<rawString>J. Jiang and C. Zhai. 2006. Exploiting domain structure for named entity recognition. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul Kripke</author>
</authors>
<title>Naming and Necessity.</title>
<date>1982</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="10008" citStr="Kripke, 1982" startWordPosition="1527" endWordPosition="1528">amount of manual effort and expertise required in rule-based NER may still be significant. In Sec. 5, we report on the lessons learned and outline several interesting research directions towards simplifying rule development and facilitating the adoption of the rule-based approach towards NER. 2 Domain Customization for NER We consider NER tasks following the broad definition put forth by (Nadeau and Sekine, 2007), formally defined as follows: Definition 1 Named entity recognition is the task of identifying and classifying mentions of entities with one or more rigid designators, as defined by (Kripke, 1982). For instance, the identification of proper nouns representing persons, organizations, locations, product names, proteins, drugs and chemicals are considered as NER tasks. Based on our experience of customizing NER annotators for multiple domains, we categorize the customizations involved into two main categories as listed below. This categorization motivates the design of NERL (Sec. 3). Data-driven (CDD): The most common NER customization is data-driven, where the customizations mostly involve the addition of new patterns and dictionary entries, driven by observations from the training data </context>
</contexts>
<marker>Kripke, 1982</marker>
<rawString>Saul Kripke. 1982. Naming and Necessity. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Krupka</author>
<author>K Hausman</author>
</authors>
<title>IsoQuest Inc.: Description of the NetOwlTM extractor system as used for MUC-7.</title>
<date>2001</date>
<booktitle>In MUC-7.</booktitle>
<contexts>
<context position="2164" citStr="Krupka and Hausman, 2001" startWordPosition="322" endWordPosition="325"> the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-based approach is still extremely appealing due to the associated transparency of the internal system state, which leads to better explainability of errors</context>
</contexts>
<marker>Krupka, Hausman, 2001</marker>
<rawString>G. R. Krupka and K. Hausman. 2001. IsoQuest Inc.: Description of the NetOwlTM extractor system as used for MUC-7. In MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>R Krishnamurthy</author>
<author>S Raghavan</author>
<author>S Vaithyanathan</author>
<author>H V Jagadish</author>
</authors>
<title>Regular expression learning for information extraction.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="41699" citStr="Li et al., 2008" startWordPosition="6454" endWordPosition="6457">e the filter rule CR4 (Fig. 2) should be applied based on the provenance of the false positives. Similarly, tools for explaining false positives in the spirit of (Huang et al., 2008), are also conceivable. Automatic Parameter Learning The most timeconsuming part in building a rule often is to decide the value of its parameters, especially for FD and CR rules. For instance, while defining a CR rule, one has to choose values for the Predicate parameter and the Context parameter (see Tab. 9). Some parameter values can be learned – for example, dictionaries (Riloff, 1993) and regular expressions (Li et al., 2008). Automatic Rule Refinement Tools automatically suggesting entire customization rules to a complex NERL program in the spirit of (Liu et al., 2010) can further reduce human effort in building NER annotators. With the help of such tools, one only needs to consider good candidate NERL rules suggested by the system without having to go through the conventional manual “trial and error” process. 6 Conclusion In this paper, we described NERL, a high-level rule language for building and customizing NER annotators. We demonstrated that a complex NER annotator built using NERL can be effectively custom</context>
</contexts>
<marker>Li, Krishnamurthy, Raghavan, Vaithyanathan, Jagadish, 2008</marker>
<rawString>Y. Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan, and H. V. Jagadish. 2008. Regular expression learning for information extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>L Chiticariu</author>
<author>V Chu</author>
<author>H V Jagadish</author>
<author>F Reiss</author>
</authors>
<title>Automatic Rule Refinement for Information Extraction.</title>
<date>2010</date>
<tech>PVLDB, 3.</tech>
<contexts>
<context position="41846" citStr="Liu et al., 2010" startWordPosition="6476" endWordPosition="6479">n the spirit of (Huang et al., 2008), are also conceivable. Automatic Parameter Learning The most timeconsuming part in building a rule often is to decide the value of its parameters, especially for FD and CR rules. For instance, while defining a CR rule, one has to choose values for the Predicate parameter and the Context parameter (see Tab. 9). Some parameter values can be learned – for example, dictionaries (Riloff, 1993) and regular expressions (Li et al., 2008). Automatic Rule Refinement Tools automatically suggesting entire customization rules to a complex NERL program in the spirit of (Liu et al., 2010) can further reduce human effort in building NER annotators. With the help of such tools, one only needs to consider good candidate NERL rules suggested by the system without having to go through the conventional manual “trial and error” process. 6 Conclusion In this paper, we described NERL, a high-level rule language for building and customizing NER annotators. We demonstrated that a complex NER annotator built using NERL can be effectively customized for different domains, achieving extraction quality superior to the state-of-the-art numbers. However, our experience also indicates that the </context>
</contexts>
<marker>Liu, Chiticariu, Chu, Jagadish, Reiss, 2010</marker>
<rawString>B. Liu, L. Chiticariu, V. Chu, H. V. Jagadish, and F. Reiss. 2010. Automatic Rule Refinement for Information Extraction. PVLDB, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>H Cunningham</author>
</authors>
<title>Towards a semantic extraction of named entities.</title>
<date>2003</date>
<booktitle>In RANLP.</booktitle>
<contexts>
<context position="6118" citStr="Maynard et al., 2003" startWordPosition="927" endWordPosition="930">., 2009). Implementing a similar approach for rule-based NER typically requires a significant amount of manual effort to (a) identify the explicit semantic changes required for the new domain (e.g., differences in entity type definition), (b) identify the portions of the (complex) core annotator that should be modified for each difference and (c) implement the required customization rules without compromising the extraction quality of the core annotator. Domain customization of rule-based NER has not received much attention in the recent literature with a few exceptions (Petasis et al., 2001; Maynard et al., 2003; Zhu et al., 2005). 1.2 Problem Statement In this paper, we explore the following natural question: Are rule-based systems still a viable approach to named-entity recognition? Specifically, (a) Is it possible to build, maintain and customize rule-based NER annotators that match the state-of-the-art results obtained using machine-learning techniques? and (b) Can this be achieved with a reasonable amount of manual effort? 1.3 Contributions In this paper, we address the challenges mentioned above by (i) defining a taxonomy of the different types of customizations that a rule developer may perfor</context>
<context position="31725" citStr="Maynard et al., 2003" startWordPosition="4896" endWordPosition="4899">2 CoNLL03dev CoreNERconll 96.49 93.76 95.11 Improvement 12.68 31.99 13.99 Table 4: Rules added during customization CoreNERorig 77.21 54.87 64.15 CoNLL03test CoreNERconll 93.89 89.75 91.77 Precision Recall Fβ=1 CoreNERconll 97.64 95.60 96.61 CoreNERenron 91.15 92.58 91.86 CoreNERace 92.32 91.22 91.77 Table 5: Quality of customization on train datasets (%) of the domain independent NER annotator, and during customizations for different domains. A total of 8 person weeks were spent on customizations, and we believe this effort is quite reasonable by rulebased extraction standards. For example, (Maynard et al., 2003) reports that customizing the ANNIE domain independent NER annotator developed using the JAPE grammar-based rule language for the ACE05 dataset required 6 weeks (and subsequent tuning over the next 6 months), resulting in improving the quality to 82% for this dataset. As we shall discuss shortly, with similar manual effort, we were able to achieve results outperforming state-ofart published results on three different datasets, including ACE05. However, one may rightfully argue that the process is still too lengthy impeding the widespread deployment of rule-based NER extraction. We elaborate on</context>
</contexts>
<marker>Maynard, Bontcheva, Cunningham, 2003</marker>
<rawString>D. Maynard, K. Bontcheva, and H. Cunningham. 2003. Towards a semantic extraction of named entities. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>W Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="1839" citStr="McCallum and Li, 2003" startWordPosition="266" endWordPosition="269"> reap the benefits of rule-based extractors’ explainability without sacrificing accuracy. We conclude by discussing lessons learned while building and customizing complex rule-based annotators and outlining several research directions towards facilitating rule development. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>A. McCallum and W. Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Minkov</author>
<author>R C Wang</author>
<author>W W Cohen</author>
</authors>
<title>Extracting personal names from emails: Applying named entity recognition to informal text.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="2034" citStr="Minkov et al., 2005" startWordPosition="303" endWordPosition="306">outlining several research directions towards facilitating rule development. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-based approach is still e</context>
<context position="8706" citStr="Minkov et al., 2005" startWordPosition="1329" endWordPosition="1332"> sports team based in that city). In the domain-independent annotator, city names were always identified as Location, as this subtle requirement was not considered during rule development. A customization to address this issue is shown in Fig. 1, which can be implemented in NERL with five rules (Fig. 2). This customization (explained in detail in Sec. 3) improved the Fp=1 score for Organization and Location by approximately 9% and 3%, respectively (Sec. 4). We used NERL to customize a domainindependent rule-based NER annotator for three different domains – CoNLL03 (Tjong et al., 2003), Enron (Minkov et al., 2005) and ACE05 (NIST, 2005). Our experimental results (Sec. 4.3) demonstrate that the customized annotators have extraction quality better than the best-known results for 1003 Affects Single Affects Multiple Entity Type Entity Types Identify New Instances CS, CDD, CDSD BR Modify Existing instances CEB, CDD CATA, CG Table 1: Categorizing NER Customizations individual domains, which were achieved with machine learning techniques. The fact that we are able to achieve such results across multiple domains answers our earlier question and confirms that we can reap the benefits of rule-based extractors’ </context>
<context position="27991" citStr="Minkov et al., 2005" startWordPosition="4292" endWordPosition="4295">zability, we require multiple datasets satisfying the following criteria: First, the datasets must cover diverse sources and styles of text. Second, the set of the most challenging NER tasks Person, Organization and Location (see Tab. 3) considered in CoreNER should be applicable to them. Finally, they should be publicly available and preferably have associated published results, against which we can compare our experimental results. Towards this end, we chose the following public datasets. • CoNLL03 (Tjong et al., 2003): a collection of Reuters news stories. Consists of formal text. • Enron (Minkov et al., 2005): a collection of emails with meeting information from the Enron dataset. Contains predominantly informal text. • ACE05 (NIST, 2005)1 a collection of broadcast news, broadcast conversations and newswire reports. Consists of both formal and informal text. Customization Process The goal of customization 1The evaluation test set is not publicly available. Thus, following the example of (Florian et al., 2006), the publicly available set is split into a 80%/20% data split, with the last 20% of the data in chronological order selected as test data. is to refine the original CoreNER (hence referred t</context>
<context position="34769" citStr="Minkov et al., 2005" startWordPosition="5346" endWordPosition="5349">of CoreNERorig, while in ACE05, ORG and LOC entity types were split into four entity types (Organization, Location, Geo-Political Entity and Facility). Customizations such as CS and CG address the above changes in named-entity type definition and substantially improve the extraction quality of CoreNERorig. Next, we compare the extraction quality of the 2CoNLL03dev and CoNLL03test correspond to the development and test sets for CoNLL03 respectively. CoNLL03dev CoNLL03test 1009 customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al., 2003) and (Minkov et al., 2005). Tab. 7 shows that our customized CoreNER outperforms the corresponding state-of-the-art numbers for all the NER tasks on both CoNLL03 and Enron. 4 These results demonstrate that high-quality annotators can be built by customizing CoreNERoTZ9 using NERL, with the final extraction quality matching that of state-of-theart machine learning-based extractors. It is worthwhile noting that the best published results for CoNLL03 (Florian et al., 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden Markov Model) a</context>
</contexts>
<marker>Minkov, Wang, Cohen, 2005</marker>
<rawString>E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Extracting personal names from emails: Applying named entity recognition to informal text. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
<author>S Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Linguisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1707" citStr="Nadeau and Sekine, 2007" startWordPosition="243" endWordPosition="246"> annotators match or outperform the best published results achieved with machine learning techniques. These results confirm that we can reap the benefits of rule-based extractors’ explainability without sacrificing accuracy. We conclude by discussing lessons learned while building and customizing complex rule-based annotators and outlining several research directions towards facilitating rule development. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 20</context>
<context position="9811" citStr="Nadeau and Sekine, 2007" startWordPosition="1494" endWordPosition="1497">oss multiple domains answers our earlier question and confirms that we can reap the benefits of rule-based extractors’ explainability without sacrificing accuracy. However, we found that even using NERL, the amount of manual effort and expertise required in rule-based NER may still be significant. In Sec. 5, we report on the lessons learned and outline several interesting research directions towards simplifying rule development and facilitating the adoption of the rule-based approach towards NER. 2 Domain Customization for NER We consider NER tasks following the broad definition put forth by (Nadeau and Sekine, 2007), formally defined as follows: Definition 1 Named entity recognition is the task of identifying and classifying mentions of entities with one or more rigid designators, as defined by (Kripke, 1982). For instance, the identification of proper nouns representing persons, organizations, locations, product names, proteins, drugs and chemicals are considered as NER tasks. Based on our experience of customizing NER annotators for multiple domains, we categorize the customizations involved into two main categories as listed below. This categorization motivates the design of NERL (Sec. 3). Data-driven</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>D. Nadeau and S. Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30(1):3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ace evaluation</title>
<date>2005</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="8729" citStr="NIST, 2005" startWordPosition="1335" endWordPosition="1336">. In the domain-independent annotator, city names were always identified as Location, as this subtle requirement was not considered during rule development. A customization to address this issue is shown in Fig. 1, which can be implemented in NERL with five rules (Fig. 2). This customization (explained in detail in Sec. 3) improved the Fp=1 score for Organization and Location by approximately 9% and 3%, respectively (Sec. 4). We used NERL to customize a domainindependent rule-based NER annotator for three different domains – CoNLL03 (Tjong et al., 2003), Enron (Minkov et al., 2005) and ACE05 (NIST, 2005). Our experimental results (Sec. 4.3) demonstrate that the customized annotators have extraction quality better than the best-known results for 1003 Affects Single Affects Multiple Entity Type Entity Types Identify New Instances CS, CDD, CDSD BR Modify Existing instances CEB, CDD CATA, CG Table 1: Categorizing NER Customizations individual domains, which were achieved with machine learning techniques. The fact that we are able to achieve such results across multiple domains answers our earlier question and confirms that we can reap the benefits of rule-based extractors’ explainability without </context>
<context position="28123" citStr="NIST, 2005" startWordPosition="4313" endWordPosition="4314">Second, the set of the most challenging NER tasks Person, Organization and Location (see Tab. 3) considered in CoreNER should be applicable to them. Finally, they should be publicly available and preferably have associated published results, against which we can compare our experimental results. Towards this end, we chose the following public datasets. • CoNLL03 (Tjong et al., 2003): a collection of Reuters news stories. Consists of formal text. • Enron (Minkov et al., 2005): a collection of emails with meeting information from the Enron dataset. Contains predominantly informal text. • ACE05 (NIST, 2005)1 a collection of broadcast news, broadcast conversations and newswire reports. Consists of both formal and informal text. Customization Process The goal of customization 1The evaluation test set is not publicly available. Thus, following the example of (Florian et al., 2006), the publicly available set is split into a 80%/20% data split, with the last 20% of the data in chronological order selected as test data. is to refine the original CoreNER (hence referred to as CoreNERoTig) in order to improve its extraction quality on the training set (in terms of F0=1) for each dataset individually. I</context>
</contexts>
<marker>NIST, 2005</marker>
<rawString>NIST. 2005. The ace evaluation plan. F. J. Och O. Bender and H. Ney. 2003. Maximum entropy models for named entity recognition. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Petasis</author>
<author>F Vichot</author>
<author>F Wolinski</author>
<author>G Paliouras</author>
<author>V Karkaletsis</author>
<author>C Spyropoulos</author>
</authors>
<title>Using machine learning to maintain rule-based named-entity recognition and classification systems.</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6096" citStr="Petasis et al., 2001" startWordPosition="923" endWordPosition="926">et al., 2008; Wu et al., 2009). Implementing a similar approach for rule-based NER typically requires a significant amount of manual effort to (a) identify the explicit semantic changes required for the new domain (e.g., differences in entity type definition), (b) identify the portions of the (complex) core annotator that should be modified for each difference and (c) implement the required customization rules without compromising the extraction quality of the core annotator. Domain customization of rule-based NER has not received much attention in the recent literature with a few exceptions (Petasis et al., 2001; Maynard et al., 2003; Zhu et al., 2005). 1.2 Problem Statement In this paper, we explore the following natural question: Are rule-based systems still a viable approach to named-entity recognition? Specifically, (a) Is it possible to build, maintain and customize rule-based NER annotators that match the state-of-the-art results obtained using machine-learning techniques? and (b) Can this be achieved with a reasonable amount of manual effort? 1.3 Contributions In this paper, we address the challenges mentioned above by (i) defining a taxonomy of the different types of customizations that a rul</context>
</contexts>
<marker>Petasis, Vichot, Wolinski, Paliouras, Karkaletsis, Spyropoulos, 2001</marker>
<rawString>G. Petasis, F. Vichot, F. Wolinski, G. Paliouras, V. Karkaletsis, and C. Spyropoulos. 2001. Using machine learning to maintain rule-based named-entity recognition and classification systems. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Poibeau</author>
<author>L Kosseim</author>
</authors>
<title>Proper name extraction from non-journalistic texts.</title>
<date>2001</date>
<booktitle>In Computational Linguistics in the Netherlands,</booktitle>
<pages>144--157</pages>
<contexts>
<context position="1988" citStr="Poibeau and Kosseim, 2001" startWordPosition="294" endWordPosition="297">g and customizing complex rule-based annotators and outlining several research directions towards facilitating rule development. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning technique</context>
</contexts>
<marker>Poibeau, Kosseim, 2001</marker>
<rawString>T. Poibeau and L. Kosseim. 2001. Proper name extraction from non-journalistic texts. In Computational Linguistics in the Netherlands, pages 144–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="41657" citStr="Riloff, 1993" startWordPosition="6449" endWordPosition="6450"> down the choices for the location where the filter rule CR4 (Fig. 2) should be applied based on the provenance of the false positives. Similarly, tools for explaining false positives in the spirit of (Huang et al., 2008), are also conceivable. Automatic Parameter Learning The most timeconsuming part in building a rule often is to decide the value of its parameters, especially for FD and CR rules. For instance, while defining a CR rule, one has to choose values for the Predicate parameter and the Context parameter (see Tab. 9). Some parameter values can be learned – for example, dictionaries (Riloff, 1993) and regular expressions (Li et al., 2008). Automatic Rule Refinement Tools automatically suggesting entire customization rules to a complex NERL program in the spirit of (Liu et al., 2010) can further reduce human effort in building NER annotators. With the help of such tools, one only needs to consider good candidate NERL rules suggested by the system without having to go through the conventional manual “trial and error” process. 6 Conclusion In this paper, we described NERL, a high-level rule language for building and customizing NER annotators. We demonstrated that a complex NER annotator </context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>E. Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>C Nobata</author>
</authors>
<title>Definition, dictionaries and tagger for extended named entity hierarchy.</title>
<date>2004</date>
<booktitle>In Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="2190" citStr="Sekine and Nobata, 2004" startWordPosition="326" endWordPosition="329">entions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-based approach is still extremely appealing due to the associated transparency of the internal system state, which leads to better explainability of errors (Siniakov, 2010). Ideally</context>
</contexts>
<marker>Sekine, Nobata, 2004</marker>
<rawString>S. Sekine and C. Nobata. 2004. Definition, dictionaries and tagger for extended named entity hierarchy. In Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Shen</author>
<author>A Doan</author>
<author>J F Naughton</author>
<author>R Ramakrishnan</author>
</authors>
<title>Declarative information extraction using datalog with embedded extraction predicates.</title>
<date>2007</date>
<booktitle>In VLDB.</booktitle>
<contexts>
<context position="14753" citStr="Shen et al., 2007" startWordPosition="2271" endWordPosition="2274">action. These limitations have been recognized in the literature, and several extensions have been proposed to allow more flexible matching semantics, and to allow overlapping annotations (Cunningham et al., 2000; Boguraev, 2003; Drozdzynski et al., 2004). However, even with these extensions, common operations such as filtering annotations (e.g. CR4 in Fig. 2), are difficult to express in grammars and often require an escape to custom procedural code. Recently, several declarative algebraic languages have been proposed for rule-based IE systems, notably AQL (Chiticariu et al., 2010) and Xlog (Shen et al., 2007). These languages are not constrained by the requirement that all rules map onto finite state transducers, and therefore can express a significantly richer semantics than grammar-based languages. In particular, the AQL rule language as implemented in SystemT (Chiticariu et al., 2010) can express many common operations used in rule-based information extraction without requiring custom code. In addition, the separation of extraction semantics from execution enables SystemT’s rule optimizer and efficient runtime engine. Indeed, as shown in (Chiticariu et al., 2010), SystemT can deliver an order o</context>
</contexts>
<marker>Shen, Doan, Naughton, Ramakrishnan, 2007</marker>
<rawString>W. Shen, A. Doan, J. F. Naughton, and R. Ramakrishnan. 2007. Declarative information extraction using datalog with embedded extraction predicates. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>D Hillard</author>
<author>C Leggeteer</author>
</authors>
<title>Minimallysupervised extraction of entities from text advertisements.</title>
<date>2010</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="2330" citStr="Singh et al., 2010" startWordPosition="350" endWordPosition="353">ile NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-based approach is still extremely appealing due to the associated transparency of the internal system state, which leads to better explainability of errors (Siniakov, 2010). Ideally, one would like to benefit from the transparency and explainability of rule-based techniques, while achieving state-of-the-art accuracy. A </context>
</contexts>
<marker>Singh, Hillard, Leggeteer, 2010</marker>
<rawString>S. Singh, D. Hillard, and C. Leggeteer. 2010. Minimallysupervised extraction of entities from text advertisements. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Siniakov</author>
</authors>
<title>GROPUS - an adaptive rule-based algorithm for information extraction.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Freie Universitat Berlin.</institution>
<contexts>
<context position="2781" citStr="Siniakov, 2010" startWordPosition="422" endWordPosition="423">Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-based approach is still extremely appealing due to the associated transparency of the internal system state, which leads to better explainability of errors (Siniakov, 2010). Ideally, one would like to benefit from the transparency and explainability of rule-based techniques, while achieving state-of-the-art accuracy. A particularly challenging aspect of rule-based NER in practice is domain customization — customizing existing annotators to produce accurate results in new domains. In machine learning-based systems, adapting to a new domain has traditionally involved acquiring additional labeled data and learning a new model from scratch. However, recent work has proposed more sophisticated approaches that learn a domain-independent base model, which can later be </context>
</contexts>
<marker>Siniakov, 2010</marker>
<rawString>P. Siniakov. 2010. GROPUS - an adaptive rule-based algorithm for information extraction. Ph.D. thesis, Freie Universitat Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
<author>C Niu</author>
<author>W Li</author>
</authors>
<title>A hybrid approach for named entity and sub-type tagging.</title>
<date>2001</date>
<booktitle>In ANLP.</booktitle>
<contexts>
<context position="2373" citStr="Srihari et al., 2001" startWordPosition="357" endWordPosition="361">cles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). 1.1 Motivation Although there are well-established rule-based systems to perform NER tasks, most, if not all, state-ofthe-art results for NER tasks are based on machine learning techniques. However, the rule-based approach is still extremely appealing due to the associated transparency of the internal system state, which leads to better explainability of errors (Siniakov, 2010). Ideally, one would like to benefit from the transparency and explainability of rule-based techniques, while achieving state-of-the-art accuracy. A particularly challenging aspect of rule-bas</context>
</contexts>
<marker>Srihari, Niu, Li, 2001</marker>
<rawString>R. Srihari, C. Niu, and W. Li. 2001. A hybrid approach for named entity and sub-type tagging. In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong</author>
<author>K Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<marker>Tjong, Sang, De Meulder, 2003</marker>
<rawString>E. F. Tjong, K. Sang, and F. De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>W S Lee</author>
<author>N Ye</author>
<author>H L Chieu</author>
</authors>
<title>Domain adaptive bootstrapping for named entity recognition.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5506" citStr="Wu et al., 2009" startWordPosition="831" endWordPosition="834"> These references should not be reclassified as Organization CR4 Discard &lt;LocationMaybeOrg&gt; If Matches Regular Expression &lt;R2&gt; on Left Context 2 Tokens Customization Requirement: City, County or State names within sports articles may refer to a sports team or to the location itself. Customization Solution (CS) : Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location Figure 1: Example Customization Requirement al., 2004; Blitzer et al., 2006; Jiang and Zhai, 2006; Arnold et al., 2008; Wu et al., 2009). Implementing a similar approach for rule-based NER typically requires a significant amount of manual effort to (a) identify the explicit semantic changes required for the new domain (e.g., differences in entity type definition), (b) identify the portions of the (complex) core annotator that should be modified for each difference and (c) implement the required customization rules without compromising the extraction quality of the core annotator. Domain customization of rule-based NER has not received much attention in the recent literature with a few exceptions (Petasis et al., 2001; Maynard </context>
</contexts>
<marker>Wu, Lee, Ye, Chieu, 2009</marker>
<rawString>D. Wu, W. S. Lee, N. Ye, and H. L. Chieu. 2009. Domain adaptive bootstrapping for named entity recognition. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>V Uren</author>
<author>E Motta</author>
</authors>
<title>Espotter: Adaptive named entity recognition for web browsing.</title>
<date>2005</date>
<booktitle>In WM.</booktitle>
<contexts>
<context position="6137" citStr="Zhu et al., 2005" startWordPosition="931" endWordPosition="934"> a similar approach for rule-based NER typically requires a significant amount of manual effort to (a) identify the explicit semantic changes required for the new domain (e.g., differences in entity type definition), (b) identify the portions of the (complex) core annotator that should be modified for each difference and (c) implement the required customization rules without compromising the extraction quality of the core annotator. Domain customization of rule-based NER has not received much attention in the recent literature with a few exceptions (Petasis et al., 2001; Maynard et al., 2003; Zhu et al., 2005). 1.2 Problem Statement In this paper, we explore the following natural question: Are rule-based systems still a viable approach to named-entity recognition? Specifically, (a) Is it possible to build, maintain and customize rule-based NER annotators that match the state-of-the-art results obtained using machine-learning techniques? and (b) Can this be achieved with a reasonable amount of manual effort? 1.3 Contributions In this paper, we address the challenges mentioned above by (i) defining a taxonomy of the different types of customizations that a rule developer may perform when adapting to </context>
</contexts>
<marker>Zhu, Uren, Motta, 2005</marker>
<rawString>J. Zhu, V. Uren, and E. Motta. 2005. Espotter: Adaptive named entity recognition for web browsing. In WM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>