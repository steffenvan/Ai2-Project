<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.966423">
Lexcalised Parsing of German V2
</title>
<author confidence="0.997907">
Yo Sato
</author>
<affiliation confidence="0.983571">
Department of Computer Science
Queen Mary, University of London
</affiliation>
<address confidence="0.499024">
Mile End Road, London E1 4NS, U.K.
</address>
<sectionHeader confidence="0.97701" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995669375">
This paper presents a method and implemen-
tation of parsing German V2 word order by
means of constraints that reside in lexical
heads. It first describes the design of the
underlying parsing engine: the head-corner
chart parsing that incorporates a procedure
that dynamically enforces word order con-
straints. While the parser could potentially
generate all the permutations of terminal sym-
bols, constraint checking is conducted locally
in an efficient manner. The paper then shows
how this parser can adequately cover a variety
of V2 word order patterns with sets of lexi-
cally encoded constraints, including non-local
preposing of an embedded argument or an ad-
verbial.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999900615384616">
This paper presents a method of parsing V2 word
order manifested in a variety of German matrix sen-
tences in a lexicalised and locality-respecting man-
ner: lexicalised, as the V2 pattern is licensed ulti-
mately encoded in verbs, in the form of constraints
that hold amongst its arguments and itself; locality-
respecting, because (a) no constraint that operates on
constituents from different subcategorisation frames
is invoked and (b) the matrix verb and the prever-
bal constituent, however ‘distant’ its origin is, are
ordered in the same projection via the slash-based
mechanism.
The underlying grammar is loosely linearisation-
based, in the sense that word order is dissoci-
ated from the syntactic structure in a discontinuity-
allowing manner, as presented in Sato (2008). The
main benefit of a linearisation approach is that syn-
tactic constituency becomes independent (to a de-
gree) of its surface realisation and hence discour-
ages constituency manipulation for the sake of word
order. In line of this spirit I will largely adopt the
simple constituency construal that faithfully corre-
spond to its semantics. However, I distance myself
from the more or less standard version of linearisa-
tion grammar where potentially non-local LP con-
ditions are permitted (Reape, 1993) or word order
patterns are imposed at the clause level (as in ‘topo-
logical field’ model of Kathol (2000)).
The crux of the proposal consists in employing
a head-corner parsing in which the set of word or-
der constraints are incorporated into a VP’s lexical
head (i.e. common or auxiliary verb). For a V2 pro-
jection, its head verb contains the constraints to the
effect that only one of its arguments can be fronted
immediately before the verb itself. To enable this,
potential discontinuity and obligatory adjacency in
part of a phrase is included in the repertoire of word
order constraints in addition to the standard LP (lin-
ear precedence) constraints.
</bodyText>
<sectionHeader confidence="0.977861" genericHeader="method">
2 The data
</sectionHeader>
<bodyText confidence="0.800930142857143">
The V2 constructions to be dealt with in this paper
are as follows (I will use as an example the tertiary
verb gebengive or its past participle gegebengiven
throughout):
1. The ‘basic’ case where dependency between
the preverbal constituent and the matrix verb is
strictly local, e.g:
</bodyText>
<page confidence="0.674987">
1
</page>
<note confidence="0.7801795">
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 1–8,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.948686">
Ein Buch geben die Eltern dem Sohn.
a book give the parents the son
‘A book the parents give the son’
</bodyText>
<equation confidence="0.84807075">
(1) Clause(=VP)
�� � � � � � � ���
� �
V
</equation>
<figure confidence="0.7693596">
geben
NP NP NP
dem Sohn
die Eltern ein Buch
2. The case where an argument of the lower verb (2)&amp;(5) �� � � PPP
is fronted across the higher auxiliary verb: Aux Clause
Ein Buch haben die Eltern dem Sohn gegeben.
a book have the parents the son given
‘A book the parents have given the son’
3. The long-distance dependency case:
Ein Buch, sagt ein Freund, dass er glaubt, dass die
Eltern dem Sohn geben.
‘A book, a friend says that he thinks that the parents
give the son’
4. Adjunct fronting
</figure>
<equation confidence="0.81779584">
haben �� � � ����
-e E. -em S. ein Buch gegeben
(3)
�� � � � �
�� � ��
CP
�� ��
C Clause
dass ��� � ��
(4)
Aux
� � � PPP
Clause
NP
ein Freund
V
sagt
NP V
er glaubt
CP
�� �
C Clause
dass
�� � � ����
-e E. -em S. ein Buch geben
</equation>
<bodyText confidence="0.9990514">
Heimlich haben die Eltern dem Sohn ein Buch gegeben. haben Adv � � � � ����
secretly have the parents the son a book given Clause
‘Secretly the parents have given the son a book.’
heimlich
-e E. -em S. ein Buch gegeben
</bodyText>
<sectionHeader confidence="0.589293" genericHeader="method">
5. Partial VP fronting
</sectionHeader>
<subsectionHeader confidence="0.6398375">
Ein Buch dem Sohn gegeben haben die Eltern.
Ein Buch gegeben haben die Eltern dem Sohn.
</subsectionHeader>
<bodyText confidence="0.999534863636364">
As stated, our approach adopts a linearisation ap-
proach in which constituency does not determine the
surface realisation, which is handled instead by word
order conditions encoded in lexical heads. My con-
tention here is not so much plausibility as a grammar
as neutrality to particular phrase structures, which
linearisation promotes. Therefore I take a rather
simplified position to use an entirely uniform phrase
structure for the verb-argument structure for com-
mon verbs, namely the flat construal where all the
arguments as well as the head project onto a clause
(‘VP’) as mutual sisters, although I hasten to add
our constraint enforcement could equally apply to
configurational analyses. In fact we take an auxil-
iary verb to subcategorise for a clause rather than
the complex verb analysis, and adopt the traditional
binary iteration analysis for adjunct-head phrases, to
see how our parser fares with configurational analy-
ses.
I sum up the assumed constituency of the above
examples graphically as trees (though this has little
impact on word order):
</bodyText>
<sectionHeader confidence="0.882978" genericHeader="method">
3 The parser
</sectionHeader>
<subsectionHeader confidence="0.999311">
3.1 Core design
</subsectionHeader>
<bodyText confidence="0.999679210526316">
The design of the parser employed here can be
called constrained free word order parsing. First,
it allows for completely free word order at default.
The core algorithm for the parse engine is what
Reape (1991) presents as a generalised permutation-
complete parser, which in turn is based on the pre-
ceding proposal of Johnson (1985). Details apart,
while using context-free production rules (no multi-
ple left-hand side non-terminal symbols), this algo-
rithm only checks for the presence of all the right-
hand side constituents, wherever in the string they
occur, potentially discontinuously,1 effectively li-
censing all the permutations of the given terminal
symbols (e.g. 3! = 6 permutations for the string
consisting of ring, up and John including up John
ring etc.). This ‘directionless’ parsing is rendered
possible by Johnson’s ‘bitvector’ representation of
partial string coverage. In the above up John ring
string, the coverage of the ring and up combina-
</bodyText>
<footnote confidence="0.996562">
1More precisely, it searches for non-overlapping combina-
tions, excluding the same word being counted more than once
or more than one word counting towards the same rule in the
same search path.
</footnote>
<page confidence="0.995962">
2
</page>
<bodyText confidence="0.999948588235294">
tion, which materially constitutes a complex verb,
is represented as [1,0,1]. This is then then merged
with the bitvector of John, [0,1,0] into [1,1,1]. Sec-
ond, however, this rather promiscuous (and expen-
sive) parsing is dynamically restricted by word or-
der constraints that obtain in individual languages.
With sufficient constraints applied during the parse,
the above combinations with ring, up and John are
restricted to ring up John and ring John up.
I do not claim for originality in this basic design.
Daniels (2005) for example describes an implemen-
tation of an algorithm that falls precisely in such
style of parsing.2 The main points of the proposal
lie in lexicalisation and localisation, which contrast
with the general trend to introduce phrasal and non-
local constraint processing for German processing,
of which Daniels’ work is an example. All the word
order constraints are stored in lexicon, more specifi-
cally in lexical heads.
To adapt this design to a practical lexically driven
parsing, the author implemented a rendering of
head-corner chart parsing. It is head-corner in the
sense described e.g. in van Noord (1991), where
the parsing of a production rule always starts from
its head. This is necessary for our design because
the parser first retrieves the word order information
from the head. Furthermore, it requires the words
to be processed first by preterminal rules since with-
out processing lexical heads the whole recognition
process does not come off the ground. Therefore, a
chart parsing algorithm that invokes lexical initiali-
sation is utilised (as described in Gazdar &amp; Mellish
(1989) rather than the classical top-down parsing of
Earley (1970)).
</bodyText>
<subsectionHeader confidence="0.999979">
3.2 Constraint checking and propagation
</subsectionHeader>
<bodyText confidence="0.999936375">
Since no non-local word order constraints are intro-
duced in our parsing, they can be fully enforced at
each application of a production rule. More specif-
ically, the checking of constraint compliance is car-
ried out at the completer operation of chart pars-
ing.3 The data structure of an edge is suitably mod-
ified. In addition to the dotted production rule, it
needs to carry the constraint set relevant to the corre-
</bodyText>
<footnote confidence="0.867059">
2A foregoing implementation by M¨uller (2004) also em-
ploys bitvector-based linearisation approach.
3The equivalent operation is called the ‘fundamental rule’ in
Gazdar &amp; Mellish (1989).
</footnote>
<bodyText confidence="0.999805714285714">
sponding production rule, retrievable from the head,
which is always processed first in our head-corner
algorithm.4 Also, as we are adopting the bitvector
representation of coverage, an edge contains its cor-
responding bitvector. The completer operation in-
volves merger of two bitvectors, so the check can be
conducted at this stage:
</bodyText>
<subsectionHeader confidence="0.550749">
Completer in constrained parsing
</subsectionHeader>
<bodyText confidence="0.98323025">
Let A and B be symbols, α, 0 and -y be arbi-
trary strings, V1 and V2 be bitvectors and VI
be their merge, then:
If the chart contains an active edge (V1, A--� α
</bodyText>
<listItem confidence="0.97732525">
• B 0) and a passive edge (V2, B--� -y • ), run
the CHECK-ORDER procedure. If it succeeds,
add edge (V &apos;, A--� αB • 0) to the chart if V1
and V2 are mergeable. If it fails, do nothing.
</listItem>
<bodyText confidence="0.999871111111111">
The CHECK-ORDER procedure consists in a bit-
wise comparison of bitvectors. It picks out the
bitvectors of the categories in question and checks
the compliance of the newly found category with re-
spect to the relevant constraints. If for example A, B
and C had been found at [0,1,0,0,0], [0,0,1,0,1] and
[1,0,0,1,0] respectively, this would validate A � B
but not A � C. Thus the edges for string combina-
tions that violate the word order constraints would
not be created, eliminating wasteful search paths.
As we will shortly see, the constraint type that
checks continuity of a phrase is also introduced.
Therefore the phrase (dis)continuity can also be as-
certained locally, which is a major advantage over a
parsing that relies largely on concatenation. Thus,
the cost of constraint checking remains very small
despite the capability of processing discontinuity.5
Note however that by locality is meant subcat-
egorisation locality (or ‘selection’ locality as de-
scribed in Sag (2007)): whatever is in the same
subcategorisation frame of a lexical head is consid-
ered local. Depending on the adopted analysis, con-
stituents ‘local’ in this sense may of course occur
in different trees. Constraints on such ‘non-local’
—in the tree sense but not in the subcategorisation
sense— constituents are still enforceable in the im-
plemented parser. The unused constraints at a node,
</bodyText>
<footnote confidence="0.997833">
4This retrieval of word order information is carried out at the
predictor stage of chart parsing.
5It is worth mentioning that the bitvector checking is con-
ducted over the whole string, the effect of applied constraints
will be never lost.
</footnote>
<page confidence="0.996692">
3
</page>
<bodyText confidence="0.999711">
for example some constraint applicable to the verb
and its subject at the VP node in the configurational
(subjectless-VP) analysis, is made to propagate up
to the upper node. Thus it is no problem to enforce
a constraint over ‘different trees’, as long as it is ap-
plied to ‘local’ constituents in our sense.6
</bodyText>
<sectionHeader confidence="0.88009" genericHeader="method">
4 Possible constraints and subtyping
</sectionHeader>
<bodyText confidence="0.980585705882353">
It is crucial, if the computational properties of the
parser is to be transparent in constrained free word
order parsing, to identify the kind of word order con-
straints admitted into lexical heads. We will remain
relatively conservative, in introducing only two op-
erators for constraint encoding. We first invoke the
binary LP operator (�) in a conventional sense: the
whole (or, equivalently, right-periphery) of a string
for category A needs to precede the whole (or left-
periphery) of a string for category B to satisfy A �
B (I will use the shorthand A � (B, C) to express
(A � B) n (A � C). Crucially, the contiguity op-
erator () is added. It takes a set of constituents as its
operand and requires the constituents in it to be con-
tiguous, regardless of their order. Thus, {A, B, C}
encodes the requirement for A, B and C as a whole
forming a contiguous string. For example, the string
I ring John up does not satisfy {ring, up} but does
satisfy {ring, John, up}.
Also important is how to succinctly generalise
on the word order patterns now encoded in lexical
items, as one would certainly want to avoid a te-
dious task of writing them all individually, if they
allow for broader classification. For example the En-
glish transitive verb generally follows its subject ar-
gument and precedes its object argument, and one
would naturally want to lump these verbs under one
umbrella. For such a cluster of lexical heads, we will
introduce a word order (sub)type. More pertinently,
the German verbs may be classified into v1-verb, v2-
verb and vf-verb according to the positions of their
arguments in their projection. We will also allow
multiple inheritance that becomes standard in the
typed feature system (cf. Pollard and Sag (1987)).
</bodyText>
<footnote confidence="0.860036">
6See Sato (2006) for details.
</footnote>
<sectionHeader confidence="0.956211" genericHeader="method">
5 Constraints for V2
</sectionHeader>
<subsectionHeader confidence="0.938544">
5.1 General setup
</subsectionHeader>
<bodyText confidence="0.997653625">
To enforce the V2 word order pattern lexically, I pro-
pose to use a combination of two word order sub-
types: dislocating-verb (disl-v) and matrix-v2-verb
(mtrx-v2-v). The former type represents a verb one
of whose arguments is to be ‘dislocated’. A verb of
this type can thus be characterised as ‘contributing’
the dislocated (preverbal) element. The latter, on the
other hand, is the type that is projected onto a ma-
trix sentence. This type should be constrained such
that one dislocated constituent must —and only one
may— precede and be adjacent to the verb itself. It
may be characterised as a verb that provides a locus
—immediately before itself— of, or ‘receives’ the
dislocated element.
Dislocation is handled by a constraint percola-
tion mechanism. I assume the dislocated constituent
is pushed into a storage that then participates in a
slash style percolation, although the storage content
would still need to be ordered by lexicalised con-
straints rather than by the percolation mechanism it-
self, as they are the sole resource for word order.7
Thus the checking as regards the dislocated con-
stituent is conducted at each projection in the per-
colation path, hence locally, while the percolation
mechanism gives some ‘global’ control over disloca-
tion. Not just the positioning of the dislocated con-
stituent at the left-periphery of the whole sentence,
but the assurance of a global singularity restriction
of dislocation —not just one constituent per clause
in multiple embeddings— becomes thus possible.
Let args be the set of the arguments of a disl-v,
disl be that of the dislocated one and situ be that of
the remaining arguments, i.e. disl C args where
|disl |= 1 and situ = {x|x C args n x V disl}.
Then the type disl-v can be characterised as having
the following constraint:
disl-v: disl --&lt; situ (disl --+ dislst)
Simply put, this says that the arguments are divided
into two parts, the dislocated and in-situ parts, the
former of which precedes the latter. We assume, as
</bodyText>
<footnote confidence="0.916523333333333">
7The adopted mechanism is close to Penn (1999), though
he invokes potentially non-local topology-based constraints and
removes the filler and gapped head entirely.
</footnote>
<page confidence="0.997362">
4
</page>
<bodyText confidence="0.988672636363636">
in the standard treatment, there is only one dislo-
cated constituent, until we consider the VP fronting.
The notation with an arrow on the right indicates this
singleton set is pushed into the storage that is prop-
agated upwards.
The mtrx-v2-v type is then characterised as fol-
lows:
mtrx-v2-v: dislst ≺ verb, {dislst, verb}
This simply says the dislocated constituent (stored
in a lower node and percolated) immediately pre-
cedes the matrix verb. (For the following presen-
tation, the storage-related notations will be omitted
and implicitly assumed unless necessary. Also, the
set variables disl and args will be used with the same
meaning.)
Thus the combination of the two types gives, for
example where args = {A, B, C}, disl = {A} and
the matrix verb is V , the following constraint set:
{A ≺ (B, C), A ≺ V, {A, V }}
which essentially says that the dislocated A immedi-
ately precedes the matrix verb V and precedes (not
necessarily immediately) the in-situ B and C.
</bodyText>
<subsectionHeader confidence="0.997692">
5.2 Local case
</subsectionHeader>
<bodyText confidence="0.9999675">
To begin with, let us see a case where dependency
between the preverbal constituent and the matrix
verb is strictly local, taking (1) as an example. Note
first that there are six possible variants:
</bodyText>
<listItem confidence="0.933696142857143">
(1)
a. Die Eltern geben dem Sohn ein Buch.
b. Die Eltern geben ein Buch dem Sohn.
c. Dem Sohn geben die Eltern ein Buch.
d. Dem Sohn geben ein Buch die Eltern.
e. Ein Buch geben die Eltern dem Sohn.
f. Ein Buch geben dem Sohn die Eltern.
</listItem>
<bodyText confidence="0.969818166666667">
In this case, geben is both a matrix (argument-
receiving) and dislocating (argument-contributing)
verb. This means that the two subtypes should be
overloaded. Let us call this overloaded sub-species
disl-mtrx-v2-v: which is given the following specifi-
cation:
disl-mtrx-v2-v:
disl ≺ situ, disl ≺ verb, {disl, verb}
To adapt this type to our verb, geben, where we rep-
resent its arguments as sNP (subject NP), ioNP (in-
direct object NP) and doNP (direct object NP), we
obtain, for the case where sNP is preposed:
</bodyText>
<equation confidence="0.994762">
{sNP ≺ (ioNP,doNP),
sNP ≺ geben, (sNP,geben)}
</equation>
<bodyText confidence="0.95720215">
where the constraints on the first line is inher-
ited from disloc-v while those on the second from
matrix-v2-v. This corresponds to the sentences (a)
and (b) above. The followings are the cases where
ioNP and doNP are preposed, corresponding to (c,d)
and (e,f), respectively.
{ioNP (sNP, doNP), ioNP geben, (ioNP, geben)}
{doNP (sNP, ioNP), doNP geben, (doNP, geben)}
These possible sets are enforced in the manner of
exclusive disjunction, that is, only one of the above
three sets actually obtains. This does not mean, how-
ever, each set must be explicitly stated in the verb
and processed blindly. Only the abstract form of
the constraint, as described under the type specifi-
cation above, is written in the lexicon. During pars-
ing, then, one of the sets, as dynamically found to
match the input string, is computed and applied. In
the subsequent discussion, therefore, only the direct-
object fronting case is considered as a representative
example for each construction.
</bodyText>
<subsectionHeader confidence="0.99858">
5.3 Argument fronting across auxiliary
</subsectionHeader>
<bodyText confidence="0.978410666666667">
We now consider the cases where the dependency is
not local, starting with an auxiliary-involving case.
The dependency between an auxiliary and an ar-
gument of its lower verb is, according to the Aux-
Clause construal adopted here, is not local. We can
however succinctly specify such non-local V2 ren-
derings as a case where the above two types are in-
stantiated separately in two verbs. The example is
reproduced below:
(2) Ein Buch haben die Eltern dem Sohn gegeben.
The argument-contributing gegebengiven is, as
before, assigned the disl-v type, but is further sub-
typed and inherits the constraints also from vf-v (v-
final verb), reflecting the fact that it occurs head-
finally.
</bodyText>
<footnote confidence="0.39907">
gegeben (type disl-vf-v):
{doNP ≺ (sNP,ioNP),
</footnote>
<page confidence="0.902192">
5
</page>
<bodyText confidence="0.890812818181818">
(sNP, doNP, ioNP) ≺ gegeben}
The dislocated doNP climbs up the tree ((2) in
Section 2) in the storage, which is then subject to
the constraints of matrix haben at the top node. This
argument-receiving auxiliary haben is, as before,
given the mtrx-v2-v status.8.
haben (type mtrx-v2-v):
{doNPst ≺ haben, (doNPst, haben)}
Thus the dislocated ein Buch is duly placed at the
left-periphery in a manner that forbids intervention
between itself and the matrix verb.
</bodyText>
<subsectionHeader confidence="0.990877">
5.4 Long-Distance Dependency
</subsectionHeader>
<bodyText confidence="0.99977225">
Having dealt with an argument fronting of the auxil-
iary construction as a non-local case, we could now
extend the same treatment to long-distance depen-
dency. Our example is:
</bodyText>
<listItem confidence="0.671066">
(3) Ein Buch, sagt ein Freund, dass er glaubt, dass
</listItem>
<bodyText confidence="0.943821722222222">
die Eltern dem Sohn geben.
(‘A book, a friend says that he thinks that the
parents give the son’)
In fact, it suffices to endow exactly the same type
as gegeben, i.e. disl-vf-v, to the occurrence of geben
in a subordinate clause.9
geben (in subord. clause, type disl-vf-v):
{doNP ≺ (sNP,ioNP),
(sNP, doNP, ioNP) ≺ geben}
This ensures that the dislocated argument goes
progressively up towards the top node. To prevent
this argument from being ‘dropped’ the half way
through, however, the non-matrix CP-taking verbs
‘in the middle’ that should be bypassed, in our case
glaubt, needs to possess the constraint that pushes
the dislocated element to the left of itself:
glaubt (in subord. clause, type ‘middle-v’):10
{doNPst ≺ glaubt}
</bodyText>
<footnote confidence="0.990415125">
8More precisely this also involves haben≺ VP(gapped)
9This means that, given the identical morphological form,
gegeben is type-ambiguous between the matrix and subordinate
occurrences. This does not add too much to parsing complexity,
however, as this ‘ambiguity’ is quickly resolved when one of its
argument is encountered.
10The constraints applicable to the usual finite verb is omit-
ted, i.e. s�P ≺ glaubt and glaubt ≺ CP(gapped).
</footnote>
<bodyText confidence="0.9040684">
Finally, a mtrx-v2-v, in our case sagt, takes care of
placing the dislocated constituent immediately be-
fore itself.
sagt (type mtrx-v2-v):11
{doNPst ≺ sagt, (doNPst, sagt)}
</bodyText>
<subsectionHeader confidence="0.994629">
5.5 Adjunct fronting
</subsectionHeader>
<bodyText confidence="0.999873151515151">
I declared at the beginning to use the traditional bi-
nary adjunction analysis for adjunct-head phrases.12
In order to achieve this, I first propose a fundamental
conceptual shift, given the iterability and optionality
of adjuncts. In the traditional concept of adjunct-
head phrases, it is the adjunct that selects for the
head it modifies rather than the other way round.
Also semantically, the adjunct is considered the ‘se-
mantic head’ that works as a functor. In light of
this background, it is not implausible to take the
adjunct as the ‘parsing head’ equipped with word
order constraints. In fact, the opposite option —
equipping the syntactic head with its relative word
order with adjuncts— is not as feasible in our lexi-
cal head-corner parsing. The iterability of adjuncts
means that the head would have to be equipped with
an infinite number of adjuncts as its ‘arguments’,
which would lead to various uninstantiation prob-
lems. Therefore, I swap the statuses and treat, in
terms of parsing, the adjunct as a functor with word
order constraints incorporated relative to its modi-
fiee.
Thus, the word order constraints are now given
to the lexical adjuncts also. I will take as an ex-
ample adverbs.13 Adverbs are now the potential lo-
cus of word order patterns relative to its modifiee
(clause/VP), but are not given any specific constraint
in German generally, because one can appear either
after or inside a clause. Our focus is solely on the
possibility of putting one before the clause it modi-
fies, when it is subject to the V2 constraint. This is
handled simply by saying, for such a type, which we
call disl-adverb, it dislocates itself, in the manner of
</bodyText>
<footnote confidence="0.99427875">
11Likewise: sagt ≺ CP(gapped) omitted.
12That is against the temptation for a constituency change
that renders adjuncts sisters on par with arguments (cf. Bouma
et al (2001)), in which case V2 would simply fall out from the
foregoing word order types.
13The same treatment can be extended to prepositional ad-
juncts (remember the unused constraints will percolate up to
the maximal projection).
</footnote>
<page confidence="0.998239">
6
</page>
<bodyText confidence="0.990825375">
‘head movement’ which is widely used in German
syntax (Kiss and Wesche, 1991; Netter, 1992).
disl-adverb: adv (adv→ dislst)
This specification ensures the adverb itself goes
onto the extraction path, to be placed at the left-
periphery, triggered by the mtrx-v2-v type. The sin-
gularity of the adverbials at the prerverbal position
is ensured by means of percolation storage control.
</bodyText>
<sectionHeader confidence="0.995924" genericHeader="method">
6 Verbal Fronting
</sectionHeader>
<bodyText confidence="0.9998331">
Our last challenge concerns fronting of verb or ver-
bal projections. From the preceding discussion, an
option that suggests itself is to treat the verb fronting
as the case of verb dislocating itself. I will in-
deed propose a strategy along this line, but this av-
enue proves more difficult due to complications spe-
cific to verb-related fronting. Firstly, generally such
fronting is limited to the environment of a lower VP
governed by a higher verb such as an auxiliary, as
can be seen from the following contrast:
</bodyText>
<figure confidence="0.9001325">
(4)
a. Gegeben haben die Eltern dem Sohn ein Buch.
b. *Geben, sagt ein Freund, dass die Eltern dem Sohn ein
Buch.
</figure>
<bodyText confidence="0.98299869047619">
Second, the type we used for gegeben in Section
5.3, namely disl-vf-v, clearly does not work, as the
verb does not occur phrase-finally (but in fact ini-
tially) relative to its sisters in (4a). Some relaxation
of LP constraints seem to be in order.
Thirdly, German displays a variety of ways to
front part of a VP:
(5)
Gegeben haben die Eltern dem Sohn ein Buch.
Dem Sohn gegeben haben die Eltern ein Buch.
Ein Buch gegeben haben die Eltern dem Sohn.
Dem Sohn ein Buch gegeben haben die Eltern.
This raises the question of whether this fits in the V2
pattern at all, coupled with the ongoing debate on the
status of the preverbal string. Quite apart from the
theoretical debate, however, how best to adequately
generate these patterns is an acute parsing issue. We
are assuming the flat clause=VP anaylsis, so relax-
ing the singularity condition seems unavoidable.
Fourthly, to make the matter worse, allowing mul-
tiple frontings and dropping LP requirements does
not solve the problem, as ordering of the preverbal
constituents is constrained, as shown in the follow-
ing data:
*Gegeben dem Sohn haben die Eltern ein Buch.
*Dem Sohn gegeben ein Buch haben die Eltern.
It is a great challenge for any syntactician to pro-
vide a unified account for such complex behaviour,
and I confine myself here to offering the ‘solution’
sets of constraints that adequately generate the de-
sired string. What I offer is this: allowing multiple
dislocations only for the verbal fronting cases via a
new word order subtype, while retaining the verb-
final LP conditions for these dislocated constituents.
For this new type we first relax the singularity
condition for dislocation. To allow multiple dislo-
cations, it would suffice to drop the |disl |= 1 condi-
tion, but an unrestricted application of disl C args
would lead to overgeneration, due to two further
constraints applicable: (1) not all arguments can and
(2) the subject argument cannot be fronted along
with the verb (as in (a) and (b) below, respectively):
</bodyText>
<listItem confidence="0.401094">
a. *Die Eltern dem Sohn ein Buch gegeben haben.
b. *Die Eltern gegeben haben dem Sohn ein Buch.
*Die Eltern ein Buch gegeben haben dem Sohn.
</listItem>
<bodyText confidence="0.996851588235294">
Therefore we add the conditions to exlude the above,
along with the the verb-final constraint applicable
the dislocated constituents to exclude (6). Let us call
this type frontable-v. The constraint specification is
as follows:
gegeben (frontable-v):
disl = {gegeben} U ptargs, ptargs ≺ gegeben
where ptargs C args and sNP ∈� ptargs
The proposed constraint set might strike as rather
ad hoc. It would clearly be better to treat both the
fronted and non-fronted occurrences of gegeben as
sharing some common word order type, and what is
meant by ‘applying the constraints amongst the dis-
located constituents’ needs to be fleshed out. Thus
this may not be an elegant solution, but nevertheless
is an generatively adequate solution. More impor-
tantly it serves as a good example for the flexibility
</bodyText>
<page confidence="0.997201">
7
</page>
<bodyText confidence="0.99971725">
and adaptability of constrained free word order pars-
ing, because it handles a rather complex word order
pattern in a way neutral to grammatical construal,
i.e. without invoking constituency manipulation.
</bodyText>
<sectionHeader confidence="0.986863" genericHeader="conclusions">
7 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999976285714286">
I conclude this paper by responding to a natural ob-
jection: why would one have to go through this con-
voluted route of lexical word order control, when
the ‘natural’ way to constrain V2 —or V1 and VF,
for that matter— would be to have some ‘global’
patterns pertinent to clause types? My responses
are primarily engineering-oriented. First, lexicalised
encoding gives the parser, through locality restric-
tion, a certain control over computational complex-
ity, as the search space for constraint enforcement is
restricted.14 However this not an entirely unique, if
more amenable, feature to lexicalised parsing, as one
could impose such a control in non-lexicalised pars-
ing. The advantage truly unique to lexicalising word
order lies in rendering the parser and grammar in-
dependent of surface realisation and hence re-usable
across languages. In short, it promotes modularity.
As we have seen, though the parser needs to con-
form to a certain strategy, the word order component
is fairly independent, as a separate procedure which
can be modified if for example more types of word
order operators are needed. The grammar could also
be kept more compact and cross-linguistically appli-
cable, because word order is abstracted away from
constituency. Therefore, paradoxically, an advan-
tage of lexicalising German parsing is to enable the
same parser/grammar to be used in other languages
too, even if it is not naturally suited to the language.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993767">
Gosse Bouma, Robert Malouf, and Ivan Sag. 2001. Sat-
isfying constraints on extraction and adjunction. Nat-
ural Language and Linguistic Theory, 19(1).
Mike Daniels. 2005. Generalized ID/LP Grammar.
Ph.D. thesis, Ohio State University.
Jay Earley. 1970. An efficient context free parsing algo-
rithm. Communications ofACM, 13:94–102.
Gerald Gazdar and Chris Mellish. 1989. Natural Lan-
guage Processing in Prolog. Addison Wesley.
14For a complexity analysis of such grammar, see Sato (2008)
and Suhre (1999).
Mark Johnson. 1985. Parsing with discontinuous con-
stituents. In Proceedings of the 23rd Annual Meeting
of the ACL, pages 127–132.
Andreas Kathol. 2000. Linear Syntax. OUP.
Tibor Kiss and B Wesche. 1991. Verb order and head
movement. In O Herzog, editor, Text Understanding
in LILOG, pages 216–40. Springer.
Stefan M¨uller. 2004. Continuous or discontinuous con-
stituents? a comparison between syntactic analyses for
constituent order and their processing systems. Re-
search on Language and Computation 2(2).
Klaus Netter. 1992. On non-head non-movement. An
HPSG treatment of finite verb position in German.
In G. G¨orz, editor, Proceedings of KONVENS 92.
Springer.
Gerald Penn. 1999. Linearization and Wh-extraction in
HPSG: Evidence from Serbo-Croatian. In R. Borsely
and A. Przepiorkowski, editors, Slavic in HPSG.
CSLI.
Carl Pollard and Ivan Sag. 1987. Information-Based
Syntax and Semantics. CSLI.
Mike Reape. 1991. Parsing bounded discontinuous con-
stituents: Generalisation of some common algorithms.
DIANA Report, Edinburgh University.
Mike Reape. 1993. A Formal Theory of Word Order.
Ph.D. thesis, Edinburgh University.
Ivan Sag. 2007. Remarks on locality. In Stefan M¨uller,
editor, Proceedings ofHPSG07. CSLI.
Yo Sato. 2006. A proposed lexicalised linearisation
grammar: a monostratal alternative. In Stefan M¨uller,
editor, Proceedings ofHPSG06. CSLI.
Yo Sato. 2008. Implementing Head-Driven Linearisa-
tion Grammar. Ph.D. thesis, King’s College London.
Oliver Suhre. 1999. Computational Aspects of a Gram-
mar Formalism for Languages with Freer Word Order.
Diplomarbeit, Eberhard-Karls-Universit¨at T¨ubingen.
Gertjan van Noord. 1991. Head corner parsing for dis-
continuous constituency. In Proceedings of the 29th
annual meeting on ACL, pages 114–121.
</reference>
<page confidence="0.998492">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.543678">
<title confidence="0.880942">Lexcalised Parsing of German V2</title>
<author confidence="0.977361">Yo</author>
<affiliation confidence="0.975119">Department of Computer Queen Mary, University of</affiliation>
<address confidence="0.892723">Mile End Road, London E1 4NS, U.K.</address>
<abstract confidence="0.982789705882353">This paper presents a method and implementation of parsing German V2 word order by means of constraints that reside in lexical heads. It first describes the design of the underlying parsing engine: the head-corner chart parsing that incorporates a procedure that dynamically enforces word order constraints. While the parser could potentially generate all the permutations of terminal symconstraint checking is conducted in an efficient manner. The paper then shows how this parser can adequately cover a variety of V2 word order patterns with sets of lexically encoded constraints, including non-local preposing of an embedded argument or an adverbial.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Robert Malouf</author>
<author>Ivan Sag</author>
</authors>
<title>Satisfying constraints on extraction and adjunction. Natural Language and Linguistic Theory,</title>
<date>2001</date>
<contexts>
<context position="23199" citStr="Bouma et al (2001)" startWordPosition="3885" endWordPosition="3888">w the potential locus of word order patterns relative to its modifiee (clause/VP), but are not given any specific constraint in German generally, because one can appear either after or inside a clause. Our focus is solely on the possibility of putting one before the clause it modifies, when it is subject to the V2 constraint. This is handled simply by saying, for such a type, which we call disl-adverb, it dislocates itself, in the manner of 11Likewise: sagt ≺ CP(gapped) omitted. 12That is against the temptation for a constituency change that renders adjuncts sisters on par with arguments (cf. Bouma et al (2001)), in which case V2 would simply fall out from the foregoing word order types. 13The same treatment can be extended to prepositional adjuncts (remember the unused constraints will percolate up to the maximal projection). 6 ‘head movement’ which is widely used in German syntax (Kiss and Wesche, 1991; Netter, 1992). disl-adverb: adv (adv→ dislst) This specification ensures the adverb itself goes onto the extraction path, to be placed at the leftperiphery, triggered by the mtrx-v2-v type. The singularity of the adverbials at the prerverbal position is ensured by means of percolation storage contr</context>
</contexts>
<marker>Bouma, Malouf, Sag, 2001</marker>
<rawString>Gosse Bouma, Robert Malouf, and Ivan Sag. 2001. Satisfying constraints on extraction and adjunction. Natural Language and Linguistic Theory, 19(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Daniels</author>
</authors>
<title>Generalized ID/LP Grammar.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Ohio State University.</institution>
<contexts>
<context position="7164" citStr="Daniels (2005)" startWordPosition="1210" endWordPosition="1211"> than once or more than one word counting towards the same rule in the same search path. 2 tion, which materially constitutes a complex verb, is represented as [1,0,1]. This is then then merged with the bitvector of John, [0,1,0] into [1,1,1]. Second, however, this rather promiscuous (and expensive) parsing is dynamically restricted by word order constraints that obtain in individual languages. With sufficient constraints applied during the parse, the above combinations with ring, up and John are restricted to ring up John and ring John up. I do not claim for originality in this basic design. Daniels (2005) for example describes an implementation of an algorithm that falls precisely in such style of parsing.2 The main points of the proposal lie in lexicalisation and localisation, which contrast with the general trend to introduce phrasal and nonlocal constraint processing for German processing, of which Daniels’ work is an example. All the word order constraints are stored in lexicon, more specifically in lexical heads. To adapt this design to a practical lexically driven parsing, the author implemented a rendering of head-corner chart parsing. It is head-corner in the sense described e.g. in va</context>
</contexts>
<marker>Daniels, 2005</marker>
<rawString>Mike Daniels. 2005. Generalized ID/LP Grammar. Ph.D. thesis, Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context free parsing algorithm.</title>
<date>1970</date>
<journal>Communications ofACM,</journal>
<pages>13--94</pages>
<contexts>
<context position="8320" citStr="Earley (1970)" startWordPosition="1394" endWordPosition="1395">sing. It is head-corner in the sense described e.g. in van Noord (1991), where the parsing of a production rule always starts from its head. This is necessary for our design because the parser first retrieves the word order information from the head. Furthermore, it requires the words to be processed first by preterminal rules since without processing lexical heads the whole recognition process does not come off the ground. Therefore, a chart parsing algorithm that invokes lexical initialisation is utilised (as described in Gazdar &amp; Mellish (1989) rather than the classical top-down parsing of Earley (1970)). 3.2 Constraint checking and propagation Since no non-local word order constraints are introduced in our parsing, they can be fully enforced at each application of a production rule. More specifically, the checking of constraint compliance is carried out at the completer operation of chart parsing.3 The data structure of an edge is suitably modified. In addition to the dotted production rule, it needs to carry the constraint set relevant to the corre2A foregoing implementation by M¨uller (2004) also employs bitvector-based linearisation approach. 3The equivalent operation is called the ‘fund</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context free parsing algorithm. Communications ofACM, 13:94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Chris Mellish</author>
</authors>
<title>Natural Language Processing in Prolog. Addison Wesley. 14For a complexity analysis of such grammar, see Sato</title>
<date>1989</date>
<contexts>
<context position="8260" citStr="Gazdar &amp; Mellish (1989)" startWordPosition="1383" endWordPosition="1386">n parsing, the author implemented a rendering of head-corner chart parsing. It is head-corner in the sense described e.g. in van Noord (1991), where the parsing of a production rule always starts from its head. This is necessary for our design because the parser first retrieves the word order information from the head. Furthermore, it requires the words to be processed first by preterminal rules since without processing lexical heads the whole recognition process does not come off the ground. Therefore, a chart parsing algorithm that invokes lexical initialisation is utilised (as described in Gazdar &amp; Mellish (1989) rather than the classical top-down parsing of Earley (1970)). 3.2 Constraint checking and propagation Since no non-local word order constraints are introduced in our parsing, they can be fully enforced at each application of a production rule. More specifically, the checking of constraint compliance is carried out at the completer operation of chart parsing.3 The data structure of an edge is suitably modified. In addition to the dotted production rule, it needs to carry the constraint set relevant to the corre2A foregoing implementation by M¨uller (2004) also employs bitvector-based linearisa</context>
</contexts>
<marker>Gazdar, Mellish, 1989</marker>
<rawString>Gerald Gazdar and Chris Mellish. 1989. Natural Language Processing in Prolog. Addison Wesley. 14For a complexity analysis of such grammar, see Sato (2008) and Suhre (1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Parsing with discontinuous constituents.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting of the ACL,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="5819" citStr="Johnson (1985)" startWordPosition="997" endWordPosition="998">pt the traditional binary iteration analysis for adjunct-head phrases, to see how our parser fares with configurational analyses. I sum up the assumed constituency of the above examples graphically as trees (though this has little impact on word order): 3 The parser 3.1 Core design The design of the parser employed here can be called constrained free word order parsing. First, it allows for completely free word order at default. The core algorithm for the parse engine is what Reape (1991) presents as a generalised permutationcomplete parser, which in turn is based on the preceding proposal of Johnson (1985). Details apart, while using context-free production rules (no multiple left-hand side non-terminal symbols), this algorithm only checks for the presence of all the righthand side constituents, wherever in the string they occur, potentially discontinuously,1 effectively licensing all the permutations of the given terminal symbols (e.g. 3! = 6 permutations for the string consisting of ring, up and John including up John ring etc.). This ‘directionless’ parsing is rendered possible by Johnson’s ‘bitvector’ representation of partial string coverage. In the above up John ring string, the coverage </context>
</contexts>
<marker>Johnson, 1985</marker>
<rawString>Mark Johnson. 1985. Parsing with discontinuous constituents. In Proceedings of the 23rd Annual Meeting of the ACL, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Kathol</author>
</authors>
<title>Linear Syntax.</title>
<date>2000</date>
<publisher>OUP.</publisher>
<contexts>
<context position="2201" citStr="Kathol (2000)" startWordPosition="348" endWordPosition="349">ato (2008). The main benefit of a linearisation approach is that syntactic constituency becomes independent (to a degree) of its surface realisation and hence discourages constituency manipulation for the sake of word order. In line of this spirit I will largely adopt the simple constituency construal that faithfully correspond to its semantics. However, I distance myself from the more or less standard version of linearisation grammar where potentially non-local LP conditions are permitted (Reape, 1993) or word order patterns are imposed at the clause level (as in ‘topological field’ model of Kathol (2000)). The crux of the proposal consists in employing a head-corner parsing in which the set of word order constraints are incorporated into a VP’s lexical head (i.e. common or auxiliary verb). For a V2 projection, its head verb contains the constraints to the effect that only one of its arguments can be fronted immediately before the verb itself. To enable this, potential discontinuity and obligatory adjacency in part of a phrase is included in the repertoire of word order constraints in addition to the standard LP (linear precedence) constraints. 2 The data The V2 constructions to be dealt with </context>
</contexts>
<marker>Kathol, 2000</marker>
<rawString>Andreas Kathol. 2000. Linear Syntax. OUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tibor Kiss</author>
<author>B Wesche</author>
</authors>
<title>Verb order and head movement. In</title>
<date>1991</date>
<booktitle>Text Understanding in LILOG,</booktitle>
<pages>216--40</pages>
<editor>O Herzog, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="23498" citStr="Kiss and Wesche, 1991" startWordPosition="3934" endWordPosition="3937">s subject to the V2 constraint. This is handled simply by saying, for such a type, which we call disl-adverb, it dislocates itself, in the manner of 11Likewise: sagt ≺ CP(gapped) omitted. 12That is against the temptation for a constituency change that renders adjuncts sisters on par with arguments (cf. Bouma et al (2001)), in which case V2 would simply fall out from the foregoing word order types. 13The same treatment can be extended to prepositional adjuncts (remember the unused constraints will percolate up to the maximal projection). 6 ‘head movement’ which is widely used in German syntax (Kiss and Wesche, 1991; Netter, 1992). disl-adverb: adv (adv→ dislst) This specification ensures the adverb itself goes onto the extraction path, to be placed at the leftperiphery, triggered by the mtrx-v2-v type. The singularity of the adverbials at the prerverbal position is ensured by means of percolation storage control. 6 Verbal Fronting Our last challenge concerns fronting of verb or verbal projections. From the preceding discussion, an option that suggests itself is to treat the verb fronting as the case of verb dislocating itself. I will indeed propose a strategy along this line, but this avenue proves more</context>
</contexts>
<marker>Kiss, Wesche, 1991</marker>
<rawString>Tibor Kiss and B Wesche. 1991. Verb order and head movement. In O Herzog, editor, Text Understanding in LILOG, pages 216–40. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan M¨uller</author>
</authors>
<title>Continuous or discontinuous constituents? a comparison between syntactic analyses for constituent order and their processing systems.</title>
<date>2004</date>
<journal>Research on Language and Computation</journal>
<volume>2</volume>
<issue>2</issue>
<marker>M¨uller, 2004</marker>
<rawString>Stefan M¨uller. 2004. Continuous or discontinuous constituents? a comparison between syntactic analyses for constituent order and their processing systems. Research on Language and Computation 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Netter</author>
</authors>
<title>On non-head non-movement. An HPSG treatment of finite verb position in German.</title>
<date>1992</date>
<booktitle>Proceedings of KONVENS 92.</booktitle>
<editor>In G. G¨orz, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="23513" citStr="Netter, 1992" startWordPosition="3938" endWordPosition="3939">straint. This is handled simply by saying, for such a type, which we call disl-adverb, it dislocates itself, in the manner of 11Likewise: sagt ≺ CP(gapped) omitted. 12That is against the temptation for a constituency change that renders adjuncts sisters on par with arguments (cf. Bouma et al (2001)), in which case V2 would simply fall out from the foregoing word order types. 13The same treatment can be extended to prepositional adjuncts (remember the unused constraints will percolate up to the maximal projection). 6 ‘head movement’ which is widely used in German syntax (Kiss and Wesche, 1991; Netter, 1992). disl-adverb: adv (adv→ dislst) This specification ensures the adverb itself goes onto the extraction path, to be placed at the leftperiphery, triggered by the mtrx-v2-v type. The singularity of the adverbials at the prerverbal position is ensured by means of percolation storage control. 6 Verbal Fronting Our last challenge concerns fronting of verb or verbal projections. From the preceding discussion, an option that suggests itself is to treat the verb fronting as the case of verb dislocating itself. I will indeed propose a strategy along this line, but this avenue proves more difficult due </context>
</contexts>
<marker>Netter, 1992</marker>
<rawString>Klaus Netter. 1992. On non-head non-movement. An HPSG treatment of finite verb position in German. In G. G¨orz, editor, Proceedings of KONVENS 92. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Penn</author>
</authors>
<title>Linearization and Wh-extraction in HPSG: Evidence from Serbo-Croatian. In</title>
<date>1999</date>
<editor>R. Borsely and A. Przepiorkowski, editors, Slavic in HPSG.</editor>
<publisher>CSLI.</publisher>
<contexts>
<context position="15465" citStr="Penn (1999)" startWordPosition="2603" endWordPosition="2604">tion —not just one constituent per clause in multiple embeddings— becomes thus possible. Let args be the set of the arguments of a disl-v, disl be that of the dislocated one and situ be that of the remaining arguments, i.e. disl C args where |disl |= 1 and situ = {x|x C args n x V disl}. Then the type disl-v can be characterised as having the following constraint: disl-v: disl --&lt; situ (disl --+ dislst) Simply put, this says that the arguments are divided into two parts, the dislocated and in-situ parts, the former of which precedes the latter. We assume, as 7The adopted mechanism is close to Penn (1999), though he invokes potentially non-local topology-based constraints and removes the filler and gapped head entirely. 4 in the standard treatment, there is only one dislocated constituent, until we consider the VP fronting. The notation with an arrow on the right indicates this singleton set is pushed into the storage that is propagated upwards. The mtrx-v2-v type is then characterised as follows: mtrx-v2-v: dislst ≺ verb, {dislst, verb} This simply says the dislocated constituent (stored in a lower node and percolated) immediately precedes the matrix verb. (For the following presentation, the</context>
</contexts>
<marker>Penn, 1999</marker>
<rawString>Gerald Penn. 1999. Linearization and Wh-extraction in HPSG: Evidence from Serbo-Croatian. In R. Borsely and A. Przepiorkowski, editors, Slavic in HPSG. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Information-Based Syntax and Semantics.</title>
<date>1987</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="13360" citStr="Pollard and Sag (1987)" startWordPosition="2248" endWordPosition="2251">tedious task of writing them all individually, if they allow for broader classification. For example the English transitive verb generally follows its subject argument and precedes its object argument, and one would naturally want to lump these verbs under one umbrella. For such a cluster of lexical heads, we will introduce a word order (sub)type. More pertinently, the German verbs may be classified into v1-verb, v2- verb and vf-verb according to the positions of their arguments in their projection. We will also allow multiple inheritance that becomes standard in the typed feature system (cf. Pollard and Sag (1987)). 6See Sato (2006) for details. 5 Constraints for V2 5.1 General setup To enforce the V2 word order pattern lexically, I propose to use a combination of two word order subtypes: dislocating-verb (disl-v) and matrix-v2-verb (mtrx-v2-v). The former type represents a verb one of whose arguments is to be ‘dislocated’. A verb of this type can thus be characterised as ‘contributing’ the dislocated (preverbal) element. The latter, on the other hand, is the type that is projected onto a matrix sentence. This type should be constrained such that one dislocated constituent must —and only one may— prece</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Carl Pollard and Ivan Sag. 1987. Information-Based Syntax and Semantics. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
</authors>
<title>Parsing bounded discontinuous constituents: Generalisation of some common algorithms.</title>
<date>1991</date>
<tech>DIANA Report,</tech>
<institution>Edinburgh University.</institution>
<contexts>
<context position="5698" citStr="Reape (1991)" startWordPosition="977" endWordPosition="978">nalyses. In fact we take an auxiliary verb to subcategorise for a clause rather than the complex verb analysis, and adopt the traditional binary iteration analysis for adjunct-head phrases, to see how our parser fares with configurational analyses. I sum up the assumed constituency of the above examples graphically as trees (though this has little impact on word order): 3 The parser 3.1 Core design The design of the parser employed here can be called constrained free word order parsing. First, it allows for completely free word order at default. The core algorithm for the parse engine is what Reape (1991) presents as a generalised permutationcomplete parser, which in turn is based on the preceding proposal of Johnson (1985). Details apart, while using context-free production rules (no multiple left-hand side non-terminal symbols), this algorithm only checks for the presence of all the righthand side constituents, wherever in the string they occur, potentially discontinuously,1 effectively licensing all the permutations of the given terminal symbols (e.g. 3! = 6 permutations for the string consisting of ring, up and John including up John ring etc.). This ‘directionless’ parsing is rendered pos</context>
</contexts>
<marker>Reape, 1991</marker>
<rawString>Mike Reape. 1991. Parsing bounded discontinuous constituents: Generalisation of some common algorithms. DIANA Report, Edinburgh University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
</authors>
<title>A Formal Theory of Word Order.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Edinburgh University.</institution>
<contexts>
<context position="2096" citStr="Reape, 1993" startWordPosition="329" endWordPosition="330">d order is dissociated from the syntactic structure in a discontinuityallowing manner, as presented in Sato (2008). The main benefit of a linearisation approach is that syntactic constituency becomes independent (to a degree) of its surface realisation and hence discourages constituency manipulation for the sake of word order. In line of this spirit I will largely adopt the simple constituency construal that faithfully correspond to its semantics. However, I distance myself from the more or less standard version of linearisation grammar where potentially non-local LP conditions are permitted (Reape, 1993) or word order patterns are imposed at the clause level (as in ‘topological field’ model of Kathol (2000)). The crux of the proposal consists in employing a head-corner parsing in which the set of word order constraints are incorporated into a VP’s lexical head (i.e. common or auxiliary verb). For a V2 projection, its head verb contains the constraints to the effect that only one of its arguments can be fronted immediately before the verb itself. To enable this, potential discontinuity and obligatory adjacency in part of a phrase is included in the repertoire of word order constraints in addit</context>
</contexts>
<marker>Reape, 1993</marker>
<rawString>Mike Reape. 1993. A Formal Theory of Word Order. Ph.D. thesis, Edinburgh University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Sag</author>
</authors>
<title>Remarks on locality.</title>
<date>2007</date>
<booktitle>Proceedings ofHPSG07.</booktitle>
<editor>In Stefan M¨uller, editor,</editor>
<publisher>CSLI.</publisher>
<contexts>
<context position="10653" citStr="Sag (2007)" startWordPosition="1786" endWordPosition="1787">us the edges for string combinations that violate the word order constraints would not be created, eliminating wasteful search paths. As we will shortly see, the constraint type that checks continuity of a phrase is also introduced. Therefore the phrase (dis)continuity can also be ascertained locally, which is a major advantage over a parsing that relies largely on concatenation. Thus, the cost of constraint checking remains very small despite the capability of processing discontinuity.5 Note however that by locality is meant subcategorisation locality (or ‘selection’ locality as described in Sag (2007)): whatever is in the same subcategorisation frame of a lexical head is considered local. Depending on the adopted analysis, constituents ‘local’ in this sense may of course occur in different trees. Constraints on such ‘non-local’ —in the tree sense but not in the subcategorisation sense— constituents are still enforceable in the implemented parser. The unused constraints at a node, 4This retrieval of word order information is carried out at the predictor stage of chart parsing. 5It is worth mentioning that the bitvector checking is conducted over the whole string, the effect of applied const</context>
</contexts>
<marker>Sag, 2007</marker>
<rawString>Ivan Sag. 2007. Remarks on locality. In Stefan M¨uller, editor, Proceedings ofHPSG07. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yo Sato</author>
</authors>
<title>A proposed lexicalised linearisation grammar: a monostratal alternative.</title>
<date>2006</date>
<booktitle>Proceedings ofHPSG06.</booktitle>
<editor>In Stefan M¨uller, editor,</editor>
<publisher>CSLI.</publisher>
<contexts>
<context position="13379" citStr="Sato (2006)" startWordPosition="2253" endWordPosition="2254">ll individually, if they allow for broader classification. For example the English transitive verb generally follows its subject argument and precedes its object argument, and one would naturally want to lump these verbs under one umbrella. For such a cluster of lexical heads, we will introduce a word order (sub)type. More pertinently, the German verbs may be classified into v1-verb, v2- verb and vf-verb according to the positions of their arguments in their projection. We will also allow multiple inheritance that becomes standard in the typed feature system (cf. Pollard and Sag (1987)). 6See Sato (2006) for details. 5 Constraints for V2 5.1 General setup To enforce the V2 word order pattern lexically, I propose to use a combination of two word order subtypes: dislocating-verb (disl-v) and matrix-v2-verb (mtrx-v2-v). The former type represents a verb one of whose arguments is to be ‘dislocated’. A verb of this type can thus be characterised as ‘contributing’ the dislocated (preverbal) element. The latter, on the other hand, is the type that is projected onto a matrix sentence. This type should be constrained such that one dislocated constituent must —and only one may— precede and be adjacent </context>
</contexts>
<marker>Sato, 2006</marker>
<rawString>Yo Sato. 2006. A proposed lexicalised linearisation grammar: a monostratal alternative. In Stefan M¨uller, editor, Proceedings ofHPSG06. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yo Sato</author>
</authors>
<title>Implementing Head-Driven Linearisation Grammar.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>King’s College London.</institution>
<contexts>
<context position="1598" citStr="Sato (2008)" startWordPosition="250" endWordPosition="251">xicalised, as the V2 pattern is licensed ultimately encoded in verbs, in the form of constraints that hold amongst its arguments and itself; localityrespecting, because (a) no constraint that operates on constituents from different subcategorisation frames is invoked and (b) the matrix verb and the preverbal constituent, however ‘distant’ its origin is, are ordered in the same projection via the slash-based mechanism. The underlying grammar is loosely linearisationbased, in the sense that word order is dissociated from the syntactic structure in a discontinuityallowing manner, as presented in Sato (2008). The main benefit of a linearisation approach is that syntactic constituency becomes independent (to a degree) of its surface realisation and hence discourages constituency manipulation for the sake of word order. In line of this spirit I will largely adopt the simple constituency construal that faithfully correspond to its semantics. However, I distance myself from the more or less standard version of linearisation grammar where potentially non-local LP conditions are permitted (Reape, 1993) or word order patterns are imposed at the clause level (as in ‘topological field’ model of Kathol (20</context>
</contexts>
<marker>Sato, 2008</marker>
<rawString>Yo Sato. 2008. Implementing Head-Driven Linearisation Grammar. Ph.D. thesis, King’s College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Suhre</author>
</authors>
<title>Computational Aspects of a Grammar Formalism for Languages with Freer Word Order.</title>
<date>1999</date>
<tech>Diplomarbeit, Eberhard-Karls-Universit¨at T¨ubingen.</tech>
<marker>Suhre, 1999</marker>
<rawString>Oliver Suhre. 1999. Computational Aspects of a Grammar Formalism for Languages with Freer Word Order. Diplomarbeit, Eberhard-Karls-Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Head corner parsing for discontinuous constituency.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th annual meeting on ACL,</booktitle>
<pages>114--121</pages>
<marker>van Noord, 1991</marker>
<rawString>Gertjan van Noord. 1991. Head corner parsing for discontinuous constituency. In Proceedings of the 29th annual meeting on ACL, pages 114–121.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>