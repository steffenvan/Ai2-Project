<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000102">
<title confidence="0.979588">
A Comparison of Event Models for
Naive Bayes Anti-Spam E-Mail Filtering
</title>
<author confidence="0.98801">
Karl-Michael Schneider
</author>
<affiliation confidence="0.9942305">
University of Passau
Department of General Linguistics
</affiliation>
<address confidence="0.888028">
Innstr. 40, D-94032 Passau
</address>
<email confidence="0.995079">
schneide@phil.uni-passau.de
</email>
<sectionHeader confidence="0.994675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997635">
We describe experiments with a Naive
Bayes text classifier in the context of
anti- spam E-mail filtering, using two
different statistical event models: a mul-
ti-variate Bernoulli model and a multi-
nomial model. We introduce a family of
feature ranking functions for feature se-
lection in the multinomial event model
that take account of the word frequency
information. We present evaluation re-
sults on two publicly available corpora
of legitimate and spam E-mails. We find
that the multinomial model is less biased
towards one class and achieves slightly
higher accuracy than the multi-variate
Bernoulli model.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999666796296297">
Text categorization is the task of assigning a text
document to one of several predefined categories.
Text categorization plays an important role in nat-
ural language processing (NLP) and information
retrieval (IR) applications. One particular applica-
tion of text categorization is anti-spam E-mail fil-
tering, where the goal is to block unsolicited mes-
sages with commercial or pornographic content
(UCE, spam) from a user&apos;s E-mail stream, while
letting other (legitimate) messages pass. Here, the
task is to assign a message to one of two cate-
gories, legitimate and spam, based on the mes-
sage&apos;s content.
In recent years, a growing body of research has
applied machine learning techniques to text cat-
egorization and (anti-spam) E-mail filtering, in-
cluding rule learning (Cohen, 1996), Naive Bayes
(Sahami et al., 1998; Androutsopoulos et al.,
2000b; Rennie, 2000), memory based learning
(Androutsopoulos et al., 2000b), decision trees
(Carreras and Marquez, 2001), support vector ma-
chines (Drucker et al., 1999) or combinations of
different learners (Sakkis et al., 2001). In these ap-
proaches a classifier is learned from training data
rather than constructed by hand, which results in
better and more robust classifiers.
The Naive Bayes classifier has been found par-
ticularly attractive for the task of text categoriza-
tion because it performs surprisingly well in many
application areas despite its simplicity (Lewis,
1998). Bayesian classifiers are based on a prob-
abilistic model of text generation. A text is gener-
ated by first choosing a class according to some
prior probability and then generating a text ac-
cording to a class-specific distribution. The model
parameters are estimated from training examples
that have been annotated with their correct class.
Given a new document, the classifier outputs the
class which is most likely to have generated the
document.
From a linguistic point of view, a document is
made up of words, and the semantics of the doc-
ument is determined by the meaning of the words
and the linguistic structure of the document. The
Naive Bayesian classifier makes the simplifying
assumption that the probability that a document is
generated in some class depends only on the prob-
abilities of the words given the context of the class,
and that the words in a document are independent
of each other. This is called the Naive Bayes as-
sumption.
The generative model underlying the Naive
Bayes classifier can be characterized with respect
to the amount of information it captures about the
</bodyText>
<page confidence="0.99751">
307
</page>
<bodyText confidence="0.999976">
words in a document. In information retrieval and
text categorization, two types of models have been
used (McCallum and Nigam, 1998). Both assume
that there is a fixed vocabulary. In the first model,
a document is generated by first choosing a sub-
set of the vocabulary and then using the selected
words any number of times, at least once, in any
order. This model is called multi-variate Bernoulli
model. It captures the information of which words
are used in a document, but not the number of
times each words is used, nor the order of the
words in the document.
In the second model, a document is generated
by choosing a set of word occurrences and arrang-
ing them in any order. This model is called multi-
nomial model. In addition to the multi-variate
Bernoulli model, it also captures the information
about how many times a word is used in a docu-
ment. Note that in both models, a document can
contain additional words that are not in the vocab-
ulary, which are considered noise and are not used
for classification.
Despite the fact that the multi-variate Bernoulli
model captures less information about a document
(compared to the multinomial model), it performs
quite well in text categorization tasks, particu-
larly when the set of words used for classification
is small. However, McCallum and Nigam (1998)
have shown that the multinomial model outper-
forms the multi-variate Bernoulli model on larger
vocabulary sizes or when the vocabulary size is
chosen optimal for both models.
Most text categorization approaches to anti-
spam E-mail filtering have used the multi-variate
Bernoulli model (Androutsopoulos et al., 2000b).
Rennie (2000) used a multinomial model but
did not compare it to the multi-variate model.
Mladenia and Grobelnik (1999) used a multino-
mial model in a different context. In this paper we
present results of experiments in which we evalu-
ated the performance of a Naive Bayes classifier
on two publicly available E-mail corpora, using
both the multi-variate Bernoulli and the multino-
mial model.
The paper is organized as follows. In Sect. 2
we describe the Naive Bayes classifier and the two
generative models in more detail. In Sect. 3 we in-
troduce feature selection methods that take into ac-
count the extra information contained in the multi-
nomial model. In Sect. 4 we describe our experi-
ments and discuss the results. Finally, in Sect. 5
we draw some conclusions.
</bodyText>
<sectionHeader confidence="0.948804" genericHeader="method">
2 Naive Bayes Classifier
</sectionHeader>
<bodyText confidence="0.998244083333333">
We follow the description of the Naive Bayes clas-
sifier given in McCallum and Nigam (1998). A
Bayesian classifier assumes that a document is
generated by a mixture model with parameters 0,
consisting of components C = {ci, cm} that
correspond to the classes. A document is gener-
ated by first selecting a component c3 E C ac-
cording to the prior distribution P(c318) and then
choosing a document di according to the parame-
ters of c3 with distribution P(dilc3; 0). The likeli-
hood of a document is given by the total probabil-
ity
</bodyText>
<equation confidence="0.9999635">
0) = p(ei
j=1 0)P(diici; 0) (1)
</equation>
<bodyText confidence="0.999966555555555">
Of course, the true parameters 0 of the mixture
model are not known. Therefore, one estimates
the parameters from labeled training documents,
i.e. documents that have been manually annotated
with their correct class. We denote the estimated
parameters with 0. Given a set of training docu-
ments D = {d1, . . . , dm}, the class prior parame-
ters are estimated as the fraction of training docu-
ments in c3, using maximum likelihood:
</bodyText>
<equation confidence="0.981963">
= P( 2jj:&apos;1P(c.i di) (2)
</equation>
<bodyText confidence="0.999515571428572">
where P(ci di) is 1 if di E cj and 0 otherwise.
The estimation of P(dilc3; 0) depends on the gen-
erative model and is described below.
Given a new (unseen) document d, classifica-
tion of d is performed by computing the poste-
rior probability of each class, given d, by applying
Bayes&apos; rule:
</bodyText>
<equation confidence="0.999957">
P( — P(ci lo)p(c! 4\
P(d10) „ (3)
</equation>
<bodyText confidence="0.988485">
The classifier simply selects the class with, the
highest posterior probability. Note that P(d18) is
</bodyText>
<equation confidence="0.511713">
P (di
</equation>
<page confidence="0.982221">
308
</page>
<bodyText confidence="0.996306">
the same for all classes, thus d can be classified by
computing
</bodyText>
<equation confidence="0.84044">
Cd = argmax P(ej 0)P(d cj; 0) (4)
ct EC
</equation>
<subsectionHeader confidence="0.989822">
2.1 Multi-variate Bernoulli Model
</subsectionHeader>
<bodyText confidence="0.9999976">
The multi-variate Bernoulli event model assumes
that a document is generated by a series of VI
Bernoulli experiments, one for each word wt in
the vocabulary V. The outcome of each experi-
ment determines whether the corresponding word
will be included at least once in the document.
Thus a document di can be represented as a bi-
nary feature vector of length I V. where each di-
mension t of the vector, denoted as Bitc {0, 1},
indicates whether word wt occurs at least once in
dz. The Naive Bayes assumption assumes that the
V trials are independent of each other. By mak-
ing the Naive Bayes assumption, we can compute
the probability of a document given a class from
the probabilities of the words given the class:
</bodyText>
<equation confidence="0.993418333333333">
I v
P(di Icj ; 0) = H(BitP(wt e;9)+
(1 — Bit)(1 — P(wtIci; 0))) (5)
</equation>
<bodyText confidence="0.9997672">
Note that words which do not occur in di con-
tribute to the probability of di as well. The param-
eters 0 = P(wt, ci; 0) of the mixture compo-
nent ci can be estimated as the fraction of training
documents in cj that contain w1:1
</bodyText>
<equation confidence="0.991914333333333">
BitP(ci di)
6tvtlet P(wtIcj; 0) =
P(cd)
</equation>
<subsectionHeader confidence="0.964104">
2.2 Multinomial Model
</subsectionHeader>
<bodyText confidence="0.999536764705882">
The multinomial event model assumes that a doc-
ument di of length di is generated by a sequence
of I d7 word events, where the outcome of each
event is a word from the vocabulary V. Following
McCallum and Nigam (1998), we assume that the
document length distribution P(Id, I) does not de-
pend on the class. Thus a document di can be rep-
resented as a vector of length I V , where each di-
mension t of the vector, denoted as Nit &gt; 0, is the
&apos;McCallum and Nigam (1998) suggest to use a Laplacean
prior to smooth the probabilities, but we found that this de-
graded the performance of the classifier.
number of times word wt occurs in cit. The Naive
Bayes assumption assumes that the dI trials are
independent of each other. By making the Naive
Bayes assumption, the probability of a document
given a class is the multinomial distribution:
</bodyText>
<equation confidence="0.899261">
Ivl (1—r P wt ci; 0)Nit
ci; = P(Iciadil! 11
t=1
(7)
The parameters O = P(wtIci; 0) of the mix-
</equation>
<bodyText confidence="0.997608666666667">
ture component cj can be estimated as the fraction
of word occurrences in the training documents in
ci that are wt:
</bodyText>
<equation confidence="0.848794">
6tutc.i — Pettit
</equation>
<sectionHeader confidence="0.986675" genericHeader="method">
3 Feature Selection
</sectionHeader>
<subsectionHeader confidence="0.993336">
3.1 Mutual Information
</subsectionHeader>
<bodyText confidence="0.999893956521739">
It is common to use only a subset of the vocabulary
for classification, in order to reduce over-fitting to
the training data and to speed up the classification
process. Following McCallum and Nigam (1998)
and Androutsopoulos et al. (2000b), we ranked
the words according to their average mutual in-
formation with the class variable and selected the
N highest ranked words. Average mutual infor-
mation between a word wt and the class vari-
able, denoted by M/(C; W1), is the difference be-
tween the entropy of the class variable, H(C),
and the entropy of the class variable given the in-
formation about the word, H(CI Wt) (Cover and
Thomas, 1991). Intuitively, M/(C: VV) measures
how much bandwidth can be saved in the transmis-
sion of a class value when the information about
the word is known.
In the multi-variate Bernoulli model, Wt is a
random variable that takes on values ft E {0, 1},
indicating whether word wt occurs in a document
or not. Thus M/(C; Wt) is the average mutual in-
formation between C and the absence or presence
of tot in a document:
</bodyText>
<equation confidence="0.983699555555556">
p(c, ft)
MI(C;Wt) — E E P(c, ft) log P(e)P(ft)
cec itc{0,1}
(9)
(6)
P(di
c-3 •
NisP(Ci
(8)
</equation>
<page confidence="0.9809">
309
</page>
<subsectionHeader confidence="0.387437">
3.2 Feature Selection in the Multinomial used feature ranking functions of the form in (10):
</subsectionHeader>
<bodyText confidence="0.980008413793103">
Model
Mutual information as in (9) has also been used
for feature selection in the multinomial model, ei-
ther by estimating the probabilities P(e, ft), P(e)
and P(h) as in the multi-variate model (McCal-
lum and Nigam, 1998) or by using the multinomial
probabilities (Mladenie and Grobelnik, 1999). Let
us call the two versions mv-M/ and mn-MI, respec-
tively.
mn-MI is not fully adequate as a feature ranking
function for the multinomial model. For example,
the token Subject: appears in every document
in the two corpora we used in our experiments
exactly once, and thus is completely uninforma-
tive. mv-M/ assigns 0 to this token, but mn-MI
yields a positive value because the average doc-
ument length in the classes is different, and thus
the class-conditional probabilities of the token are
different across classes in the multinomial model.
On the other hand, assume that some token occurs
once in every document in el and twice in every
document in e2, and that the average document
length in c2 is twice the average document length
in el. Then both mv-M/ and mn-MI will assign 0
to the token, although it is clearly highly informa-
tive in the multinomial model.
We experimented with feature scoring functions
that take into account the average number of times
a word occurs in a document. Let N(ci, wt) =
</bodyText>
<equation confidence="0.822371">
v
N(c1) = N (c , w I) and
N (wt) = Ej 1N(ci, tut) denote the number
</equation>
<bodyText confidence="0.999954333333333">
of times word wt occurs in class ej, the total
number of word occurrences in ej, and the total
number of occurrences of wt, respectively. Let
d(e) = En P(cd) denote the number of
documents in ej. Then the average number of
times wt occurs in a document in cj is defined
</bodyText>
<equation confidence="0.9030475">
N(ci./Dt)
by mtfle&amp;quot;, wt) = d(c) (mean term frequency).
</equation>
<bodyText confidence="0.7047315">
The average number of times wt occurs in a docu-
ment is defined by mtflwt) = (&amp;quot;)
</bodyText>
<equation confidence="0.665284">
I DI •
</equation>
<bodyText confidence="0.999969">
In the multinomial model, a word is informative
with respect to the class value if its mean term fre-
quency in some class is different from its (global)
</bodyText>
<equation confidence="0.787218">
mean term frequency, i.e. if nuf(c3&apos;&amp;quot;) I. We
IC
wt) = E f (ei,wt) log R(ei, wt) (10)
i=1
</equation>
<bodyText confidence="0.995059333333333">
R(ci, wt) measures the amount of information that
wt gives about ci. f (ej,wt) is a weighting func-
tion. Table 1 lists the feature ranking functions
that we used in our experiments. mn-MI is the av-
erage mutual information where the probabilities
are estimated as in the multinomial model. dmn-
MI differs from mn-MI in that the class prior prob-
abilities are estimated as the fraction of documents
in each class, rather than the fraction of word oc-
currences. YLMI, dtf-MI and tftf-MI use mean term
frequency to measure the correlation between tut
and cj and use different weighting functions.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.601115">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999993208333333">
We performed experiments on two publicly avail-
able E-mail corpora:2 Ling-Spam (Androutsopou-
los et al., 2000b) and PU1 (Androutsopoulos et al.,
2000a). We trained a Naive Bayes classifier with
a multi-variate Bernoulli model and a multinomial
model on each of the two datasets.
The Ling-Spam corpus consists of 2412 mes-
sages from the Linguist list3 and 481 spam mes-
sages. Thus spam messages are 16.6% of the
corpus. Attachments, HTML tags and all E-mail
headers except the Subject line have been stripped
off. We used the lemmatized version of the cor-
pus, with the tokenization given in the corpus and
with no additional processing, stop list, etc. The
total vocabulary size is 59829 words.
The PU1 corpus consists of 618 English legit-
imate messages and 481 spam messages. Mes-
sages in this corpus are encrypted: Each token
has been replaced by a unique number, such that
different occurrences of the same token get the
same number (the only non encrypted token is the
Subject: header name). Spam messages are
43.8% of the corpus. As with the Ling-Spam cor-
pus, we used the lemmatized version with no ad-
</bodyText>
<footnote confidence="0.976423">
2 available from the publications section of
http: //www . aueb. gr/users/ion/
3http: / /www . linguistlist org/
</footnote>
<page confidence="0.991439">
310
</page>
<table confidence="0.9968742">
Name f (ci , wt) R(ej, wt)
mn-MI N (ei , wt) Ph ,wt) N(cpwt) EsIvji N (ws)
P(cj,wt) P(c)P(wt) N (ci) N (wt)
—
ws
E8-1N()
dmn-MI P(wtei)P(ci) = N(ci, Wt) d(c) P(wt Ie.&apos;) N(ci, wt)/N(c)
l
N (ci) 11)1 P (wt) Er 1 P(ck)(N(ck, wt)IN(ck))
if-MI N (c1, wt) mt.f(ci ,wt) N (ci , Int) 11)1
P
(cj , t) — Iv, mtfewt) d(c) Net)
Els ! LL N (w s)
dtf-MI N (c wi) d(c) intf(c wi) N (c. w I) 11&apos;1
=
P(wdej)P(c.i) mtf(wt) d(c) N (wt)
N(j) 11)1
tftf-MI N (c.i , wt) mtf(ej , wt) N (ci , t) 1)1
mff(cj , t)= mtf(wt) d(c) N (wt)
d(ci)
</table>
<tableCaption confidence="0.999766">
Table 1: Feature ranking functions for the multinomial event model (see text).
</tableCaption>
<bodyText confidence="0.999391125">
ditional processing. The total vocabulary size is
21706 words.
Both corpora are divided into 10 parts of equal
size, with equal proportion of legitimate and spam
messages across the 10 parts. Following (Androut-
sopoulos et al., 2000b), we used 10-fold cross-
validation in all experiments, using nine parts for
training and the remaining part for testing, with a
different test set in each trial. The evaluation mea-
sures were then averaged across the 10 iterations.
We performed experiments on each of the cor-
pora, using the multi-variate Bernoulli model with
mv-M/, as well as the multinomial model with my-
MI and the feature ranking functions in Table 1,
and varying the number of selected words from 50
to 5000 by 50.
</bodyText>
<sectionHeader confidence="0.628634" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.99993541025641">
For each event model and feature ranking func-
tion, we determined the minimum number of
words with highest recall for which recall equaled
precision (breakeven point). Tables 2 and 3
present the breakeven points with the number of
selected words, recall in each class, and accuracy.
In some cases, precision and recall were differ-
ent over the entire range of the number of selected
words. In these cases we give the recall and accu-
racy for the minimum number of words for which
accuracy was highest.
Figures 1 and 2 show recall curves for the multi-
variate Bernoulli model and three feature rank-
ing functions in the multinomial model for Ling-
Spam, and Figures 3 and 4 for PU I .
Some observations can be made from these re-
sults. First, the multi-variate Bernoulli model fa-
vors the Ling resp. Legit classes over the Spam
classes, whereas the multinomial model is more
balanced in conjunction with mv-MI, if-MI and
tftf-MI. This may be due to the relatively specific
vocabulary used especially in the Ling-Spam cor-
pus, and to the uneven distribution of the doc-
uments in the classes. Second, the multinomial
model achieves higher accuracy than the multi-
variate Bernoulli model. If-Mi even achieves high
accuracy at a comparatively small vocabulary size
(1200 and 2400 words, respectively). In general,
PU1 seems to be more difficult to classify.
Androutsopoulos et al. (2000b) used cost-sen-
sitive evaluation metrics to account for the fact
that it may be more serious an error when a le-
gitimate message is classified as spam than vice
versa. However, such cost-sensitive measures are
problematic with a Naive Bayes classifier because
the probabilities computed by Naive Bayes are not
reliable, due to the independence assumptions it
makes. Therefore we did not use cost-sensitive
measures.4
</bodyText>
<sectionHeader confidence="0.99842" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9998135">
We performed experiments with two different sta-
tistical event models (a multi-variate Bernoulli
</bodyText>
<footnote confidence="0.999219666666667">
4Despite this, Naive Bayes can be an optimal classifier be-
cause it uses only the ranking implied by the probabilities, not
the probabilities themselves (Dorningos and Pazzani, 1997).
</footnote>
<page confidence="0.991946">
311
</page>
<table confidence="0.9995245">
Name Vocabulary size Ling Spain Accuracy
Bernoulli 3900 99.88 88.57 98.00
mv-MI 1050 99.34 96.47 98.86
mn-MI 200 98.92 93.76 98.06
dmn-MI 500 99.21 16.84 85.52
if-MI 1200 99.34 96.05 98.79
dtf-MI 200 99.09 95.43 98.48
W-MI 4550 99.30 96.26 98.79
</table>
<tableCaption confidence="0.854312">
Table 2: Precision/recall breakeven points for Ling-Spam. Rows printed in italic show the point of
maximum accuracy in cases where precision and recall were different for all vocabulary sizes. Values
that are no more than 0.5% below the highest value in a column are printed in bold.
</tableCaption>
<table confidence="0.999832625">
Name Vocabulary size Legit Spam Accuracy
Bernoulli 4800 98.54 92.52 95.91
mv-MI 4450 97.41 96.67 97.09
inn-MI 900 96.44 95.43 96.00
dmn-MI 4400 99.51 92.52 96.45
if-MI 2400 97.73 97.09 97.45
dtf-MI 1000 96.60 95.63 96.18
W-MI 2600 97.57 97.30 97.45
</table>
<tableCaption confidence="0.999797">
Table 3: Precision/recall breakeven points for PUL
</tableCaption>
<figure confidence="0.678211333333333">
Ling-Spam corpus
/ -
I / -
tf-MI
- 1 tftf-MI
Multi-variate Bernoulli
I&amp;quot;I
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Number of attributes
</figure>
<figureCaption confidence="0.9991435">
Figure 1: Ling recall in the Ling-Spam corpus for different feature ranking functions and at different
vocabulary sizes. mv-MI, if-Mi and tftf-MI use the multinomial event model.
</figureCaption>
<figure confidence="0.97269475">
cct 100
99.8
99.6
99.4
99.2
99
98.8
98.6
312
Ling-Spam corpus
100
I I I I
95 _ ,
5-
85 — - - - -
tftf-MI
Multi-variate Bernoulli
80 —
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Number of attributes
</figure>
<figureCaption confidence="0.870027">
Figure 2: Spam recall in the Ling-Spam corpus at different vocabulary sizes.
</figureCaption>
<figure confidence="0.993228571428571">
PU1 corpus
1 1 1
\
r
-
11 -MI
- - -
tftf-MI
\
Multi-variate Bernoulli
II Ai&apos;
IIIIIIIi
500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Number of attributes
</figure>
<figureCaption confidence="0.983354">
Figure 3: Legitimate recall in the PU1 corpus at different vocabulary sizes.
</figureCaption>
<figure confidence="0.994390692307692">
PU1 corpus
100
98
94
ct 92 —------------
12(&apos;) 90
mv-MI
88 —
86 — tftf-MI
04 — Multi-variate Bernoulli
iIIIIIII
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Number of attributes
</figure>
<figureCaption confidence="0.998998">
Figure 4: Spam recall in the PU1 corpus at different vocabulary sizes.
</figureCaption>
<figure confidence="0.978985125">
100
99
98
97
96
95 —
94
0
</figure>
<page confidence="0.998781">
313
</page>
<bodyText confidence="0.99998972">
model and a multinomial model) for a Naive
Bayes text classifier using two publicly available
E-mail corpora. We used several feature ranking
functions for feature selection in the multinomial
model that explicitly take into account the word
frequency information contained in the multino-
mial document representation. The main conclu-
sion we draw from these experiments is that the
multinomial model is less biased towards one class
and can achieve higher accuracy than the multi-
variate Bernoulli model, in particular when fre-
quency information is taken into account also in
the feature selection process.
Our plans for future work are to evaluate the
feature selection functions for the multinomial
model introduced in this paper on other corpora,
and to provide a better theoretical foundation for
these functions. Most studies on feature selection
have concentrated on the multi-variate Bernoulli
model (Yang and Pedersen, 1997). We believe that
the information contained in the multinomial doc-
ument representation has been neglected in previ-
ous studies, and that the development of feature
selection functions especially for the multinomial
model could improve its performance.
</bodyText>
<sectionHeader confidence="0.999256" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999867493333333">
Ion Androutsopoulos, John Koutsias, Konstantinos V.
Chandrinos, and Constantine D. Spyropoulos.
2000a. An experimental comparison of Naive
Bayesian and keyword-based anti-spam filtering
with personal e-mail messages. In N. J. Belkin,
P. Inwersen, and M.-K. Leong, editors, Proc. 23rd
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR 2000), pages
160-167, Athens, Greece.
Ion Androutsopoulos, Georgios Paliouras, Vangelis
Karkaletsis, Georgios Sakkis, Constantine D. Spy-
ropoulos, and Panagiotis Stamatopoulos. 2000b.
Learning to filter spam e-mail: A comparison of
a Naive Bayesian and a memory-based approach.
In H. Zaragoza, P. Gallinari, and M. Rajman, edi-
tors, Proc. Workshop on Machine Learning and Tex-
tual Infbrmation Access, 4th European Conference
on Principles and Practice of Knowledge Discov-
ery in Databases (PKDD 2000), pages 1-13, Lyon,
France.
Xavier Carreras and Lillis Marquez. 2001. Boosting
trees for anti-spam email filtering. In Proc. Inter-
national Conference on Recent Advances in Natural
Language Processing (RANLP-01), Tzigov Chark,
Bulgaria.
William W. Cohen. 1996. Learning rules that classify
e-mail. In Papers from the AAAI Spring Symposium
on Machine Learning in Information Access, pages
18-25, Stanford, CA. AAAI Press.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of InfOrmation Theory. John Wiley, New York.
Pedro Domingos and Michael Pazzani. 1997. On the
optimality of the simple bayesian classifier under
zero-one loss. Machine Learning, 29:103-130.
Harris Drucker, Donghui Wu, and Vladimir N. Vapnik.
1999. Support vector machines for spam categoriza-
tion. IEEE Trans. on Neural Networks, l0(5):1048-
1054.
David D. Lewis. 1998. Naive (Bayes) at forty:
The independence assumption in information re-
trieval. In Proc. 10th European Conference on Ma-
chine Learning (ECML98), volume 1398 of Lecture
Notes in Computer Science, pages 4-15, Heidelberg.
Springer.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text clas-
sification. In Proc. AAAI-98 Workshop on Learning
for Text Categorization, pages 41-48. AAAI Press.
Dunja Mladeni6 and Marko Grobelnik. 1999. Feature
selection for unbalanced class distribution and Naive
Bayes. In I. Bratko and S. Dzeroski, editors, Proc.
16th International Conference on Machine Learn-
ing (ICML-99), pages 258-267, San Francisco, CA.
Morgan Kaufmann Publishers.
Jason D. M. Rennie. 2000. ifile: An application of
machine learning to e-mail filtering. In Proc. KDD-
2000 Workshop on Text Mining, Boston, MA.
Mehran Sahami, Susan Dumais, David Heckerman,
and Eric Horvitz. 1998. A bayesian approach to fil-
tering junk e-mail. In Learning for Text Categoriza-
tion: Papers from the AAAI Workshop, pages 55-62,
Madison Wisconsin. AAAI Press. Technical Report
WS-98-05.
Georgios Sakkis, Ion Androutsopoulos, Georgios
Paliouras, Vangelis Karkaletsis, Constantine D. Spy-
ropoulos, and Panagiotis Stamatopoulos. 2001.
Stacking classifiers for anti-spam filtering of e-mail.
In L. Lee and D. Harman, editors, Proc. 6th Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2001), pages 44-50, Pitts-
burgh, PA. Carnegie Mellon University.
Yiming Yang and Jan 0. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proc. 14th International Conference on Machine
Learning (ICML-97), pages 412-420.
</reference>
<page confidence="0.99913">
314
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797800">
<title confidence="0.9984015">A Comparison of Event Models for Naive Bayes Anti-Spam E-Mail Filtering</title>
<author confidence="0.998622">Karl-Michael Schneider</author>
<affiliation confidence="0.9977555">University of Passau Department of General Linguistics</affiliation>
<address confidence="0.944936">Innstr. 40, D-94032 Passau</address>
<abstract confidence="0.991009">We describe experiments with a Naive Bayes text classifier in the context of antispam E-mail filtering, using two different statistical event models: a multi-variate Bernoulli model and a multinomial model. We introduce a family of feature ranking functions for feature selection in the multinomial event model that take account of the word frequency information. We present evaluation results on two publicly available corpora of legitimate and spam E-mails. We find that the multinomial model is less biased towards one class and achieves slightly higher accuracy than the multi-variate Bernoulli model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Ion Androutsopoulos</author>
<author>John Koutsias</author>
<author>Konstantinos V Chandrinos</author>
<author>Constantine D Spyropoulos</author>
</authors>
<title>An experimental comparison of Naive Bayesian and keyword-based anti-spam filtering with personal e-mail messages. In</title>
<date>2000</date>
<booktitle>Proc. 23rd ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2000),</booktitle>
<pages>160--167</pages>
<editor>N. J. Belkin, P. Inwersen, and M.-K. Leong, editors,</editor>
<location>Athens, Greece.</location>
<contexts>
<context position="1682" citStr="Androutsopoulos et al., 2000" startWordPosition="251" endWordPosition="254">lications. One particular application of text categorization is anti-spam E-mail filtering, where the goal is to block unsolicited messages with commercial or pornographic content (UCE, spam) from a user&apos;s E-mail stream, while letting other (legitimate) messages pass. Here, the task is to assign a message to one of two categories, legitimate and spam, based on the message&apos;s content. In recent years, a growing body of research has applied machine learning techniques to text categorization and (anti-spam) E-mail filtering, including rule learning (Cohen, 1996), Naive Bayes (Sahami et al., 1998; Androutsopoulos et al., 2000b; Rennie, 2000), memory based learning (Androutsopoulos et al., 2000b), decision trees (Carreras and Marquez, 2001), support vector machines (Drucker et al., 1999) or combinations of different learners (Sakkis et al., 2001). In these approaches a classifier is learned from training data rather than constructed by hand, which results in better and more robust classifiers. The Naive Bayes classifier has been found particularly attractive for the task of text categorization because it performs surprisingly well in many application areas despite its simplicity (Lewis, 1998). Bayesian classifiers </context>
<context position="4976" citStr="Androutsopoulos et al., 2000" startWordPosition="793" endWordPosition="796">ed for classification. Despite the fact that the multi-variate Bernoulli model captures less information about a document (compared to the multinomial model), it performs quite well in text categorization tasks, particularly when the set of words used for classification is small. However, McCallum and Nigam (1998) have shown that the multinomial model outperforms the multi-variate Bernoulli model on larger vocabulary sizes or when the vocabulary size is chosen optimal for both models. Most text categorization approaches to antispam E-mail filtering have used the multi-variate Bernoulli model (Androutsopoulos et al., 2000b). Rennie (2000) used a multinomial model but did not compare it to the multi-variate model. Mladenia and Grobelnik (1999) used a multinomial model in a different context. In this paper we present results of experiments in which we evaluated the performance of a Naive Bayes classifier on two publicly available E-mail corpora, using both the multi-variate Bernoulli and the multinomial model. The paper is organized as follows. In Sect. 2 we describe the Naive Bayes classifier and the two generative models in more detail. In Sect. 3 we introduce feature selection methods that take into account t</context>
<context position="9752" citStr="Androutsopoulos et al. (2000" startWordPosition="1657" endWordPosition="1660">other. By making the Naive Bayes assumption, the probability of a document given a class is the multinomial distribution: Ivl (1—r P wt ci; 0)Nit ci; = P(Iciadil! 11 t=1 (7) The parameters O = P(wtIci; 0) of the mixture component cj can be estimated as the fraction of word occurrences in the training documents in ci that are wt: 6tutc.i — Pettit 3 Feature Selection 3.1 Mutual Information It is common to use only a subset of the vocabulary for classification, in order to reduce over-fitting to the training data and to speed up the classification process. Following McCallum and Nigam (1998) and Androutsopoulos et al. (2000b), we ranked the words according to their average mutual information with the class variable and selected the N highest ranked words. Average mutual information between a word wt and the class variable, denoted by M/(C; W1), is the difference between the entropy of the class variable, H(C), and the entropy of the class variable given the information about the word, H(CI Wt) (Cover and Thomas, 1991). Intuitively, M/(C: VV) measures how much bandwidth can be saved in the transmission of a class value when the information about the word is known. In the multi-variate Bernoulli model, Wt is a ran</context>
<context position="13500" citStr="Androutsopoulos et al., 2000" startWordPosition="2326" endWordPosition="2330">function. Table 1 lists the feature ranking functions that we used in our experiments. mn-MI is the average mutual information where the probabilities are estimated as in the multinomial model. dmnMI differs from mn-MI in that the class prior probabilities are estimated as the fraction of documents in each class, rather than the fraction of word occurrences. YLMI, dtf-MI and tftf-MI use mean term frequency to measure the correlation between tut and cj and use different weighting functions. 4 Experiments 4.1 Corpora We performed experiments on two publicly available E-mail corpora:2 Ling-Spam (Androutsopoulos et al., 2000b) and PU1 (Androutsopoulos et al., 2000a). We trained a Naive Bayes classifier with a multi-variate Bernoulli model and a multinomial model on each of the two datasets. The Ling-Spam corpus consists of 2412 messages from the Linguist list3 and 481 spam messages. Thus spam messages are 16.6% of the corpus. Attachments, HTML tags and all E-mail headers except the Subject line have been stripped off. We used the lemmatized version of the corpus, with the tokenization given in the corpus and with no additional processing, stop list, etc. The total vocabulary size is 59829 words. The PU1 corpus co</context>
<context position="15435" citStr="Androutsopoulos et al., 2000" startWordPosition="2679" endWordPosition="2683"> P (wt) Er 1 P(ck)(N(ck, wt)IN(ck)) if-MI N (c1, wt) mt.f(ci ,wt) N (ci , Int) 11)1 P (cj , t) — Iv, mtfewt) d(c) Net) Els ! LL N (w s) dtf-MI N (c wi) d(c) intf(c wi) N (c. w I) 11&apos;1 = P(wdej)P(c.i) mtf(wt) d(c) N (wt) N(j) 11)1 tftf-MI N (c.i , wt) mtf(ej , wt) N (ci , t) 1)1 mff(cj , t)= mtf(wt) d(c) N (wt) d(ci) Table 1: Feature ranking functions for the multinomial event model (see text). ditional processing. The total vocabulary size is 21706 words. Both corpora are divided into 10 parts of equal size, with equal proportion of legitimate and spam messages across the 10 parts. Following (Androutsopoulos et al., 2000b), we used 10-fold crossvalidation in all experiments, using nine parts for training and the remaining part for testing, with a different test set in each trial. The evaluation measures were then averaged across the 10 iterations. We performed experiments on each of the corpora, using the multi-variate Bernoulli model with mv-M/, as well as the multinomial model with myMI and the feature ranking functions in Table 1, and varying the number of selected words from 50 to 5000 by 50. 4.2 Results For each event model and feature ranking function, we determined the minimum number of words with high</context>
<context position="17302" citStr="Androutsopoulos et al. (2000" startWordPosition="2997" endWordPosition="3000">ti-variate Bernoulli model favors the Ling resp. Legit classes over the Spam classes, whereas the multinomial model is more balanced in conjunction with mv-MI, if-MI and tftf-MI. This may be due to the relatively specific vocabulary used especially in the Ling-Spam corpus, and to the uneven distribution of the documents in the classes. Second, the multinomial model achieves higher accuracy than the multivariate Bernoulli model. If-Mi even achieves high accuracy at a comparatively small vocabulary size (1200 and 2400 words, respectively). In general, PU1 seems to be more difficult to classify. Androutsopoulos et al. (2000b) used cost-sensitive evaluation metrics to account for the fact that it may be more serious an error when a legitimate message is classified as spam than vice versa. However, such cost-sensitive measures are problematic with a Naive Bayes classifier because the probabilities computed by Naive Bayes are not reliable, due to the independence assumptions it makes. Therefore we did not use cost-sensitive measures.4 5 Conclusions We performed experiments with two different statistical event models (a multi-variate Bernoulli 4Despite this, Naive Bayes can be an optimal classifier because it uses o</context>
</contexts>
<marker>Androutsopoulos, Koutsias, Chandrinos, Spyropoulos, 2000</marker>
<rawString>Ion Androutsopoulos, John Koutsias, Konstantinos V. Chandrinos, and Constantine D. Spyropoulos. 2000a. An experimental comparison of Naive Bayesian and keyword-based anti-spam filtering with personal e-mail messages. In N. J. Belkin, P. Inwersen, and M.-K. Leong, editors, Proc. 23rd ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2000), pages 160-167, Athens, Greece.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ion Androutsopoulos</author>
</authors>
<title>Georgios Paliouras, Vangelis Karkaletsis, Georgios Sakkis, Constantine D. Spyropoulos, and Panagiotis Stamatopoulos. 2000b. Learning to filter spam e-mail: A comparison of a Naive Bayesian and a memory-based approach.</title>
<booktitle>Proc. Workshop on Machine Learning and Textual Infbrmation Access, 4th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD 2000),</booktitle>
<pages>1--13</pages>
<editor>In H. Zaragoza, P. Gallinari, and M. Rajman, editors,</editor>
<location>Lyon, France.</location>
<marker>Androutsopoulos, </marker>
<rawString>Ion Androutsopoulos, Georgios Paliouras, Vangelis Karkaletsis, Georgios Sakkis, Constantine D. Spyropoulos, and Panagiotis Stamatopoulos. 2000b. Learning to filter spam e-mail: A comparison of a Naive Bayesian and a memory-based approach. In H. Zaragoza, P. Gallinari, and M. Rajman, editors, Proc. Workshop on Machine Learning and Textual Infbrmation Access, 4th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD 2000), pages 1-13, Lyon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lillis Marquez</author>
</authors>
<title>Boosting trees for anti-spam email filtering.</title>
<date>2001</date>
<booktitle>In Proc. International Conference on Recent Advances in Natural Language Processing (RANLP-01), Tzigov Chark,</booktitle>
<contexts>
<context position="1798" citStr="Carreras and Marquez, 2001" startWordPosition="266" endWordPosition="269">k unsolicited messages with commercial or pornographic content (UCE, spam) from a user&apos;s E-mail stream, while letting other (legitimate) messages pass. Here, the task is to assign a message to one of two categories, legitimate and spam, based on the message&apos;s content. In recent years, a growing body of research has applied machine learning techniques to text categorization and (anti-spam) E-mail filtering, including rule learning (Cohen, 1996), Naive Bayes (Sahami et al., 1998; Androutsopoulos et al., 2000b; Rennie, 2000), memory based learning (Androutsopoulos et al., 2000b), decision trees (Carreras and Marquez, 2001), support vector machines (Drucker et al., 1999) or combinations of different learners (Sakkis et al., 2001). In these approaches a classifier is learned from training data rather than constructed by hand, which results in better and more robust classifiers. The Naive Bayes classifier has been found particularly attractive for the task of text categorization because it performs surprisingly well in many application areas despite its simplicity (Lewis, 1998). Bayesian classifiers are based on a probabilistic model of text generation. A text is generated by first choosing a class according to so</context>
</contexts>
<marker>Carreras, Marquez, 2001</marker>
<rawString>Xavier Carreras and Lillis Marquez. 2001. Boosting trees for anti-spam email filtering. In Proc. International Conference on Recent Advances in Natural Language Processing (RANLP-01), Tzigov Chark, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Learning rules that classify e-mail.</title>
<date>1996</date>
<booktitle>In Papers from the AAAI Spring Symposium on Machine Learning in Information Access,</booktitle>
<pages>18--25</pages>
<publisher>AAAI Press.</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="1618" citStr="Cohen, 1996" startWordPosition="243" endWordPosition="244">cessing (NLP) and information retrieval (IR) applications. One particular application of text categorization is anti-spam E-mail filtering, where the goal is to block unsolicited messages with commercial or pornographic content (UCE, spam) from a user&apos;s E-mail stream, while letting other (legitimate) messages pass. Here, the task is to assign a message to one of two categories, legitimate and spam, based on the message&apos;s content. In recent years, a growing body of research has applied machine learning techniques to text categorization and (anti-spam) E-mail filtering, including rule learning (Cohen, 1996), Naive Bayes (Sahami et al., 1998; Androutsopoulos et al., 2000b; Rennie, 2000), memory based learning (Androutsopoulos et al., 2000b), decision trees (Carreras and Marquez, 2001), support vector machines (Drucker et al., 1999) or combinations of different learners (Sakkis et al., 2001). In these approaches a classifier is learned from training data rather than constructed by hand, which results in better and more robust classifiers. The Naive Bayes classifier has been found particularly attractive for the task of text categorization because it performs surprisingly well in many application a</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>William W. Cohen. 1996. Learning rules that classify e-mail. In Papers from the AAAI Spring Symposium on Machine Learning in Information Access, pages 18-25, Stanford, CA. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of InfOrmation Theory.</title>
<date>1991</date>
<publisher>John Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="10154" citStr="Cover and Thomas, 1991" startWordPosition="1728" endWordPosition="1731">use only a subset of the vocabulary for classification, in order to reduce over-fitting to the training data and to speed up the classification process. Following McCallum and Nigam (1998) and Androutsopoulos et al. (2000b), we ranked the words according to their average mutual information with the class variable and selected the N highest ranked words. Average mutual information between a word wt and the class variable, denoted by M/(C; W1), is the difference between the entropy of the class variable, H(C), and the entropy of the class variable given the information about the word, H(CI Wt) (Cover and Thomas, 1991). Intuitively, M/(C: VV) measures how much bandwidth can be saved in the transmission of a class value when the information about the word is known. In the multi-variate Bernoulli model, Wt is a random variable that takes on values ft E {0, 1}, indicating whether word wt occurs in a document or not. Thus M/(C; Wt) is the average mutual information between C and the absence or presence of tot in a document: p(c, ft) MI(C;Wt) — E E P(c, ft) log P(e)P(ft) cec itc{0,1} (9) (6) P(di c-3 • NisP(Ci (8) 309 3.2 Feature Selection in the Multinomial used feature ranking functions of the form in (10): Mo</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements of InfOrmation Theory. John Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Michael Pazzani</author>
</authors>
<title>On the optimality of the simple bayesian classifier under zero-one loss.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--103</pages>
<marker>Domingos, Pazzani, 1997</marker>
<rawString>Pedro Domingos and Michael Pazzani. 1997. On the optimality of the simple bayesian classifier under zero-one loss. Machine Learning, 29:103-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harris Drucker</author>
<author>Donghui Wu</author>
<author>Vladimir N Vapnik</author>
</authors>
<title>Support vector machines for spam categorization.</title>
<date>1999</date>
<journal>IEEE Trans. on Neural Networks,</journal>
<pages>0--5</pages>
<contexts>
<context position="1846" citStr="Drucker et al., 1999" startWordPosition="274" endWordPosition="277"> content (UCE, spam) from a user&apos;s E-mail stream, while letting other (legitimate) messages pass. Here, the task is to assign a message to one of two categories, legitimate and spam, based on the message&apos;s content. In recent years, a growing body of research has applied machine learning techniques to text categorization and (anti-spam) E-mail filtering, including rule learning (Cohen, 1996), Naive Bayes (Sahami et al., 1998; Androutsopoulos et al., 2000b; Rennie, 2000), memory based learning (Androutsopoulos et al., 2000b), decision trees (Carreras and Marquez, 2001), support vector machines (Drucker et al., 1999) or combinations of different learners (Sakkis et al., 2001). In these approaches a classifier is learned from training data rather than constructed by hand, which results in better and more robust classifiers. The Naive Bayes classifier has been found particularly attractive for the task of text categorization because it performs surprisingly well in many application areas despite its simplicity (Lewis, 1998). Bayesian classifiers are based on a probabilistic model of text generation. A text is generated by first choosing a class according to some prior probability and then generating a text </context>
</contexts>
<marker>Drucker, Wu, Vapnik, 1999</marker>
<rawString>Harris Drucker, Donghui Wu, and Vladimir N. Vapnik. 1999. Support vector machines for spam categorization. IEEE Trans. on Neural Networks, l0(5):1048-1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Naive (Bayes) at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>In Proc. 10th European Conference on Machine Learning (ECML98),</booktitle>
<volume>1398</volume>
<pages>4--15</pages>
<publisher>Springer.</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="2259" citStr="Lewis, 1998" startWordPosition="340" endWordPosition="341"> 1998; Androutsopoulos et al., 2000b; Rennie, 2000), memory based learning (Androutsopoulos et al., 2000b), decision trees (Carreras and Marquez, 2001), support vector machines (Drucker et al., 1999) or combinations of different learners (Sakkis et al., 2001). In these approaches a classifier is learned from training data rather than constructed by hand, which results in better and more robust classifiers. The Naive Bayes classifier has been found particularly attractive for the task of text categorization because it performs surprisingly well in many application areas despite its simplicity (Lewis, 1998). Bayesian classifiers are based on a probabilistic model of text generation. A text is generated by first choosing a class according to some prior probability and then generating a text according to a class-specific distribution. The model parameters are estimated from training examples that have been annotated with their correct class. Given a new document, the classifier outputs the class which is most likely to have generated the document. From a linguistic point of view, a document is made up of words, and the semantics of the document is determined by the meaning of the words and the lin</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Naive (Bayes) at forty: The independence assumption in information retrieval. In Proc. 10th European Conference on Machine Learning (ECML98), volume 1398 of Lecture Notes in Computer Science, pages 4-15, Heidelberg. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for Naive Bayes text classification.</title>
<date>1998</date>
<booktitle>In Proc. AAAI-98 Workshop on Learning for Text Categorization,</booktitle>
<pages>41--48</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="3487" citStr="McCallum and Nigam, 1998" startWordPosition="543" endWordPosition="546">stic structure of the document. The Naive Bayesian classifier makes the simplifying assumption that the probability that a document is generated in some class depends only on the probabilities of the words given the context of the class, and that the words in a document are independent of each other. This is called the Naive Bayes assumption. The generative model underlying the Naive Bayes classifier can be characterized with respect to the amount of information it captures about the 307 words in a document. In information retrieval and text categorization, two types of models have been used (McCallum and Nigam, 1998). Both assume that there is a fixed vocabulary. In the first model, a document is generated by first choosing a subset of the vocabulary and then using the selected words any number of times, at least once, in any order. This model is called multi-variate Bernoulli model. It captures the information of which words are used in a document, but not the number of times each words is used, nor the order of the words in the document. In the second model, a document is generated by choosing a set of word occurrences and arranging them in any order. This model is called multinomial model. In addition </context>
<context position="5858" citStr="McCallum and Nigam (1998)" startWordPosition="943" endWordPosition="946">e of a Naive Bayes classifier on two publicly available E-mail corpora, using both the multi-variate Bernoulli and the multinomial model. The paper is organized as follows. In Sect. 2 we describe the Naive Bayes classifier and the two generative models in more detail. In Sect. 3 we introduce feature selection methods that take into account the extra information contained in the multinomial model. In Sect. 4 we describe our experiments and discuss the results. Finally, in Sect. 5 we draw some conclusions. 2 Naive Bayes Classifier We follow the description of the Naive Bayes classifier given in McCallum and Nigam (1998). A Bayesian classifier assumes that a document is generated by a mixture model with parameters 0, consisting of components C = {ci, cm} that correspond to the classes. A document is generated by first selecting a component c3 E C according to the prior distribution P(c318) and then choosing a document di according to the parameters of c3 with distribution P(dilc3; 0). The likelihood of a document is given by the total probability 0) = p(ei j=1 0)P(diici; 0) (1) Of course, the true parameters 0 of the mixture model are not known. Therefore, one estimates the parameters from labeled training do</context>
<context position="8631" citStr="McCallum and Nigam (1998)" startWordPosition="1454" endWordPosition="1457"> from the probabilities of the words given the class: I v P(di Icj ; 0) = H(BitP(wt e;9)+ (1 — Bit)(1 — P(wtIci; 0))) (5) Note that words which do not occur in di contribute to the probability of di as well. The parameters 0 = P(wt, ci; 0) of the mixture component ci can be estimated as the fraction of training documents in cj that contain w1:1 BitP(ci di) 6tvtlet P(wtIcj; 0) = P(cd) 2.2 Multinomial Model The multinomial event model assumes that a document di of length di is generated by a sequence of I d7 word events, where the outcome of each event is a word from the vocabulary V. Following McCallum and Nigam (1998), we assume that the document length distribution P(Id, I) does not depend on the class. Thus a document di can be represented as a vector of length I V , where each dimension t of the vector, denoted as Nit &gt; 0, is the &apos;McCallum and Nigam (1998) suggest to use a Laplacean prior to smooth the probabilities, but we found that this degraded the performance of the classifier. number of times word wt occurs in cit. The Naive Bayes assumption assumes that the dI trials are independent of each other. By making the Naive Bayes assumption, the probability of a document given a class is the multinomial</context>
<context position="10973" citStr="McCallum and Nigam, 1998" startWordPosition="1875" endWordPosition="1879"> random variable that takes on values ft E {0, 1}, indicating whether word wt occurs in a document or not. Thus M/(C; Wt) is the average mutual information between C and the absence or presence of tot in a document: p(c, ft) MI(C;Wt) — E E P(c, ft) log P(e)P(ft) cec itc{0,1} (9) (6) P(di c-3 • NisP(Ci (8) 309 3.2 Feature Selection in the Multinomial used feature ranking functions of the form in (10): Model Mutual information as in (9) has also been used for feature selection in the multinomial model, either by estimating the probabilities P(e, ft), P(e) and P(h) as in the multi-variate model (McCallum and Nigam, 1998) or by using the multinomial probabilities (Mladenie and Grobelnik, 1999). Let us call the two versions mv-M/ and mn-MI, respectively. mn-MI is not fully adequate as a feature ranking function for the multinomial model. For example, the token Subject: appears in every document in the two corpora we used in our experiments exactly once, and thus is completely uninformative. mv-M/ assigns 0 to this token, but mn-MI yields a positive value because the average document length in the classes is different, and thus the class-conditional probabilities of the token are different across classes in the </context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In Proc. AAAI-98 Workshop on Learning for Text Categorization, pages 41-48. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dunja Mladeni6</author>
<author>Marko Grobelnik</author>
</authors>
<title>Feature selection for unbalanced class distribution and Naive Bayes.</title>
<date>1999</date>
<booktitle>Proc. 16th International Conference on Machine Learning (ICML-99),</booktitle>
<pages>258--267</pages>
<editor>In I. Bratko and S. Dzeroski, editors,</editor>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco, CA.</location>
<marker>Mladeni6, Grobelnik, 1999</marker>
<rawString>Dunja Mladeni6 and Marko Grobelnik. 1999. Feature selection for unbalanced class distribution and Naive Bayes. In I. Bratko and S. Dzeroski, editors, Proc. 16th International Conference on Machine Learning (ICML-99), pages 258-267, San Francisco, CA. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D M Rennie</author>
</authors>
<title>ifile: An application of machine learning to e-mail filtering.</title>
<date>2000</date>
<booktitle>In Proc. KDD2000 Workshop on Text Mining,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1698" citStr="Rennie, 2000" startWordPosition="255" endWordPosition="256">cation of text categorization is anti-spam E-mail filtering, where the goal is to block unsolicited messages with commercial or pornographic content (UCE, spam) from a user&apos;s E-mail stream, while letting other (legitimate) messages pass. Here, the task is to assign a message to one of two categories, legitimate and spam, based on the message&apos;s content. In recent years, a growing body of research has applied machine learning techniques to text categorization and (anti-spam) E-mail filtering, including rule learning (Cohen, 1996), Naive Bayes (Sahami et al., 1998; Androutsopoulos et al., 2000b; Rennie, 2000), memory based learning (Androutsopoulos et al., 2000b), decision trees (Carreras and Marquez, 2001), support vector machines (Drucker et al., 1999) or combinations of different learners (Sakkis et al., 2001). In these approaches a classifier is learned from training data rather than constructed by hand, which results in better and more robust classifiers. The Naive Bayes classifier has been found particularly attractive for the task of text categorization because it performs surprisingly well in many application areas despite its simplicity (Lewis, 1998). Bayesian classifiers are based on a p</context>
<context position="4993" citStr="Rennie (2000)" startWordPosition="797" endWordPosition="798">he fact that the multi-variate Bernoulli model captures less information about a document (compared to the multinomial model), it performs quite well in text categorization tasks, particularly when the set of words used for classification is small. However, McCallum and Nigam (1998) have shown that the multinomial model outperforms the multi-variate Bernoulli model on larger vocabulary sizes or when the vocabulary size is chosen optimal for both models. Most text categorization approaches to antispam E-mail filtering have used the multi-variate Bernoulli model (Androutsopoulos et al., 2000b). Rennie (2000) used a multinomial model but did not compare it to the multi-variate model. Mladenia and Grobelnik (1999) used a multinomial model in a different context. In this paper we present results of experiments in which we evaluated the performance of a Naive Bayes classifier on two publicly available E-mail corpora, using both the multi-variate Bernoulli and the multinomial model. The paper is organized as follows. In Sect. 2 we describe the Naive Bayes classifier and the two generative models in more detail. In Sect. 3 we introduce feature selection methods that take into account the extra informat</context>
</contexts>
<marker>Rennie, 2000</marker>
<rawString>Jason D. M. Rennie. 2000. ifile: An application of machine learning to e-mail filtering. In Proc. KDD2000 Workshop on Text Mining, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Susan Dumais</author>
<author>David Heckerman</author>
<author>Eric Horvitz</author>
</authors>
<title>A bayesian approach to filtering junk e-mail.</title>
<date>1998</date>
<booktitle>In Learning for Text Categorization: Papers from the AAAI Workshop,</booktitle>
<tech>Technical Report WS-98-05.</tech>
<pages>55--62</pages>
<publisher>AAAI Press.</publisher>
<location>Madison Wisconsin.</location>
<contexts>
<context position="1652" citStr="Sahami et al., 1998" startWordPosition="247" endWordPosition="250">on retrieval (IR) applications. One particular application of text categorization is anti-spam E-mail filtering, where the goal is to block unsolicited messages with commercial or pornographic content (UCE, spam) from a user&apos;s E-mail stream, while letting other (legitimate) messages pass. Here, the task is to assign a message to one of two categories, legitimate and spam, based on the message&apos;s content. In recent years, a growing body of research has applied machine learning techniques to text categorization and (anti-spam) E-mail filtering, including rule learning (Cohen, 1996), Naive Bayes (Sahami et al., 1998; Androutsopoulos et al., 2000b; Rennie, 2000), memory based learning (Androutsopoulos et al., 2000b), decision trees (Carreras and Marquez, 2001), support vector machines (Drucker et al., 1999) or combinations of different learners (Sakkis et al., 2001). In these approaches a classifier is learned from training data rather than constructed by hand, which results in better and more robust classifiers. The Naive Bayes classifier has been found particularly attractive for the task of text categorization because it performs surprisingly well in many application areas despite its simplicity (Lewis</context>
</contexts>
<marker>Sahami, Dumais, Heckerman, Horvitz, 1998</marker>
<rawString>Mehran Sahami, Susan Dumais, David Heckerman, and Eric Horvitz. 1998. A bayesian approach to filtering junk e-mail. In Learning for Text Categorization: Papers from the AAAI Workshop, pages 55-62, Madison Wisconsin. AAAI Press. Technical Report WS-98-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgios Sakkis</author>
</authors>
<title>Ion Androutsopoulos, Georgios Paliouras, Vangelis Karkaletsis, Constantine</title>
<date>2001</date>
<booktitle>Proc. 6th Conference on Empirical Methods in Natural Language Processing (EMNLP 2001),</booktitle>
<pages>44--50</pages>
<editor>In L. Lee and D. Harman, editors,</editor>
<publisher>Carnegie Mellon University.</publisher>
<location>Pittsburgh, PA.</location>
<marker>Sakkis, 2001</marker>
<rawString>Georgios Sakkis, Ion Androutsopoulos, Georgios Paliouras, Vangelis Karkaletsis, Constantine D. Spyropoulos, and Panagiotis Stamatopoulos. 2001. Stacking classifiers for anti-spam filtering of e-mail. In L. Lee and D. Harman, editors, Proc. 6th Conference on Empirical Methods in Natural Language Processing (EMNLP 2001), pages 44-50, Pittsburgh, PA. Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proc. 14th International Conference on Machine Learning (ICML-97),</booktitle>
<pages>412--420</pages>
<marker>Yang, Jan, 1997</marker>
<rawString>Yiming Yang and Jan 0. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proc. 14th International Conference on Machine Learning (ICML-97), pages 412-420.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>