<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000963">
<title confidence="0.996792">
Grouping Language Model Boundary Words to Speed K–Best Extraction
from Hypergraphs
</title>
<author confidence="0.99755">
Kenneth Heafield*,† Philipp Koehn* Alon Lavie†
</author>
<affiliation confidence="0.9881305">
* School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.996083">
10 Crichton Street
Edinburgh EH8 9AB, UK
</address>
<email confidence="0.996701">
pkoehn@inf.ed.ac.uk
</email>
<affiliation confidence="0.78106">
† Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.770695">
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
</address>
<email confidence="0.999413">
{heafield,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982941176471">
We propose a new algorithm to approximately
extract top-scoring hypotheses from a hyper-
graph when the score includes an N–gram
language model. In the popular cube prun-
ing algorithm, every hypothesis is annotated
with boundary words and permitted to recom-
bine only if all boundary words are equal.
However, many hypotheses share some, but
not all, boundary words. We use these com-
mon boundary words to group hypotheses and
do so recursively, resulting in a tree of hy-
potheses. This tree forms the basis for our
new search algorithm that iteratively refines
groups of boundary words on demand. Ma-
chine translation experiments show our algo-
rithm makes translation 1.50 to 3.51 times as
fast as with cube pruning in common cases.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995530315789474">
This work presents a new algorithm to search a
packed data structure for high-scoring hypothe-
ses when the score includes an N–gram language
model. Many natural language processing systems
have this sort of problem e.g. hypergraph search
in hierarchical and syntactic machine translation
(Mi et al., 2008; Klein and Manning, 2001), lat-
tice rescoring in speech recognition, and confusion
network decoding in optical character recognition
(Tong and Evans, 1996). Large language models
have been shown to improve quality, especially in
machine translation (Brants et al., 2007; Koehn and
Haddow, 2012). However, language models make
search computationally expensive because they ex-
amine surface words without regard to the structure
at North Korea
in North Korea
with North Korea
with the DPRK
</bodyText>
<figureCaption confidence="0.9993965">
Figure 1: Hypotheses are grouped by common prefixes
and suffixes.
</figureCaption>
<bodyText confidence="0.9994146">
of the packed search space. Prior work, including
cube pruning (Chiang, 2007), has largely treated the
language model as a black box. Our new search
algorithm groups hypotheses by common prefixes
and suffixes, exploiting the tendency of the language
model to score these hypotheses similarly. An exam-
ple is shown in Figure 1. The result is a substantial
improvement over the time-accuracy trade-off pre-
sented by cube pruning.
The search spaces mentioned in the previous para-
graph are special cases of a directed acyclic hyper-
graph. As used here, the difference from a nor-
mal graph is that an edge can go from one vertex
to any number of vertices; this number is the arity
of the edge. Lattices and confusion networks are
hypergraphs in which every edge happens to have
arity one. We experiment with parsing-based ma-
chine translation, where edges represent grammar
rules that may have any number of non-terminals,
including zero.
Hypotheses are paths in the hypergraph scored by
a linear combination of features. Many features are
additive: they can be expressed as weights on edges
that sum to form hypothesis features. However, log
probability from an N–gram language model is non-
</bodyText>
<figure confidence="0.9785178">
�with the DPRK
in
at
}
North Korea
</figure>
<page confidence="0.94422">
958
</page>
<note confidence="0.4660755">
Proceedings of NAACL-HLT 2013, pages 958–968,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999906366666667">
additive because it examines surface strings across
edge and vertex boundaries. Non-additivity makes
search difficult because locally optimal hypotheses
may not be globally optimal.
In order to properly compute the language model
score, each hypothesis is annotated with its bound-
ary words, collectively referred to as its state (Li
and Khudanpur, 2008). Hypotheses with equal state
may be recombined, so a straightforward dynamic
programming approach (Bar-Hillel et al., 1964) sim-
ply treats state as an additional dimension in the dy-
namic programming table. However, this approach
quickly becomes intractable for large language mod-
els where the number of states is too large.
Beam search (Chiang, 2005; Lowerre, 1976) ap-
proximates the straightforward algorithm by remem-
bering a beam of up to k hypotheses1 in each vertex.
It visits each vertex in bottom-up order, each time
calling a beam filling algorithm to select k hypothe-
ses. The parameter k is a time-accuracy trade-off:
larger k increases both CPU time and accuracy.
We contribute a new beam filling algorithm that
improves the time-accuracy trade-off over the popu-
lar cube pruning algorithm (Chiang, 2007) discussed
in §2.3. The algorithm is based on the observation
that competing hypotheses come from the same im-
put, so their language model states are often similar.
Grouping hypotheses by these similar words enables
our algorithm to reason over multiple hypotheses at
once. The algorithm is fully described in §3.
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999704">
2.1 Alternatives to Bottom-Up Search
</subsectionHeader>
<bodyText confidence="0.9997804">
Beam search visits each vertex in the hypergraph
in bottom-up (topological) order. The hypergraph
can also be searched in left-to-right order (Watanabe
et al., 2006; Huang and Mi, 2010). Alternatively,
hypotheses can be generated on demand with cube
growing (Huang and Chiang, 2007), though we note
that it showed little improvement in Moses (Xu and
Koehn, 2012). All of these options are compatible
with our algorithm. However, we only experiment
with bottom-up beam search.
</bodyText>
<footnote confidence="0.9932195">
1We use K to denote the number of fully-formed hypotheses
requested by the user and k to denote beam size.
</footnote>
<subsectionHeader confidence="0.993263">
2.2 Exhaustive Beam Filling
</subsectionHeader>
<bodyText confidence="0.999828272727273">
Originally, beam search was used with an exhaustive
beam filling algorithm (Chiang, 2005). It generates
every possible hypothesis (subject to the beams in
previous vertices), selects the top k by score, and
discards the remaining hypotheses. This is expen-
sive: just one edge of arity a encodes O(1 + ak)
hypotheses and each edge is evaluated exhaustively.
In the worst case, our algorithm is exhaustive and
generates the same number of hypotheses as beam
search; in practice, we are concerned with the aver-
age case.
</bodyText>
<subsectionHeader confidence="0.99796">
2.3 Baseline: Cube Pruning
</subsectionHeader>
<bodyText confidence="0.999953129032258">
Cube pruning (Chiang, 2007) is a fast approximate
beam filling algorithm and our baseline. It chooses
k hypotheses by popping them off the top of a prior-
ity queue. Initially, the queue is populated with hy-
potheses made from the best (highest-scoring) parts.
These parts are an edge and a hypothesis from each
vertex referenced by the edge. When a hypothesis
is popped, several next-best alternatives are pushed.
These alternatives substitute the next-best edge or a
next-best hypothesis from one of the vertices.
Our work follows a similar pattern of popping one
queue entry then pushing multiple entries. However,
our queue entries are a group of hypotheses while
cube pruning’s entries are a single hypothesis.
Hypotheses are usually fully scored before being
placed in the priority queue. An alternative priori-
tizes hypotheses by their additive score. The addi-
tive score is the edge’s score plus the score of each
component hypothesis, ignoring the non-additive as-
pect of the language model. When the additive score
is used, the language model is only called k times,
once for each hypothesis popped from the queue.
Cube pruning can produce duplicate queue en-
tries. Gesmundo and Henderson (2010) modified the
algorithm prevent duplicates instead of using a hash
table. We include their work in the experiments.
Hopkins and Langmead (2009) characterized
cube pruning as A* search (Hart et al., 1968) with an
inadmissible heuristic. Their analysis showed deep
and unbalanced search trees. Our work can be inter-
preted as a partial rebalancing of these search trees.
</bodyText>
<page confidence="0.997479">
959
</page>
<subsectionHeader confidence="0.985906">
2.4 Exact Algorithms
</subsectionHeader>
<bodyText confidence="0.999988321428572">
A number of exact search algorithms have been de-
veloped. We are not aware of an exact algorithm that
tractably scales to the size of hypergraphs and lan-
guage models used in many modern machine trans-
lation systems (Callison-Burch et al., 2012).
The hypergraph and language model can be com-
piled into an integer linear program. The best hy-
pothesis can then be recovered by taking the dual
and solving by Lagrangian relaxation (Rush and
Collins, 2011). However, that work only dealt with
language models up to order three.
Iglesias et al. (2011) represent the search space
as a recursive transition network and the language
model as a weighted finite state transducer. Using
standard finite state algorithms, they intersect the
two automatons then exactly search for the highest-
scoring paths. However, the intersected automaton
is too large. The authors suggested removing low
probability entries from the language model, but this
form of pruning negatively impacts translation qual-
ity (Moore and Quirk, 2009; Chelba et al., 2010).
Their work bears some similarity to our algorithm
in that partially overlapping state will be collapsed
and efficiently handled together. However, the key
advatage to our approach is that groups have a score
that can be used for pruning before the group is ex-
panded, enabling pruning without first constructing
the intersected automaton.
</bodyText>
<subsectionHeader confidence="0.994986">
2.5 Coarse-to-Fine
</subsectionHeader>
<bodyText confidence="0.999861583333333">
Coarse-to-fine (Petrov et al., 2008) performs mul-
tiple pruning passes, each time with more detail.
Search is a subroutine of coarse-to-fine and our work
is inside search, so the two are compatible. There are
several forms of coarse-to-fine search; the closest to
our work increases the language model order each
iteration. However, by operating inside search, our
algorithm is able to handle hypotheses at different
levels of refinement and use scores to choose where
to further refine hypotheses. Coarse-to-fine decod-
ing cannot do this because it determines the level of
refinement before calling search.
</bodyText>
<sectionHeader confidence="0.98292" genericHeader="method">
3 Our New Beam Filling Algorithm
</sectionHeader>
<bodyText confidence="0.999888">
In our algorithm, the primary idea is to group hy-
potheses with similar language model state. The
following sections formalize what these groups are
(partial state), that the groups have a recursive struc-
ture (state tree), how groups are split (bread crumbs),
using groups with hypergraph edges (partial edge),
prioritizing search (scoring) and best-first search
(priority queue).
</bodyText>
<subsectionHeader confidence="0.999669">
3.1 Partial State
</subsectionHeader>
<bodyText confidence="0.999937814814815">
An N–gram language model (with order N) com-
putes the probability of a word given the N − 1 pre-
ceding words. The left state of a hypothesis is the
first N − 1 words, which have insufficient context
to be scored. Right state is the last N − 1 words;
these might become context for another hypothesis.
Collectively, they are known as state. State mini-
mization (Li and Khudanpur, 2008) may reduce the
size of state due to backoff in the language model.
For example, the hypothesis “the few nations that
have diplomatic relations with North Korea” might
have left state “the few” and right state “Korea”
after state minimization determined that “North”
could be elided. Collectively, the state is denoted
(the few -1 o �- Korea). The diamond o is a stand-in
for elided words. Terminators -1 and �- indicate when
left and right state are exhausted, respectively2.
Our algorithm is based on partial state. Par-
tial state is simply state with more inner words
elided. For example, (the o Korea) is a partial state
for (the few -1 o �- Korea). Terminators -1 and �- can
be elided just like words. Empty state is denoted
using the customary symbol for empty string, c. For
example, (c o c) is the empty partial state. The termi-
nators serve to distinguish a completed state (which
may be short due to state minimization) from an in-
complete partial state.
</bodyText>
<subsectionHeader confidence="0.999941">
3.2 State Tree
</subsectionHeader>
<bodyText confidence="0.894415">
States (the few -1 o �- Korea) and (the -1 o �- Korea)
have words in common, so the partial state
(the o Korea) can be used to reason over both of
them. Generalizing this notion to the set of hypothe-
ses in a beam, we build a state tree. The root of
the tree is the empty partial state (c o c) that reasons
2A corner case arises for hypotheses with less than N − 1
words. For these hypotheses, we still attempt state minimiza-
tion and, if successful, the state is treated normally. If state
minimization fails, a flag is set in the state. For purposes of the
state tree, the flag acts like a different terminator symbol.
</bodyText>
<page confidence="0.993442">
960
</page>
<figureCaption confidence="0.9619765">
Figure 2: A state tree containing five states: (the few a o ` Korea), (the a o ` Korea), (some a o ` DPRK),
(a a o ` in Korea), and (a a o ` Korea). Nodes of the tree are partial states. The branching order is the first word,
the last word, the second word, and so on. If the left or right state is exhausted, then branching continues with the
remaining state. For purposes of branching, termination symbols a and ` act like normal words.
</figureCaption>
<figure confidence="0.99964948">
(the few o Korea) (the few o ` Korea) (the few a o ` Korea)
(the o e) (the o Korea)
(the a o Korea) (the a o ` Korea)
(a a o in Korea) (a a o ` in Korea)
(a o e) (a o Korea) (a a o Korea) (a a o ` Korea)
(some o e) (some o DPRK) (some a o DPRK) (some a o ` DPRK)
(e o e)
(the few a � ` Korea)
(the few a
(the few a o ` Korea)
(the o Korea)
(the a o ` Korea)
(the o Korea)[0+]
(the
(the a o ` Korea)
(the a
(e o e)[1+]
(e o e)
(some a o ` DPRK)
(some a o ` DPRK)
(a a o ` in Korea)
(a a o ` in Korea)
(a a o Korea) (a a o Korea)
(a a o ` Korea)
(a a o ` Korea)
</figure>
<figureCaption confidence="0.979570166666667">
Figure 3: The optimized version of Figure 2. Nodes
immediately reveal the longest shared prefix and suffix
among hypotheses below them.
Figure 4: Visiting the root node partitions the tree into
best child (the o Korea)[0+] and bread crumb (e o e)[1+].
The data structure remains intact for use elsewhere.
</figureCaption>
<bodyText confidence="0.999947324324324">
over all hypotheses. From the root, the tree branches
by the first word of state, the last word, the second
word, the second-to-last word, and so on. If left or
right state is exhausted, then branching continues us-
ing the remaining state. The branching order priori-
tizes the outermost words because these can be used
to update the language model probability. The deci-
sion to start with left state is arbitrary. An example
tree is shown in Figure 2.
As an optimization, each node determines the
longest shared prefix and suffix of the hypotheses
below it. The node reports these words immedi-
ately, rendering some other nodes redundant. This
makes our algorithm faster because it will then only
encounter nodes when there is a branching decision
to be made. The original tree is shown in Figure 2
and the optimized version is shown in Figure 3. As
a side effect of branching by left state first, the al-
gorithm did not notice that states (the o Korea) and
(a a o Korea) both end with Korea. We designed the
tree building algorithm for speed and plan to exper-
iment with alternatives as future work.
The state tree is built lazily. A node initially holds
a flat array of all the hypotheses below it. When its
children are first needed, the hypotheses are grouped
by the branching word and an array of child nodes
is built. In turn, these newly created children each
initially hold an array of hypotheses. CPU time is
saved because nodes containing low-scoring nodes
may never construct their children.
Each node has a score. For leaves, this score is
copied from the underlying hypothesis (or best hy-
pothesis if some other feature prevented recombina-
tion). The score of an internal node is the maximum
score of its children. As an example, the root node’s
score is the same as the highest-scoring hypothesis
in the tree. Children are sorted by score.
</bodyText>
<page confidence="0.987783">
961
</page>
<subsectionHeader confidence="0.997892">
3.3 Bread Crumbs
</subsectionHeader>
<bodyText confidence="0.999971636363636">
The state tree is explored in a best-first manner.
Specifically, when the algorithm visits a node, it
considers that node’s best child. The best child re-
veals more words, so the score may go up or down
when the language model is consulted. Therefore,
simply following best children may lead to a poor
hypothesis. Some backtracking mechanism is re-
quired, for which we use bread crumbs. Visiting a
node results in two items: the best child and a bread
crumb. The bread crumb encodes the node that was
visited and how many children have already been
considered. Figure 4 shows an example.
More formally, each node has an array of chil-
dren sorted by score, so it suffices for the bread
crumb to keep an index in this array. An in-
dex of zero denotes that no child has been vis-
ited. Continuing the example from Figure 3,
(E o E)[0+] denotes the root partial state with chil-
dren starting at index 0 (i.e. all of them). Visit-
ing (E o E)[0+] yields best child (the o Korea)[0+]
and bread crumb (E o E)[1+]. Later, the search al-
gorithm may return to (E o E)[1+], yielding best
child (some -1 o �- DPRK)[0+] and bread crumb
(E o E)[2+]. If there is no remaining sibling, visit-
ing yields only the best child.
The index serves to restrict the array of children
to those with that index or above. Formally, let d
map from a node or bread crumb to the set of leaves
descended from it. The descendants of a node n are
those of its children
where LJ takes the union of disjoint sets and n[i] is
the ith child. In a bread crumb with index c, only de-
scendents by the remaining children are considered
</bodyText>
<subsectionHeader confidence="0.961496">
3.4 Partial Edge
</subsectionHeader>
<bodyText confidence="0.999989272727273">
The beam filling algorithm is tasked with selecting
hypotheses given a number of hypergraph edges.
Hypergraph edges are strings comprised of words
and references to vertices (in parsing, terminals and
non-terminals). A hypergraph edge is converted to a
partial edge by replacing each vertex reference with
the root node from that vertex. For example, the hy-
pergraph edge “is v .” referencing vertex v becomes
partial edge “is (E o E)[0+] .”
Partial edges allow our algorithm to reason over
a large set of hypotheses at once. Visiting a
partial edge divides that set into two as follows.
A heuristic chooses one of the non-leaf nodes to
visit. Currently, this heuristic picks the node with
the fewest words revealed. As a tie breaker, it
chooses the leftmost node. The chosen node is
visited (partitioned), yielding the best child and
bread crumb as described in the previous section.
These are substituted into separate copies of the par-
tial edge. Continuing our example with the vertex
shown in Figure 3, “is (E o E)[0+] .” partitions into
“is (the o Korea)[0+] .” and “is (E o E)[1+] .”
</bodyText>
<subsectionHeader confidence="0.916433">
3.5 Scoring
</subsectionHeader>
<bodyText confidence="0.999778636363637">
Every partial edge has a score that determines its
search priority. Initially, this score is the sum of the
edge’s score and the scores of each bread crumb (de-
fined below). As words are revealed, the score is
updated to account for new language model context.
Each edge score includes a log language model
probability and possibly additive features. When-
ever there is insufficient context to compute the lan-
guage model probability of a word, an estimate r is
used. For example, edge “is v .” incorporates esti-
mate
</bodyText>
<equation confidence="0.913643">
log r(is)r(.)
</equation>
<bodyText confidence="0.54705">
into its score. The same applies to hypotheses:
(the few -1 o �- Korea) includes estimate
</bodyText>
<equation confidence="0.992839666666667">
d(n) = |n|−1 d(n[i])
H
Z=0
</equation>
<bodyText confidence="0.9952585">
It follows that the set of descendants is partitioned
into two disjoint sets
</bodyText>
<equation confidence="0.915111333333333">
�
d(n[c+]) = d(n[c]) d(n[c + 1+])
log r(the)r(few  |the)
</equation>
<bodyText confidence="0.999783166666667">
because the words in left state are those with insuf-
ficient context.
In common practice (Chiang, 2007; Hoang et al.,
2009; Dyer et al., 2010), the estimate is taken from
the language model: r = p. However, querying
the language model with incomplete context leads
</bodyText>
<figure confidence="0.653720333333333">
d(n[c+]) = |n|−1 d(n[i])
H
Z=c
</figure>
<page confidence="0.984978">
962
</page>
<bodyText confidence="0.99963">
Kneser-Ney smoothing (Kneser and Ney, 1995) to
assume that backoff has occurred. An alternative is
to use average-case rest costs explicitly stored in the
language model (Heafield et al., 2012). Both options
are used in the experiments3.
The score of a bread crumb is the maximum score
of its descendants as defined in §3.3. For example,
the bread crumb (E o E)[1+] has a lower score than
(E o E)[0+] because the best child (the o Korea)[0+]
and its descendants no longer contribute to the max-
imum.
The score of partial edge “is (E o E)[0+] .” is
the sum of scores from its two parts: edge
“is v .” and bread crumb (E o E)[0+]. The
edge’s score includes estimated log probability
log r(is)r(.) as explained earlier. The bread crumb’s
score comes from its highest-scoring descendent
(the few -1 o �- Korea) and therefore includes esti-
mate log r(the)r(few  |the).
Estimates are updated as words are revealed.
Continuing the example, “is (E o E)[0+] .” has best
child “is (the o Korea)[0+] .” In this best child, the
estimate r(.) is updated to r(.  |Korea). Similarly,
r(the) is replaced with r(the  |is). Updates exam-
ine only words that have been revealed: r(few  |the)
remains unrevised.
Updates are computed efficiently by using point-
ers (Heafield et al., 2011) with KenLM. To summa-
rize, the language model computes
</bodyText>
<equation confidence="0.91007875">
r(wn|r(wn|wn−1
1 )
wn−1
i )
</equation>
<bodyText confidence="0.9857753125">
in a single call. In the popular reverse trie data struc-
ture, the language model visits wni while retrieving
wn1 , so the cost is the same as a single query. More-
over, when the language model earlier provided es-
timate r(wn|wn−1
i ), it also returned a data-structure
pointer t(wni ). Pointers are retained in hypotheses,
edges, and partial edges for each word with an esti-
mated probability. When context is revealed, our al-
gorithm queries the language model with new con-
text wi−1
1 and pointer t(wni ). The language model
uses this pointer to immediately retrieve denomina-
tor r(wn|wn−1
i ) and as a starting point to retrieve nu-
merator r(wn|wn−1
</bodyText>
<footnote confidence="0.9056905">
1 ). It can therefore avoid looking
3We also tested upper bounds (Huang et al., 2012; Carter et
al., 2012) but the result is still approximate due to beam pruning
and initial experiments showed degraded performance.
</footnote>
<equation confidence="0.8368125">
up r(wn), r(wn|wn−1), ... , r(wn|wn−1
i+1 ) as would
</equation>
<bodyText confidence="0.96296">
normally be required with a reverse trie.
</bodyText>
<subsectionHeader confidence="0.995914">
3.6 Priority Queue
</subsectionHeader>
<bodyText confidence="0.999980190476191">
Our beam filling algorithm is controlled by a priority
queue containing partial edges. The queue is popu-
lated by converting all outgoing hypergraph edges
into partial edges and pushing them onto the queue.
After this initialization, the algorithm loops. Each
iteration begins by popping the top-scoring partial
edge off the queue. If all nodes are leaves, then the
partial edge is converted to a hypothesis and placed
in the beam. Otherwise, the partial edge is parti-
tioned as described in §3.3. The two resulting partial
edges are pushed onto the queue. Looping continues
with the next iteration until the queue is empty or the
beam is full. After the loop terminates, the beam is
given to the root node of the state tree; other nodes
will be built lazily as described in §3.2.
Overall, the algorithm visits hypergraph vertices
in bottom-up order. Our beam filling algorithm runs
in each vertex, making use of state trees in vertices
below. The top of the tree contains full hypotheses.
If a K-best list is desired, packing and extraction
works the same way as with cube pruning.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998652">
Performance is measured by translating the 3003-
sentence German-English test set from the 2011
Workshop on Machine Translation (Callison-Burch
et al., 2011). Two translation models were built, one
hierarchical (Chiang, 2007) and one with target syn-
tax. The target-syntax system is based on English
parses from the Collins (1999) parser. Both were
trained on Europarl (Koehn, 2005). The language
model interpolates models built on Europarl, news
commentary, and news data provided by the evalua-
tion. Interpolation weights were tuned on the 2010
test set. Language models were built with SRILM
(Stolcke, 2002), modified Kneser-Ney smoothing
(Kneser and Ney, 1995; Chen and Goodman, 1998),
default pruning, and order 5. Feature weights were
tuned with MERT (Och, 2003), beam size 1000,
100-best output, and cube pruning. Systems were
built with the Moses (Hoang et al., 2009) pipeline.
Measurements were collected by running the de-
coder on all 3003 sentences. For consistency, all
</bodyText>
<page confidence="0.996639">
963
</page>
<figure confidence="0.9943075">
0 1 2
CPU seconds/sentence
</figure>
<figureCaption confidence="0.996417333333333">
Figure 5: Hierarchial system in Moses with our algo-
rithm, cube pruning with additive scores, and cube prun-
ing with full scores (§2.3). The two baselines overlap.
</figureCaption>
<figure confidence="0.9926025">
0 1 2
CPU seconds/sentence
</figure>
<figureCaption confidence="0.990139333333333">
Figure 6: Hierarchical system in cdec with our algorithm,
similarly-performing variants of cube pruning defined in
Gesmundo and Henderson (2010), and the default.
</figureCaption>
<figure confidence="0.983650230769231">
This work
Additive cube pruning
Cube pruning
This work
Gesmundo 1
Gesmundo 2
Cube pruning
Average model score -101.4
-101.5
-101.6
Average model score -101.4
-101.5
-101.6
</figure>
<bodyText confidence="0.999824684210526">
relevant files were forced into the operating system
disk cache before each run. CPU time is the to-
tal user and system time taken by the decoder mi-
nus loading time. Loading time was measured by
running the decoder with empty input. In partic-
ular, CPU time includes the cost of parsing. Our
test system has 32 cores and 64 GB of RAM; no
run came close to running out of memory. While
multi-threaded experiments showed improvements
as well, we only report single-threaded results to re-
duce noise and to compare with cdec (Dyer et al.,
2010). Decoders were compiled with the optimiza-
tion settings suggested in their documentation.
Search accuracy is measured by average model
score; higher is better. Only relative comparisons
are meaningful because model scores have arbitrary
scale and include constant factors. Beam sizes start
at 5 and rise until a time limit determined by running
the slowest algorithm with beam size 1000.
</bodyText>
<subsectionHeader confidence="0.998088">
4.1 Comparison Inside Moses
</subsectionHeader>
<bodyText confidence="0.999883708333333">
Figure 5 shows Moses performance with this work
and with cube pruning. These results used the hi-
erarchical system with common-practice estimates
(§3.5). The two cube pruning variants are explained
in §2.3. Briefly, the queue can be prioritized using
additive or full scores. Performance with additive
scores is roughly the same as using full scores with
half the beam size.
Our algorithm is faster for every beam size tested.
It is also more accurate than additive cube pruning
with the same beam size. However, when compared
with full scores cube pruning, it is less accurate for
beam sizes below 300. This makes sense because
our algorithm starts with additive estimates and iter-
atively refines them by calling the language model.
Moreover, when beams are small, there are fewer
chances to group hypotheses. With beams larger
than 300, our algorithm can group more hypotheses,
overtaking both forms of cube pruning.
Accuracy improvements can be interpreted as
speed improvements by asking how much time each
algorithm takes to achieve a set level of accuracy.
By this metric, our algorithm is 2.04 to 3.37 times as
fast as both baselines.
</bodyText>
<subsectionHeader confidence="0.998795">
4.2 Comparison Inside cdec
</subsectionHeader>
<bodyText confidence="0.99993525">
We also implemented our algorithm in cdec (Dyer
et al., 2010). Figure 6 compares with two enhanced
versions of cube pruning (Gesmundo and Hender-
son, 2010) and the cdec baseline. The model scores
</bodyText>
<page confidence="0.994734">
964
</page>
<figure confidence="0.992784333333333">
-101.4
Average model score
-101.5
-101.6
22
Uncased BLEU
21.8
21.6
21.4
0 1 2
CPU seconds/sentence
0 1 2
CPU seconds/sentence
Rest+This work
This work
Rest+Cube pruning
Cube pruning
Rest+This work
This work
Rest+Cube pruning
Cube pruning
</figure>
<figureCaption confidence="0.999994">
Figure 7: Effect of rest costs on our algorithm and on cube pruning in Moses. Noisy BLEU scores reflect model errors.
</figureCaption>
<bodyText confidence="0.996432333333333">
are comparable with Moses4.
Measuring at equal accuracy, our algorithm
makes cdec 1.56 to 2.24 times as fast as the best
baseline. At first, this seems to suggest that cdec is
faster. In fact, the opposite is true: comparing Fig-
ures 5 and 6 reveals that cdec has a higher parsing
cost than Moses5, thereby biasing the speed ratio to-
wards 1. In subsequent experiments, we use Moses
because it more accurately reflects search costs.
</bodyText>
<subsectionHeader confidence="0.999887">
4.3 Average-Case Rest Costs
</subsectionHeader>
<bodyText confidence="0.991909">
Previous experiments used the common-practice
probability estimate described in §3.5. Figure 7
shows the impact of average-case rest costs on our
algorithm and on cube pruning in Moses. We also
looked at uncased BLEU (Papineni et al., 2002)
scores, finding that our algorithm attains near-peak
BLEU in less time. The relationship between model
score and BLEU is noisy due to model errors.
4The glue rule builds hypotheses left-to-right. In Moses,
glued hypotheses start with &lt;s&gt; and thus have empty left state.
In cdec, sentence boundary tokens are normally added last, so
intermediate hypotheses have spurious left state. Running cdec
with the Moses glue rule led to improved time-accuracy perfor-
mance. The improved version is used in all results reported. We
accounted for constant-factor differences in feature definition
i.e. whether &lt;s&gt; is part of the word count.
5In-memory phrase tables were used with both decoders.
The on-disk phrase table makes Moses slower than cdec.
Average-case rest costs impact our algorithm
more than they impact cube pruning. For small beam
sizes, our algorithm becomes more accurate, mostly
eliminating the disadvantage reported in §4.1. Com-
pared to the common-practice estimate with beam
size 1000, rest costs made our algorithm 1.62 times
as fast and cube pruning 1.22 times as fast.
Table 1 compares our best result with the best
baseline: our algorithm and cube pruning, both with
rest costs inside Moses. In this scenario, our algo-
rithm is 2.59 to 3.51 times as fast as cube pruning.
</bodyText>
<subsectionHeader confidence="0.99953">
4.4 Target-Syntax
</subsectionHeader>
<bodyText confidence="0.9988282">
We took the best baseline and best result from previ-
ous experiments (Moses with rest costs) and ran the
target-syntax system. Results are shown in Figure
8. Parsing and search are far more expensive. For
beam size 5, our algorithm attains equivalent accu-
racy 1.16 times as fast. Above 5, our algorithm is
1.50 to 2.00 times as fast as cube pruning. More-
over, our algorithm took less time with beam size
6900 than cube pruning took with beam size 1000.
A small bump in model score occurs around 15
seconds. This is due to translating “durchzoge-
nen” as “criss-crossed” instead of passing it through,
which incurs a severe penalty (-100). The only rule
capable of doing so translates “X durchzogenen” as
“criss-crossed PP”; a direct translation rule was not
</bodyText>
<page confidence="0.992935">
965
</page>
<figure confidence="0.9982943125">
Rest+This work
Rest+Cube pruning
Rest+This work
Rest+Cube pruning
21.4
21
-104.2
Average model score
-104.4
-104.6
-104.8
-105
Uncased BLEU
21.2
0 10 20 0 10 20
CPU seconds/sentence CPU seconds/sentence
</figure>
<figureCaption confidence="0.99999">
Figure 8: Performance of Moses with the target-syntax system.
</figureCaption>
<bodyText confidence="0.998240666666667">
extracted due to reordering. An appropriate prepo-
sitional phrase (PP) was pruned with smaller beam
sizes because it is disfluent.
</bodyText>
<subsectionHeader confidence="0.986032">
4.5 Memory
</subsectionHeader>
<bodyText confidence="0.999947714285714">
Peak virtual memory usage was measured before
each process terminated. Compared with cube prun-
ing at a beam size of 1000, our algorithm uses 160
MB more RAM in Moses and 298 MB less RAM in
cdec. The differences are smaller with lower beam
sizes and minor relative to 12-13 GB total size, most
of which is the phrase table and language model.
</bodyText>
<table confidence="0.9984328">
k Rest+This work Rest+Cube pruning
CPU Model BLEU CPU Model BLEU
5 0.068 -1.698 21.59 0.243 -1.667 21.75
10 0.076 -1.593 21.89 0.255 -1.592 21.97
50 0.125 -1.463 22.07 0.353 -1.480 22.04
75 0.157 -1.446 22.06 0.408 -1.462 22.05
100 0.176 -1.436 22.03 0.496 -1.451 22.05
500 0.589 -1.408 22.00 1.356 -1.415 22.00
750 0.861 -1.405 21.96 1.937 -1.409 21.98
1000 1.099 -1.403 21.97 2.502 -1.407 21.98
</table>
<tableCaption confidence="0.88780975">
Table 1: Numerical results from the hierarchical system
for select beam sizes k comparing our best result with the
best baseline, both in Moses with rest costs enabled. To
conserve space, model scores are shown with 100 added.
</tableCaption>
<sectionHeader confidence="0.99745" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991363636364">
We have described a new search algorithm that
achieves equivalent accuracy 1.16 to 3.51 times as
fast as cube pruning, including two implementations
and four variants. The algorithm is based on group-
ing similar language model feature states together
and dynamically expanding these groups. In do-
ing so, it exploits the language model’s ability to
estimate with incomplete information. Our imple-
mentation is available under the LGPL as a stand-
alone from http://kheafield.com/code/
and distributed with Moses and cdec.
</bodyText>
<sectionHeader confidence="0.994953" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997962076923077">
This research work was supported in part by the Na-
tional Science Foundation under grant IIS-0713402,
by a NPRP grant (NPRP 09-1140-1-177) from the
Qatar National Research Fund (a member of the
Qatar Foundation), and by computing resources pro-
vided by the NSF-sponsored XSEDE program under
grant TG-CCR110017. The statements made herein
are solely the responsibility of the authors. The re-
search leading to these results has received funding
from the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreements
287576 (CASMACAT), 287658 (EU BRIDGE),
287688 (MateCat), and 288769 (ACCEPT).
</bodyText>
<page confidence="0.997009">
966
</page>
<sectionHeader confidence="0.989861" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999477466666667">
Yehoshua Bar-Hillel, Micha Perles, and Eli Shamir.
1964. On Formal Properties of Simple Phrase Struc-
ture Grammars. Hebrew University Students’ Press.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Language
Learning, pages 858–867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22–64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10–
51, Montr´eal, Canada, June. Association for Compu-
tational Linguistics.
Simon Carter, Marc Dymetman, and Guillaume
Bouchard. 2012. Exact sampling and decoding in
high-order hidden Markov models. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1125–1134, Jeju
Island, Korea, July.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and Kneser-Ney smoothing. In Proceedings of In-
terspeech, pages 2242–2245.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, Au-
gust.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263–270, Ann Arbor, Michi-
gan, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201–228, June.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A decoder, alignment, and learning framework for
finite-state and context-free translation models. In
Proceedings of the ACL 2010 System Demonstrations,
ACLDemos ’10, pages 7–12.
Andrea Gesmundo and James Henderson. 2010. Faster
cube pruning. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT),
pages 267–274.
Peter Hart, Nils Nilsson, and Bertram Raphael. 1968. A
formal basis for the heuristic determination of mini-
mum cost paths. IEEE Transactions on Systems Sci-
ence and Cybernetics, 4(2):100–107, July.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
model state for syntactic machine translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, San Francisco, CA, USA, De-
cember.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language model rest costs and space-efficient storage.
In Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning, Jeju Is-
land, Korea.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 152–159, Tokyo, Japan.
Mark Hopkins and Greg Langmead. 2009. Cube pruning
as heuristic search. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 62–71, Singapore, August.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 273–283, Cambridge,
MA, October.
Zhiheng Huang, Yi Chang, Bo Long, Jean-Francois Cre-
spo, Anlei Dong, Sathiya Keerthi, and Su-Lin Wu.
2012. Iterative Viterbi A* algorithm for k-best se-
quential decoding. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1125–1134, Jeju Island, Korea,
July.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adri`a
de Gispert, and Michael Riley. 2011. Hierarchical
phrase-based translation representations. In Proceed-
ings of the 2011 Conference on Empirical Methods in
</reference>
<page confidence="0.976259">
967
</page>
<reference confidence="0.99976751948052">
Natural Language Processing, pages 1373–1383, Ed-
inburgh, Scotland, UK, July. Association for Compu-
tational Linguistics.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the Seventh Inter-
national Workshop on Parsing Technologies, Beijing,
China, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181–
184.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317–321,
Montr´eal, Canada, June. Association for Computa-
tional Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the Second ACL Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
10–18, Columbus, Ohio, June.
Bruce Lowerre. 1976. The Harpy Speech Recognition
System. Ph.D. thesis, Carnegie Mellon University.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192–199, Columbus, Ohio, June.
Robert C. Moore and Chris Quirk. 2009. Less is more:
Significance-based n-gram selection for smaller, better
language models. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 746–755, August.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ’03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160–167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, PA, July.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 108–116, Honolulu, HI, USA, October.
Alexander Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 72–82, Portland, Oregon, USA,
June.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901–904.
Xiang Tong and David A. Evans. 1996. A statistical
approach to automatic OCR error correction in con-
text. In Proceedings of the Fourth Workshop on Very
Large Corpora, pages 88–100, Copenhagen, Den-
mark, April.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pages 777–
784, Sydney, Australia, July.
Wenduan Xu and Philipp Koehn. 2012. Extending hiero
decoding in Moses with cube growing. The Prague
Bulletin of Mathematical Linguistics, 98:133–142.
</reference>
<page confidence="0.997505">
968
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.058657">
<title confidence="0.9857065">Language Model Boundary Words to Speed from Hypergraphs</title>
<author confidence="0.842905">Alon</author>
<affiliation confidence="0.582811">of University of</affiliation>
<address confidence="0.5438935">10 Crichton Edinburgh EH8 9AB,</address>
<email confidence="0.988255">pkoehn@inf.ed.ac.uk</email>
<author confidence="0.392588">Technologies</author>
<affiliation confidence="0.981065">Carnegie Mellon</affiliation>
<address confidence="0.997337">5000 Forbes Pittsburgh, PA 15213,</address>
<abstract confidence="0.999564444444445">We propose a new algorithm to approximately extract top-scoring hypotheses from a hyperwhen the score includes an language model. In the popular cube pruning algorithm, every hypothesis is annotated with boundary words and permitted to recombine only if all boundary words are equal. However, many hypotheses share some, but not all, boundary words. We use these common boundary words to group hypotheses and do so recursively, resulting in a tree of hypotheses. This tree forms the basis for our new search algorithm that iteratively refines groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yehoshua Bar-Hillel</author>
<author>Micha Perles</author>
<author>Eli Shamir</author>
</authors>
<title>On Formal Properties of Simple Phrase Structure Grammars. Hebrew</title>
<date>1964</date>
<publisher>University Students’ Press.</publisher>
<contexts>
<context position="3806" citStr="Bar-Hillel et al., 1964" startWordPosition="586" endWordPosition="589">} North Korea 958 Proceedings of NAACL-HLT 2013, pages 958–968, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics additive because it examines surface strings across edge and vertex boundaries. Non-additivity makes search difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a time-accuracy trade-off: larger k increases both CPU time and accuracy. We contribute a new beam filling algorithm that impr</context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Yehoshua Bar-Hillel, Micha Perles, and Eli Shamir. 1964. On Formal Properties of Simple Phrase Structure Grammars. Hebrew University Students’ Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="1696" citStr="Brants et al., 2007" startWordPosition="253" endWordPosition="256">h cube pruning in common cases. 1 Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N–gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure at North Korea in North Korea with North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, pages 858–867, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="22524" citStr="Callison-Burch et al., 2011" startWordPosition="3847" endWordPosition="3850"> the beam is full. After the loop terminates, the beam is given to the root node of the state tree; other nodes will be built lazily as described in §3.2. Overall, the algorithm visits hypergraph vertices in bottom-up order. Our beam filling algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003- sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (O</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7845" citStr="Callison-Burch et al., 2012" startWordPosition="1241" endWordPosition="1244">ed the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 959 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can then be recovered by taking the dual and solving by Lagrangian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10– 51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Carter</author>
<author>Marc Dymetman</author>
<author>Guillaume Bouchard</author>
</authors>
<title>Exact sampling and decoding in high-order hidden Markov models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1125--1134</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="21048" citStr="Carter et al., 2012" startWordPosition="3604" endWordPosition="3607">e same as a single query. Moreover, when the language model earlier provided estimate r(wn|wn−1 i ), it also returned a data-structure pointer t(wni ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context wi−1 1 and pointer t(wni ). The language model uses this pointer to immediately retrieve denominator r(wn|wn−1 i ) and as a starting point to retrieve numerator r(wn|wn−1 1 ). It can therefore avoid looking 3We also tested upper bounds (Huang et al., 2012; Carter et al., 2012) but the result is still approximate due to beam pruning and initial experiments showed degraded performance. up r(wn), r(wn|wn−1), ... , r(wn|wn−1 i+1 ) as would normally be required with a reverse trie. 3.6 Priority Queue Our beam filling algorithm is controlled by a priority queue containing partial edges. The queue is populated by converting all outgoing hypergraph edges into partial edges and pushing them onto the queue. After this initialization, the algorithm loops. Each iteration begins by popping the top-scoring partial edge off the queue. If all nodes are leaves, then the partial edg</context>
</contexts>
<marker>Carter, Dymetman, Bouchard, 2012</marker>
<rawString>Simon Carter, Marc Dymetman, and Guillaume Bouchard. 2012. Exact sampling and decoding in high-order hidden Markov models. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1125–1134, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Thorsten Brants</author>
<author>Will Neveitt</author>
<author>Peng Xu</author>
</authors>
<title>Study on interaction between entropy pruning and Kneser-Ney smoothing.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<pages>2242--2245</pages>
<contexts>
<context position="8630" citStr="Chelba et al., 2010" startWordPosition="1365" endWordPosition="1368">ngian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears some similarity to our algorithm in that partially overlapping state will be collapsed and efficiently handled together. However, the key advatage to our approach is that groups have a score that can be used for pruning before the group is expanded, enabling pruning without first constructing the intersected automaton. 2.5 Coarse-to-Fine Coarse-to-fine (Petrov et al., 2008) performs multiple pruning passes, each time with more detail. Search is a subroutine of coarse-to-fine and our work is inside search, so the two are compatible. There are several forms of coarse-to-fine s</context>
</contexts>
<marker>Chelba, Brants, Neveitt, Xu, 2010</marker>
<rawString>Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng Xu. 2010. Study on interaction between entropy pruning and Kneser-Ney smoothing. In Proceedings of Interspeech, pages 2242–2245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="23053" citStr="Chen and Goodman, 1998" startWordPosition="3928" endWordPosition="3931">n-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all 963 0 1 2 CPU seconds/sentence Figure 5: Hierarchial system in Moses with our algorithm, cube pruning with additive scores, and cube pruning with full scores (§2.3). The two baselines overlap. 0 1 2 CPU seconds/sentence Figure 6: Hierarchical system in cdec with our algorithm, similarly-performing variants</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="4031" citStr="Chiang, 2005" startWordPosition="625" endWordPosition="626">vity makes search difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a time-accuracy trade-off: larger k increases both CPU time and accuracy. We contribute a new beam filling algorithm that improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model </context>
<context position="5558" citStr="Chiang, 2005" startWordPosition="869" endWordPosition="870">hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous vertices), selects the top k by score, and discards the remaining hypotheses. This is expensive: just one edge of arity a encodes O(1 + ak) hypotheses and each edge is evaluated exhaustively. In the worst case, our algorithm is exhaustive and generates the same number of hypotheses as beam search; in practice, we are concerned with the average case. 2.3 Baseline: Cube Pruning Cube pruning (Chiang, 2007) is a fast approximate beam filling algorithm and our baseline. It chooses k hypotheses by popping them off the top of </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--201</pages>
<contexts>
<context position="2057" citStr="Chiang, 2007" startWordPosition="311" endWordPosition="312">ng, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure at North Korea in North Korea with North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces mentioned in the previous paragraph are special cases of a directed acyclic hypergraph. As used here, the difference from a normal graph is that an edge can go from one vertex to any number of vertices; this number is the arity of th</context>
<context position="4493" citStr="Chiang, 2007" startWordPosition="700" endWordPosition="701"> table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a time-accuracy trade-off: larger k increases both CPU time and accuracy. We contribute a new beam filling algorithm that improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on de</context>
<context position="6039" citStr="Chiang, 2007" startWordPosition="949" endWordPosition="950">ote beam size. 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous vertices), selects the top k by score, and discards the remaining hypotheses. This is expensive: just one edge of arity a encodes O(1 + ak) hypotheses and each edge is evaluated exhaustively. In the worst case, our algorithm is exhaustive and generates the same number of hypotheses as beam search; in practice, we are concerned with the average case. 2.3 Baseline: Cube Pruning Cube pruning (Chiang, 2007) is a fast approximate beam filling algorithm and our baseline. It chooses k hypotheses by popping them off the top of a priority queue. Initially, the queue is populated with hypotheses made from the best (highest-scoring) parts. These parts are an edge and a hypothesis from each vertex referenced by the edge. When a hypothesis is popped, several next-best alternatives are pushed. These alternatives substitute the next-best edge or a next-best hypothesis from one of the vertices. Our work follows a similar pattern of popping one queue entry then pushing multiple entries. However, our queue en</context>
<context position="18757" citStr="Chiang, 2007" startWordPosition="3204" endWordPosition="3205">ge score includes a log language model probability and possibly additive features. Whenever there is insufficient context to compute the language model probability of a word, an estimate r is used. For example, edge “is v .” incorporates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few -1 o �- Korea) includes estimate d(n) = |n|−1 d(n[i]) H Z=0 It follows that the set of descendants is partitioned into two disjoint sets � d(n[c+]) = d(n[c]) d(n[c + 1+]) log r(the)r(few |the) because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads d(n[c+]) = |n|−1 d(n[i]) H Z=c 962 Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3. The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (E o E)[1+] has a lower score than (E o E)[0+] b</context>
<context position="22592" citStr="Chiang, 2007" startWordPosition="3858" endWordPosition="3859"> the state tree; other nodes will be built lazily as described in §3.2. Overall, the algorithm visits hypergraph vertices in bottom-up order. Our beam filling algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003- sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. System</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33:201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="22696" citStr="Collins (1999)" startWordPosition="3876" endWordPosition="3877">ypergraph vertices in bottom-up order. Our beam filling algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003- sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the de</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="18797" citStr="Dyer et al., 2010" startWordPosition="3210" endWordPosition="3213">odel probability and possibly additive features. Whenever there is insufficient context to compute the language model probability of a word, an estimate r is used. For example, edge “is v .” incorporates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few -1 o �- Korea) includes estimate d(n) = |n|−1 d(n[i]) H Z=0 It follows that the set of descendants is partitioned into two disjoint sets � d(n[c+]) = d(n[c]) d(n[c + 1+]) log r(the)r(few |the) because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads d(n[c+]) = |n|−1 d(n[i]) H Z=c 962 Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3. The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (E o E)[1+] has a lower score than (E o E)[0+] because the best child (the o Korea)[0+] </context>
<context position="24439" citStr="Dyer et al., 2010" startWordPosition="4160" endWordPosition="4163">erage model score -101.4 -101.5 -101.6 Average model score -101.4 -101.5 -101.6 relevant files were forced into the operating system disk cache before each run. CPU time is the total user and system time taken by the decoder minus loading time. Loading time was measured by running the decoder with empty input. In particular, CPU time includes the cost of parsing. Our test system has 32 cores and 64 GB of RAM; no run came close to running out of memory. While multi-threaded experiments showed improvements as well, we only report single-threaded results to reduce noise and to compare with cdec (Dyer et al., 2010). Decoders were compiled with the optimization settings suggested in their documentation. Search accuracy is measured by average model score; higher is better. Only relative comparisons are meaningful because model scores have arbitrary scale and include constant factors. Beam sizes start at 5 and rise until a time limit determined by running the slowest algorithm with beam size 1000. 4.1 Comparison Inside Moses Figure 5 shows Moses performance with this work and with cube pruning. These results used the hierarchical system with common-practice estimates (§3.5). The two cube pruning variants a</context>
<context position="26084" citStr="Dyer et al., 2010" startWordPosition="4425" endWordPosition="4428">es sense because our algorithm starts with additive estimates and iteratively refines them by calling the language model. Moreover, when beams are small, there are fewer chances to group hypotheses. With beams larger than 300, our algorithm can group more hypotheses, overtaking both forms of cube pruning. Accuracy improvements can be interpreted as speed improvements by asking how much time each algorithm takes to achieve a set level of accuracy. By this metric, our algorithm is 2.04 to 3.37 times as fast as both baselines. 4.2 Comparison Inside cdec We also implemented our algorithm in cdec (Dyer et al., 2010). Figure 6 compares with two enhanced versions of cube pruning (Gesmundo and Henderson, 2010) and the cdec baseline. The model scores 964 -101.4 Average model score -101.5 -101.6 22 Uncased BLEU 21.8 21.6 21.4 0 1 2 CPU seconds/sentence 0 1 2 CPU seconds/sentence Rest+This work This work Rest+Cube pruning Cube pruning Rest+This work This work Rest+Cube pruning Cube pruning Figure 7: Effect of rest costs on our algorithm and on cube pruning in Moses. Noisy BLEU scores reflect model errors. are comparable with Moses4. Measuring at equal accuracy, our algorithm makes cdec 1.56 to 2.24 times as fa</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
</authors>
<title>Faster cube pruning.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>267--274</pages>
<contexts>
<context position="7210" citStr="Gesmundo and Henderson (2010)" startWordPosition="1137" endWordPosition="1140">entry then pushing multiple entries. However, our queue entries are a group of hypotheses while cube pruning’s entries are a single hypothesis. Hypotheses are usually fully scored before being placed in the priority queue. An alternative prioritizes hypotheses by their additive score. The additive score is the edge’s score plus the score of each component hypothesis, ignoring the non-additive aspect of the language model. When the additive score is used, the language model is only called k times, once for each hypothesis popped from the queue. Cube pruning can produce duplicate queue entries. Gesmundo and Henderson (2010) modified the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 959 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation sy</context>
<context position="23710" citStr="Gesmundo and Henderson (2010)" startWordPosition="4033" endWordPosition="4036"> 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all 963 0 1 2 CPU seconds/sentence Figure 5: Hierarchial system in Moses with our algorithm, cube pruning with additive scores, and cube pruning with full scores (§2.3). The two baselines overlap. 0 1 2 CPU seconds/sentence Figure 6: Hierarchical system in cdec with our algorithm, similarly-performing variants of cube pruning defined in Gesmundo and Henderson (2010), and the default. This work Additive cube pruning Cube pruning This work Gesmundo 1 Gesmundo 2 Cube pruning Average model score -101.4 -101.5 -101.6 Average model score -101.4 -101.5 -101.6 relevant files were forced into the operating system disk cache before each run. CPU time is the total user and system time taken by the decoder minus loading time. Loading time was measured by running the decoder with empty input. In particular, CPU time includes the cost of parsing. Our test system has 32 cores and 64 GB of RAM; no run came close to running out of memory. While multi-threaded experiments</context>
<context position="26177" citStr="Gesmundo and Henderson, 2010" startWordPosition="4439" endWordPosition="4443">nes them by calling the language model. Moreover, when beams are small, there are fewer chances to group hypotheses. With beams larger than 300, our algorithm can group more hypotheses, overtaking both forms of cube pruning. Accuracy improvements can be interpreted as speed improvements by asking how much time each algorithm takes to achieve a set level of accuracy. By this metric, our algorithm is 2.04 to 3.37 times as fast as both baselines. 4.2 Comparison Inside cdec We also implemented our algorithm in cdec (Dyer et al., 2010). Figure 6 compares with two enhanced versions of cube pruning (Gesmundo and Henderson, 2010) and the cdec baseline. The model scores 964 -101.4 Average model score -101.5 -101.6 22 Uncased BLEU 21.8 21.6 21.4 0 1 2 CPU seconds/sentence 0 1 2 CPU seconds/sentence Rest+This work This work Rest+Cube pruning Cube pruning Rest+This work This work Rest+Cube pruning Cube pruning Figure 7: Effect of rest costs on our algorithm and on cube pruning in Moses. Noisy BLEU scores reflect model errors. are comparable with Moses4. Measuring at equal accuracy, our algorithm makes cdec 1.56 to 2.24 times as fast as the best baseline. At first, this seems to suggest that cdec is faster. In fact, the op</context>
</contexts>
<marker>Gesmundo, Henderson, 2010</marker>
<rawString>Andrea Gesmundo and James Henderson. 2010. Faster cube pruning. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 267–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hart</author>
<author>Nils Nilsson</author>
<author>Bertram Raphael</author>
</authors>
<title>A formal basis for the heuristic determination of minimum cost paths.</title>
<date>1968</date>
<journal>IEEE Transactions on Systems Science and Cybernetics,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="7413" citStr="Hart et al., 1968" startWordPosition="1169" endWordPosition="1172">y queue. An alternative prioritizes hypotheses by their additive score. The additive score is the edge’s score plus the score of each component hypothesis, ignoring the non-additive aspect of the language model. When the additive score is used, the language model is only called k times, once for each hypothesis popped from the queue. Cube pruning can produce duplicate queue entries. Gesmundo and Henderson (2010) modified the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 959 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can then be recovered by taking the dual and solving by Lagrangi</context>
</contexts>
<marker>Hart, Nilsson, Raphael, 1968</marker>
<rawString>Peter Hart, Nils Nilsson, and Bertram Raphael. 1968. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2):100–107, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Tetsuo Kiso</author>
<author>Marcello Federico</author>
</authors>
<title>Left language model state for syntactic machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="20213" citStr="Heafield et al., 2011" startWordPosition="3455" endWordPosition="3458">’s score includes estimated log probability log r(is)r(.) as explained earlier. The bread crumb’s score comes from its highest-scoring descendent (the few -1 o �- Korea) and therefore includes estimate log r(the)r(few |the). Estimates are updated as words are revealed. Continuing the example, “is (E o E)[0+] .” has best child “is (the o Korea)[0+] .” In this best child, the estimate r(.) is updated to r(. |Korea). Similarly, r(the) is replaced with r(the |is). Updates examine only words that have been revealed: r(few |the) remains unrevised. Updates are computed efficiently by using pointers (Heafield et al., 2011) with KenLM. To summarize, the language model computes r(wn|r(wn|wn−1 1 ) wn−1 i ) in a single call. In the popular reverse trie data structure, the language model visits wni while retrieving wn1 , so the cost is the same as a single query. Moreover, when the language model earlier provided estimate r(wn|wn−1 i ), it also returned a data-structure pointer t(wni ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context wi−1 1 and pointer t(wni ). The language m</context>
</contexts>
<marker>Heafield, Hoang, Koehn, Kiso, Federico, 2011</marker>
<rawString>Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo Kiso, and Marcello Federico. 2011. Left language model state for syntactic machine translation. In Proceedings of the International Workshop on Spoken Language Translation, San Francisco, CA, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
<author>Alon Lavie</author>
</authors>
<title>Language model rest costs and space-efficient storage.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="19148" citStr="Heafield et al., 2012" startWordPosition="3267" endWordPosition="3270">follows that the set of descendants is partitioned into two disjoint sets � d(n[c+]) = d(n[c]) d(n[c + 1+]) log r(the)r(few |the) because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads d(n[c+]) = |n|−1 d(n[i]) H Z=c 962 Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3. The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (E o E)[1+] has a lower score than (E o E)[0+] because the best child (the o Korea)[0+] and its descendants no longer contribute to the maximum. The score of partial edge “is (E o E)[0+] .” is the sum of scores from its two parts: edge “is v .” and bread crumb (E o E)[0+]. The edge’s score includes estimated log probability log r(is)r(.) as explained earlier. The bread crumb’s score comes from its highest-scoring descendent (the few -1</context>
</contexts>
<marker>Heafield, Koehn, Lavie, 2012</marker>
<rawString>Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012. Language model rest costs and space-efficient storage. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Adam Lopez</author>
</authors>
<title>A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>152--159</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="18777" citStr="Hoang et al., 2009" startWordPosition="3206" endWordPosition="3209">des a log language model probability and possibly additive features. Whenever there is insufficient context to compute the language model probability of a word, an estimate r is used. For example, edge “is v .” incorporates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few -1 o �- Korea) includes estimate d(n) = |n|−1 d(n[i]) H Z=0 It follows that the set of descendants is partitioned into two disjoint sets � d(n[c+]) = d(n[c]) d(n[c + 1+]) log r(the)r(few |the) because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads d(n[c+]) = |n|−1 d(n[i]) H Z=c 962 Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3. The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (E o E)[1+] has a lower score than (E o E)[0+] because the best chil</context>
<context position="23240" citStr="Hoang et al., 2009" startWordPosition="3959" endWordPosition="3962">The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all 963 0 1 2 CPU seconds/sentence Figure 5: Hierarchial system in Moses with our algorithm, cube pruning with additive scores, and cube pruning with full scores (§2.3). The two baselines overlap. 0 1 2 CPU seconds/sentence Figure 6: Hierarchical system in cdec with our algorithm, similarly-performing variants of cube pruning defined in Gesmundo and Henderson (2010), and the default. This work Additive cube pruning Cube pruning This work Gesmundo 1 Gesmundo 2 Cube pruning Average model score -</context>
</contexts>
<marker>Hoang, Koehn, Lopez, 2009</marker>
<rawString>Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation, pages 152–159, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Greg Langmead</author>
</authors>
<title>Cube pruning as heuristic search.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>62--71</pages>
<location>Singapore,</location>
<contexts>
<context position="7353" citStr="Hopkins and Langmead (2009)" startWordPosition="1159" endWordPosition="1162">ypotheses are usually fully scored before being placed in the priority queue. An alternative prioritizes hypotheses by their additive score. The additive score is the edge’s score plus the score of each component hypothesis, ignoring the non-additive aspect of the language model. When the additive score is used, the language model is only called k times, once for each hypothesis popped from the queue. Cube pruning can produce duplicate queue entries. Gesmundo and Henderson (2010) modified the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 959 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can </context>
</contexts>
<marker>Hopkins, Langmead, 2009</marker>
<rawString>Mark Hopkins and Greg Langmead. 2009. Cube pruning as heuristic search. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 62–71, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5140" citStr="Huang and Chiang, 2007" startWordPosition="799" endWordPosition="802">algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous vertices), selects the top k by score, and discards the remaining hypotheses. This is expensive: just one e</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Haitao Mi</author>
</authors>
<title>Efficient incremental decoding for tree-to-string translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>273--283</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="5043" citStr="Huang and Mi, 2010" startWordPosition="785" endWordPosition="788">racy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous vertices),</context>
</contexts>
<marker>Huang, Mi, 2010</marker>
<rawString>Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiheng Huang</author>
<author>Yi Chang</author>
<author>Bo Long</author>
<author>Jean-Francois Crespo</author>
<author>Anlei Dong</author>
<author>Sathiya Keerthi</author>
<author>Su-Lin Wu</author>
</authors>
<title>Iterative Viterbi A* algorithm for k-best sequential decoding.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1125--1134</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="21026" citStr="Huang et al., 2012" startWordPosition="3600" endWordPosition="3603"> , so the cost is the same as a single query. Moreover, when the language model earlier provided estimate r(wn|wn−1 i ), it also returned a data-structure pointer t(wni ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context wi−1 1 and pointer t(wni ). The language model uses this pointer to immediately retrieve denominator r(wn|wn−1 i ) and as a starting point to retrieve numerator r(wn|wn−1 1 ). It can therefore avoid looking 3We also tested upper bounds (Huang et al., 2012; Carter et al., 2012) but the result is still approximate due to beam pruning and initial experiments showed degraded performance. up r(wn), r(wn|wn−1), ... , r(wn|wn−1 i+1 ) as would normally be required with a reverse trie. 3.6 Priority Queue Our beam filling algorithm is controlled by a priority queue containing partial edges. The queue is populated by converting all outgoing hypergraph edges into partial edges and pushing them onto the queue. After this initialization, the algorithm loops. Each iteration begins by popping the top-scoring partial edge off the queue. If all nodes are leaves</context>
</contexts>
<marker>Huang, Chang, Long, Crespo, Dong, Keerthi, Wu, 2012</marker>
<rawString>Zhiheng Huang, Yi Chang, Bo Long, Jean-Francois Crespo, Anlei Dong, Sathiya Keerthi, and Su-Lin Wu. 2012. Iterative Viterbi A* algorithm for k-best sequential decoding. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1125–1134, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Cyril Allauzen</author>
<author>William Byrne</author>
<author>Adri`a de Gispert</author>
<author>Michael Riley</author>
</authors>
<title>Hierarchical phrase-based translation representations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1373--1383</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK,</location>
<marker>Iglesias, Allauzen, Byrne, de Gispert, Riley, 2011</marker>
<rawString>Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adri`a de Gispert, and Michael Riley. 2011. Hierarchical phrase-based translation representations. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373–1383, Edinburgh, Scotland, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Workshop on Parsing Technologies,</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="1453" citStr="Klein and Manning, 2001" startWordPosition="218" endWordPosition="221">ting in a tree of hypotheses. This tree forms the basis for our new search algorithm that iteratively refines groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 1 Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N–gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure at North Korea in North Korea with North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. of the packed search space. Prior work, including cube pruning (Chiang, 2</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In Proceedings of the Seventh International Workshop on Parsing Technologies, Beijing, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="18998" citStr="Kneser and Ney, 1995" startWordPosition="3243" endWordPosition="3246">rates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few -1 o �- Korea) includes estimate d(n) = |n|−1 d(n[i]) H Z=0 It follows that the set of descendants is partitioned into two disjoint sets � d(n[c+]) = d(n[c]) d(n[c + 1+]) log r(the)r(few |the) because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads d(n[c+]) = |n|−1 d(n[i]) H Z=c 962 Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3. The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (E o E)[1+] has a lower score than (E o E)[0+] because the best child (the o Korea)[0+] and its descendants no longer contribute to the maximum. The score of partial edge “is (E o E)[0+] .” is the sum of scores from its two parts: edge “is v .” and bread crumb (E o E)[0+]. The edge’s scor</context>
<context position="23028" citStr="Kneser and Ney, 1995" startWordPosition="3924" endWordPosition="3927">e 3003- sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all 963 0 1 2 CPU seconds/sentence Figure 5: Hierarchial system in Moses with our algorithm, cube pruning with additive scores, and cube pruning with full scores (§2.3). The two baselines overlap. 0 1 2 CPU seconds/sentence Figure 6: Hierarchical system in cdec with our algorithm, simi</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181– 184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Towards effective use of training data in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>317--321</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1721" citStr="Koehn and Haddow, 2012" startWordPosition="257" endWordPosition="260">mon cases. 1 Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N–gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure at North Korea in North Korea with North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a</context>
</contexts>
<marker>Koehn, Haddow, 2012</marker>
<rawString>Philipp Koehn and Barry Haddow. 2012. Towards effective use of training data in statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 317–321, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="22748" citStr="Koehn, 2005" startWordPosition="3884" endWordPosition="3885">g algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003- sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all 96</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>10--18</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3682" citStr="Li and Khudanpur, 2008" startWordPosition="569" endWordPosition="572">es that sum to form hypothesis features. However, log probability from an N–gram language model is non�with the DPRK in at } North Korea 958 Proceedings of NAACL-HLT 2013, pages 958–968, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics additive because it examines surface strings across edge and vertex boundaries. Non-additivity makes search difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a </context>
<context position="10407" citStr="Li and Khudanpur, 2008" startWordPosition="1651" endWordPosition="1654"> state), that the groups have a recursive structure (state tree), how groups are split (bread crumbs), using groups with hypergraph edges (partial edge), prioritizing search (scoring) and best-first search (priority queue). 3.1 Partial State An N–gram language model (with order N) computes the probability of a word given the N − 1 preceding words. The left state of a hypothesis is the first N − 1 words, which have insufficient context to be scored. Right state is the last N − 1 words; these might become context for another hypothesis. Collectively, they are known as state. State minimization (Li and Khudanpur, 2008) may reduce the size of state due to backoff in the language model. For example, the hypothesis “the few nations that have diplomatic relations with North Korea” might have left state “the few” and right state “Korea” after state minimization determined that “North” could be elided. Collectively, the state is denoted (the few -1 o �- Korea). The diamond o is a stand-in for elided words. Terminators -1 and �- indicate when left and right state are exhausted, respectively2. Our algorithm is based on partial state. Partial state is simply state with more inner words elided. For example, (the o Ko</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 10–18, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Lowerre</author>
</authors>
<title>The Harpy Speech Recognition System.</title>
<date>1976</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="4047" citStr="Lowerre, 1976" startWordPosition="627" endWordPosition="628">rch difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a time-accuracy trade-off: larger k increases both CPU time and accuracy. We contribute a new beam filling algorithm that improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often</context>
</contexts>
<marker>Lowerre, 1976</marker>
<rawString>Bruce Lowerre. 1976. The Harpy Speech Recognition System. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>192--199</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1427" citStr="Mi et al., 2008" startWordPosition="214" endWordPosition="217">ecursively, resulting in a tree of hypotheses. This tree forms the basis for our new search algorithm that iteratively refines groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 1 Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N–gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure at North Korea in North Korea with North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. of the packed search space. Prior work, includi</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-08: HLT, pages 192–199, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Less is more: Significance-based n-gram selection for smaller, better language models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>746--755</pages>
<contexts>
<context position="8608" citStr="Moore and Quirk, 2009" startWordPosition="1361" endWordPosition="1364">al and solving by Lagrangian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears some similarity to our algorithm in that partially overlapping state will be collapsed and efficiently handled together. However, the key advatage to our approach is that groups have a score that can be used for pruning before the group is expanded, enabling pruning without first constructing the intersected automaton. 2.5 Coarse-to-Fine Coarse-to-fine (Petrov et al., 2008) performs multiple pruning passes, each time with more detail. Search is a subroutine of coarse-to-fine and our work is inside search, so the two are compatible. There are several for</context>
</contexts>
<marker>Moore, Quirk, 2009</marker>
<rawString>Robert C. Moore and Chris Quirk. 2009. Less is more: Significance-based n-gram selection for smaller, better language models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 746–755, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="23133" citStr="Och, 2003" startWordPosition="3943" endWordPosition="3944">). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all 963 0 1 2 CPU seconds/sentence Figure 5: Hierarchial system in Moses with our algorithm, cube pruning with additive scores, and cube pruning with full scores (§2.3). The two baselines overlap. 0 1 2 CPU seconds/sentence Figure 6: Hierarchical system in cdec with our algorithm, similarly-performing variants of cube pruning defined in Gesmundo and Henderson (2010), and the default. This</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evalution of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="27277" citStr="Papineni et al., 2002" startWordPosition="4624" endWordPosition="4627">cdec 1.56 to 2.24 times as fast as the best baseline. At first, this seems to suggest that cdec is faster. In fact, the opposite is true: comparing Figures 5 and 6 reveals that cdec has a higher parsing cost than Moses5, thereby biasing the speed ratio towards 1. In subsequent experiments, we use Moses because it more accurately reflects search costs. 4.3 Average-Case Rest Costs Previous experiments used the common-practice probability estimate described in §3.5. Figure 7 shows the impact of average-case rest costs on our algorithm and on cube pruning in Moses. We also looked at uncased BLEU (Papineni et al., 2002) scores, finding that our algorithm attains near-peak BLEU in less time. The relationship between model score and BLEU is noisy due to model errors. 4The glue rule builds hypotheses left-to-right. In Moses, glued hypotheses start with &lt;s&gt; and thus have empty left state. In cdec, sentence boundary tokens are normally added last, so intermediate hypotheses have spurious left state. Running cdec with the Moses glue rule led to improved time-accuracy performance. The improved version is used in all results reported. We accounted for constant-factor differences in feature definition i.e. whether &lt;s</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evalution of machine translation. In Proceedings 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>108--116</pages>
<location>Honolulu, HI, USA,</location>
<contexts>
<context position="9025" citStr="Petrov et al., 2008" startWordPosition="1425" endWordPosition="1428"> intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears some similarity to our algorithm in that partially overlapping state will be collapsed and efficiently handled together. However, the key advatage to our approach is that groups have a score that can be used for pruning before the group is expanded, enabling pruning without first constructing the intersected automaton. 2.5 Coarse-to-Fine Coarse-to-fine (Petrov et al., 2008) performs multiple pruning passes, each time with more detail. Search is a subroutine of coarse-to-fine and our work is inside search, so the two are compatible. There are several forms of coarse-to-fine search; the closest to our work increases the language model order each iteration. However, by operating inside search, our algorithm is able to handle hypotheses at different levels of refinement and use scores to choose where to further refine hypotheses. Coarse-to-fine decoding cannot do this because it determines the level of refinement before calling search. 3 Our New Beam Filling Algorit</context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108–116, Honolulu, HI, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Rush</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>72--82</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="8051" citStr="Rush and Collins, 2011" startWordPosition="1276" endWordPosition="1279">ssible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 959 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can then be recovered by taking the dual and solving by Lagrangian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears so</context>
</contexts>
<marker>Rush, Collins, 2011</marker>
<rawString>Alexander Rush and Michael Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 72–82, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="22975" citStr="Stolcke, 2002" startWordPosition="3919" endWordPosition="3920">ments Performance is measured by translating the 3003- sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all 963 0 1 2 CPU seconds/sentence Figure 5: Hierarchial system in Moses with our algorithm, cube pruning with additive scores, and cube pruning with full scores (§2.3). The two baselines overlap. 0 1 2 CPU seconds/sentence Figure 6:</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the Seventh International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Tong</author>
<author>David A Evans</author>
</authors>
<title>A statistical approach to automatic OCR error correction in context.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth Workshop on Very Large Corpora,</booktitle>
<pages>88--100</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="1582" citStr="Tong and Evans, 1996" startWordPosition="236" endWordPosition="239">ds on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 1 Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N–gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure at North Korea in North Korea with North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and su</context>
</contexts>
<marker>Tong, Evans, 1996</marker>
<rawString>Xiang Tong and David A. Evans. 1996. A statistical approach to automatic OCR error correction in context. In Proceedings of the Fourth Workshop on Very Large Corpora, pages 88–100, Copenhagen, Denmark, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase-based translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>777--784</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="5022" citStr="Watanabe et al., 2006" startWordPosition="781" endWordPosition="784"> improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams i</context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase-based translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 777– 784, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenduan Xu</author>
<author>Philipp Koehn</author>
</authors>
<title>Extending hiero decoding in Moses with cube growing.</title>
<date>2012</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>98--133</pages>
<contexts>
<context position="5220" citStr="Xu and Koehn, 2012" startWordPosition="813" endWordPosition="816">mput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous vertices), selects the top k by score, and discards the remaining hypotheses. This is expensive: just one edge of arity a encodes O(1 + ak) hypotheses and each edge is evaluated exhaustiv</context>
</contexts>
<marker>Xu, Koehn, 2012</marker>
<rawString>Wenduan Xu and Philipp Koehn. 2012. Extending hiero decoding in Moses with cube growing. The Prague Bulletin of Mathematical Linguistics, 98:133–142.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>