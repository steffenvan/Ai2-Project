<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001075">
<title confidence="0.998969">
Verb Classification using Distributional Similarity
in Syntactic and Semantic Structures
</title>
<author confidence="0.9939">
Danilo Croce
</author>
<affiliation confidence="0.996472">
University of Tor Vergata
</affiliation>
<address confidence="0.578539">
00133 Roma, Italy
</address>
<email confidence="0.949043">
croce@info.uniroma2.it
</email>
<author confidence="0.998157">
Roberto Basili
</author>
<affiliation confidence="0.999102">
University of Tor Vergata
</affiliation>
<address confidence="0.643787">
00133 Roma, Italy
</address>
<email confidence="0.995274">
basili@info.uniroma2.it
</email>
<sectionHeader confidence="0.995567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999665733333333">
In this paper, we propose innovative repre-
sentations for automatic classification of verbs
according to mainstream linguistic theories,
namely VerbNet and FrameNet. First, syntac-
tic and semantic structures capturing essential
lexical and syntactic properties of verbs are
defined. Then, we design advanced similarity
functions between such structures, i.e., seman-
tic tree kernel functions, for exploiting distri-
butional and grammatical information in Sup-
port Vector Machines. The extensive empir-
ical analysis on VerbNet class and frame de-
tection shows that our models capture mean-
ingful syntactic/semantic structures, which al-
lows for improving the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999835055555556">
Verb classification is a fundamental topic of com-
putational linguistics research given its importance
for understanding the role of verbs in conveying se-
mantics of natural language (NL). Additionally, gen-
eralization based on verb classification is central to
many NL applications, ranging from shallow seman-
tic parsing to semantic search or information extrac-
tion. Currently, a lot of interest has been paid to
two verb categorization schemes: VerbNet (Schuler,
2005) and FrameNet (Baker et al., 1998), which
has also fostered production of many automatic ap-
proaches to predicate argument extraction.
Such work has shown that syntax is necessary
for helping to predict the roles of verb arguments
and consequently their verb sense (Gildea and Juras-
fky, 2002; Pradhan et al., 2005; Gildea and Palmer,
2002). However, the definition of models for opti-
mally combining lexical and syntactic constraints is
</bodyText>
<author confidence="0.782422">
Alessandro Moschitti
</author>
<affiliation confidence="0.910278">
University of Trento
</affiliation>
<address confidence="0.452425">
38123 Povo (TN), Italy
</address>
<email confidence="0.953415">
moschitti@disi.unitn.it
</email>
<author confidence="0.9541">
Martha Palmer
</author>
<affiliation confidence="0.8621665">
University of Colorado at Boulder
Boulder, CO 80302, USA
</affiliation>
<email confidence="0.989642">
mpalmer@colorado.edu
</email>
<bodyText confidence="0.999765787878788">
still far for being accomplished. In particular, the ex-
haustive design and experimentation of lexical and
syntactic features for learning verb classification ap-
pears to be computationally problematic. For exam-
ple, the verb order can belongs to the two VerbNet
classes:
– The class 60.1, i.e., order someone to do some-
thing as shown in: The Illinois Supreme Court or-
dered the commission to audit Commonwealth Edi-
son ’s construction expenses and refund any unrea-
sonable expenses.
– The class 13.5.1: order or request something like
in: ... Michelle blabs about it to a sandwich man
while ordering lunch over the phone .
Clearly, the syntactic realization can be used to dis-
cern the cases above but it would not be enough to
correctly classify the following verb occurrence: ..
ordered the lunch to be delivered .. in Verb class
13.5.1. For such a case, selectional restrictions are
needed. These have also been shown to be use-
ful for semantic role classification (Zapirain et al.,
2010). Note that their coding in learning algorithms
is rather complex: we need to take into account syn-
tactic structures, which may require an exponential
number of syntactic features (i.e., all their possible
substructures). Moreover, these have to be enriched
with lexical information to trig lexical preference.
In this paper, we tackle the problem above
by studying innovative representations for auto-
matic verb classification according to VerbNet and
FrameNet. We define syntactic and semantic struc-
tures capturing essential lexical and syntactic prop-
erties of verbs. Then, we apply similarity between
</bodyText>
<page confidence="0.982086">
263
</page>
<note confidence="0.985722">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 263–272,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999877375">
such structures, i.e., kernel functions, which can also
exploit distributional lexical semantics, to train au-
tomatic classifiers. The basic idea of such functions
is to compute the similarity between two verbs in
terms of all the possible substructures of their syn-
tactic frames. We define and automatically extract
a lexicalized approximation of the latter. Then, we
apply kernel functions that jointly model structural
and lexical similarity so that syntactic properties are
combined with generalized lexemes. The nice prop-
erty of kernel functions is that they can be used
in place of the scalar product of feature vectors to
train algorithms such as Support Vector Machines
(SVMs). This way SVMs can learn the association
between syntactic (sub-) structures whose lexical ar-
guments are generalized and target verb classes, i.e.,
they can also learn selectional restrictions.
We carried out extensive experiments on verb
class and frame detection which showed that our
models greatly improve on the state-of-the-art (up
to about 13% of relative error reduction). Such re-
sults are nicely assessed by manually inspecting the
most important substructures used by the classifiers
as they largely correlate with syntactic frames de-
fined in VerbNet.
In the rest of the paper, Sec. 2 reports on related
work, Sec. 3 and Sec. 4 describe previous and our
models for syntactic and semantic similarity, respec-
tively, Sec. 5 illustrates our experiments, Sec. 6 dis-
cusses the output of the models in terms of error
analysis and important structures and finally Sec. 7
derives the conclusions.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99988">
Our target task is verb classification but at the same
time our models exploit distributional models as
well as structural kernels. The next three subsec-
tions report related work in such areas.
Verb Classification. The introductory verb classi-
fication example has intuitively shown the complex-
ity of defining a comprehensive feature representa-
tion. Hereafter, we report on analysis carried out in
previous work.
It has been often observed that verb senses tend
to show different selectional constraints in a specific
argument position and the above verb order is a clear
example. In the direct object position of the example
sentence for the first sense 60.1 of order, we found
commission in the role PATIENT of the predicate. It
clearly satisfies the +ANIMATE/+ORGANIZATION
restriction on the PATIENT role. This is not true
for the direct object dependency of the alternative
sense 13.5.1, which usually expresses the THEME
role, with unrestricted type selection. When prop-
erly generalized, the direct object information has
thus been shown highly predictive about verb sense
distinctions.
In (Brown et al., 2011), the so called dynamic
dependency neighborhoods (DDN), i.e., the set of
verbs that are typically collocated with a direct ob-
ject, are shown to be more helpful than lexical in-
formation (e.g., WordNet). The set of typical verbs
taking a noun n as a direct object is in fact a strong
characterization for semantic similarity, as all the
nouns m similar to n tend to collocate with the same
verbs. This is true also for other syntactic depen-
dencies, among which the direct object dependency
is possibly the strongest cue (as shown for example
in (Dligach and Palmer, 2008)).
In order to generalize the above DDN feature, dis-
tributional models are ideal, as they are designed
to model all the collocations of a given noun, ac-
cording to large scale corpus analysis. Their abil-
ity to capture lexical similarity is well established in
WSD tasks (e.g. (Schutze, 1998)), thesauri harvest-
ing (Lin, 1998), semantic role labeling (Croce et al.,
2010)) as well as information retrieval (e.g. (Furnas
et al., 1988)).
Distributional Models (DMs). These models fol-
low the distributional hypothesis (Firth, 1957) and
characterize lexical meanings in terms of context of
use, (Wittgenstein, 1953). By inducing geometrical
notions of vectors and norms through corpus analy-
sis, they provide a topological definition of seman-
tic similarity, i.e., distance in a space. DMs can
capture the similarity between words such as dele-
gation, deputation or company and commission. In
case of sense 60.1 of the verb order, DMs can be
used to suggest that the role PATIENT can be inher-
ited by all these words, as suitable Organisations.
In supervised language learning, when few exam-
ples are available, DMs support cost-effective lexi-
cal generalizations, often outperforming knowledge
based resources (such as WordNet, as in (Pantel et
al., 2007)). Obviously, the choice of the context
</bodyText>
<page confidence="0.99593">
264
</page>
<bodyText confidence="0.999845514285715">
type determines the type of targeted semantic prop-
erties. Wider contexts (e.g., entire documents) are
shown to suggest topical relations. Smaller con-
texts tend to capture more specific semantic as-
pects, e.g. the syntactic behavior, and better capture
paradigmatic relations, such as synonymy. In partic-
ular, word space models, as described in (Sahlgren,
2006), define contexts as the words appearing in a
n-sized window, centered around a target word. Co-
occurrence counts are thus collected in a words-by-
words matrix, where each element records the num-
ber of times two words co-occur within a single win-
dow of word tokens. Moreover, robust weighting
schemas are used to smooth counts against too fre-
quent co-occurrence pairs: Pointwise Mutual Infor-
mation (PMI) scores (Turney and Pantel, 2010) are
commonly adopted.
Structural Kernels. Tree and sequence kernels
have been successfully used in many NLP applica-
tions, e.g., parse reranking and adaptation, (Collins
and Duffy, 2002; Shen et al., 2003; Toutanova et
al., 2004; Kudo et al., 2005; Titov and Hender-
son, 2006), chunking and dependency parsing, e.g.,
(Kudo and Matsumoto, 2003; Daum´e III and Marcu,
2004), named entity recognition, (Cumby and Roth,
2003), text categorization, e.g., (Cancedda et al.,
2003; Gliozzo et al., 2005), and relation extraction,
e.g., (Zelenko et al., 2002; Bunescu and Mooney,
2005; Zhang et al., 2006).
Recently, DMs have been also proposed in in-
tegrated syntactic-semantic structures that feed ad-
vanced learning functions, such as the semantic
tree kernels discussed in (Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b; Mehdad et
al., 2010; Croce et al., 2011).
</bodyText>
<sectionHeader confidence="0.981638" genericHeader="method">
3 Structural Similarity Functions
</sectionHeader>
<bodyText confidence="0.999137666666667">
In this paper we model verb classifiers by exploiting
previous technology for kernel methods. In particu-
lar, we design new models for verb classification by
adopting algorithms for structural similarity, known
as Smoothed Partial Tree Kernels (SPTKs) (Croce et
al., 2011). We define new innovative structures and
similarity functions based on LSA.
The main idea of SPTK is rather simple: (i) mea-
suring the similarity between two trees in terms of
the number of shared subtrees; and (ii) such number
also includes similar fragments whose lexical nodes
are just related (so they can be different). The con-
tribution of (ii) is proportional to the lexical similar-
ity of the tree lexical nodes, where the latter can be
evaluated according to distributional models or also
lexical resources, e.g., WordNet.
In the following, we define our models based on
previous work on LSA and SPTKs.
</bodyText>
<subsectionHeader confidence="0.998955">
3.1 LSA as lexical similarity model
</subsectionHeader>
<bodyText confidence="0.986108304347826">
Robust representations can be obtained through
intelligent dimensionality reduction methods. In
LSA the original word-by-context matrix M is de-
composed through Singular Value Decomposition
(SVD) (Landauer and Dumais, 1997; Golub and Ka-
han, 1965) into the product of three new matrices:
U, S, and V so that S is diagonal and M = USVT .
M is then approximated by Mk = UkSkVkT , where
only the first k columns of U and V are used,
corresponding to the first k greatest singular val-
ues. This approximation supplies a way to project
a generic term wi into the k-dimensional space us-
ing W = UkS1/2
k , where each row corresponds to
the representation vectors iwi. The original statisti-
cal information about M is captured by the new k-
dimensional space, which preserves the global struc-
ture while removing low-variant dimensions, i.e.,
distribution noise. Given two words w1 and w2,
the term similarity function a is estimated as the
cosine similarity between the corresponding projec-
tions iw1, iw2 in the LSA space, i.e a(w1, w2) =
w1 · ~w2 This is known as Latent Semantic Ker-
</bodyText>
<subsectionHeader confidence="0.387902">
Il~w1ll I~w�JI
</subsectionHeader>
<bodyText confidence="0.9866585">
nel (L K), proposed in (Cristianini et al., 2001),
as it defines a positive semi-definite Gram matrix
G = a(w1, w2) dw1, w2 (Shawe-Taylor and Cris-
tianini, 2004). a is thus a valid kernel and can be
combined with other kernels, as discussed in the
next session.
</bodyText>
<subsectionHeader confidence="0.998251">
3.2 Tree Kernels driven by Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.9999836">
To our knowledge, two main types of tree kernels
exploit lexical similarity: the syntactic semantic tree
kernel defined in (Bloehdorn and Moschitti, 2007a)
applied to constituency trees and the smoothed
partial tree kernels (SPTKs) defined in (Croce et
al., 2011), which generalizes the former. We report
the definition of the latter as we modified it for our
purposes. SPTK computes the number of common
substructures between two trees T1 and T2 without
explicitly considering the whole fragment space. Its
</bodyText>
<page confidence="0.979321">
265
</page>
<figure confidence="0.8923294">
S
VP
NP-1 S
NN -
com
266
TARGET-order::v
to::t ROOT VBD
T
O OP
</figure>
<page confidence="0.980882">
267
</page>
<bodyText confidence="0.999926777777778">
(Baker et al., 1998). We selected the subset of
frames containing more than 100 sentences anno-
tated with a verbal predicate for a total of 62,813
sentences in 187 frames (i.e., very close to the Verb-
Net datasets). For both the datasets, we used 70% of
instances for training and 30% for testing.
Our verb (multi) classifier is designed with
the one-vs-all (Rifkin and Klautau, 2004) multi-
classification schema. This uses a set of binary
SVM classifiers, one for each verb class (frame) i.
The sentences whose verb is labeled with the class
i are positive examples for the classifier i. The sen-
tences whose verbs are compatible with the class i
but evoking a different class or labeled with none
(no current verb class applies) are added as negative
examples. In the classification phase the binary clas-
sifiers are applied by (i) only considering classes that
are compatible with the target verbs; and (ii) select-
ing the class associated with the maximum positive
SVM margin. If all classifiers provide a negative
score the example is labeled with none.
To learn the binary classifiers of the schema
above, we coded our modified SPTK in SVM-Light-
TK3 (Moschitti, 2006). The parameterization of
each classifier is carried on a held-out set (30% of
the training) and is concerned with the setting of the
trade-off parameter (option -c) and the leaf weight
(lw) (see Alg. 1), which is used to linearly scale
the contribution of the leaf nodes. In contrast, the
cost-factor parameter of SVM-Light-TK is set as the
ratio between the number of negative and positive
examples for attempting to have a balanced Preci-
sion/Recall.
Regarding SPTK setting, we used the lexical simi-
larity Q defined in Sec. 3.1. In more detail, LSA was
applied to ukWak (Baroni et al., 2009), which is a
large scale document collection made up of 2 billion
tokens. M is constructed by applying POS tagging to
build rows with pairs lemma, ::POS) (lemma::POS
in brief). The contexts of such items are the columns
of M and are short windows of size [−3,+3], cen-
tered on the items. This allows for better captur-
ing syntactic properties of words. The most frequent
20,000 items are selected along with their 20k con-
texts. The entries of M are the point-wise mutual
</bodyText>
<footnote confidence="0.944185">
3(Structural kernels in SVMLight (Joachims, 2000)) avail-
able at http://disi.unitn.it/moschitti/Tree-Kernel.htm
</footnote>
<table confidence="0.99956825">
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 83.83% 8 84.57% 8 84.46%
GRCT - 84.83% 8 85.15% 8 85.28%
LCT - 77.73% 0.1 86.03% 0.2 86.72%
Br. et Al. 84.64%
BOW 79.08%
SK 82.08%
</table>
<tableCaption confidence="0.996032">
Table 1: VerbNet accuracy with the none class
</tableCaption>
<table confidence="0.996983166666667">
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
GRCT - 92.67% 6 92.97% 0.4 93.54%
LCT - 90.28% 6 92.99% 0.3 93.78%
BOW 91.13%
SK 91.84%
</table>
<tableCaption confidence="0.999337">
Table 2: FrameNet accuracy without the none class
</tableCaption>
<bodyText confidence="0.998791">
information between them. SVD reduction is then
applied to M, with a dimensionality cut of l = 250.
For generating the CT, GRCT and LCT struc-
tures, we used the constituency trees generated by
the Charniak parser (Charniak, 2000) and the de-
pendency structures generated by the LTH syntactic
parser (described in (Johansson and Nugues, 2008)).
The classification performance is measured with
accuracy (i.e., the percentage of correct classifica-
tion). We also derive statistical significance of the
results by using the model described in (Yeh, 2000)
and implemented in (Pad´o, 2006).
</bodyText>
<subsectionHeader confidence="0.621206">
5.2 VerbNet and FrameNet Classification
</subsectionHeader>
<sectionHeader confidence="0.712993" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.994751285714286">
To assess the performance of our settings, we also
derive a simple baseline based on the bag-of-words
(BOW) model. For it, we represent an instance of
a verb in a sentence using all words of the sentence
(by creating a special feature for the predicate word).
We also used sequence kernels (SK), i.e., PTK ap-
plied to a tree composed of a fake root and only one
level of sentence words. For efficiency reasons4, we
only consider the 10 words before and after the pred-
icate with subsequence features of length up to 5.
Table 1 reports the accuracy of different mod-
els for VerbNet classification. It should be noted
that: first, SK produces a much higher accuracy than
BOW, i.e., 82.08 vs. 79.08. On one hand, this is
</bodyText>
<footnote confidence="0.991288666666667">
4The average running time of the SK is much higher than the
one of PTK. When a tree is composed by only one level PTK
collapses to SK.
</footnote>
<page confidence="0.965379">
268
</page>
<table confidence="0.996973714285714">
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 91.14% 8 91.66% 6 91.66%
GRCT - 91.71% 8 92.38% 4 92.33%
LCT - 89.20% 0.2 92.54% 0.1 92.55%
BOW 88.16%
SK 89.86%
</table>
<tableCaption confidence="0.999924">
Table 3: VerbNet accuracy without the none class
</tableCaption>
<bodyText confidence="0.999910775">
generally in contrast with standard text categoriza-
tion tasks, for which n-gram models show accuracy
comparable to the simpler BOW. On the other hand,
it simply confirms that verb classification requires
the dependency information between words (i.e., at
least the sequential structure information provided
by SK).
Second, SK is 2.56 percent points below the state-
of-the-art achieved in (Brown et al., 2011) (BR), i.e,
82.08 vs. 84.64. In contrast, STK applied to our rep-
resentation (CT, GRCT and LCT) produces compa-
rable accuracy, e.g., 84.83, confirming that syntactic
representation is needed to reach the state-of-the-art.
Third, PTK, which produces more general struc-
tures, improves over BR by almost 1.5 (statistically
significant result) when using our dependency struc-
tures GRCT and LCT. CT does not produce the same
improvement since it does not allow PTK to directly
compare the lexical structure (lexemes are all leaf
nodes in CT and to connect some pairs of them very
large trees are needed).
Finally, the best model of SPTK (i.e, using LCT)
improves over the best PTK (i.e., using LCT) by al-
most 1 point (statistically significant result): this dif-
ference is only given by lexical similarity. SPTK im-
proves on the state-of-the-art by about 2.08 absolute
percent points, which, given the high accuracy of the
baseline, corresponds to 13.5% of relative error re-
duction.
We carried out similar experiments for frame clas-
sification. One interesting difference is that SK im-
proves BOW by only 0.70, i.e., 4 times less than in
the VerbNet setting. This suggests that word order
around the predicate is more important for deriving
the VerbNet class than the FrameNet frame. Ad-
ditionally, LCT or GRCT seems to be invariant for
both PTK and SPTK whereas the lexical similarity
still produces a relevant improvement on PTK, i.e.,
13% of relative error reduction, for an absolute accu-
racy of 93.78%. The latter improves over the state-
</bodyText>
<figure confidence="0.954474">
0% 20% 40% 60% 80% 100%
Percentage of train examples
</figure>
<figureCaption confidence="0.9936635">
Figure 4: Learning curves: VerbNet accuracy with the
none Class
</figureCaption>
<bodyText confidence="0.999830947368421">
of-the-art, i.e., 92.63% derived in (Giuglea and Mos-
chitti, 2006), by using STK on CT on 133 frames.
We also carried out experiments to understand
the role of the none class. Table 3 reports on the
VerbNet classification without its instances. This is
of course an unrealistic setting as it would assume
that the current VerbNet release already includes all
senses for English verbs. In the table, we note that
the overall accuracy highly increases and the differ-
ence between models reduces. The similarities play
no role anymore. This may suggest that SPTK can
help in complex settings, where verb class character-
ization is more difficult. Another important role of
SPTK models is their ability to generalize. To test
this aspect, Figure 4 illustrates the learning curves
of SPTK with respect to BOW and the accuracy
achieved by BR (with a constant line). It is impres-
sive to note that with only 40% of the data SPTK can
reach the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.767586" genericHeader="evaluation">
6 Model Analysis and Discussion
</sectionHeader>
<bodyText confidence="0.999918615384615">
We carried out analysis of system errors and its in-
duced features. These can be examined by apply-
ing the reverse engineering tool5 proposed in (Pighin
and Moschitti, 2010; Pighin and Moschitti, 2009a;
Pighin and Moschitti, 2009b), which extracts the
most important features for the classification model.
Many mistakes are related to false positives and neg-
atives of the none class (about 72% of the errors).
This class also causes data imbalance. Most errors
are also due to lack of lexical information available
to the SPTK kernel: (i) in 30% of the errors, the
argument heads were proper nouns for which the
lexical generalization provided by the DMs was not
</bodyText>
<footnote confidence="0.648162">
5http://danielepighin.net/cms/software/flink
</footnote>
<figure confidence="0.984510290322581">
SPTK
BOW
Brown et al.
Accuracy 90%
80%
70%
60%
50%
269
VerbNet class 13.5.1
(IM(VB(target))(OBJ))
(VC(VB(target))(OBJ))
(VC(VBG(target))(OBJ))
(OPRD(TO)(IM(VB(target))(OBJ)))
(PMOD(VBG(target))(OBJ))
(VB(target))
(VC(VBN(target)))
(PRP(TO)(IM(VB(target))(OBJ)))
(IM(VB(target))(OBJ)(ADV(IN)(PMOD)))
(OPRD(TO)(IM(VB(target))(OBJ)(ADV(IN)(PMOD))))
VerbNet class 60
(VC(VB(target))(OBJ))
(NMOD(VBG(target))(OPRD))
(VC(VBN(target))(OPRD))
(NMOD(VBN(target))(OPRD))
(PMOD(VBG(target))(OBJ))
(ROOT(SBJ)(VBD(target))(OBJ)(P(,)))
(VC(VB(target))(OPRD))
(ROOT(SBJ)(VBZ(target))(OBJ)(P(,)))
(NMOD(SBJ(WDT))(VBZ(target))(OPRD))
(NMOD(SBJ)(VBZ(target))(OPRD(SBJ)(TO)(IM)))
</figure>
<tableCaption confidence="0.982835">
Table 4: GRCT fragments
</tableCaption>
<bodyText confidence="0.999318333333333">
available; and (ii) in 76% of the errors only 2 or less
argument heads are included in the extracted tree,
therefore tree kernels cannot exploit enough lexical
information to disambiguate verb senses. Addition-
ally, ambiguity characterizes errors where the sys-
tem is linguistically consistent but the learned selec-
tional preferences are not sufficient to separate verb
senses. These errors are mainly due to the lack of
contextual information. While error analysis sug-
gests that further improvement is possible (e.g. by
exploiting proper nouns), the type of generalizations
currently achieved by SPTK are rather effective. Ta-
ble 4 and 5 report the tree structures characterizing
the most informative training examples of the two
senses of the verb order, i.e. the VerbNet classes
13.5.1 (make a request for something) and 60 (give
instructions to or direct somebody to do something
with authority).
In line with the method discussed in (Pighin and
Moschitti, 2009b), these fragments are extracted as
they appear in most of the support vectors selected
during SVM training. As easily seen, the two classes
are captured by rather different patterns. The typ-
ical accusative form with an explicit direct object
emerges as characterizing the sense 13.5.1, denot-
ing the THEME role. All fragments of the sense 60
emphasize instead the sentential complement of the
verb that in fact expresses the standard PROPOSI-
TION role in VerbNet. Notice that tree fragments
correspond to syntactic patterns. The a posteriori
</bodyText>
<figure confidence="0.997396083333333">
VerbNet class 13.5.1
(VP(VB(target))(NP))
(VP(VBG(target))(NP))
(VP(VBD(target))(NP))
(VP(TO)(VP(VB(target))(NP)))
(S(NP-SBJ)(VP(VBP(target))(NP)))
VerbNet class 60
(VBN(target))
(VP(VBD(target))(S))
(VP(VBZ(target))(S))
(VBP(target))
(VP(VBD(target))(NP-1)(S(NP-SBJ)(VP)))
</figure>
<tableCaption confidence="0.981575">
Table 5: CT fragments
</tableCaption>
<bodyText confidence="0.9997797">
analysis of the learned models (i.e. the underlying
support vectors) confirm very interesting grammati-
cal generalizations, i.e. the capability of tree kernels
to implicitly trigger useful linguistic inductions for
complex semantic tasks. When SPTK are adopted,
verb arguments can be lexically generalized into
word classes, i.e., clusters of argument heads (e.g.
commission vs. delegation, or gift vs. present). Au-
tomatic generation of such classes is an interesting
direction for future research.
</bodyText>
<sectionHeader confidence="0.991017" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999829">
We have proposed new approaches to characterize
verb classes in learning algorithms. The key idea is
the use of structural representation of verbs based on
syntactic dependencies and the use of structural ker-
nels to measure similarity between such representa-
tions. The advantage of kernel methods is that they
can be directly used in some learning algorithms,
e.g., SVMs, to train verb classifiers. Very interest-
ingly, we can encode distributional lexical similar-
ity in the similarity function acting over syntactic
structures and this allows for generalizing selection
restrictions through a sort of (supervised) syntactic
and semantic co-clustering.
The verb classification results show a large im-
provement over the state-of-the-art for both Verb-
Net and FrameNet, with a relative error reduction
of about 13.5% and 16.0%, respectively. In the fu-
ture, we plan to exploit the models learned from
FrameNet and VerbNet to carry out automatic map-
ping of verbs from one theory to the other.
</bodyText>
<footnote confidence="0.983793333333333">
Acknowledgements This research is partially sup-
ported by the European Community’s Seventh Frame-
work Programme (FP7/2007-2013) under grant numbers
247758 (ETERNALS), 288024 (LIMOSINE) and 231126
(LIVINGKNOWLEDGE). Many thanks to the reviewers
for their valuable suggestions.
</footnote>
<page confidence="0.995304">
270
</page>
<sectionHeader confidence="0.98939" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999504269230769">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a collec-
tion of very large linguistically processed web-crawled
corpora. LRE, 43(3):209–226.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Gianni Amati, Claudio Carpineto, and
Gianni Romano, editors, Proceedings of ECIR, vol-
ume 4425 of Lecture Notes in Computer Science,
pages 307–318. Springer, APR.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In CIKM’07: Proceedings of the sixteenth ACM con-
ference on Conference on information and knowledge
management, pages 861–864, New York, NY, USA.
ACM.
Susan Windisch Brown, Dmitriy Dligach, and Martha
Palmer. 2011. Verbnet class assignment as a wsd task.
In Proceedings of the Ninth International Conference
on Computational Semantics, IWCS ’11, pages 85–94,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724–731,
Vancouver, British Columbia, Canada, October.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059–1082.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL’00.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL’02.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2001. Latent semantic kernels. In Carla Brodley and
Andrea Danyluk, editors, Proceedings of ICML-01,
18th International Conference on Machine Learning,
pages 66–73, Williams College, US. Morgan Kauf-
mann Publishers, San Francisco, US.
Danilo Croce, Cristina Giannone, Paolo Annesi, and
Roberto Basili. 2010. Towards open-domain semantic
role labeling. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 237–246, Uppsala, Sweden, July. Association
for Computational Linguistics.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured Lexical Similarity via Convolution
Kernels on Dependency Trees. In Proceedings of
EMNLP 2011.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daum´e III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP’04.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
ACL (Short Papers), pages 29–32. The Association for
Computer Linguistics.
J. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis. Philological
Society, Oxford. reprinted in Palmer, F. (ed. 1968) Se-
lected Papers of J. R. Firth, Longman, Harlow.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proc. of SIGIR ’88, New York, USA.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496–530.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA.
Ana-Maria Giuglea and Alessandro Moschitti. 2006. Se-
mantic role labeling via framenet, verbnet and prop-
bank. In Proceedings ofACL, pages 929–936, Sydney,
Australia, July.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings ofACL’05, pages 403–410.
G. Golub and W. Kahan. 1965. Calculating the singular
values and pseudo-inverse of a matrix. Journal of the
Society for Industrial and Applied Mathematics: Se-
ries B, Numerical Analysis.
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proceedings of
ICML’00.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis
with PropBank and NomBank. In Proceedings of
CoNLL 2008, pages 183–187.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL’03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings ofACL’05.
Tom Landauer and Sue Dumais. 1997. A solution to
plato’s problem: The latent semantic analysis theory
</reference>
<page confidence="0.961426">
271
</page>
<reference confidence="0.999909761363637">
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar word. In Proceedings of COLING-ACL, Mon-
treal, Canada.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between prop-
bank and verbnet. In In Proceedings of the 7th Inter-
national Workshop on Computational Linguistics.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In HLT-NAACL,
pages 1020–1028.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML’06, pages 318–329.
Sebastian Pad´o, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. Isp:
Learning inferential selectional preferences. In Pro-
ceedings of HLT/NAACL 2007.
Daniele Pighin and Alessandro Moschitti. 2009a. Ef-
ficient linearization of tree kernel functions. In Pro-
ceedings of CoNLL’09.
Daniele Pighin and Alessandro Moschitti. 2009b. Re-
verse engineering of tree kernel feature spaces. In Pro-
ceedings of EMNLP, pages 111–120, Singapore, Au-
gust. Association for Computational Linguistics.
Daniele Pighin and Alessandro Moschitti. 2010. On
reverse feature engineering of syntactic tree kernels.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, CoNLL ’10,
pages 223–233, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. Journal of Machine Learning
Research, 5:101–141.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylyania.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97–
123.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Empirical Methods for Natural Language Processing
(EMNLP), pages 89–96, Sapporo, Japan.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141–
188.
Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Blackwells, Oxford.
Alexander S. Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In COLING,
pages 947–953.
Be˜nat Zapirain, Eneko Agirre, Llu´ıs M`arquez, and Mi-
hai Surdeanu. 2010. Improving semantic role classi-
fication with selectional preferences. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, HLT ’10, pages 373–376,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181–201.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
</reference>
<page confidence="0.997324">
272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.908988">
<title confidence="0.9997445">Verb Classification using Distributional in Syntactic and Semantic Structures</title>
<author confidence="0.997983">Danilo Croce</author>
<affiliation confidence="0.99968">University of Tor Vergata</affiliation>
<address confidence="0.999952">00133 Roma, Italy</address>
<email confidence="0.996494">croce@info.uniroma2.it</email>
<author confidence="0.999771">Roberto Basili</author>
<affiliation confidence="0.999468">University of Tor Vergata</affiliation>
<address confidence="0.999949">00133 Roma, Italy</address>
<email confidence="0.99492">basili@info.uniroma2.it</email>
<abstract confidence="0.994952625">In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture meaningful syntactic/semantic structures, which allows for improving the state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<contexts>
<context position="1444" citStr="Baker et al., 1998" startWordPosition="198" endWordPosition="201">els capture meaningful syntactic/semantic structures, which allows for improving the state-of-the-art. 1 Introduction Verb classification is a fundamental topic of computational linguistics research given its importance for understanding the role of verbs in conveying semantics of natural language (NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Alessandro Moschitti University of Trento 38123 Povo (TN), Italy moschitti@disi.unitn.it Martha Palmer University of Colorado at Boulder Boulder, CO 80302, USA mpalmer@colorado.edu still far for bein</context>
<context position="12987" citStr="Baker et al., 1998" startWordPosition="2047" endWordPosition="2050">Semantic Similarity To our knowledge, two main types of tree kernels exploit lexical similarity: the syntactic semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a) applied to constituency trees and the smoothed partial tree kernels (SPTKs) defined in (Croce et al., 2011), which generalizes the former. We report the definition of the latter as we modified it for our purposes. SPTK computes the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Its 265 S VP NP-1 S NN - com 266 TARGET-order::v to::t ROOT VBD T O OP 267 (Baker et al., 1998). We selected the subset of frames containing more than 100 sentences annotated with a verbal predicate for a total of 62,813 sentences in 187 frames (i.e., very close to the VerbNet datasets). For both the datasets, we used 70% of instances for training and 30% for testing. Our verb (multi) classifier is designed with the one-vs-all (Rifkin and Klautau, 2004) multiclassification schema. This uses a set of binary SVM classifiers, one for each verb class (frame) i. The sentences whose verb is labeled with the class i are positive examples for the classifier i. The sentences whose verbs are comp</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed web-crawled corpora.</title>
<date>2009</date>
<journal>LRE,</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="14726" citStr="Baroni et al., 2009" startWordPosition="2342" endWordPosition="2345">-LightTK3 (Moschitti, 2006). The parameterization of each classifier is carried on a held-out set (30% of the training) and is concerned with the setting of the trade-off parameter (option -c) and the leaf weight (lw) (see Alg. 1), which is used to linearly scale the contribution of the leaf nodes. In contrast, the cost-factor parameter of SVM-Light-TK is set as the ratio between the number of negative and positive examples for attempting to have a balanced Precision/Recall. Regarding SPTK setting, we used the lexical similarity Q defined in Sec. 3.1. In more detail, LSA was applied to ukWak (Baroni et al., 2009), which is a large scale document collection made up of 2 billion tokens. M is constructed by applying POS tagging to build rows with pairs lemma, ::POS) (lemma::POS in brief). The contexts of such items are the columns of M and are short windows of size [−3,+3], centered on the items. This allows for better capturing syntactic properties of words. The most frequent 20,000 items are selected along with their 20k contexts. The entries of M are the point-wise mutual 3(Structural kernels in SVMLight (Joachims, 2000)) available at http://disi.unitn.it/moschitti/Tree-Kernel.htm STK PTK SPTK lw Acc.</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. LRE, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Combined syntactic and semantic kernels for text classification.</title>
<date>2007</date>
<booktitle>Proceedings of ECIR,</booktitle>
<volume>4425</volume>
<pages>307--318</pages>
<editor>In Gianni Amati, Claudio Carpineto, and Gianni Romano, editors,</editor>
<publisher>Springer, APR.</publisher>
<contexts>
<context position="9967" citStr="Bloehdorn and Moschitti, 2007" startWordPosition="1540" endWordPosition="1543">ins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new innovative structures and similarity functions based on LSA. The main idea of SPTK is rather simple: (i) measuring the similarity between two trees in terms of the number of shared subtrees; and (ii) su</context>
<context position="12541" citStr="Bloehdorn and Moschitti, 2007" startWordPosition="1970" endWordPosition="1973">d as the cosine similarity between the corresponding projections iw1, iw2 in the LSA space, i.e a(w1, w2) = w1 · ~w2 This is known as Latent Semantic KerIl~w1ll I~w�JI nel (L K), proposed in (Cristianini et al., 2001), as it defines a positive semi-definite Gram matrix G = a(w1, w2) dw1, w2 (Shawe-Taylor and Cristianini, 2004). a is thus a valid kernel and can be combined with other kernels, as discussed in the next session. 3.2 Tree Kernels driven by Semantic Similarity To our knowledge, two main types of tree kernels exploit lexical similarity: the syntactic semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a) applied to constituency trees and the smoothed partial tree kernels (SPTKs) defined in (Croce et al., 2011), which generalizes the former. We report the definition of the latter as we modified it for our purposes. SPTK computes the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Its 265 S VP NP-1 S NN - com 266 TARGET-order::v to::t ROOT VBD T O OP 267 (Baker et al., 1998). We selected the subset of frames containing more than 100 sentences annotated with a verbal predicate for a total of 62,813 sentences in 187 frames (i.e</context>
</contexts>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007a. Combined syntactic and semantic kernels for text classification. In Gianni Amati, Claudio Carpineto, and Gianni Romano, editors, Proceedings of ECIR, volume 4425 of Lecture Notes in Computer Science, pages 307–318. Springer, APR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structure and semantics for expressive text kernels.</title>
<date>2007</date>
<booktitle>In CIKM’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>861--864</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9967" citStr="Bloehdorn and Moschitti, 2007" startWordPosition="1540" endWordPosition="1543">ins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new innovative structures and similarity functions based on LSA. The main idea of SPTK is rather simple: (i) measuring the similarity between two trees in terms of the number of shared subtrees; and (ii) su</context>
<context position="12541" citStr="Bloehdorn and Moschitti, 2007" startWordPosition="1970" endWordPosition="1973">d as the cosine similarity between the corresponding projections iw1, iw2 in the LSA space, i.e a(w1, w2) = w1 · ~w2 This is known as Latent Semantic KerIl~w1ll I~w�JI nel (L K), proposed in (Cristianini et al., 2001), as it defines a positive semi-definite Gram matrix G = a(w1, w2) dw1, w2 (Shawe-Taylor and Cristianini, 2004). a is thus a valid kernel and can be combined with other kernels, as discussed in the next session. 3.2 Tree Kernels driven by Semantic Similarity To our knowledge, two main types of tree kernels exploit lexical similarity: the syntactic semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a) applied to constituency trees and the smoothed partial tree kernels (SPTKs) defined in (Croce et al., 2011), which generalizes the former. We report the definition of the latter as we modified it for our purposes. SPTK computes the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Its 265 S VP NP-1 S NN - com 266 TARGET-order::v to::t ROOT VBD T O OP 267 (Baker et al., 1998). We selected the subset of frames containing more than 100 sentences annotated with a verbal predicate for a total of 62,813 sentences in 187 frames (i.e</context>
</contexts>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007b. Structure and semantics for expressive text kernels. In CIKM’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 861–864, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Windisch Brown</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Verbnet class assignment as a wsd task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics, IWCS ’11,</booktitle>
<pages>85--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6522" citStr="Brown et al., 2011" startWordPosition="991" endWordPosition="994">traints in a specific argument position and the above verb order is a clear example. In the direct object position of the example sentence for the first sense 60.1 of order, we found commission in the role PATIENT of the predicate. It clearly satisfies the +ANIMATE/+ORGANIZATION restriction on the PATIENT role. This is not true for the direct object dependency of the alternative sense 13.5.1, which usually expresses the THEME role, with unrestricted type selection. When properly generalized, the direct object information has thus been shown highly predictive about verb sense distinctions. In (Brown et al., 2011), the so called dynamic dependency neighborhoods (DDN), i.e., the set of verbs that are typically collocated with a direct object, are shown to be more helpful than lexical information (e.g., WordNet). The set of typical verbs taking a noun n as a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the a</context>
<context position="17795" citStr="Brown et al., 2011" startWordPosition="2875" endWordPosition="2878">SK. 268 STK PTK SPTK lw Acc. lw Acc. lw Acc. CT - 91.14% 8 91.66% 6 91.66% GRCT - 91.71% 8 92.38% 4 92.33% LCT - 89.20% 0.2 92.54% 0.1 92.55% BOW 88.16% SK 89.86% Table 3: VerbNet accuracy without the none class generally in contrast with standard text categorization tasks, for which n-gram models show accuracy comparable to the simpler BOW. On the other hand, it simply confirms that verb classification requires the dependency information between words (i.e., at least the sequential structure information provided by SK). Second, SK is 2.56 percent points below the stateof-the-art achieved in (Brown et al., 2011) (BR), i.e, 82.08 vs. 84.64. In contrast, STK applied to our representation (CT, GRCT and LCT) produces comparable accuracy, e.g., 84.83, confirming that syntactic representation is needed to reach the state-of-the-art. Third, PTK, which produces more general structures, improves over BR by almost 1.5 (statistically significant result) when using our dependency structures GRCT and LCT. CT does not produce the same improvement since it does not allow PTK to directly compare the lexical structure (lexemes are all leaf nodes in CT and to connect some pairs of them very large trees are needed). Fi</context>
</contexts>
<marker>Brown, Dligach, Palmer, 2011</marker>
<rawString>Susan Windisch Brown, Dmitriy Dligach, and Martha Palmer. 2011. Verbnet class assignment as a wsd task. In Proceedings of the Ninth International Conference on Computational Semantics, IWCS ’11, pages 85–94, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT and EMNLP,</booktitle>
<pages>724--731</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="9746" citStr="Bunescu and Mooney, 2005" startWordPosition="1507" endWordPosition="1510">Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 20</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of HLT and EMNLP, pages 724–731, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Jean Michel Renders</author>
</authors>
<title>Word sequence kernels.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1059</pages>
<contexts>
<context position="9643" citStr="Cancedda et al., 2003" startWordPosition="1491" endWordPosition="1494">ghting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting</context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders. 2003. Word sequence kernels. Journal of Machine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL’00.</booktitle>
<contexts>
<context position="15928" citStr="Charniak, 2000" startWordPosition="2557" endWordPosition="2558"> SPTK lw Acc. lw Acc. lw Acc. CT - 83.83% 8 84.57% 8 84.46% GRCT - 84.83% 8 85.15% 8 85.28% LCT - 77.73% 0.1 86.03% 0.2 86.72% Br. et Al. 84.64% BOW 79.08% SK 82.08% Table 1: VerbNet accuracy with the none class STK PTK SPTK lw Acc. lw Acc. lw Acc. GRCT - 92.67% 6 92.97% 0.4 93.54% LCT - 90.28% 6 92.99% 0.3 93.78% BOW 91.13% SK 91.84% Table 2: FrameNet accuracy without the none class information between them. SVD reduction is then applied to M, with a dimensionality cut of l = 250. For generating the CT, GRCT and LCT structures, we used the constituency trees generated by the Charniak parser (Charniak, 2000) and the dependency structures generated by the LTH syntactic parser (described in (Johansson and Nugues, 2008)). The classification performance is measured with accuracy (i.e., the percentage of correct classification). We also derive statistical significance of the results by using the model described in (Yeh, 2000) and implemented in (Pad´o, 2006). 5.2 VerbNet and FrameNet Classification Results To assess the performance of our settings, we also derive a simple baseline based on the bag-of-words (BOW) model. For it, we represent an instance of a verb in a sentence using all words of the sen</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02.</booktitle>
<contexts>
<context position="9357" citStr="Collins and Duffy, 2002" startWordPosition="1446" endWordPosition="1449">006), define contexts as the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Mosch</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In Proceedings of ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
<author>Huma Lodhi</author>
</authors>
<title>Latent semantic kernels.</title>
<date>2001</date>
<booktitle>Proceedings of ICML-01, 18th International Conference on Machine Learning,</booktitle>
<pages>66--73</pages>
<editor>In Carla Brodley and Andrea Danyluk, editors,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Williams College, US.</location>
<contexts>
<context position="12129" citStr="Cristianini et al., 2001" startWordPosition="1902" endWordPosition="1905">s a way to project a generic term wi into the k-dimensional space using W = UkS1/2 k , where each row corresponds to the representation vectors iwi. The original statistical information about M is captured by the new kdimensional space, which preserves the global structure while removing low-variant dimensions, i.e., distribution noise. Given two words w1 and w2, the term similarity function a is estimated as the cosine similarity between the corresponding projections iw1, iw2 in the LSA space, i.e a(w1, w2) = w1 · ~w2 This is known as Latent Semantic KerIl~w1ll I~w�JI nel (L K), proposed in (Cristianini et al., 2001), as it defines a positive semi-definite Gram matrix G = a(w1, w2) dw1, w2 (Shawe-Taylor and Cristianini, 2004). a is thus a valid kernel and can be combined with other kernels, as discussed in the next session. 3.2 Tree Kernels driven by Semantic Similarity To our knowledge, two main types of tree kernels exploit lexical similarity: the syntactic semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a) applied to constituency trees and the smoothed partial tree kernels (SPTKs) defined in (Croce et al., 2011), which generalizes the former. We report the definition of the latter as we m</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, Lodhi, 2001</marker>
<rawString>Nello Cristianini, John Shawe-Taylor, and Huma Lodhi. 2001. Latent semantic kernels. In Carla Brodley and Andrea Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 66–73, Williams College, US. Morgan Kaufmann Publishers, San Francisco, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Cristina Giannone</author>
<author>Paolo Annesi</author>
<author>Roberto Basili</author>
</authors>
<title>Towards open-domain semantic role labeling.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>237--246</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="7460" citStr="Croce et al., 2010" startWordPosition="1151" endWordPosition="1154">ity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DMs can be used to suggest that the role PATIEN</context>
</contexts>
<marker>Croce, Giannone, Annesi, Basili, 2010</marker>
<rawString>Danilo Croce, Cristina Giannone, Paolo Annesi, and Roberto Basili. 2010. Towards open-domain semantic role labeling. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 237–246, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured Lexical Similarity via Convolution Kernels on Dependency Trees.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="10042" citStr="Croce et al., 2011" startWordPosition="1552" endWordPosition="1555">tov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new innovative structures and similarity functions based on LSA. The main idea of SPTK is rather simple: (i) measuring the similarity between two trees in terms of the number of shared subtrees; and (ii) such number also includes similar fragments whose lexical nodes are just rela</context>
<context position="12651" citStr="Croce et al., 2011" startWordPosition="1987" endWordPosition="1990">s is known as Latent Semantic KerIl~w1ll I~w�JI nel (L K), proposed in (Cristianini et al., 2001), as it defines a positive semi-definite Gram matrix G = a(w1, w2) dw1, w2 (Shawe-Taylor and Cristianini, 2004). a is thus a valid kernel and can be combined with other kernels, as discussed in the next session. 3.2 Tree Kernels driven by Semantic Similarity To our knowledge, two main types of tree kernels exploit lexical similarity: the syntactic semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a) applied to constituency trees and the smoothed partial tree kernels (SPTKs) defined in (Croce et al., 2011), which generalizes the former. We report the definition of the latter as we modified it for our purposes. SPTK computes the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Its 265 S VP NP-1 S NN - com 266 TARGET-order::v to::t ROOT VBD T O OP 267 (Baker et al., 1998). We selected the subset of frames containing more than 100 sentences annotated with a verbal predicate for a total of 62,813 sentences in 187 frames (i.e., very close to the VerbNet datasets). For both the datasets, we used 70% of instances for training and 30% f</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured Lexical Similarity via Convolution Kernels on Dependency Trees. In Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>Kernel Methods for Relational Learning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="9592" citStr="Cumby and Roth, 2003" startWordPosition="1484" endWordPosition="1487"> single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we de</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. Kernel Methods for Relational Learning. In Proceedings of ICML 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Np bracketing by maximum entropy tagging and SVM reranking.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04.</booktitle>
<marker>Daum´e, Marcu, 2004</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2004. Np bracketing by maximum entropy tagging and SVM reranking. In Proceedings of EMNLP’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Novel semantic features for verb sense disambiguation.</title>
<date>2008</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>29--32</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="7091" citStr="Dligach and Palmer, 2008" startWordPosition="1090" endWordPosition="1093">e about verb sense distinctions. In (Brown et al., 2011), the so called dynamic dependency neighborhoods (DDN), i.e., the set of verbs that are typically collocated with a direct object, are shown to be more helpful than lexical information (e.g., WordNet). The set of typical verbs taking a noun n as a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenste</context>
</contexts>
<marker>Dligach, Palmer, 2008</marker>
<rawString>Dmitriy Dligach and Martha Palmer. 2008. Novel semantic features for verb sense disambiguation. In ACL (Short Papers), pages 29–32. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955.</title>
<date>1957</date>
<booktitle>In Studies in Linguistic Analysis. Philological Society,</booktitle>
<editor>Palmer, F. (ed.</editor>
<location>Oxford.</location>
<note>reprinted in</note>
<contexts>
<context position="7617" citStr="Firth, 1957" startWordPosition="1175" endWordPosition="1176">ency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DMs can be used to suggest that the role PATIENT can be inherited by all these words, as suitable Organisations. In supervised language learning, when few examples are available, DMs support cost-effectiv</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. Firth. 1957. A synopsis of linguistic theory 1930-1955. In Studies in Linguistic Analysis. Philological Society, Oxford. reprinted in Palmer, F. (ed. 1968) Selected Papers of J. R. Firth, Longman, Harlow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Furnas</author>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>R A Harshman</author>
<author>L A Streeter</author>
<author>K E Lochbaum</author>
</authors>
<title>Information retrieval using a singular value decomposition model of latent semantic structure.</title>
<date>1988</date>
<booktitle>In Proc. of SIGIR ’88,</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="7522" citStr="Furnas et al., 1988" startWordPosition="1161" endWordPosition="1164">he same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DMs can be used to suggest that the role PATIENT can be inherited by all these words, as suitable Organisatio</context>
</contexts>
<marker>Furnas, Deerwester, Dumais, Landauer, Harshman, Streeter, Lochbaum, 1988</marker>
<rawString>G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Landauer, R. A. Harshman, L. A. Streeter, and K. E. Lochbaum. 1988. Information retrieval using a singular value decomposition model of latent semantic structure. In Proc. of SIGIR ’88, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurasfky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistic,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1700" citStr="Gildea and Jurasfky, 2002" startWordPosition="238" endWordPosition="242">verbs in conveying semantics of natural language (NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Alessandro Moschitti University of Trento 38123 Povo (TN), Italy moschitti@disi.unitn.it Martha Palmer University of Colorado at Boulder Boulder, CO 80302, USA mpalmer@colorado.edu still far for being accomplished. In particular, the exhaustive design and experimentation of lexical and syntactic features for learning verb classification appears to be computationally problematic. For example, the verb order can belongs to the two VerbNet classes: – The</context>
</contexts>
<marker>Gildea, Jurasfky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurasfky. 2002. Automatic labeling of semantic roles. Computational Linguistic, 28(3):496–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1748" citStr="Gildea and Palmer, 2002" startWordPosition="247" endWordPosition="250">(NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Alessandro Moschitti University of Trento 38123 Povo (TN), Italy moschitti@disi.unitn.it Martha Palmer University of Colorado at Boulder Boulder, CO 80302, USA mpalmer@colorado.edu still far for being accomplished. In particular, the exhaustive design and experimentation of lexical and syntactic features for learning verb classification appears to be computationally problematic. For example, the verb order can belongs to the two VerbNet classes: – The class 60.1, i.e., order someone to do something</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Giuglea</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic role labeling via framenet, verbnet and propbank.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>929--936</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="19506" citStr="Giuglea and Moschitti, 2006" startWordPosition="3157" endWordPosition="3161">OW by only 0.70, i.e., 4 times less than in the VerbNet setting. This suggests that word order around the predicate is more important for deriving the VerbNet class than the FrameNet frame. Additionally, LCT or GRCT seems to be invariant for both PTK and SPTK whereas the lexical similarity still produces a relevant improvement on PTK, i.e., 13% of relative error reduction, for an absolute accuracy of 93.78%. The latter improves over the state0% 20% 40% 60% 80% 100% Percentage of train examples Figure 4: Learning curves: VerbNet accuracy with the none Class of-the-art, i.e., 92.63% derived in (Giuglea and Moschitti, 2006), by using STK on CT on 133 frames. We also carried out experiments to understand the role of the none class. Table 3 reports on the VerbNet classification without its instances. This is of course an unrealistic setting as it would assume that the current VerbNet release already includes all senses for English verbs. In the table, we note that the overall accuracy highly increases and the difference between models reduces. The similarities play no role anymore. This may suggest that SPTK can help in complex settings, where verb class characterization is more difficult. Another important role o</context>
</contexts>
<marker>Giuglea, Moschitti, 2006</marker>
<rawString>Ana-Maria Giuglea and Alessandro Moschitti. 2006. Semantic role labeling via framenet, verbnet and propbank. In Proceedings ofACL, pages 929–936, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL’05,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="9666" citStr="Gliozzo et al., 2005" startWordPosition="1495" endWordPosition="1498"> to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structu</context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings ofACL’05, pages 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Golub</author>
<author>W Kahan</author>
</authors>
<title>Calculating the singular values and pseudo-inverse of a matrix.</title>
<date>1965</date>
<journal>Journal of the Society for Industrial and Applied Mathematics: Series B, Numerical Analysis.</journal>
<contexts>
<context position="11240" citStr="Golub and Kahan, 1965" startWordPosition="1738" endWordPosition="1742">ical nodes are just related (so they can be different). The contribution of (ii) is proportional to the lexical similarity of the tree lexical nodes, where the latter can be evaluated according to distributional models or also lexical resources, e.g., WordNet. In the following, we define our models based on previous work on LSA and SPTKs. 3.1 LSA as lexical similarity model Robust representations can be obtained through intelligent dimensionality reduction methods. In LSA the original word-by-context matrix M is decomposed through Singular Value Decomposition (SVD) (Landauer and Dumais, 1997; Golub and Kahan, 1965) into the product of three new matrices: U, S, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT , where only the first k columns of U and V are used, corresponding to the first k greatest singular values. This approximation supplies a way to project a generic term wi into the k-dimensional space using W = UkS1/2 k , where each row corresponds to the representation vectors iwi. The original statistical information about M is captured by the new kdimensional space, which preserves the global structure while removing low-variant dimensions, i.e., distribution nois</context>
</contexts>
<marker>Golub, Kahan, 1965</marker>
<rawString>G. Golub and W. Kahan. 1965. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics: Series B, Numerical Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Estimating the generalization performance of a SVM efficiently.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML’00.</booktitle>
<contexts>
<context position="15244" citStr="Joachims, 2000" startWordPosition="2433" endWordPosition="2434"> similarity Q defined in Sec. 3.1. In more detail, LSA was applied to ukWak (Baroni et al., 2009), which is a large scale document collection made up of 2 billion tokens. M is constructed by applying POS tagging to build rows with pairs lemma, ::POS) (lemma::POS in brief). The contexts of such items are the columns of M and are short windows of size [−3,+3], centered on the items. This allows for better capturing syntactic properties of words. The most frequent 20,000 items are selected along with their 20k contexts. The entries of M are the point-wise mutual 3(Structural kernels in SVMLight (Joachims, 2000)) available at http://disi.unitn.it/moschitti/Tree-Kernel.htm STK PTK SPTK lw Acc. lw Acc. lw Acc. CT - 83.83% 8 84.57% 8 84.46% GRCT - 84.83% 8 85.15% 8 85.28% LCT - 77.73% 0.1 86.03% 0.2 86.72% Br. et Al. 84.64% BOW 79.08% SK 82.08% Table 1: VerbNet accuracy with the none class STK PTK SPTK lw Acc. lw Acc. lw Acc. GRCT - 92.67% 6 92.97% 0.4 93.54% LCT - 90.28% 6 92.99% 0.3 93.78% BOW 91.13% SK 91.84% Table 2: FrameNet accuracy without the none class information between them. SVD reduction is then applied to M, with a dimensionality cut of l = 250. For generating the CT, GRCT and LCT structur</context>
</contexts>
<marker>Joachims, 2000</marker>
<rawString>T. Joachims. 2000. Estimating the generalization performance of a SVM efficiently. In Proceedings of ICML’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>183--187</pages>
<contexts>
<context position="16039" citStr="Johansson and Nugues, 2008" startWordPosition="2572" endWordPosition="2575">7.73% 0.1 86.03% 0.2 86.72% Br. et Al. 84.64% BOW 79.08% SK 82.08% Table 1: VerbNet accuracy with the none class STK PTK SPTK lw Acc. lw Acc. lw Acc. GRCT - 92.67% 6 92.97% 0.4 93.54% LCT - 90.28% 6 92.99% 0.3 93.78% BOW 91.13% SK 91.84% Table 2: FrameNet accuracy without the none class information between them. SVD reduction is then applied to M, with a dimensionality cut of l = 250. For generating the CT, GRCT and LCT structures, we used the constituency trees generated by the Charniak parser (Charniak, 2000) and the dependency structures generated by the LTH syntactic parser (described in (Johansson and Nugues, 2008)). The classification performance is measured with accuracy (i.e., the percentage of correct classification). We also derive statistical significance of the results by using the model described in (Yeh, 2000) and implemented in (Pad´o, 2006). 5.2 VerbNet and FrameNet Classification Results To assess the performance of our settings, we also derive a simple baseline based on the bag-of-words (BOW) model. For it, we represent an instance of a verb in a sentence using all words of the sentence (by creating a special feature for the predicate word). We also used sequence kernels (SK), i.e., PTK app</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with PropBank and NomBank. In Proceedings of CoNLL 2008, pages 183–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03.</booktitle>
<contexts>
<context position="9513" citStr="Kudo and Matsumoto, 2003" startWordPosition="1472" endWordPosition="1475">matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classif</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boosting-based parse reranking with subtree features.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL’05.</booktitle>
<contexts>
<context position="9419" citStr="Kudo et al., 2005" startWordPosition="1458" endWordPosition="1461">ntered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 20</context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In Proceedings ofACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Landauer</author>
<author>Sue Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<contexts>
<context position="11216" citStr="Landauer and Dumais, 1997" startWordPosition="1734" endWordPosition="1737">similar fragments whose lexical nodes are just related (so they can be different). The contribution of (ii) is proportional to the lexical similarity of the tree lexical nodes, where the latter can be evaluated according to distributional models or also lexical resources, e.g., WordNet. In the following, we define our models based on previous work on LSA and SPTKs. 3.1 LSA as lexical similarity model Robust representations can be obtained through intelligent dimensionality reduction methods. In LSA the original word-by-context matrix M is decomposed through Singular Value Decomposition (SVD) (Landauer and Dumais, 1997; Golub and Kahan, 1965) into the product of three new matrices: U, S, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT , where only the first k columns of U and V are used, corresponding to the first k greatest singular values. This approximation supplies a way to project a generic term wi into the k-dimensional space using W = UkS1/2 k , where each row corresponds to the representation vectors iwi. The original statistical information about M is captured by the new kdimensional space, which preserves the global structure while removing low-variant dimensions,</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Tom Landauer and Sue Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar word.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7415" citStr="Lin, 1998" startWordPosition="1146" endWordPosition="1147">haracterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DM</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar word. In Proceedings of COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Szu ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>Combining lexical resources: Mapping between propbank and verbnet. In</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th International Workshop on Computational Linguistics.</booktitle>
<marker>Loper, Yi, Palmer, 2007</marker>
<rawString>Edward Loper, Szu ting Yi, and Martha Palmer. 2007. Combining lexical resources: Mapping between propbank and verbnet. In In Proceedings of the 7th International Workshop on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Alessandro Moschitti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Syntactic/semantic structures for textual entailment recognition.</title>
<date>2010</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>1020--1028</pages>
<contexts>
<context position="10021" citStr="Mehdad et al., 2010" startWordPosition="1548" endWordPosition="1551">Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new innovative structures and similarity functions based on LSA. The main idea of SPTK is rather simple: (i) measuring the similarity between two trees in terms of the number of shared subtrees; and (ii) such number also includes similar fragments whose lexica</context>
</contexts>
<marker>Mehdad, Moschitti, Zanzotto, 2010</marker>
<rawString>Yashar Mehdad, Alessandro Moschitti, and Fabio Massimo Zanzotto. 2010. Syntactic/semantic structures for textual entailment recognition. In HLT-NAACL, pages 1020–1028.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML’06,</booktitle>
<pages>318--329</pages>
<contexts>
<context position="14133" citStr="Moschitti, 2006" startWordPosition="2243" endWordPosition="2244">tive examples for the classifier i. The sentences whose verbs are compatible with the class i but evoking a different class or labeled with none (no current verb class applies) are added as negative examples. In the classification phase the binary classifiers are applied by (i) only considering classes that are compatible with the target verbs; and (ii) selecting the class associated with the maximum positive SVM margin. If all classifiers provide a negative score the example is labeled with none. To learn the binary classifiers of the schema above, we coded our modified SPTK in SVM-LightTK3 (Moschitti, 2006). The parameterization of each classifier is carried on a held-out set (30% of the training) and is concerned with the setting of the trade-off parameter (option -c) and the leaf weight (lw) (see Alg. 1), which is used to linearly scale the contribution of the leaf nodes. In contrast, the cost-factor parameter of SVM-Light-TK is set as the ratio between the number of negative and positive examples for attempting to have a balanced Precision/Recall. Regarding SPTK setting, we used the lexical similarity Q defined in Sec. 3.1. In more detail, LSA was applied to ukWak (Baroni et al., 2009), which</context>
<context position="19506" citStr="Moschitti, 2006" startWordPosition="3159" endWordPosition="3161">.70, i.e., 4 times less than in the VerbNet setting. This suggests that word order around the predicate is more important for deriving the VerbNet class than the FrameNet frame. Additionally, LCT or GRCT seems to be invariant for both PTK and SPTK whereas the lexical similarity still produces a relevant improvement on PTK, i.e., 13% of relative error reduction, for an absolute accuracy of 93.78%. The latter improves over the state0% 20% 40% 60% 80% 100% Percentage of train examples Figure 4: Learning curves: VerbNet accuracy with the none Class of-the-art, i.e., 92.63% derived in (Giuglea and Moschitti, 2006), by using STK on CT on 133 frames. We also carried out experiments to understand the role of the none class. Table 3 reports on the VerbNet classification without its instances. This is of course an unrealistic setting as it would assume that the current VerbNet release already includes all senses for English verbs. In the table, we note that the overall accuracy highly increases and the difference between models reduces. The similarities play no role anymore. This may suggest that SPTK can help in complex settings, where verb class characterization is more difficult. Another important role o</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of ECML’06, pages 318–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>User’s guide to sigf: Significance testing by approximate randomisation.</title>
<date>2006</date>
<marker>Pad´o, 2006</marker>
<rawString>Sebastian Pad´o, 2006. User’s guide to sigf: Significance testing by approximate randomisation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Bonaventura Coppola</author>
<author>Timothy Chklovski</author>
<author>Eduard Hovy</author>
</authors>
<title>Isp: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<contexts>
<context position="8335" citStr="Pantel et al., 2007" startWordPosition="1287" endWordPosition="1290">eometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DMs can be used to suggest that the role PATIENT can be inherited by all these words, as suitable Organisations. In supervised language learning, when few examples are available, DMs support cost-effective lexical generalizations, often outperforming knowledge based resources (such as WordNet, as in (Pantel et al., 2007)). Obviously, the choice of the context 264 type determines the type of targeted semantic properties. Wider contexts (e.g., entire documents) are shown to suggest topical relations. Smaller contexts tend to capture more specific semantic aspects, e.g. the syntactic behavior, and better capture paradigmatic relations, such as synonymy. In particular, word space models, as described in (Sahlgren, 2006), define contexts as the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number </context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. Isp: Learning inferential selectional preferences. In Proceedings of HLT/NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient linearization of tree kernel functions.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL’09.</booktitle>
<contexts>
<context position="20621" citStr="Pighin and Moschitti, 2009" startWordPosition="3347" endWordPosition="3350">PTK can help in complex settings, where verb class characterization is more difficult. Another important role of SPTK models is their ability to generalize. To test this aspect, Figure 4 illustrates the learning curves of SPTK with respect to BOW and the accuracy achieved by BR (with a constant line). It is impressive to note that with only 40% of the data SPTK can reach the state-of-the-art. 6 Model Analysis and Discussion We carried out analysis of system errors and its induced features. These can be examined by applying the reverse engineering tool5 proposed in (Pighin and Moschitti, 2010; Pighin and Moschitti, 2009a; Pighin and Moschitti, 2009b), which extracts the most important features for the classification model. Many mistakes are related to false positives and negatives of the none class (about 72% of the errors). This class also causes data imbalance. Most errors are also due to lack of lexical information available to the SPTK kernel: (i) in 30% of the errors, the argument heads were proper nouns for which the lexical generalization provided by the DMs was not 5http://danielepighin.net/cms/software/flink SPTK BOW Brown et al. Accuracy 90% 80% 70% 60% 50% 269 VerbNet class 13.5.1 (IM(VB(target))(</context>
<context position="22776" citStr="Pighin and Moschitti, 2009" startWordPosition="3618" endWordPosition="3621">not sufficient to separate verb senses. These errors are mainly due to the lack of contextual information. While error analysis suggests that further improvement is possible (e.g. by exploiting proper nouns), the type of generalizations currently achieved by SPTK are rather effective. Table 4 and 5 report the tree structures characterizing the most informative training examples of the two senses of the verb order, i.e. the VerbNet classes 13.5.1 (make a request for something) and 60 (give instructions to or direct somebody to do something with authority). In line with the method discussed in (Pighin and Moschitti, 2009b), these fragments are extracted as they appear in most of the support vectors selected during SVM training. As easily seen, the two classes are captured by rather different patterns. The typical accusative form with an explicit direct object emerges as characterizing the sense 13.5.1, denoting the THEME role. All fragments of the sense 60 emphasize instead the sentential complement of the verb that in fact expresses the standard PROPOSITION role in VerbNet. Notice that tree fragments correspond to syntactic patterns. The a posteriori VerbNet class 13.5.1 (VP(VB(target))(NP)) (VP(VBG(target))</context>
</contexts>
<marker>Pighin, Moschitti, 2009</marker>
<rawString>Daniele Pighin and Alessandro Moschitti. 2009a. Efficient linearization of tree kernel functions. In Proceedings of CoNLL’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Reverse engineering of tree kernel feature spaces.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>111--120</pages>
<contexts>
<context position="20621" citStr="Pighin and Moschitti, 2009" startWordPosition="3347" endWordPosition="3350">PTK can help in complex settings, where verb class characterization is more difficult. Another important role of SPTK models is their ability to generalize. To test this aspect, Figure 4 illustrates the learning curves of SPTK with respect to BOW and the accuracy achieved by BR (with a constant line). It is impressive to note that with only 40% of the data SPTK can reach the state-of-the-art. 6 Model Analysis and Discussion We carried out analysis of system errors and its induced features. These can be examined by applying the reverse engineering tool5 proposed in (Pighin and Moschitti, 2010; Pighin and Moschitti, 2009a; Pighin and Moschitti, 2009b), which extracts the most important features for the classification model. Many mistakes are related to false positives and negatives of the none class (about 72% of the errors). This class also causes data imbalance. Most errors are also due to lack of lexical information available to the SPTK kernel: (i) in 30% of the errors, the argument heads were proper nouns for which the lexical generalization provided by the DMs was not 5http://danielepighin.net/cms/software/flink SPTK BOW Brown et al. Accuracy 90% 80% 70% 60% 50% 269 VerbNet class 13.5.1 (IM(VB(target))(</context>
<context position="22776" citStr="Pighin and Moschitti, 2009" startWordPosition="3618" endWordPosition="3621">not sufficient to separate verb senses. These errors are mainly due to the lack of contextual information. While error analysis suggests that further improvement is possible (e.g. by exploiting proper nouns), the type of generalizations currently achieved by SPTK are rather effective. Table 4 and 5 report the tree structures characterizing the most informative training examples of the two senses of the verb order, i.e. the VerbNet classes 13.5.1 (make a request for something) and 60 (give instructions to or direct somebody to do something with authority). In line with the method discussed in (Pighin and Moschitti, 2009b), these fragments are extracted as they appear in most of the support vectors selected during SVM training. As easily seen, the two classes are captured by rather different patterns. The typical accusative form with an explicit direct object emerges as characterizing the sense 13.5.1, denoting the THEME role. All fragments of the sense 60 emphasize instead the sentential complement of the verb that in fact expresses the standard PROPOSITION role in VerbNet. Notice that tree fragments correspond to syntactic patterns. The a posteriori VerbNet class 13.5.1 (VP(VB(target))(NP)) (VP(VBG(target))</context>
</contexts>
<marker>Pighin, Moschitti, 2009</marker>
<rawString>Daniele Pighin and Alessandro Moschitti. 2009b. Reverse engineering of tree kernel feature spaces. In Proceedings of EMNLP, pages 111–120, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Alessandro Moschitti</author>
</authors>
<title>On reverse feature engineering of syntactic tree kernels.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10,</booktitle>
<pages>223--233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="20593" citStr="Pighin and Moschitti, 2010" startWordPosition="3343" endWordPosition="3346">ore. This may suggest that SPTK can help in complex settings, where verb class characterization is more difficult. Another important role of SPTK models is their ability to generalize. To test this aspect, Figure 4 illustrates the learning curves of SPTK with respect to BOW and the accuracy achieved by BR (with a constant line). It is impressive to note that with only 40% of the data SPTK can reach the state-of-the-art. 6 Model Analysis and Discussion We carried out analysis of system errors and its induced features. These can be examined by applying the reverse engineering tool5 proposed in (Pighin and Moschitti, 2010; Pighin and Moschitti, 2009a; Pighin and Moschitti, 2009b), which extracts the most important features for the classification model. Many mistakes are related to false positives and negatives of the none class (about 72% of the errors). This class also causes data imbalance. Most errors are also due to lack of lexical information available to the SPTK kernel: (i) in 30% of the errors, the argument heads were proper nouns for which the lexical generalization provided by the DMs was not 5http://danielepighin.net/cms/software/flink SPTK BOW Brown et al. Accuracy 90% 80% 70% 60% 50% 269 VerbNet c</context>
</contexts>
<marker>Pighin, Moschitti, 2010</marker>
<rawString>Daniele Pighin and Alessandro Moschitti. 2010. On reverse feature engineering of syntactic tree kernels. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10, pages 223–233, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valeri Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<journal>Machine Learning Journal.</journal>
<contexts>
<context position="1722" citStr="Pradhan et al., 2005" startWordPosition="243" endWordPosition="246">s of natural language (NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Alessandro Moschitti University of Trento 38123 Povo (TN), Italy moschitti@disi.unitn.it Martha Palmer University of Colorado at Boulder Boulder, CO 80302, USA mpalmer@colorado.edu still far for being accomplished. In particular, the exhaustive design and experimentation of lexical and syntactic features for learning verb classification appears to be computationally problematic. For example, the verb order can belongs to the two VerbNet classes: – The class 60.1, i.e., ord</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Rifkin</author>
<author>Aldebaro Klautau</author>
</authors>
<title>In defense of one-vs-all classification.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--101</pages>
<contexts>
<context position="13349" citStr="Rifkin and Klautau, 2004" startWordPosition="2109" endWordPosition="2112"> we modified it for our purposes. SPTK computes the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Its 265 S VP NP-1 S NN - com 266 TARGET-order::v to::t ROOT VBD T O OP 267 (Baker et al., 1998). We selected the subset of frames containing more than 100 sentences annotated with a verbal predicate for a total of 62,813 sentences in 187 frames (i.e., very close to the VerbNet datasets). For both the datasets, we used 70% of instances for training and 30% for testing. Our verb (multi) classifier is designed with the one-vs-all (Rifkin and Klautau, 2004) multiclassification schema. This uses a set of binary SVM classifiers, one for each verb class (frame) i. The sentences whose verb is labeled with the class i are positive examples for the classifier i. The sentences whose verbs are compatible with the class i but evoking a different class or labeled with none (no current verb class applies) are added as negative examples. In the classification phase the binary classifiers are applied by (i) only considering classes that are compatible with the target verbs; and (ii) selecting the class associated with the maximum positive SVM margin. If all </context>
</contexts>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>Ryan Rifkin and Aldebaro Klautau. 2004. In defense of one-vs-all classification. Journal of Machine Learning Research, 5:101–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University.</institution>
<contexts>
<context position="8738" citStr="Sahlgren, 2006" startWordPosition="1350" endWordPosition="1351">s. In supervised language learning, when few examples are available, DMs support cost-effective lexical generalizations, often outperforming knowledge based resources (such as WordNet, as in (Pantel et al., 2007)). Obviously, the choice of the context 264 type determines the type of targeted semantic properties. Wider contexts (e.g., entire documents) are shown to suggest topical relations. Smaller contexts tend to capture more specific semantic aspects, e.g. the syntactic behavior, and better capture paradigmatic relations, such as synonymy. In particular, word space models, as described in (Sahlgren, 2006), define contexts as the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Coll</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Ph.D. thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper Schuler</author>
</authors>
<title>VerbNet: A broadcoverage, comprehensive verb lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylyania.</institution>
<contexts>
<context position="1410" citStr="Schuler, 2005" startWordPosition="194" endWordPosition="195"> detection shows that our models capture meaningful syntactic/semantic structures, which allows for improving the state-of-the-art. 1 Introduction Verb classification is a fundamental topic of computational linguistics research given its importance for understanding the role of verbs in conveying semantics of natural language (NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Alessandro Moschitti University of Trento 38123 Povo (TN), Italy moschitti@disi.unitn.it Martha Palmer University of Colorado at Boulder Boulder, CO 80302, USA mpalm</context>
</contexts>
<marker>Schuler, 2005</marker>
<rawString>Karin Kipper Schuler. 2005. VerbNet: A broadcoverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylyania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Journal of Computational Linguistics,</journal>
<volume>24</volume>
<pages>123</pages>
<contexts>
<context position="7381" citStr="Schutze, 1998" startWordPosition="1141" endWordPosition="1142"> a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case o</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>Hinrich Schutze. 1998. Automatic word sense discrimination. Journal of Computational Linguistics, 24:97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="12240" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1920" endWordPosition="1924"> corresponds to the representation vectors iwi. The original statistical information about M is captured by the new kdimensional space, which preserves the global structure while removing low-variant dimensions, i.e., distribution noise. Given two words w1 and w2, the term similarity function a is estimated as the cosine similarity between the corresponding projections iw1, iw2 in the LSA space, i.e a(w1, w2) = w1 · ~w2 This is known as Latent Semantic KerIl~w1ll I~w�JI nel (L K), proposed in (Cristianini et al., 2001), as it defines a positive semi-definite Gram matrix G = a(w1, w2) dw1, w2 (Shawe-Taylor and Cristianini, 2004). a is thus a valid kernel and can be combined with other kernels, as discussed in the next session. 3.2 Tree Kernels driven by Semantic Similarity To our knowledge, two main types of tree kernels exploit lexical similarity: the syntactic semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a) applied to constituency trees and the smoothed partial tree kernels (SPTKs) defined in (Croce et al., 2011), which generalizes the former. We report the definition of the latter as we modified it for our purposes. SPTK computes the number of common substructures between two trees T1 and T2 witho</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Aravind k Joshi</author>
</authors>
<title>Using LTAG Based Features in Parse Reranking.</title>
<date>2003</date>
<booktitle>In Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>89--96</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9376" citStr="Shen et al., 2003" startWordPosition="1450" endWordPosition="1453">the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehd</context>
</contexts>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Using LTAG Based Features in Parse Reranking. In Empirical Methods for Natural Language Processing (EMNLP), pages 89–96, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Porting statistical parsers with data-defined kernels.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="9447" citStr="Titov and Henderson, 2006" startWordPosition="1462" endWordPosition="1466">get word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 S</context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>Ivan Titov and James Henderson. 2006. Porting statistical parsers with data-defined kernels. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Penka Markova</author>
<author>Christopher Manning</author>
</authors>
<title>The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="9400" citStr="Toutanova et al., 2004" startWordPosition="1454" endWordPosition="1457"> in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b</context>
</contexts>
<marker>Toutanova, Markova, Manning, 2004</marker>
<rawString>Kristina Toutanova, Penka Markova, and Christopher Manning. 2004. The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<pages>188</pages>
<contexts>
<context position="9172" citStr="Turney and Pantel, 2010" startWordPosition="1419" endWordPosition="1422">e more specific semantic aspects, e.g. the syntactic behavior, and better capture paradigmatic relations, such as synonymy. In particular, word space models, as described in (Sahlgren, 2006), define contexts as the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Rec</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludwig Wittgenstein</author>
</authors>
<title>Philosophical Investigations.</title>
<date>1953</date>
<publisher>Blackwells,</publisher>
<location>Oxford.</location>
<contexts>
<context position="7700" citStr="Wittgenstein, 1953" startWordPosition="1187" endWordPosition="1188">mer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DMs can be used to suggest that the role PATIENT can be inherited by all these words, as suitable Organisations. In supervised language learning, when few examples are available, DMs support cost-effective lexical generalizations, often outperforming knowledge based resources (such as W</context>
</contexts>
<marker>Wittgenstein, 1953</marker>
<rawString>Ludwig Wittgenstein. 1953. Philosophical Investigations. Blackwells, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander S Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In COLING,</booktitle>
<pages>947--953</pages>
<contexts>
<context position="16247" citStr="Yeh, 2000" startWordPosition="2604" endWordPosition="2605">3% SK 91.84% Table 2: FrameNet accuracy without the none class information between them. SVD reduction is then applied to M, with a dimensionality cut of l = 250. For generating the CT, GRCT and LCT structures, we used the constituency trees generated by the Charniak parser (Charniak, 2000) and the dependency structures generated by the LTH syntactic parser (described in (Johansson and Nugues, 2008)). The classification performance is measured with accuracy (i.e., the percentage of correct classification). We also derive statistical significance of the results by using the model described in (Yeh, 2000) and implemented in (Pad´o, 2006). 5.2 VerbNet and FrameNet Classification Results To assess the performance of our settings, we also derive a simple baseline based on the bag-of-words (BOW) model. For it, we represent an instance of a verb in a sentence using all words of the sentence (by creating a special feature for the predicate word). We also used sequence kernels (SK), i.e., PTK applied to a tree composed of a fake root and only one level of sentence words. For efficiency reasons4, we only consider the 10 words before and after the predicate with subsequence features of length up to 5. </context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander S. Yeh. 2000. More accurate tests for the statistical significance of result differences. In COLING, pages 947–953.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Be˜nat Zapirain</author>
<author>Eneko Agirre</author>
<author>Llu´ıs M`arquez</author>
<author>Mihai Surdeanu</author>
</authors>
<title>Improving semantic role classification with selectional preferences.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>373--376</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Zapirain, Agirre, M`arquez, Surdeanu, 2010</marker>
<rawString>Be˜nat Zapirain, Eneko Agirre, Llu´ıs M`arquez, and Mihai Surdeanu. 2010. Improving semantic role classification with selectional preferences. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 373–376, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>181--201</pages>
<contexts>
<context position="9720" citStr="Zelenko et al., 2002" startWordPosition="1503" endWordPosition="1506">irs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2002</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2002. Kernel methods for relation extraction. In Proceedings of EMNLP-ACL, pages 181–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
</authors>
<title>Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="9767" citStr="Zhang et al., 2006" startWordPosition="1511" endWordPosition="1514">Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new in</context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel. In Proceedings of NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>