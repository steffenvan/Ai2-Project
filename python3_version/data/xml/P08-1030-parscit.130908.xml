<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989209">
Refining Event Extraction through Cross-document Inference
</title>
<author confidence="0.999049">
Heng Ji Ralph Grishman
</author>
<affiliation confidence="0.998177">
Computer Science Department
</affiliation>
<address confidence="0.7477715">
New York University
New York, NY 10003, USA
</address>
<email confidence="0.999195">
(hengji, grishman)@cs.nyu.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965823529412">
We apply the hypothesis of “One Sense Per
Discourse” (Yarowsky, 1995) to information
extraction (IE), and extend the scope of “dis-
course” from one single document to a cluster
of topically-related documents. We employ a
similar approach to propagate consistent event
arguments across sentences and documents.
Combining global evidence from related doc-
uments with local decisions, we design a sim-
ple scheme to conduct cross-document
inference for improving the ACE event ex-
traction task1. Without using any additional
labeled data this new approach obtained 7.6%
higher F-Measure in trigger labeling and 6%
higher F-Measure in argument labeling over a
state-of-the-art IE system which extracts
events independently for each sentence.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999822">
Identifying events of a particular type within indi-
vidual documents – ‘classical’ information extrac-
tion – remains a difficult task. Recognizing the
different forms in which an event may be ex-
pressed, distinguishing events of different types,
and finding the arguments of an event are all chal-
lenging tasks.
Fortunately, many of these events will be re-
ported multiple times, in different forms, both
within the same document and within topically-
related documents (i.e. a collection of documents
sharing participants in potential events). We can
</bodyText>
<footnote confidence="0.840335">
1 http://www.nist.gov/speech/tests/ace/
</footnote>
<bodyText confidence="0.999962666666667">
take advantage of these alternate descriptions to
improve event extraction in the original document,
by favoring consistency of interpretation across
sentences and documents. Several recent studies
involving specific event types have stressed the
benefits of going beyond traditional single-
document extraction; in particular, Yangarber
(2006) has emphasized this potential in his work
on medical information extraction. In this paper we
demonstrate that appreciable improvements are
possible over the variety of event types in the ACE
(Automatic Content Extraction) evaluation through
the use of cross-sentence and cross-document evi-
dence.
As we shall describe below, we can make use of
consistency at several levels: consistency of word
sense across different instances of the same word
in related documents, and consistency of argu-
ments and roles across different mentions of the
same or related events. Such methods allow us to
build dynamic background knowledge as required
to interpret a document and can compensate for the
limited annotated training data which can be pro-
vided for each event type.
</bodyText>
<sectionHeader confidence="0.948495" genericHeader="introduction">
2 Task and Baseline System
</sectionHeader>
<subsectionHeader confidence="0.782142">
2.1 ACE Event Extraction Task
</subsectionHeader>
<bodyText confidence="0.999309666666667">
The event extraction task we are addressing is that
of the Automatic Content Extraction (ACE) evalu-
ations2. ACE defines the following terminology:
</bodyText>
<footnote confidence="0.697545">
2 In this paper we don’t consider event mention coreference
resolution and so don’t distinguish event mentions and events.
</footnote>
<page confidence="0.955805">
254
</page>
<note confidence="0.715365">
Proceedings of ACL-08: HLT, pages 254–262,
</note>
<page confidence="0.482782">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.965331333333333">
entity: an object or a set of objects in one of the
semantic categories of interest
mention: a reference to an entity (typically, a
noun phrase)
event trigger: the main word which most clearly
expresses an event occurrence
event arguments: the mentions that are in-
volved in an event (participants)
event mention: a phrase or sentence within
which an event is described, including trigger
and arguments
The 2005 ACE evaluation had 8 types of events,
with 33 subtypes; for the purpose of this paper, we
will treat these simply as 33 distinct event types.
For example, for a sentence:
</bodyText>
<subsubsectionHeader confidence="0.4775095">
Barry Diller on Wednesday quit as chief of Vivendi
Universal Entertainment.
</subsubsectionHeader>
<bodyText confidence="0.9983428">
the event extractor should detect a “Person-
nel_End-Position” event mention, with the trigger
word, the position, the person who quit the posi-
tion, the organization, and the time during which
the event happened:
</bodyText>
<table confidence="0.995941571428571">
Trigger Quit
Arguments Role = Person Barry Diller
Role = Vivendi Universal
Organization Entertainment
Role = Position Chief
Role = Wednesday
Time-within
</table>
<tableCaption confidence="0.999771">
Table 1. Event Extraction Example
</tableCaption>
<bodyText confidence="0.999011">
We define the following standards to determine
the correctness of an event mention:
</bodyText>
<listItem confidence="0.9987595">
• A trigger is correctly labeled if its event type
and offsets match a reference trigger.
• An argument is correctly identified if its event
type and offsets match any of the reference ar-
gument mentions.
• An argument is correctly identified and classi-
fied if its event type, offsets, and role match
any of the reference argument mentions.
</listItem>
<subsectionHeader confidence="0.998791">
2.2 A Baseline Within-Sentence Event Tagger
</subsectionHeader>
<bodyText confidence="0.9989162">
We use a state-of-the-art English IE system as our
baseline (Grishman et al., 2005). This system ex-
tracts events independently for each sentence. Its
training and test procedures are as follows.
The system combines pattern matching with sta-
tistical models. For every event mention in the
ACE training corpus, patterns are constructed
based on the sequences of constituent heads sepa-
rating the trigger and arguments. In addition, a set
of Maximum Entropy based classifiers are trained:
</bodyText>
<listItem confidence="0.971534727272727">
• Trigger Labeling: to distinguish event men-
tions from non-event-mentions, to classify
event mentions by type;
• Argument Classifier: to distinguish arguments
from non-arguments;
• Role Classifier: to classify arguments by ar-
gument role.
• Reportable-Event Classifier: Given a trigger,
an event type, and a set of arguments, to de-
termine whether there is a reportable event
mention.
</listItem>
<bodyText confidence="0.999529133333333">
In the test procedure, each document is scanned
for instances of triggers from the training corpus.
When an instance is found, the system tries to
match the environment of the trigger against the set
of patterns associated with that trigger. This pat-
tern-matching process, if successful, will assign
some of the mentions in the sentence as arguments
of a potential event mention. The argument clas-
sifier is applied to the remaining mentions in the
sentence; for any argument passing that classifier,
the role classifier is used to assign a role to it. Fi-
nally, once all arguments have been assigned, the
reportable-event classifier is applied to the poten-
tial event mention; if the result is successful, this
event mention is reported.
</bodyText>
<sectionHeader confidence="0.994871" genericHeader="method">
3 Motivations
</sectionHeader>
<bodyText confidence="0.999181333333333">
In this section we shall present our motivations
based on error analysis for the baseline event tag-
ger.
</bodyText>
<subsectionHeader confidence="0.999888">
3.1 One Trigger Sense Per Cluster
</subsectionHeader>
<bodyText confidence="0.999520777777778">
Across a heterogeneous document corpus, a partic-
ular verb can sometimes be trigger and sometimes
not, and can represent different event types. How-
ever, for a collection of topically-related docu-
ments, the distribution may be much more
convergent. We investigate this hypothesis by au-
tomatically obtaining 25 related documents for
each test text. The statistics of some trigger exam-
ples are presented in table 2.
</bodyText>
<page confidence="0.99754">
255
</page>
<table confidence="0.999774941176471">
Candidate Triggers Event Type Perc./Freq. as Perc./Freq. as Perc./Freq. as
trigger in ACE trigger in test trigger in test +
training corpora document related
documents
Correct advance Movement Transport 31% of 16 50% of 2 88.9% of 27
Event
Triggers
fire Personnel_End-Position 7% of 81 100% of 2 100% of 10
fire Conflict_Attack 54% of 81 100% of 3 100% of 19
replace Personnel_End-Position 5% of 20 100% of 1 83.3% of 6
form Business Start-Org 12% of 8 100% of 2 100% of 23
talk Contact Meet 59% of 74 100% of 4 100% of 26
Incorrect hurt Life Injure 24% of 33 0% of 2 0% of 7
Event
Triggers
execution Life Die 12% of 8 0% of 4 4% of 24
_
</table>
<tableCaption confidence="0.999821">
Table 2. Examples: Percentage of a Word as Event Trigger in Different Data Collections
</tableCaption>
<bodyText confidence="0.983858727272727">
As we can see from the table, the likelihood of a
candidate word being an event trigger in the test
document is closer to its distribution in the collec-
tion of related documents than the uniform training
corpora. So if we can determine the sense (event
type) of a word in the related documents, this will
allow us to infer its sense in the test document. In
this way related documents can help recover event
mentions missed by within-sentence extraction.
For example, in a document about “the advance
into Baghdad”:
</bodyText>
<equation confidence="0.2066915">
Example 1:
[Test Sentence]
</equation>
<bodyText confidence="0.62387025">
Most US army commanders believe it is critical to
pause the breakneck advance towards Baghdad to se-
cure the supply lines and make sure weapons are oper-
able and troops resupplied....
</bodyText>
<subsectionHeader confidence="0.723554">
[Sentences from Related Documents]
</subsectionHeader>
<bodyText confidence="0.964955681818182">
British and US forces report gains in the advance on
Baghdad and take control of Umm Qasr, despite a
fierce sandstorm which slows another flank.
...
The baseline event tagger is not able to detect
“advance” as a “Movement_Transport” event trig-
ger because there is no pattern “advance towards
[Place]” in the ACE training corpora (“advance”
by itself is too ambiguous). The training data,
however, does include the pattern “advance on
[Place]”, which allows the instance of “advance” in
the related documents to be successfully identified
with high confidence by pattern matching as an
event. This provides us much stronger “feedback”
confidence in tagging ‘advance’ in the test sen-
tence as a correct trigger.
On the other hand, if a word is not tagged as an
event trigger in most related documents, then it’s
less likely to be correct in the test sentence despite
its high local confidence. For example, in a docu-
ment about “assessment of Russian president Pu-
tin”:
</bodyText>
<equation confidence="0.2156655">
Example 2:
[Test Sentence]
</equation>
<bodyText confidence="0.347304333333333">
But few at the Kremlin forum suggested that Putin&apos;s
own standing among voters will be hurt by Russia&apos;s
apparent diplomacy failures.
</bodyText>
<subsectionHeader confidence="0.64811">
[Sentences from Related Documents]
</subsectionHeader>
<bodyText confidence="0.949166538461538">
Putin boosted ties with the United States by throwing
his support behind its war on terrorism after the Sept.
11 attacks, but the Iraq war has hurt the relationship.
...
The word “hurt” in the test sentence is mistaken-
ly identified as a “Life_Injure” trigger with high
local confidence (because the within-sentence ex-
tractor misanalyzes “voters” as the object of “hurt”
and so matches the pattern “[Person] be hurt”).
Based on the fact that many other instances of
“hurt” are not “Life_Injure” triggers in the related
documents, we can successfully remove this wrong
event mention in the test document.
</bodyText>
<subsectionHeader confidence="0.9973">
3.2 One Argument Role Per Cluster
</subsectionHeader>
<bodyText confidence="0.999948166666667">
Inspired by the observation about trigger distribu-
tion, we propose a similar hypothesis – one argu-
ment role per cluster for event arguments. In other
words, each entity plays the same argument role, or
no role, for events with the same type in a collec-
tion of related documents. For example,
</bodyText>
<page confidence="0.991233">
256
</page>
<figure confidence="0.25349">
Example 3:
[Test Sentence]
Vivendi earlier this week confirmed months of press
speculation that it planned to shed its entertainment
assets by the end of the year.
[Sentences from Related Documents]
Vivendi has been trying to sell assets to pay off huge
debt, estimated at the end of last month at more than
$13 billion.
Under the reported plans, Blackstone Group would
buy Vivendi&apos;s theme park division, including Universal
Studios Hollywood, Universal Orlando in Florida...
...
</figure>
<bodyText confidence="0.999480166666667">
The above test sentence doesn’t include an ex-
plicit trigger word to indicate “Vivendi” as a “sel-
ler” of a “Transaction_Transfer-Ownership” event
mention, but “Vivendi” is correctly identified as
“seller” in many other related sentences (by match-
ing patterns “[Seller] sell” and “buy [Seller]’s”).
So we can incorporate such additional information
to enhance the confidence of “Vivendi” as a “sel-
ler” in the test sentence.
On the other hand, we can remove spurious ar-
guments with low cross-document frequency and
confidence. In the following example,
</bodyText>
<equation confidence="0.49395">
Example 4:
[Test Sentence]
</equation>
<bodyText confidence="0.969640857142857">
The Davao Medical Center, a regional government
hospital, recorded 19 deaths with 50 wounded.
“the Davao Medical Center” is mistakenly
tagged as “Place” for a “Life_Die” event mention.
But the same annotation for this mention doesn’t
appear again in the related documents, so we can
determine it’s a spurious argument.
</bodyText>
<sectionHeader confidence="0.990905" genericHeader="method">
4 System Approach Overview
</sectionHeader>
<bodyText confidence="0.999970875">
Based on the above motivations we propose to in-
corporate global evidence from a cluster of related
documents to refine local decisions. This section
gives more details about the baseline within-
sentence event tagger, and the information retrieval
system we use to obtain related documents. In the
next section we shall focus on describing the infe-
rence procedure.
</bodyText>
<subsectionHeader confidence="0.992047">
4.1 System Pipeline
</subsectionHeader>
<bodyText confidence="0.975944333333333">
Figure 1 depicts the general procedure of our ap-
proach. EMSet represents a set of event mentions
which is gradually updated.
</bodyText>
<listItem confidence="0.9266686">
• LConf(trigger,etype): The probability of a
string trigger indicating an event mention with
type etype; if the event mention is produced by
pattern matching then assign confidence 1.
• LConf(arg, etype): The probability that a men-
tion arg is an argument of some particular
event type etype.
• LConf(arg, etype, role): If arg is an argument
with event type etype, the probability of arg
having some particular role.
</listItem>
<bodyText confidence="0.998083">
We apply within-sentence event extraction to get
an initial set of event mentions EMSett , and con-
</bodyText>
<equation confidence="0.390035">
0
</equation>
<bodyText confidence="0.976772333333333">
duct cross-sentence inference (details will be pre-
sented in section 5) to get an updated set of event
mentions EMSett .
</bodyText>
<sectionHeader confidence="0.426972" genericHeader="method">
1
</sectionHeader>
<subsectionHeader confidence="0.989046">
4.3 Information Retrieval
</subsectionHeader>
<bodyText confidence="0.99994">
We then use the INDRI retrieval system (Strohman
et al., 2005) to obtain the top N (N=25 in this pa-
</bodyText>
<figure confidence="0.999744423076923">
Within-sent
Event Extraction
Cross-sent
Inference
Test doc
Cross-doc
Inference
EMSett0
EMSett1
Query
Construction
Information
Retrieval
Unlabeled
Corpora
Query
EMSet1
r
Within-sent
Event Extraction
Cross-sent
Inference
Related
docs
EMSetr
0
</figure>
<figureCaption confidence="0.931675">
Figure 1. Cross-doc Inference for Event Extraction
</figureCaption>
<figure confidence="0.664244285714286">
4.2 Within-Sentence Event Extraction
For each event mention in a test document t , the
baseline Maximum Entropy based classifiers pro-
duce three types of confidence values:
EMSe
t2
t
</figure>
<page confidence="0.989594">
257
</page>
<bodyText confidence="0.9971021">
per3) related documents. We construct an INDRI
query from the triggers and arguments, each
weighted by local confidence and frequency in the
test document. For each argument we also add oth-
er names coreferential with or bearing some ACE
relation to the argument.
For each related document r returned by INDRI,
we repeat the within-sentence event extraction and
cross-sentence inference procedure, and get an ex-
panded event mention set EMSett +r . Then we apply
</bodyText>
<figure confidence="0.617976666666667">
1
cross-document inference to EMSett +r and get the
1final event mention output EMSett.
</figure>
<page confidence="0.752996">
2
</page>
<sectionHeader confidence="0.976555" genericHeader="method">
5 Global Inference
</sectionHeader>
<bodyText confidence="0.941341777777778">
The central idea of inference is to obtain docu-
ment-wide and cluster-wide statistics about the
frequency with which triggers and arguments are
associated with particular types of events, and then
use this information to correct event and argument
identification and classification.
For a set of event mentions we tabulate the fol-
lowing document-wide and cluster-wide confi-
dence-weighted frequencies:
</bodyText>
<listItem confidence="0.973015857142857">
• for each trigger string, the frequency with
which it appears as the trigger of an event of a
particular type;
• for each event argument string and the names
coreferential with or related to the argument,
the frequency of the event type;
• for each event argument string and the names
</listItem>
<bodyText confidence="0.8899244">
coreferential with or related to the argument,
the frequency of the event type and role.
Besides these frequencies, we also define the
following margin metric to compute the confi-
dence of the best (most frequent) event type or role:
</bodyText>
<equation confidence="0.692052">
Margin =
(WeightedFrequency (most frequent value)
</equation>
<bodyText confidence="0.991782222222222">
– WeightedFrequency (second most freq value))/
WeightedFrequency (second most freq value)
A large margin indicates greater confidence in
the most frequent value. We summarize the fre-
quency and confidence metrics in Table 3.
Based on these confidence metrics, we designed
the inference rules in Table 4. These rules are ap-
plied in the order (1) to (9) based on the principle
of improving ‘local’ information before global
</bodyText>
<footnote confidence="0.5542585">
3 We tested different N e [10, 75] on dev set; and N=25
achieved best gains.
</footnote>
<bodyText confidence="0.9647545">
propagation. Although the rules may seem com-
plex, they basically serve two functions:
</bodyText>
<listItem confidence="0.9958696">
• to remove triggers and arguments with low
(local or cluster-wide) confidence;
• to adjust trigger and argument identification
and classification to achieve (document-wide
or cluster-wide) consistency.
</listItem>
<sectionHeader confidence="0.945358" genericHeader="evaluation">
6 Experimental Results and Analysis
</sectionHeader>
<bodyText confidence="0.999412333333333">
In this section we present the results of applying
this inference method to improve ACE event ex-
traction.
</bodyText>
<subsectionHeader confidence="0.989081">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999916">
We used 10 newswire texts from ACE 2005 train-
ing corpora (from March to May of 2003) as our
development set, and then conduct blind test on a
separate set of 40 ACE 2005 newswire texts. For
each test text we retrieved 25 related texts from
English TDT5 corpus which in total consists of
278,108 texts (from April to September of 2003).
</bodyText>
<subsectionHeader confidence="0.997566">
6.2 Confidence Metric Thresholding
</subsectionHeader>
<bodyText confidence="0.997838714285714">
We select the thresholds (Sk with k=1~13) for vari-
ous confidence metrics by optimizing the F-
measure score of each rule on the development set,
as shown in Figure 2 and 3 as follows.
Each curve in Figure 2 and 3 shows the effect on
precision and recall of varying the threshold for an
individual rule.
</bodyText>
<figureCaption confidence="0.915188">
Figure 2. Trigger Labeling Performance with
Confidence Thresholding on Dev Set
</figureCaption>
<page confidence="0.985543">
258
</page>
<figureCaption confidence="0.980825">
Figure 3. Argument Labeling Performance with
Confidence Thresholding on Dev Set
</figureCaption>
<bodyText confidence="0.999978428571429">
The labeled point on each curve shows the best
F-measure that can be obtained on the develop-
ment set by adjusting the threshold for that rule.
The gain obtained by applying successive rules can
be seen in the progression of successive points to-
wards higher recall and, for argument labeling,
precision4.
</bodyText>
<subsectionHeader confidence="0.999582">
6.3 Overall Performance
</subsectionHeader>
<bodyText confidence="0.999749315789474">
Table 5 shows the overall Precision (P), Recall (R)
and F-Measure (F) scores for the blind test set. In
addition, we also measured the performance of two
human annotators who prepared the ACE 2005
training data on 28 newswire texts (a subset of the
blind test set). The final key was produced by re-
view and adjudication of the two annotations.
Both cross-sentence and cross-document infe-
rences provided significant improvement over the
baseline with local confidence thresholds con-
trolled.
We conducted the Wilcoxon Matched-Pairs
Signed-Ranks Test on a document basis. The re-
sults show that the improvement using cross-
sentence inference is significant at a 99.9% confi-
dence level for both trigger and argument labeling;
adding cross-document inference is significant at a
99.9% confidence level for trigger labeling and
93.4% confidence level for argument labeling.
</bodyText>
<footnote confidence="0.80707">
4 We didn’t show the classification adjusting rules (2), (6) and
(8) here because of their relatively small impact on dev set.
</footnote>
<subsectionHeader confidence="0.960825">
6.4 Discussion
</subsectionHeader>
<bodyText confidence="0.993529448979592">
From table 5 we can see that for trigger labeling
our approach dramatically enhanced recall (22.9%
improvement) with some loss (7.4%) in precision.
This precision loss was much larger than that for
the development set (0.3%). This indicates that the
trigger propagation thresholds optimized on the
development set were too low for the blind test set
and thus more spurious triggers got propagated.
The improved trigger labeling is better than one
human annotator and only 4.7% worse than anoth-
er.
For argument labeling we can see that cross-
sentence inference improved both identification
(3.7% higher F-Measure) and classification (6.1%
higher accuracy); and cross-document inference
mainly provided further gains (1.9%) in classifica-
tion. This shows that identification consistency
may be achieved within a narrower context while
the classification task favors more global back-
ground knowledge in order to solve some difficult
cases. This matches the situation of human annota-
tion as well: we may decide whether a mention is
involved in some particular event or not by reading
and analyzing the target sentence itself; but in or-
der to decide the argument’s role we may need to
frequently refer to wider discourse in order to infer
and confirm our decision. In fact sometimes it re-
quires us to check more similar web pages or even
wikipedia databases. This was exactly the intuition
of our approach. We should also note that human
annotators label arguments based on perfect entity
mentions, but our system used the output from the
IE system. So the gap was also partially due to
worse entity detection.
Error analysis on the inference procedure shows
that the propagation rules (3), (4), (7) and (9) pro-
duced a few extra false alarms. For trigger labe-
ling, most of these errors appear for support verbs
such as “take” and “get” which can only represent
an event mention together with other verbs or
nouns. Some other errors happen on nouns and
adjectives. These are difficult tasks even for human
annotators. As shown in table 5 the inter-annotator
agreement on trigger identification is only about
40%. Besides some obvious overlooked cases (it’s
probably difficult for a human to remember 33 dif-
ferent event types during annotation), most diffi-
culties were caused by judging generic verbs,
nouns and adjectives.
</bodyText>
<page confidence="0.992284">
259
</page>
<table confidence="0.999786166666667">
Performance Trigger Argument Argument Argument
System/Human Identification Identification Classification Identification
+Classification Accuracy +Classification
P R F P R F P R F
Within-Sentence IE with 67.6 53.5 59.7 47.8 38.3 42.5 86.0 41.2 32.9 36.6
Rule (1) (Baseline)
Cross-sentence Inference 64.3 59.4 61.8 54.6 38.5 45.1 90.2 49.2 34.7 40.7
Cross-sentence+ 60.2 76.4 67.3 55.7 39.5 46.2 92.1 51.3 36.4 42.6
Cross-doc Inference
Human Annotator1 59.2 59.4 59.3 60.0 69.4 64.4 85.8 51.6 59.5 55.3
Human Annotator2 69.2 75.0 72.0 62.7 85.4 72.3 86.3 54.1 73.7 62.4
Inter-Annotator Agreement 41.9 38.8 40.3 55.2 46.7 50.6 91.7 50.6 42.9 46.4
</table>
<tableCaption confidence="0.999388">
Table 5. Overall Performance on Blind Test Set (%)
</tableCaption>
<bodyText confidence="0.9999185">
In fact, compared to a statistical tagger trained on
the corpus after expert adjudication, a human an-
notator tends to make more mistakes in trigger
classification. For example it’s hard to decide
whether “named” represents a “Person-
nel_Nominate” or “Personnel_Start-Position”
event mention; “hacked to death” represents a
“Life_Die” or “Conflict_Attack” event mention
without following more specific annotation guide-
lines.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999979615384615">
The trigger labeling task described in this paper is
in part a task of word sense disambiguation
(WSD), so we have used the idea of sense consis-
tency introduced in (Yarowsky, 1995), extending
it to operate across related documents.
Almost all the current event extraction systems
focus on processing single documents and, except
for coreference resolution, operate a sentence at a
time (Grishman et al., 2005; Ahn, 2006; Hardy et
al., 2006).
We share the view of using global inference to
improve event extraction with some recent re-
search. Yangarber et al. (Yangarber and Jokipii,
2005; Yangarber, 2006; Yangarber et al., 2007)
applied cross-document inference to correct local
extraction results for disease name, location and
start/end time. Mann (2007) encoded specific infe-
rence rules to improve extraction of CEO (name,
start year, end year) in the MUC management
succession task. In addition, Patwardhan and Ri-
loff (2007) also demonstrated that selectively ap-
plying event patterns to relevant regions can
improve MUC event extraction. We expand the
idea to more general event types and use informa-
tion retrieval techniques to obtain wider back-
ground knowledge from related documents.
</bodyText>
<sectionHeader confidence="0.96084" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999977310344827">
One of the initial goals for IE was to create a da-
tabase of relations and events from the entire input
corpus, and allow further logical reasoning on the
database. The artificial constraint that extraction
should be done independently for each document
was introduced in part to simplify the task and its
evaluation. In this paper we propose a new ap-
proach to break down the document boundaries
for event extraction. We gather together event ex-
traction results from a set of related documents,
and then apply inference and constraints to en-
hance IE performance.
In the short term, the approach provides a plat-
form for many byproducts. For example, we can
naturally get an event-driven summary for the col-
lection of related documents; the sentences includ-
ing high-confidence events can be used as
additional training data to bootstrap the event tag-
ger; from related events in different timeframes
we can derive entailment rules; the refined consis-
tent events can serve better for other NLP tasks
such as template based question-answering. The
aggregation approach described here can be easily
extended to improve relation detection and corefe-
rence resolution (two argument mentions referring
to the same role of related events are likely to
corefer). Ultimately we would like to extend the
system to perform essential, although probably
lightweight, event prediction.
</bodyText>
<page confidence="0.855214">
260
</page>
<bodyText confidence="0.703656230769231">
XSent-Trigger-Freq(trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event
of type etype across all sentences within a document
XDoc-Trigger-Freq (trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event
of type etype across all documents in a cluster
XDoc-Trigger-BestFreq (trigger) Maximum over all etypes of XDoc-Trigger-Freq (trigger, etype)
XDoc-Arg-Freq(arg, etype) The weighted frequency of arg appearing as an argument of an event of type
etype across all documents in a cluster
XDoc-Role-Freq(arg, etype, role) The weighted frequency of arg appearing as an argument of an event of type
etype with role role across all documents in a cluster
XDoc-Role-BestFreq(arg) Maximum over all etypes and roles of XDoc-Role-Freq(arg, etype, role)
XSent-Trigger-Margin(trigger) The margin value of trigger in XSent-Trigger-Freq
XDoc-Trigger-Margin(trigger) The margin value of trigger in XDoc-Trigger-Freq
XDoc-Role-Margin(arg) The margin value of arg in XDoc-Role-Freq
</bodyText>
<tableCaption confidence="0.959985">
Table 3. Global Frequency and Confidence Metrics
</tableCaption>
<figureCaption confidence="0.985531807692308">
Rule (1): Remove Triggers and Arguments with Low Local Confidence
If LConf(trigger, etype) &lt; δ1, then delete the whole event mention EM;
If LConf(arg, etype) &lt; δ2 or LConf(arg, etype, role) &lt; δ3, then delete arg.
Rule (2): Adjust Trigger Classification to Achieve Document-wide Consistency
If XSent-Trigger-Margin(trigger) &gt;δ4, then propagate the most frequent etype to all event mentions with trigger in
the document; and correct roles for corresponding arguments.
Rule (3): Adjust Trigger Identification to Achieve Document-wide Consistency
If LConf(trigger, etype) &gt; δ5, then propagate etype to all unlabeled strings trigger in the document.
Rule (4): Adjust Argument Identification to Achieve Document-wide Consistency
If LConf(arg, etype) &gt; δ6, then in the document, for each sentence containing an event mention EM with etype, add
any unlabeled mention in that sentence with the same head as arg as an argument of EM with role.
Rule (5): Remove Triggers and Arguments with Low Cluster-wide Confidence
If XDoc-Trigger-Freq (trigger, etype) &lt; δ7, then delete EM;
If XDoc-Arg-Freq(arg, etype) &lt; δ8 or XDoc-Role-Freq(arg, etype, role) &lt; δ9, then delete arg.
Rule (6): Adjust Trigger Classification to Achieve Cluster-wide Consistency
If XDoc-Trigger-Margin(trigger) &gt;δ10, then propagate most frequent etype to all event mentions with trigger in the
cluster; and correct roles for corresponding arguments.
Rule (7): Adjust Trigger Identification to Achieve Cluster-wide Consistency
If XDoc-Trigger-BestFreq (trigger) &gt;δ11, then propagate etype to all unlabeled strings trigger in the cluster, override
the results of Rule (3) if conflict.
Rule (8): Adjust Argument Classification to Achieve Cluster-wide Consistency
If XDoc-Role-Margin(arg) &gt;δ12, then propagate the most frequent etype and role to all arguments with the same
head as arg in the entire cluster.
Rule (9): Adjust Argument Identification to Achieve Cluster-wide Consistency
If XDoc-Role-BestFreq(arg) &gt; δ13, then in the cluster, for each sentence containing an event mention EM with etype,
add any unlabeled mention in that sentence with the same head as arg as an argument of EM with role.
</figureCaption>
<tableCaption confidence="0.967565">
Table 4. Probabilistic Inference Rule
</tableCaption>
<sectionHeader confidence="0.999367" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.80109625">
This material is based upon work supported by the
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023, and the Na-
tional Science Foundation under Grant IIS-
00325657. Any opinions, findings and conclusions
expressed in this material are those of the authors
and do not necessarily reflect the views of the U. S.
Government.
</reference>
<page confidence="0.998529">
261
</page>
<sectionHeader confidence="0.995805" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878047619048">
David Ahn. 2006. The stages of event extraction. Proc.
COLING/ACL 2006 Workshop on Annotating and
Reasoning about Time and Events. Sydney, Aus-
tralia.
Ralph Grishman, David Westbrook and Adam Meyers.
2005. NYU’s English ACE 2005 System Descrip-
tion. Proc. ACE 2005 Evaluation Workshop. Wash-
ington, US.
Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-
kowski. 2006. Automatic Event Classification Us-
ing Surface Text Features. Proc. AAAI06 Workshop
on Event Extraction and Synthesis. Boston, Massa-
chusetts. US.
Gideon Mann. 2007. Multi-document Relationship Fu-
sion via Constraints on Probabilistic Databases.
Proc. HLT/NAACL 2007. Rochester, NY, US.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
Information Extraction with Semantic Affinity Pat-
terns and Relevant Regions. Proc. EMNLP 2007.
Prague, Czech Republic.
Trevor Strohman, Donald Metzler, Howard Turtle and
W. Bruce Croft. 2005. Indri: A Language-model
based Search Engine for Complex Queries (ex-
tended version). Technical Report IR-407, CIIR,
Umass Amherst, US.
Roman Yangarber, Clive Best, Peter von Etter, Flavio
Fuart, David Horby and Ralf Steinberger. 2007.
Combining Information about Epidemic Threats
from Multiple Sources. Proc. RANLP 2007 work-
shop on Multi-source, Multilingual Information Ex-
traction and Summarization. Borovets, Bulgaria.
Roman Yangarber. 2006. Verification of Facts across
Document Boundaries. Proc. International Work-
shop on Intelligent Information Access. Helsinki,
Finland.
Roman Yangarber and Lauri Jokipii. 2005. Redundan-
cy-based Correction of Automatically Extracted
Facts. Proc. HLT/EMNLP 2005. Vancouver, Cana-
da.
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. Proc.
ACL 1995. Cambridge, MA, US.
</reference>
<page confidence="0.997291">
262
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979498">
<title confidence="0.999261">Refining Event Extraction through Cross-document Inference</title>
<author confidence="0.99992">Heng Ji Ralph Grishman</author>
<affiliation confidence="0.996694">Computer Science Department New York University</affiliation>
<address confidence="0.999958">New York, NY 10003, USA</address>
<email confidence="0.999913">(hengji,grishman)@cs.nyu.edu</email>
<abstract confidence="0.999244666666667">We apply the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995) to information extraction (IE), and extend the scope of “discourse” from one single document to a cluster of topically-related documents. We employ a similar approach to propagate consistent event arguments across sentences and documents. Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event ex- Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This material is based upon work supported by the Defense Advanced Research Projects Agency under Contract No. HR0011-06-C-0023, and the National Science Foundation under Grant IIS00325657. Any opinions, findings and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the U.</title>
<journal>S. Government.</journal>
<marker></marker>
<rawString>This material is based upon work supported by the Defense Advanced Research Projects Agency under Contract No. HR0011-06-C-0023, and the National Science Foundation under Grant IIS00325657. Any opinions, findings and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the U. S. Government.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ahn</author>
</authors>
<title>The stages of event extraction.</title>
<date>2006</date>
<booktitle>Proc. COLING/ACL 2006 Workshop on Annotating and Reasoning about Time and Events.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="22029" citStr="Ahn, 2006" startWordPosition="3530" endWordPosition="3531"> “Personnel_Start-Position” event mention; “hacked to death” represents a “Life_Die” or “Conflict_Attack” event mention without following more specific annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve</context>
</contexts>
<marker>Ahn, 2006</marker>
<rawString>David Ahn. 2006. The stages of event extraction. Proc. COLING/ACL 2006 Workshop on Annotating and Reasoning about Time and Events. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>David Westbrook</author>
<author>Adam Meyers</author>
</authors>
<date>2005</date>
<booktitle>NYU’s English ACE 2005 System Description. Proc. ACE 2005 Evaluation Workshop.</booktitle>
<location>Washington, US.</location>
<contexts>
<context position="4692" citStr="Grishman et al., 2005" startWordPosition="713" endWordPosition="716">n Chief Role = Wednesday Time-within Table 1. Event Extraction Example We define the following standards to determine the correctness of an event mention: • A trigger is correctly labeled if its event type and offsets match a reference trigger. • An argument is correctly identified if its event type and offsets match any of the reference argument mentions. • An argument is correctly identified and classified if its event type, offsets, and role match any of the reference argument mentions. 2.2 A Baseline Within-Sentence Event Tagger We use a state-of-the-art English IE system as our baseline (Grishman et al., 2005). This system extracts events independently for each sentence. Its training and test procedures are as follows. The system combines pattern matching with statistical models. For every event mention in the ACE training corpus, patterns are constructed based on the sequences of constituent heads separating the trigger and arguments. In addition, a set of Maximum Entropy based classifiers are trained: • Trigger Labeling: to distinguish event mentions from non-event-mentions, to classify event mentions by type; • Argument Classifier: to distinguish arguments from non-arguments; • Role Classifier: </context>
<context position="22018" citStr="Grishman et al., 2005" startWordPosition="3526" endWordPosition="3529">“Personnel_Nominate” or “Personnel_Start-Position” event mention; “hacked to death” represents a “Life_Die” or “Conflict_Attack” event mention without following more specific annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions </context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>Ralph Grishman, David Westbrook and Adam Meyers. 2005. NYU’s English ACE 2005 System Description. Proc. ACE 2005 Evaluation Workshop. Washington, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilda Hardy</author>
<author>Vika Kanchakouskaya</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>Automatic Event Classification Using Surface Text Features.</title>
<date>2006</date>
<booktitle>Proc. AAAI06 Workshop on Event Extraction and Synthesis.</booktitle>
<location>Boston, Massachusetts. US.</location>
<contexts>
<context position="22050" citStr="Hardy et al., 2006" startWordPosition="3532" endWordPosition="3535">_Start-Position” event mention; “hacked to death” represents a “Life_Die” or “Conflict_Attack” event mention without following more specific annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction</context>
</contexts>
<marker>Hardy, Kanchakouskaya, Strzalkowski, 2006</marker>
<rawString>Hilda Hardy, Vika Kanchakouskaya and Tomek Strzalkowski. 2006. Automatic Event Classification Using Surface Text Features. Proc. AAAI06 Workshop on Event Extraction and Synthesis. Boston, Massachusetts. US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Mann</author>
</authors>
<title>Multi-document Relationship Fusion via Constraints on Probabilistic Databases.</title>
<date>2007</date>
<booktitle>Proc. HLT/NAACL 2007.</booktitle>
<location>Rochester, NY, US.</location>
<contexts>
<context position="22366" citStr="Mann (2007)" startWordPosition="3581" endWordPosition="3582">ency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use information retrieval techniques to obtain wider background knowledge from related documents. 8 Conclusion and Future Work One of the initial goals for IE was to create a database of relations and events from the entire input corpus, and allow further logical</context>
</contexts>
<marker>Mann, 2007</marker>
<rawString>Gideon Mann. 2007. Multi-document Relationship Fusion via Constraints on Probabilistic Databases. Proc. HLT/NAACL 2007. Rochester, NY, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions.</title>
<date>2007</date>
<booktitle>Proc. EMNLP</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="22538" citStr="Patwardhan and Riloff (2007)" startWordPosition="3606" endWordPosition="3610">single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use information retrieval techniques to obtain wider background knowledge from related documents. 8 Conclusion and Future Work One of the initial goals for IE was to create a database of relations and events from the entire input corpus, and allow further logical reasoning on the database. The artificial constraint that extraction should be done independently for each document was introduced in part to simplify the task and its eva</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. Proc. EMNLP 2007. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Strohman</author>
<author>Donald Metzler</author>
<author>Howard Turtle</author>
<author>W Bruce Croft</author>
</authors>
<title>Indri: A Language-model based Search Engine for Complex Queries (extended version).</title>
<date>2005</date>
<tech>Technical Report IR-407, CIIR,</tech>
<location>Umass Amherst, US.</location>
<contexts>
<context position="12995" citStr="Strohman et al., 2005" startWordPosition="2084" endWordPosition="2087"> if the event mention is produced by pattern matching then assign confidence 1. • LConf(arg, etype): The probability that a mention arg is an argument of some particular event type etype. • LConf(arg, etype, role): If arg is an argument with event type etype, the probability of arg having some particular role. We apply within-sentence event extraction to get an initial set of event mentions EMSett , and con0 duct cross-sentence inference (details will be presented in section 5) to get an updated set of event mentions EMSett . 1 4.3 Information Retrieval We then use the INDRI retrieval system (Strohman et al., 2005) to obtain the top N (N=25 in this paWithin-sent Event Extraction Cross-sent Inference Test doc Cross-doc Inference EMSett0 EMSett1 Query Construction Information Retrieval Unlabeled Corpora Query EMSet1 r Within-sent Event Extraction Cross-sent Inference Related docs EMSetr 0 Figure 1. Cross-doc Inference for Event Extraction 4.2 Within-Sentence Event Extraction For each event mention in a test document t , the baseline Maximum Entropy based classifiers produce three types of confidence values: EMSe t2 t 257 per3) related documents. We construct an INDRI query from the triggers and arguments,</context>
</contexts>
<marker>Strohman, Metzler, Turtle, Croft, 2005</marker>
<rawString>Trevor Strohman, Donald Metzler, Howard Turtle and W. Bruce Croft. 2005. Indri: A Language-model based Search Engine for Complex Queries (extended version). Technical Report IR-407, CIIR, Umass Amherst, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Clive Best</author>
<author>Peter von Etter</author>
<author>Flavio Fuart</author>
<author>David Horby</author>
<author>Ralf Steinberger</author>
</authors>
<title>Combining Information about Epidemic Threats from Multiple Sources.</title>
<date>2007</date>
<booktitle>Proc. RANLP 2007 workshop on Multi-source, Multilingual Information Extraction and Summarization. Borovets,</booktitle>
<location>Bulgaria.</location>
<marker>Yangarber, Best, von Etter, Fuart, Horby, Steinberger, 2007</marker>
<rawString>Roman Yangarber, Clive Best, Peter von Etter, Flavio Fuart, David Horby and Ralf Steinberger. 2007. Combining Information about Epidemic Threats from Multiple Sources. Proc. RANLP 2007 workshop on Multi-source, Multilingual Information Extraction and Summarization. Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Verification of Facts across Document Boundaries.</title>
<date>2006</date>
<booktitle>Proc. International Workshop on Intelligent Information Access.</booktitle>
<location>Helsinki, Finland.</location>
<contexts>
<context position="1868" citStr="Yangarber (2006)" startWordPosition="265" endWordPosition="266">tunately, many of these events will be reported multiple times, in different forms, both within the same document and within topicallyrelated documents (i.e. a collection of documents sharing participants in potential events). We can 1 http://www.nist.gov/speech/tests/ace/ take advantage of these alternate descriptions to improve event extraction in the original document, by favoring consistency of interpretation across sentences and documents. Several recent studies involving specific event types have stressed the benefits of going beyond traditional singledocument extraction; in particular, Yangarber (2006) has emphasized this potential in his work on medical information extraction. In this paper we demonstrate that appreciable improvements are possible over the variety of event types in the ACE (Automatic Content Extraction) evaluation through the use of cross-sentence and cross-document evidence. As we shall describe below, we can make use of consistency at several levels: consistency of word sense across different instances of the same word in related documents, and consistency of arguments and roles across different mentions of the same or related events. Such methods allow us to build dynam</context>
<context position="22213" citStr="Yangarber, 2006" startWordPosition="3560" endWordPosition="3561">elated Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use information retrieval techniques to obtain wider background knowledge from related documents. 8 Conclusion</context>
</contexts>
<marker>Yangarber, 2006</marker>
<rawString>Roman Yangarber. 2006. Verification of Facts across Document Boundaries. Proc. International Workshop on Intelligent Information Access. Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Lauri Jokipii</author>
</authors>
<title>Redundancy-based Correction of Automatically Extracted Facts.</title>
<date>2005</date>
<booktitle>Proc. HLT/EMNLP</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="22196" citStr="Yangarber and Jokipii, 2005" startWordPosition="3556" endWordPosition="3559">ic annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use information retrieval techniques to obtain wider background knowledge from related docume</context>
</contexts>
<marker>Yangarber, Jokipii, 2005</marker>
<rawString>Roman Yangarber and Lauri Jokipii. 2005. Redundancy-based Correction of Automatically Extracted Facts. Proc. HLT/EMNLP 2005. Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>Proc. ACL</booktitle>
<location>Cambridge, MA, US.</location>
<contexts>
<context position="21790" citStr="Yarowsky, 1995" startWordPosition="3494" endWordPosition="3495">fact, compared to a statistical tagger trained on the corpus after expert adjudication, a human annotator tends to make more mistakes in trigger classification. For example it’s hard to decide whether “named” represents a “Personnel_Nominate” or “Personnel_Start-Position” event mention; “hacked to death” represents a “Life_Die” or “Conflict_Attack” event mention without following more specific annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific infere</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. Proc. ACL 1995. Cambridge, MA, US.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>