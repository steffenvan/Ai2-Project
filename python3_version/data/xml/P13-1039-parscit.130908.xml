<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000519">
<title confidence="0.9641155">
Probabilistic Domain Modelling With Contextualized Distributional
Semantic Vectors
</title>
<author confidence="0.99926">
Jackie Chi Kit Cheung
</author>
<affiliation confidence="0.999536">
University of Toronto
</affiliation>
<address confidence="0.994573">
10 King’s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
</address>
<email confidence="0.999304">
jcheung@cs.toronto.edu
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999822619047619">
Generative probabilistic models have been
used for content modelling and template
induction, and are typically trained on
small corpora in the target domain. In
contrast, vector space models of distribu-
tional semantics are trained on large cor-
pora, but are typically applied to domain-
general lexical disambiguation tasks. We
introduce Distributional Semantic Hidden
Markov Models, a novel variant of a hid-
den Markov model that integrates these
two approaches by incorporating contex-
tualized distributional semantic vectors
into a generative model as observed emis-
sions. Experiments in slot induction show
that our approach yields improvements in
learning coherent entity clusters in a do-
main. In a subsequent extrinsic evalua-
tion, we show that these improvements are
also reflected in multi-document summa-
rization.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99976825">
Detailed domain knowledge is crucial to many
NLP tasks, either as an input for language un-
derstanding, or as the goal itself, to acquire such
knowledge. For example, in information extrac-
tion, a list of slots in the target domain is given
to the system, and in natural language generation,
content models are trained to learn the content
structure of texts in the target domain for infor-
mation structuring and automatic summarization.
Generative probabilistic models have been one
popular approach to content modelling. An impor-
tant advantage of this approach is that the structure
of the model can be adapted to fit the assumptions
about the structure of the domain and the nature
of the end task. As this field has progressed, the
formal structures that are assumed to represent a
</bodyText>
<author confidence="0.696551">
Gerald Penn
</author>
<affiliation confidence="0.964762">
University of Toronto
</affiliation>
<address confidence="0.979517">
10 King’s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
</address>
<email confidence="0.994221">
gpenn@cs.toronto.edu
</email>
<bodyText confidence="0.999915731707317">
domain have increased in complexity and become
more hierarchical. Earlier work assumes a flat set
of topics (Barzilay and Lee, 2004), which are ex-
pressed as states of a latent random variable in the
model. Later work organizes topics into a hierar-
chy from general to specific (Haghighi and Van-
derwende, 2009; Celikyilmaz and Hakkani-Tur,
2010). Recently, Cheung et al. (2013) formalized
a domain as a set of frames consisting of proto-
typical sequences of events, slots, and slot fillers
or entities, inspired by classical AI work such as
Schank and Abelson’s (1977) scripts. We adopt
much of this terminology in this work. For exam-
ple, in the CRIMINAL INVESTIGATIONS domain,
there may be events such as a murder, an investi-
gation of the crime, an arrest, and a trial. These
would be indicated by event heads such as kill, ar-
rest, charge, plead. Relevant slots would include
VICTIM, SUSPECT, AUTHORITIES, PLEA, etc.
One problem faced by this line of work is that,
by their nature, these models are typically trained
on a small corpus from the target domain, on the
order of hundreds of documents. The small size of
the training corpus makes it difficult to estimate re-
liable statistics, especially for more powerful fea-
tures such as higher-order N-gram features or syn-
tactic features.
By contrast, distributional semantic models are
trained on large, domain-general corpora. These
methods model word meaning using the contexts
in the training corpus in which the word appears.
The most popular approach today is a vector space
representation, in which each dimension corre-
sponds to some context word, and the value at that
dimension corresponds to the strength of the as-
sociation between the context word and the target
word being modelled. A notion of word similarity
arises naturally from these models by comparing
the similarity of the word vectors, for example by
using a cosine measure. Recently, these models
have been extended by considering how distribu-
</bodyText>
<page confidence="0.963829">
392
</page>
<note confidence="0.914343">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392–401,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999964177777778">
tional representations can be modified depending
on the specific context in which the word appears
(Mitchell and Lapata, 2008, for example). Con-
textualization has been found to improve perfor-
mance in tasks like lexical substitution and word
sense disambiguation (Thater et al., 2011).
In this paper, we propose to inject contextual-
ized distributional semantic vectors into genera-
tive probabilistic models, in order to combine their
complementary strengths for domain modelling.
There are a number of potential advantages that
distributional semantic models offer. First, they
provide domain-general representations of word
meaning that cannot be reliably estimated from the
small target-domain corpora on which probabilis-
tic models are trained. Second, the contextualiza-
tion process allows the semantic vectors to implic-
itly encode disambiguated word sense and syntac-
tic information, without further adding to the com-
plexity of the generative model.
Our model, the Distributional Semantic Hidden
Markov Model (DSHMM), incorporates contextu-
alized distributional semantic vectors into a gen-
erative probabilistic model as observed emissions.
We demonstrate the effectiveness of our model in
two domain modelling tasks. First, we apply it to
slot induction on guided summarization data over
five different domains. We show that our model
outperforms a baseline version of our method that
does not use distributional semantic vectors, as
well as a recent state-of-the-art template induction
method. Then, we perform an extrinsic evaluation
using multi-document summarization, wherein we
show that our model is able to learn event and slot
topics that are appropriate to include in a sum-
mary. From a modelling perspective, these results
show that probabilistic models for content mod-
elling and template induction benefit from distri-
butional semantics trained on a much larger cor-
pus. From the perspective of distributional seman-
tics, this work broadens the variety of problems to
which distributional semantics can be applied, and
proposes methods to perform inference in a prob-
abilistic setting beyond geometric measures such
as cosine similarity.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999836563636364">
Probabilistic content models were proposed by
Barzilay and Lee (2004), and related models have
since become popular for summarization (Fung
and Ngai, 2006; Haghighi and Vanderwende,
2009), and information ordering (Elsner et al.,
2007; Louis and Nenkova, 2012). Other related
generative models include topic models and struc-
tured versions thereof (Blei et al., 2003; Gruber
et al., 2007; Wallach, 2008). In terms of domain
learning in the form of template induction, heuris-
tic methods involving multiple clustering steps
have been proposed (Filatova et al., 2006; Cham-
bers and Jurafsky, 2011). Most recently, Cheung
et al. (2013) propose PROFINDER, a probabilis-
tic model for frame induction inspired by content
models. Our work is similar in that we assume
much of the same structure within a domain and
consequently in the model as well (Section 3), but
whereas PROFINDER focuses on finding the “cor-
rect” number of frames, events, and slots with a
nonparametric method, this work focuses on in-
tegrating global knowledge in the form of distri-
butional semantics into a probabilistic model. We
adopt one of their evaluation procedures and use it
to compare with PROFINDER in Section 5.
Vector space models form the basis of modern
information retrieval (Salton et al., 1975), but only
recently have distributional models been proposed
that are compositional (Mitchell and Lapata, 2008;
Clark et al., 2008; Grefenstette and Sadrzadeh,
2011, inter alia), or that contextualize the meaning
of a word using other words in the same phrase
(co-compositionality) (Erk and Pad´o, 2008; Dinu
and Lapata, 2010; Thater et al., 2011). We re-
cently showed how such models can be evaluated
for their ability to support semantic inference for
use in complex NLP tasks like question answering
or automatic summarization (Cheung and Penn,
2012).
Combining distributional information and prob-
abilistic models has actually been explored in pre-
vious work. Usually, an ad-hoc clustering step
precedes training and is used to bias the initializa-
tion of the probabilistic model (Barzilay and Lee,
2004; Louis and Nenkova, 2012), or the clustering
is interleaved with iterations of training (Fung et
al., 2003). By contrast, our method better modu-
larizes the two, and provides a principled way to
train the model. More importantly, previous ad-
hoc clustering methods only use distributional in-
formation derived from the target domain itself;
initializing based on domain-general distributional
information can be problematic because it can bias
training towards a local optimum that is inappro-
priate for the target domain, leading to poor per-
</bodyText>
<page confidence="0.999467">
393
</page>
<figureCaption confidence="0.981218333333333">
Figure 1: Graphical representation of our model.
Distributions that generate the latent variables and
hyperparameters are omitted for clarity.
</figureCaption>
<bodyText confidence="0.88489">
formance.
</bodyText>
<sectionHeader confidence="0.7739855" genericHeader="method">
3 Distributional Semantic Hidden
Markov Models
</sectionHeader>
<bodyText confidence="0.993705266666667">
We now describe the DSHMM model. This model
can be thought of as an HMM with two layers
of latent variables, representing events and slots
in the domain. Given a document consisting of
a sequence of T clauses headed by propositional
heads H� (verbs or event nouns), and argument
noun phrases A, a DSHMM models the joint prob-
ability of observations H, A, and latent random
variables E� and S� representing domain events and
slots respectively; i.e., P( H, A, E, 5).
The basic structure of our model is similar to
PROFINDER. Each timestep in the model gener-
ates one clause in the document. More specifi-
cally, it generates the event heads and arguments
which are crucial in identifying events and slots.
We assume that event heads are verbs or event
nouns, while arguments are the head words of their
syntactically dependent noun phrases. We also as-
sume that the sequence of clauses and the clause-
internal syntactic structure are fixed, for example
by applying a dependency parser. Within each
clause, a hierarchy of latent and observed variables
maps to corresponding elements in the clause (Ta-
ble 1), as follows:
Event Variables At the top-level, a categorical
latent variable Et with NE possible states repre-
sents the event that is described by clause t. Its
value is conditioned on the previous time step’s
event variable, following the standard, first-order
Markov assumption (PE(Et|Et−1), or PEinit(E1)
</bodyText>
<table confidence="0.935773">
Node Component Textual unit
Et Event Clause
Sta Slot Noun phrase
Ht Event head Verb/event noun
Ata Event argument Noun phrase
</table>
<tableCaption confidence="0.998342">
Table 1: The correspondence between nodes in our
</tableCaption>
<bodyText confidence="0.988157148148148">
graphical model, the domain components that they
model, and the related elements in the clause.
for the first clause). The internal structure of the
clause is generated by conditioning on the state of
Et, including the head of the clause, and the slots
for each argument in the clause.
Slot Variables Categorical latent variables with
NS possible states represent the slot that an argu-
ment fills, and are conditioned on the event vari-
able in the clause, Et (i.e., PS(Sta|Et), for the
ath slot variable). The state of Sta is then used to
generate an argument Ata.
Head and Argument Emissions The head of
the clause Ht is conditionally dependent on Et,
and each argument Ata is likewise conditioned on
its slot variable Sta. Unlike in most applications of
HMMs in text processing, in which the represen-
tation of a token is simply its word or lemma iden-
tity, tokens in DSHMM are also associated with a
vector representation of their meaning in context
according to a distributional semantic model (Sec-
tion 3.1). Thus, the emissions can be decomposed
into pairs Ht = (lemma(Ht), sem(Ht)) and
Ata = (lemma(Ata), sem(Ata)), where lemma
and sem are functions that return the lemma iden-
tity and the semantic vector respectively. The
probability of the head of a clause is thus:
</bodyText>
<equation confidence="0.9918905">
PH(Ht|Et) = PHlemm(lemma(Ht)|Et) (1)
× P Hsem(sem(Ht)|Et),
</equation>
<bodyText confidence="0.9801845">
and the probability of a clausal argument is like-
wise:
</bodyText>
<equation confidence="0.986818">
PA(Ata|Sta) = PAlemm(lemma(Ata)|Sta) (2)
× PAsem(sem(Ata)|Sta).
</equation>
<bodyText confidence="0.99943825">
All categorical distributions are smoothed using
add-δ smoothing (i.e., uniform Dirichlet priors).
Based on the independence assumptions described
above, the joint probability distribution can be fac-
</bodyText>
<figure confidence="0.997829333333333">
C1
PlemmA PA
sem
S1
A1
E1
H1
NS
PH PH
lemm sem
. . .
CT
AN
ST
ET
HT
Ng
D
</figure>
<page confidence="0.812774">
394
</page>
<equation confidence="0.535486">
tored into:
P( ~H, ~A, ~E, ~S)= PE init(E1) (3)
</equation>
<subsectionHeader confidence="0.963465">
3.1 Vector Space Models of Semantics
</subsectionHeader>
<bodyText confidence="0.995017527777778">
In this section, we describe several methods for
producing the semantic vectors associated with
each event head or argument; i.e., the function
sem. We chose several simple, but widely studied
models, to investigate whether they can be effec-
tively integrated into DSHMM. We start with a de-
scription of the training of a basic model without
any contextualization, then describe several con-
textualized models based on recent work.
Simple Vector Space Model In the basic ver-
sion of the model (SIMPLE), we train a term-
context matrix, where rows correspond to target
words, and columns correspond to context words.
Training begins by counting context words that ap-
pear within five words of the target word, ignor-
ing stopwords. We then convert the raw counts
to positive pointwise mutual information scores,
which has been shown to improve word similarity
correlation results (Turney and Pantel, 2010). We
set thresholds on the frequencies of words for in-
clusion as target and context words (given in Sec-
tion 4). Target words which fall below the thresh-
old are modelled as UNK. All the methods below
start from this basic vector representation.
Component-wise Operators Mitchell and Lap-
ata (2008) investigate using component-wise op-
erators to combine the vectors of verbs and their
intransitive subjects. We use component-wise op-
erators to contextualize our vectors, but by com-
bining with all of the arguments, and regardless
of the event head’s category. Let event head h
be the syntactic head of a number of arguments
a1, a2, ...am, and ~vh,~va1,~va2, ...~vam be their re-
spective vector representations according to the
SIMPLE method. Then, their contextualized vec-
tors ~cM&amp;L
</bodyText>
<equation confidence="0.984833285714286">
h ,~cM&amp;L
a1 , ...~cM&amp;L
am would be:
~cM&amp;L
h = ~vh � (
~cM&amp;L
ai = ~vai O ~vh, ∀i = 1...m, (5)
</equation>
<bodyText confidence="0.99837835">
where (D represents a component-wise operator,
addition or multiplication, and G) represents its
repeated application. We tested component-wise
addition (M&amp;L+) and multiplication (M&amp;L×).
Selectional Preferences Erk and Pad´o (2008)
(E&amp;P) incorporate inverse selectional preferences
into their contextualization function. The intu-
ition is that a word should be contextualized such
that its vector representation becomes more sim-
ilar to the vectors of other words that its depen-
dency neighbours often take in the same syntactic
position. For example, suppose catch is the head
of the noun ball, in the relation of a direct object.
Then, the vector for ball would be contextualized
to become similar to the vectors for other frequent
direct objects of catch, such as baseball, or cold.
Likewise, the vector for catch would be contextu-
alized to become similar to the vectors for throw,
hit, etc. Formally, let h take a as its argument in
relation r. Then:
</bodyText>
<equation confidence="0.9986875">
freq(w, r, ai) · ~vw, (6)
freq(h, r, w) · ~vw, (7)
</equation>
<bodyText confidence="0.991401923076923">
where freq(h, r, a) is the frequency of h occur-
ring as the head of a in relation r in the train-
ing corpus, L is the lexicon, and × represents
component-wise multiplication.
Dimensionality Reduction and Vector Emission
After contextualization, we apply singular value
decomposition (SVD) for dimensionality reduc-
tion to reduce the number of model parameters,
keeping the k most significant singular values and
vectors. In particular, we apply SVD to the m-by-
n term-context matrix M produced by the SIM-
PLE method, resulting in the truncated matrices
M ≈ UkEkVkT, where Uk is a m-by-k matrix, Ek
is k-by-k, and Vk is n-by-k. This takes place af-
ter contextualization, so the component-wise op-
erators apply in the original semantic space. Af-
terwards, the contextualized vector in the original
space, ~c, can be transformed into a vector in the
reduced space, ~cR, by ~cR = E−1
k VkT ~c.
Distributional semantic vectors are traditionally
compared by measures which ignore vector mag-
nitudes, such as cosine similarity, but a multivari-
ate Gaussian is sensitive to magnitudes. Thus, the
final step is to normalize ~cR into a unit vector by
dividing it by its L2 norm, ||~cR||
</bodyText>
<equation confidence="0.894871208333333">
T T
× H PE(Et|Et−1) H PH(Ht|Et)
t=2 t=1
PS(Sta|Et)PA(Ata|Sta).
×
Ct
H
a=1
T
H
t=1
m
0 ~vam) (4)
i=1
~cE&amp;P h = ~vh ×
Hm
i=1
�
wEL
�= ~va ×
wEL
~cE&amp;P
a
.
</equation>
<page confidence="0.990654">
395
</page>
<bodyText confidence="0.999797636363636">
We model the emission of these contextualized
vectors in DSHMM as multivariate Gaussian dis-
tributions, so the semantic vector emissions can be
written as P Hsem, P Asem — N(µ, E), where µ E Rk
is the mean and E E Rk×k is the covariance
matrix. To avoid overfitting, we regularize the
covariance using its conjugate prior, the Inverse-
Wishart distribution. We follow the “neutral” set-
ting of hyperparameters given by Ormoneit and
Tresp (1995), so that the MAP estimate for the co-
variance matrix for (event or slot) state i becomes:
</bodyText>
<equation confidence="0.988214">
� jrij(xj — µi)(xj — µi)T + 0J (8)
i = Ej rij + 1
</equation>
<bodyText confidence="0.999987571428571">
where j indexes all the relevant semantic vectors
xj in the training set, rij is the posterior respon-
sibility of state i for vector xj, and 0 is the re-
maining hyperparameter that we tune to adjust the
amount of regularization. To further reduce model
complexity, we set the off-diagonal entries of the
resulting covariance matrix to zero.
</bodyText>
<subsectionHeader confidence="0.999221">
3.2 Training and Inference
</subsectionHeader>
<bodyText confidence="0.999121">
Inference in DSHMM is accomplished by the stan-
dard Inside-Outside and tree-Viterbi algorithms,
except that the tree structure is fixed, so there
is no need to sum over all possible subtrees.
Model parameters are learned by the Expectation-
Maximization (EM) algorithm. We tune the hy-
perparameters (NE, NS, δ, 0, k) and the number
of EM iterations by two-fold cross-validation1.
</bodyText>
<subsectionHeader confidence="0.995533">
3.3 Summary and Generative Process
</subsectionHeader>
<bodyText confidence="0.993866">
In summary, the following steps are applied to
train a DSHMM:
</bodyText>
<listItem confidence="0.998678615384615">
1. Train a distributional semantic model on a
large, domain-general corpus.
2. Preprocess and generate contextualized vec-
tors of event heads and arguments in the
small corpus in the target domain.
3. Train the DSHMM using the EM algorithm.
The formal generative process is as follows:
1. Draw categorical distributions PEinit;
PE, PS, PHlemm (one per event state);
PAlemm (one per slot state) from Dirichlet
priors.
2. Draw multivariate Gaussians P Hsem, PAsem for
each event and slot state, respectively.
</listItem>
<footnote confidence="0.929982">
1The topic cluster splits and the hyperparameter set-
tings are available at http://www.cs.toronto.edu/
˜jcheung/dshmm/dshmm.html.
</footnote>
<listItem confidence="0.9826651">
3. Generate the documents, clause by clause.
Generating a clause at position t consists of
these steps:
1. Generate the event state Et _ PE (or PEinit).
2. Generate the event head components
lemm(Ht) ti PHlemm, sem(Ht) — P Hsem.
3. Generate a number of slot states Sta — PS.
4. For each slot, generate the argument compo-
nents lemm(Ata) _ PAlemm, sem(Ata) —
PAsem.
</listItem>
<sectionHeader confidence="0.999239" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999991805555556">
We trained the distributional semantic models us-
ing the Annotated Gigaword corpus (Napoles et
al., 2012), which has been automatically prepro-
cessed and is based on Gigaword 5th edition. This
corpus contains almost ten million news articles
and more than 4 billion tokens. We used those ar-
ticles marked as “stories” — the vast majority of
them. We modelled the 50,000 most common lem-
mata as target words, and the 3,000 most common
lemmata as context words.
We then trained DSHMM and conducted our
evaluations on the TAC 2010 guided summa-
rization data set (Owczarzak and Dang, 2010).
Lemmatization and extraction of event heads and
arguments are done by preprocessing with the
Stanford CoreNLP tool suite (Toutanova et al.,
2003; de Marneffe et al., 2006). This data set con-
tains 46 topic clusters of 20 articles each, grouped
into five topic categories or domains. For exam-
ple, one topic cluster in the ATTACK category is
about the Columbine Massacre. Each topic cluster
contains eight human-written “model” summaries
(“model” here meaning a gold standard). Half of
the articles and model summaries in a topic cluster
are used in the guided summarization task, and the
rest are used in the update summarization task.
We chose this data set because it allows us
to conduct various domain-modelling evaluations.
First, templates for the domains are provided, and
the model summaries are annotated with slots
from the template, allowing for an intrinsic eval-
uation of slot induction (Section 5). Second, it
contains multiple domain instances for each of the
domains, and each domain instance comes anno-
tated with eight model summaries, allowing for an
extrinsic evaluation of our system (Section 6).
</bodyText>
<page confidence="0.999057">
396
</page>
<sectionHeader confidence="0.995088" genericHeader="method">
5 Guided Summarization Slot Induction
</sectionHeader>
<bodyText confidence="0.99977532">
We first evaluated our models on their ability to
produce coherent clusters of entities belonging to
the same slot, adopting the experimental proce-
dure of Cheung et al. (2013).
As part of the official TAC evaluation proce-
dure, model summaries were manually segmented
into contributors, and labelled with the slot in
the TAC template that the contributor expresses.
For example, a summary fragment such as On 20
April 1999, a massacre occurred at Columbine
High School is segmented into the contributors:
(On 20 April 1999, WHEN); (a massacre oc-
curred, WHAT); and (at Columbine High School,
WHERE).
In the slot induction evaluation, this annotation
is used as follows. First, the maximal noun phrases
are extracted from the contributors and clustered
based on the TAC slot of the contributor. These
clusters of noun phrases then become the gold
standard clusters against which automatic systems
are compared. Noun phrases are considered to be
matched if the lemmata of their head words are the
same and they are extracted from the same sum-
mary. This accounts for the fact that human an-
notators often only label the first occurrence of a
word that belongs to a slot in a summary, and fol-
lows the standard evaluation procedure in previ-
ous information extraction tasks, such as MUC-4.
Pronouns and demonstratives are ignored. This
extraction process is noisy, because the meaning
of some contributors depends on an entire verb
phrase, but we keep this representation to allow
a direct comparison to previous work.
Because we are evaluating unsupervised sys-
tems, the clusters produced by the systems are not
labelled, and must be matched to the gold stan-
dard clusters. This matching is performed by map-
ping to each gold cluster the best system cluster
according to F1. The same system cluster may be
mapped multiple times, because several TAC slots
can overlap. For example, in the NATURAL DIS-
ASTERS domain, an earthquake may fit both the
WHAT slot as well as the CAUSE slot, because it
generated a tsunami.
We trained a DSHMM separately for each of the
five domains with different semantic models, tun-
ing hyperparameters by two-fold cross-validation.
We then extracted noun phrase clusters from the
model summaries according to the slot labels pro-
duced by running the Viterbi algorithm on them.
</bodyText>
<table confidence="0.999832571428572">
Method P R F1
HMM w/o semantics 13.8 64.1 22.6*
DSHMM w/ SIMPLE 20.9 27.5 23.7
DSHMM w/ E&amp;P 20.7 27.9 23.8
PROFINDER 23.7 25.0 24.3
DSHMM w/ M&amp;L+ 19.7 36.3 25.6*
DSHMM w/ M&amp;L× 22.1 33.2 26.5*
</table>
<tableCaption confidence="0.996726">
Table 2: Slot induction results on the TAC guided
</tableCaption>
<bodyText confidence="0.959031837837838">
summarization data set. Asterisks (*) indicate
that the model is statistically significantly differ-
ent from PROFINDER in terms of F1 at p &lt; 0.05.
Results We compared DSHMM to two base-
lines. Our first baseline is PROFINDER, a state-
of-the-art template inducer which Cheung et al.
(2013) showed to outperform the previous heuris-
tic clustering method of Chambers and Jurafsky
(2011). Our second baseline is our DSHMM
model, without the semantic vector component,
(HMM w/o semantics). To calculate statistical
significance, we use the paired bootstrap method,
which can accommodate complex evaluation met-
rics like F1 (Berg-Kirkpatrick et al., 2012).
Table 2 shows that performance of the mod-
els. Overall, PROFINDER significantly outper-
forms the HMM baseline, but not any of the
DSHMM models by F1. DSHMM with contextu-
alized semantic vectors achieves the highest F1s,
and are significantly better than PROFINDER. All
of the differences in precision and recall between
PROFINDER and the other models are significant.
The baseline HMM model has highly imbalanced
precision and recall. We think this is because the
model is unable to successfully produce coher-
ent clusters, so the best-case mapping procedure
during evaluation picked large clusters that have
high recall. PROFINDER has slightly higher preci-
sion, which may be due to its non-parametric split-
merge heuristic. We plan to investigate whether
this learning method could improve DSHMM’s
performance further. Importantly, the contextual-
ization of the vectors seems to be beneficial, at
least with the M&amp;L component-wise operators.
In the next section, we show that the improve-
ment from contextualization transfers to multi-
document summarization results.
</bodyText>
<page confidence="0.998237">
397
</page>
<sectionHeader confidence="0.848937" genericHeader="method">
6 Multi-document Summarization: An
Extrinsic Evaluation
</sectionHeader>
<bodyText confidence="0.999996823529412">
We next evaluated our models extrinsically in the
setting of extractive, multi-document summariza-
tion. To use the trained DSHMM for extractive
summarization, we need a decoding procedure for
selecting sentences in the source text to include in
the summary. Inspired by the KLSUM and HI-
ERSUM methods of Haghighi and Vanderwende
(2009), we develop a criterion based on Kullback-
Leibler (KL) divergence between distributions es-
timated from the source text, and those estimated
from the summary. The assumption here is that
these distributions should match in a good sum-
mary. We describe two methods to use this crite-
rion: a basic unsupervised method (Section 6.1),
and a supervised variant that makes use of in-
domain summaries to learn the salient slots and
events in the domain (Section 6.2).
</bodyText>
<subsectionHeader confidence="0.999403">
6.1 A KL-based Criterion
</subsectionHeader>
<bodyText confidence="0.999788181818182">
There are four main component distributions from
our model that should be considered during extrac-
tion: (1) the distribution of events, (2) the distri-
bution of slots, (3) the distribution of event heads,
and (4) the distribution of arguments. We estimate
(1) as the context-independent probability of being
in a certain event state, which can be calculated
using the Inside-Outside algorithm. Given a col-
lection of documents D which make up the source
text, the distribution of event topics PˆE(E) is es-
timated as:
</bodyText>
<equation confidence="0.998875">
Int(e)Outt(e) (9)
P(d)
</equation>
<bodyText confidence="0.99959155">
where Int(e) and Outt(e) are the values of the
inside and outside trellises at timestep t for some
event state e, and Z is a normalization constant.
The distribution for a set of sentences in a can-
didate summary, ˆQE(E), is identical, except the
summation is over the clauses in the candidate
summary. Slot distributions PˆS(5) and ˆQS(5) (2)
are defined analogously, where the summation oc-
curs along all the slot variables.
For (3) and (4), we simply use the MLE es-
timates of the lemma emissions, where the esti-
mates are made over the source text and the can-
didate summary instead of over the entire train-
ing set. All of the candidate summary distribu-
tions (i.e., the “ Qˆ distributions”) are smoothed by
a small amount, so that the KL-divergence is al-
ways finite. Our KL criterion combines the above
components linearly, weighting the lemma distri-
butions by the probability of their respective event
or slot state:
</bodyText>
<equation confidence="0.997619125">
KL5core = (10)
PˆE ||ˆQE) + DKL( PˆS ||ˆQS)
NE
+ PˆE(e)DKL( PˆH(H|e)||ˆQH(H|e))
e=1
NS
+1: PˆS(s)DKL( PˆA(A|s) ||ˆQA(A|s))
s=1
</equation>
<bodyText confidence="0.9999686">
To produce a summary, sentences from the
source text are greedily added such that KL5core
is minimized at each step, until the desired sum-
mary length is reached, discarding sentences with
fewer than five words.
</bodyText>
<subsectionHeader confidence="0.999528">
6.2 Supervised Learning
</subsectionHeader>
<bodyText confidence="0.999963823529412">
The above unsupervised method results in sum-
maries that closely mirror the source text in terms
of the event and slot distributions, but this ig-
nores the fact that not all such topics should be
included in a summary. It also ignores genre-
specific, stylistic considerations about character-
istics of good summary sentences. For example,
Woodsend and Lapata (2012) find several factors
that indicate sentences should not be included in
an extractive summary, such as the presence of
personal pronouns. Thus, we implemented a sec-
ond method, in which we modify the KL criterion
above by estimating PˆE and PˆS from other model
summaries that are drawn from the same domain
(i.e. topic category), except for those summaries
that are written for the specific topic cluster to be
used for evaluation.
</bodyText>
<subsectionHeader confidence="0.99989">
6.3 Method and Results
</subsectionHeader>
<bodyText confidence="0.999946727272727">
We used the best performing models from the slot
induction task and the above unsupervised and su-
pervised methods based on KL-divergence to pro-
duce 100-word summaries of the guided summa-
rization source text clusters. We did not com-
pare against PROFINDER, as its structure is dif-
ferent and would have required a different proce-
dure than the KL-criterion we developed above.
As shown in the previous evaluation, however, the
HMM baseline without semantics and DSHMM
with SIMPLE perform similarly in terms of F1,
</bodyText>
<equation confidence="0.705395666666667">
Pˆ E(E = e) = 1 Z 1:
d∈D t
DKL(
</equation>
<page confidence="0.99037">
398
</page>
<table confidence="0.999852375">
Method ROUGE-1 ROUGE-2 ROUGE-SU4
unsup. sup. unsup. sup. unsup. sup.
Leading baseline 28.0 − 5.39 − 8.6 −
HMM w/o semantics 32.3 32.7 6.45 6.49 10.1 10.2
DSHMM w/ SIMPLE 32.1 32.7 5.81 6.50 9.8 10.2
DSHMM w/ M&amp;L+ 32.1 33.4 6.27 6.82 10.0 10.6
DSHMM w/ M&amp;Lx 32.4 34.3* 6.35 7.11&amp;quot; 10.2 11.0*
DSHMM w/ E&amp;P 32.8 33.8* 6.38 7.31* 10.3 10.8*
</table>
<tableCaption confidence="0.998482">
Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the
</tableCaption>
<bodyText confidence="0.986206149253732">
model is statistically significantly better than the HMM model without semantics at a 95% confidence
interval, a caret &amp;quot; indicates that the value is marginally so.
so we consider these competitive baselines. We
did not evaluate with the update summarization
task, because our method has not been adapted to
it. For the evaluation measure, we used the stan-
dard ROUGE suite of automatic evaluation mea-
sures (Lin, 2004). Note that the evaluation con-
ditions of TAC 2010 are different, and thus those
results are not directly comparable to ours. For in-
stance, top performing systems in TAC 2010 make
use of manually constructed lists of entities known
to fit the slots in the provided templates and sam-
ple topic statements, which our method automat-
ically learns. We include the leading baseline re-
sults from the competition as a point of reference,
as it is a well-known and non-trivial one for news
articles. This baseline summary consists of the
leading sentences from the most recent document
in the source text cluster up to the word length
limit.
Table 3 shows the summarization results for the
three most widely-used settings of ROUGE. All
of our models outperform the leading baseline by
a large margin, demonstrating the effective of the
KL-criterion. In terms of unsupervised perfor-
mance, all of our models perform similarly. Be-
cause the unsupervised method mimics the distri-
butions in the source text at all levels, the method
may negate the benefit of learning and simply pro-
duce summaries that match the source text in the
word distributions, thus being an approximation
of KLSUM. Looking at the supervised results,
however, the semantic vector models show clear
gains in ROUGE, whereas the baseline method
does not obtain much benefit from supervision. As
in the previous evaluation, the models with con-
textualized semantic vectors provide the best per-
formance. M&amp;Lx performs very well, as in slot
induction, but E&amp;P also performs well, unlike in
the previous evaluation. This result reinforces the
importance of the contextualization procedure for
distributional semantic models.
Analysis To better understand what is gained by
supervision using in-domain summaries, we ana-
lyzed the best performing M&amp;Lx model’s output
summaries for one document cluster from each
domain. For each event state, we calculated the
PˆE
summ(e)/Pˆ Esource(e), for the probability of
an event state e as estimated from the training
summaries and the the source text respectively.
Likewise, we calculated Pˆ S summ(s)/ Pˆ S source(s) for
the slot states. This ratio indicates the change in
state’s probability after supervision; the greater the
ratio, the more preferred that state becomes after
training. We selected the most preferred and dis-
preferred event and slot for each document clus-
ter, and took the three most probable lemmata
from the associated lemma distribution (Table 4).
It seems that supervision is beneficial because it
picks out important event heads and arguments in
the domain, such as charge, trial, and murder in
the TRIALS domain. It also helps the summarizer
avoid semantically generic words (be or have),
pronouns, quotatives, and common but irrelevant
words (home, city, restaurant in TRIALS).
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9748181">
We have shown that contextualized distributional
semantic vectors can be successfully integrated
into a generative probabilistic model for domain
modelling, as demonstrated by improvements in
slot induction and multi-document summariza-
tion. The effectiveness of our model stems from
the use of a large domain-general corpus to train
the distributional semantic vectors, and the im-
plicit syntactic and word sense information pro-
ratio
</bodyText>
<page confidence="0.994569">
399
</page>
<table confidence="0.999281666666667">
Domain Event Heads Slot Arguments
+ − + −
ATTACKS say2, cause, say2, be, have attack, hostage, he, it, they
doctor troops
TRIALS charge, trial, say, be, have prison, murder, home, city, restau-
accuse charge rant
RESOURCES reduce, increase, say, be, have government, he, they, it
university effort, program
DISASTERS flood, strengthen, say, be, have production, he, it, they
engulf statoil, barrel
HEALTH be, department, say, do, make food, product, she, people, way
have meat
</table>
<tableCaption confidence="0.929232">
Table 4: Analysis of the most probable event heads and arguments in the most preferred (+) and dispre-
ferred (−) events and slots after supervised training.
</tableCaption>
<bodyText confidence="0.999721384615385">
vided by the contextualization process. Our ap-
proach is modular, and allows principled train-
ing of the probabilistic model using standard tech-
niques. While we have focused on the overall clus-
tering of entities and the distribution of event and
slot topics in this work, we would also like to in-
vestigate discourse modelling and content struc-
turing. Finally, our work shows that the applica-
tion of distributional semantics to NLP tasks need
not be confined to lexical disambiguation. We
would like to see modern distributional semantic
methods incorporated into an even greater variety
of applications.
</bodyText>
<sectionHeader confidence="0.998649" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9824075">
This work is supported by the Natural Sciences
and Engineering Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.998085" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982860245283019">
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics: HLT-NAACL 2004.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statisti-
cal significance in NLP. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
2The event head say happens to appear in both the most
preferred and dispreferred events in the ATTACKS domain.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of
Machine Learning Research, 3.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 815–824, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 976–
986, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Jackie Chi Kit Cheung and Gerald Penn. 2012. Evalu-
ating distributional models of semantics for syntacti-
cally invariant inference. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 33–43,
Avignon, France, April. Association for Computa-
tional Linguistics.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
Second Quantum Interaction Symposium (QI-2008).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In LREC 2006.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In Proceed-
</reference>
<page confidence="0.966356">
400
</page>
<reference confidence="0.999435558558558">
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1162–1172.
Micha Elsner, Joseph Austerweil, and Eugene Char-
niak. 2007. A unified local and global model for
discourse coherence. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference,
Rochester, New York, April. Association for Com-
putational Linguistics.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897–
906.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of
domain templates. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 207–214, Sydney, Australia, July. Association
for Computational Linguistics.
Pascale Fung and Grace Ngai. 2006. One story, one
flow: Hidden markov story models for multilin-
gual multidocument summarization. ACM Transac-
tions on Speech and Language Processing (TSLP),
3(2):1–16.
Pascale Fung, Grace Ngai, and Chi-Shun Cheung.
2003. Combining optimal clustering and hidden
markov models for extractive summarization. In
Proceedings of the ACL 2003 Workshop on Multilin-
gual Summarization and Question Answering, pages
21–28, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394–
1404, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss.
2007. Hidden topic markov models. Artificial In-
telligence and Statistics (AISTATS).
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 362–370, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Chin Y. Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Stan Szpakowicz and
Marie-Francine Moens, editors, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244.
1992. Proceedings of the Fourth Message Understand-
ing Conference (MUC-4). Morgan Kaufmann.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the NAACL-HLT Joint Workshop on Au-
tomatic Knowledge Base Construction &amp; Web-scale
Knowledge Extraction (AKBC-WEKEX), pages 95–
100.
Dirk Ormoneit and Volker Tresp. 1995. Improved
gaussian mixture density estimates using bayesian
penalty terms and network averaging. In Advances
in Neural Information Processing, pages 542–548.
Karolina Owczarzak and Hoa T. Dang. 2010. TAC
2010 guided summarization task guidelines.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613–620.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding: An Inquiry Into
Human Knowledge Structures. Lawrence Erlbaum,
July.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of IJCNLP.
Kristina Toutanova, Dan Klein, Christoper D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, page 180.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Hanna M. Wallach. 2008. Structured topic models for
language. Doctoral dissertation, University of Cam-
bridge.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea, July. Association for
Computational Linguistics.
</reference>
<page confidence="0.998646">
401
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.745585">
<title confidence="0.9992925">Probabilistic Domain Modelling With Contextualized Semantic Vectors</title>
<author confidence="0.999824">Jackie Chi Kit</author>
<affiliation confidence="0.999154">University of</affiliation>
<address confidence="0.9677225">10 King’s College Rd., Room Toronto, ON, Canada M5S</address>
<email confidence="0.998957">jcheung@cs.toronto.edu</email>
<abstract confidence="0.9906175">Generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. In contrast, vector space models of distributional semantics are trained on large corpora, but are typically applied to domaingeneral lexical disambiguation tasks. We introduce Distributional Semantic Hidden Markov Models, a novel variant of a hidden Markov model that integrates these two approaches by incorporating contextualized distributional semantic vectors into a generative model as observed emissions. Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a domain. In a subsequent extrinsic evaluation, we show that these improvements are also reflected in multi-document summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</booktitle>
<contexts>
<context position="2081" citStr="Barzilay and Lee, 2004" startWordPosition="316" endWordPosition="319">d automatic summarization. Generative probabilistic models have been one popular approach to content modelling. An important advantage of this approach is that the structure of the model can be adapted to fit the assumptions about the structure of the domain and the nature of the end task. As this field has progressed, the formal structures that are assumed to represent a Gerald Penn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the CRIMINAL INVESTIGATIONS domain, there may be events such as a murder, an investigation of</context>
<context position="6327" citStr="Barzilay and Lee (2004)" startWordPosition="973" endWordPosition="976"> able to learn event and slot topics that are appropriate to include in a summary. From a modelling perspective, these results show that probabilistic models for content modelling and template induction benefit from distributional semantics trained on a much larger corpus. From the perspective of distributional semantics, this work broadens the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic mode</context>
<context position="8333" citStr="Barzilay and Lee, 2004" startWordPosition="1290" endWordPosition="1293"> that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializing based on domain-general distributional information can be problematic because it can bias training towards a local optimum that is inappropriate for the target domain, leading to poor per393 Figure 1: Graphical representation of our model. Distribu</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>An empirical investigation of statistical significance in NLP.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="23903" citStr="Berg-Kirkpatrick et al., 2012" startWordPosition="3873" endWordPosition="3876"> data set. Asterisks (*) indicate that the model is statistically significantly different from PROFINDER in terms of F1 at p &lt; 0.05. Results We compared DSHMM to two baselines. Our first baseline is PROFINDER, a stateof-the-art template inducer which Cheung et al. (2013) showed to outperform the previous heuristic clustering method of Chambers and Jurafsky (2011). Our second baseline is our DSHMM model, without the semantic vector component, (HMM w/o semantics). To calculate statistical significance, we use the paired bootstrap method, which can accommodate complex evaluation metrics like F1 (Berg-Kirkpatrick et al., 2012). Table 2 shows that performance of the models. Overall, PROFINDER significantly outperforms the HMM baseline, but not any of the DSHMM models by F1. DSHMM with contextualized semantic vectors achieves the highest F1s, and are significantly better than PROFINDER. All of the differences in precision and recall between PROFINDER and the other models are significant. The baseline HMM model has highly imbalanced precision and recall. We think this is because the model is unable to successfully produce coherent clusters, so the best-case mapping procedure during evaluation picked large clusters tha</context>
</contexts>
<marker>Berg-Kirkpatrick, Burkett, Klein, 2012</marker>
<rawString>Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical significance in NLP. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea, July. Association for Computational Linguistics. 2The event head say happens to appear in both the most preferred and dispreferred events in the ATTACKS domain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="6623" citStr="Blei et al., 2003" startWordPosition="1017" endWordPosition="1020">ributional semantics, this work broadens the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric me</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>A hybrid hierarchical model for multi-document summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>815--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2292" citStr="Celikyilmaz and Hakkani-Tur, 2010" startWordPosition="351" endWordPosition="354">d to fit the assumptions about the structure of the domain and the nature of the end task. As this field has progressed, the formal structures that are assumed to represent a Gerald Penn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the CRIMINAL INVESTIGATIONS domain, there may be events such as a murder, an investigation of the crime, an arrest, and a trial. These would be indicated by event heads such as kill, arrest, charge, plead. Relevant slots would include VICTIM, SUSPECT, AUTHORITIES, PLEA, etc. One problem faced by this li</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, 2010</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hybrid hierarchical model for multi-document summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 815–824, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Template-based information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>976--986</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6850" citStr="Chambers and Jurafsky, 2011" startWordPosition="1052" endWordPosition="1056">s cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with PROFINDER in Section 5. Vector s</context>
<context position="23638" citStr="Chambers and Jurafsky (2011)" startWordPosition="3835" endWordPosition="3838">n them. Method P R F1 HMM w/o semantics 13.8 64.1 22.6* DSHMM w/ SIMPLE 20.9 27.5 23.7 DSHMM w/ E&amp;P 20.7 27.9 23.8 PROFINDER 23.7 25.0 24.3 DSHMM w/ M&amp;L+ 19.7 36.3 25.6* DSHMM w/ M&amp;L× 22.1 33.2 26.5* Table 2: Slot induction results on the TAC guided summarization data set. Asterisks (*) indicate that the model is statistically significantly different from PROFINDER in terms of F1 at p &lt; 0.05. Results We compared DSHMM to two baselines. Our first baseline is PROFINDER, a stateof-the-art template inducer which Cheung et al. (2013) showed to outperform the previous heuristic clustering method of Chambers and Jurafsky (2011). Our second baseline is our DSHMM model, without the semantic vector component, (HMM w/o semantics). To calculate statistical significance, we use the paired bootstrap method, which can accommodate complex evaluation metrics like F1 (Berg-Kirkpatrick et al., 2012). Table 2 shows that performance of the models. Overall, PROFINDER significantly outperforms the HMM baseline, but not any of the DSHMM models by F1. DSHMM with contextualized semantic vectors achieves the highest F1s, and are significantly better than PROFINDER. All of the differences in precision and recall between PROFINDER and th</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Template-based information extraction without the templates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 976– 986, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Gerald Penn</author>
</authors>
<title>Evaluating distributional models of semantics for syntactically invariant inference.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>33--43</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="8082" citStr="Cheung and Penn, 2012" startWordPosition="1252" endWordPosition="1255">s form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializi</context>
</contexts>
<marker>Cheung, Penn, 2012</marker>
<rawString>Jackie Chi Kit Cheung and Gerald Penn. 2012. Evaluating distributional models of semantics for syntactically invariant inference. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–43, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Probabilistic frame induction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association</booktitle>
<contexts>
<context position="2324" citStr="Cheung et al. (2013)" startWordPosition="356" endWordPosition="359">f the domain and the nature of the end task. As this field has progressed, the formal structures that are assumed to represent a Gerald Penn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the CRIMINAL INVESTIGATIONS domain, there may be events such as a murder, an investigation of the crime, an arrest, and a trial. These would be indicated by event heads such as kill, arrest, charge, plead. Relevant slots would include VICTIM, SUSPECT, AUTHORITIES, PLEA, etc. One problem faced by this line of work is that, by their nat</context>
<context position="6887" citStr="Cheung et al. (2013)" startWordPosition="1059" endWordPosition="1062">stic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with PROFINDER in Section 5. Vector space models form the basis of modern </context>
<context position="20902" citStr="Cheung et al. (2013)" startWordPosition="3377" endWordPosition="3380">ng evaluations. First, templates for the domains are provided, and the model summaries are annotated with slots from the template, allowing for an intrinsic evaluation of slot induction (Section 5). Second, it contains multiple domain instances for each of the domains, and each domain instance comes annotated with eight model summaries, allowing for an extrinsic evaluation of our system (Section 6). 396 5 Guided Summarization Slot Induction We first evaluated our models on their ability to produce coherent clusters of entities belonging to the same slot, adopting the experimental procedure of Cheung et al. (2013). As part of the official TAC evaluation procedure, model summaries were manually segmented into contributors, and labelled with the slot in the TAC template that the contributor expresses. For example, a summary fragment such as On 20 April 1999, a massacre occurred at Columbine High School is segmented into the contributors: (On 20 April 1999, WHEN); (a massacre occurred, WHAT); and (at Columbine High School, WHERE). In the slot induction evaluation, this annotation is used as follows. First, the maximal noun phrases are extracted from the contributors and clustered based on the TAC slot of </context>
<context position="23544" citStr="Cheung et al. (2013)" startWordPosition="3821" endWordPosition="3824">del summaries according to the slot labels produced by running the Viterbi algorithm on them. Method P R F1 HMM w/o semantics 13.8 64.1 22.6* DSHMM w/ SIMPLE 20.9 27.5 23.7 DSHMM w/ E&amp;P 20.7 27.9 23.8 PROFINDER 23.7 25.0 24.3 DSHMM w/ M&amp;L+ 19.7 36.3 25.6* DSHMM w/ M&amp;L× 22.1 33.2 26.5* Table 2: Slot induction results on the TAC guided summarization data set. Asterisks (*) indicate that the model is statistically significantly different from PROFINDER in terms of F1 at p &lt; 0.05. Results We compared DSHMM to two baselines. Our first baseline is PROFINDER, a stateof-the-art template inducer which Cheung et al. (2013) showed to outperform the previous heuristic clustering method of Chambers and Jurafsky (2011). Our second baseline is our DSHMM model, without the semantic vector component, (HMM w/o semantics). To calculate statistical significance, we use the paired bootstrap method, which can accommodate complex evaluation metrics like F1 (Berg-Kirkpatrick et al., 2012). Table 2 shows that performance of the models. Overall, PROFINDER significantly outperforms the HMM baseline, but not any of the DSHMM models by F1. DSHMM with contextualized semantic vectors achieves the highest F1s, and are significantly </context>
</contexts>
<marker>Cheung, Poon, Vanderwende, 2013</marker>
<rawString>Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Vanderwende. 2013. Probabilistic frame induction. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Quantum Interaction Symposium (QI-2008).</booktitle>
<contexts>
<context position="7660" citStr="Clark et al., 2008" startWordPosition="1185" endWordPosition="1188">within a domain and consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with PROFINDER in Section 5. Vector space models form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to b</context>
</contexts>
<marker>Clark, Coecke, Sadrzadeh, 2008</marker>
<rawString>Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of meaning. In Proceedings of the Second Quantum Interaction Symposium (QI-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses. In</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1162--1172</pages>
<contexts>
<context position="7855" citStr="Dinu and Lapata, 2010" startWordPosition="1215" endWordPosition="1218">work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with PROFINDER in Section 5. Vector space models form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast,</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Joseph Austerweil</author>
<author>Eugene Charniak</author>
</authors>
<title>A unified local and global model for discourse coherence.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="6492" citStr="Elsner et al., 2007" startWordPosition="997" endWordPosition="1000"> modelling and template induction benefit from distributional semantics trained on a much larger corpus. From the perspective of distributional semantics, this work broadens the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as w</context>
</contexts>
<marker>Elsner, Austerweil, Charniak, 2007</marker>
<rawString>Micha Elsner, Joseph Austerweil, and Eugene Charniak. 2007. A unified local and global model for discourse coherence. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 897– 906.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatic creation of domain templates.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>207--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="6820" citStr="Filatova et al., 2006" startWordPosition="1048" endWordPosition="1051">ometric measures such as cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with PR</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, McKeown, 2006</marker>
<rawString>Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen McKeown. 2006. Automatic creation of domain templates. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207–214, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Grace Ngai</author>
</authors>
<title>One story, one flow: Hidden markov story models for multilingual multidocument summarization.</title>
<date>2006</date>
<journal>ACM Transactions on Speech and Language Processing (TSLP),</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="6412" citStr="Fung and Ngai, 2006" startWordPosition="986" endWordPosition="989"> modelling perspective, these results show that probabilistic models for content modelling and template induction benefit from distributional semantics trained on a much larger corpus. From the perspective of distributional semantics, this work broadens the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assu</context>
</contexts>
<marker>Fung, Ngai, 2006</marker>
<rawString>Pascale Fung and Grace Ngai. 2006. One story, one flow: Hidden markov story models for multilingual multidocument summarization. ACM Transactions on Speech and Language Processing (TSLP), 3(2):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Grace Ngai</author>
<author>Chi-Shun Cheung</author>
</authors>
<title>Combining optimal clustering and hidden markov models for extractive summarization.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering,</booktitle>
<pages>21--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="8441" citStr="Fung et al., 2003" startWordPosition="1307" endWordPosition="1310">d´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializing based on domain-general distributional information can be problematic because it can bias training towards a local optimum that is inappropriate for the target domain, leading to poor per393 Figure 1: Graphical representation of our model. Distributions that generate the latent variables and hyperparameters are omitted for clarity. formance. 3 Distributi</context>
</contexts>
<marker>Fung, Ngai, Cheung, 2003</marker>
<rawString>Pascale Fung, Grace Ngai, and Chi-Shun Cheung. 2003. Combining optimal clustering and hidden markov models for extractive summarization. In Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering, pages 21–28, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="7694" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1189" endWordPosition="1192">consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with PROFINDER in Section 5. Vector space models form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the prob</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394– 1404, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michael Rosen-Zvi</author>
<author>Yair Weiss</author>
</authors>
<date>2007</date>
<booktitle>Hidden topic markov models. Artificial Intelligence and Statistics (AISTATS).</booktitle>
<contexts>
<context position="6644" citStr="Gruber et al., 2007" startWordPosition="1021" endWordPosition="1024">s, this work broadens the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focus</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2007</marker>
<rawString>Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007. Hidden topic markov models. Artificial Intelligence and Statistics (AISTATS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="2256" citStr="Haghighi and Vanderwende, 2009" startWordPosition="346" endWordPosition="350">cture of the model can be adapted to fit the assumptions about the structure of the domain and the nature of the end task. As this field has progressed, the formal structures that are assumed to represent a Gerald Penn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the CRIMINAL INVESTIGATIONS domain, there may be events such as a murder, an investigation of the crime, an arrest, and a trial. These would be indicated by event heads such as kill, arrest, charge, plead. Relevant slots would include VICTIM, SUSPECT, AUTHORITIES, PLE</context>
<context position="6445" citStr="Haghighi and Vanderwende, 2009" startWordPosition="990" endWordPosition="993">e, these results show that probabilistic models for content modelling and template induction benefit from distributional semantics trained on a much larger corpus. From the perspective of distributional semantics, this work broadens the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure wit</context>
<context position="25358" citStr="Haghighi and Vanderwende (2009)" startWordPosition="4094" endWordPosition="4097">ntly, the contextualization of the vectors seems to be beneficial, at least with the M&amp;L component-wise operators. In the next section, we show that the improvement from contextualization transfers to multidocument summarization results. 397 6 Multi-document Summarization: An Extrinsic Evaluation We next evaluated our models extrinsically in the setting of extractive, multi-document summarization. To use the trained DSHMM for extractive summarization, we need a decoding procedure for selecting sentences in the source text to include in the summary. Inspired by the KLSUM and HIERSUM methods of Haghighi and Vanderwende (2009), we develop a criterion based on KullbackLeibler (KL) divergence between distributions estimated from the source text, and those estimated from the summary. The assumption here is that these distributions should match in a good summary. We describe two methods to use this criterion: a basic unsupervised method (Section 6.1), and a supervised variant that makes use of indomain summaries to learn the salient slots and events in the domain (Section 6.2). 6.1 A KL-based Criterion There are four main component distributions from our model that should be considered during extraction: (1) the distri</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin Y Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz and Marie-Francine Moens, editors, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="29871" citStr="Lin, 2004" startWordPosition="4858" endWordPosition="4859"> DSHMM w/ M&amp;Lx 32.4 34.3* 6.35 7.11&amp;quot; 10.2 11.0* DSHMM w/ E&amp;P 32.8 33.8* 6.38 7.31* 10.3 10.8* Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the model is statistically significantly better than the HMM model without semantics at a 95% confidence interval, a caret &amp;quot; indicates that the value is marginally so. so we consider these competitive baselines. We did not evaluate with the update summarization task, because our method has not been adapted to it. For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004). Note that the evaluation conditions of TAC 2010 are different, and thus those results are not directly comparable to ours. For instance, top performing systems in TAC 2010 make use of manually constructed lists of entities known to fit the slots in the provided templates and sample topic statements, which our method automatically learns. We include the leading baseline results from the competition as a point of reference, as it is a well-known and non-trivial one for news articles. This baseline summary consists of the leading sentences from the most recent document in the source text cluste</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin Y. Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Stan Szpakowicz and Marie-Francine Moens, editors, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>A coherence model based on syntactic patterns.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6518" citStr="Louis and Nenkova, 2012" startWordPosition="1001" endWordPosition="1004">te induction benefit from distributional semantics trained on a much larger corpus. From the perspective of distributional semantics, this work broadens the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but where</context>
<context position="8359" citStr="Louis and Nenkova, 2012" startWordPosition="1294" endWordPosition="1297">meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializing based on domain-general distributional information can be problematic because it can bias training towards a local optimum that is inappropriate for the target domain, leading to poor per393 Figure 1: Graphical representation of our model. Distributions that generate the la</context>
</contexts>
<marker>Louis, Nenkova, 2012</marker>
<rawString>Annie Louis and Ani Nenkova. 2012. A coherence model based on syntactic patterns. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="4225" citStr="Mitchell and Lapata, 2008" startWordPosition="662" endWordPosition="665">strength of the association between the context word and the target word being modelled. A notion of word similarity arises naturally from these models by comparing the similarity of the word vectors, for example by using a cosine measure. Recently, these models have been extended by considering how distribu392 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392–401, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tional representations can be modified depending on the specific context in which the word appears (Mitchell and Lapata, 2008, for example). Contextualization has been found to improve performance in tasks like lexical substitution and word sense disambiguation (Thater et al., 2011). In this paper, we propose to inject contextualized distributional semantic vectors into generative probabilistic models, in order to combine their complementary strengths for domain modelling. There are a number of potential advantages that distributional semantic models offer. First, they provide domain-general representations of word meaning that cannot be reliably estimated from the small target-domain corpora on which probabilistic </context>
<context position="7640" citStr="Mitchell and Lapata, 2008" startWordPosition="1181" endWordPosition="1184">much of the same structure within a domain and consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with PROFINDER in Section 5. Vector space models form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes train</context>
<context position="13656" citStr="Mitchell and Lapata (2008)" startWordPosition="2162" endWordPosition="2166">rds, and columns correspond to context words. Training begins by counting context words that appear within five words of the target word, ignoring stopwords. We then convert the raw counts to positive pointwise mutual information scores, which has been shown to improve word similarity correlation results (Turney and Pantel, 2010). We set thresholds on the frequencies of words for inclusion as target and context words (given in Section 4). Target words which fall below the threshold are modelled as UNK. All the methods below start from this basic vector representation. Component-wise Operators Mitchell and Lapata (2008) investigate using component-wise operators to combine the vectors of verbs and their intransitive subjects. We use component-wise operators to contextualize our vectors, but by combining with all of the arguments, and regardless of the event head’s category. Let event head h be the syntactic head of a number of arguments a1, a2, ...am, and ~vh,~va1,~va2, ...~vam be their respective vector representations according to the SIMPLE method. Then, their contextualized vectors ~cM&amp;L h ,~cM&amp;L a1 , ...~cM&amp;L am would be: ~cM&amp;L h = ~vh � ( ~cM&amp;L ai = ~vai O ~vh, ∀i = 1...m, (5) where (D represents a com</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<date>1992</date>
<booktitle>Proceedings of the Fourth Message Understanding Conference (MUC-4).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<marker>1992</marker>
<rawString>1992. Proceedings of the Fourth Message Understanding Conference (MUC-4). Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matthew Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated gigaword.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT Joint Workshop on Automatic Knowledge Base Construction &amp; Web-scale Knowledge Extraction (AKBC-WEKEX),</booktitle>
<pages>95--100</pages>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated gigaword. In Proceedings of the NAACL-HLT Joint Workshop on Automatic Knowledge Base Construction &amp; Web-scale Knowledge Extraction (AKBC-WEKEX), pages 95– 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Ormoneit</author>
<author>Volker Tresp</author>
</authors>
<title>Improved gaussian mixture density estimates using bayesian penalty terms and network averaging.</title>
<date>1995</date>
<booktitle>In Advances in Neural Information Processing,</booktitle>
<pages>542--548</pages>
<contexts>
<context position="16997" citStr="Ormoneit and Tresp (1995)" startWordPosition="2735" endWordPosition="2738">g it by its L2 norm, ||~cR|| T T × H PE(Et|Et−1) H PH(Ht|Et) t=2 t=1 PS(Sta|Et)PA(Ata|Sta). × Ct H a=1 T H t=1 m 0 ~vam) (4) i=1 ~cE&amp;P h = ~vh × Hm i=1 � wEL �= ~va × wEL ~cE&amp;P a . 395 We model the emission of these contextualized vectors in DSHMM as multivariate Gaussian distributions, so the semantic vector emissions can be written as P Hsem, P Asem — N(µ, E), where µ E Rk is the mean and E E Rk×k is the covariance matrix. To avoid overfitting, we regularize the covariance using its conjugate prior, the InverseWishart distribution. We follow the “neutral” setting of hyperparameters given by Ormoneit and Tresp (1995), so that the MAP estimate for the covariance matrix for (event or slot) state i becomes: � jrij(xj — µi)(xj — µi)T + 0J (8) i = Ej rij + 1 where j indexes all the relevant semantic vectors xj in the training set, rij is the posterior responsibility of state i for vector xj, and 0 is the remaining hyperparameter that we tune to adjust the amount of regularization. To further reduce model complexity, we set the off-diagonal entries of the resulting covariance matrix to zero. 3.2 Training and Inference Inference in DSHMM is accomplished by the standard Inside-Outside and tree-Viterbi algorithms,</context>
</contexts>
<marker>Ormoneit, Tresp, 1995</marker>
<rawString>Dirk Ormoneit and Volker Tresp. 1995. Improved gaussian mixture density estimates using bayesian penalty terms and network averaging. In Advances in Neural Information Processing, pages 542–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Hoa T Dang</author>
</authors>
<title>guided summarization task guidelines.</title>
<date>2010</date>
<journal>TAC</journal>
<contexts>
<context position="19569" citStr="Owczarzak and Dang, 2010" startWordPosition="3164" endWordPosition="3167">a) _ PAlemm, sem(Ata) — PAsem. 4 Experiments We trained the distributional semantic models using the Annotated Gigaword corpus (Napoles et al., 2012), which has been automatically preprocessed and is based on Gigaword 5th edition. This corpus contains almost ten million news articles and more than 4 billion tokens. We used those articles marked as “stories” — the vast majority of them. We modelled the 50,000 most common lemmata as target words, and the 3,000 most common lemmata as context words. We then trained DSHMM and conducted our evaluations on the TAC 2010 guided summarization data set (Owczarzak and Dang, 2010). Lemmatization and extraction of event heads and arguments are done by preprocessing with the Stanford CoreNLP tool suite (Toutanova et al., 2003; de Marneffe et al., 2006). This data set contains 46 topic clusters of 20 articles each, grouped into five topic categories or domains. For example, one topic cluster in the ATTACK category is about the Columbine Massacre. Each topic cluster contains eight human-written “model” summaries (“model” here meaning a gold standard). Half of the articles and model summaries in a topic cluster are used in the guided summarization task, and the rest are use</context>
</contexts>
<marker>Owczarzak, Dang, 2010</marker>
<rawString>Karolina Owczarzak and Hoa T. Dang. 2010. TAC 2010 guided summarization task guidelines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Anita Wong</author>
<author>Chung-Shu Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="7530" citStr="Salton et al., 1975" startWordPosition="1166" endWordPosition="1169">robabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with PROFINDER in Section 5. Vector space models form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and proba</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Lawrence Erlbaum,</title>
<date>1977</date>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Lawrence Erlbaum, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Word meaning in context: A simple and effective vector model.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<marker>Thater, F¨urstenau, Pinkal, 2011</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective vector model. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christoper D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>180</pages>
<contexts>
<context position="19715" citStr="Toutanova et al., 2003" startWordPosition="3186" endWordPosition="3189">, which has been automatically preprocessed and is based on Gigaword 5th edition. This corpus contains almost ten million news articles and more than 4 billion tokens. We used those articles marked as “stories” — the vast majority of them. We modelled the 50,000 most common lemmata as target words, and the 3,000 most common lemmata as context words. We then trained DSHMM and conducted our evaluations on the TAC 2010 guided summarization data set (Owczarzak and Dang, 2010). Lemmatization and extraction of event heads and arguments are done by preprocessing with the Stanford CoreNLP tool suite (Toutanova et al., 2003; de Marneffe et al., 2006). This data set contains 46 topic clusters of 20 articles each, grouped into five topic categories or domains. For example, one topic cluster in the ATTACK category is about the Columbine Massacre. Each topic cluster contains eight human-written “model” summaries (“model” here meaning a gold standard). Half of the articles and model summaries in a topic cluster are used in the guided summarization task, and the rest are used in the update summarization task. We chose this data set because it allows us to conduct various domain-modelling evaluations. First, templates </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christoper D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, page 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="13361" citStr="Turney and Pantel, 2010" startWordPosition="2113" endWordPosition="2116">We start with a description of the training of a basic model without any contextualization, then describe several contextualized models based on recent work. Simple Vector Space Model In the basic version of the model (SIMPLE), we train a termcontext matrix, where rows correspond to target words, and columns correspond to context words. Training begins by counting context words that appear within five words of the target word, ignoring stopwords. We then convert the raw counts to positive pointwise mutual information scores, which has been shown to improve word similarity correlation results (Turney and Pantel, 2010). We set thresholds on the frequencies of words for inclusion as target and context words (given in Section 4). Target words which fall below the threshold are modelled as UNK. All the methods below start from this basic vector representation. Component-wise Operators Mitchell and Lapata (2008) investigate using component-wise operators to combine the vectors of verbs and their intransitive subjects. We use component-wise operators to contextualize our vectors, but by combining with all of the arguments, and regardless of the event head’s category. Let event head h be the syntactic head of a n</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Structured topic models for language. Doctoral dissertation,</title>
<date>2008</date>
<institution>University of Cambridge.</institution>
<contexts>
<context position="6660" citStr="Wallach, 2008" startWordPosition="1025" endWordPosition="1026"> the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Related Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006; Haghighi and Vanderwende, 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose PROFINDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas PROFINDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integratin</context>
</contexts>
<marker>Wallach, 2008</marker>
<rawString>Hanna M. Wallach. 2008. Structured topic models for language. Doctoral dissertation, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28018" citStr="Woodsend and Lapata (2012)" startWordPosition="4537" endWordPosition="4540">DKL( PˆA(A|s) ||ˆQA(A|s)) s=1 To produce a summary, sentences from the source text are greedily added such that KL5core is minimized at each step, until the desired summary length is reached, discarding sentences with fewer than five words. 6.2 Supervised Learning The above unsupervised method results in summaries that closely mirror the source text in terms of the event and slot distributions, but this ignores the fact that not all such topics should be included in a summary. It also ignores genrespecific, stylistic considerations about characteristics of good summary sentences. For example, Woodsend and Lapata (2012) find several factors that indicate sentences should not be included in an extractive summary, such as the presence of personal pronouns. Thus, we implemented a second method, in which we modify the KL criterion above by estimating PˆE and PˆS from other model summaries that are drawn from the same domain (i.e. topic category), except for those summaries that are written for the specific topic cluster to be used for evaluation. 6.3 Method and Results We used the best performing models from the slot induction task and the above unsupervised and supervised methods based on KL-divergence to produ</context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>