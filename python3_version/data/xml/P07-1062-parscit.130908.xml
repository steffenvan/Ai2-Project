<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001754">
<title confidence="0.9017">
The utility of parse-derived features for automatic discourse segmentation
</title>
<author confidence="0.900977">
Seeger Fisher and Brian Roark
</author>
<affiliation confidence="0.758483">
Center for Spoken Language Understanding, OGI School of Science &amp; Engineering
Oregon Health &amp; Science University, Beaverton, Oregon, 97006 USA
</affiliation>
<email confidence="0.999338">
{fishers,roark}@cslu.ogi.edu
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841916666667">
We investigate different feature sets for
performing automatic sentence-level dis-
course segmentation within a general ma-
chine learning approach, including features
derived from either finite-state or context-
free annotations. We achieve the best re-
ported performance on this task, and demon-
strate that our SPADE-inspired context-free
features are critical to achieving this level of
accuracy. This counters recent results sug-
gesting that purely finite-state approaches
can perform competitively.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930291666667">
Discourse structure annotations have been demon-
strated to be of high utility for a number of NLP
applications, including automatic text summariza-
tion (Marcu, 1998; Marcu, 1999; Cristea et al.,
2005), sentence compression (Sporleder and Lap-
ata, 2005), natural language generation (Prasad et
al., 2005) and question answering (Verberne et al.,
2006). These annotations include sentence segmen-
tation into discourse units along with the linking
of discourse units, both within and across sentence
boundaries, into a labeled hierarchical structure. For
example, the tree in Figure 1 shows a sentence-level
discourse tree for the string “Prices have dropped but
remain quite high, according to CEO Smith,” which
has three discourse segments, each labeled with ei-
ther “Nucleus” or “Satellite” depending on how cen-
tral the segment is to the coherence of the text.
There are a number of corpora annotated with
discourse structure, including the well-known RST
Treebank (Carlson et al., 2002); the Discourse
GraphBank (Wolf and Gibson, 2005); and the Penn
Discourse Treebank (Miltsakaki et al., 2004). While
the annotation approaches differ across these cor-
pora, the requirement of sentence segmentation into
</bodyText>
<equation confidence="0.900787444444444">
Root
�
����� � � � �
Satellite
����� �� � � �
according to CEO Smith
��� � ���� ��
� � � �
Prices have dropped
</equation>
<figureCaption confidence="0.999399">
Figure 1: Example Nucleus/Satellite labeled sentence-level
discourse tree.
</figureCaption>
<bodyText confidence="0.999929925925926">
sub-sentential discourse units is shared across all ap-
proaches. These resources have facilitated research
into stochastic models and algorithms for automatic
discourse structure annotation in recent years.
Using the RST Treebank as training and evalua-
tion data, Soricut and Marcu (2003) demonstrated
that their automatic sentence-level discourse pars-
ing system could achieve near-human levels of ac-
curacy, if it was provided with manual segmenta-
tions and manual parse trees. Manual segmenta-
tion was primarily responsible for this performance
boost over their fully automatic system, thus mak-
ing the case that automatic discourse segmentation is
the primary impediment to high accuracy automatic
sentence-level discourse structure annotation. Their
models and algorithm – subsequently packaged to-
gether into the publicly available SPADE discourse
parser1 – make use of the output of the Charniak
(2000) parser to derive syntactic indicator features
for segmentation and discourse parsing.
Sporleder and Lapata (2005) also used the RST
Treebank as training data for data-driven discourse
parsing algorithms, though their focus, in contrast
to Soricut and Marcu (2003), was to avoid context-
free parsing and rely exclusively on features in their
model that could be derived via finite-state chunkers
and taggers. The annotations that they derive are dis-
</bodyText>
<footnote confidence="0.778189">
1http://www.isi.edu/publications/licensed-sw/spade/
</footnote>
<figure confidence="0.84722925">
Nucleus
���� � � � �
Nucleus Satellite
but remain quite high
</figure>
<page confidence="0.967703">
488
</page>
<note confidence="0.9251805">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 488–495,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999632211538461">
course “chunks”, i.e., sentence-level segmentation
and non-hierarchical nucleus/span labeling of seg-
ments. They demonstrate that their models achieve
comparable results to SPADE without the use of any
context-free features. Once again, segmentation is
the part of the process where the automatic algo-
rithms most seriously underperform.
In this paper we take up the question posed by
the results of Sporleder and Lapata (2005): how
much, if any, accuracy reduction should we expect
if we choose to use only finite-state derived fea-
tures, rather than those derived from full context-
free parses? If little accuracy is lost, as their re-
sults suggest, then it would make sense to avoid rel-
atively expensive context-free parsing, particularly
if the amount of text to be processed is large or if
there are real-time processing constraints on the sys-
tem. If, however, the accuracy loss is substantial,
one might choose to avoid context-free parsing only
in the most time-constrained scenarios.
While Sporleder and Lapata (2005) demonstrated
that their finite-state system could perform as well as
the SPADE system, which uses context-free parse
trees, this does not directly answer the question of
the utility of context-free derived features for this
task. SPADE makes use of a particular kind of fea-
ture from the parse trees, and does not train a gen-
eral classifier making use of other features beyond
the parse-derived indicator features. As we shall
show, its performance is not the highest that can be
achieved via context-free parser derived features.
In this paper, we train a classifier using a gen-
eral machine learning approach and a range of finite-
state and context-free derived features. We investi-
gate the impact on discourse segmentation perfor-
mance when one feature set is used versus another,
in such a way establishing the utility of features de-
rived from context-free parses. In the course of so
doing, we achieve the best reported performance on
this task, an absolute F-score improvement of 5.0%
over SPADE, which represents a more than 34% rel-
ative error rate reduction.
By focusing on segmentation, we provide an ap-
proach that is generally applicable to all of the
various annotation approaches, given the similari-
ties between the various sentence-level segmenta-
tion guidelines. Given that segmentation has been
shown to be a primary impediment to high accu-
racy sentence-level discourse structure annotation,
this represents a large step forward in our ability to
automatically parse the discourse structure of text,
whatever annotation approach we choose.
</bodyText>
<sectionHeader confidence="0.998961" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.99832">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999975894736842">
For our experiments we use the Rhetorical Structure
Theory Discourse Treebank (Carlson et al., 2002),
which we will denote RST-DT, a corpus annotated
with discourse segmentation and relations according
to Rhetorical Structure Theory (Mann and Thomp-
son, 1988). The RST-DT consists of 385 docu-
ments from the Wall Street Journal, about 176,000
words, which overlaps with the Penn Wall St. Jour-
nal (WSJ) Treebank (Marcus et al., 1993).
The segmentation of sentences in the RST-DT
is into clause-like units, known as elementary dis-
course units, or edus. We will use the two terms
‘edu’ and ‘segment’ interchangeably throughout the
rest of the paper. Human agreement for this segmen-
tation task is quite high, with agreement between
two annotators at an F-score of 98.3 for unlabeled
segmentation (Soricut and Marcu, 2003).
The RST-DT corpus annotates edu breaks, which
typically include sentence boundaries, but sentence
boundaries are not explicitly annotated in the corpus.
To perform sentence-level processing and evalua-
tion, we aligned the RST-DT documents to the same
documents in the Penn WSJ Treebank, and used the
sentence boundaries from that corpus.2 An addi-
tional benefit of this alignment is that the Penn WSJ
Treebank tokenization is then available for parsing
purposes. Simple minimum edit distance alignment
effectively allowed for differences in punctuation
representation (e.g., double quotes) and tokenization
when deriving the optimal alignment.
The RST-DT corpus is partitioned into a train-
ing set of 347 documents and a test set of 38 doc-
uments. This test set consists of 991 sentences with
2,346 segments. For training purposes, we created
a held-out development set by selecting every tenth
sentence of the training set. This development set
was used for feature development and for selecting
the number of iterations used when training models.
</bodyText>
<subsectionHeader confidence="0.996057">
2.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.990355666666667">
Previous research into RST-DT segmentation and
parsing has focused on subsets of the 991 sentence
test set during evaluation. Soricut and Marcu (2003)
</bodyText>
<footnote confidence="0.974489333333333">
2A small number of document final parentheticals are in the
RST-DT and not in the Penn WSJ Treebank, which our align-
ment approach takes into account.
</footnote>
<page confidence="0.998932">
489
</page>
<bodyText confidence="0.999824075">
omitted sentences that were not exactly spanned by
a subtree of the treebank, so that they could fo-
cus on sentence-level discourse parsing. By our
count, this eliminates 40 of the 991 sentences in the
test set from consideration. Sporleder and Lapata
(2005) went further and established a smaller sub-
set of 608 sentences, which omitted sentences with
only one segment, i.e., sentences which themselves
are atomic edus.
Since the primary focus of this paper is on seg-
mentation, there is no strong reason to omit any sen-
tences from the test set, hence our results will eval-
uate on all 991 test sentences, with two exceptions.
First, in Section 2.3, we compare SPADE results un-
der our configuration with results from Sporleder
and Lapata (2005) in order to establish compara-
bility, and this is done on their 608 sentence sub-
set. Second, in Section 3.2, we investigate feed-
ing our segmentation into the SPADE system, in or-
der to evaluate the impact of segmentation improve-
ments on their sentence-level discourse parsing per-
formance. For those trials, the 951 sentence subset
from Soricut and Marcu (2003) is used. All other
trials use the full 991 sentence test set.
Segmentation evaluation is done with precision,
recall and F1-score of segmentation boundaries.
Given a word string w1 ... wk, we can index word
boundaries from 0 to k, so that each word wi falls
between boundaries i−1 and i. For sentence-based
segmentation, indices 0 and k, representing the be-
ginning and end of the string, are known to be seg-
ment boundaries. Hence Soricut and Marcu (2003)
evaluate with respect to sentence internal segmenta-
tion boundaries, i.e., with indices j such that 0&lt;j&lt;k
for a sentence of length k. Let g be the number
of sentence-internal segmentation boundaries in the
gold standard, t the number of sentence-internal seg-
mentation boundaries in the system output, and m
the number of correct sentence-internal segmenta-
tion boundaries in the system output. Then
</bodyText>
<equation confidence="0.9996275">
P = m t R = m g and F1 = 2PR
P+R = 2m g+t
</equation>
<bodyText confidence="0.999939222222222">
In Sporleder and Lapata (2005), they were pri-
marily interested in labeled segmentation, where the
segment initial boundary was labeled with the seg-
ment type. In such a scenario, the boundary at in-
dex 0 is no longer known, hence their evaluation in-
cluded those boundaries, even when reporting un-
labeled results. Thus, in section 2.3, for compar-
ison with reported results in Sporleder and Lapata
(2005), our F1-score is defined accordingly, i.e., seg-
</bodyText>
<table confidence="0.9822146">
Segmentation system F1
Sporleder and Lapata best (reported) 88.40
SPADE 87.06
Sporleder and Lapata configuration (reported):
current configuration: 91.04
</table>
<tableCaption confidence="0.992616666666667">
Table 1: Segmentation results on the Sporleder and Lapata
(2005) data set, with accuracy defined to include sentence initial
segmentation boundaries.
</tableCaption>
<bodyText confidence="0.976222285714286">
mentation boundaries j such that 0 &lt; j &lt; k.
In addition, we will report unlabeled bracketing
precision, recall and F1-score, as defined in the
PARSEVAL metrics (Black et al., 1991) and eval-
uated via the widely used evalb package. We also
use evalb when reporting labeled and unlabeled dis-
course parsing results in Section 3.2.
</bodyText>
<subsectionHeader confidence="0.987376">
2.3 Baseline SPADE setup
</subsectionHeader>
<bodyText confidence="0.999941142857143">
The publicly available SPADE package, which en-
codes the approach in Soricut and Marcu (2003),
is taken as the baseline for this paper. We made
several modifications to the script from the default,
which account for better baseline performance than
is achieved with the default configuration. First, we
modified the script to take given parse trees as input,
rather than running the Charniak parser itself. This
allowed us to make two modifications that improved
performance: turning off tokenization in the Char-
niak parser, and reranking. The default script that
comes with SPADE does not turn off tokenization
inside of the parser, which leads to degraded perfor-
mance when the input has already been tokenized in
the Penn Treebank style. Secondly, Charniak and
Johnson (2005) showed how reranking of the 50-
best output of the Charniak (2000) parser gives sub-
stantial improvements in parsing accuracy. These
two modifications to the Charniak parsing output
used by the SPADE system lead to improvements
in its performance compared to previously reported
results.
Table 1 compares segmentation results of three
systems on the Sporleder and Lapata (2005) 608
sentence subset of the evaluation data: (1) their best
reported system; (2) the SPADE system results re-
ported in that paper; and (3) the SPADE system re-
sults with our current configuration. The evaluation
uses the unlabeled F1 measure as defined in that pa-
per, which counts sentence initial boundaries in the
scoring, as discussed in the previous section. As can
be seen from these results, our improved configu-
ration of SPADE gives us large improvements over
the previously reported SPADE performance on this
subset. As a result, we feel that we can use SPADE
</bodyText>
<page confidence="0.881887">
490
</page>
<bodyText confidence="0.998839339622642">
as a very strong baseline for evaluation on the entire on this held-out data, and chose the model that op-
test set. timized accuracy on that set. The averaged percep-
Additionally, we modified the SPADE script to al- tron was used on held-out and evaluation sets. See
low us to provide our segmentations to the full dis- Collins (2002) for more details on this approach.
course parsing that it performs, in order to evalu- 2.5 Features
ate the improvements to discourse parsing yielded To tease apart the utility of finite-state derived fea-
by any improvements to segmentation. tures and context-free derived features, we consider
2.4 Segmentation classifier three feature sets: (1) basic finite-state features; (2)
For this paper, we trained a binary classifier, which context-free features; and (3) finite-state approxima-
was applied independently at each word wi in the tion to context-free features. Note that every feature
string w1 ... wk, to decide whether that word is the must include exactly one class label c in order to
last in a segment. Note that wk is the last word in discriminate between classes in equation 1. Hence
the string, and is hence ignored. We used a log- when presenting features, it can be assumed that the
linear model with no Markov dependency between class label is part of the feature, even if it is not ex-
adjacent tags,3 and trained the parameters of the plicitly mentioned.
model with the perceptron algorithm, with averag- The three feature sets are not completely disjoint.
ing to control for over-training (Collins, 2002). We include simple position-based features in every
Let C={E, I} be the set of classes: seg- system, defined as follows. Because edus are typi-
mentation boundary (E) or non-boundary (I). Let cally multi-word strings, it is less likely for a word
f(c, i, w1 ... wk) be a function that takes as in- near the beginning or end of a sentence to be at an
put a class value c, a word index i and the word edu boundary. Thus it is reasonable to expect the
string w1 ... wk and returns a d-dimensional vector position within a sentence of a token to be a helpful
of feature values for that word index in that string feature. We created 101 indicator features, repre-
with that class. For example, one feature might be senting percentages from 0 to 100. For a string of
(c = E, wi = the), which returns the value 1 when length k, at position i, we round i/k to two decimal
c = E and the current word is ‘the’, and returns places and provide a value of 1 for the corresponding
0 otherwise. Given a d-dimensional parameter vec- quantized position feature and 0 for the other posi-
tor 0, the output of the classifier is that class which tion features.
maximizes the dot product between the feature and 2.5.1 Basic finite-state features
parameter vectors: Our baseline finite-state feature set includes simple
c(i, w1 ... wk) = argmax 0 · f(c, i, w1 ... wk) (1) tagger derived features, as well as features based on
cEC position in the string and n-grams4. We annotate
In training, the weights in 0 are initialized to 0. tag sequences onto the word sequence via a compet-
For m epochs (passes over the training data), for itive discriminatively trained tagger (Hollingshead
each word in the training data (except sentence final et al., 2005), trained for each of two kinds of tag
words), the model is updated. Let i be the current sequences: part-of-speech (POS) tags and shallow
word position in string w1 ... wk and suppose that parse tags. The shallow parse tags define non-
there have been j−1 previous updates to the model hierarchical base constituents (“chunks”), as defined
parameters. Let ci be the true class label, and let ci for the CoNLL-2000 shared task (Tjong Kim Sang
be shorthand for c(i, w1 ... wk) in equation 1. Then and Buchholz, 2000). These can either be used
the parameter vector 0j at step j is updated as fol- as tag or chunk sequences. For example, the tree
lows: in Figure 2 represents a shallow (non-hierarchical)
Oj _ 0j−1 − f(c, i, w1 ... wk) + f(c, i, w1 ... wk) (2) parse tree, with four base constituents. Each base
As stated in Section 2.1, we reserved every tenth constituent X begins with a word labeled with BX,
sentence as held-out data. After each pass over the which signifies that this word begins the constituent.
training data, we evaluated the system performance All other words within a constituent X are labeled
4We tried using a list of 311 cue phrases from Knott (1996)
to define features, but did not derive any system improvement
through this list, presumably because our simple n-gram fea-
tures already capture many such lexical cues.
3Because of the sparsity of boundary tags, Markov depen-
dencies between tags buy no additional system accuracy.
491
</bodyText>
<equation confidence="0.876205">
ROOT
��������� � � �� � �
� � � � �
� � � �
NP
BNP
NN
tomorrow
</equation>
<figureCaption confidence="0.997476">
Figure 2: Tree representation of shallow parses, with B(egin)
and I(nside) tags
</figureCaption>
<bodyText confidence="0.999603576923077">
IX, and words outside of any base constituent are la-
beled O. In such a way, each word is labeled with
both a POS-tag and a B/I/O tag.
For our three sequences (lexical, POS-tag and
shallow tag), we define n-gram features surround-
ing the potential discourse boundary. If the current
word is wi, the hypothesized boundary will occur
between wi and wi+1. For this boundary position,
the 6-gram including the three words before and the
three words after the boundary is included as a fea-
ture; additionally, all n-grams for n &lt; 6 such that
either wi or wi+1 (or both) is in the n-gram are in-
cluded as features. In other words, all n-grams in a
six word window of boundary position i are included
as features, except those that include neither wi nor
wi+1 in the n-gram. The identical feature templates
are used with POS-tag and shallow tag sequences as
well, to define tag n-gram features.
This feature set is very close to that used in
Sporleder and Lapata (2005), but not identical.
Their n-gram feature definitions were different
(though similar), and they made use of cue phrases
from Knott (1996). In addition, they used a rule-
based clauser that we did not. Despite such differ-
ences, this feature set is quite close to what is de-
scribed in that paper.
</bodyText>
<subsubsectionHeader confidence="0.486375">
2.5.2 Context-free features
</subsubsectionHeader>
<bodyText confidence="0.999963923076923">
To describe our context-free features, we first
present how SPADE made use of context-free parse
trees within their segmentation algorithm, since this
forms the basis of our features. The SPADE features
are based on productions extracted from full syntac-
tic parses of the given sentence. The primary feature
for a discourse boundary after word wi is based on
the lowest constituent in the tree that spans words
wm ... wn such that m ≤ i &lt; n. For example, in
the parse tree schematic in Figure 3, the constituent
labeled with A is the lowest constituent in the tree
whose span crosses the potential discourse bound-
ary after wi. The primary feature is the production
</bodyText>
<equation confidence="0.889004">
A
�������� �� �� � � � � � �
� � � �
B1 ... Bj_1 Bj . . . B
�� � �
C1 . . . Cn
��
... Ti
wi
</equation>
<figureCaption confidence="0.99573">
Figure 3: Parse tree schematic for describing context-free seg-
mentation features
</figureCaption>
<bodyText confidence="0.99980195">
that expands this constituent in the tree, with the
proposed segmentation boundary marked, which in
this case is: A → B1 ... Bj_1||Bj ... Bm, where
 ||denotes the segmentation boundary. In SPADE,
the production is lexicalized by the head words of
each constituent, which are determined using stan-
dard head-percolation techniques. This feature is
used to predict a boundary as follows: if the relative
frequency estimate of a boundary given the produc-
tion feature in the corpus is greater than 0.5, then a
boundary is predicted; otherwise not. If the produc-
tion has not been observed frequently enough, the
lexicalization is removed and the relative frequency
of a boundary given the unlexicalized production is
used for prediction. If the observations of the unlex-
icalized production are also too sparse, then only the
children adjacent to the boundary are maintained in
the feature, e.g., A → ∗Bj_1||Bj∗ where ∗ repre-
sents zero or more categories. Further smoothing is
used when even this is unobserved.
We use these features as the starting point for our
context-free feature set: the lexicalized production
A → B1 ... Bj_1||Bj ... Bm, as defined above for
SPADE, is a feature in our model, as is the unlexi-
calized version of the production. As with the other
features that we have described, this feature is used
as an indicator feature in the classifier applied at the
word wi preceding the hypothesized boundary. In
addition to these full production features, we use the
production with only children adjacent to the bound-
ary, denoted by A → ∗Bj_1||Bj∗. This production
is used in four ways: fully lexicalized; unlexicalized;
only category Bj_1 lexicalized; and only category
Bj lexicalized. We also use A → ∗Bj_2Bj_1||∗
and A → ∗||BjBj+1∗ features, both unlexicalized
and with the boundary-adjacent category lexical-
ized. If there is no category Bj_2 or Bj+1, they are
replaced with “N/A”.
In addition to these features, we fire the same fea-
tures for all productions on the path from A down
</bodyText>
<figure confidence="0.99923732">
BNP
DT
the
NP
�� ��
INP
NN
broker
VP
�� ��
BVP
M
IVP
D
VBD
will sell
BNP
DT
the
NP
�� ��
INP
NNS
stocks
�
</figure>
<page confidence="0.994553">
492
</page>
<table confidence="0.999900125">
Segmentation system Segment Boundary accuracy F1 Recall Bracketing F1
Recall Precision accuracy
Precision
SPADE 85.4 85.5 85.5 77.7 77.9 77.8
Classifier: Basic finite-state 81.5 83.3 82.4 73.6 74.5 74.0
Classifier: Full finite-state 84.1 87.9 86.0 78.0 80.0 79.0
Classifier: Context-free 84.7 91.1 87.8 80.3 83.7 82.0
Classifier: All features 89.7 91.3 90.5 84.9 85.8 85.3
</table>
<tableCaption confidence="0.99196725">
Table 2: Segmentation results on all 991 sentences in the RST-DT test set. Segment boundary accuracy is for sentence internal
boundaries only, following Soricut and Marcu (2003). Bracketing accuracy is for unlabeled flat bracketing of the same segments.
While boundary accuracy correctly depicts segmentation results, the harsher flat bracketing metric better predicts discourse parsing
performance.
</tableCaption>
<bodyText confidence="0.999808555555556">
to the word wi. For these productions, the seg-
mentation boundary  ||will occur after all children
in the production, e.g., Bj_1 → C1 ... C,,||, which
is then used in both lexicalized and unlexicalized
forms. For the feature with only categories adja-
cent to the boundary, we again use “N/A” to denote
the fact that no category occurs to the right of the
boundary: Bj_1 → ∗C,,,||N/A. Once again, these
are lexicalized as described above.
</bodyText>
<subsectionHeader confidence="0.602236">
2.5.3 Finite-state approximation features
</subsectionHeader>
<bodyText confidence="0.999870375">
An approximation to our context-free features can
be made by using the shallow parse tree, as shown
in Figure 2, in lieu of the full hierarchical parse
tree. For example, if the current word was “sell”
in the tree in Figure 2, the primary feature would
be ROOT → NP VP||NP NP, and it would have an
unlexicalized version and three lexicalized versions:
the category immediately prior to the boundary lex-
icalized; the category immediately after the bound-
ary lexicalized; and both lexicalized. For lexicaliza-
tion, we choose the final word in the constituent as
the lexical head for the constituent. This is a rea-
sonable first approximation, because such typically
left-headed categories as PP and VP lose their argu-
ments in the shallow parse.
As a practical matter, we limit the number of cat-
egories in the flat production to 8 to the left and 8 to
the right of the boundary. In a manner similar to the
n-gram features that we defined in Section 2.5.1, we
allow all combinations with less than 8 contiguous
categories on each side, provided that at least one
of the adjacent categories is included in the feature.
Each feature has an unlexicalized and three lexical-
ized versions, as described above.
</bodyText>
<sectionHeader confidence="0.999373" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9997216875">
We performed a number of experiments to deter-
mine the relative utility of features derived from
full context-free syntactic parses and those derived
solely from shallow finite-state tagging. Our pri-
mary concern is with intra-sentential discourse seg-
mentation, but we are also interested in how much
the improved segmentation helps discourse parsing.
The syntactic parser we use for all context-free
syntactic parses used in either SPADE or our clas-
sifier is the Charniak parser with reranking, as de-
scribed in Charniak and Johnson (2005). The Char-
niak parser and reranker were trained on the sections
of the Penn Treebank not included in the RST-DT
test set.
All statistical significance testing is done via the
stratified shuffling test (Yeh, 2000).
</bodyText>
<subsectionHeader confidence="0.999604">
3.1 Segmentation
</subsectionHeader>
<bodyText confidence="0.999873458333333">
Table 2 presents segmentation results for SPADE
and four versions of our classifier. The “Basic finite-
state” system uses only finite-state sequence fea-
tures as defined in Section 2.5.1, while the “Full
finite-state” also includes the finite-state approxima-
tion features from Section 2.5.3. The “Context-free”
system uses the SPADE-inspired features detailed in
Section 2.5.2, but none of the features from Sections
2.5.1 or 2.5.3. Finally, the “All features” section in-
cludes features from all three sections.5
Note that the full finite-state system is consider-
ably better than the basic finite-state system, demon-
strating the utility of these approximations of the
SPADE-like context-free features. The performance
of the resulting “Full” finite-state system is not sta-
tistically significantly different from that of SPADE
(p=0.193), despite no reliance on features derived
from context-free parses.
The context-free features, however, even without
any of the finite-state sequence features (even lex-
ical n-grams) outperforms the best finite-state sys-
tem by almost two percent absolute, and the sys-
tem with all features improves on the best finite-state
system by over four percent absolute. The system
</bodyText>
<footnote confidence="0.6904034">
5In the “All features” condition, the finite-state approxima-
tion features defined in Section 2.5.3 only include a maximum
of 3 children to the left and right of the boundary, versus a max-
imum of 8 for the “Full finite-state” system. This was found to
be optimal on the development set.
</footnote>
<page confidence="0.996451">
493
</page>
<table confidence="0.999813">
Segmentation Unlabeled Nuc/Sat
SPADE 76.9 70.2
Classifier: Full finite state 78.1 71.1
Classifier: All features 83.5 76.1
</table>
<tableCaption confidence="0.997113">
Table 3: Discourse parsing results on the 951 sentence Sori-
</tableCaption>
<bodyText confidence="0.964485666666667">
cut and Marcu (2003) evaluation set, using SPADE for parsing,
and various methods for segmentation. Scores are unlabeled
and labeled (Nucleus/Satellite) bracketing accuracy (F1). The
first line shows SPADE performing both segmentation and dis-
course parsing. The other two lines show SPADE performing
discourse parsing with segmentations produced by our classi-
fier using different combinations of features.
with all features is statistically significantly better
than both SPADE and the “Full finite-state” classi-
fier system, at p &lt; 0.001. This large improvement
demonstrates that the context-free features can pro-
vide a very large system improvement.
</bodyText>
<subsectionHeader confidence="0.999698">
3.2 Discourse parsing
</subsectionHeader>
<bodyText confidence="0.999987620689655">
It has been shown that accurate discourse segmen-
tation within a sentence greatly improves the over-
all parsing accuracy to near human levels (Sori-
cut and Marcu, 2003). Given our improved seg-
mentation results presented in the previous section,
improvements would be expected in full sentence-
level discourse parsing. To achieve this, we modi-
fied the SPADE script to accept our segmentations
when building the fully hierarchical discourse tree.
The results for three systems are presented in Ta-
ble 3: SPADE, our “Full finite-state” system, and
our system with all features. Results for unlabeled
bracketing are presented, along with results for la-
beled bracketing, where the label is either Nucleus
or Satellite, depending upon whether or not the node
is more central (Nucleus) to the coherence of the text
than its sibling(s) (Satellite). This label set has been
shown to be of particular utility for indicating which
segments are more important to include in an auto-
matically created summary or compressed sentence
(Sporleder and Lapata, 2005; Marcu, 1998; Marcu,
1999; Cristea et al., 2005).
Once again, the finite-state system does not
perform statistically significantly different from
SPADE on either labeled or unlabeled discourse
parsing. Using all features, however, results in
greater than 5% absolute accuracy improvement
over both of these systems, which is significant, in
all cases, at p &lt; 0.001.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="discussions">
4 Discussion and future directions
</sectionHeader>
<bodyText confidence="0.999991537037037">
Our results show that context-free parse derived fea-
tures are critical for achieving the highest level of
accuracy in sentence-level discourse segmentation.
Given that edus are by definition clause-like units,
it is not surprising that accurate full syntactic parse
trees provide highly relevant information unavail-
able from finite-state approaches. Adding context-
free features to our best finite-state feature model
reduces error in segmentation by 32.1%, an in-
crease in absolute F-score of 4.5%. These increases
are against a finite-state segmentation model that is
powerful enough to be statistically indistinguishable
from SPADE.
Our experiments also confirm that increased seg-
mentation accuracy yields significantly better dis-
course parsing accuracy, as previously shown to be
the case when providing reference segmentations to
a parser (Soricut and Marcu, 2003). The segmen-
tation reduction in error of 34.5% propagates to a
28.6% reduction in error for unlabeled discourse
parse trees, and a 19.8% reduction in error for trees
labeled with Nucleus and Satellite.
We have several key directions in which to con-
tinue this work. First, given that a general ma-
chine learning approach allowed us to improve upon
SPADE’s segmentation performance, we also be-
lieve that it will prove useful for improving full
discourse parsing, both at the sentence level and
beyond. For efficient inter-sentential discourse
parsing, we see the need for an additional level
of segmentation at the paragraph level. Whereas
most sentences correspond to a well-formed subtree,
Sporleder and Lascarides (2004) report that over
20% of the paragraph boundaries in the RST-DT do
not correspond to a well-formed subtree in the hu-
man annotated discourse parse for that document.
Therefore, to perform accurate and efficient pars-
ing of the RST-DT at the paragraph level, the text
should be segmented into paragraph-like segments
that conform to the human-annotated subtree bound-
aries, just as sentences are segmented into edus.
We also intend to begin work on the other dis-
course annotated corpora. While most work on tex-
tual discourse parsing has made use of the RST-DT
corpus, the Discourse GraphBank corpus provides a
competing annotation that is not constrained to tree
structures (Wolf and Gibson, 2005). Once accurate
levels of segmentation and parsing for both corpora
are attained, it will be possible to perform extrinsic
evaluations to determine their relative utility for dif-
ferent NLP tasks. Recent work has shown promis-
ing preliminary results for recognizing and labeling
relations of GraphBank structures (Wellner et al.,
2006), in the case that the algorithm is provided with
</bodyText>
<page confidence="0.997025">
494
</page>
<bodyText confidence="0.999885705882353">
manually segmented sentences. Sentence-level seg-
mentation in the GraphBank is very similar to that in
the RST-DT, so our segmentation approach should
work well for Discourse GraphBank style parsing.
The Penn Discourse Treebank (Miltsakaki et al.,
2004), or PDTB, uses a relatively flat annotation of
discourse structure, in contrast to the hierarchical
structures found in the other two corpora. It contains
annotations for discourse connectives and their argu-
ments, where an argument can be as small as a nom-
inalization or as large as several sentences. This ap-
proach obviates the need to create a set of discourse
relations, but sentence internal segmentation is still
a necessary step. Though segmentation in the PDTB
tends to larger units than edus, our approach to seg-
mentation should be straightforwardly applicable to
their segmentation style.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999856428571429">
Thanks to Caroline Sporleder and Mirella Lapata for
their test data and helpful comments. Thanks also to
Radu Soricut for helpful input. This research was
supported in part by NSF Grant #IIS-0447214. Any
opinions, findings, conclusions or recommendations
expressed in this publication are those of the authors
and do not necessarily reflect the views of the NSF.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870475609756">
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grish-
man, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M.P. Marcus, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quantita-
tively comparing the syntactic coverage of english gram-
mars. In DARPA Speech and Natural Language Workshop,
pages 306–311.
L. Carlson, D. Marcu, and M.E. Okurowski. 2002. RST dis-
course treebank. Linguistic Data Consortium, Catalog #
LDC2002T07. ISBN LDC2002T07.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proceedings of
the 43rd Annual Meeting of ACL, pages 173–180.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st Conference of the North American
Chapter of the Association for Computational Linguistics,
pages 132–139.
M.J. Collins. 2002. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages
1–8.
D. Cristea, O. Postolache, and I. Pistol. 2005. Summarisation
through discourse structure. In 6th International Conference
on Computational Linguistics and Intelligent Text Process-
ing (CICLing).
K. Hollingshead, S. Fisher, and B. Roark. 2005. Comparing
and combining finite-state and context-free parsers. In Pro-
ceedings of HLT-EMNLP, pages 787–794.
A. Knott. 1996. A Data-Driven Methodology for Motivating
a Set of Coherence Relations. Ph.D. thesis, Department of
Artificial Intelligence, University of Edinburgh.
W.C. Mann and S.A. Thompson. 1988. Rhetorical structure
theory: Toward a functional theory of text organization. Text,
8(3):243–281.
D. Marcu. 1998. Improving summarization through rhetorical
parsing tuning. In The 6th Workshop on Very Large Corpora.
D. Marcu. 1999. Discourse trees are good indicators of im-
portance in text. In I. Mani and M. Maybury, editors, Ad-
vances in Automatic Text Summarization, pages 123–136.
MIT Press, Cambridge, MA.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313–330.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004. The
Penn Discourse TreeBank. In Proceedings of the Language
Resources and Evaluation Conference.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, and
B. Webber. 2005. The Penn Discourse TreeBank as a re-
source for natural language generation. In Proceedings of
the Corpus Linguistics Workshop on Using Corpora for Nat-
ural Language Generation.
R. Soricut and D. Marcu. 2003. Sentence level discourse pars-
ing using syntactic and lexical information. In Human Lan-
guage Technology Conference of the North American Asso-
ciation for Computational Linguistics (HLT-NAACL).
C. Sporleder and M. Lapata. 2005. Discourse chunking and its
application to sentence compression. In Human Language
Technology Conference and the Conference on Empirical
Methods in Natural Language Processing (HLT-EMNLP),
pages 257–264.
C. Sporleder and A. Lascarides. 2004. Combining hierarchi-
cal clustering and machine learning to predict high-level dis-
course structure. In Proceedings of the International Confer-
ence in Computational Linguistics (COLING), pages 43–49.
E.F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proceedings of
CoNLL, pages 127–132.
S. Verberne, L. Boves, N. Oostdijk, and P.A. Coppen. 2006.
Discourse-based answering of why-questions. Traitement
Automatique des Langues (TAL).
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky, and
R. Sauri. 2006. Classification of discourse coherence re-
lations: An exploratory study using multiple knowledge
sources. In Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue.
F. Wolf and E. Gibson. 2005. Representing discourse coher-
ence: A corpus-based analysis. Computational Linguistics,
31(2):249–288.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of the 18th Inter-
national COLING, pages 947–953.
</reference>
<page confidence="0.999129">
495
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966728">
<title confidence="0.999846">The utility of parse-derived features for automatic discourse segmentation</title>
<author confidence="0.999895">Seeger Fisher</author>
<author confidence="0.999895">Brian Roark</author>
<affiliation confidence="0.999709">Center for Spoken Language Understanding, OGI School of Science &amp; Engineering</affiliation>
<address confidence="0.975203">Oregon Health &amp; Science University, Beaverton, Oregon, 97006 USA</address>
<abstract confidence="0.999355384615385">We investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M P Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of english grammars.</title>
<date>1991</date>
<booktitle>In DARPA Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="11533" citStr="Black et al., 1991" startWordPosition="1799" endWordPosition="1802">us, in section 2.3, for comparison with reported results in Sporleder and Lapata (2005), our F1-score is defined accordingly, i.e., segSegmentation system F1 Sporleder and Lapata best (reported) 88.40 SPADE 87.06 Sporleder and Lapata configuration (reported): current configuration: 91.04 Table 1: Segmentation results on the Sporleder and Lapata (2005) data set, with accuracy defined to include sentence initial segmentation boundaries. mentation boundaries j such that 0 &lt; j &lt; k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package. We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2. 2.3 Baseline SPADE setup The publicly available SPADE package, which encodes the approach in Soricut and Marcu (2003), is taken as the baseline for this paper. We made several modifications to the script from the default, which account for better baseline performance than is achieved with the default configuration. First, we modified the script to take given parse trees as input, rather than running the Charniak parser itself. This allowed us to mak</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M.P. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of english grammars. In DARPA Speech and Natural Language Workshop, pages 306–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<date>2002</date>
<booktitle>RST discourse treebank. Linguistic Data Consortium, Catalog # LDC2002T07. ISBN LDC2002T07.</booktitle>
<contexts>
<context position="1778" citStr="Carlson et al., 2002" startWordPosition="253" endWordPosition="256">tions include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “Satellite” depending on how central the segment is to the coherence of the text. There are a number of corpora annotated with discourse structure, including the well-known RST Treebank (Carlson et al., 2002); the Discourse GraphBank (Wolf and Gibson, 2005); and the Penn Discourse Treebank (Miltsakaki et al., 2004). While the annotation approaches differ across these corpora, the requirement of sentence segmentation into Root � ����� � � � � Satellite ����� �� � � � according to CEO Smith ��� � ���� �� � � � � Prices have dropped Figure 1: Example Nucleus/Satellite labeled sentence-level discourse tree. sub-sentential discourse units is shared across all approaches. These resources have facilitated research into stochastic models and algorithms for automatic discourse structure annotation in recen</context>
<context position="6524" citStr="Carlson et al., 2002" startWordPosition="984" endWordPosition="987">ate reduction. By focusing on segmentation, we provide an approach that is generally applicable to all of the various annotation approaches, given the similarities between the various sentence-level segmentation guidelines. Given that segmentation has been shown to be a primary impediment to high accuracy sentence-level discourse structure annotation, this represents a large step forward in our ability to automatically parse the discourse structure of text, whatever annotation approach we choose. 2 Methods 2.1 Data For our experiments we use the Rhetorical Structure Theory Discourse Treebank (Carlson et al., 2002), which we will denote RST-DT, a corpus annotated with discourse segmentation and relations according to Rhetorical Structure Theory (Mann and Thompson, 1988). The RST-DT consists of 385 documents from the Wall Street Journal, about 176,000 words, which overlaps with the Penn Wall St. Journal (WSJ) Treebank (Marcus et al., 1993). The segmentation of sentences in the RST-DT is into clause-like units, known as elementary discourse units, or edus. We will use the two terms ‘edu’ and ‘segment’ interchangeably throughout the rest of the paper. Human agreement for this segmentation task is quite hig</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2002</marker>
<rawString>L. Carlson, D. Marcu, and M.E. Okurowski. 2002. RST discourse treebank. Linguistic Data Consortium, Catalog # LDC2002T07. ISBN LDC2002T07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of ACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="12481" citStr="Charniak and Johnson (2005)" startWordPosition="1952" endWordPosition="1955">l modifications to the script from the default, which account for better baseline performance than is achieved with the default configuration. First, we modified the script to take given parse trees as input, rather than running the Charniak parser itself. This allowed us to make two modifications that improved performance: turning off tokenization in the Charniak parser, and reranking. The default script that comes with SPADE does not turn off tokenization inside of the parser, which leads to degraded performance when the input has already been tokenized in the Penn Treebank style. Secondly, Charniak and Johnson (2005) showed how reranking of the 50- best output of the Charniak (2000) parser gives substantial improvements in parsing accuracy. These two modifications to the Charniak parsing output used by the SPADE system lead to improvements in its performance compared to previously reported results. Table 1 compares segmentation results of three systems on the Sporleder and Lapata (2005) 608 sentence subset of the evaluation data: (1) their best reported system; (2) the SPADE system results reported in that paper; and (3) the SPADE system results with our current configuration. The evaluation uses the unla</context>
<context position="25525" citStr="Charniak and Johnson (2005)" startWordPosition="4193" endWordPosition="4196">re has an unlexicalized and three lexicalized versions, as described above. 3 Experiments We performed a number of experiments to determine the relative utility of features derived from full context-free syntactic parses and those derived solely from shallow finite-state tagging. Our primary concern is with intra-sentential discourse segmentation, but we are also interested in how much the improved segmentation helps discourse parsing. The syntactic parser we use for all context-free syntactic parses used in either SPADE or our classifier is the Charniak parser with reranking, as described in Charniak and Johnson (2005). The Charniak parser and reranker were trained on the sections of the Penn Treebank not included in the RST-DT test set. All statistical significance testing is done via the stratified shuffling test (Yeh, 2000). 3.1 Segmentation Table 2 presents segmentation results for SPADE and four versions of our classifier. The “Basic finitestate” system uses only finite-state sequence features as defined in Section 2.5.1, while the “Full finite-state” also includes the finite-state approximation features from Section 2.5.3. The “Context-free” system uses the SPADE-inspired features detailed in Section </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of ACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="3082" citStr="Charniak (2000)" startWordPosition="453" endWordPosition="454"> demonstrated that their automatic sentence-level discourse parsing system could achieve near-human levels of accuracy, if it was provided with manual segmentations and manual parse trees. Manual segmentation was primarily responsible for this performance boost over their fully automatic system, thus making the case that automatic discourse segmentation is the primary impediment to high accuracy automatic sentence-level discourse structure annotation. Their models and algorithm – subsequently packaged together into the publicly available SPADE discourse parser1 – make use of the output of the Charniak (2000) parser to derive syntactic indicator features for segmentation and discourse parsing. Sporleder and Lapata (2005) also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003), was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers. The annotations that they derive are dis1http://www.isi.edu/publications/licensed-sw/spade/ Nucleus ���� � � � � Nucleus Satellite but remain quite high 488 Proceedings of the 45th Annual Meet</context>
<context position="12548" citStr="Charniak (2000)" startWordPosition="1966" endWordPosition="1967">ne performance than is achieved with the default configuration. First, we modified the script to take given parse trees as input, rather than running the Charniak parser itself. This allowed us to make two modifications that improved performance: turning off tokenization in the Charniak parser, and reranking. The default script that comes with SPADE does not turn off tokenization inside of the parser, which leads to degraded performance when the input has already been tokenized in the Penn Treebank style. Secondly, Charniak and Johnson (2005) showed how reranking of the 50- best output of the Charniak (2000) parser gives substantial improvements in parsing accuracy. These two modifications to the Charniak parsing output used by the SPADE system lead to improvements in its performance compared to previously reported results. Table 1 compares segmentation results of three systems on the Sporleder and Lapata (2005) 608 sentence subset of the evaluation data: (1) their best reported system; (2) the SPADE system results reported in that paper; and (3) the SPADE system results with our current configuration. The evaluation uses the unlabeled F1 measure as defined in that paper, which counts sentence in</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="13760" citStr="Collins (2002)" startWordPosition="2168" endWordPosition="2169">initial boundaries in the scoring, as discussed in the previous section. As can be seen from these results, our improved configuration of SPADE gives us large improvements over the previously reported SPADE performance on this subset. As a result, we feel that we can use SPADE 490 as a very strong baseline for evaluation on the entire on this held-out data, and chose the model that optest set. timized accuracy on that set. The averaged percepAdditionally, we modified the SPADE script to al- tron was used on held-out and evaluation sets. See low us to provide our segmentations to the full dis- Collins (2002) for more details on this approach. course parsing that it performs, in order to evalu- 2.5 Features ate the improvements to discourse parsing yielded To tease apart the utility of finite-state derived feaby any improvements to segmentation. tures and context-free derived features, we consider 2.4 Segmentation classifier three feature sets: (1) basic finite-state features; (2) For this paper, we trained a binary classifier, which context-free features; and (3) finite-state approximawas applied independently at each word wi in the tion to context-free features. Note that every feature string w1</context>
<context position="14982" citStr="Collins, 2002" startWordPosition="2367" endWordPosition="2368"> to decide whether that word is the must include exactly one class label c in order to last in a segment. Note that wk is the last word in discriminate between classes in equation 1. Hence the string, and is hence ignored. We used a log- when presenting features, it can be assumed that the linear model with no Markov dependency between class label is part of the feature, even if it is not exadjacent tags,3 and trained the parameters of the plicitly mentioned. model with the perceptron algorithm, with averag- The three feature sets are not completely disjoint. ing to control for over-training (Collins, 2002). We include simple position-based features in every Let C={E, I} be the set of classes: seg- system, defined as follows. Because edus are typimentation boundary (E) or non-boundary (I). Let cally multi-word strings, it is less likely for a word f(c, i, w1 ... wk) be a function that takes as in- near the beginning or end of a sentence to be at an put a class value c, a word index i and the word edu boundary. Thus it is reasonable to expect the string w1 ... wk and returns a d-dimensional vector position within a sentence of a token to be a helpful of feature values for that word index in that </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M.J. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cristea</author>
<author>O Postolache</author>
<author>I Pistol</author>
</authors>
<title>Summarisation through discourse structure.</title>
<date>2005</date>
<booktitle>In 6th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing).</booktitle>
<contexts>
<context position="994" citStr="Cristea et al., 2005" startWordPosition="133" endWordPosition="136"> segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. 1 Introduction Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005), sentence compression (Sporleder and Lapata, 2005), natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006). These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “Satellite” depending on</context>
<context position="29193" citStr="Cristea et al., 2005" startWordPosition="4757" endWordPosition="4760">ms are presented in Table 3: SPADE, our “Full finite-state” system, and our system with all features. Results for unlabeled bracketing are presented, along with results for labeled bracketing, where the label is either Nucleus or Satellite, depending upon whether or not the node is more central (Nucleus) to the coherence of the text than its sibling(s) (Satellite). This label set has been shown to be of particular utility for indicating which segments are more important to include in an automatically created summary or compressed sentence (Sporleder and Lapata, 2005; Marcu, 1998; Marcu, 1999; Cristea et al., 2005). Once again, the finite-state system does not perform statistically significantly different from SPADE on either labeled or unlabeled discourse parsing. Using all features, however, results in greater than 5% absolute accuracy improvement over both of these systems, which is significant, in all cases, at p &lt; 0.001. 4 Discussion and future directions Our results show that context-free parse derived features are critical for achieving the highest level of accuracy in sentence-level discourse segmentation. Given that edus are by definition clause-like units, it is not surprising that accurate fu</context>
</contexts>
<marker>Cristea, Postolache, Pistol, 2005</marker>
<rawString>D. Cristea, O. Postolache, and I. Pistol. 2005. Summarisation through discourse structure. In 6th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hollingshead</author>
<author>S Fisher</author>
<author>B Roark</author>
</authors>
<title>Comparing and combining finite-state and context-free parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>787--794</pages>
<marker>Hollingshead, Fisher, Roark, 2005</marker>
<rawString>K. Hollingshead, S. Fisher, and B. Roark. 2005. Comparing and combining finite-state and context-free parsers. In Proceedings of HLT-EMNLP, pages 787–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Knott</author>
</authors>
<title>A Data-Driven Methodology for Motivating a Set of Coherence Relations.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Artificial Intelligence, University of Edinburgh.</institution>
<contexts>
<context position="17879" citStr="Knott (1996)" startWordPosition="2890" endWordPosition="2891"> at step j is updated as fol- as tag or chunk sequences. For example, the tree lows: in Figure 2 represents a shallow (non-hierarchical) Oj _ 0j−1 − f(c, i, w1 ... wk) + f(c, i, w1 ... wk) (2) parse tree, with four base constituents. Each base As stated in Section 2.1, we reserved every tenth constituent X begins with a word labeled with BX, sentence as held-out data. After each pass over the which signifies that this word begins the constituent. training data, we evaluated the system performance All other words within a constituent X are labeled 4We tried using a list of 311 cue phrases from Knott (1996) to define features, but did not derive any system improvement through this list, presumably because our simple n-gram features already capture many such lexical cues. 3Because of the sparsity of boundary tags, Markov dependencies between tags buy no additional system accuracy. 491 ROOT ��������� � � �� � � � � � � � � � � � NP BNP NN tomorrow Figure 2: Tree representation of shallow parses, with B(egin) and I(nside) tags IX, and words outside of any base constituent are labeled O. In such a way, each word is labeled with both a POS-tag and a B/I/O tag. For our three sequences (lexical, POS-ta</context>
<context position="19400" citStr="Knott (1996)" startWordPosition="3159" endWordPosition="3160">ure; additionally, all n-grams for n &lt; 6 such that either wi or wi+1 (or both) is in the n-gram are included as features. In other words, all n-grams in a six word window of boundary position i are included as features, except those that include neither wi nor wi+1 in the n-gram. The identical feature templates are used with POS-tag and shallow tag sequences as well, to define tag n-gram features. This feature set is very close to that used in Sporleder and Lapata (2005), but not identical. Their n-gram feature definitions were different (though similar), and they made use of cue phrases from Knott (1996). In addition, they used a rulebased clauser that we did not. Despite such differences, this feature set is quite close to what is described in that paper. 2.5.2 Context-free features To describe our context-free features, we first present how SPADE made use of context-free parse trees within their segmentation algorithm, since this forms the basis of our features. The SPADE features are based on productions extracted from full syntactic parses of the given sentence. The primary feature for a discourse boundary after word wi is based on the lowest constituent in the tree that spans words wm ..</context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>A. Knott. 1996. A Data-Driven Methodology for Motivating a Set of Coherence Relations. Ph.D. thesis, Department of Artificial Intelligence, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="6682" citStr="Mann and Thompson, 1988" startWordPosition="1006" endWordPosition="1010">larities between the various sentence-level segmentation guidelines. Given that segmentation has been shown to be a primary impediment to high accuracy sentence-level discourse structure annotation, this represents a large step forward in our ability to automatically parse the discourse structure of text, whatever annotation approach we choose. 2 Methods 2.1 Data For our experiments we use the Rhetorical Structure Theory Discourse Treebank (Carlson et al., 2002), which we will denote RST-DT, a corpus annotated with discourse segmentation and relations according to Rhetorical Structure Theory (Mann and Thompson, 1988). The RST-DT consists of 385 documents from the Wall Street Journal, about 176,000 words, which overlaps with the Penn Wall St. Journal (WSJ) Treebank (Marcus et al., 1993). The segmentation of sentences in the RST-DT is into clause-like units, known as elementary discourse units, or edus. We will use the two terms ‘edu’ and ‘segment’ interchangeably throughout the rest of the paper. Human agreement for this segmentation task is quite high, with agreement between two annotators at an F-score of 98.3 for unlabeled segmentation (Soricut and Marcu, 2003). The RST-DT corpus annotates edu breaks, w</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W.C. Mann and S.A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Improving summarization through rhetorical parsing tuning.</title>
<date>1998</date>
<booktitle>In The 6th Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="958" citStr="Marcu, 1998" startWordPosition="129" endWordPosition="130">c sentence-level discourse segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. 1 Introduction Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005), sentence compression (Sporleder and Lapata, 2005), natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006). These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “</context>
<context position="29157" citStr="Marcu, 1998" startWordPosition="4753" endWordPosition="4754">he results for three systems are presented in Table 3: SPADE, our “Full finite-state” system, and our system with all features. Results for unlabeled bracketing are presented, along with results for labeled bracketing, where the label is either Nucleus or Satellite, depending upon whether or not the node is more central (Nucleus) to the coherence of the text than its sibling(s) (Satellite). This label set has been shown to be of particular utility for indicating which segments are more important to include in an automatically created summary or compressed sentence (Sporleder and Lapata, 2005; Marcu, 1998; Marcu, 1999; Cristea et al., 2005). Once again, the finite-state system does not perform statistically significantly different from SPADE on either labeled or unlabeled discourse parsing. Using all features, however, results in greater than 5% absolute accuracy improvement over both of these systems, which is significant, in all cases, at p &lt; 0.001. 4 Discussion and future directions Our results show that context-free parse derived features are critical for achieving the highest level of accuracy in sentence-level discourse segmentation. Given that edus are by definition clause-like units, i</context>
</contexts>
<marker>Marcu, 1998</marker>
<rawString>D. Marcu. 1998. Improving summarization through rhetorical parsing tuning. In The 6th Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Discourse trees are good indicators of importance in text.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>123--136</pages>
<editor>In I. Mani and M. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="971" citStr="Marcu, 1999" startWordPosition="131" endWordPosition="132">vel discourse segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. 1 Introduction Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005), sentence compression (Sporleder and Lapata, 2005), natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006). These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “</context>
<context position="29170" citStr="Marcu, 1999" startWordPosition="4755" endWordPosition="4756">r three systems are presented in Table 3: SPADE, our “Full finite-state” system, and our system with all features. Results for unlabeled bracketing are presented, along with results for labeled bracketing, where the label is either Nucleus or Satellite, depending upon whether or not the node is more central (Nucleus) to the coherence of the text than its sibling(s) (Satellite). This label set has been shown to be of particular utility for indicating which segments are more important to include in an automatically created summary or compressed sentence (Sporleder and Lapata, 2005; Marcu, 1998; Marcu, 1999; Cristea et al., 2005). Once again, the finite-state system does not perform statistically significantly different from SPADE on either labeled or unlabeled discourse parsing. Using all features, however, results in greater than 5% absolute accuracy improvement over both of these systems, which is significant, in all cases, at p &lt; 0.001. 4 Discussion and future directions Our results show that context-free parse derived features are critical for achieving the highest level of accuracy in sentence-level discourse segmentation. Given that edus are by definition clause-like units, it is not surp</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>D. Marcu. 1999. Discourse trees are good indicators of importance in text. In I. Mani and M. Maybury, editors, Advances in Automatic Text Summarization, pages 123–136. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="6854" citStr="Marcus et al., 1993" startWordPosition="1037" endWordPosition="1040"> structure annotation, this represents a large step forward in our ability to automatically parse the discourse structure of text, whatever annotation approach we choose. 2 Methods 2.1 Data For our experiments we use the Rhetorical Structure Theory Discourse Treebank (Carlson et al., 2002), which we will denote RST-DT, a corpus annotated with discourse segmentation and relations according to Rhetorical Structure Theory (Mann and Thompson, 1988). The RST-DT consists of 385 documents from the Wall Street Journal, about 176,000 words, which overlaps with the Penn Wall St. Journal (WSJ) Treebank (Marcus et al., 1993). The segmentation of sentences in the RST-DT is into clause-like units, known as elementary discourse units, or edus. We will use the two terms ‘edu’ and ‘segment’ interchangeably throughout the rest of the paper. Human agreement for this segmentation task is quite high, with agreement between two annotators at an F-score of 98.3 for unlabeled segmentation (Soricut and Marcu, 2003). The RST-DT corpus annotates edu breaks, which typically include sentence boundaries, but sentence boundaries are not explicitly annotated in the corpus. To perform sentence-level processing and evaluation, we alig</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Miltsakaki</author>
<author>R Prasad</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse TreeBank.</title>
<date>2004</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="1886" citStr="Miltsakaki et al., 2004" startWordPosition="269" endWordPosition="272">within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “Satellite” depending on how central the segment is to the coherence of the text. There are a number of corpora annotated with discourse structure, including the well-known RST Treebank (Carlson et al., 2002); the Discourse GraphBank (Wolf and Gibson, 2005); and the Penn Discourse Treebank (Miltsakaki et al., 2004). While the annotation approaches differ across these corpora, the requirement of sentence segmentation into Root � ����� � � � � Satellite ����� �� � � � according to CEO Smith ��� � ���� �� � � � � Prices have dropped Figure 1: Example Nucleus/Satellite labeled sentence-level discourse tree. sub-sentential discourse units is shared across all approaches. These resources have facilitated research into stochastic models and algorithms for automatic discourse structure annotation in recent years. Using the RST Treebank as training and evaluation data, Soricut and Marcu (2003) demonstrated that </context>
<context position="32465" citStr="Miltsakaki et al., 2004" startWordPosition="5258" endWordPosition="5261"> levels of segmentation and parsing for both corpora are attained, it will be possible to perform extrinsic evaluations to determine their relative utility for different NLP tasks. Recent work has shown promising preliminary results for recognizing and labeling relations of GraphBank structures (Wellner et al., 2006), in the case that the algorithm is provided with 494 manually segmented sentences. Sentence-level segmentation in the GraphBank is very similar to that in the RST-DT, so our segmentation approach should work well for Discourse GraphBank style parsing. The Penn Discourse Treebank (Miltsakaki et al., 2004), or PDTB, uses a relatively flat annotation of discourse structure, in contrast to the hierarchical structures found in the other two corpora. It contains annotations for discourse connectives and their arguments, where an argument can be as small as a nominalization or as large as several sentences. This approach obviates the need to create a set of discourse relations, but sentence internal segmentation is still a necessary step. Though segmentation in the PDTB tends to larger units than edus, our approach to segmentation should be straightforwardly applicable to their segmentation style. A</context>
</contexts>
<marker>Miltsakaki, Prasad, Joshi, Webber, 2004</marker>
<rawString>E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004. The Penn Discourse TreeBank. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>A Joshi</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse TreeBank as a resource for natural language generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Corpus Linguistics Workshop on Using Corpora for Natural Language Generation.</booktitle>
<contexts>
<context position="1096" citStr="Prasad et al., 2005" startWordPosition="147" endWordPosition="150">-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. 1 Introduction Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005), sentence compression (Sporleder and Lapata, 2005), natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006). These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “Satellite” depending on how central the segment is to the coherence of the text. There are a number of corpora annotated with</context>
</contexts>
<marker>Prasad, Joshi, Dinesh, Lee, Miltsakaki, Webber, 2005</marker>
<rawString>R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, and B. Webber. 2005. The Penn Discourse TreeBank as a resource for natural language generation. In Proceedings of the Corpus Linguistics Workshop on Using Corpora for Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Human Language Technology Conference of the North American Association for Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="2467" citStr="Soricut and Marcu (2003)" startWordPosition="361" endWordPosition="364">iscourse Treebank (Miltsakaki et al., 2004). While the annotation approaches differ across these corpora, the requirement of sentence segmentation into Root � ����� � � � � Satellite ����� �� � � � according to CEO Smith ��� � ���� �� � � � � Prices have dropped Figure 1: Example Nucleus/Satellite labeled sentence-level discourse tree. sub-sentential discourse units is shared across all approaches. These resources have facilitated research into stochastic models and algorithms for automatic discourse structure annotation in recent years. Using the RST Treebank as training and evaluation data, Soricut and Marcu (2003) demonstrated that their automatic sentence-level discourse parsing system could achieve near-human levels of accuracy, if it was provided with manual segmentations and manual parse trees. Manual segmentation was primarily responsible for this performance boost over their fully automatic system, thus making the case that automatic discourse segmentation is the primary impediment to high accuracy automatic sentence-level discourse structure annotation. Their models and algorithm – subsequently packaged together into the publicly available SPADE discourse parser1 – make use of the output of the </context>
<context position="7239" citStr="Soricut and Marcu, 2003" startWordPosition="1099" endWordPosition="1102"> according to Rhetorical Structure Theory (Mann and Thompson, 1988). The RST-DT consists of 385 documents from the Wall Street Journal, about 176,000 words, which overlaps with the Penn Wall St. Journal (WSJ) Treebank (Marcus et al., 1993). The segmentation of sentences in the RST-DT is into clause-like units, known as elementary discourse units, or edus. We will use the two terms ‘edu’ and ‘segment’ interchangeably throughout the rest of the paper. Human agreement for this segmentation task is quite high, with agreement between two annotators at an F-score of 98.3 for unlabeled segmentation (Soricut and Marcu, 2003). The RST-DT corpus annotates edu breaks, which typically include sentence boundaries, but sentence boundaries are not explicitly annotated in the corpus. To perform sentence-level processing and evaluation, we aligned the RST-DT documents to the same documents in the Penn WSJ Treebank, and used the sentence boundaries from that corpus.2 An additional benefit of this alignment is that the Penn WSJ Treebank tokenization is then available for parsing purposes. Simple minimum edit distance alignment effectively allowed for differences in punctuation representation (e.g., double quotes) and tokeni</context>
<context position="9706" citStr="Soricut and Marcu (2003)" startWordPosition="1497" endWordPosition="1500"> there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from Sporleder and Lapata (2005) in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used. All other trials use the full 991 sentence test set. Segmentation evaluation is done with precision, recall and F1-score of segmentation boundaries. Given a word string w1 ... wk, we can index word boundaries from 0 to k, so that each word wi falls between boundaries i−1 and i. For sentence-based segmentation, indices 0 and k, representing the beginning and end of the string, are known to be segment boundaries. Hence Soricut and Marcu (2003) evaluate with respect to sentence internal segmentation boundaries, i.e., with indices j such that 0&lt;j&lt;k for a sentence of length k. Let g be th</context>
<context position="11797" citStr="Soricut and Marcu (2003)" startWordPosition="1843" endWordPosition="1846"> current configuration: 91.04 Table 1: Segmentation results on the Sporleder and Lapata (2005) data set, with accuracy defined to include sentence initial segmentation boundaries. mentation boundaries j such that 0 &lt; j &lt; k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package. We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2. 2.3 Baseline SPADE setup The publicly available SPADE package, which encodes the approach in Soricut and Marcu (2003), is taken as the baseline for this paper. We made several modifications to the script from the default, which account for better baseline performance than is achieved with the default configuration. First, we modified the script to take given parse trees as input, rather than running the Charniak parser itself. This allowed us to make two modifications that improved performance: turning off tokenization in the Charniak parser, and reranking. The default script that comes with SPADE does not turn off tokenization inside of the parser, which leads to degraded performance when the input has alre</context>
<context position="23077" citStr="Soricut and Marcu (2003)" startWordPosition="3793" endWordPosition="3796"> �� �� BVP M IVP D VBD will sell BNP DT the NP �� �� INP NNS stocks � 492 Segmentation system Segment Boundary accuracy F1 Recall Bracketing F1 Recall Precision accuracy Precision SPADE 85.4 85.5 85.5 77.7 77.9 77.8 Classifier: Basic finite-state 81.5 83.3 82.4 73.6 74.5 74.0 Classifier: Full finite-state 84.1 87.9 86.0 78.0 80.0 79.0 Classifier: Context-free 84.7 91.1 87.8 80.3 83.7 82.0 Classifier: All features 89.7 91.3 90.5 84.9 85.8 85.3 Table 2: Segmentation results on all 991 sentences in the RST-DT test set. Segment boundary accuracy is for sentence internal boundaries only, following Soricut and Marcu (2003). Bracketing accuracy is for unlabeled flat bracketing of the same segments. While boundary accuracy correctly depicts segmentation results, the harsher flat bracketing metric better predicts discourse parsing performance. to the word wi. For these productions, the segmentation boundary ||will occur after all children in the production, e.g., Bj_1 → C1 ... C,,||, which is then used in both lexicalized and unlexicalized forms. For the feature with only categories adjacent to the boundary, we again use “N/A” to denote the fact that no category occurs to the right of the boundary: Bj_1 → ∗C,,,||N</context>
<context position="27451" citStr="Soricut and Marcu (2003)" startWordPosition="4489" endWordPosition="4493">ercent absolute, and the system with all features improves on the best finite-state system by over four percent absolute. The system 5In the “All features” condition, the finite-state approximation features defined in Section 2.5.3 only include a maximum of 3 children to the left and right of the boundary, versus a maximum of 8 for the “Full finite-state” system. This was found to be optimal on the development set. 493 Segmentation Unlabeled Nuc/Sat SPADE 76.9 70.2 Classifier: Full finite state 78.1 71.1 Classifier: All features 83.5 76.1 Table 3: Discourse parsing results on the 951 sentence Soricut and Marcu (2003) evaluation set, using SPADE for parsing, and various methods for segmentation. Scores are unlabeled and labeled (Nucleus/Satellite) bracketing accuracy (F1). The first line shows SPADE performing both segmentation and discourse parsing. The other two lines show SPADE performing discourse parsing with segmentations produced by our classifier using different combinations of features. with all features is statistically significantly better than both SPADE and the “Full finite-state” classifier system, at p &lt; 0.001. This large improvement demonstrates that the context-free features can provide a </context>
<context position="30413" citStr="Soricut and Marcu, 2003" startWordPosition="4934" endWordPosition="4937">e full syntactic parse trees provide highly relevant information unavailable from finite-state approaches. Adding contextfree features to our best finite-state feature model reduces error in segmentation by 32.1%, an increase in absolute F-score of 4.5%. These increases are against a finite-state segmentation model that is powerful enough to be statistically indistinguishable from SPADE. Our experiments also confirm that increased segmentation accuracy yields significantly better discourse parsing accuracy, as previously shown to be the case when providing reference segmentations to a parser (Soricut and Marcu, 2003). The segmentation reduction in error of 34.5% propagates to a 28.6% reduction in error for unlabeled discourse parse trees, and a 19.8% reduction in error for trees labeled with Nucleus and Satellite. We have several key directions in which to continue this work. First, given that a general machine learning approach allowed us to improve upon SPADE’s segmentation performance, we also believe that it will prove useful for improving full discourse parsing, both at the sentence level and beyond. For efficient inter-sentential discourse parsing, we see the need for an additional level of segmenta</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R. Soricut and D. Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Human Language Technology Conference of the North American Association for Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>M Lapata</author>
</authors>
<title>Discourse chunking and its application to sentence compression.</title>
<date>2005</date>
<booktitle>In Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP),</booktitle>
<pages>257--264</pages>
<contexts>
<context position="1045" citStr="Sporleder and Lapata, 2005" startWordPosition="139" endWordPosition="143">ng approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. 1 Introduction Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005), sentence compression (Sporleder and Lapata, 2005), natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006). These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “Satellite” depending on how central the segment is to the coherence of the</context>
<context position="3196" citStr="Sporleder and Lapata (2005)" startWordPosition="466" endWordPosition="469">vels of accuracy, if it was provided with manual segmentations and manual parse trees. Manual segmentation was primarily responsible for this performance boost over their fully automatic system, thus making the case that automatic discourse segmentation is the primary impediment to high accuracy automatic sentence-level discourse structure annotation. Their models and algorithm – subsequently packaged together into the publicly available SPADE discourse parser1 – make use of the output of the Charniak (2000) parser to derive syntactic indicator features for segmentation and discourse parsing. Sporleder and Lapata (2005) also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003), was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers. The annotations that they derive are dis1http://www.isi.edu/publications/licensed-sw/spade/ Nucleus ���� � � � � Nucleus Satellite but remain quite high 488 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 488–495, Prague, Czech Republic, June 2007. c�2007 Asso</context>
<context position="4854" citStr="Sporleder and Lapata (2005)" startWordPosition="717" endWordPosition="720">s of Sporleder and Lapata (2005): how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios. While Sporleder and Lapata (2005) demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task. SPADE makes use of a particular kind of feature from the parse trees, and does not train a general classifier making use of other features beyond the parse-derived indicator features. As we shall show, its performance is not the highest that can be achieved via context-free parser derived features. In this paper, we train a classifier using a general machine learning</context>
<context position="8863" citStr="Sporleder and Lapata (2005)" startWordPosition="1354" endWordPosition="1357">e number of iterations used when training models. 2.2 Evaluation Previous research into RST-DT segmentation and parsing has focused on subsets of the 991 sentence test set during evaluation. Soricut and Marcu (2003) 2A small number of document final parentheticals are in the RST-DT and not in the Penn WSJ Treebank, which our alignment approach takes into account. 489 omitted sentences that were not exactly spanned by a subtree of the treebank, so that they could focus on sentence-level discourse parsing. By our count, this eliminates 40 of the 991 sentences in the test set from consideration. Sporleder and Lapata (2005) went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from Sporleder and Lapata (2005) in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we inve</context>
<context position="10631" citStr="Sporleder and Lapata (2005)" startWordPosition="1659" endWordPosition="1662">nce-based segmentation, indices 0 and k, representing the beginning and end of the string, are known to be segment boundaries. Hence Soricut and Marcu (2003) evaluate with respect to sentence internal segmentation boundaries, i.e., with indices j such that 0&lt;j&lt;k for a sentence of length k. Let g be the number of sentence-internal segmentation boundaries in the gold standard, t the number of sentence-internal segmentation boundaries in the system output, and m the number of correct sentence-internal segmentation boundaries in the system output. Then P = m t R = m g and F1 = 2PR P+R = 2m g+t In Sporleder and Lapata (2005), they were primarily interested in labeled segmentation, where the segment initial boundary was labeled with the segment type. In such a scenario, the boundary at index 0 is no longer known, hence their evaluation included those boundaries, even when reporting unlabeled results. Thus, in section 2.3, for comparison with reported results in Sporleder and Lapata (2005), our F1-score is defined accordingly, i.e., segSegmentation system F1 Sporleder and Lapata best (reported) 88.40 SPADE 87.06 Sporleder and Lapata configuration (reported): current configuration: 91.04 Table 1: Segmentation result</context>
<context position="12858" citStr="Sporleder and Lapata (2005)" startWordPosition="2010" endWordPosition="2013">d reranking. The default script that comes with SPADE does not turn off tokenization inside of the parser, which leads to degraded performance when the input has already been tokenized in the Penn Treebank style. Secondly, Charniak and Johnson (2005) showed how reranking of the 50- best output of the Charniak (2000) parser gives substantial improvements in parsing accuracy. These two modifications to the Charniak parsing output used by the SPADE system lead to improvements in its performance compared to previously reported results. Table 1 compares segmentation results of three systems on the Sporleder and Lapata (2005) 608 sentence subset of the evaluation data: (1) their best reported system; (2) the SPADE system results reported in that paper; and (3) the SPADE system results with our current configuration. The evaluation uses the unlabeled F1 measure as defined in that paper, which counts sentence initial boundaries in the scoring, as discussed in the previous section. As can be seen from these results, our improved configuration of SPADE gives us large improvements over the previously reported SPADE performance on this subset. As a result, we feel that we can use SPADE 490 as a very strong baseline for </context>
<context position="19263" citStr="Sporleder and Lapata (2005)" startWordPosition="3136" endWordPosition="3139">etween wi and wi+1. For this boundary position, the 6-gram including the three words before and the three words after the boundary is included as a feature; additionally, all n-grams for n &lt; 6 such that either wi or wi+1 (or both) is in the n-gram are included as features. In other words, all n-grams in a six word window of boundary position i are included as features, except those that include neither wi nor wi+1 in the n-gram. The identical feature templates are used with POS-tag and shallow tag sequences as well, to define tag n-gram features. This feature set is very close to that used in Sporleder and Lapata (2005), but not identical. Their n-gram feature definitions were different (though similar), and they made use of cue phrases from Knott (1996). In addition, they used a rulebased clauser that we did not. Despite such differences, this feature set is quite close to what is described in that paper. 2.5.2 Context-free features To describe our context-free features, we first present how SPADE made use of context-free parse trees within their segmentation algorithm, since this forms the basis of our features. The SPADE features are based on productions extracted from full syntactic parses of the given s</context>
<context position="29144" citStr="Sporleder and Lapata, 2005" startWordPosition="4749" endWordPosition="4752">erarchical discourse tree. The results for three systems are presented in Table 3: SPADE, our “Full finite-state” system, and our system with all features. Results for unlabeled bracketing are presented, along with results for labeled bracketing, where the label is either Nucleus or Satellite, depending upon whether or not the node is more central (Nucleus) to the coherence of the text than its sibling(s) (Satellite). This label set has been shown to be of particular utility for indicating which segments are more important to include in an automatically created summary or compressed sentence (Sporleder and Lapata, 2005; Marcu, 1998; Marcu, 1999; Cristea et al., 2005). Once again, the finite-state system does not perform statistically significantly different from SPADE on either labeled or unlabeled discourse parsing. Using all features, however, results in greater than 5% absolute accuracy improvement over both of these systems, which is significant, in all cases, at p &lt; 0.001. 4 Discussion and future directions Our results show that context-free parse derived features are critical for achieving the highest level of accuracy in sentence-level discourse segmentation. Given that edus are by definition clause-</context>
</contexts>
<marker>Sporleder, Lapata, 2005</marker>
<rawString>C. Sporleder and M. Lapata. 2005. Discourse chunking and its application to sentence compression. In Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP), pages 257–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>A Lascarides</author>
</authors>
<title>Combining hierarchical clustering and machine learning to predict high-level discourse structure.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference in Computational Linguistics (COLING),</booktitle>
<pages>43--49</pages>
<contexts>
<context position="31133" citStr="Sporleder and Lascarides (2004)" startWordPosition="5048" endWordPosition="5051">or unlabeled discourse parse trees, and a 19.8% reduction in error for trees labeled with Nucleus and Satellite. We have several key directions in which to continue this work. First, given that a general machine learning approach allowed us to improve upon SPADE’s segmentation performance, we also believe that it will prove useful for improving full discourse parsing, both at the sentence level and beyond. For efficient inter-sentential discourse parsing, we see the need for an additional level of segmentation at the paragraph level. Whereas most sentences correspond to a well-formed subtree, Sporleder and Lascarides (2004) report that over 20% of the paragraph boundaries in the RST-DT do not correspond to a well-formed subtree in the human annotated discourse parse for that document. Therefore, to perform accurate and efficient parsing of the RST-DT at the paragraph level, the text should be segmented into paragraph-like segments that conform to the human-annotated subtree boundaries, just as sentences are segmented into edus. We also intend to begin work on the other discourse annotated corpora. While most work on textual discourse parsing has made use of the RST-DT corpus, the Discourse GraphBank corpus provi</context>
</contexts>
<marker>Sporleder, Lascarides, 2004</marker>
<rawString>C. Sporleder and A. Lascarides. 2004. Combining hierarchical clustering and machine learning to predict high-level discourse structure. In Proceedings of the International Conference in Computational Linguistics (COLING), pages 43–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>127--132</pages>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E.F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of CoNLL, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Verberne</author>
<author>L Boves</author>
<author>N Oostdijk</author>
<author>P A Coppen</author>
</authors>
<title>Discourse-based answering of why-questions. Traitement Automatique des Langues (TAL).</title>
<date>2006</date>
<contexts>
<context position="1143" citStr="Verberne et al., 2006" startWordPosition="154" endWordPosition="157"> the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. 1 Introduction Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005), sentence compression (Sporleder and Lapata, 2005), natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006). These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “Satellite” depending on how central the segment is to the coherence of the text. There are a number of corpora annotated with discourse structure, including the well-known </context>
</contexts>
<marker>Verberne, Boves, Oostdijk, Coppen, 2006</marker>
<rawString>S. Verberne, L. Boves, N. Oostdijk, and P.A. Coppen. 2006. Discourse-based answering of why-questions. Traitement Automatique des Langues (TAL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellner</author>
<author>J Pustejovsky</author>
<author>C Havasi</author>
<author>A Rumshisky</author>
<author>R Sauri</author>
</authors>
<title>Classification of discourse coherence relations: An exploratory study using multiple knowledge sources.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="32159" citStr="Wellner et al., 2006" startWordPosition="5211" endWordPosition="5214">o edus. We also intend to begin work on the other discourse annotated corpora. While most work on textual discourse parsing has made use of the RST-DT corpus, the Discourse GraphBank corpus provides a competing annotation that is not constrained to tree structures (Wolf and Gibson, 2005). Once accurate levels of segmentation and parsing for both corpora are attained, it will be possible to perform extrinsic evaluations to determine their relative utility for different NLP tasks. Recent work has shown promising preliminary results for recognizing and labeling relations of GraphBank structures (Wellner et al., 2006), in the case that the algorithm is provided with 494 manually segmented sentences. Sentence-level segmentation in the GraphBank is very similar to that in the RST-DT, so our segmentation approach should work well for Discourse GraphBank style parsing. The Penn Discourse Treebank (Miltsakaki et al., 2004), or PDTB, uses a relatively flat annotation of discourse structure, in contrast to the hierarchical structures found in the other two corpora. It contains annotations for discourse connectives and their arguments, where an argument can be as small as a nominalization or as large as several se</context>
</contexts>
<marker>Wellner, Pustejovsky, Havasi, Rumshisky, Sauri, 2006</marker>
<rawString>B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky, and R. Sauri. 2006. Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wolf</author>
<author>E Gibson</author>
</authors>
<title>Representing discourse coherence: A corpus-based analysis.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="1827" citStr="Wolf and Gibson, 2005" startWordPosition="260" endWordPosition="263">se units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “Satellite” depending on how central the segment is to the coherence of the text. There are a number of corpora annotated with discourse structure, including the well-known RST Treebank (Carlson et al., 2002); the Discourse GraphBank (Wolf and Gibson, 2005); and the Penn Discourse Treebank (Miltsakaki et al., 2004). While the annotation approaches differ across these corpora, the requirement of sentence segmentation into Root � ����� � � � � Satellite ����� �� � � � according to CEO Smith ��� � ���� �� � � � � Prices have dropped Figure 1: Example Nucleus/Satellite labeled sentence-level discourse tree. sub-sentential discourse units is shared across all approaches. These resources have facilitated research into stochastic models and algorithms for automatic discourse structure annotation in recent years. Using the RST Treebank as training and e</context>
<context position="31826" citStr="Wolf and Gibson, 2005" startWordPosition="5161" endWordPosition="5164">correspond to a well-formed subtree in the human annotated discourse parse for that document. Therefore, to perform accurate and efficient parsing of the RST-DT at the paragraph level, the text should be segmented into paragraph-like segments that conform to the human-annotated subtree boundaries, just as sentences are segmented into edus. We also intend to begin work on the other discourse annotated corpora. While most work on textual discourse parsing has made use of the RST-DT corpus, the Discourse GraphBank corpus provides a competing annotation that is not constrained to tree structures (Wolf and Gibson, 2005). Once accurate levels of segmentation and parsing for both corpora are attained, it will be possible to perform extrinsic evaluations to determine their relative utility for different NLP tasks. Recent work has shown promising preliminary results for recognizing and labeling relations of GraphBank structures (Wellner et al., 2006), in the case that the algorithm is provided with 494 manually segmented sentences. Sentence-level segmentation in the GraphBank is very similar to that in the RST-DT, so our segmentation approach should work well for Discourse GraphBank style parsing. The Penn Disco</context>
</contexts>
<marker>Wolf, Gibson, 2005</marker>
<rawString>F. Wolf and E. Gibson. 2005. Representing discourse coherence: A corpus-based analysis. Computational Linguistics, 31(2):249–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International COLING,</booktitle>
<pages>947--953</pages>
<contexts>
<context position="25737" citStr="Yeh, 2000" startWordPosition="4230" endWordPosition="4231">e derived solely from shallow finite-state tagging. Our primary concern is with intra-sentential discourse segmentation, but we are also interested in how much the improved segmentation helps discourse parsing. The syntactic parser we use for all context-free syntactic parses used in either SPADE or our classifier is the Charniak parser with reranking, as described in Charniak and Johnson (2005). The Charniak parser and reranker were trained on the sections of the Penn Treebank not included in the RST-DT test set. All statistical significance testing is done via the stratified shuffling test (Yeh, 2000). 3.1 Segmentation Table 2 presents segmentation results for SPADE and four versions of our classifier. The “Basic finitestate” system uses only finite-state sequence features as defined in Section 2.5.1, while the “Full finite-state” also includes the finite-state approximation features from Section 2.5.3. The “Context-free” system uses the SPADE-inspired features detailed in Section 2.5.2, but none of the features from Sections 2.5.1 or 2.5.3. Finally, the “All features” section includes features from all three sections.5 Note that the full finite-state system is considerably better than the</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>A. Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th International COLING, pages 947–953.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>