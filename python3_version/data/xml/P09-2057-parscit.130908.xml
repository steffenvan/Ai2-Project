<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024363">
<title confidence="0.997923">
A Beam-Search Extraction Algorithm for Comparable Data
</title>
<author confidence="0.84934">
Christoph Tillmann
</author>
<affiliation confidence="0.483962">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.41222">
Yorktown Heights, N.Y. 10598
</address>
<email confidence="0.968627">
ctill@us.ibm.com
</email>
<sectionHeader confidence="0.992947" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897375">
This paper extends previous work on ex-
tracting parallel sentence pairs from com-
parable data (Munteanu and Marcu, 2005).
For a given source sentence 5, a max-
imum entropy (ME) classifier is applied
to a large set of candidate target transla-
tions . A beam-search algorithm is used
to abandon target sentences as non-parallel
early on during classification if they fall
outside the beam. This way, our novel
algorithm avoids any document-level pre-
filtering step. The algorithm increases the
number of extracted parallel sentence pairs
significantly, which leads to a BLEU im-
provement of about 1 % on our Spanish-
English data.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999190153846154">
The paper presents a novel algorithm for ex-
tracting parallel sentence pairs from comparable
monolingual news data. We select source-target
sentence pairs (5, T) based on a ME classifier
(Munteanu and Marcu, 2005). Because the set of
target sentences T considered can be huge, pre-
vious work (Fung and Cheung, 2004; Resnik and
Smith, 2003; Snover et al., 2008; Munteanu and
Marcu, 2005) pre-selects target sentences T at the
document level . We have re-implemented a par-
ticular filtering scheme based on BM25 (Quirk et
al., 2007; Utiyama and Isahara, 2003; Robertson
et al., 1995). In this paper, we demonstrate a dif-
ferent strategy . We compute the ME score in-
crementally at the word level and apply a beam-
search algorithm to a large number of sentences.
We abandon target sentences early on during clas-
sification if they fall outside the beam. For com-
parison purposes, we run our novel extraction al-
gorithm with and without the document-level pre-
filtering step. The results in Section 4 show that
the number of extracted sentence pairs is more
than doubled which also leads to an increase in
BLEU by about 1 % on the Spanish-English data.
The classification probability is defined as fol-
lows:
</bodyText>
<equation confidence="0.999347666666667">
exp( wT - f(c,5,T) )
p(c|5,T) = ,(1)
Z(5,T)
</equation>
<bodyText confidence="0.999737">
where 5 = sJ1 is a source sentence of length J and
T = tI1 is a target sentence of length I. c E t0, 11
is a binary variable . p(c|5, T) E [0, 1] is a proba-
bility where a value p(c = 1|5, T) close to 1.0 in-
dicates that 5 and T are translations of each other.
w E Rn is a weight vector obtained during train-
ing. f(c, 5, T) is a feature vector where the fea-
tures are co-indexed with respect to the alignment
variable c. Finally, Z(5, T) is an appropriately
chosen normalization constant.
Section 2 summarizes the use of the binary clas-
sifier. Section 3 presents the beam-search algo-
rithm. In Section 4, we show experimental results.
Finally, Section 5 discusses the novel algorithm.
</bodyText>
<sectionHeader confidence="0.965911" genericHeader="method">
2 Classifier Training
</sectionHeader>
<bodyText confidence="0.9999713125">
The classifier in Eq. 1 is based on several real-
valued feature functions fi . Their computation
is based on the so-called IBM Model-1 (Brown et
al., 1993). The Model-1 is trained on some paral-
lel data available for a language pair, i.e. the data
used to train the baseline systems in Section 4.
p(s|T) is the Model-1 probability assigned to a
source word s given the target sentence T , p(t|5)
is defined accordingly. p(s|t) and p(t|s) are word
translation probabilities obtained by two parallel
Model-1 training steps on the same data, but swap-
ping the role of source and target language. To
compute these values efficiently, the implementa-
tion techniques in (Tillmann and Xu, 2009) are
used. Coverage and fertility features are defined
based on the Model-1 Viterbi alignment: a source
</bodyText>
<page confidence="0.982763">
225
</page>
<note confidence="0.927387">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 225–228,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.989796054054054">
word s is said to be covered if there is a target
word t ∈ T such that its probability is above a
threshold ǫ: p(s|t) &gt; ǫ . We define the fertility
of a source word s as the number of target words
t ∈ T for which p(s|t) &gt; ǫ. Target word cover-
age and fertility are defined accordingly. A large
number of ‘uncovered‘ source and target positions
as well as a large number of high fertility words
indicate non-parallelism. We use the following
N = 7 features: 1,2) lexical Model-1 weight-
ing: Es −log( p(s|T) ) and Et −log( p(t|S) ),
3,4) number of uncovered source and target po-
sitions, 5,6) sum of source and target fertilities,
7) number of covered source and target positions
. These features are defined in a way that they
can be computed incrementally at the word level.
Some thresholding is applied, e.g. a sequence of
uncovered positions has to be at least 3 positions
long to generate a non-zero feature value . In the
feature vector f(c, S, T), each feature fi occurs
potentially twice, once for each class c ∈ {0, 1}.
For the feature vector f(c = 1, S, T), all the fea-
ture values corresponding to class c = 0 are set
to 0, and vice versa. This particular way of defin-
ing the feature vector is needed for the search in
Section 3: the contribution of the ’negative’ fea-
tures for c = 0 is only computed when Eq. 1 is
evaluated for the highest scoring final hypothesis
in the beam. To train the classifier, we have manu-
ally annotated a collection of 524 sentence pairs .
A sentence pair is considered parallel if at least
75 % of source and target words have a corre-
sponding translation in the other sentence, other-
wise it is labeled as non-parallel. A weight vector
w ∈ R2∗N is trained with respect to classification
accuracy using the on-line maxent training algo-
rithm in (Tillmann and Zhang, 2007).
</bodyText>
<sectionHeader confidence="0.971512" genericHeader="method">
3 Beam Search Algorithm
</sectionHeader>
<bodyText confidence="0.999989888888889">
We process the comparable data at the sentence
level: sentences are indexed based on their publi-
cation date. For each source sentence S, a match-
ing score is computed over all the target sentences
Tm ∈ Θ that have a publication date which differs
less than 7 days from the publication date of the
source sentence 1. We are aiming at finding the Tˆ
with the highest probability p(c = 1|S, Tˆ), but we
cannot compute that probability for all sentence
</bodyText>
<footnote confidence="0.831842333333333">
1In addition, the sentence length filter in (Munteanu and
Marcu, 2005) is used: the length ratio max(J, I)/min(J, I)
of source and target sentence has to be smaller than 2.
</footnote>
<bodyText confidence="0.996204741935484">
pairs (S, Tm) since |Θ |can be in tens of thousands
of sentences . Instead, we use a beam-search algo-
rithm to search for the sentence pair (S, Tˆ) with
the highest matching score wT · f(1, S, Tˆ) 2. The
’light-weight’ features defined in Section 2 are
such that the matching score can be computed in-
crementally while processing the source and target
sentence positions in some order. To that end, we
maintain a stack of matching hypotheses for each
source position j. Each hypothesis is assigned a
partial matching score based on the source and tar-
get positions processed so far. Whenever a partial
matching score is low compared to partial match-
ing scores of other target sentence candidates, that
translation pair can be discarded by carrying out
a beam-search pruning step. The search is orga-
nized in a single left-to-right run over the source
positions 1 ≤ j ≤ J and all active partial hypothe-
ses match the same portion of that source sentence.
There is at most a single active hypothesis for each
different target sentence Ti, and search states are
defined as follows:
[ m , j , uj , ui ; d ] .
Here, m ∈ {1, · · · , |Θ|} is a target sentence in-
dex. j is a position in the source sentence, uj and
ui are the number of uncovered source and target
positions to the left of source position j and tar-
get position i (coverage computation is explained
above), and d is the partial matching score . The
target position i corresponding to the source posi-
tion j is computed deterministically as follows:
</bodyText>
<equation confidence="0.995397">
i = ⌈I · jJ ⌉ , (2)
</equation>
<bodyText confidence="0.9194249375">
where the sentence lengths I and J are known
for a sentence pair (S, T). Covering an additional
source position leads to covering additional target
positions as well, and source and target features
are computed accordingly. The search is initial-
ized by adding a single hypothesis for each target
sentence Tm ∈ Θ to the stack for j = 1:
[ m , j = 1 , uj = 0 , ui = 0 ; 0 ] .
During the left-to-right search, state transitions of
the following type occur:
[ m , j , uj , ui ; d ] →
[ m , j + 1 , u′ j , u′ i ; d′ ] ,
2This is similar to standard phrase-based SMT decoding,
where a set of real-valued features is used and any sentence-
level normalization is ignored during decoding. We assume
the effect of this approximation to be small.
</bodyText>
<page confidence="0.993115">
226
</page>
<bodyText confidence="0.999959">
where the partial score is updated as: d′ = d +
wT · f(1, j, i) . Here, f(1, j, i) is a partial fea-
ture vector computed for all the additional source
and target positions processed in the last extension
step. The number of uncovered source and target
positions u′ is updated as well. The beam-search
algorithm is carried out until all source positions j
have been processed. We extract the highest scor-
ing partial hypothesis from the final stack j = J
. For that hypothesis, we compute a global feature
vector f(1, S, T) by adding all the local f(1, j, i)’s
component-wise. The ‘negative‘ feature vector
f(0, S, T) is computed from f(1, S, T) by copy-
ing its feature values. We then use Eq. 1 to com-
pute the probability p(1|S, T) and apply a thresh-
old of 0 = 0.75 to extract parallel sentence pairs.
We have adjusted beam-search pruning techniques
taken from regular SMT decoding (Tillmann et al.,
1997; Koehn, 2004) to reduce the number of hy-
potheses after each extension step. Currently, only
histogram pruning is employed to reduce the num-
ber of hypotheses in each stack.
The resulting beam-search algorithm is similar
to a monotone decoder for SMT: rather then in-
crementally generating a target translation, the de-
coder is used to select entire target sentences out of
a pre-defined list. That way, our beam search algo-
rithm is similar to algorithms in large-scale speech
recognition (Ney, 1984; Vintsyuk, 1971), where
an acoustic signal is matched to a pre-assigned list
of words in the recognizer vocabulary.
</bodyText>
<sectionHeader confidence="0.999651" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999876944444444">
The parallel sentence extraction algorithm pre-
sented in this paper is tested in detail on all of the
large-scale Spanish-English Gigaword data (Graff,
2006; Graff, 2007) as well as on some smaller
Portuguese-English news data . For the Spanish-
English data, matching sentence pairs come from
the same news feed. Table 1 shows the size of
the comparable data, and Table 2 shows the ef-
fect of including the additional sentence pairs into
the training of a phrase-based SMT system. Here,
both languages use a test set with a single ref-
erence. The test data comes from Spanish and
Portuguese news web pages that have been trans-
lated into English. Including about 1.35 million
sentence pairs extracted from the Gigaword data,
we obtain a statistically significant improvement
from 42.3 to 45.7 in BLEU. The baseline system
has been trained on about 1.8 million sentence
</bodyText>
<tableCaption confidence="0.999513">
Table 1: Corpus statistics for comparable data.
</tableCaption>
<table confidence="0.9989925">
Spanish English
Sentences 19.4 million 47.9 million
Words 601.5 million 1.36 billion
Portuguese English
Sentences 366.0 thousand 5.3 million
Words 11.6 million 171.1 million
</table>
<bodyText confidence="0.999135536585366">
pairs from Europarl and FBIS parallel data. We
also present results for a Portuguese-English sys-
tem: the baseline has been trained on Europarl and
JRC data. Parallel sentence pairs are extracted
from comparable news data published in 2006.
For this data, no document-level information was
available. To gauge the effect of the document-
level pre-filtering step, we have re-implemented
an IR technique based on BM25 (Robertson et al.,
1995). This type of pre-filtering has also been used
in (Quirk et al., 2007; Utiyama and Isahara, 2003).
We split the Spanish data into documents. Each
Spanish document is translated into a bag of En-
glish words using Model-1 lexicon probabilities
trained on the baseline data. Each of these English
bag-of-words is then issued as a query against all
the English documents that have been published
within a 7 day window of the source document.
We select the 20 highest scoring English docu-
ments for each source document . These 20 docu-
ments provide a restricted set of target sentence
candidates. The sentence-level beam-search al-
gorithm without the document-level filtering step
searches through close to 1 trillion sentence pairs.
For the data obtained by the BM25-based filtering
step, we still use the same beam-search algorithm
but on a much smaller candidate set of only 25.4
billion sentence pairs. The probability selection
threshold 0 is determined on some development
set in terms of precision and recall (based on the
definitions in (Munteanu and Marcu, 2005)). The
classifier obtains an F-measure classifications per-
formance of about 85 %. The BM25 filtering step
leads to a significantly more complex processing
pipeline since sentences have to be indexed with
respect to document boundaries and publication
date. The document-level pre-filtering reduces the
overall processing time by about 40 % (from 4 to
2.5 days on a 100-CPU cluster). However, the ex-
haustive sentence-level search improves the BLEU
score by about 1 % on the Spanish-English data.
</bodyText>
<page confidence="0.994074">
227
</page>
<table confidence="0.7884979375">
con Extraction via Bootstrapping and EM. In Proc,
of EMNLP 2004, pages 57–63, Barcelona, Spain,
July.
Dave Graff. 2006. LDC2006T12: Spanish Gigaword
Corpus First Edition. LDC.
Table 2: Spanish-English and Portuguese-English
extraction results. Extraction threshold is θ =
0.75 for both language pairs. # cands reports the
size of the overall search space in terms of sen-
tence pairs processed .
Data Source # cands # pairs Bleu
Baseline - 1.826 M 42.3
+ Giga 999.3 B 1.357 M 45.7
+ Giga (BM25) 25.4 B 0.609 M 44.8
Baseline - 2.222 M 45.3
+ News Data 2006 77.8 B 56 K 47.2
</table>
<sectionHeader confidence="0.988093" genericHeader="discussions">
5 Future Work and Discussion
</sectionHeader>
<bodyText confidence="0.999980833333333">
In this paper, we have presented a novel beam-
search algorithm to extract sentence pairs from
comparable data . It can avoid any pre-filtering
at the document level (Resnik and Smith, 2003;
Snover et al., 2008; Utiyama and Isahara, 2003;
Munteanu and Marcu, 2005; Fung and Cheung,
2004). The novel algorithm is successfully eval-
uated on news data for two language pairs. A
related approach that also avoids any document-
level pre-filtering has been presented in (Tillmann
and Xu, 2009). The efficient implementation tech-
niques in that paper are extended for the ME clas-
sifier and beam search algorithm in the current pa-
per, i.e. feature function values are cached along
with Model-1 probabilities.
The search-driven extraction algorithm presented
in this paper might also be applicable to other
NLP extraction task, e.g. named entity extraction.
Rather then employing a cascade of filtering steps,
a one-stage search with a specially adopted feature
set and search space organization might be carried
out . Such a search-driven approach makes less
assumptions about the data and may increase the
number of extracted entities, i.e. increase recall.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998475">
We would like to thanks the anonymous reviewers
for their valuable remarks.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999900320754717">
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. CL, 19(2):263–311.
Pascale Fung and Percy Cheung. 2004. Mining Very-
Non-Parallel Corpora: Parallel Sentence and Lexi-
Dave Graff. 2007. LDC2007T07: English Gigaword
Corpus Third Edition. LDC.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings ofAMTA’04, Washing-
ton DC, September-October.
Dragos S. Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. CL, 31(4):477–504.
H. Ney. 1984. The Use of a One-stage Dynamic Pro-
gramming Algorithm for Connected Word Recogni-
tion. IEEE Transactions on Acoustics, Speech, and
Signal Processing, 32(2):263–271.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative Models of Noisy Translations
with Applications to Parallel Fragment Extraction.
In Proc. of the MT Summit XI, pages 321–327,
Copenhagen,Demark, September.
Philip Resnik and Noah Smith. 2003. The Web as
Parallel Corpus. CL, 29(3):349–380.
S E Robertson, S Walker, M M Beaulieu, and M Gat-
ford. 1995. Okapi at TREC-4. In Proc. of the 4th
Text Retrieval Conference (TREC-4), pages 73–96.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and Translation Model Adaptation
using Comparable Corpora. In Proc. of EMNLP08,
pages 856–865, Honolulu, Hawaii, October.
Christoph Tillmann and Jian-Ming Xu. 2009. A Sim-
ple Sentence-Level Extraction Algorithm for Com-
parable Data. In Companion Vol. of NAACL HLT
09, pages 93–96, Boulder, Colorado, June.
Christoph Tillmann and Tong Zhang. 2007. A Block
Bigram Prediction Model for Statistical Machine
Translation. ACM-TSLP, 4(6):1–31, July.
Christoph Tillmann, Stefan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-based Search Using
Monotone Alignments in Statistical Translation. In
Proc. of ACL 97, pages 289–296, Madrid,Spain,
July.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
Measures for Aligning Japanese-English News Arti-
cles and Sentences. In Proc. ofACL03, pages 72–79,
Sapporo, Japan, July.
T.K. Vintsyuk. 1971. Element-Wise Recognition of
Continuous Speech Consisting of Words From a
Specified Vocabulary. Cybernetics (Kibernetica),
(2):133–143, March-April.
</reference>
<page confidence="0.997667">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.457259">
<title confidence="0.999734">A Beam-Search Extraction Algorithm for Comparable Data</title>
<author confidence="0.999613">Christoph Tillmann</author>
<affiliation confidence="0.999946">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.990189">Yorktown Heights, N.Y. 10598</address>
<email confidence="0.999524">ctill@us.ibm.com</email>
<abstract confidence="0.964751235294118">This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). a given source sentence a maximum entropy (ME) classifier is applied to a large set of candidate target translations . A beam-search algorithm is used to abandon target sentences as non-parallel early on during classification if they fall outside the beam. This way, our novel algorithm avoids any document-level prefiltering step. The algorithm increases the number of extracted parallel sentence pairs significantly, which leads to a BLEU improvement of about 1 % on our Spanish- English data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>CL,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2891" citStr="Brown et al., 1993" startWordPosition="495" endWordPosition="498">tions of each other. w E Rn is a weight vector obtained during training. f(c, 5, T) is a feature vector where the features are co-indexed with respect to the alignment variable c. Finally, Z(5, T) is an appropriately chosen normalization constant. Section 2 summarizes the use of the binary classifier. Section 3 presents the beam-search algorithm. In Section 4, we show experimental results. Finally, Section 5 discusses the novel algorithm. 2 Classifier Training The classifier in Eq. 1 is based on several realvalued feature functions fi . Their computation is based on the so-called IBM Model-1 (Brown et al., 1993). The Model-1 is trained on some parallel data available for a language pair, i.e. the data used to train the baseline systems in Section 4. p(s|T) is the Model-1 probability assigned to a source word s given the target sentence T , p(t|5) is defined accordingly. p(s|t) and p(t|s) are word translation probabilities obtained by two parallel Model-1 training steps on the same data, but swapping the role of source and target language. To compute these values efficiently, the implementation techniques in (Tillmann and Xu, 2009) are used. Coverage and fertility features are defined based on the Mod</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. CL, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Percy Cheung</author>
</authors>
<title>Mining VeryNon-Parallel Corpora: Parallel Sentence and LexiDave Graff.</title>
<date>2004</date>
<publisher>LDC.</publisher>
<contexts>
<context position="1109" citStr="Fung and Cheung, 2004" startWordPosition="171" endWordPosition="174"> on during classification if they fall outside the beam. This way, our novel algorithm avoids any document-level prefiltering step. The algorithm increases the number of extracted parallel sentence pairs significantly, which leads to a BLEU improvement of about 1 % on our SpanishEnglish data. 1 Introduction The paper presents a novel algorithm for extracting parallel sentence pairs from comparable monolingual news data. We select source-target sentence pairs (5, T) based on a ME classifier (Munteanu and Marcu, 2005). Because the set of target sentences T considered can be huge, previous work (Fung and Cheung, 2004; Resnik and Smith, 2003; Snover et al., 2008; Munteanu and Marcu, 2005) pre-selects target sentences T at the document level . We have re-implemented a particular filtering scheme based on BM25 (Quirk et al., 2007; Utiyama and Isahara, 2003; Robertson et al., 1995). In this paper, we demonstrate a different strategy . We compute the ME score incrementally at the word level and apply a beamsearch algorithm to a large number of sentences. We abandon target sentences early on during classification if they fall outside the beam. For comparison purposes, we run our novel extraction algorithm with </context>
<context position="13866" citStr="Fung and Cheung, 2004" startWordPosition="2426" endWordPosition="2429">d is θ = 0.75 for both language pairs. # cands reports the size of the overall search space in terms of sentence pairs processed . Data Source # cands # pairs Bleu Baseline - 1.826 M 42.3 + Giga 999.3 B 1.357 M 45.7 + Giga (BM25) 25.4 B 0.609 M 44.8 Baseline - 2.222 M 45.3 + News Data 2006 77.8 B 56 K 47.2 5 Future Work and Discussion In this paper, we have presented a novel beamsearch algorithm to extract sentence pairs from comparable data . It can avoid any pre-filtering at the document level (Resnik and Smith, 2003; Snover et al., 2008; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Fung and Cheung, 2004). The novel algorithm is successfully evaluated on news data for two language pairs. A related approach that also avoids any documentlevel pre-filtering has been presented in (Tillmann and Xu, 2009). The efficient implementation techniques in that paper are extended for the ME classifier and beam search algorithm in the current paper, i.e. feature function values are cached along with Model-1 probabilities. The search-driven extraction algorithm presented in this paper might also be applicable to other NLP extraction task, e.g. named entity extraction. Rather then employing a cascade of filter</context>
</contexts>
<marker>Fung, Cheung, 2004</marker>
<rawString>Pascale Fung and Percy Cheung. 2004. Mining VeryNon-Parallel Corpora: Parallel Sentence and LexiDave Graff. 2007. LDC2007T07: English Gigaword Corpus Third Edition. LDC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA’04,</booktitle>
<location>Washington DC, September-October.</location>
<contexts>
<context position="9281" citStr="Koehn, 2004" startWordPosition="1679" endWordPosition="1680">orithm is carried out until all source positions j have been processed. We extract the highest scoring partial hypothesis from the final stack j = J . For that hypothesis, we compute a global feature vector f(1, S, T) by adding all the local f(1, j, i)’s component-wise. The ‘negative‘ feature vector f(0, S, T) is computed from f(1, S, T) by copying its feature values. We then use Eq. 1 to compute the probability p(1|S, T) and apply a threshold of 0 = 0.75 to extract parallel sentence pairs. We have adjusted beam-search pruning techniques taken from regular SMT decoding (Tillmann et al., 1997; Koehn, 2004) to reduce the number of hypotheses after each extension step. Currently, only histogram pruning is employed to reduce the number of hypotheses in each stack. The resulting beam-search algorithm is similar to a monotone decoder for SMT: rather then incrementally generating a target translation, the decoder is used to select entire target sentences out of a pre-defined list. That way, our beam search algorithm is similar to algorithms in large-scale speech recognition (Ney, 1984; Vintsyuk, 1971), where an acoustic signal is matched to a pre-assigned list of words in the recognizer vocabulary. 4</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models. In Proceedings ofAMTA’04, Washington DC, September-October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos S Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving Machine Translation Performance by Exploiting Non-Parallel Corpora.</title>
<date>2005</date>
<journal>CL,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="1009" citStr="Munteanu and Marcu, 2005" startWordPosition="153" endWordPosition="156"> target translations . A beam-search algorithm is used to abandon target sentences as non-parallel early on during classification if they fall outside the beam. This way, our novel algorithm avoids any document-level prefiltering step. The algorithm increases the number of extracted parallel sentence pairs significantly, which leads to a BLEU improvement of about 1 % on our SpanishEnglish data. 1 Introduction The paper presents a novel algorithm for extracting parallel sentence pairs from comparable monolingual news data. We select source-target sentence pairs (5, T) based on a ME classifier (Munteanu and Marcu, 2005). Because the set of target sentences T considered can be huge, previous work (Fung and Cheung, 2004; Resnik and Smith, 2003; Snover et al., 2008; Munteanu and Marcu, 2005) pre-selects target sentences T at the document level . We have re-implemented a particular filtering scheme based on BM25 (Quirk et al., 2007; Utiyama and Isahara, 2003; Robertson et al., 1995). In this paper, we demonstrate a different strategy . We compute the ME score incrementally at the word level and apply a beamsearch algorithm to a large number of sentences. We abandon target sentences early on during classification</context>
<context position="6004" citStr="Munteanu and Marcu, 2005" startWordPosition="1053" endWordPosition="1056">ccuracy using the on-line maxent training algorithm in (Tillmann and Zhang, 2007). 3 Beam Search Algorithm We process the comparable data at the sentence level: sentences are indexed based on their publication date. For each source sentence S, a matching score is computed over all the target sentences Tm ∈ Θ that have a publication date which differs less than 7 days from the publication date of the source sentence 1. We are aiming at finding the Tˆ with the highest probability p(c = 1|S, Tˆ), but we cannot compute that probability for all sentence 1In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used: the length ratio max(J, I)/min(J, I) of source and target sentence has to be smaller than 2. pairs (S, Tm) since |Θ |can be in tens of thousands of sentences . Instead, we use a beam-search algorithm to search for the sentence pair (S, Tˆ) with the highest matching score wT · f(1, S, Tˆ) 2. The ’light-weight’ features defined in Section 2 are such that the matching score can be computed incrementally while processing the source and target sentence positions in some order. To that end, we maintain a stack of matching hypotheses for each source position j. Each hypothesis is assigned a</context>
<context position="12482" citStr="Munteanu and Marcu, 2005" startWordPosition="2193" endWordPosition="2196">e select the 20 highest scoring English documents for each source document . These 20 documents provide a restricted set of target sentence candidates. The sentence-level beam-search algorithm without the document-level filtering step searches through close to 1 trillion sentence pairs. For the data obtained by the BM25-based filtering step, we still use the same beam-search algorithm but on a much smaller candidate set of only 25.4 billion sentence pairs. The probability selection threshold 0 is determined on some development set in terms of precision and recall (based on the definitions in (Munteanu and Marcu, 2005)). The classifier obtains an F-measure classifications performance of about 85 %. The BM25 filtering step leads to a significantly more complex processing pipeline since sentences have to be indexed with respect to document boundaries and publication date. The document-level pre-filtering reduces the overall processing time by about 40 % (from 4 to 2.5 days on a 100-CPU cluster). However, the exhaustive sentence-level search improves the BLEU score by about 1 % on the Spanish-English data. 227 con Extraction via Bootstrapping and EM. In Proc, of EMNLP 2004, pages 57–63, Barcelona, Spain, July.</context>
<context position="13842" citStr="Munteanu and Marcu, 2005" startWordPosition="2422" endWordPosition="2425">sults. Extraction threshold is θ = 0.75 for both language pairs. # cands reports the size of the overall search space in terms of sentence pairs processed . Data Source # cands # pairs Bleu Baseline - 1.826 M 42.3 + Giga 999.3 B 1.357 M 45.7 + Giga (BM25) 25.4 B 0.609 M 44.8 Baseline - 2.222 M 45.3 + News Data 2006 77.8 B 56 K 47.2 5 Future Work and Discussion In this paper, we have presented a novel beamsearch algorithm to extract sentence pairs from comparable data . It can avoid any pre-filtering at the document level (Resnik and Smith, 2003; Snover et al., 2008; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Fung and Cheung, 2004). The novel algorithm is successfully evaluated on news data for two language pairs. A related approach that also avoids any documentlevel pre-filtering has been presented in (Tillmann and Xu, 2009). The efficient implementation techniques in that paper are extended for the ME classifier and beam search algorithm in the current paper, i.e. feature function values are cached along with Model-1 probabilities. The search-driven extraction algorithm presented in this paper might also be applicable to other NLP extraction task, e.g. named entity extraction. Rather then emplo</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos S. Munteanu and Daniel Marcu. 2005. Improving Machine Translation Performance by Exploiting Non-Parallel Corpora. CL, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
</authors>
<title>The Use of a One-stage Dynamic Programming Algorithm for Connected Word Recognition.</title>
<date>1984</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="9763" citStr="Ney, 1984" startWordPosition="1758" endWordPosition="1759">nce pairs. We have adjusted beam-search pruning techniques taken from regular SMT decoding (Tillmann et al., 1997; Koehn, 2004) to reduce the number of hypotheses after each extension step. Currently, only histogram pruning is employed to reduce the number of hypotheses in each stack. The resulting beam-search algorithm is similar to a monotone decoder for SMT: rather then incrementally generating a target translation, the decoder is used to select entire target sentences out of a pre-defined list. That way, our beam search algorithm is similar to algorithms in large-scale speech recognition (Ney, 1984; Vintsyuk, 1971), where an acoustic signal is matched to a pre-assigned list of words in the recognizer vocabulary. 4 Experiments The parallel sentence extraction algorithm presented in this paper is tested in detail on all of the large-scale Spanish-English Gigaword data (Graff, 2006; Graff, 2007) as well as on some smaller Portuguese-English news data . For the SpanishEnglish data, matching sentence pairs come from the same news feed. Table 1 shows the size of the comparable data, and Table 2 shows the effect of including the additional sentence pairs into the training of a phrase-based SMT</context>
</contexts>
<marker>Ney, 1984</marker>
<rawString>H. Ney. 1984. The Use of a One-stage Dynamic Programming Algorithm for Connected Word Recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32(2):263–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Raghavendra Udupa</author>
<author>Arul Menezes</author>
</authors>
<title>Generative Models of Noisy Translations with Applications to Parallel Fragment Extraction.</title>
<date>2007</date>
<booktitle>In Proc. of the MT Summit XI,</booktitle>
<pages>321--327</pages>
<location>Copenhagen,Demark,</location>
<contexts>
<context position="1323" citStr="Quirk et al., 2007" startWordPosition="207" endWordPosition="210">, which leads to a BLEU improvement of about 1 % on our SpanishEnglish data. 1 Introduction The paper presents a novel algorithm for extracting parallel sentence pairs from comparable monolingual news data. We select source-target sentence pairs (5, T) based on a ME classifier (Munteanu and Marcu, 2005). Because the set of target sentences T considered can be huge, previous work (Fung and Cheung, 2004; Resnik and Smith, 2003; Snover et al., 2008; Munteanu and Marcu, 2005) pre-selects target sentences T at the document level . We have re-implemented a particular filtering scheme based on BM25 (Quirk et al., 2007; Utiyama and Isahara, 2003; Robertson et al., 1995). In this paper, we demonstrate a different strategy . We compute the ME score incrementally at the word level and apply a beamsearch algorithm to a large number of sentences. We abandon target sentences early on during classification if they fall outside the beam. For comparison purposes, we run our novel extraction algorithm with and without the document-level prefiltering step. The results in Section 4 show that the number of extracted sentence pairs is more than doubled which also leads to an increase in BLEU by about 1 % on the Spanish-E</context>
<context position="11488" citStr="Quirk et al., 2007" startWordPosition="2033" endWordPosition="2036">million 1.36 billion Portuguese English Sentences 366.0 thousand 5.3 million Words 11.6 million 171.1 million pairs from Europarl and FBIS parallel data. We also present results for a Portuguese-English system: the baseline has been trained on Europarl and JRC data. Parallel sentence pairs are extracted from comparable news data published in 2006. For this data, no document-level information was available. To gauge the effect of the documentlevel pre-filtering step, we have re-implemented an IR technique based on BM25 (Robertson et al., 1995). This type of pre-filtering has also been used in (Quirk et al., 2007; Utiyama and Isahara, 2003). We split the Spanish data into documents. Each Spanish document is translated into a bag of English words using Model-1 lexicon probabilities trained on the baseline data. Each of these English bag-of-words is then issued as a query against all the English documents that have been published within a 7 day window of the source document. We select the 20 highest scoring English documents for each source document . These 20 documents provide a restricted set of target sentence candidates. The sentence-level beam-search algorithm without the document-level filtering s</context>
</contexts>
<marker>Quirk, Udupa, Menezes, 2007</marker>
<rawString>Chris Quirk, Raghavendra Udupa, and Arul Menezes. 2007. Generative Models of Noisy Translations with Applications to Parallel Fragment Extraction. In Proc. of the MT Summit XI, pages 321–327, Copenhagen,Demark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah Smith</author>
</authors>
<title>The Web as Parallel Corpus.</title>
<date>2003</date>
<journal>CL,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="1133" citStr="Resnik and Smith, 2003" startWordPosition="175" endWordPosition="178">on if they fall outside the beam. This way, our novel algorithm avoids any document-level prefiltering step. The algorithm increases the number of extracted parallel sentence pairs significantly, which leads to a BLEU improvement of about 1 % on our SpanishEnglish data. 1 Introduction The paper presents a novel algorithm for extracting parallel sentence pairs from comparable monolingual news data. We select source-target sentence pairs (5, T) based on a ME classifier (Munteanu and Marcu, 2005). Because the set of target sentences T considered can be huge, previous work (Fung and Cheung, 2004; Resnik and Smith, 2003; Snover et al., 2008; Munteanu and Marcu, 2005) pre-selects target sentences T at the document level . We have re-implemented a particular filtering scheme based on BM25 (Quirk et al., 2007; Utiyama and Isahara, 2003; Robertson et al., 1995). In this paper, we demonstrate a different strategy . We compute the ME score incrementally at the word level and apply a beamsearch algorithm to a large number of sentences. We abandon target sentences early on during classification if they fall outside the beam. For comparison purposes, we run our novel extraction algorithm with and without the document</context>
<context position="13768" citStr="Resnik and Smith, 2003" startWordPosition="2410" endWordPosition="2413">tion. LDC. Table 2: Spanish-English and Portuguese-English extraction results. Extraction threshold is θ = 0.75 for both language pairs. # cands reports the size of the overall search space in terms of sentence pairs processed . Data Source # cands # pairs Bleu Baseline - 1.826 M 42.3 + Giga 999.3 B 1.357 M 45.7 + Giga (BM25) 25.4 B 0.609 M 44.8 Baseline - 2.222 M 45.3 + News Data 2006 77.8 B 56 K 47.2 5 Future Work and Discussion In this paper, we have presented a novel beamsearch algorithm to extract sentence pairs from comparable data . It can avoid any pre-filtering at the document level (Resnik and Smith, 2003; Snover et al., 2008; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Fung and Cheung, 2004). The novel algorithm is successfully evaluated on news data for two language pairs. A related approach that also avoids any documentlevel pre-filtering has been presented in (Tillmann and Xu, 2009). The efficient implementation techniques in that paper are extended for the ME classifier and beam search algorithm in the current paper, i.e. feature function values are cached along with Model-1 probabilities. The search-driven extraction algorithm presented in this paper might also be applicable to </context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah Smith. 2003. The Web as Parallel Corpus. CL, 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>M M Beaulieu</author>
<author>M Gatford</author>
</authors>
<title>Okapi at TREC-4.</title>
<date>1995</date>
<booktitle>In Proc. of the 4th Text Retrieval Conference (TREC-4),</booktitle>
<pages>73--96</pages>
<contexts>
<context position="1375" citStr="Robertson et al., 1995" startWordPosition="215" endWordPosition="218">% on our SpanishEnglish data. 1 Introduction The paper presents a novel algorithm for extracting parallel sentence pairs from comparable monolingual news data. We select source-target sentence pairs (5, T) based on a ME classifier (Munteanu and Marcu, 2005). Because the set of target sentences T considered can be huge, previous work (Fung and Cheung, 2004; Resnik and Smith, 2003; Snover et al., 2008; Munteanu and Marcu, 2005) pre-selects target sentences T at the document level . We have re-implemented a particular filtering scheme based on BM25 (Quirk et al., 2007; Utiyama and Isahara, 2003; Robertson et al., 1995). In this paper, we demonstrate a different strategy . We compute the ME score incrementally at the word level and apply a beamsearch algorithm to a large number of sentences. We abandon target sentences early on during classification if they fall outside the beam. For comparison purposes, we run our novel extraction algorithm with and without the document-level prefiltering step. The results in Section 4 show that the number of extracted sentence pairs is more than doubled which also leads to an increase in BLEU by about 1 % on the Spanish-English data. The classification probability is defin</context>
<context position="11418" citStr="Robertson et al., 1995" startWordPosition="2020" endWordPosition="2023">able data. Spanish English Sentences 19.4 million 47.9 million Words 601.5 million 1.36 billion Portuguese English Sentences 366.0 thousand 5.3 million Words 11.6 million 171.1 million pairs from Europarl and FBIS parallel data. We also present results for a Portuguese-English system: the baseline has been trained on Europarl and JRC data. Parallel sentence pairs are extracted from comparable news data published in 2006. For this data, no document-level information was available. To gauge the effect of the documentlevel pre-filtering step, we have re-implemented an IR technique based on BM25 (Robertson et al., 1995). This type of pre-filtering has also been used in (Quirk et al., 2007; Utiyama and Isahara, 2003). We split the Spanish data into documents. Each Spanish document is translated into a bag of English words using Model-1 lexicon probabilities trained on the baseline data. Each of these English bag-of-words is then issued as a query against all the English documents that have been published within a 7 day window of the source document. We select the 20 highest scoring English documents for each source document . These 20 documents provide a restricted set of target sentence candidates. The sente</context>
</contexts>
<marker>Robertson, Walker, Beaulieu, Gatford, 1995</marker>
<rawString>S E Robertson, S Walker, M M Beaulieu, and M Gatford. 1995. Okapi at TREC-4. In Proc. of the 4th Text Retrieval Conference (TREC-4), pages 73–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Language and Translation Model Adaptation using Comparable Corpora.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP08,</booktitle>
<pages>856--865</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1154" citStr="Snover et al., 2008" startWordPosition="179" endWordPosition="182">the beam. This way, our novel algorithm avoids any document-level prefiltering step. The algorithm increases the number of extracted parallel sentence pairs significantly, which leads to a BLEU improvement of about 1 % on our SpanishEnglish data. 1 Introduction The paper presents a novel algorithm for extracting parallel sentence pairs from comparable monolingual news data. We select source-target sentence pairs (5, T) based on a ME classifier (Munteanu and Marcu, 2005). Because the set of target sentences T considered can be huge, previous work (Fung and Cheung, 2004; Resnik and Smith, 2003; Snover et al., 2008; Munteanu and Marcu, 2005) pre-selects target sentences T at the document level . We have re-implemented a particular filtering scheme based on BM25 (Quirk et al., 2007; Utiyama and Isahara, 2003; Robertson et al., 1995). In this paper, we demonstrate a different strategy . We compute the ME score incrementally at the word level and apply a beamsearch algorithm to a large number of sentences. We abandon target sentences early on during classification if they fall outside the beam. For comparison purposes, we run our novel extraction algorithm with and without the document-level prefiltering s</context>
<context position="13789" citStr="Snover et al., 2008" startWordPosition="2414" endWordPosition="2417">ish-English and Portuguese-English extraction results. Extraction threshold is θ = 0.75 for both language pairs. # cands reports the size of the overall search space in terms of sentence pairs processed . Data Source # cands # pairs Bleu Baseline - 1.826 M 42.3 + Giga 999.3 B 1.357 M 45.7 + Giga (BM25) 25.4 B 0.609 M 44.8 Baseline - 2.222 M 45.3 + News Data 2006 77.8 B 56 K 47.2 5 Future Work and Discussion In this paper, we have presented a novel beamsearch algorithm to extract sentence pairs from comparable data . It can avoid any pre-filtering at the document level (Resnik and Smith, 2003; Snover et al., 2008; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Fung and Cheung, 2004). The novel algorithm is successfully evaluated on news data for two language pairs. A related approach that also avoids any documentlevel pre-filtering has been presented in (Tillmann and Xu, 2009). The efficient implementation techniques in that paper are extended for the ME classifier and beam search algorithm in the current paper, i.e. feature function values are cached along with Model-1 probabilities. The search-driven extraction algorithm presented in this paper might also be applicable to other NLP extraction </context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2008. Language and Translation Model Adaptation using Comparable Corpora. In Proc. of EMNLP08, pages 856–865, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Jian-Ming Xu</author>
</authors>
<title>A Simple Sentence-Level Extraction Algorithm for Comparable Data.</title>
<date>2009</date>
<booktitle>In Companion Vol. of NAACL HLT 09,</booktitle>
<pages>93--96</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="3420" citStr="Tillmann and Xu, 2009" startWordPosition="584" endWordPosition="587">e functions fi . Their computation is based on the so-called IBM Model-1 (Brown et al., 1993). The Model-1 is trained on some parallel data available for a language pair, i.e. the data used to train the baseline systems in Section 4. p(s|T) is the Model-1 probability assigned to a source word s given the target sentence T , p(t|5) is defined accordingly. p(s|t) and p(t|s) are word translation probabilities obtained by two parallel Model-1 training steps on the same data, but swapping the role of source and target language. To compute these values efficiently, the implementation techniques in (Tillmann and Xu, 2009) are used. Coverage and fertility features are defined based on the Model-1 Viterbi alignment: a source 225 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 225–228, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP word s is said to be covered if there is a target word t ∈ T such that its probability is above a threshold ǫ: p(s|t) &gt; ǫ . We define the fertility of a source word s as the number of target words t ∈ T for which p(s|t) &gt; ǫ. Target word coverage and fertility are defined accordingly. A large number of ‘uncovered‘ source and target positions as well as a large </context>
<context position="14064" citStr="Tillmann and Xu, 2009" startWordPosition="2458" endWordPosition="2461">.3 B 1.357 M 45.7 + Giga (BM25) 25.4 B 0.609 M 44.8 Baseline - 2.222 M 45.3 + News Data 2006 77.8 B 56 K 47.2 5 Future Work and Discussion In this paper, we have presented a novel beamsearch algorithm to extract sentence pairs from comparable data . It can avoid any pre-filtering at the document level (Resnik and Smith, 2003; Snover et al., 2008; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Fung and Cheung, 2004). The novel algorithm is successfully evaluated on news data for two language pairs. A related approach that also avoids any documentlevel pre-filtering has been presented in (Tillmann and Xu, 2009). The efficient implementation techniques in that paper are extended for the ME classifier and beam search algorithm in the current paper, i.e. feature function values are cached along with Model-1 probabilities. The search-driven extraction algorithm presented in this paper might also be applicable to other NLP extraction task, e.g. named entity extraction. Rather then employing a cascade of filtering steps, a one-stage search with a specially adopted feature set and search space organization might be carried out . Such a search-driven approach makes less assumptions about the data and may in</context>
</contexts>
<marker>Tillmann, Xu, 2009</marker>
<rawString>Christoph Tillmann and Jian-Ming Xu. 2009. A Simple Sentence-Level Extraction Algorithm for Comparable Data. In Companion Vol. of NAACL HLT 09, pages 93–96, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A Block Bigram Prediction Model for Statistical Machine Translation.</title>
<date>2007</date>
<journal>ACM-TSLP,</journal>
<volume>4</volume>
<issue>6</issue>
<contexts>
<context position="5460" citStr="Tillmann and Zhang, 2007" startWordPosition="957" endWordPosition="960">the feature vector is needed for the search in Section 3: the contribution of the ’negative’ features for c = 0 is only computed when Eq. 1 is evaluated for the highest scoring final hypothesis in the beam. To train the classifier, we have manually annotated a collection of 524 sentence pairs . A sentence pair is considered parallel if at least 75 % of source and target words have a corresponding translation in the other sentence, otherwise it is labeled as non-parallel. A weight vector w ∈ R2∗N is trained with respect to classification accuracy using the on-line maxent training algorithm in (Tillmann and Zhang, 2007). 3 Beam Search Algorithm We process the comparable data at the sentence level: sentences are indexed based on their publication date. For each source sentence S, a matching score is computed over all the target sentences Tm ∈ Θ that have a publication date which differs less than 7 days from the publication date of the source sentence 1. We are aiming at finding the Tˆ with the highest probability p(c = 1|S, Tˆ), but we cannot compute that probability for all sentence 1In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used: the length ratio max(J, I)/min(J, I) of source</context>
</contexts>
<marker>Tillmann, Zhang, 2007</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2007. A Block Bigram Prediction Model for Statistical Machine Translation. ACM-TSLP, 4(6):1–31, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Alex Zubiaga</author>
</authors>
<title>A DP-based Search Using Monotone Alignments in Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proc. of ACL 97,</booktitle>
<pages>289--296</pages>
<location>Madrid,Spain,</location>
<contexts>
<context position="9267" citStr="Tillmann et al., 1997" startWordPosition="1675" endWordPosition="1678">ll. The beam-search algorithm is carried out until all source positions j have been processed. We extract the highest scoring partial hypothesis from the final stack j = J . For that hypothesis, we compute a global feature vector f(1, S, T) by adding all the local f(1, j, i)’s component-wise. The ‘negative‘ feature vector f(0, S, T) is computed from f(1, S, T) by copying its feature values. We then use Eq. 1 to compute the probability p(1|S, T) and apply a threshold of 0 = 0.75 to extract parallel sentence pairs. We have adjusted beam-search pruning techniques taken from regular SMT decoding (Tillmann et al., 1997; Koehn, 2004) to reduce the number of hypotheses after each extension step. Currently, only histogram pruning is employed to reduce the number of hypotheses in each stack. The resulting beam-search algorithm is similar to a monotone decoder for SMT: rather then incrementally generating a target translation, the decoder is used to select entire target sentences out of a pre-defined list. That way, our beam search algorithm is similar to algorithms in large-scale speech recognition (Ney, 1984; Vintsyuk, 1971), where an acoustic signal is matched to a pre-assigned list of words in the recognizer</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, 1997</marker>
<rawString>Christoph Tillmann, Stefan Vogel, Hermann Ney, and Alex Zubiaga. 1997. A DP-based Search Using Monotone Alignments in Statistical Translation. In Proc. of ACL 97, pages 289–296, Madrid,Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Reliable Measures for Aligning Japanese-English News Articles and Sentences.</title>
<date>2003</date>
<booktitle>In Proc. ofACL03,</booktitle>
<pages>72--79</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1350" citStr="Utiyama and Isahara, 2003" startWordPosition="211" endWordPosition="214">LEU improvement of about 1 % on our SpanishEnglish data. 1 Introduction The paper presents a novel algorithm for extracting parallel sentence pairs from comparable monolingual news data. We select source-target sentence pairs (5, T) based on a ME classifier (Munteanu and Marcu, 2005). Because the set of target sentences T considered can be huge, previous work (Fung and Cheung, 2004; Resnik and Smith, 2003; Snover et al., 2008; Munteanu and Marcu, 2005) pre-selects target sentences T at the document level . We have re-implemented a particular filtering scheme based on BM25 (Quirk et al., 2007; Utiyama and Isahara, 2003; Robertson et al., 1995). In this paper, we demonstrate a different strategy . We compute the ME score incrementally at the word level and apply a beamsearch algorithm to a large number of sentences. We abandon target sentences early on during classification if they fall outside the beam. For comparison purposes, we run our novel extraction algorithm with and without the document-level prefiltering step. The results in Section 4 show that the number of extracted sentence pairs is more than doubled which also leads to an increase in BLEU by about 1 % on the Spanish-English data. The classifica</context>
<context position="11516" citStr="Utiyama and Isahara, 2003" startWordPosition="2037" endWordPosition="2040"> Portuguese English Sentences 366.0 thousand 5.3 million Words 11.6 million 171.1 million pairs from Europarl and FBIS parallel data. We also present results for a Portuguese-English system: the baseline has been trained on Europarl and JRC data. Parallel sentence pairs are extracted from comparable news data published in 2006. For this data, no document-level information was available. To gauge the effect of the documentlevel pre-filtering step, we have re-implemented an IR technique based on BM25 (Robertson et al., 1995). This type of pre-filtering has also been used in (Quirk et al., 2007; Utiyama and Isahara, 2003). We split the Spanish data into documents. Each Spanish document is translated into a bag of English words using Model-1 lexicon probabilities trained on the baseline data. Each of these English bag-of-words is then issued as a query against all the English documents that have been published within a 7 day window of the source document. We select the 20 highest scoring English documents for each source document . These 20 documents provide a restricted set of target sentence candidates. The sentence-level beam-search algorithm without the document-level filtering step searches through close t</context>
<context position="13816" citStr="Utiyama and Isahara, 2003" startWordPosition="2418" endWordPosition="2421">guese-English extraction results. Extraction threshold is θ = 0.75 for both language pairs. # cands reports the size of the overall search space in terms of sentence pairs processed . Data Source # cands # pairs Bleu Baseline - 1.826 M 42.3 + Giga 999.3 B 1.357 M 45.7 + Giga (BM25) 25.4 B 0.609 M 44.8 Baseline - 2.222 M 45.3 + News Data 2006 77.8 B 56 K 47.2 5 Future Work and Discussion In this paper, we have presented a novel beamsearch algorithm to extract sentence pairs from comparable data . It can avoid any pre-filtering at the document level (Resnik and Smith, 2003; Snover et al., 2008; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Fung and Cheung, 2004). The novel algorithm is successfully evaluated on news data for two language pairs. A related approach that also avoids any documentlevel pre-filtering has been presented in (Tillmann and Xu, 2009). The efficient implementation techniques in that paper are extended for the ME classifier and beam search algorithm in the current paper, i.e. feature function values are cached along with Model-1 probabilities. The search-driven extraction algorithm presented in this paper might also be applicable to other NLP extraction task, e.g. named entity ext</context>
</contexts>
<marker>Utiyama, Isahara, 2003</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2003. Reliable Measures for Aligning Japanese-English News Articles and Sentences. In Proc. ofACL03, pages 72–79, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Vintsyuk</author>
</authors>
<title>Element-Wise Recognition of Continuous Speech Consisting of Words From a Specified Vocabulary. Cybernetics (Kibernetica),</title>
<date>1971</date>
<location>(2):133–143, March-April.</location>
<contexts>
<context position="9780" citStr="Vintsyuk, 1971" startWordPosition="1760" endWordPosition="1761">We have adjusted beam-search pruning techniques taken from regular SMT decoding (Tillmann et al., 1997; Koehn, 2004) to reduce the number of hypotheses after each extension step. Currently, only histogram pruning is employed to reduce the number of hypotheses in each stack. The resulting beam-search algorithm is similar to a monotone decoder for SMT: rather then incrementally generating a target translation, the decoder is used to select entire target sentences out of a pre-defined list. That way, our beam search algorithm is similar to algorithms in large-scale speech recognition (Ney, 1984; Vintsyuk, 1971), where an acoustic signal is matched to a pre-assigned list of words in the recognizer vocabulary. 4 Experiments The parallel sentence extraction algorithm presented in this paper is tested in detail on all of the large-scale Spanish-English Gigaword data (Graff, 2006; Graff, 2007) as well as on some smaller Portuguese-English news data . For the SpanishEnglish data, matching sentence pairs come from the same news feed. Table 1 shows the size of the comparable data, and Table 2 shows the effect of including the additional sentence pairs into the training of a phrase-based SMT system. Here, bo</context>
</contexts>
<marker>Vintsyuk, 1971</marker>
<rawString>T.K. Vintsyuk. 1971. Element-Wise Recognition of Continuous Speech Consisting of Words From a Specified Vocabulary. Cybernetics (Kibernetica), (2):133–143, March-April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>