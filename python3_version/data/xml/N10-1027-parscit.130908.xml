<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001039">
<title confidence="0.978133">
Language Identification: The Long and the Short of the Matter
</title>
<author confidence="0.996994">
Timothy Baldwin and Marco Lui
</author>
<affiliation confidence="0.9991075">
Dept of Computer Science and Software Engineering
University of Melbourne, VIC 3010 Australia
</affiliation>
<email confidence="0.995082">
tb@ldwin.net,saffsd@gmail.com
</email>
<sectionHeader confidence="0.993822" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999711461538461">
Language identification is the task of identify-
ing the language a given document is written
in. This paper describes a detailed examina-
tion of what models perform best under dif-
ferent conditions, based on experiments across
three separate datasets and a range of tokeni-
sation strategies. We demonstrate that the task
becomes increasingly difficult as we increase
the number of languages, reduce the amount
of training data and reduce the length of docu-
ments. We also show that it is possible to per-
form language identification without having to
perform explicit character encoding detection.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961222222222">
With the growth of the worldwide web, ever-
increasing numbers of documents have become
available, in more and more languages. This growth
has been a double-edged sword, however, in that
content in a given language has become more preva-
lent but increasingly hard to find, due to the web’s
sheer size and diversity of content. While the ma-
jority of (X)HTML documents declare their charac-
ter encoding, only a tiny minority specify what lan-
guage they are written in, despite support for lan-
guage declaration existing in the various (X)HTML
standards.1 Additionally, a single encoding can gen-
erally be used to render a large number of languages
such that the document encoding at best filters out
a subset of languages which are incompatible with
the given encoding, rather than disambiguates the
source language. Given this, the need for automatic
means to determine the source language of web doc-
</bodyText>
<footnote confidence="0.9416755">
1http://dev.opera.com/articles/view/
mama-head-structure/
</footnote>
<bodyText confidence="0.999853216216216">
uments is crucial for web aggregators of various
types.
There is widespread misconception of language
identification being a “solved task”, generally as
a result of isolated experiments over homogeneous
datasets with small numbers of languages (Hughes
et al., 2006; Xia et al., 2009). Part of the motivation
for this paper is to draw attention to the fact that, as
a field, we are still a long way off perfect language
identification of web documents, as evaluated under
realistic conditions.
In this paper we describe experiments on lan-
guage identification of web documents, focusing on
the broad question of what combination of tokenisa-
tion strategy and classification model achieves the
best overall performance. We additionally evalu-
ate the impact of the volume of training data and
the test document length on the accuracy of lan-
guage identification, and investigate the interaction
between character encoding detection and language
identification.
One assumption we make in this research, follow-
ing standard assumptions made in the field, is that all
documents are monolingual. This is clearly an un-
realistic assumption when dealing with general web
documents (Hughes et al., 2006), and we plan to re-
turn to investigate language identification over mul-
tilingual documents in future work.
Our contributions in this paper are: the demon-
stration that language identification is: (a) trivial
over datasets with smaller numbers of languages
and approximately even amounts of training data per
language, but (b) considerably harder over datasets
with larger numbers of languages with more skew
in the amount of training data per language; byte-
based tokenisation without character encoding de-
tection is superior to codepoint-based tokenisation
</bodyText>
<page confidence="0.982076">
229
</page>
<note confidence="0.752015">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 229–237,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999687428571429">
with character encoding detection; and simple co-
sine similarity-based nearest neighbour classifica-
tion is equal to or better than models including sup-
port vector machines and naive Bayes over the lan-
guage identification task. We also develop datasets
to facilitate standardised evaluation of language
identification.
</bodyText>
<sectionHeader confidence="0.955877" genericHeader="introduction">
2 Background Research
</sectionHeader>
<bodyText confidence="0.999747594594595">
Language identification was arguably established as
a task by Gold (1967), who construed it as a closed
class problem: given data in each of a predefined set
of possible languages, human subjects were asked
to classify the language of a given test document. It
wasn’t until the 1990s, however, that the task was
popularised as a text categorisation task.
The text categorisation approach to language
identification applies a standard supervised classi-
fication framework to the task. Perhaps the best-
known such model is that of Cavnar and Tren-
kle (1994), as popularised in the textcat tool.2
The method uses a per-language character frequency
model, and classifies documents via their relative
“out of place” distance from each language (see
Section 5.1). Variants on this basic method in-
clude Bayesian models for character sequence pre-
diction (Dunning, 1994), dot products of word fre-
quency vectors (Darnashek, 1995) and information-
theoretic measures of document similarity (Aslam
and Frost, 2003; Martins and Silva, 2005). More
recently, support vector machines (SVMs) and ker-
nel methods have been applied to the task of lan-
guage identification task with success (Teytaud and
Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al.,
2005), and Markov logic has been used for joint in-
ferencing in contexts where there are multiple evi-
dence sources (Xia et al., 2009).
Language identification has also been carried out
via linguistically motivated models. Johnson (1993)
used a list of stop words from different languages to
identify the language of a given document, choos-
ing the language with the highest stop word over-
lap with the document. Grefenstette (1995) used
word and part of speech (POS) correlation to de-
termine if two text samples were from the same
or different languages. Giguet (1995) developed a
</bodyText>
<footnote confidence="0.859388">
2http://www.let.rug.nl/vannoord/TextCat/
</footnote>
<bodyText confidence="0.999943547619048">
cross-language tokenisation model and used it to
identify the language of a given document based
on its tokenisation similarity with training data.
Dueire Lins and Gonc¸alves (2004) considered the
use of syntactically-derived closed grammatical-
class models, matching syntactic structure rather
than words or character sequences.
The observant reader will have noticed that some
of the above approaches make use of notions such
as “word”, typically based on the naive assumption
that the language uses white space to delimit words.
These approaches are appropriate in contexts where
there is a guarantee of a document being in one of
a select set of languages where words are space-
delimited, or where manual segmentation has been
performed (e.g. interlinear glossed text). However,
we are interested in language identification of web
documents, which can be in any language, includ-
ing languages that do not overtly mark word bound-
aries, such as Japanese, Chinese and Thai; while
relatively few languages fall into this categories,
they are among the most populous web languages
and therefore an important consideration. There-
fore, approaches that assume a language is space-
delimited are clearly not suitable for our purposes.
Equally, approaches which make assumptions about
the availability of particular resources for each lan-
guage to be identified (e.g. POS taggers, or the ex-
istence of precompiled stop word lists) cannot be
used.
Language identification has been applied in a
number of contexts, the most immediate applica-
tion being in multilingual text retrieval, where re-
trieval results are generally superior if the language
of the query is known, and the search is restricted
to only those documents predicted to be in that lan-
guage (McNamee and Mayfield, 2004). It can also
be used to “word spot” foreign language terms in
multilingual documents, e.g. to improve parsing per-
formance (Alex et al., 2007), or for linguistic corpus
creation purposes (Baldwin et al., 2006; Xia et al.,
2009; Xia and Lewis, 2009).
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="method">
3 Datasets
</sectionHeader>
<bodyText confidence="0.998788333333333">
In the experiments reported in this paper, we em-
ploy three novel datasets, with differing properties
relevant to language identification research:
</bodyText>
<page confidence="0.994417">
230
</page>
<table confidence="0.998025">
Corpus Documents Languages Encodings Document Length (bytes)
EUROGOV 1500 10 1 17460.5±39353.4
TCL 3174 60 12 2623.2±3751.9
WIKIPEDIA 4963 67 1 1480.8±4063.9
</table>
<tableCaption confidence="0.999734">
Table 1: Summary of the three language identification datasets
</tableCaption>
<figureCaption confidence="0.989958666666667">
Figure 1: Distribution of languages in the three datasets
(vector of languages vs. the proportion of documents in
that language)
</figureCaption>
<bodyText confidence="0.999968473684211">
EUROGOV: longer documents, all in a single en-
coding, spread evenly across a relatively small num-
ber (10) of Western European languages; this dataset
is comparable to the datasets conventionally used in
language identification research. As the name would
suggest, the documents were sourced from the Euro-
GOV document collection, as used in the 2005 Web-
CLEF task.
TCL: a larger number of languages (60) across a
wider range of language families, with shorter docu-
ments and a range of character encodings (12). The
collection was manually sourced by the Thai Com-
putational Linguistics Laboratory (TCL) in 2005
from online news sources.
WIKIPEDIA: a slightly larger number of lan-
guages again (67), a single encoding, and shorter
documents; the distribution of languages is intended
to approximate that of the actual web. This col-
lection was automatically constructed by taking the
dumps of all versions of Wikipedia with 1000 or
more documents in non-constructed languages, and
randomly selecting documents from them in a bias-
preserving manner (i.e. preserving the document
distribution in the full collection); this is intended to
represent the document language bias observed on
the web. All three corpora are available on request.
We outline the characteristics of the three datasets
in Table 1. We further detail the language distri-
bution in Figure 1, using a constant vector of lan-
guages for all three datasets, based on the order of
languages in the WIKIPEDIA dataset (in descending
order of documents per language). Of note are the
contrasting language distributions between the three
datasets, in terms of both the languages represented
and the relative skew of documents per language. In
the following sections, we provide details of the cor-
pus compilation and document sampling method for
each dataset.
</bodyText>
<sectionHeader confidence="0.983996" genericHeader="method">
4 Document Representation
</sectionHeader>
<bodyText confidence="0.99999496">
As we are interested in performing language iden-
tification over arbitrary web documents, we re-
quire a language-neutral document representation
which does not make artificial assumptions about the
source language of the document. Separately, there
is the question of whether it is necessary to deter-
mine the character encoding of the document in or-
der to extract out character sequences, or whether
the raw byte stream is sufficient. To explore this
question, we experiment with two document repre-
sentations: (1) byte n-grams, and (2) codepoint n-
grams. In both cases, a document is represented as a
feature vector of token counts.
Byte n-grams can be extracted directly without
explicit encoding detection. Codepoint n-grams, on
the other hand, require that we know the character
encoding of the document in order to perform to-
kenisation. Additionally, they should be based on a
common encoding to prevent: (a) over-fragmenting
the feature space (e.g. ending up with discrete fea-
ture spaces for euc-jp, s-jis and utf-8 in
the case of Japanese); and (b) spurious matches be-
tween encodings (e.g. Japanese hiragana and Ko-
rean hangul mapping onto the same codepoint in
euc-jp and euc-kr, respectively). We use uni-
</bodyText>
<page confidence="0.984067">
231
</page>
<bodyText confidence="0.999947416666667">
code as the common encoding for all documents.
In practice, character encoding detection is an is-
sue only for TCL, as the other two datasets are in
a single encoding. Where a character encoding was
provided for a document in TCL and it was possi-
ble to transcode the document to unicode based on
that encoding, we used the encoding information. In
cases where a unique encoding was not provided,
we used an encoding detection library based on the
Mozilla browser.3 Having disambiguated the encod-
ing for each document, we transcoded it into uni-
code.
</bodyText>
<sectionHeader confidence="0.994896" genericHeader="method">
5 Models
</sectionHeader>
<bodyText confidence="0.999886">
In our experiments we use a number of different
language identification models, as outlined below.
We first describe the nearest-neighbour and nearest-
prototype models, and a selection of distance and
similarity metrics combined with each. We then
present three standalone text categorisation models.
</bodyText>
<subsectionHeader confidence="0.890092">
5.1 Nearest-Neighbour and Nearest-Prototype
Models
</subsectionHeader>
<bodyText confidence="0.999461272727273">
The 1-nearest-neighbour (1NN) model is a common
classification technique, whereby a test document
D is classified based on the language of the clos-
est training document Di (with language l(Di)), as
determined by a given distance or similarity metric.
In nearest-neighbour models, each training doc-
ument is represented as a single instance, mean-
ing that the computational cost of classifying a test
document is proportional to the number of training
documents. A related model which aims to reduce
this cost is nearest-prototype (AM), where each lan-
guage is represented as a single instance, by merging
all of the training instances for that language into a
single centroid via the arithmetic mean.
For both nearest-neighbour and nearest-prototype
methods, we experimented with three similarity and
distance measures in this research:
Cosine similarity (COS): the cosine of the angle
between two feature vectors, as measured by the dot
product of the two vectors, normalised to unit length.
Skew divergence (SKEW): a variant of Kullback-
Leibler divergence, whereby the second distribution
</bodyText>
<footnote confidence="0.836913">
3http://chardet.feedparser.org/
</footnote>
<listItem confidence="0.9141815">
(y) is smoothed by linear interpolation with the first
(x) using a smoothing factor a (Lee, 2001):
</listItem>
<equation confidence="0.93868025">
sα(x, y) = D(x  ||ay + (1 − a)x)
where:
D(x  ||y) = X xi(log2 xi − log2 yi)
i
</equation>
<bodyText confidence="0.8887689">
In all our experiments, we set a to 0.99.
Out-of-place (OOP): a ranklist-based distance
metric, where the distance between two documents
is calculated as (Cavnar and Trenkle, 1994):
oop(Dx, Dy) = X abs(RDx(t) − RDy(t))
tEDxVDy
RD(t) is the rank of term t in document D, based
on the descending order of frequency in document
D; terms not occurring in document D are conven-
tionally given the rank 1 + maxi RD(ti).
</bodyText>
<subsectionHeader confidence="0.981445">
5.2 Naive Bayes (NB)
</subsectionHeader>
<bodyText confidence="0.99957275">
Naive Bayes is a popular text classification model,
due to it being lightweight, robust and easy to up-
date. The language of test document D is predicted
by:
</bodyText>
<equation confidence="0.9554365">
ˆl(D) = arg max
liEL
</equation>
<bodyText confidence="0.957871333333333">
where L is the set of languages in the training data,
ND,tj is the frequency of the jth term in D, V is the
set of all terms, and:
</bodyText>
<equation confidence="0.9874482">
P(t|li) =
1 + P|D |1 Nk,tP(li|Dk)
|V  |+ P|V  |P|D|
k=1 Nk,tjP(li|Dk)
j=1
</equation>
<bodyText confidence="0.999831666666667">
In this research, we use the rainbow imple-
mentation of multinominal naive Bayes (McCallum,
1996).
</bodyText>
<subsectionHeader confidence="0.998249">
5.3 Support Vector Machines (SVM)
</subsectionHeader>
<bodyText confidence="0.999846833333333">
Support vector machines (SVMs) are one of the
most popular methods for text classification, largely
because they can automatically weight large num-
bers of features, capturing feature interactions in the
process (Joachims, 1998; Manning et al., 2008). The
basic principle underlying SVMs is to maximize the
</bodyText>
<equation confidence="0.9884145">
P(tj|li)ND,tj
ND,tj!
P(li) Y |V |
j=1
</equation>
<page confidence="0.946185">
232
</page>
<bodyText confidence="0.997483222222222">
margin between training instances and the calculated
decision boundary based on structural risk minimi-
sation (Vapnik, 1995).
In this work, we have made use of bsvm,4 an
implementation of SVMs with multiclass classifica-
tion support (Hsu et al., 2008). We only report re-
sults for multi-class bound-constrained support vec-
tor machines with linear kernels, as they were found
to perform best over our data.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="method">
6 Experimental Methodology
</sectionHeader>
<bodyText confidence="0.957139444444445">
We carry out experiments over the cross-product of
the following options, as described above:
model (x7): nearest-neighbour (COS1NN,
SKEW1NN, OOP1NN), nearest-prototype
(COSAM, SKEWAM),5 NB, SVM
tokenisation (x2): byte, codepoint
n-gram (x3): 1-gram, 2-gram, 3-gram
for a total of 42 distinct classifiers. Each classi-
fier is run across the 3 datasets (EUROGOV, TCL
and WIKIPEDIA) based on 10-fold stratified cross-
validation.
We evaluate the models using micro-averaged
precision (Pµ), recall (Rµ) and F-score (Fµ), as well
as macro-averaged precision (PM), recall (RM) and
F-score (FM). The micro-averaged scores indicate
the average performance per document; as we al-
ways make a unique prediction per document, the
micro-averaged precision, recall and F-score are al-
ways identical (as is the classification accuracy).
The macro-averaged scores, on the other hand, indi-
cate the average performance per language. In each
case, we average the precision, recall and F-score
across the 10 folds of cross validation.6
As a baseline, we use a majority class, or ZeroR,
classifier (ZEROR), which assigns the language with
highest prior in the training data to each of the test
documents.
</bodyText>
<footnote confidence="0.998727333333333">
4http://www.csie.ntu.edu.tw/˜cjlin/bsvm/
5We do not include the results for nearest-prototype classi-
fiers with the OOP distance metric as the results were consid-
erably lower than the other methods.
6Note that this means that the averaged Far is not necessar-
ily the harmonic mean of the averaged Pm and Rm.
</footnote>
<table confidence="0.972212875">
Model Token PM RM FM Pµ/Rµ/Fµ
ZEROR — .020 .084 .032 .100
COS1NN byte .975 .978 .976 .975
COS1NN codepoint .968 .973 .970 .971
COSAM byte .922 .938 .926 .937
COSAM codepoint .908 .930 .913 .931
SKEW1NN byte .979 .979 .979 .977
SKEW1NN codepoint .978 .978 .978 .976
SKEWAM byte .974 .972 .972 .969
SKEWAM codepoint .974 .972 .973 .970
OOP1NN byte .953 .952 .953 .949
OOP1NN codepoint .961 .960 .960 .957
NB byte .975 .973 .974 .971
NB codepoint .975 .973 .974 .971
SVM byte .989 .985 .987 .987
SVM codepoint .988 .985 .986 .987
</table>
<tableCaption confidence="0.977167">
Table 2: Results for byte vs. codepoint (bigram) tokeni-
sation over EUROGOV
</tableCaption>
<sectionHeader confidence="0.999566" genericHeader="method">
7 Results
</sectionHeader>
<bodyText confidence="0.999911375">
In our experiments, we first compare the different
models for fixed n-gram order, then come back to
vary the n-gram order. Subsequently, we examine
the relative performance of the different models on
test documents of differing lengths, and finally look
into the impact of the amount of training data for
a given language on the performance for that lan-
guage.
</bodyText>
<sectionHeader confidence="0.8525115" genericHeader="method">
7.1 Results for the Different Models and
Tokenisation Strategies
</sectionHeader>
<bodyText confidence="0.999973352941177">
First, we present the results for each of the classifiers
in Tables 2–4, based on byte or codepoint tokenisa-
tion and bigrams. In each case, we present the best
result in each column in bold.
The relative performance over EUROGOV and
TCL is roughly comparable for all methods barring
SKEW1NN, with near-perfect scores over all 6 eval-
uation metrics. SKEW1NN is near-perfect over EU-
ROGOV and TCL, but drops to baseline levels over
WIKIPEDIA; we return to discuss this effect in Sec-
tion 7.2.
In the case of EUROGOV, the near-perfect re-
sults are in line with our expectations for the dataset,
based on its characteristics and results reported for
comparable datasets. The results for WIKIPEDIA,
however, fall off considerably, with the best model
achieving an FM of .671 and Fµ of .869, due to
</bodyText>
<page confidence="0.996129">
233
</page>
<table confidence="0.9519654375">
Model Token PM RM FM Pµ/Rµ/Fµ
ZEROR — .003 .017 .005 .173
COS1NN byte .981 .975 .975 .982
COS1NN codepoint .931 .930 .925 .961
COSAM byte .967 .975 .965 .965
COSAM codepoint .979 .977 .974 .964
SKEW1NN byte .984 .974 .976 .987
SKEW1NN codepoint .910 .210 .320 .337
SKEWAM byte .962 .959 .950 .972
SKEWAM codepoint .968 .961 .957 .967
OOP1NN byte .964 .945 .951 .974
OOP1NN codepoint .901 .892 .893 .933
NB byte .905 .905 .896 .969
NB codepoint .722 .711 .696 .845
SVM byte .981 .973 .977 .984
SVM codepoint .979 .970 .974 .980
</table>
<tableCaption confidence="0.6630225">
Table 3: Results for byte vs. codepoint (bigram) tokeni-
sation over TCL
</tableCaption>
<table confidence="0.978238">
Model Token PM RM FM Pµ/Rµ/Fµ
ZEROR — .004 .013 .007 .328
COS1NN byte .740 .646 .671 .869
COS1NN codepoint .685 .604 .625 .835
COSAM byte .587 .634 .573 .776
COSAM codepoint .486 .556 .483 .725
SKEW1NN byte .005 .013 .008 .304
SKEW1NN codepoint .006 .013 .007 .241
SKEWAM byte .605 .617 .588 .844
SKEWAM codepoint .552 .575 .532 .807
OOP1NN byte .619 .518 .548 .831
OOP1NN codepoint .598 .486 .520 .807
NB byte .496 .454 .442 .851
NB codepoint .426 .349 .360 .798
SVM byte .667 .545 .577 .845
SVM codepoint .634 .494 .536 .818
</table>
<tableCaption confidence="0.97698">
Table 4: Results for byte vs. codepoint (bigram) tokeni-
sation over WIKIPEDIA
</tableCaption>
<bodyText confidence="0.999786945945946">
the larger number of languages, smaller documents,
and skew in the amounts of training data per lan-
guage. All models are roughly balanced in the rel-
ative scores they attain for PM, RM and FM (i.e.
there are no models that have notably higher PM rel-
ative to RM, for example).
The nearest-neighbour models outperform the
corresponding nearest-prototype models to varying
degrees, with the one exception of SKEW1NN over
WIKIPEDIA. The nearest-prototype classifiers were
certainly faster than the nearest-neighbour classi-
fiers, by roughly an order of 10, but this is more
than outweighed by the drop in classification per-
formance. With the exception of SKEW1NN over
WIKIPEDIA, all methods were well above the base-
lines for all three datasets.
The two methods which perform consistently well
at this point are COS1NN and SVM, with COS1NN
holding up particularly well under micro-averaged
F-score while NB drops away over WIKIPEDIA, the
most skewed dataset; this is due to the biasing effect
of the prior in NB.
Looking to the impact of byte- vs. codepoint-
tokenisation on classifier performance over the three
datasets, we find that overall, bytes outperform
codepoints. This is most notable for TCL and
WIKIPEDIA, and the SKEW1NN and NB models.
Given this result, we present only results for byte-
based tokenisation in the remainder of this paper.
The results for byte tokenisation of TCL are par-
ticularly noteworthy. The transcoding into unicode
and use of codepoints, if anything, hurts perfor-
mance, suggesting that implicit character encoding
detection based on byte tokenisation is the best ap-
proach: it is both more accurate and simplifies the
system, in removing the need to perform encoding
detection prior to language identification.
</bodyText>
<sectionHeader confidence="0.67539" genericHeader="evaluation">
7.2 Results for Differing n-gram Sizes
</sectionHeader>
<bodyText confidence="0.999882533333333">
We present results with byte unigrams, bigrams and
trigrams in Table 5 for WIKIPEDIA.7 We omit re-
sults for the other two datasets, as the overall trend is
the same as for WIKIPEDIA, with lessened relative
differences between n-gram orders due to the rela-
tive simplicity of the respective classification tasks.
SKEW1NN is markedly different to the other meth-
ods in achieving the best performance with uni-
grams, moving from the worst-performing method
by far to one of the best-performing methods. This
is the result of the interaction between data sparse-
ness and heavy-handed smoothing with the α con-
stant. Rather than using a constant α value for all
n-gram orders, it may be better to parameterise it
using an exponential scale such as α = 1− /-2n (with
</bodyText>
<footnote confidence="0.9267696">
7The results for OOP1NN over byte trigrams are missing
due to the computational cost associated with the method, and
our experiment hence not having run to completion at the time
of writing. Extrapolating from the results for the other two
datasets, we predict similar results to bigrams.
</footnote>
<page confidence="0.989488">
234
</page>
<table confidence="0.981485727272727">
Model n-gram PM RM FM Pµ/Rµ/Fµ
ZEROR — .004 .013 .007 .328
COS1NN 1 .644 .579 .599 .816
COS1NN 2 .740 .646 .671 .869
COS1NN 3 .744 .656 .680 .862
COSAM 1 .526 .543 .487 .654
COSAM 2 .587 .634 .573 .776
COSAM 3 .553 .632 .545 .761
SKEW1NN 1 .691 .598 .625 .848
SKEW1NN 2 .005 .013 .008 .304
SKEW1NN 3 .005 .013 .004 .100
SKEWAM 1 .552 .569 .532 .740
SKEWAM 2 .605 .617 .588 .844
SKEWAM 3 .551 .631 .554 .825
OOP1NN 1 .519 .446 .468 .747
OOP1NN 2 .619 .518 .548 .831
NB 1 .576 .578 .555 .778
NB 2 .496 .454 .442 .851
NB 3 .493 .435 .432 .863
SVM 1 .585 .505 .523 .812
SVM 2 .667 .545 .577 .845
SVM 3 .717 .547 .594 .840
</table>
<tableCaption confidence="0.8910635">
Table 5: Results for different n-gram orders over
WIKIPEDIA
</tableCaption>
<bodyText confidence="0.999754291666667">
Q = 0.01, e.g.), based on the n-gram order. We
leave this for future research.
For most methods, bigrams and trigrams are bet-
ter than unigrams, with the one notable exception
of SKEW1NN. In general, there is little separating
bigrams and trigrams, although the best result for is
achieved slightly more often for bigrams than for tri-
grams.
For direct comparability with Cavnar and Tren-
kle (1994), we additionally carried out a preliminary
experiment with hybrid byte n-grams (all of 1- to 5-
grams), combined with simple frequency-based fea-
ture selection of the top-1000 features for each n-
gram order. The significance of this setting is that it
is the strategy adopted by textcat, based on the
original paper of Cavnar and Trenkle (1994) (with
the one exception that we use 1000 features rather
than 300, as all methods other than OOP1NN bene-
fitted from more features). The results are shown in
Table 6.
Compared to the results in Table 5, SKEW1NN and
SKEWAM both increase markedly to achieve the best
overall results. OOP1NN, on the other hand, rises
slightly, while the remaining three methods actually
</bodyText>
<table confidence="0.999582125">
Model PM RM FM Pµ/Rµ/Fµ
ZEROR .004 .013 .007 .328
COS1NN .735 .664 .682 .865
COSAM .592 .626 .580 .766
SKEW1NN .789 .708 .729 .902
SKEWAM .681 .718 .680 .870
OOP1NN .697 .595 .626 .864
SVM .669 .500 .544 .832
</table>
<tableCaption confidence="0.986922">
Table 6: Results for mixed n-grams (1–5) and feature se-
lection over WIKIPEDIA (a l´a Cavnar and Trenkle (1994))
</tableCaption>
<bodyText confidence="0.999885875">
drop back slightly. Clearly, there is considerably
more experimentation to be done here with mixed
n-gram models and different feature selection meth-
ods, but the results indicate that some methods cer-
tainly benefit from n-gram hybridisation and feature
selection, and also that we have been able to sur-
pass the results of Cavnar and Trenkle (1994) with
SKEW1NN in an otherwise identical framework.
</bodyText>
<subsectionHeader confidence="0.996921">
7.3 Breakdown Across Test Document Length
</subsectionHeader>
<bodyText confidence="0.99998662962963">
To better understand the impact of test document
size on classification accuracy, we divided the test
documents into 5 equal-size bins according to their
length, measured by the number of tokens. We then
computed Fµ individually for each bin across the 10
folds of cross validation. We present the breakdown
of results for WIKIPEDIA in Figure 2.
WIKIPEDIA shows a pseudo-logarithmic growth
in Fµ (= Pµ = Rµ) as the test document size in-
creases. This fits with our intuition, as the model
has progressively more evidence to base the classi-
fication on. It also suggests that performance over
shorter documents appears to be the dominating fac-
tor in the overall ranking of the different methods.
In particular, COS1NN and SVM appear to be able to
classify shorter documents most reliably, leading to
the overall result of them being the best-performing
methods.
While we do not show the graph for reasons of
space, the equivalent graph for EUROGOV displays
a curious effect: Fµ drops off as the test documents
get longer. Error analysis of the data indicates that
this is due to longer documents being more likely
to be “contaminated” with either data from a sec-
ond language or extra-linguistic data, such as large
tables of numbers or chemical names. This sug-
gests that all the models are brittle when the assump-
</bodyText>
<page confidence="0.997447">
235
</page>
<figureCaption confidence="0.9985605">
Figure 2: Breakdown of FN, over WIKIPEDIA for test
documents of increasing length
</figureCaption>
<bodyText confidence="0.999091615384615">
most languages in EUROGOV (the squares) both
having reasonably large amounts of training data and
achieving high FM values, but the majority of lan-
guages in WIKIPEDIA (the crosses) having very lit-
tle data (including a number of languages with no
training data, as there is a singleton document in that
language in the dataset). As an overall trend, we can
observe that the greater the volume of training data,
the higher the FM across all three datasets, but there
is considerable variation between the languages in
terms of their FM for a given training data size (the
column of crosses for WIKIPEDIA to the left of the
graph is particularly striking).
</bodyText>
<figureCaption confidence="0.998096">
Figure 3: Per-language Fm for COS1NN, relative to the
training data size (in MB) for that language
</figureCaption>
<bodyText confidence="0.999926285714286">
tion of strict monolingualism is broken, or when
the document is dominated by extra-linguistic data.
Clearly, this underlines our assumption of monolin-
gual documents, and suggests multilingual language
identification is a fertile research area even in terms
of optimising performance over our “monolingual”
datasets.
</bodyText>
<subsectionHeader confidence="0.974712">
7.4 Performance Relative to Training Data Size
</subsectionHeader>
<bodyText confidence="0.999981">
As a final data point in our analysis, we calculated
the FM for each language relative to the amount of
training data available for that language, and present
the results in the form of a combined scatter plot for
the three datasets in Figure 3. The differing distri-
butions of the three datasets are self-evident, with
</bodyText>
<sectionHeader confidence="0.999418" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.99998247826087">
We have carried out a thorough (re)examination of
the task of language identification, that is predict-
ing the language that a given document is written
in, focusing on monolingual documents at present.
We experimented with a total of 7 models, and
tested each over two tokenisation strategies (bigrams
vs. codepoints) and three token n-gram orders (un-
igrams, bigrams and trigrams). At the same time
as reproducing results from earlier research on how
easy the task can be over small numbers of lan-
guages with longer documents, we demonstrated
that the task becomes much harder for larger num-
bers of languages, shorter documents and greater
class skew. We also found that explicit character
encoding detection is not necessary in language de-
tection, and that the most consistent model overall
is either a simple 1-NN model with cosine similar-
ity, or an SVM with a linear kernel, using a byte
bigram or trigram document representation. We also
confirmed that longer documents tend to be easier to
classify, but also that multilingual documents cause
problems for the standard model of language identi-
fication.
</bodyText>
<sectionHeader confidence="0.996521" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.870922">
This research was supported by a Google Research
Award.
</bodyText>
<sectionHeader confidence="0.998719" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993564">
Beatrice Alex, Amit Dubey, and Frank Keller. 2007.
Using foreign inclusion detection to improve parsing
performance. In Proceedings of the Joint Conference
</reference>
<page confidence="0.990807">
236
</page>
<reference confidence="0.997234825242719">
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
2007 (EMNLP-CoNLL 2007), pages 151–160, Prague,
Czech Republic.
Javed A. Aslam and Meredith Frost. 2003. An
information-theoretic measure for document similar-
ity. In Proceedings of 26th International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR 2003), pages 449–450, Toronto,
Canada.
Timothy Baldwin, Steven Bird, and Baden Hughes.
2006. Collecting low-density language materials on
the web. In Proceedings of the 12th Australasian Web
Conference (AusWeb06). http://www.ausweb.
scu.edu.au/ausweb06/edited/hughes/.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on Document Analysis and Informa-
tion Retrieval, Las Vegas, USA.
Marc Darnashek. 1995. Gauging similarity with n-
grams: Language-independent categorization of text.
Science, 267:843–848.
Rafael Dueire Lins and Paulo Gonc¸alves. 2004. Au-
tomatic language identification of written texts. In
Proceedings of the 2004 ACM Symposium on Applied
Computing (SAC 2004), pages 1128–1133, Nicosia,
Cyprus.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS 940-273, Computing
Research Laboratory, New Mexico State University.
Emmanuel Giguet. 1995. Categorization according to
language: A step toward combining linguistic knowl-
edge and statistic learning. In Proceedings of the
4th International Workshop on Parsing Technologies
(IWPT-1995), Prague, Czech Republic.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 5:447–474.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of Analisi Sta-
tistica dei Dati Testuali (JADT), pages 263–268.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2008. A practical guide to support vector classifica-
tion. Technical report, Department of Computer Sci-
ence National Taiwan University.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006), pages 485–488, Genoa, Italy.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant fea-
tures. In Proceedings of the 10th European Confer-
ence on Machine Learning, pages 137–142, Chemnitz,
Germany.
Stephen Johnson. 1993. Solving the problem of lan-
guage recognition. Technical report, School of Com-
puter Studies, University of Leeds.
Canasai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In Pro-
ceedings of the 5th International Symposium on Com-
munications and Information Technologies (ISCIT-
2005), pages 896–899, Beijing, China.
Lillian Lee. 2001. On the effectiveness of the skew diver-
gence for statistical language analysis. In Proceedings
of Artificial Intelligence and Statistics 2001 (AISTATS
2001), pages 65–72, Key West, USA.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal ofMachine Learning
Research, 2:419–444.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Bruno Martins and M´ario J. Silva. 2005. Language iden-
tification in web pages. In Proceedings of the 2005
ACM symposium on Applied computing, pages 764–
768, Santa Fe, USA.
Andrew Kachites McCallum. 1996. Bow: A toolkit for
statistical language modeling, text retrieval, classifica-
tion and clustering. http://www.cs.cmu.edu/
˜mccallum/bow.
Paul McNamee and James Mayfield. 2004. Character N-
gram Tokenization for European Language Text Re-
trieval. Information Retrieval, 7(1–2):73–97.
Olivier Teytaud and Radwan Jalam. 2001. Kernel-
based text categorization. In Proceedings of the
International Joint Conference on Neural Networks
(IJCNN’2001), Washington DC, USA.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin, Germany.
Fei Xia and William Lewis. 2009. Applying NLP tech-
nologies to the collection and enrichment of language
data on the web to aid linguistic research. In Pro-
ceedings of the EACL 2009 Workshop on Language
Technology and Resources for Cultural Heritage, So-
cial Sciences, Humanities, and Education (LaTeCH –
SHELT&amp;R 2009), pages 51–59, Athens, Greece.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data off
the web. In Proceedings of the 12th Conference of the
EACL (EACL 2009), pages 870–878, Athens, Greece.
</reference>
<page confidence="0.997227">
237
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.565779">
<title confidence="0.998172">Language Identification: The Long and the Short of the Matter</title>
<author confidence="0.999946">Baldwin Lui</author>
<affiliation confidence="0.999984">Dept of Computer Science and Software Engineering</affiliation>
<address confidence="0.57626">University of Melbourne, VIC 3010 Australia</address>
<abstract confidence="0.998827285714286">Language identification is the task of identifying the language a given document is written in. This paper describes a detailed examination of what models perform best under different conditions, based on experiments across three separate datasets and a range of tokenisation strategies. We demonstrate that the task becomes increasingly difficult as we increase the number of languages, reduce the amount of training data and reduce the length of documents. We also show that it is possible to perform language identification without having to perform explicit character encoding detection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beatrice Alex</author>
<author>Amit Dubey</author>
<author>Frank Keller</author>
</authors>
<title>Using foreign inclusion detection to improve parsing performance.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>151--160</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7825" citStr="Alex et al., 2007" startWordPosition="1208" endWordPosition="1211"> particular resources for each language to be identified (e.g. POS taggers, or the existence of precompiled stop word lists) cannot be used. Language identification has been applied in a number of contexts, the most immediate application being in multilingual text retrieval, where retrieval results are generally superior if the language of the query is known, and the search is restricted to only those documents predicted to be in that language (McNamee and Mayfield, 2004). It can also be used to “word spot” foreign language terms in multilingual documents, e.g. to improve parsing performance (Alex et al., 2007), or for linguistic corpus creation purposes (Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009). 3 Datasets In the experiments reported in this paper, we employ three novel datasets, with differing properties relevant to language identification research: 230 Corpus Documents Languages Encodings Document Length (bytes) EUROGOV 1500 10 1 17460.5±39353.4 TCL 3174 60 12 2623.2±3751.9 WIKIPEDIA 4963 67 1 1480.8±4063.9 Table 1: Summary of the three language identification datasets Figure 1: Distribution of languages in the three datasets (vector of languages vs. the proportion of document</context>
</contexts>
<marker>Alex, Dubey, Keller, 2007</marker>
<rawString>Beatrice Alex, Amit Dubey, and Frank Keller. 2007. Using foreign inclusion detection to improve parsing performance. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning 2007 (EMNLP-CoNLL 2007), pages 151–160, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javed A Aslam</author>
<author>Meredith Frost</author>
</authors>
<title>An information-theoretic measure for document similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of 26th International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR</booktitle>
<pages>449--450</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="5062" citStr="Aslam and Frost, 2003" startWordPosition="772" endWordPosition="775">ion approach to language identification applies a standard supervised classification framework to the task. Perhaps the bestknown such model is that of Cavnar and Trenkle (1994), as popularised in the textcat tool.2 The method uses a per-language character frequency model, and classifies documents via their relative “out of place” distance from each language (see Section 5.1). Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the hig</context>
</contexts>
<marker>Aslam, Frost, 2003</marker>
<rawString>Javed A. Aslam and Meredith Frost. 2003. An information-theoretic measure for document similarity. In Proceedings of 26th International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003), pages 449–450, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Steven Bird</author>
<author>Baden Hughes</author>
</authors>
<title>Collecting low-density language materials on the web.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th Australasian Web Conference (AusWeb06). http://www.ausweb. scu.edu.au/ausweb06/edited/hughes/.</booktitle>
<contexts>
<context position="7891" citStr="Baldwin et al., 2006" startWordPosition="1218" endWordPosition="1221">OS taggers, or the existence of precompiled stop word lists) cannot be used. Language identification has been applied in a number of contexts, the most immediate application being in multilingual text retrieval, where retrieval results are generally superior if the language of the query is known, and the search is restricted to only those documents predicted to be in that language (McNamee and Mayfield, 2004). It can also be used to “word spot” foreign language terms in multilingual documents, e.g. to improve parsing performance (Alex et al., 2007), or for linguistic corpus creation purposes (Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009). 3 Datasets In the experiments reported in this paper, we employ three novel datasets, with differing properties relevant to language identification research: 230 Corpus Documents Languages Encodings Document Length (bytes) EUROGOV 1500 10 1 17460.5±39353.4 TCL 3174 60 12 2623.2±3751.9 WIKIPEDIA 4963 67 1 1480.8±4063.9 Table 1: Summary of the three language identification datasets Figure 1: Distribution of languages in the three datasets (vector of languages vs. the proportion of documents in that language) EUROGOV: longer documents, all in a single enc</context>
</contexts>
<marker>Baldwin, Bird, Hughes, 2006</marker>
<rawString>Timothy Baldwin, Steven Bird, and Baden Hughes. 2006. Collecting low-density language materials on the web. In Proceedings of the 12th Australasian Web Conference (AusWeb06). http://www.ausweb. scu.edu.au/ausweb06/edited/hughes/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>Ngram-based text categorization.</title>
<date>1994</date>
<booktitle>In Proceedings of the Third Symposium on Document Analysis and Information Retrieval,</booktitle>
<location>Las Vegas, USA.</location>
<contexts>
<context position="4618" citStr="Cavnar and Trenkle (1994)" startWordPosition="705" endWordPosition="709">tandardised evaluation of language identification. 2 Background Research Language identification was arguably established as a task by Gold (1967), who construed it as a closed class problem: given data in each of a predefined set of possible languages, human subjects were asked to classify the language of a given test document. It wasn’t until the 1990s, however, that the task was popularised as a text categorisation task. The text categorisation approach to language identification applies a standard supervised classification framework to the task. Perhaps the bestknown such model is that of Cavnar and Trenkle (1994), as popularised in the textcat tool.2 The method uses a per-language character frequency model, and classifies documents via their relative “out of place” distance from each language (see Section 5.1). Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task w</context>
<context position="13874" citStr="Cavnar and Trenkle, 1994" startWordPosition="2165" endWordPosition="2168">ty (COS): the cosine of the angle between two feature vectors, as measured by the dot product of the two vectors, normalised to unit length. Skew divergence (SKEW): a variant of KullbackLeibler divergence, whereby the second distribution 3http://chardet.feedparser.org/ (y) is smoothed by linear interpolation with the first (x) using a smoothing factor a (Lee, 2001): sα(x, y) = D(x ||ay + (1 − a)x) where: D(x ||y) = X xi(log2 xi − log2 yi) i In all our experiments, we set a to 0.99. Out-of-place (OOP): a ranklist-based distance metric, where the distance between two documents is calculated as (Cavnar and Trenkle, 1994): oop(Dx, Dy) = X abs(RDx(t) − RDy(t)) tEDxVDy RD(t) is the rank of term t in document D, based on the descending order of frequency in document D; terms not occurring in document D are conventionally given the rank 1 + maxi RD(ti). 5.2 Naive Bayes (NB) Naive Bayes is a popular text classification model, due to it being lightweight, robust and easy to update. The language of test document D is predicted by: ˆl(D) = arg max liEL where L is the set of languages in the training data, ND,tj is the frequency of the jth term in D, V is the set of all terms, and: P(t|li) = 1 + P|D |1 Nk,tP(li|Dk) |V </context>
<context position="23834" citStr="Cavnar and Trenkle (1994)" startWordPosition="3855" endWordPosition="3859">9 .518 .548 .831 NB 1 .576 .578 .555 .778 NB 2 .496 .454 .442 .851 NB 3 .493 .435 .432 .863 SVM 1 .585 .505 .523 .812 SVM 2 .667 .545 .577 .845 SVM 3 .717 .547 .594 .840 Table 5: Results for different n-gram orders over WIKIPEDIA Q = 0.01, e.g.), based on the n-gram order. We leave this for future research. For most methods, bigrams and trigrams are better than unigrams, with the one notable exception of SKEW1NN. In general, there is little separating bigrams and trigrams, although the best result for is achieved slightly more often for bigrams than for trigrams. For direct comparability with Cavnar and Trenkle (1994), we additionally carried out a preliminary experiment with hybrid byte n-grams (all of 1- to 5- grams), combined with simple frequency-based feature selection of the top-1000 features for each ngram order. The significance of this setting is that it is the strategy adopted by textcat, based on the original paper of Cavnar and Trenkle (1994) (with the one exception that we use 1000 features rather than 300, as all methods other than OOP1NN benefitted from more features). The results are shown in Table 6. Compared to the results in Table 5, SKEW1NN and SKEWAM both increase markedly to achieve t</context>
<context position="25213" citStr="Cavnar and Trenkle (1994)" startWordPosition="4090" endWordPosition="4093">7 .328 COS1NN .735 .664 .682 .865 COSAM .592 .626 .580 .766 SKEW1NN .789 .708 .729 .902 SKEWAM .681 .718 .680 .870 OOP1NN .697 .595 .626 .864 SVM .669 .500 .544 .832 Table 6: Results for mixed n-grams (1–5) and feature selection over WIKIPEDIA (a l´a Cavnar and Trenkle (1994)) drop back slightly. Clearly, there is considerably more experimentation to be done here with mixed n-gram models and different feature selection methods, but the results indicate that some methods certainly benefit from n-gram hybridisation and feature selection, and also that we have been able to surpass the results of Cavnar and Trenkle (1994) with SKEW1NN in an otherwise identical framework. 7.3 Breakdown Across Test Document Length To better understand the impact of test document size on classification accuracy, we divided the test documents into 5 equal-size bins according to their length, measured by the number of tokens. We then computed Fµ individually for each bin across the 10 folds of cross validation. We present the breakdown of results for WIKIPEDIA in Figure 2. WIKIPEDIA shows a pseudo-logarithmic growth in Fµ (= Pµ = Rµ) as the test document size increases. This fits with our intuition, as the model has progressively m</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. Ngram-based text categorization. In Proceedings of the Third Symposium on Document Analysis and Information Retrieval, Las Vegas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Darnashek</author>
</authors>
<title>Gauging similarity with ngrams: Language-independent categorization of text.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>267--843</pages>
<contexts>
<context position="4982" citStr="Darnashek, 1995" startWordPosition="763" endWordPosition="764">he task was popularised as a text categorisation task. The text categorisation approach to language identification applies a standard supervised classification framework to the task. Perhaps the bestknown such model is that of Cavnar and Trenkle (1994), as popularised in the textcat tool.2 The method uses a per-language character frequency model, and classifies documents via their relative “out of place” distance from each language (see Section 5.1). Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages </context>
</contexts>
<marker>Darnashek, 1995</marker>
<rawString>Marc Darnashek. 1995. Gauging similarity with ngrams: Language-independent categorization of text. Science, 267:843–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael Dueire Lins</author>
<author>Paulo Gonc¸alves</author>
</authors>
<title>Automatic language identification of written texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 ACM Symposium on Applied Computing (SAC 2004),</booktitle>
<pages>1128--1133</pages>
<location>Nicosia, Cyprus.</location>
<marker>Lins, Gonc¸alves, 2004</marker>
<rawString>Rafael Dueire Lins and Paulo Gonc¸alves. 2004. Automatic language identification of written texts. In Proceedings of the 2004 ACM Symposium on Applied Computing (SAC 2004), pages 1128–1133, Nicosia, Cyprus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical identification of language.</title>
<date>1994</date>
<tech>Technical Report MCCS 940-273,</tech>
<institution>Computing Research Laboratory, New Mexico State University.</institution>
<contexts>
<context position="4924" citStr="Dunning, 1994" startWordPosition="754" endWordPosition="755">est document. It wasn’t until the 1990s, however, that the task was popularised as a text categorisation task. The text categorisation approach to language identification applies a standard supervised classification framework to the task. Perhaps the bestknown such model is that of Cavnar and Trenkle (1994), as popularised in the textcat tool.2 The method uses a per-language character frequency model, and classifies documents via their relative “out of place” distance from each language (see Section 5.1). Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson </context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical identification of language. Technical Report MCCS 940-273, Computing Research Laboratory, New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Giguet</author>
</authors>
<title>Categorization according to language: A step toward combining linguistic knowledge and statistic learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the 4th International Workshop on Parsing Technologies (IWPT-1995),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5860" citStr="Giguet (1995)" startWordPosition="906" endWordPosition="907">m, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the highest stop word overlap with the document. Grefenstette (1995) used word and part of speech (POS) correlation to determine if two text samples were from the same or different languages. Giguet (1995) developed a 2http://www.let.rug.nl/vannoord/TextCat/ cross-language tokenisation model and used it to identify the language of a given document based on its tokenisation similarity with training data. Dueire Lins and Gonc¸alves (2004) considered the use of syntactically-derived closed grammaticalclass models, matching syntactic structure rather than words or character sequences. The observant reader will have noticed that some of the above approaches make use of notions such as “word”, typically based on the naive assumption that the language uses white space to delimit words. These approache</context>
</contexts>
<marker>Giguet, 1995</marker>
<rawString>Emmanuel Giguet. 1995. Categorization according to language: A step toward combining linguistic knowledge and statistic learning. In Proceedings of the 4th International Workshop on Parsing Technologies (IWPT-1995), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mark Gold</author>
</authors>
<date>1967</date>
<booktitle>Language identification in the limit. Information and Control,</booktitle>
<pages>5--447</pages>
<contexts>
<context position="4139" citStr="Gold (1967)" startWordPosition="629" endWordPosition="630"> Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 229–237, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics with character encoding detection; and simple cosine similarity-based nearest neighbour classification is equal to or better than models including support vector machines and naive Bayes over the language identification task. We also develop datasets to facilitate standardised evaluation of language identification. 2 Background Research Language identification was arguably established as a task by Gold (1967), who construed it as a closed class problem: given data in each of a predefined set of possible languages, human subjects were asked to classify the language of a given test document. It wasn’t until the 1990s, however, that the task was popularised as a text categorisation task. The text categorisation approach to language identification applies a standard supervised classification framework to the task. Perhaps the bestknown such model is that of Cavnar and Trenkle (1994), as popularised in the textcat tool.2 The method uses a per-language character frequency model, and classifies documents</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E. Mark Gold. 1967. Language identification in the limit. Information and Control, 5:447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Comparing two language identification schemes.</title>
<date>1995</date>
<booktitle>In Proceedings of Analisi Statistica dei Dati Testuali (JADT),</booktitle>
<pages>263--268</pages>
<contexts>
<context position="5723" citStr="Grefenstette (1995)" startWordPosition="882" endWordPosition="883"> support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the highest stop word overlap with the document. Grefenstette (1995) used word and part of speech (POS) correlation to determine if two text samples were from the same or different languages. Giguet (1995) developed a 2http://www.let.rug.nl/vannoord/TextCat/ cross-language tokenisation model and used it to identify the language of a given document based on its tokenisation similarity with training data. Dueire Lins and Gonc¸alves (2004) considered the use of syntactically-derived closed grammaticalclass models, matching syntactic structure rather than words or character sequences. The observant reader will have noticed that some of the above approaches make us</context>
</contexts>
<marker>Grefenstette, 1995</marker>
<rawString>Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of Analisi Statistica dei Dati Testuali (JADT), pages 263–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Wei Hsu</author>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>A practical guide to support vector classification.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science National Taiwan University.</institution>
<contexts>
<context position="15238" citStr="Hsu et al., 2008" startWordPosition="2402" endWordPosition="2405">ctor Machines (SVM) Support vector machines (SVMs) are one of the most popular methods for text classification, largely because they can automatically weight large numbers of features, capturing feature interactions in the process (Joachims, 1998; Manning et al., 2008). The basic principle underlying SVMs is to maximize the P(tj|li)ND,tj ND,tj! P(li) Y |V | j=1 232 margin between training instances and the calculated decision boundary based on structural risk minimisation (Vapnik, 1995). In this work, we have made use of bsvm,4 an implementation of SVMs with multiclass classification support (Hsu et al., 2008). We only report results for multi-class bound-constrained support vector machines with linear kernels, as they were found to perform best over our data. 6 Experimental Methodology We carry out experiments over the cross-product of the following options, as described above: model (x7): nearest-neighbour (COS1NN, SKEW1NN, OOP1NN), nearest-prototype (COSAM, SKEWAM),5 NB, SVM tokenisation (x2): byte, codepoint n-gram (x3): 1-gram, 2-gram, 3-gram for a total of 42 distinct classifiers. Each classifier is run across the 3 datasets (EUROGOV, TCL and WIKIPEDIA) based on 10-fold stratified crossvalida</context>
</contexts>
<marker>Hsu, Chang, Lin, 2008</marker>
<rawString>Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin. 2008. A practical guide to support vector classification. Technical report, Department of Computer Science National Taiwan University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baden Hughes</author>
<author>Timothy Baldwin</author>
<author>Steven Bird</author>
<author>Jeremy Nicholson</author>
<author>Andrew MacKinlay</author>
</authors>
<title>Reconsidering language identification for written language resources.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>485--488</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="2046" citStr="Hughes et al., 2006" startWordPosition="311" endWordPosition="314"> used to render a large number of languages such that the document encoding at best filters out a subset of languages which are incompatible with the given encoding, rather than disambiguates the source language. Given this, the need for automatic means to determine the source language of web doc1http://dev.opera.com/articles/view/ mama-head-structure/ uments is crucial for web aggregators of various types. There is widespread misconception of language identification being a “solved task”, generally as a result of isolated experiments over homogeneous datasets with small numbers of languages (Hughes et al., 2006; Xia et al., 2009). Part of the motivation for this paper is to draw attention to the fact that, as a field, we are still a long way off perfect language identification of web documents, as evaluated under realistic conditions. In this paper we describe experiments on language identification of web documents, focusing on the broad question of what combination of tokenisation strategy and classification model achieves the best overall performance. We additionally evaluate the impact of the volume of training data and the test document length on the accuracy of language identification, and inve</context>
</contexts>
<marker>Hughes, Baldwin, Bird, Nicholson, MacKinlay, 2006</marker>
<rawString>Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy Nicholson, and Andrew MacKinlay. 2006. Reconsidering language identification for written language resources. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), pages 485–488, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<location>Chemnitz, Germany.</location>
<contexts>
<context position="14867" citStr="Joachims, 1998" startWordPosition="2344" endWordPosition="2345">est document D is predicted by: ˆl(D) = arg max liEL where L is the set of languages in the training data, ND,tj is the frequency of the jth term in D, V is the set of all terms, and: P(t|li) = 1 + P|D |1 Nk,tP(li|Dk) |V |+ P|V |P|D| k=1 Nk,tjP(li|Dk) j=1 In this research, we use the rainbow implementation of multinominal naive Bayes (McCallum, 1996). 5.3 Support Vector Machines (SVM) Support vector machines (SVMs) are one of the most popular methods for text classification, largely because they can automatically weight large numbers of features, capturing feature interactions in the process (Joachims, 1998; Manning et al., 2008). The basic principle underlying SVMs is to maximize the P(tj|li)ND,tj ND,tj! P(li) Y |V | j=1 232 margin between training instances and the calculated decision boundary based on structural risk minimisation (Vapnik, 1995). In this work, we have made use of bsvm,4 an implementation of SVMs with multiclass classification support (Hsu et al., 2008). We only report results for multi-class bound-constrained support vector machines with linear kernels, as they were found to perform best over our data. 6 Experimental Methodology We carry out experiments over the cross-product </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning, pages 137–142, Chemnitz, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Johnson</author>
</authors>
<title>Solving the problem of language recognition.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>School of Computer Studies, University of Leeds.</institution>
<contexts>
<context position="5530" citStr="Johnson (1993)" startWordPosition="849" endWordPosition="850">g, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the highest stop word overlap with the document. Grefenstette (1995) used word and part of speech (POS) correlation to determine if two text samples were from the same or different languages. Giguet (1995) developed a 2http://www.let.rug.nl/vannoord/TextCat/ cross-language tokenisation model and used it to identify the language of a given document based on its tokenisation similarity with training data. Dueire Lins and Gonc¸alves (2004) considered the use of syntacticall</context>
</contexts>
<marker>Johnson, 1993</marker>
<rawString>Stephen Johnson. 1993. Solving the problem of language recognition. Technical report, School of Computer Studies, University of Leeds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Prapass Srichaivattana</author>
<author>Virach Sornlertlamvanich</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Language identification based on string kernels.</title>
<date>2005</date>
<booktitle>In Proceedings of the 5th International Symposium on Communications and Information Technologies (ISCIT2005),</booktitle>
<pages>896--899</pages>
<location>Beijing, China.</location>
<contexts>
<context position="5300" citStr="Kruengkrai et al., 2005" startWordPosition="811" endWordPosition="814">er-language character frequency model, and classifies documents via their relative “out of place” distance from each language (see Section 5.1). Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the highest stop word overlap with the document. Grefenstette (1995) used word and part of speech (POS) correlation to determine if two text samples were from the same or different languages. Giguet (1995) developed a 2http://www.let.rug.nl/vann</context>
</contexts>
<marker>Kruengkrai, Srichaivattana, Sornlertlamvanich, Isahara, 2005</marker>
<rawString>Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlertlamvanich, and Hitoshi Isahara. 2005. Language identification based on string kernels. In Proceedings of the 5th International Symposium on Communications and Information Technologies (ISCIT2005), pages 896–899, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>On the effectiveness of the skew divergence for statistical language analysis.</title>
<date>2001</date>
<booktitle>In Proceedings of Artificial Intelligence and Statistics</booktitle>
<pages>65--72</pages>
<location>Key West, USA.</location>
<contexts>
<context position="13616" citStr="Lee, 2001" startWordPosition="2119" endWordPosition="2120">of the training instances for that language into a single centroid via the arithmetic mean. For both nearest-neighbour and nearest-prototype methods, we experimented with three similarity and distance measures in this research: Cosine similarity (COS): the cosine of the angle between two feature vectors, as measured by the dot product of the two vectors, normalised to unit length. Skew divergence (SKEW): a variant of KullbackLeibler divergence, whereby the second distribution 3http://chardet.feedparser.org/ (y) is smoothed by linear interpolation with the first (x) using a smoothing factor a (Lee, 2001): sα(x, y) = D(x ||ay + (1 − a)x) where: D(x ||y) = X xi(log2 xi − log2 yi) i In all our experiments, we set a to 0.99. Out-of-place (OOP): a ranklist-based distance metric, where the distance between two documents is calculated as (Cavnar and Trenkle, 1994): oop(Dx, Dy) = X abs(RDx(t) − RDy(t)) tEDxVDy RD(t) is the rank of term t in document D, based on the descending order of frequency in document D; terms not occurring in document D are conventionally given the rank 1 + maxi RD(ti). 5.2 Naive Bayes (NB) Naive Bayes is a popular text classification model, due to it being lightweight, robust </context>
</contexts>
<marker>Lee, 2001</marker>
<rawString>Lillian Lee. 2001. On the effectiveness of the skew divergence for statistical language analysis. In Proceedings of Artificial Intelligence and Statistics 2001 (AISTATS 2001), pages 65–72, Key West, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="5274" citStr="Lodhi et al., 2002" startWordPosition="807" endWordPosition="810"> The method uses a per-language character frequency model, and classifies documents via their relative “out of place” distance from each language (see Section 5.1). Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the highest stop word overlap with the document. Grefenstette (1995) used word and part of speech (POS) correlation to determine if two text samples were from the same or different languages. Giguet (1995) developed a 2</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal ofMachine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Martins</author>
<author>M´ario J Silva</author>
</authors>
<title>Language identification in web pages.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 ACM symposium on Applied computing,</booktitle>
<pages>764--768</pages>
<location>Santa Fe, USA.</location>
<contexts>
<context position="5088" citStr="Martins and Silva, 2005" startWordPosition="776" endWordPosition="779">e identification applies a standard supervised classification framework to the task. Perhaps the bestknown such model is that of Cavnar and Trenkle (1994), as popularised in the textcat tool.2 The method uses a per-language character frequency model, and classifies documents via their relative “out of place” distance from each language (see Section 5.1). Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the highest stop word overlap wit</context>
</contexts>
<marker>Martins, Silva, 2005</marker>
<rawString>Bruno Martins and M´ario J. Silva. 2005. Language identification in web pages. In Proceedings of the 2005 ACM symposium on Applied computing, pages 764– 768, Santa Fe, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering.</title>
<date>1996</date>
<note>http://www.cs.cmu.edu/ ˜mccallum/bow.</note>
<contexts>
<context position="14605" citStr="McCallum, 1996" startWordPosition="2306" endWordPosition="2307">der of frequency in document D; terms not occurring in document D are conventionally given the rank 1 + maxi RD(ti). 5.2 Naive Bayes (NB) Naive Bayes is a popular text classification model, due to it being lightweight, robust and easy to update. The language of test document D is predicted by: ˆl(D) = arg max liEL where L is the set of languages in the training data, ND,tj is the frequency of the jth term in D, V is the set of all terms, and: P(t|li) = 1 + P|D |1 Nk,tP(li|Dk) |V |+ P|V |P|D| k=1 Nk,tjP(li|Dk) j=1 In this research, we use the rainbow implementation of multinominal naive Bayes (McCallum, 1996). 5.3 Support Vector Machines (SVM) Support vector machines (SVMs) are one of the most popular methods for text classification, largely because they can automatically weight large numbers of features, capturing feature interactions in the process (Joachims, 1998; Manning et al., 2008). The basic principle underlying SVMs is to maximize the P(tj|li)ND,tj ND,tj! P(li) Y |V | j=1 232 margin between training instances and the calculated decision boundary based on structural risk minimisation (Vapnik, 1995). In this work, we have made use of bsvm,4 an implementation of SVMs with multiclass classifi</context>
</contexts>
<marker>McCallum, 1996</marker>
<rawString>Andrew Kachites McCallum. 1996. Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. http://www.cs.cmu.edu/ ˜mccallum/bow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>James Mayfield</author>
</authors>
<title>Character Ngram Tokenization for European Language Text Retrieval. Information Retrieval,</title>
<date>2004</date>
<pages>7--1</pages>
<contexts>
<context position="7683" citStr="McNamee and Mayfield, 2004" startWordPosition="1184" endWordPosition="1187">hat assume a language is spacedelimited are clearly not suitable for our purposes. Equally, approaches which make assumptions about the availability of particular resources for each language to be identified (e.g. POS taggers, or the existence of precompiled stop word lists) cannot be used. Language identification has been applied in a number of contexts, the most immediate application being in multilingual text retrieval, where retrieval results are generally superior if the language of the query is known, and the search is restricted to only those documents predicted to be in that language (McNamee and Mayfield, 2004). It can also be used to “word spot” foreign language terms in multilingual documents, e.g. to improve parsing performance (Alex et al., 2007), or for linguistic corpus creation purposes (Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009). 3 Datasets In the experiments reported in this paper, we employ three novel datasets, with differing properties relevant to language identification research: 230 Corpus Documents Languages Encodings Document Length (bytes) EUROGOV 1500 10 1 17460.5±39353.4 TCL 3174 60 12 2623.2±3751.9 WIKIPEDIA 4963 67 1 1480.8±4063.9 Table 1: Summary of the three </context>
</contexts>
<marker>McNamee, Mayfield, 2004</marker>
<rawString>Paul McNamee and James Mayfield. 2004. Character Ngram Tokenization for European Language Text Retrieval. Information Retrieval, 7(1–2):73–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Teytaud</author>
<author>Radwan Jalam</author>
</authors>
<title>Kernelbased text categorization.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Joint Conference on Neural Networks (IJCNN’2001),</booktitle>
<location>Washington DC, USA.</location>
<contexts>
<context position="5254" citStr="Teytaud and Jalam, 2001" startWordPosition="803" endWordPosition="806">sed in the textcat tool.2 The method uses a per-language character frequency model, and classifies documents via their relative “out of place” distance from each language (see Section 5.1). Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the highest stop word overlap with the document. Grefenstette (1995) used word and part of speech (POS) correlation to determine if two text samples were from the same or different languages. Giguet </context>
</contexts>
<marker>Teytaud, Jalam, 2001</marker>
<rawString>Olivier Teytaud and Radwan Jalam. 2001. Kernelbased text categorization. In Proceedings of the International Joint Conference on Neural Networks (IJCNN’2001), Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="15112" citStr="Vapnik, 1995" startWordPosition="2382" endWordPosition="2383">i|Dk) j=1 In this research, we use the rainbow implementation of multinominal naive Bayes (McCallum, 1996). 5.3 Support Vector Machines (SVM) Support vector machines (SVMs) are one of the most popular methods for text classification, largely because they can automatically weight large numbers of features, capturing feature interactions in the process (Joachims, 1998; Manning et al., 2008). The basic principle underlying SVMs is to maximize the P(tj|li)ND,tj ND,tj! P(li) Y |V | j=1 232 margin between training instances and the calculated decision boundary based on structural risk minimisation (Vapnik, 1995). In this work, we have made use of bsvm,4 an implementation of SVMs with multiclass classification support (Hsu et al., 2008). We only report results for multi-class bound-constrained support vector machines with linear kernels, as they were found to perform best over our data. 6 Experimental Methodology We carry out experiments over the cross-product of the following options, as described above: model (x7): nearest-neighbour (COS1NN, SKEW1NN, OOP1NN), nearest-prototype (COSAM, SKEWAM),5 NB, SVM tokenisation (x2): byte, codepoint n-gram (x3): 1-gram, 2-gram, 3-gram for a total of 42 distinct </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>William Lewis</author>
</authors>
<title>Applying NLP technologies to the collection and enrichment of language data on the web to aid linguistic research.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education (LaTeCH – SHELT&amp;R</booktitle>
<pages>51--59</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="7931" citStr="Xia and Lewis, 2009" startWordPosition="1226" endWordPosition="1229">led stop word lists) cannot be used. Language identification has been applied in a number of contexts, the most immediate application being in multilingual text retrieval, where retrieval results are generally superior if the language of the query is known, and the search is restricted to only those documents predicted to be in that language (McNamee and Mayfield, 2004). It can also be used to “word spot” foreign language terms in multilingual documents, e.g. to improve parsing performance (Alex et al., 2007), or for linguistic corpus creation purposes (Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009). 3 Datasets In the experiments reported in this paper, we employ three novel datasets, with differing properties relevant to language identification research: 230 Corpus Documents Languages Encodings Document Length (bytes) EUROGOV 1500 10 1 17460.5±39353.4 TCL 3174 60 12 2623.2±3751.9 WIKIPEDIA 4963 67 1 1480.8±4063.9 Table 1: Summary of the three language identification datasets Figure 1: Distribution of languages in the three datasets (vector of languages vs. the proportion of documents in that language) EUROGOV: longer documents, all in a single encoding, spread evenly across a relatively</context>
</contexts>
<marker>Xia, Lewis, 2009</marker>
<rawString>Fei Xia and William Lewis. 2009. Applying NLP technologies to the collection and enrichment of language data on the web to aid linguistic research. In Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education (LaTeCH – SHELT&amp;R 2009), pages 51–59, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>William Lewis</author>
<author>Hoifung Poon</author>
</authors>
<title>Language ID in the context of harvesting language data off the web.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the EACL (EACL</booktitle>
<pages>870--878</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="2065" citStr="Xia et al., 2009" startWordPosition="315" endWordPosition="318">ge number of languages such that the document encoding at best filters out a subset of languages which are incompatible with the given encoding, rather than disambiguates the source language. Given this, the need for automatic means to determine the source language of web doc1http://dev.opera.com/articles/view/ mama-head-structure/ uments is crucial for web aggregators of various types. There is widespread misconception of language identification being a “solved task”, generally as a result of isolated experiments over homogeneous datasets with small numbers of languages (Hughes et al., 2006; Xia et al., 2009). Part of the motivation for this paper is to draw attention to the fact that, as a field, we are still a long way off perfect language identification of web documents, as evaluated under realistic conditions. In this paper we describe experiments on language identification of web documents, focusing on the broad question of what combination of tokenisation strategy and classification model achieves the best overall performance. We additionally evaluate the impact of the volume of training data and the test document length on the accuracy of language identification, and investigate the interac</context>
<context position="5427" citStr="Xia et al., 2009" startWordPosition="834" endWordPosition="837">tion 5.1). Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and informationtheoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005). More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009). Language identification has also been carried out via linguistically motivated models. Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the highest stop word overlap with the document. Grefenstette (1995) used word and part of speech (POS) correlation to determine if two text samples were from the same or different languages. Giguet (1995) developed a 2http://www.let.rug.nl/vannoord/TextCat/ cross-language tokenisation model and used it to identify the language of a given document based on its tokenisat</context>
<context position="7909" citStr="Xia et al., 2009" startWordPosition="1222" endWordPosition="1225">stence of precompiled stop word lists) cannot be used. Language identification has been applied in a number of contexts, the most immediate application being in multilingual text retrieval, where retrieval results are generally superior if the language of the query is known, and the search is restricted to only those documents predicted to be in that language (McNamee and Mayfield, 2004). It can also be used to “word spot” foreign language terms in multilingual documents, e.g. to improve parsing performance (Alex et al., 2007), or for linguistic corpus creation purposes (Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009). 3 Datasets In the experiments reported in this paper, we employ three novel datasets, with differing properties relevant to language identification research: 230 Corpus Documents Languages Encodings Document Length (bytes) EUROGOV 1500 10 1 17460.5±39353.4 TCL 3174 60 12 2623.2±3751.9 WIKIPEDIA 4963 67 1 1480.8±4063.9 Table 1: Summary of the three language identification datasets Figure 1: Distribution of languages in the three datasets (vector of languages vs. the proportion of documents in that language) EUROGOV: longer documents, all in a single encoding, spread even</context>
</contexts>
<marker>Xia, Lewis, Poon, 2009</marker>
<rawString>Fei Xia, William Lewis, and Hoifung Poon. 2009. Language ID in the context of harvesting language data off the web. In Proceedings of the 12th Conference of the EACL (EACL 2009), pages 870–878, Athens, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>