<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.989034">
Effective Measures of Domain Similarity for Parsing
</title>
<author confidence="0.985849">
Barbara Plank Gertjan van Noord
</author>
<affiliation confidence="0.9269135">
University of Groningen University of Groningen
The Netherlands The Netherlands
</affiliation>
<email confidence="0.9965">
b.plank@rug.nl G.J.M.van.Noord@rug.nl
</email>
<sectionHeader confidence="0.99559" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909863636364">
It is well known that parsing accuracy suf-
fers when a model is applied to out-of-domain
data. It is also known that the most benefi-
cial data to parse a given domain is data that
matches the domain (Sekine, 1997; Gildea,
2001). Hence, an important task is to select
appropriate domains. However, most previ-
ous work on domain adaptation relied on the
implicit assumption that domains are some-
how given. As more and more data becomes
available, automatic ways to select data that is
beneficial for a new (unknown) target domain
are becoming attractive. This paper evaluates
various ways to automatically acquire related
training data for a given test set. The results
show that an unsupervised technique based on
topic models is effective – it outperforms ran-
dom data selection on both languages exam-
ined, English and Dutch. Moreover, the tech-
nique works better than manually assigned la-
bels gathered from meta-data that is available
for English.
</bodyText>
<sectionHeader confidence="0.990748" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.997636954545455">
Previous research on domain adaptation has focused
on the task of adapting a system trained on one do-
main, say newspaper text, to a particular new do-
main, say biomedical data. Usually, some amount
of (labeled or unlabeled) data from the new domain
was given – which has been determined by a human.
However, with the growth of the web, more and
more data is becoming available, where each doc-
ument “is potentially its own domain” (McClosky
et al., 2010). It is not straightforward to determine
which data or model (in case we have several source
domain models) will perform best on a new (un-
known) target domain. Therefore, an important is-
sue that arises is how to measure domain similar-
ity, i.e. whether we can find a simple yet effective
method to determine which model or data is most
beneficial for an arbitrary piece of new text. More-
over, if we had such a measure, a related question is
whether it can tell us something more about what is
actually meant by “domain”. So far, it was mostly
arbitrarily used to refer to some kind of coherent
unit (related to topic, style or genre), e.g.: newspa-
per text, biomedical abstracts, questions, fiction.
Most previous work on domain adaptation, for in-
stance Hara et al. (2005), McClosky et al. (2006),
Blitzer et al. (2006), Daum´e III (2007), sidestepped
this problem of automatic domain selection and
adaptation. For parsing, to our knowledge only one
recent study has started to examine this issue (Mc-
Closky et al., 2010) – we will discuss their approach
in Section 2. Rather, an implicit assumption of all of
these studies is that domains are given, i.e. that they
are represented by the respective corpora. Thus, a
corpus has been considered a homogeneous unit. As
more data is becoming available, it is unlikely that
domains will be ‘given’. Moreover, a given corpus
might not always be as homogeneous as originally
thought (Webber, 2009; Lippincott et al., 2010). For
instance, recent work has shown that the well-known
Penn Treebank (PT) Wall Street Journal (WSJ) ac-
tually contains a variety of genres, including letters,
wit and short verse (Webber, 2009).
In this study we take a different approach. Rather
than viewing a given corpus as a monolithic entity,
</bodyText>
<page confidence="0.927578">
1566
</page>
<note confidence="0.9792965">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1566–1576,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9941614">
we break it down to the article-level and disregard
corpora boundaries. Given the resulting set of doc-
uments (articles), we evaluate various ways to au-
tomatically acquire related training data for a given
test set, to find answers to the following questions:
</bodyText>
<listItem confidence="0.981404625">
• Given a pool of data (a collection of articles
from unknown domains) and a test article, is
there a way to automatically select data that is
relevant for the new domain? If so:
• Which similarity measure is good for parsing?
• How does it compare to human-annotated data?
• Is the measure also useful for other languages
and/or tasks?
</listItem>
<bodyText confidence="0.99989445">
To this end, we evaluate measures of domain sim-
ilarity and feature representations and their impact
on dependency parsing accuracy. Given a collection
of annotated articles, and a new article that we want
to parse, we want to select the most similar articles
to train the best parser for that new article.
In the following, we will first compare automatic
measures to human-annotated labels by examining
parsing performance within subdomains of the Penn
Treebank WSJ. Then, we extend the experiments to
the domain adaptation scenario. Experiments were
performed on two languages: English and Dutch.
The empirical results show that a simple measure
based on topic distributions is effective for both lan-
guages and works well also for Part-of-Speech tag-
ging. As the approach is based on plain surface-
level information (words) and it finds related data in
a completely unsupervised fashion, it can be easily
applied to other tasks or languages for which anno-
tated (or automatically annotated) data is available.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999276693877551">
The work most related to ours is McClosky et al.
(2010). They try to find the best combination of
source models to parse data from a new domain,
which is related to Plank and Sima’an (2008). In
the latter, unlabeled data was used to create sev-
eral parsers by weighting trees in the WSJ accord-
ing to their similarity to the subdomain. McClosky
et al. (2010) coined the term multiple source domain
adaptation. Inspired by work on parsing accuracy
prediction (Ravi et al., 2008), they train a linear re-
gression model to predict the best (linear interpola-
tion) of source domain models. Similar to us, Mc-
Closky et al. (2010) regard a target domain as mix-
ture of source domains, but they focus on phrase-
structure parsing. Furthermore, our approach differs
from theirs in two respects: we do not treat source
corpora as one entity and try to mix models, but
rather consider articles as base units and try to find
subsets of related articles (the most similar articles);
moreover, instead of creating a supervised model (in
their case to predict parsing accuracy), our approach
is ‘simplistic’: we apply measures of domain simi-
larity directly (in an unsupervised fashion), without
the necessity to train a supervised model.
Two other related studies are (Lippincott et al.,
2010; Van Asch and Daelemans, 2010). Van Asch
and Daelemans (2010) explore a measure of domain
difference (Renyi divergence) between pairs of do-
mains and its correlation to Part-of-Speech tagging
accuracy. Their empirical results show a linear cor-
relation between the measure and the performance
loss. Their goal is different, but related: rather than
finding related data for a new domain, they want to
estimate the loss in accuracy of a PoS tagger when
applied to a new domain. We will briefly discuss
results obtained with the Renyi divergence in Sec-
tion 5.1. Lippincott et al. (2010) examine subdomain
variation in biomedicine corpora and propose aware-
ness of NLP tools to such variation. However, they
did not yet evaluate the effect on a practical task,
thus our study is somewhat complementary to theirs.
The issue of data selection has recently been ex-
amined for Language Modeling (Moore and Lewis,
2010). A subset of the available data is automati-
cally selected as training data for a Language Model
based on a scoring mechanism that compares cross-
entropy scores. Their approach considerably outper-
formed random selection and two previous proposed
approaches both based on perplexity scoring.1
</bodyText>
<sectionHeader confidence="0.902879" genericHeader="method">
3 Measures of Domain Similarity
</sectionHeader>
<subsectionHeader confidence="0.999219">
3.1 Measuring Similarity Automatically
</subsectionHeader>
<footnote confidence="0.797727">
Feature Representations A similarity function
may be defined over any set of events that are con-
1We tested data selection by perplexity scoring, but found
the Language Models too small to be useful in our setting.
</footnote>
<page confidence="0.995998">
1567
</page>
<bodyText confidence="0.996992347826087">
sidered to be relevant for the task at hand. For
parsing, these might be words, characters, n-grams
(of words or characters), Part-of-Speech (PoS) tags,
bilexical dependencies, syntactic rules, etc. How-
ever, to obtain more abstract types such as PoS tags
or dependency relations, one would first need to
gather respective labels. The necessary tools for this
are again trained on particular corpora, and will suf-
fer from domain shifts, rendering labels noisy.
Therefore, we want to gauge the effect of the sim-
plest representation possible: plain surface charac-
teristics (unlabeled text). This has the advantage
that we do not need to rely on additional supervised
tools; moreover, it is interesting to know how far we
can get with this level of information only.
We examine the following feature representa-
tions: relative frequencies of words, relative fre-
quencies of character tetragrams, and topic mod-
els. Our motivation was as follows. Relative fre-
quencies of words are a simple and effective rep-
resentation used e.g. in text classification (Manning
and Sch¨utze, 1999), while character n-grams have
proven successful in genre classification (Wu et al.,
2010). Topic models (Blei et al., 2003; Steyvers
and Griffiths, 2007) can be considered an advanced
model over word distributions: every article is repre-
sented by a topic distribution, which in turn is a dis-
tribution over words. Similarity between documents
can be measured by comparing topic distributions.
Similarity Functions There are many possible
similarity (or distance) functions. They fall broadly
into two categories: probabilistically-motivated and
geometrically-motivated functions. The similarity
functions examined in this study will be described
in the following.
The Kullback-Leibler (KL) divergence D(q||r) is
a classical measure of ‘distance’2 between two prob-
ability distributions, and is defined as: D(q||r) =
Ey q(y) log q(y)
r(y). It is a non-negative, additive,
asymmetric measure, and 0 iff the two distributions
are identical. However, the KL-divergence is unde-
fined if there exists an event y such that q(y) &gt; 0
but r(y) = 0, which is a property that “makes it
unsuitable for distributions derived via maximum-
likelihood estimates” (Lee, 2001).
</bodyText>
<footnote confidence="0.800367">
2It is not a proper distance metric since it is asymmetric.
</footnote>
<bodyText confidence="0.9996467">
One option to overcome this limitation is to apply
smoothing techniques to gather non-zero estimates
for all y. The alternative, examined in this paper, is
to consider an approximation to the KL divergence,
such as the Jensen-Shannon (JS) divergence (Lin,
1991) and the skew divergence (Lee, 2001).
The Jensen-Shannon divergence, which is sym-
metric, computes the KL-divergence between q, r,
and the average between the two. We use the JS
divergence as defined in Lee (2001): JS(q, r) =
</bodyText>
<equation confidence="0.9516785">
2[D(q||avg(q, r)) + D(r||avg(q, r))]. The asym-
1
</equation>
<bodyText confidence="0.970161888888889">
metric skew divergence sα, proposed by Lee (2001),
mixes one distribution with the other by a degree de-
fined by a E [0, 1): sα(q, r, a) = D(q||ar + (1 −
a)q). As a approaches 1, the skew divergence ap-
proximates the KL-divergence.
An alternative way to measure similarity is to
consider the distributions as vectors and apply
geometrically-motivated distance functions. This
family of similarity functions includes the cosine
</bodyText>
<equation confidence="0.87231925">
cos(q,r) = q(y) · r(y)/||q(y) |r(y)||, euclidean
�Eeuc(q, r) = y(q(y) − r(y))2 and variational
(also known as L1 or Manhattan) distance function,
defined as var(q, r) = E
</equation>
<subsectionHeader confidence="0.994433">
3.2 Human-annotated data
</subsectionHeader>
<bodyText confidence="0.999579904761905">
In contrast to the automatic measures devised in the
previous section, we might have access to human an-
notated data. That is, use label information such as
topic or genre to define the set of similar articles.
Genre For the Penn Treebank (PT) Wall Street
Journal (WSJ) section, more specifically, the subset
available in the Penn Discourse Treebank, there ex-
ists a partition of the data by genre (Webber, 2009).
Every article is assigned one of the following genre
labels: news, letters, highlights, essays, errata, wit
and short verse, quarterly progress reports, notable
and quotable. This classification has been made on
the basis of meta-data (Webber, 2009). It is well-
known that there is no meta-data directly associated
with the individual WSJ files in the Penn Treebank.
However, meta-data can be obtained by looking at
the articles in the ACL/DCI corpus (LDC99T42),
and a mapping file that aligns document numbers of
DCI (DOCNO) to WSJ keys (Webber, 2009). An
example document is given in Figure 1. The meta-
data field HL contains headlines, SO source info, and
</bodyText>
<equation confidence="0.660372">
y |q(y) − r(y)|.
</equation>
<page confidence="0.797963">
1568
</page>
<figure confidence="0.985546058823529">
the IN field includes topic markers.
&lt;DOC&gt;&lt;DOCNO&gt; 891102-0186. &lt;/DOCNO&gt;
&lt;WSJKEY&gt; wsj_0008 &lt;/WSJKEY&gt;
&lt;AN&gt; 891102-0186. &lt;/AN&gt;
&lt;HL&gt; U.S. Savings Bonds Sales
@ Suspended by Debt Limit &lt;/HL&gt;
&lt;DD&gt; 11/02/89 &lt;/DD&gt;
&lt;SO&gt; WALL STREET JOURNAL (J) &lt;/SO&gt;
&lt;IN&gt; FINANCIAL, ACCOUNTING, LEASING (FIN)
BOND MARKET NEWS (BON) &lt;/IN&gt;
&lt;GV&gt; TREASURY DEPARTMENT (TRE) &lt;/GV&gt;
&lt;DATELINE&gt; WASHINGTON &lt;/DATELINE&gt;
&lt;TXT&gt;
&lt;p&gt;&lt;s&gt;
The federal government suspended sales of U.S.
savings bonds because Congress hasn’t lifted
the ceiling on government debt.&lt;/s&gt;&lt;/p&gt; [...]
</figure>
<figureCaption confidence="0.999">
Figure 1: Example of ACL/DCI article. We have aug-
mented it with the WSJ filename (WSJKEY).
</figureCaption>
<bodyText confidence="0.9950263125">
Topic On the basis of the same meta-data, we
devised a classification of the Penn Treebank WSJ
by topic. That is, while the genre division has been
mostly made on the basis of headlines, we use the
information of the IN field. Every article is assigned
one, more than one or none of a predefined set of
keywords. While their origin remains unclear,3
these keywords seem to come from a controlled
vocabulary. There are 76 distinct topic markers.
The three most frequent keywords are: TENDER
OFFERS, MERGERS, ACQUISITIONS (TNM),
EARNINGS (ERN),STOCK MARKET, OFFERINGS
(STK). This reflects the fact that a lot of arti-
cles come from the financial domain. But the
corpus also contains articles from more distant do-
mains, like MARKETING, ADVERTISING (MKT),
</bodyText>
<sectionHeader confidence="0.91129325" genericHeader="method">
COMPUTERS AND INFORMATION TECHNOLOGY
(CPR),HEALTH CARE PROVIDERS, MEDICINE,
DENTISTRY (HEA), PETROLEUM (PET).
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.995981">
4.1 Tools &amp; Evaluation
</subsectionHeader>
<bodyText confidence="0.986806840909091">
The parsing system used in this study is the MST
parser (McDonald et al., 2005), a state-of-the-art
data-driven graph-based dependency parser. It is
3It is not known what IN stands for, as also stated in Mark
Liberman’s notes in the readme of the ACL/DCI corpus. How-
ever, a reviewer suggested that IN might stand for “index terms”
which seems plausible.
a system that can be trained on a variety of lan-
guages given training data in CoNLL format (Buch-
holz and Marsi, 2006). Additionally, the parser im-
plements both projective and non-projective pars-
ing algorithms. The projective algorithm is used for
the experiments on English, while the non-projective
variant is used for Dutch. We train the parser using
default settings. MST takes PoS-tagged data as in-
put; we use gold-standard tags in the experiments.
We estimate topic models using Latent Dirichlet
Allocation (Blei et al., 2003) implemented in the
MALLET4 toolkit. Like Lippincott et al. (2010),
we set the number of topics to 100, and otherwise
use standard settings (no further optimization). We
experimented with the removal of stopwords, but
found no deteriorating effect while keeping them.
Thus, all experiments are carried out on data where
stopwords were not removed.
We implemented the similarity measures pre-
sented in Section 3.1. For skew divergence, that re-
quires parameter α, we set α = .99 (close to KL
divergence) since that has shown previously to work
best (Lee, 2001). Additionally, we evaluate the ap-
proach on English PoS tagging using two different
taggers: MXPOST, the MaxEnt tagger of Ratna-
parkhi5 and Citar,6 a trigram HMM tagger.
In all experiments, parsing performance is mea-
sured as Labeled Attachment Score (LAS), the per-
centage of tokens with correct dependency edge and
label. To compute LAS, we use the CoNLL 2007
evaluation script7 with punctuation tokens excluded
from scoring (as was the default setting in CoNLL
2006). PoS tagging accuracy is measured as the per-
centage of correctly labeled words out of all words.
Statistical significance is determined by Approxi-
mate Randomization Test (Noreen, 1989; Yeh, 2000)
with 10,000 iterations.
</bodyText>
<subsectionHeader confidence="0.983687">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.999589">
English - WSJ For English, we use the portion of
the Penn Treebank Wall Street Journal (WSJ) that
has been made available in the CoNLL 2008 shared
</bodyText>
<footnote confidence="0.9950994">
4http://mallet.cs.umass.edu/
5ftp://ftp.cis.upenn.edu/pub/adwait/jmx/
6Citar has been implemented by Dani¨el de Kok and is avail-
able at: https://github.com/danieldk/citar
7http://nextens.uvt.nl/depparse-wiki/
</footnote>
<page confidence="0.995781">
1569
</page>
<bodyText confidence="0.999894833333333">
task. This data has been automatically converted8
into dependency structure, and contains three files:
the training set (sections 02-21), development set
(section 24) and test set (section 23).
Since we use articles as basic units, we actually
split the data to get back original article boundaries.9
This led to a total of 2,034 articles (1 million words).
Further statistics on the datasets are given in Ta-
ble 1. In the first set of experiments on WSJ subdo-
mains, we consider articles from section 23 and 24
that contain at least 50 sentences as test sets (target
domains). This amounted to 22 test articles.
</bodyText>
<table confidence="0.96269375">
EN: WSJ WSJ+G+B Dutch
articles 2,034 3,776 51,454
sentences 43,117 77,422 1,663,032
words 1,051,997 1,784,543 20,953,850
</table>
<tableCaption confidence="0.999748">
Table 1: Overview of the datasets for English and Dutch.
</tableCaption>
<bodyText confidence="0.999728857142857">
To test whether we have a reasonable system,
we performed a sanity check and trained the MST
parser on the training section (02-21). The result
on the standard test set (section 23) is identical to
previously reported results (excluding punctuation
tokens: LAS 87.50, Unlabeled Attachment Score
(UAS) 90.75; with punctuation tokens: LAS 87.07,
UAS 89.95). The latter has been reported in (Sur-
deanu and Manning, 2010).
English - Genia (G) &amp; Brown (B) For the Do-
main Adaptation experiments, we added 1,552 ar-
ticles from the GENIA10 treebank (biomedical ab-
stracts from Medline) and 190 files from the Brown
corpus to the pool of data. We converted the data
to CoNLL format with the LTH converter (Johans-
son and Nugues, 2007). The size of the test files is,
respectively: Genia 1,360 sentences with an aver-
age number of 26.20 words per sentence; the Brown
test set is the same as used in the CoNLL 2008
shared task and contains 426 sentences with a mean
of 16.80 words.
</bodyText>
<footnote confidence="0.980375142857143">
8Using the LTH converter: http://nlp.cs.lth.se/
software/treebank_converter/
9This was a non-trivial task, as we actually noticed that some
sentences have been omitted from the CoNLL 2008 shared task.
10We use the GENIA distribution in Penn Treebank for-
mat available at http://bllip.cs.brown.edu/download/
genia1.0-division-rel1.tar.gz
</footnote>
<sectionHeader confidence="0.932049" genericHeader="method">
5 Experiments on English
</sectionHeader>
<subsectionHeader confidence="0.969389">
5.1 Experiments within the WSJ
</subsectionHeader>
<bodyText confidence="0.999910025">
In the first set of experiments, we focus on the WSJ
and evaluate the similarity functions to gather re-
lated data for a given test article. We have 22 WSJ
articles as test set, sampled from sections 23 and
24. Regarding feature representations, we examined
three possibilities: relative frequencies of words, rel-
ative frequencies of character tetragrams (both un-
smoothed) and document topic distributions.
In the following, we only discuss representations
based on words or topic models as we found charac-
ter tetragrams less stable; they performed sometimes
like their word-based counterparts but other times,
considerably worse.
Results of Similarity Measures Table 2 com-
pares the effect of the different ways to select re-
lated data in comparison to the random baseline for
increasing amounts of training data. The table gives
the average over 22 test articles (rather than show-
ing individual tables for the 22 articles). We select
articles up to various thresholds that specify the to-
tal number of sentences selected in each round (e.g.
0.3k, 1.2k, etc.).11 In more detail, Table 2 shows the
result of applying various similarity functions (intro-
duced in Section 3.1) over the two different feature
representations (w: words; tm: topic model) for in-
creasing amounts of data. We additionally provide
results of using the Renyi divergence.12
Clearly, as more and more data is selected, the
differences become smaller, because we are close
to the data limit. However, for all data points less
than 38k (97%), selection by jensen-shannon, varia-
tional and cosine similarity outperform random data
selection significantly for both types of feature rep-
resentations (words and topic model). For selection
by topic models, this additionally holds for the eu-
clidean measure.
From the various measures we can see that se-
lection by jensen-shannon divergence and varia-
tional distance perform best, followed by cosine
similarity, skew divergence, euclidean and renyi.
</bodyText>
<footnote confidence="0.9197925">
11Rather than choosing k articles, as article length may differ.
12The Renyi divergence (R´enyi, 1961), also used by Van
Asch and Daelemans (2010), is defined as D.(q, r) = 1/(α −
1) log(E q&apos;rl «).
</footnote>
<page confidence="0.752017">
1570
</page>
<table confidence="0.999692">
1% 3% 25% 49% 97%
(0.3k) (1.2k) (9.6k) (19.2k) (38k)
random 70.61 77.21 82.98 84.48 85.51
w-js 74.07* 79.41* 83.98* 84.94* 85.68
w-var 74.07* 79.60* 83.82* 84.94* 85.45
w-skw 74.20* 78.95* 83.68* 84.60 85.55
w-cos 73.77* 79.30* 83.87* 84.96* 85.59
w-euc 73.85* 78.90* 83.52* 84.68 85.57
w-ryi 73.41* 78.31 83.76* 84.46 85.46
tm-js 74.23* 79.49* 84.04* 85.01* 85.45
tm-var 74.29* 79.59* 83.93* 84.94* 85.43
tm-skw 74.13* 79.42* 84.13* 84.82 85.73
tm-cos 74.04* 79.27* 84.14* 84.99* 85.42
tm-euc 74.27* 79.53* 83.93* 85.15* 85.62
tm-ryi 71.26 78.64* 83.79* 84.85 85.58
</table>
<bodyText confidence="0.999263875">
gence on one side, and cosine and euclidean on
the other side, i.e. between probabilistically- and
geometrically-motivated functions. Variational has
a higher overlap with the probabilistic functions. In-
terestingly, the ‘peaks’ in Table 4 (underlined, i.e.
the highest pair-wise overlaps) are the same for the
different feature representations.
In the following we analyze selection by topic
model and words, as they are relatively different
from each other, despite similar performance. For
the word-based model, we use jensen-shannon as
similarity function, as it turned out to be the best
measure. For topic model, we use the simpler vari-
ational metric. However, very similar results were
achieved using jensen-shannon. Cosine and eu-
clidean did not perform as well.
</bodyText>
<tableCaption confidence="0.964907">
Table 2: Comparison of similarity measures based
on words (w) and topic model (tm): parsing accu-
racy for increasing amounts of training data as average
</tableCaption>
<table confidence="0.964674833333333">
over 22 WSJ articles (js=jensen-shannon; cos=cosine;
skw=skew; var=variational; euc=euclidean; ryi=renyi).
Best score (per representation) underlined, best overall
score bold; * indicates significantly better (p &lt; 0.05)
than random.
ran w-js w-var w-skw w-cos w-euc
– 10.3 10.4 10.0 10.4 10.2
12.1 41.6 39.6 36.0 29.3 28.6
12.3 40.8 39.3 34.9 29.3 28.5
11.8 40.9 39.7 36.8 30.0 30.1
14.0 31.7 30.7 27.3 24.1 23.2
14.6 27.5 27.2 23.4 22.6 22.1
</table>
<bodyText confidence="0.879178655172414">
ran
tm-js
tm-var
tm-skw
tm-cos
tm-euc
Renyi divergence does not perform as well as other
probabilistically-motivated functions. Regarding
feature representations, the representation based on
topic models works slightly better than the respec-
tive word-based measure (cf. Table 2) and often
achieves the overall best score (boldface).
Overall, the differences in accuracy between the
various similarity measures are small; but interest-
ingly, the overlap between them is not that large.
Table 3 and Table 4 show the overlap (in terms of
proportion of identically selected articles) between
pairs of similarity measures. As shown in Table 3,
for all measures there is only a small overlap with
the random baseline (around 10%-14%). Despite
similar performance, topic model selection has inter-
estingly no substantial overlap with any other word-
based similarity measures: their overlap is at most
41.6%. Moreover, Table 4 compares the overlap of
the various similarity functions within a certain fea-
ture representation (here x stands for either topic
model – left value – or words – right value). The
table shows that there is quite some overlap be-
tween jensen-shannon, variational and skew diver-
</bodyText>
<tableCaption confidence="0.968187">
Table 3: Average overlap (in %) of similarity measure:
random selection (ran) vs. measures based on words (w)
and topic model (tm).
</tableCaption>
<table confidence="0.9823866">
x=tm/w x-js x-var x-skw x-cos x-euc
tm/w-var 76/74 – 60/63 55/48 49/47
tm/w-skw 69/72 60/63 – 48/41 42/42
tm/w-cos 57/42 55/48 48/41 – 62/71
tm/w-euc 47/41 49/47 42/42 62/71 –
</table>
<tableCaption confidence="0.995074666666667">
Table 4: Average overlap (in %) for different feature
representations x as tm/w, where tm=topic model and
w=words. Highest pair-wise overlap is underlined.
</tableCaption>
<bodyText confidence="0.991946222222222">
Automatic Measure vs. Human labels The next
question is how these automatic measures compare
to human-annotated data. We compare word-based
and topic model selection (by using jensen-shannon
and variational, respectively) to selection based on
human-given labels: genre and topic. For genre, we
randomly select larger amounts of training data for
a given test article from the same genre. For topic,
the approach is similar, but as an article might have
</bodyText>
<page confidence="0.98141">
1571
</page>
<bodyText confidence="0.982152">
several topic markers (keywords in the IN field), we
rank articles by proportion of overlapping keywords.
</bodyText>
<figure confidence="0.966478">
Average
0 5000 10000 15000 20000
number of sentences
</figure>
<figureCaption confidence="0.99898325">
Figure 2: Comparison of automatic measures (words us-
ing jensen-shannon and topic model using variational)
with human-annotated labels (genre/topic). Automatic
measures outperform human labels (p &lt; 0.05).
</figureCaption>
<bodyText confidence="0.981757461538462">
Figure 2 shows that human-labels do actually not
perform better than the automatic measures. Both
are close to random selection. Moreover, the line
of selection by topic marker (IN fields) stops early
– we believe the reason for this is that the IN fields
are too fine-grained, which limits the number of ar-
ticles that are considered relevant for a given test
article. However, manually aggregating articles on
similar topics did not improve topic-based selection
either. We conclude that the automatic selection
techniques perform significantly better than human-
annotated data, at least within the WSJ domain con-
sidered here.
</bodyText>
<subsectionHeader confidence="0.996843">
5.2 Domain Adaptation Results
</subsectionHeader>
<bodyText confidence="0.997420625">
Until now, we compared similarity measures by re-
stricting ourselves to articles from the WSJ. In this
section, we extend the experiments to the domain
adaptation scenario. We augment the pool of WSJ
articles with articles coming from two other corpora:
Genia and Brown. We want to gauge the effective-
ness of the domain similarity measures in the multi-
domain setting, where articles are selected from the
pool of data without knowing their identity (which
corpus the articles came from).
The test sets are the standard evaluation sets from
the three corpora: the standard WSJ (section 23)
and Brown test set from CoNLL 2008 (they contain
2,399 and 426 sentences, respectively) and the Ge-
nia test set (1,370 sentences). As a reference, we
give results of models trained on the respective cor-
pora (per-corpus models; i.e. if we consider corpora
boundaries and train a model on the respective do-
main – this model is ‘supervised’ in the sense that it
knows from which corpus the test article came from)
as well as a baseline model trained on all data, i.e.
the union of all three corpora (wsj+genia+brown),
which is a standard baseline in domain adapta-
tion (Daum´e III, 2007; McClosky et al., 2010).
</bodyText>
<table confidence="0.998299857142857">
WSJ Brown Genia
(38k) (28k) (19k)
random 86.58 73.81 83.77
per-corpus 87.50 81.55 86.63
union 87.05 79.12 81.57
topic model (var) 87.11* 81.760 86.770
words (js) 86.30 81.470 86.440
</table>
<tableCaption confidence="0.9855605">
Table 5: Domain Adaptation Results on English (signifi-
cantly better: * than random; 0 than random and union).
</tableCaption>
<bodyText confidence="0.982398208333333">
The learning curves are shown in Figure 3, the
scores for a specific amount of data are given in
Table 5. The performance of the reference mod-
els (per-corpus and union in Table 5) are indicated
in Figure 3 with horizontal lines: the dashed line
represents the per-corpus performance (‘supervised’
model); the solid line shows the performance of the
union baseline trained on all available data (77k sen-
tences). For the former, the vertical dashed lines in-
dicate the amount of data the model was trained on
(e.g. 23k sentences for Brown).
Simply taking all available data has a deteriorat-
ing effect: on all three test sets, the performance of
the union model is below the presumably best per-
formance of a model trained on the respective corpus
(per-corpus model).
The empirical results show that automatic data se-
lection by topic model outperforms random selec-
tion on all three test sets and the union baseline in
two out of three cases. More specifically, selection
by topic model outperforms random selection sig-
nificantly on all three test sets and all points in the
graph (p &lt; 0.001). Selection by the word-based
measure (words-js) achieves a significant improve-
</bodyText>
<figure confidence="0.991770565217392">
Accuracy
76 78 80 82 84 86
●
●
●
●
● random
words−js
topic model−var
genre
topic (IN fields)
●
1572
wsj23all brown genia
Accuracy
80 82 84 86 88
●
●
●
●
●
Accuracy
70 75 80
●
●
●
● ●
●
Accuracy
76 78 80 82 84 86 88
● random
● words−js
topic model−var
per−corpus model
union (wsj+genia+brown)
●
●
●
●
●
0 10000 20000 30000 40000
number of sentences
0 10000 20000 30000 40000
number of sentences
0 10000 20000 30000 40000
number of sentences
</figure>
<figureCaption confidence="0.9989445">
Figure 3: Domain Adaptation Results for English Parsing with Increasing Amounts of Training Data. The vertical line
represents the amount of data the per-corpus model is trained on.
</figureCaption>
<bodyText confidence="0.944936605263158">
0.90 0.92 0.94 0.96 0.98
Accuracy
ment over the random baseline on two out of the
three test sets – it falls below the random baseline on
the WSJ test set. Thus, selection by topic model per-
forms best – it achieves better performance than the
union baseline with comparatively little data (Genia:
4k; Brown: 19k – in comparison: union has 77k).
Moreover, it comes very close to the supervised per-
corpus model performance13 with a similar amount
of data (cf. vertical dashed line). This is a very good
result, given that the technique disregards the origin
of the articles and just uses plain words as informa-
tion. It automatically finds data that is beneficial for
an unknown target domain.
So far we examined domain similarity measures
for parsing, and concluded that selection by topic
model performs best, closely followed by word-
based selection using the jensen-shannon diver-
gence. The question that remains is whether the
measure is more widely applicable: How does it per-
form on another language and task?
PoS tagging We perform similar Domain Adap-
tation experiments on WSJ, Genia and Brown for
PoS tagging. We use two taggers (HMM and Max-
Ent) and the same three test articles as before. The
results are shown in Figure 4 (it depicts the aver-
age over the three test sets, WSJ, Genia, Brown, for
space reasons). The left figure shows the perfor-
mance of the HMM tagger; on the right is the Max-
Ent tagger. The graphs show that automatic train-
ing data selection outperforms random data selec-
13On Genia and Brown (cf. Table 5) there is no significant
difference between topic model and per-corpus model.
tion, and again topic model selection performs best,
closely followed by words-js. This confirms previ-
ous findings and shows that the domain similarity
measures are effective also for this task.
</bodyText>
<figure confidence="0.942872538461539">
Average HMM tagger Average MXPOST tagger
●
●
●
●
●
●
●
● random
words−js
topic model−var
0 10000 20000 30000 40000 0 10000 20000 30000 40000
number of sentences number of sentences
</figure>
<figureCaption confidence="0.9997">
Figure 4: PoS tagging results, average over 3 test sets.
</figureCaption>
<sectionHeader confidence="0.975169" genericHeader="method">
6 Experiments on Dutch
</sectionHeader>
<bodyText confidence="0.998065785714286">
For Dutch, we evaluate the approach on a bigger and
more varied dataset. It contains in total over 50k ar-
ticles and 20 million words (cf. Table 1). In con-
trast to the English data, only a small portion of the
dataset is manually annotated: 281 articles.14
Since we want to evaluate the performance of
different similarity measures, we want to keep the
influence of noise as low as possible. Therefore,
we annotated the remaining articles with a parsing
system that is more accurate (Plank and van No-
ord, 2010), the Alpino parser (van Noord, 2006).
Note that using a more accurate parsing system to
train another parser has recently also been proposed
by Petrov et al. (2010) as uptraining. Alpino is a
</bodyText>
<figure confidence="0.956744583333333">
14http://www.let.rug.nl/vannoord/Lassy/
●
●
●
●
●
Accuracy
0.90 0.92 0.94 0.96 0.98
●
● random
words−js
topic model−var
</figure>
<page confidence="0.973183">
1573
</page>
<bodyText confidence="0.9995892">
parser tailored to Dutch, that has been developed
over the last ten years, and reaches an accuracy level
of 90% on general newspaper text. It uses a condi-
tional MaxEnt model as parse selection component.
Details of the parser are given in (van Noord, 2006).
</bodyText>
<figure confidence="0.950804333333333">
Average
0 5000 10000 15000 20000 25000 30000
number of sentences
</figure>
<figureCaption confidence="0.999932">
Figure 5: Result on Dutch; average over 30 articles.
</figureCaption>
<bodyText confidence="0.997874947368421">
Data and Results The Dutch dataset contains
articles from a variety of sources: Wikipedia15,
EMEA16 (documents from the European Medicines
Agency) and the Dutch parallel corpus17 (DPC), that
covers a variety of subdomains. The Dutch arti-
cles were parsed with Alpino and automatically con-
verted to CoNLL format with the treebank conver-
sion software from CoNLL 2006, where PoS tags
have been replaced with more fine-grained Alpino
tags as that had a positive effect on MST. The 281
annotated articles come from all three sources. As
with English, we consider as test set articles with
at least 50 sentences, from which 30 are randomly
sampled.
The results on Dutch are shown in Figure 5. Do-
main similarity measures clearly outperform random
data selection also in this setting with another lan-
guage and a considerably larger pool of data (20 mil-
lion words; 51k articles).
</bodyText>
<sectionHeader confidence="0.999659" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.993012666666667">
In this paper we have shown the effectiveness of a
simple technique that considers only plain words as
domain selection measure for two tasks, dependency
</bodyText>
<footnote confidence="0.999343666666667">
15http://ilps.science.uva.nl/WikiXML/
16http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php
17http://www.kuleuven-kortrijk.be/DPC
</footnote>
<bodyText confidence="0.999894238095238">
parsing and PoS tagging. Interestingly, human-
annotated labels did not perform better than the au-
tomatic measures. The best technique is based on
topic models, and compares document topic distri-
butions estimated by LDA (Blei et al., 2003) using
the variational metric (very similar results were ob-
tained using jensen-shannon). Topic model selec-
tion significantly outperforms random data selection
on both examined languages, English and Dutch,
and has a positive effect on PoS tagging. More-
over, it outperformed a standard Domain Adapta-
tion baseline (union) on two out of three test sets.
Topic model is closely followed by the word-based
measure using jensen-shannon divergence. By ex-
amining the overlap between word-based and topic
model-based techniques, we found that despite sim-
ilar performance their overlap is rather small. Given
these results and the fact that no optimization has
been done for the topic model itself, results are en-
couraging: there might be an even better measure
that exploits the information from both techniques.
So far, we tested a simple combination of the two by
selecting half of the articles by a measure based on
words and the other half by a measure based on topic
models (by testing different metrics). However, this
simple combination technique did not improve re-
sults yet – topic model alone still performed best.
Overall, plain surface characteristics seem to
carry important information of what kind of data is
relevant for a given domain. Undoubtedly, parsing
accuracy will be influenced by more factors than lex-
ical information. Nevertheless, as we have seen, lex-
ical differences constitute an important factor.
Applying divergence measures over syntactic pat-
terns, adding additional articles to the pool of
data (by uptraining (Petrov et al., 2010), selftrain-
ing (McClosky et al., 2006) or active learning (Hwa,
2004)), gauging the effect of weighting instances
according to their similarity to the test data (Jiang
and Zhai, 2007; Plank and Sima’an, 2008), as well
as analyzing differences between gathered data are
venues for further research.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999649">
The authors would like to thank Bonnie Webber and
the three anonymous reviewers for their valuable
comments on earlier drafts of this paper.
</bodyText>
<figure confidence="0.997356636363636">
●
●
●
●
●
● ● random
topic model−var
words−js
●
74 76 78 80 82 84 86
Accuracy
</figure>
<page confidence="0.986674">
1574
</page>
<sectionHeader confidence="0.988982" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999815866666667">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Correspon-
dence Learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Sydney, Australia.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL-X), pages 149–
164, New York City.
Hal Daum´e III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Meeting of the Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing,
Pittsburgh, PA.
Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Adapting a Probabilistic Disambiguation Model
of an HPSG Parser to a New Domain. In Robert Dale,
Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors,
Natural Language Processing IJCNLP 2005, volume
3651 of Lecture Notes in Computer Science, pages
199–210. Springer Berlin / Heidelberg.
Rebecca Hwa. 2004. Sample Selection for Statistical
Parsing. Compututational Linguistics, 30:253–276,
September.
Jing Jiang and ChengXiang Zhai. 2007. Instance
Weighting for Domain Adaptation in NLP. In Pro-
ceedings of the 45th Meeting of the Association for
Computational Linguistics, pages 264–271, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Richard Johansson and Pierre Nugues. 2007. Extended
Constituent-to-dependency Conversion for English. In
Proceedings of NODALIDA, Tartu, Estonia.
Lillian Lee. 2001. On the Effectiveness of the Skew Di-
vergence for Statistical Language Analysis. In In Ar-
tificial Intelligence and Statistics 2001, pages 65–72,
Key West, Florida.
J. Lin. 1991. Divergence measures based on the Shannon
entropy. Information Theory, IEEE Transactions on,
37(1):145 –151, January.
Tom Lippincott, Diarmuid O´ S´eaghdha, Lin Sun, and
Anna Korhonen. 2010. Exploring variation across
biomedical subdomains. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 689–697, Beijing, China, August.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge Mass.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective Self-Training for Parsing. In Pro-
ceedings of Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 152–159, Brooklyn,
New York. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic Domain Adaptation for Parsing. In
Proceedings of Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 28–36, Los An-
geles, California, June. Association for Computational
Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective Dependency Parsing
using Spanning Tree Algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523–530, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220–224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for Accurate Deter-
ministic Question Parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705–713, Cambridge, MA,
October. Association for Computational Linguistics.
Barbara Plank and Khalil Sima’an. 2008. Subdomain
Sensitive Statistical Parsing using Raw Corpora. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation, Marrakech, Mo-
rocco, May.
Barbara Plank and Gertjan van Noord. 2010. Grammar-
Driven versus Data-Driven: Which Parsing System Is
More Affected by Domain Shifts? In Proceedings of
the 2010 Workshop on NLP and Linguistics: Finding
the Common Ground, pages 25–33, Uppsala, Sweden,
July. Association for Computational Linguistics.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic Prediction of Parser Accuracy. In EMNLP
’08: Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 887–
</reference>
<page confidence="0.785783">
1575
</page>
<reference confidence="0.9999212">
896, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
A. R´enyi. 1961. On measures of information and en-
tropy. In Proceedings of the 4th Berkeley Sympo-
sium on Mathematics, Statistics and Probability, pages
547–561, Berkeley.
Satoshi Sekine. 1997. The Domain Dependence of Pars-
ing. In In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 96–102,
Washington D.C.
Mark Steyvers and Tom Griffiths, 2007. Probabilistic
Topic Models. Lawrence Erlbaum Associates.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble Models for Dependency Parsing: Cheap and
Good? In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
649–652, Los Angeles, California, June. Association
for Computational Linguistics.
Vincent Van Asch and Walter Daelemans. 2010. Us-
ing Domain Similarity for Performance Estimation. In
Proceedings of the 2010 Workshop on Domain Adap-
tation for Natural Language Processing, pages 31–36,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Gertjan van Noord. 2006. At Last Parsing Is Now Oper-
ational. In TALN 2006 Verbum Ex Machina, Actes De
La 13e Conference sur Le TraitementAutomatique des
Langues naturelles, pages 20–42, Leuven.
Bonnie Webber. 2009. Genre distinctions for Discourse
in the Penn TreeBank. In Proceedings of the 47th
Meeting of the Association for Computational Linguis-
tics, pages 674–682, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Zhili Wu, Katja Markert, and Serge Sharoff. 2010. Fine-
Grained Genre Classification Using Structural Learn-
ing Algorithms. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 749–759, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947–953, Morristown, NJ, USA. Association for
Computational Linguistics.
</reference>
<page confidence="0.992541">
1576
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.663700">
<title confidence="0.99995">Effective Measures of Domain Similarity for Parsing</title>
<author confidence="0.999657">Barbara Plank Gertjan van_Noord</author>
<affiliation confidence="0.999794">University of Groningen University of Groningen</affiliation>
<address confidence="0.966107">The Netherlands The Netherlands</address>
<email confidence="0.974189">b.plank@rug.nlG.J.M.van.Noord@rug.nl</email>
<abstract confidence="0.986910608695652">It is well known that parsing accuracy suffers when a model is applied to out-of-domain data. It is also known that the most beneficial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001). Hence, an important task is to select appropriate domains. However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. As more and more data becomes available, automatic ways to select data that is beneficial for a new (unknown) target domain are becoming attractive. This paper evaluates various ways to automatically acquire related training data for a given test set. The results show that an unsupervised technique based on topic models is effective – it outperforms random data selection on both languages examined, English and Dutch. Moreover, the technique works better than manually assigned labels gathered from meta-data that is available for English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="9170" citStr="Blei et al., 2003" startWordPosition="1497" endWordPosition="1500">ed text). This has the advantage that we do not need to rely on additional supervised tools; moreover, it is interesting to know how far we can get with this level of information only. We examine the following feature representations: relative frequencies of words, relative frequencies of character tetragrams, and topic models. Our motivation was as follows. Relative frequencies of words are a simple and effective representation used e.g. in text classification (Manning and Sch¨utze, 1999), while character n-grams have proven successful in genre classification (Wu et al., 2010). Topic models (Blei et al., 2003; Steyvers and Griffiths, 2007) can be considered an advanced model over word distributions: every article is represented by a topic distribution, which in turn is a distribution over words. Similarity between documents can be measured by comparing topic distributions. Similarity Functions There are many possible similarity (or distance) functions. They fall broadly into two categories: probabilistically-motivated and geometrically-motivated functions. The similarity functions examined in this study will be described in the following. The Kullback-Leibler (KL) divergence D(q||r) is a classical</context>
<context position="14923" citStr="Blei et al., 2003" startWordPosition="2412" endWordPosition="2415">ver, a reviewer suggested that IN might stand for “index terms” which seems plausible. a system that can be trained on a variety of languages given training data in CoNLL format (Buchholz and Marsi, 2006). Additionally, the parser implements both projective and non-projective parsing algorithms. The projective algorithm is used for the experiments on English, while the non-projective variant is used for Dutch. We train the parser using default settings. MST takes PoS-tagged data as input; we use gold-standard tags in the experiments. We estimate topic models using Latent Dirichlet Allocation (Blei et al., 2003) implemented in the MALLET4 toolkit. Like Lippincott et al. (2010), we set the number of topics to 100, and otherwise use standard settings (no further optimization). We experimented with the removal of stopwords, but found no deteriorating effect while keeping them. Thus, all experiments are carried out on data where stopwords were not removed. We implemented the similarity measures presented in Section 3.1. For skew divergence, that requires parameter α, we set α = .99 (close to KL divergence) since that has shown previously to work best (Lee, 2001). Additionally, we evaluate the approach on</context>
<context position="33842" citStr="Blei et al., 2003" startWordPosition="5473" endWordPosition="5476"> another language and a considerably larger pool of data (20 million words; 51k articles). 7 Discussion In this paper we have shown the effectiveness of a simple technique that considers only plain words as domain selection measure for two tasks, dependency 15http://ilps.science.uva.nl/WikiXML/ 16http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php 17http://www.kuleuven-kortrijk.be/DPC parsing and PoS tagging. Interestingly, humanannotated labels did not perform better than the automatic measures. The best technique is based on topic models, and compares document topic distributions estimated by LDA (Blei et al., 2003) using the variational metric (very similar results were obtained using jensen-shannon). Topic model selection significantly outperforms random data selection on both examined languages, English and Dutch, and has a positive effect on PoS tagging. Moreover, it outperformed a standard Domain Adaptation baseline (union) on two out of three test sets. Topic model is closely followed by the word-based measure using jensen-shannon divergence. By examining the overlap between word-based and topic model-based techniques, we found that despite similar performance their overlap is rather small. Given t</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain Adaptation with Structural Correspondence Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2454" citStr="Blitzer et al. (2006)" startWordPosition="406" endWordPosition="409">rises is how to measure domain similarity, i.e. whether we can find a simple yet effective method to determine which model or data is most beneficial for an arbitrary piece of new text. Moreover, if we had such a measure, a related question is whether it can tell us something more about what is actually meant by “domain”. So far, it was mostly arbitrarily used to refer to some kind of coherent unit (related to topic, style or genre), e.g.: newspaper text, biomedical abstracts, questions, fiction. Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus might not always be as homogeneous as originally t</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain Adaptation with Structural Correspondence Learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>149--164</pages>
<location>New York City.</location>
<contexts>
<context position="14509" citStr="Buchholz and Marsi, 2006" startWordPosition="2348" endWordPosition="2352">COMPUTERS AND INFORMATION TECHNOLOGY (CPR),HEALTH CARE PROVIDERS, MEDICINE, DENTISTRY (HEA), PETROLEUM (PET). 4 Experimental Setup 4.1 Tools &amp; Evaluation The parsing system used in this study is the MST parser (McDonald et al., 2005), a state-of-the-art data-driven graph-based dependency parser. It is 3It is not known what IN stands for, as also stated in Mark Liberman’s notes in the readme of the ACL/DCI corpus. However, a reviewer suggested that IN might stand for “index terms” which seems plausible. a system that can be trained on a variety of languages given training data in CoNLL format (Buchholz and Marsi, 2006). Additionally, the parser implements both projective and non-projective parsing algorithms. The projective algorithm is used for the experiments on English, while the non-projective variant is used for Dutch. We train the parser using default settings. MST takes PoS-tagged data as input; we use gold-standard tags in the experiments. We estimate topic models using Latent Dirichlet Allocation (Blei et al., 2003) implemented in the MALLET4 toolkit. Like Lippincott et al. (2010), we set the number of topics to 100, and otherwise use standard settings (no further optimization). We experimented wit</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 149– 164, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly Easy Domain Adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Meeting of the Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus Variation and Parser Performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Pittsburgh, PA.</location>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus Variation and Parser Performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadayoshi Hara</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Adapting a Probabilistic Disambiguation Model of an HPSG Parser to a New Domain.</title>
<date>2005</date>
<booktitle>Natural Language Processing IJCNLP 2005,</booktitle>
<volume>3651</volume>
<pages>199--210</pages>
<editor>In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="2407" citStr="Hara et al. (2005)" startWordPosition="398" endWordPosition="401">domain. Therefore, an important issue that arises is how to measure domain similarity, i.e. whether we can find a simple yet effective method to determine which model or data is most beneficial for an arbitrary piece of new text. Moreover, if we had such a measure, a related question is whether it can tell us something more about what is actually meant by “domain”. So far, it was mostly arbitrarily used to refer to some kind of coherent unit (related to topic, style or genre), e.g.: newspaper text, biomedical abstracts, questions, fiction. Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus mig</context>
</contexts>
<marker>Hara, Miyao, Tsujii, 2005</marker>
<rawString>Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Adapting a Probabilistic Disambiguation Model of an HPSG Parser to a New Domain. In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors, Natural Language Processing IJCNLP 2005, volume 3651 of Lecture Notes in Computer Science, pages 199–210. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample Selection for Statistical Parsing.</title>
<date>2004</date>
<journal>Compututational Linguistics,</journal>
<pages>30--253</pages>
<marker>Hwa, 2004</marker>
<rawString>Rebecca Hwa. 2004. Sample Selection for Statistical Parsing. Compututational Linguistics, 30:253–276, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance Weighting for Domain Adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>264--271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance Weighting for Domain Adaptation in NLP. In Proceedings of the 45th Meeting of the Association for Computational Linguistics, pages 264–271, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended Constituent-to-dependency Conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA,</booktitle>
<location>Tartu, Estonia.</location>
<contexts>
<context position="18047" citStr="Johansson and Nugues, 2007" startWordPosition="2907" endWordPosition="2911">arser on the training section (02-21). The result on the standard test set (section 23) is identical to previously reported results (excluding punctuation tokens: LAS 87.50, Unlabeled Attachment Score (UAS) 90.75; with punctuation tokens: LAS 87.07, UAS 89.95). The latter has been reported in (Surdeanu and Manning, 2010). English - Genia (G) &amp; Brown (B) For the Domain Adaptation experiments, we added 1,552 articles from the GENIA10 treebank (biomedical abstracts from Medline) and 190 files from the Brown corpus to the pool of data. We converted the data to CoNLL format with the LTH converter (Johansson and Nugues, 2007). The size of the test files is, respectively: Genia 1,360 sentences with an average number of 26.20 words per sentence; the Brown test set is the same as used in the CoNLL 2008 shared task and contains 426 sentences with a mean of 16.80 words. 8Using the LTH converter: http://nlp.cs.lth.se/ software/treebank_converter/ 9This was a non-trivial task, as we actually noticed that some sentences have been omitted from the CoNLL 2008 shared task. 10We use the GENIA distribution in Penn Treebank format available at http://bllip.cs.brown.edu/download/ genia1.0-division-rel1.tar.gz 5 Experiments on En</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended Constituent-to-dependency Conversion for English. In Proceedings of NODALIDA, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>On the Effectiveness of the Skew Divergence for Statistical Language Analysis. In</title>
<date>2001</date>
<booktitle>In Artificial Intelligence and Statistics</booktitle>
<pages>65--72</pages>
<location>Key West, Florida.</location>
<contexts>
<context position="10200" citStr="Lee, 2001" startWordPosition="1653" endWordPosition="1654">d geometrically-motivated functions. The similarity functions examined in this study will be described in the following. The Kullback-Leibler (KL) divergence D(q||r) is a classical measure of ‘distance’2 between two probability distributions, and is defined as: D(q||r) = Ey q(y) log q(y) r(y). It is a non-negative, additive, asymmetric measure, and 0 iff the two distributions are identical. However, the KL-divergence is undefined if there exists an event y such that q(y) &gt; 0 but r(y) = 0, which is a property that “makes it unsuitable for distributions derived via maximumlikelihood estimates” (Lee, 2001). 2It is not a proper distance metric since it is asymmetric. One option to overcome this limitation is to apply smoothing techniques to gather non-zero estimates for all y. The alternative, examined in this paper, is to consider an approximation to the KL divergence, such as the Jensen-Shannon (JS) divergence (Lin, 1991) and the skew divergence (Lee, 2001). The Jensen-Shannon divergence, which is symmetric, computes the KL-divergence between q, r, and the average between the two. We use the JS divergence as defined in Lee (2001): JS(q, r) = 2[D(q||avg(q, r)) + D(r||avg(q, r))]. The asym1 metr</context>
<context position="15480" citStr="Lee, 2001" startWordPosition="2505" endWordPosition="2506">using Latent Dirichlet Allocation (Blei et al., 2003) implemented in the MALLET4 toolkit. Like Lippincott et al. (2010), we set the number of topics to 100, and otherwise use standard settings (no further optimization). We experimented with the removal of stopwords, but found no deteriorating effect while keeping them. Thus, all experiments are carried out on data where stopwords were not removed. We implemented the similarity measures presented in Section 3.1. For skew divergence, that requires parameter α, we set α = .99 (close to KL divergence) since that has shown previously to work best (Lee, 2001). Additionally, we evaluate the approach on English PoS tagging using two different taggers: MXPOST, the MaxEnt tagger of Ratnaparkhi5 and Citar,6 a trigram HMM tagger. In all experiments, parsing performance is measured as Labeled Attachment Score (LAS), the percentage of tokens with correct dependency edge and label. To compute LAS, we use the CoNLL 2007 evaluation script7 with punctuation tokens excluded from scoring (as was the default setting in CoNLL 2006). PoS tagging accuracy is measured as the percentage of correctly labeled words out of all words. Statistical significance is determin</context>
</contexts>
<marker>Lee, 2001</marker>
<rawString>Lillian Lee. 2001. On the Effectiveness of the Skew Divergence for Statistical Language Analysis. In In Artificial Intelligence and Statistics 2001, pages 65–72, Key West, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Divergence measures based on the Shannon entropy. Information Theory,</title>
<date>1991</date>
<journal>IEEE Transactions on,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>151</pages>
<contexts>
<context position="10523" citStr="Lin, 1991" startWordPosition="1705" endWordPosition="1706">ve, asymmetric measure, and 0 iff the two distributions are identical. However, the KL-divergence is undefined if there exists an event y such that q(y) &gt; 0 but r(y) = 0, which is a property that “makes it unsuitable for distributions derived via maximumlikelihood estimates” (Lee, 2001). 2It is not a proper distance metric since it is asymmetric. One option to overcome this limitation is to apply smoothing techniques to gather non-zero estimates for all y. The alternative, examined in this paper, is to consider an approximation to the KL divergence, such as the Jensen-Shannon (JS) divergence (Lin, 1991) and the skew divergence (Lee, 2001). The Jensen-Shannon divergence, which is symmetric, computes the KL-divergence between q, r, and the average between the two. We use the JS divergence as defined in Lee (2001): JS(q, r) = 2[D(q||avg(q, r)) + D(r||avg(q, r))]. The asym1 metric skew divergence sα, proposed by Lee (2001), mixes one distribution with the other by a degree defined by a E [0, 1): sα(q, r, a) = D(q||ar + (1 − a)q). As a approaches 1, the skew divergence approximates the KL-divergence. An alternative way to measure similarity is to consider the distributions as vectors and apply ge</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>J. Lin. 1991. Divergence measures based on the Shannon entropy. Information Theory, IEEE Transactions on, 37(1):145 –151, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Lippincott</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Exploring variation across biomedical subdomains.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>689--697</pages>
<location>Beijing, China,</location>
<marker>Lippincott, S´eaghdha, Sun, Korhonen, 2010</marker>
<rawString>Tom Lippincott, Diarmuid O´ S´eaghdha, Lin Sun, and Anna Korhonen. 2010. Exploring variation across biomedical subdomains. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 689–697, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge Mass.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective Self-Training for Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Brooklyn, New York.</location>
<contexts>
<context position="2431" citStr="McClosky et al. (2006)" startWordPosition="402" endWordPosition="405">n important issue that arises is how to measure domain similarity, i.e. whether we can find a simple yet effective method to determine which model or data is most beneficial for an arbitrary piece of new text. Moreover, if we had such a measure, a related question is whether it can tell us something more about what is actually meant by “domain”. So far, it was mostly arbitrarily used to refer to some kind of coherent unit (related to topic, style or genre), e.g.: newspaper text, biomedical abstracts, questions, fiction. Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus might not always be as homo</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective Self-Training for Parsing. In Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 152–159, Brooklyn, New York. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Automatic Domain Adaptation for Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="1637" citStr="McClosky et al., 2010" startWordPosition="262" endWordPosition="265">nglish and Dutch. Moreover, the technique works better than manually assigned labels gathered from meta-data that is available for English. 1 Introduction and Motivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given – which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine which data or model (in case we have several source domain models) will perform best on a new (unknown) target domain. Therefore, an important issue that arises is how to measure domain similarity, i.e. whether we can find a simple yet effective method to determine which model or data is most beneficial for an arbitrary piece of new text. Moreover, if we had such a measure, a related question is whether it can tell us something more about what is actually meant by “domain”. So far, it was mostly arbitrarily used to refer to some kind of coherent unit (r</context>
<context position="5271" citStr="McClosky et al. (2010)" startWordPosition="864" endWordPosition="867">ebank WSJ. Then, we extend the experiments to the domain adaptation scenario. Experiments were performed on two languages: English and Dutch. The empirical results show that a simple measure based on topic distributions is effective for both languages and works well also for Part-of-Speech tagging. As the approach is based on plain surfacelevel information (words) and it finds related data in a completely unsupervised fashion, it can be easily applied to other tasks or languages for which annotated (or automatically annotated) data is available. 2 Related Work The work most related to ours is McClosky et al. (2010). They try to find the best combination of source models to parse data from a new domain, which is related to Plank and Sima’an (2008). In the latter, unlabeled data was used to create several parsers by weighting trees in the WSJ according to their similarity to the subdomain. McClosky et al. (2010) coined the term multiple source domain adaptation. Inspired by work on parsing accuracy prediction (Ravi et al., 2008), they train a linear regression model to predict the best (linear interpolation) of source domain models. Similar to us, McClosky et al. (2010) regard a target domain as mixture o</context>
<context position="27100" citStr="McClosky et al., 2010" startWordPosition="4335" endWordPosition="4338">WSJ (section 23) and Brown test set from CoNLL 2008 (they contain 2,399 and 426 sentences, respectively) and the Genia test set (1,370 sentences). As a reference, we give results of models trained on the respective corpora (per-corpus models; i.e. if we consider corpora boundaries and train a model on the respective domain – this model is ‘supervised’ in the sense that it knows from which corpus the test article came from) as well as a baseline model trained on all data, i.e. the union of all three corpora (wsj+genia+brown), which is a standard baseline in domain adaptation (Daum´e III, 2007; McClosky et al., 2010). WSJ Brown Genia (38k) (28k) (19k) random 86.58 73.81 83.77 per-corpus 87.50 81.55 86.63 union 87.05 79.12 81.57 topic model (var) 87.11* 81.760 86.770 words (js) 86.30 81.470 86.440 Table 5: Domain Adaptation Results on English (significantly better: * than random; 0 than random and union). The learning curves are shown in Figure 3, the scores for a specific amount of data are given in Table 5. The performance of the reference models (per-corpus and union in Table 5) are indicated in Figure 3 with horizontal lines: the dashed line represents the per-corpus performance (‘supervised’ model); t</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic Domain Adaptation for Parsing. In Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 28–36, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective Dependency Parsing using Spanning Tree Algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 523–530, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent Selection of Language Model Training Data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="7394" citStr="Moore and Lewis, 2010" startWordPosition="1218" endWordPosition="1221">mance loss. Their goal is different, but related: rather than finding related data for a new domain, they want to estimate the loss in accuracy of a PoS tagger when applied to a new domain. We will briefly discuss results obtained with the Renyi divergence in Section 5.1. Lippincott et al. (2010) examine subdomain variation in biomedicine corpora and propose awareness of NLP tools to such variation. However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis, 2010). A subset of the available data is automatically selected as training data for a Language Model based on a scoring mechanism that compares crossentropy scores. Their approach considerably outperformed random selection and two previous proposed approaches both based on perplexity scoring.1 3 Measures of Domain Similarity 3.1 Measuring Similarity Automatically Feature Representations A similarity function may be defined over any set of events that are con1We tested data selection by perplexity scoring, but found the Language Models too small to be useful in our setting. 1567 sidered to be relev</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent Selection of Language Model Training Data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220–224, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>WileyInterscience.</publisher>
<contexts>
<context position="16130" citStr="Noreen, 1989" startWordPosition="2608" endWordPosition="2609">ch on English PoS tagging using two different taggers: MXPOST, the MaxEnt tagger of Ratnaparkhi5 and Citar,6 a trigram HMM tagger. In all experiments, parsing performance is measured as Labeled Attachment Score (LAS), the percentage of tokens with correct dependency edge and label. To compute LAS, we use the CoNLL 2007 evaluation script7 with punctuation tokens excluded from scoring (as was the default setting in CoNLL 2006). PoS tagging accuracy is measured as the percentage of correctly labeled words out of all words. Statistical significance is determined by Approximate Randomization Test (Noreen, 1989; Yeh, 2000) with 10,000 iterations. 4.2 Data English - WSJ For English, we use the portion of the Penn Treebank Wall Street Journal (WSJ) that has been made available in the CoNLL 2008 shared 4http://mallet.cs.umass.edu/ 5ftp://ftp.cis.upenn.edu/pub/adwait/jmx/ 6Citar has been implemented by Dani¨el de Kok and is available at: https://github.com/danieldk/citar 7http://nextens.uvt.nl/depparse-wiki/ 1569 task. This data has been automatically converted8 into dependency structure, and contains three files: the training set (sections 02-21), development set (section 24) and test set (section 23).</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Pi-Chuan Chang</author>
<author>Michael Ringgaard</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Uptraining for Accurate Deterministic Question Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>705--713</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="31916" citStr="Petrov et al. (2010)" startWordPosition="5172" endWordPosition="5175">re varied dataset. It contains in total over 50k articles and 20 million words (cf. Table 1). In contrast to the English data, only a small portion of the dataset is manually annotated: 281 articles.14 Since we want to evaluate the performance of different similarity measures, we want to keep the influence of noise as low as possible. Therefore, we annotated the remaining articles with a parsing system that is more accurate (Plank and van Noord, 2010), the Alpino parser (van Noord, 2006). Note that using a more accurate parsing system to train another parser has recently also been proposed by Petrov et al. (2010) as uptraining. Alpino is a 14http://www.let.rug.nl/vannoord/Lassy/ ● ● ● ● ● Accuracy 0.90 0.92 0.94 0.96 0.98 ● ● random words−js topic model−var 1573 parser tailored to Dutch, that has been developed over the last ten years, and reaches an accuracy level of 90% on general newspaper text. It uses a conditional MaxEnt model as parse selection component. Details of the parser are given in (van Noord, 2006). Average 0 5000 10000 15000 20000 25000 30000 number of sentences Figure 5: Result on Dutch; average over 30 articles. Data and Results The Dutch dataset contains articles from a variety of </context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for Accurate Deterministic Question Parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705–713, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Khalil Sima’an</author>
</authors>
<title>Subdomain Sensitive Statistical Parsing using Raw Corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<location>Marrakech, Morocco,</location>
<marker>Plank, Sima’an, 2008</marker>
<rawString>Barbara Plank and Khalil Sima’an. 2008. Subdomain Sensitive Statistical Parsing using Raw Corpora. In Proceedings of the 6th International Conference on Language Resources and Evaluation, Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Gertjan van Noord</author>
</authors>
<title>GrammarDriven versus Data-Driven: Which Parsing System Is More Affected by Domain Shifts?</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground,</booktitle>
<pages>25--33</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Plank, van Noord, 2010</marker>
<rawString>Barbara Plank and Gertjan van Noord. 2010. GrammarDriven versus Data-Driven: Which Parsing System Is More Affected by Domain Shifts? In Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, pages 25–33, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
<author>Radu Soricut</author>
</authors>
<title>Automatic Prediction of Parser Accuracy.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>887--896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5691" citStr="Ravi et al., 2008" startWordPosition="937" endWordPosition="940">shion, it can be easily applied to other tasks or languages for which annotated (or automatically annotated) data is available. 2 Related Work The work most related to ours is McClosky et al. (2010). They try to find the best combination of source models to parse data from a new domain, which is related to Plank and Sima’an (2008). In the latter, unlabeled data was used to create several parsers by weighting trees in the WSJ according to their similarity to the subdomain. McClosky et al. (2010) coined the term multiple source domain adaptation. Inspired by work on parsing accuracy prediction (Ravi et al., 2008), they train a linear regression model to predict the best (linear interpolation) of source domain models. Similar to us, McClosky et al. (2010) regard a target domain as mixture of source domains, but they focus on phrasestructure parsing. Furthermore, our approach differs from theirs in two respects: we do not treat source corpora as one entity and try to mix models, but rather consider articles as base units and try to find subsets of related articles (the most similar articles); moreover, instead of creating a supervised model (in their case to predict parsing accuracy), our approach is ‘s</context>
</contexts>
<marker>Ravi, Knight, Soricut, 2008</marker>
<rawString>Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Automatic Prediction of Parser Accuracy. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 887– 896, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R´enyi</author>
</authors>
<title>On measures of information and entropy.</title>
<date>1961</date>
<booktitle>In Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability,</booktitle>
<pages>547--561</pages>
<location>Berkeley.</location>
<marker>R´enyi, 1961</marker>
<rawString>A. R´enyi. 1961. On measures of information and entropy. In Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability, pages 547–561, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>The Domain Dependence of Parsing. In</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>96--102</pages>
<location>Washington D.C.</location>
<marker>Sekine, 1997</marker>
<rawString>Satoshi Sekine. 1997. The Domain Dependence of Parsing. In In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 96–102, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<title>Probabilistic Topic Models. Lawrence Erlbaum Associates.</title>
<date>2007</date>
<contexts>
<context position="9201" citStr="Steyvers and Griffiths, 2007" startWordPosition="1501" endWordPosition="1504">the advantage that we do not need to rely on additional supervised tools; moreover, it is interesting to know how far we can get with this level of information only. We examine the following feature representations: relative frequencies of words, relative frequencies of character tetragrams, and topic models. Our motivation was as follows. Relative frequencies of words are a simple and effective representation used e.g. in text classification (Manning and Sch¨utze, 1999), while character n-grams have proven successful in genre classification (Wu et al., 2010). Topic models (Blei et al., 2003; Steyvers and Griffiths, 2007) can be considered an advanced model over word distributions: every article is represented by a topic distribution, which in turn is a distribution over words. Similarity between documents can be measured by comparing topic distributions. Similarity Functions There are many possible similarity (or distance) functions. They fall broadly into two categories: probabilistically-motivated and geometrically-motivated functions. The similarity functions examined in this study will be described in the following. The Kullback-Leibler (KL) divergence D(q||r) is a classical measure of ‘distance’2 between</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Mark Steyvers and Tom Griffiths, 2007. Probabilistic Topic Models. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Ensemble Models for Dependency Parsing: Cheap and Good? In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>649--652</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="17742" citStr="Surdeanu and Manning, 2010" startWordPosition="2852" endWordPosition="2856">is amounted to 22 test articles. EN: WSJ WSJ+G+B Dutch articles 2,034 3,776 51,454 sentences 43,117 77,422 1,663,032 words 1,051,997 1,784,543 20,953,850 Table 1: Overview of the datasets for English and Dutch. To test whether we have a reasonable system, we performed a sanity check and trained the MST parser on the training section (02-21). The result on the standard test set (section 23) is identical to previously reported results (excluding punctuation tokens: LAS 87.50, Unlabeled Attachment Score (UAS) 90.75; with punctuation tokens: LAS 87.07, UAS 89.95). The latter has been reported in (Surdeanu and Manning, 2010). English - Genia (G) &amp; Brown (B) For the Domain Adaptation experiments, we added 1,552 articles from the GENIA10 treebank (biomedical abstracts from Medline) and 190 files from the Brown corpus to the pool of data. We converted the data to CoNLL format with the LTH converter (Johansson and Nugues, 2007). The size of the test files is, respectively: Genia 1,360 sentences with an average number of 26.20 words per sentence; the Brown test set is the same as used in the CoNLL 2008 shared task and contains 426 sentences with a mean of 16.80 words. 8Using the LTH converter: http://nlp.cs.lth.se/ so</context>
</contexts>
<marker>Surdeanu, Manning, 2010</marker>
<rawString>Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble Models for Dependency Parsing: Cheap and Good? In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 649–652, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Van Asch</author>
<author>Walter Daelemans</author>
</authors>
<title>Using Domain Similarity for Performance Estimation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,</booktitle>
<pages>31--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Van Asch, Daelemans, 2010</marker>
<rawString>Vincent Van Asch and Walter Daelemans. 2010. Using Domain Similarity for Performance Estimation. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 31–36, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At Last Parsing Is Now Operational.</title>
<date>2006</date>
<booktitle>In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le TraitementAutomatique des Langues naturelles,</booktitle>
<pages>20--42</pages>
<location>Leuven.</location>
<marker>van Noord, 2006</marker>
<rawString>Gertjan van Noord. 2006. At Last Parsing Is Now Operational. In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le TraitementAutomatique des Langues naturelles, pages 20–42, Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
</authors>
<title>Genre distinctions for Discourse in the Penn TreeBank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>674--682</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="3074" citStr="Webber, 2009" startWordPosition="510" endWordPosition="511"> III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus might not always be as homogeneous as originally thought (Webber, 2009; Lippincott et al., 2010). For instance, recent work has shown that the well-known Penn Treebank (PT) Wall Street Journal (WSJ) actually contains a variety of genres, including letters, wit and short verse (Webber, 2009). In this study we take a different approach. Rather than viewing a given corpus as a monolithic entity, 1566 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1566–1576, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics we break it down to the article-level and disregard corpora boundaries. Given </context>
<context position="11827" citStr="Webber, 2009" startWordPosition="1923" endWordPosition="1924">cosine cos(q,r) = q(y) · r(y)/||q(y) |r(y)||, euclidean �Eeuc(q, r) = y(q(y) − r(y))2 and variational (also known as L1 or Manhattan) distance function, defined as var(q, r) = E 3.2 Human-annotated data In contrast to the automatic measures devised in the previous section, we might have access to human annotated data. That is, use label information such as topic or genre to define the set of similar articles. Genre For the Penn Treebank (PT) Wall Street Journal (WSJ) section, more specifically, the subset available in the Penn Discourse Treebank, there exists a partition of the data by genre (Webber, 2009). Every article is assigned one of the following genre labels: news, letters, highlights, essays, errata, wit and short verse, quarterly progress reports, notable and quotable. This classification has been made on the basis of meta-data (Webber, 2009). It is wellknown that there is no meta-data directly associated with the individual WSJ files in the Penn Treebank. However, meta-data can be obtained by looking at the articles in the ACL/DCI corpus (LDC99T42), and a mapping file that aligns document numbers of DCI (DOCNO) to WSJ keys (Webber, 2009). An example document is given in Figure 1. The</context>
</contexts>
<marker>Webber, 2009</marker>
<rawString>Bonnie Webber. 2009. Genre distinctions for Discourse in the Penn TreeBank. In Proceedings of the 47th Meeting of the Association for Computational Linguistics, pages 674–682, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhili Wu</author>
<author>Katja Markert</author>
<author>Serge Sharoff</author>
</authors>
<title>FineGrained Genre Classification Using Structural Learning Algorithms.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>749--759</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9137" citStr="Wu et al., 2010" startWordPosition="1491" endWordPosition="1494">surface characteristics (unlabeled text). This has the advantage that we do not need to rely on additional supervised tools; moreover, it is interesting to know how far we can get with this level of information only. We examine the following feature representations: relative frequencies of words, relative frequencies of character tetragrams, and topic models. Our motivation was as follows. Relative frequencies of words are a simple and effective representation used e.g. in text classification (Manning and Sch¨utze, 1999), while character n-grams have proven successful in genre classification (Wu et al., 2010). Topic models (Blei et al., 2003; Steyvers and Griffiths, 2007) can be considered an advanced model over word distributions: every article is represented by a topic distribution, which in turn is a distribution over words. Similarity between documents can be measured by comparing topic distributions. Similarity Functions There are many possible similarity (or distance) functions. They fall broadly into two categories: probabilistically-motivated and geometrically-motivated functions. The similarity functions examined in this study will be described in the following. The Kullback-Leibler (KL) </context>
</contexts>
<marker>Wu, Markert, Sharoff, 2010</marker>
<rawString>Zhili Wu, Katja Markert, and Serge Sharoff. 2010. FineGrained Genre Classification Using Structural Learning Algorithms. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 749–759, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>947--953</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16142" citStr="Yeh, 2000" startWordPosition="2610" endWordPosition="2611">PoS tagging using two different taggers: MXPOST, the MaxEnt tagger of Ratnaparkhi5 and Citar,6 a trigram HMM tagger. In all experiments, parsing performance is measured as Labeled Attachment Score (LAS), the percentage of tokens with correct dependency edge and label. To compute LAS, we use the CoNLL 2007 evaluation script7 with punctuation tokens excluded from scoring (as was the default setting in CoNLL 2006). PoS tagging accuracy is measured as the percentage of correctly labeled words out of all words. Statistical significance is determined by Approximate Randomization Test (Noreen, 1989; Yeh, 2000) with 10,000 iterations. 4.2 Data English - WSJ For English, we use the portion of the Penn Treebank Wall Street Journal (WSJ) that has been made available in the CoNLL 2008 shared 4http://mallet.cs.umass.edu/ 5ftp://ftp.cis.upenn.edu/pub/adwait/jmx/ 6Citar has been implemented by Dani¨el de Kok and is available at: https://github.com/danieldk/citar 7http://nextens.uvt.nl/depparse-wiki/ 1569 task. This data has been automatically converted8 into dependency structure, and contains three files: the training set (sections 02-21), development set (section 24) and test set (section 23). Since we us</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th conference on Computational linguistics, pages 947–953, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>