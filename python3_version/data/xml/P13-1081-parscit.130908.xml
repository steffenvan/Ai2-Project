<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000353">
<title confidence="0.982788">
Enlisting the Ghost: Modeling Empty Categories for Machine Translation
</title>
<author confidence="0.914175">
Bing Xiang
</author>
<note confidence="0.609626">
IBM T. J. Watson Research Center
</note>
<address confidence="0.647512">
1101 Kitchawan Rd
Yorktown Heights, NY 10598
</address>
<email confidence="0.995664">
bxiang@us.ibm.com
</email>
<author confidence="0.913883">
Xiaoqiang Luo *
</author>
<affiliation confidence="0.760815">
Google Inc.
</affiliation>
<address confidence="0.855148">
111 8th Ave
New York, NY 10011
</address>
<email confidence="0.997284">
xql@google.com
</email>
<note confidence="0.605069">
Bowen Zhou
IBM T. J. Watson Research Center
</note>
<address confidence="0.632412">
1101 Kitchawan Rd
Yorktown Heights, NY 10598
</address>
<email confidence="0.996659">
zhou@us.ibm.com
</email>
<sectionHeader confidence="0.993825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999773571428572">
Empty categories (EC) are artificial ele-
ments in Penn Treebanks motivated by the
government-binding (GB) theory to ex-
plain certain language phenomena such as
pro-drop. ECs are ubiquitous in languages
like Chinese, but they are tacitly ignored
in most machine translation (MT) work
because of their elusive nature. In this
paper we present a comprehensive treat-
ment of ECs by first recovering them with
a structured MaxEnt model with a rich
set of syntactic and lexical features, and
then incorporating the predicted ECs into
a Chinese-to-English machine translation
task through multiple approaches, includ-
ing the extraction of EC-specific sparse
features. We show that the recovered
empty categories not only improve the
word alignment quality, but also lead to
significant improvements in a large-scale
state-of-the-art syntactic MT system.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.96558935">
One of the key challenges in statistical machine
translation (SMT) is to effectively model inher-
ent differences between the source and the target
language. Take the Chinese-English SMT as an
example: it is non-trivial to produce correct pro-
nouns on the target side when the source-side pro-
noun is missing. In addition, the pro-drop prob-
lem can also degrade the word alignment qual-
ity in the training data. A sentence pair observed
in the real data is shown in Figure 1 along with
the word alignment obtained from an automatic
word aligner, where the English subject pronoun
* This work was done when the author was with IBM.
“that” is missing on the Chinese side. Conse-
quently, “that” is incorrectly aligned to the second
to the last Chinese word “De”, due to their high
co-occurrence frequency in the training data. If
the dropped pronoun were recovered, “that” would
have been aligned with the dropped-pro (cf. Fig-
ure 3), which is a much more sensible alignment.
</bodyText>
<figureCaption confidence="0.894346">
Figure 1: Example of incorrect word alignment
due to missing pronouns on the Chinese side.
</figureCaption>
<bodyText confidence="0.998452117647059">
In order to account for certain language phe-
nomena such as pro-drop and wh-movement, a set
of special tokens, called empty categories (EC),
are used in Penn Treebanks (Marcus et al., 1993;
Bies and Maamouri, 2003; Xue et al., 2005). Since
empty categories do not exist in the surface form
of a language, they are often deemed elusive and
recovering ECs is even figuratively called “chas-
ing the ghost” (Yang and Xue, 2010).
In this work we demonstrate that, with the avail-
ability of large-scale EC annotations, it is feasi-
ble to predict and recover ECs with high accu-
racy. More importantly, with various approaches
of modeling the recovered ECs in SMT, we are
able to achieve significant improvements1.
The contributions of this paper include the fol-
lowing:
</bodyText>
<listItem confidence="0.853947">
• Propose a novel structured approach to EC
prediction, including the exact word-level lo-
</listItem>
<footnote confidence="0.919109">
1Hence “Enlisting the ghost” in the title of this paper.
</footnote>
<page confidence="0.903113">
822
</page>
<note confidence="0.903837">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 822–831,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.815069666666667">
cation and EC labels. Our results are sig-
nificantly higher in accuracy than that of the
state-of-the-art;
</bodyText>
<listItem confidence="0.989849555555555">
• Measure the effect of ECs on automatic word
alignment for machine translation after inte-
grating recovered ECs into the MT data;
• Design EC-specific features for phrases and
syntactic tree-to-string rules in translation
grammar;
• Show significant improvement on top of the
state-of-the-art large-scale hierarchical and
syntactic machine translation systems.
</listItem>
<bodyText confidence="0.99977325">
The rest of the paper is organized as follows. In
Section 2, we present a structured approach to EC
prediction. In Section 3, we describe the integra-
tion of Chinese ECs in MT. The experimental re-
sults for both EC prediction and SMT are reported
in Section 4. A survey on the related work is con-
ducted in Section 5, and Section 6 summarizes the
work and introduces some future work.
</bodyText>
<sectionHeader confidence="0.952128" genericHeader="method">
2 Chinese Empty Category Prediction
</sectionHeader>
<bodyText confidence="0.99609775">
The empty categories in the Chinese Treebank
(CTB) include trace markers for A’- and A-
movement, dropped pronoun, big PRO etc. A
complete list of categories used in CTB is shown
in Table 1 along with their intended usages. Read-
ers are referred to the documentation (Xue et al.,
2005) of CTB for detailed discussions about the
characterization of empty categories.
</bodyText>
<table confidence="0.993311571428572">
EC Meaning
*T* trace of A’-movement
* trace of A-movement
*PRO* big PRO in control structures
*pro* pro-drop
*OP* operator in relative clauses
*RNR* for right node raising
</table>
<tableCaption confidence="0.999884">
Table 1: List of empty categories in the CTB.
</tableCaption>
<bodyText confidence="0.9999155">
In this section, we tackle the problem of recov-
ering Chinese ECs. The problem has been studied
before in the literature. For instance, Yang and
Xue (2010) attempted to predict the existence of
an EC before a word; Luo and Zhao (2011) pre-
dicted ECs on parse trees, but the position infor-
mation of some ECs is partially lost in their repre-
sentation. Furthermore, Luo and Zhao (2011) con-
ducted experiments on gold parse trees only. In
our opinion, recovering ECs from machine parse
trees is more meaningful since that is what one
would encounter when developing a downstream
application such as machine translation. In this
paper, we aim to have a more comprehensive treat-
ment of the problem: all EC types along with
their locations are predicted, and we will report the
results on both human parse trees and machine-
generated parse trees.
</bodyText>
<subsectionHeader confidence="0.998954">
2.1 Representation of Empty Categories
</subsectionHeader>
<bodyText confidence="0.99992345">
Our effort of recovering ECs is a two-step process:
first, at training time, ECs in the Chinese Treebank
are moved and preserved in the portion of the tree
structures pertaining to surface words only. Origi-
nal ECs and their subtrees are then deleted without
loss of information; second, a model is trained on
transformed trees to predict and recover ECs.
Empty categories heavily depend on syntac-
tic tree structure. For this reason, we choose to
project them onto a parse tree node. To facili-
tate presentation, we first distinguish a solid vs.
an empty non-terminal node. A non-terminal node
is solid if and only if it contains at least one child
node that spans one or more surface words (as op-
posed to an EC); accordingly, an empty node is a
non-terminal node that spans only ECs. In the left
half of Figure 2, the NP node that is the immediate
child of IP has only one child node spanning an
EC – (-NONE- *pro*), and is thus an empty
node; while all other non-terminal nodes have at
least one surface word as their child and are thus
all solid nodes.
We decide to attach an EC to its lowest solid
ancestor node. That is, the EC is moved up to the
first solid node in the syntactic tree. After ECs
are attached, all empty nodes and ECs are deleted
from the tree. In order to uniquely recover ECs,
we also need to encode the position information.
To this end, the relative child index of an EC is
affixed to the EC tag. Take the NP node spanning
the *pro* in Figure 2 as an example, the *pro*
is moved to the lowest solid ancestor, IP node,
and its position is encoded by @1 since the deleted
NP is the second child of the IP node (we use 0-
based indices). With this transformation, we are
able to recover not only the position of an EC, but
its type as well. A special tag NULL is attached
to non-terminal nodes without EC. Since an EC is
introduced to express the structure of a sentence,
it is a good practice to associate it with the syn-
</bodyText>
<page confidence="0.999317">
823
</page>
<figureCaption confidence="0.9793325">
Figure 2: Example of tree transformation on training data to encode an empty category and its position
information.
</figureCaption>
<bodyText confidence="0.999297769230769">
tactic tree, as opposed to simply attaching it to a
neighboring word, as was done in (Yang and Xue,
2010). We believe this is one of the reasons why
our model has better accuracy than that of (Yang
and Xue, 2010) (cf. Table 7).
In summary, a projected tag consists of an EC
type (such as *pro*) and the EC’s position in-
formation. The problem of predicting ECs is then
cast into predicting an EC tag at each non-terminal
node. Notice that the input to such a predictor is
a syntactic tree without ECs, e.g., the parse tree
on the right hand of Figure 2 without the EC tag
*pro*@1 is such an example.
</bodyText>
<subsectionHeader confidence="0.997349">
2.2 A Structured Empty Category Model
</subsectionHeader>
<bodyText confidence="0.9984211">
We propose a structured MaxEnt model for pre-
dicting ECs. Specially, given a syntactic tree, T,
whose ECs have been projected onto solid nodes
with the procedure described in Section 2.1, we
traverse it in post-order (i.e., child nodes are vis-
ited recursively first before the current node is vis-
ited). Let T = t1t2 · · · tn be the sequence of
nodes produced by the post-order traversal, and
ei(i = 1, 2, · · · , n) be the EC tag associated with
ti. The probabilistic model is then:
</bodyText>
<equation confidence="0.985596833333333">
P(eiIT,ei 1
1 )
exp( Ek λkfk(ei 1
1 , T, ei))
Z(ei 11, T) (1)
Eq. (1) is the familiar log linear (or MaxEnt)
model, where fk(ei 1
1 ,T, ei) is the feature func-
tion and
Z(ei 1
1 , T) = Ee∈E exp( Ek λkfk(ei 1
1 , T, e))
</equation>
<bodyText confidence="0.99727096969697">
is the normalization factor. £ is the set of ECs to be
predicted. In the CTB 7.0 processed by the proce-
dure in Section 2.1, the set consists of 32 EC tags
plus a special NULL symbol, obtained by modulat-
ing the list of ECs in Table 1 with their positions
(e.g., *pro*@1 in Figure 2).
Once the model is chosen, the next step is to de-
cide a set of features {fk(ei 1
1 , T, ei)I to be used
in the model. One advantage of having the rep-
resentation in Section 2.1 is that it is very easy to
compute features from tree structures. Indeed, all
features used in our system are computed from the
syntactic trees, including lexical features.
There are 3 categories of features used in the
model: (1) tree label features; (2) lexical features;
(3) EC features, and we list them in Table 2. In
the feature description column, all node positions
(e.g., “left”, “right”) are relative to the current
node being predicted.
Feature 1 to 10 are computed directly from
parse trees, and are straightforward. We include
up to 2 siblings when computing feature 9 and 10.
Feature 11 to 17 are lexical features. Note that we
use words at the edge of the current node: fea-
ture 11 and 12 are words at the internal boundary
of the current node, while feature 13 and 14 are
the immediately neighboring word external to the
current node. Feature 15 and 17 are from head
word information of the current node and the par-
ent node. Feature 18 and 19 are computed from
predicted ECs in the past – that’s why the model
in Eq. (1) conditions on ei 1
</bodyText>
<equation confidence="0.592925">
1 .
</equation>
<bodyText confidence="0.999776285714286">
Besides the features presented in Table 2, we
also use conjunction features between the current
node label with the parent node label; the cur-
rent node label with features computed from child
nodes; the current node label with features from
left and sibling nodes; the current node label with
lexical features.
</bodyText>
<equation confidence="0.9932582">
n
P(en1 IT) =
i=1
n
i=1
</equation>
<page confidence="0.995799">
824
</page>
<table confidence="0.985997">
No. Tree Label Features
1 current node label
2 parent node label
3 grand-parent node label
4 left-most child label or POS tag
5 right-most child label or POS tag
6 label or POS tag of the head child
7 the number of child nodes
8 one level CFG rule
9 left-sibling label or POS tag
10 right-sibling label or POS tag
Lexical Features
11 left-most word under the current node
12 right-most word under the current node
13 word immediately left to the span of the
current node
14 word immediately right to the span of the
current node
15 head word of the current node
16 head word of the parent node
17 is the current node head child of its parent?
EC Features
18 predicted EC of the left sibling
19 the set of predicted ECs of child nodes
</table>
<tableCaption confidence="0.991557">
Table 2: List of features.
</tableCaption>
<sectionHeader confidence="0.7749295" genericHeader="method">
3 Integrating Empty Categories in
Machine Translation
</sectionHeader>
<bodyText confidence="0.999883">
In this section, we explore multiple approaches of
utilizing recovered ECs in machine translation.
</bodyText>
<subsectionHeader confidence="0.9994">
3.1 Explicit Recovery of ECs in MT
</subsectionHeader>
<bodyText confidence="0.999994583333333">
We conducted some initial error analysis on our
MT system output and found that most of the er-
rors that are related to ECs are due to the missing
*pro* and *PRO*. This is also consistent with
the findings in (Chung and Gildea, 2010). One of
the other frequent ECs, *OP*, appears in the Chi-
nese relative clauses, which usually have a Chi-
nese word “De” aligned to the target side “that”
or “which”. And the trace, *T*, exists in both
Chinese and English sides. For MT we want to fo-
cus on the places where there exist mismatches be-
tween the source and target languages. A straight-
forward way of utilizing the recovered *pro* and
*PRO* is to pre-process the MT training and test
data by inserting ECs into the original source text
(i.e. Chinese in this case). As mentioned in the
previous section, the output of our EC predictor
is a new parse tree with the labels and positions
encoded in the tags. Based on the positional in-
formation in the tags, we can move the predicted
ECs down to the surface level and insert them be-
tween original source words. The same prediction
and “pull-down” procedure can be conducted con-
sistently cross the MT training and test data.
</bodyText>
<subsectionHeader confidence="0.9896545">
3.2 Grammar Extraction on Augmented
Data
</subsectionHeader>
<bodyText confidence="0.999974966666667">
With the pre-processed MT training corpus, an un-
supervised word aligner, such as GIZA++, can be
used to generate automatic word alignment, as the
first step of a system training pipeline. The ef-
fect of inserting ECs is two-fold: first, it can im-
pact the automatic word alignment since now it al-
lows the target-side words, especially the function
words, to align to the inserted ECs and fix some
errors in the original word alignment; second, new
phrases and rules can be extracted from the pre-
processed training data. For example, for a hier-
archical MT system, some phrase pairs and Hiero
(Chiang, 2005) rules can be extracted with recov-
ered *pro* and *PRO* at the Chinese side.
In this work we also take advantages of the aug-
mented Chinese parse trees (with ECs projected
to the surface) and extract tree-to-string grammar
(Liu et al., 2006) for a tree-to-string MT system.
Due to the recovered ECs in the source parse
trees, the tree-to-string grammar extracted from
such trees can be more discriminative, with an in-
creased capability of distinguishing different con-
text. An example of an augmented Chinese parse
tree aligned to an English string is shown in Figure
3, in which the incorrect alignment in Figure 1 is
fixed. A few examples of the extracted Hiero rules
and tree-to-string rules are also listed, which we
would not have been able to extract from the orig-
inal incorrect word alignment when the *pro*
was missing.
</bodyText>
<subsectionHeader confidence="0.990327">
3.3 Soft Recovery: EC-Specific Sparse
Features
</subsectionHeader>
<bodyText confidence="0.9999363">
Recovered ECs are often good indicators of what
hypothesis should be chosen during decoding. In
addition to the augmented syntax-based grammar,
we propose sparse features as a soft constraint to
boost the performance. For each phrase pair, Hi-
ero rule or tree-to-string rule in the MT system,
a binary feature fk fires if there exists a *pro*
on the source side and it aligns to one of its most
frequently aligned target words found in the train-
ing corpus. We also fire another feature if *pro*
</bodyText>
<page confidence="0.996929">
825
</page>
<figureCaption confidence="0.996944">
Figure 3: Fixed word alignment and examples of
extracted Hiero rules and tree-to-string rules.
</figureCaption>
<bodyText confidence="0.9959118">
aligns to any other target words so the model can
choose to penalize them based on a tuning set.
Similar features can fire for *PRO*. The feature
weights can be tuned on a tuning set in a log-linear
model along with other usual features/costs, in-
cluding language model scores, bi-direction trans-
lation probabilities, etc. The motivation for such
sparse features is to reward those phrase pairs
and rules that have highly confident lexical pairs
specifically related to ECs, and penalize those who
don’t have such lexical pairs.
Table 3 listed some of the most frequent English
words aligned to *pro* or *PRO* in a Chinese-
English parallel corpus with 2M sentence pairs.
Their co-occurrence counts and the lexical trans-
lation probabilities are also shown in the table. In
total we use 15 sparse features for frequent lexical
pairs, including 13 for *pro* and 2 for *PRO*,
and two more features for any other target words
that align to *pro* or *PRO*.
</bodyText>
<table confidence="0.999570777777778">
Source Target Counts P(tIs)
*pro* the 93100 0.11
*pro* to 86965 0.10
*pro* it 45423 0.05
*pro* in 36129 0.04
*pro* we 24509 0.03
*pro* which 17259 0.02
*PRO* to 195464 0.32
*PRO* for 31200 0.05
</table>
<tableCaption confidence="0.987928">
Table 3: Example of frequent word pairs used for
sparse features.
</tableCaption>
<sectionHeader confidence="0.998081" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.998925">
4.1 Empty Category Prediction
</subsectionHeader>
<bodyText confidence="0.999990959183674">
We use Chinese Treebank (CTB) v7.0 to train and
test the EC prediction model. We partition the
data into training, development and test sets. The
training set includes 32925 sentences from CTB
files 0001-0325, 0400-0454, 0500-0542, 0600-
0840, 0590-0596, 1001-1120, 2000-3000, cctv,
cnn, msnbc, and phoenix 00-06. The development
set has 3033 sentences, from files 0549-0554,
0900-0931, 1136-1151, 3076-3145, and phoenix
10-11. The test set contains 3297 sentences, from
files 0543-0548, 0841-0885, 1121-1135, 3001-
3075, and phoenix 07-09.
To measure the accuracy of EC prediction, we
project the predicted tags from the upper level
nodes in the parse trees down to the surface level
based on the position information encoded in the
tags. The position index for each inserted EC,
counted at the surface level, is attached for scor-
ing purpose. The same operation is applied on
both the reference and the system output trees.
Such projection is necessary, especially when the
two trees differ in structure (e.g. gold trees vs.
machine-generated trees). We compute the pre-
cision, recall and F1 scores for each EC on the
test set, and collect their counts in the reference
and system output. The results are shown in Ta-
ble 4, where the LDC gold parse trees are used to
extract syntactic features for the model. The first
row in the table shows the accuracy for the places
where no EC should be inserted. The predictor
achieves 99.5% F1 score for this category, with
limited number of missing or false positives. The
F1 scores for majority of the ECs are above 70%,
except for “*”, which is relatively rare in the data.
For the two categories that are interesting to MT,
*pro* and *PRO*, the predictor achieves 74.3%
and 81.5% in F1 scores, respectively.
The results reported above are based on the
LDC gold parse trees. To apply the EC predic-
tion to NLP applications, such as MT, it is impos-
sible to always rely on the gold trees due to its
limited availability. We parse our test set with a
maximum entropy based statistical parser (Ratna-
parkhi, 1997) first. The parser accuracy is around
84% on the test set. Then we extract features based
on the system-generated parse trees, and decode
with the previously trained model. The results are
shown in Table 5. Compared to those in Table 4,
the F1 scores dropped by different degrees for dif-
</bodyText>
<page confidence="0.997248">
826
</page>
<table confidence="0.99846975">
Tag Ref Sys P R F1
NULL 75159 75508 99.3 99.7 99.5
*pro* 1692 1442 80.8 68.9 74.3
*PRO* 1410 1282 85.6 77.8 81.5
*T* 1851 1845 82.8 82.5 82.7
*OP* 1721 1853 90.9 97.9 94.2
*RNR* 51 39 87.2 66.7 75.6
* 156 96 63.5 39.1 48.4
</table>
<tableCaption confidence="0.996948">
Table 4: Prediction accuracy with gold parse trees,
</tableCaption>
<bodyText confidence="0.999344333333333">
where NULL represents the cases where no ECs
should be produced.
ferent types. Such performance drop is expected
since the system relies heavily on syntactic struc-
ture, and parsing errors create an inherent mis-
matching condition between the training and test-
ing time. The smallest drop among all types is on
NULL, at about 1.6%. The largest drop occurs for
*OP*, at 27.1%, largely due to the parsing errors
on the CP nodes. The F1 scores for *pro* and
*PRO* when using system-generated parse trees
are between 50% to 60%.
</bodyText>
<table confidence="0.9994635">
Tag Precision Recall F1
NULL 97.6 98.2 97.9
*pro* 51.1 50.1 50.6
*PRO* 66.4 50.5 57.3
*T* 68.2 59.9 63.8
*OP* 66.8 67.3 67.1
*RNR* 70.0 54.9 61.5
* 60.9 35.9 45.2
</table>
<tableCaption confidence="0.963473">
Table 5: Prediction accuracy with system-
generated parse trees.
</tableCaption>
<bodyText confidence="0.999696533333333">
To show the effect of ECs other than *pro*
and *PRO*, we remove all ECs in the training data
except *pro* and *PRO*. So the model only
predicts NULL, *pro* or *PRO*. The results on
the test set are listed in Table 6. There is 0.8% and
0.5% increase on NULL and *pro*, respectively.
The F1 score for *PRO* drops by 0.2% slightly.
As mentioned earlier, for MT we focus on re-
covering *pro* and *PRO* only. The model
generating the results in Table 6 is the one we ap-
plied in our MT experiments reported later.
In order to compare to the state-of-the-art mod-
els to see where our model stands, we switch our
training, development and test data to those used
in the work of (Yang and Xue, 2010) and (Cai et
</bodyText>
<table confidence="0.9975585">
Tag Precision Recall F1
NULL 98.5 98.9 98.7
*pro* 51.0 51.1 51.1
*PRO* 66.0 50.4 57.1
</table>
<tableCaption confidence="0.755289666666667">
Table 6: Prediction accuracy with system-
generated parse trees, modeling *pro* and
*PRO* only.
</tableCaption>
<bodyText confidence="0.999351444444444">
al., 2011), for the purpose of a direct comparison.
The training set includes CTB files 0081 through
0900. The development set includes files 0041 to
0080, and the test set contains files 0001-0040 and
0901-0931. We merge all empty categories into
a single type in the training data before training
our EC prediction model. To compare the perfor-
mance on system-generated parse trees, we also
train a Berkeley parser on the same training data
and parse the test set. The prediction accuracy
for such single type on the test set with gold or
system-generated parse trees is shown in Table 7,
compared to the numbers reported in (Yang and
Xue, 2010) and (Cai et al., 2011). The model we
proposed achieves 6% higher F1 score than that in
(Yang and Xue, 2010) and 2.6% higher than that in
(Cai et al., 2011), which is significant. This shows
the effectiveness of our structured approach.
</bodyText>
<table confidence="0.9985845">
Model T P R F1
(Yang and Xue, 2010) G 95.9 83.0 89.0
Structured (this work) G 96.5 93.6 95.0
(Yang and Xue, 2010) S 80.3 52.1 63.2
(Cai et al., 2011) S 74.0 61.3 67.0
Structured (this work) S 74.9 65.1 69.6
</table>
<tableCaption confidence="0.9731735">
Table 7: Comparison with the previous results, us-
ing the same training and test data. T: parse trees.
G: gold parse trees. S: system-generated parse
trees. P: precision. R: recall.
</tableCaption>
<subsectionHeader confidence="0.996913">
4.2 MT Results
</subsectionHeader>
<bodyText confidence="0.999475222222222">
In the Chinese-to-English MT experiments, we
test two state-of-the-art MT systems. One is an re-
implementation of Hiero (Chiang, 2005), and the
other is a hybrid syntax-based tree-to-string sys-
tem (Zhao and Al-onaizan, 2008), where normal
phrase pairs and Hiero rules are used as a backoff
for tree-to-string rules.
The MT training data includes 2 million sen-
tence pairs from the parallel corpora released by
</bodyText>
<page confidence="0.993514">
827
</page>
<bodyText confidence="0.999879479166667">
LDC over the years, with the data from United
Nations and Hong Kong excluded 2. The Chi-
nese text is segmented with a segmenter trained
on the CTB data using conditional random field
(CRF), followed by the longest-substring match
segmentation in a second pass. Our language
model (LM) training data consists of about 10 bil-
lion English words, which includes Gigaword and
other newswire and web data released by LDC,
as well as the English side of the parallel train-
ing corpus. We train a 6-gram LM with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998). Our tuning set for MT contains 1275 sen-
tences from LDC2010E30. We test our system
on the NIST MT08 Newswire (691 sentences)
and Weblog (666 sentences) sets. Both tuning
and test sets have 4 sets of references for each
sentence. The MT systems are optimized with
pairwise ranking optimization (Hopkins and May,
2011) to maximize BLEU (Papineni et al., 2002).
We first predict *pro* and *PRO* with our
annotation model for all Chinese sentences in the
parallel training data, with *pro* and *PRO* in-
serted between the original Chinese words. Then
we run GIZA++ (Och and Ney, 2000) to generate
the word alignment for each direction and apply
grow-diagonal-final (Koehn et al., 2003), same as
in the baseline. We want to measure the impact on
the word alignment, which is an important step for
the system building. We append a 300-sentence
set, which we have human hand alignment avail-
able as reference, to the 2M training sentence pairs
before running GIZA++. The alignment accuracy
measured on this alignment test set, with or with-
out *pro* and *PRO* inserted before running
GIZA++, is shown in Table 8. To make a fair
comparison with the baseline alignment, any tar-
get words aligned to ECs are deemed as unaligned
during scoring. We observe 1.2% improvement on
function word related links, and almost the same
accuracy on content words. This is understand-
able since *pro* and *PRO* are mostly aligned
to the function words at the target side. The pre-
cision and recall for function words are shown in
Table 9. We can see higher accuracy in both pre-
cision and recall when ECs (*pro* and *PRO*)
are recovered in the Chinese side. Especially, the
precision is improved by 2% absolute.
</bodyText>
<footnote confidence="0.99690425">
2The training corpora include LDC2003E07,
LDC2003E08, LDC2005T10, LDC2006E26, LDC2006G05,
LDC2007E103, LDC2008G05, LDC2009G01, and
LDC2009G02.
</footnote>
<table confidence="0.998927666666667">
System Function Content All
Baseline 51.7 69.7 65.4
+EC 52.9 69.6 65.7
</table>
<tableCaption confidence="0.9696915">
Table 8: Word alignment F1 scores with or without
*pro* and *PRO*.
</tableCaption>
<table confidence="0.999885333333333">
System Precision Recall F1
Baseline 54.1 49.5 51.7
+EC 56.0 50.1 52.9
</table>
<tableCaption confidence="0.981965">
Table 9: Word alignment accuracy for function
words only.
</tableCaption>
<bodyText confidence="0.999564689655172">
Next we extract phrase pairs, Hiero rules and
tree-to-string rules from the original word align-
ment and the improved word alignment, and tune
all the feature weights on the tuning set. The
weights include those for usual costs and also the
sparse features proposed in this work specifically
for ECs. We test all the systems on the MT08
Newswire and Weblog sets.
The BLEU scores from different systems are
shown in Table 10 and Table 11, respectively. We
measure the incremental effect of prediction (in-
serting *pro* and *PRO*) and sparse features.
Pre-processing of the data with ECs inserted im-
proves the BLEU scores by about 0.6 for newswire
and 0.2 to 0.3 for the weblog data, compared to
each baseline separately. On top of that, adding
sparse features helps by another 0.3 on newswire
and 0.2 to 0.4 on weblog. Overall, the Hiero
and tree-to-string systems are improved by about 1
point for newswire and 0.4 to 0.7 for weblog. The
smaller gain on the weblog data could be due to
the more difficult data to parse, which affects the
accuracy of EC prediction. All the results in Table
10 and 11 marked with “*” are statistically signif-
icant with p &lt; 0.05 using the sign test described
in (Collins et al., 2005), compared to the baseline
results in each table. Two MT examples are given
in Table 12, which show the effectiveness of the
recovered ECs in MT.
</bodyText>
<table confidence="0.979382">
System MT08-nw MT08-wb
Hiero 33.99 25.40
+prediction 34.62* 25.63
+prediction+sparse 34.95* 25.80*
</table>
<tableCaption confidence="0.996264">
Table 10: BLEU scores in the Hiero system.
</tableCaption>
<page confidence="0.928255">
828
</page>
<table confidence="0.99860325">
System MT08-nw MT08-wb
T2S+Hiero 34.53 25.80
+prediction 35.17* 26.08
+prediction+sparse 35.51* 26.53*
</table>
<tableCaption confidence="0.9752205">
Table 11: BLEU scores in the tree-to-string system
with Hiero rules as backoff.
</tableCaption>
<sectionHeader confidence="0.999854" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999937596491228">
Empty categories have been studied in recent
years for several languages, mostly in the con-
text of reference resolution and syntactic process-
ing for English, such as in (Johnson, 2002; Di-
enes and Dubey, 2003; Gabbard et al., 2006).
More recently, EC recovery for Chinese started
emerging in literature. In (Guo et al., 2007),
non-local dependencies are migrated from En-
glish to Chinese for generating proper predicate-
argument-modifier structures from surface context
free phrase structure trees. In (Zhao and Ng,
2007), a decision tree learning algorithm is pre-
sented to identify and resolve Chinese anaphoric
zero pronouns. and achieves a performance com-
parable to a heuristic rule-based approach. Similar
to the work in (Dienes and Dubey, 2003), empty
detection is formulated as a tagging problem in
(Yang and Xue, 2010), where each word in the
sentence receives a tag indicating whether there is
an EC before it. A maximum entropy model is
utilized to predict the tags, but different types of
ECs are not distinguished. In (Cai et al., 2011),
a language-independent method was proposed to
integrate the recovery of empty elements into syn-
tactic parsing. As shown in the previous section,
our model outperforms the model in (Yang and
Xue, 2010) and (Cai et al., 2011) significantly us-
ing the same training and test data. (Luo and Zhao,
2011) also tries to predict the existence of an EC
in Chinese sentences, but the ECs in the middle of
a tree constituent are lumped into a single position
and are not uniquely recoverable.
There exists only a handful of previous work on
applying ECs explicitly to machine translation so
far. One of them is the work reported in (Chung
and Gildea, 2010), where three approaches are
compared, based on either pattern matching, CRF,
or parsing. However, there is no comparison be-
tween using gold trees and automatic trees. There
also exist a few major differences on the MT
part between our work and theirs. First, in ad-
dition to the pre-processing of training data and
inserting recovered empty categories, we imple-
ment sparse features to further boost the perfor-
mance, and tune the feature weights directly to-
wards maximizing the machine translation met-
ric. Second, there is no discussion on the quality
of word alignment in (Chung and Gildea, 2010),
while we show the alignment improvement on a
hand-aligned set. Last, they use a phase-based
system trained on only 60K sentences, while we
conduct experiments on more advanced Hiero and
tree-to-string systems, trained on 2M sentences in
a much larger corpus. We directly take advantage
of the augmented parse trees in the tree-to-string
grammar, which could have larger impact on the
MT system performance.
</bodyText>
<sectionHeader confidence="0.998963" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99993275">
In this paper, we presented a novel structured ap-
proach to EC prediction, which utilizes a max-
imum entropy model with various syntactic fea-
tures and shows significantly higher accuracy than
the state-of-the-art approaches. We also applied
the predicted ECs to a large-scale Chinese-to-
English machine translation task and achieved sig-
nificant improvement over two strong MT base-
</bodyText>
<page confidence="0.995303">
829
</page>
<bodyText confidence="0.994641444444445">
lines, i.e. a hierarchical phase-based system and
a tree-to-string syntax-based system. More work
remain to be done next to further take advantages
of ECs. For example, the recovered ECs can be
encoded in a forest as the input to the MT decoder
and allow the decoder to pick the best MT output
based on various features in addition to the sparse
features we proposed in this work. Many promis-
ing approaches can be explored in the future.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999984555555555">
We would like to acknowledge the support
of DARPA under Grant HR0011-12-C-0015 for
funding part of this work. The views, opin-
ions, and/or findings contained in this arti-
cle/presentation are those of the author/presenter
and should not be interpreted as representing the
official views or policies, either expressed or im-
plied, of the Defense Advanced Research Projects
Agency or the Department of Defense.
</bodyText>
<sectionHeader confidence="0.997262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998667939759036">
Ann Bies and Mohamed Maamouri. 2003.
Penn Arabic treebank guidelines. In
http://www.ircs.upenn.edu/arabic/Jan03release/
guidelines-TB-1-28-03.pdf.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty ele-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:short papers, pages 212–216.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. In Technical Report TR-10-98, Computer
Science Group, Harvard University.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263–270,
Ann Arbor, Michigan, June.
Tagyoung Chung and Daniel Gildea. 2010. Effects
of empty categories on machine translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 636–
645.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 531–540.
Peter Dienes and Amit Dubey. 2003. Deep syntactic
processing by combining shallow methods. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics.
Ryan Gabbard, Seth Kulick, and Mitchell Marcus.
2006. Fully parsing the penn treebank. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the ACL.
Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2007. Recovering non-local dependencies for Chi-
nese. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
ofHLT-NAACL, pages 48–54.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 609–616.
Xiaoqiang Luo and Bing Zhao. 2011. A statistical
tree annotator and its applications. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1230–1238.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. In Compu-
tational Linguistics, volume 19(2), pages 313–330.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440–447, Hong Kong,
China, October.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Associationfor Com-
putational Linguistics, pages 311–318, Philadelphia,
PA.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of Second Conference on Empirical
</reference>
<page confidence="0.97284">
830
</page>
<reference confidence="0.998213695652174">
Methods in Natural Language Processing, pages 1–
10.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering, volume 11(2), pages 207–
238.
Yaqin Yang and Nianwen Xue. 2010. Chasing the
ghost: Recovering empty categories in the Chi-
nese Treebank. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 1382–1390, Beijing, China, August.
Bing Zhao and Yaser Al-onaizan. 2008. Generaliz-
ing local and non-local word-reordering patterns for
syntax-based machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 572–581.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of Chinese zero pronouns: A ma-
chine learning approach. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
</reference>
<page confidence="0.998387">
831
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.179035">
<title confidence="0.99993">Enlisting the Ghost: Modeling Empty Categories for Machine Translation</title>
<author confidence="0.983098">Bing</author>
<affiliation confidence="0.988285">IBM T. J. Watson Research</affiliation>
<address confidence="0.7178145">1101 Kitchawan Yorktown Heights, NY</address>
<email confidence="0.999168">bxiang@us.ibm.com</email>
<author confidence="0.986812">Xiaoqiang Luo</author>
<affiliation confidence="0.999955">Google Inc.</affiliation>
<address confidence="0.9015805">111 8th New York, NY</address>
<email confidence="0.998464">xql@google.com</email>
<author confidence="0.928316">Bowen</author>
<affiliation confidence="0.979332">IBM T. J. Watson Research</affiliation>
<address confidence="0.704245">1101 Kitchawan Yorktown Heights, NY</address>
<email confidence="0.999359">zhou@us.ibm.com</email>
<abstract confidence="0.998705545454545">Empty categories (EC) are artificial elements in Penn Treebanks motivated by the government-binding (GB) theory to explain certain language phenomena such as pro-drop. ECs are ubiquitous in languages like Chinese, but they are tacitly ignored in most machine translation (MT) work because of their elusive nature. In this paper we present a comprehensive treatment of ECs by first recovering them with a structured MaxEnt model with a rich set of syntactic and lexical features, and then incorporating the predicted ECs into a Chinese-to-English machine translation task through multiple approaches, including the extraction of EC-specific sparse features. We show that the recovered empty categories not only improve the word alignment quality, but also lead to significant improvements in a large-scale state-of-the-art syntactic MT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mohamed Maamouri</author>
</authors>
<date>2003</date>
<contexts>
<context position="2492" citStr="Bies and Maamouri, 2003" startWordPosition="396" endWordPosition="399"> the Chinese side. Consequently, “that” is incorrectly aligned to the second to the last Chinese word “De”, due to their high co-occurrence frequency in the training data. If the dropped pronoun were recovered, “that” would have been aligned with the dropped-pro (cf. Figure 3), which is a much more sensible alignment. Figure 1: Example of incorrect word alignment due to missing pronouns on the Chinese side. In order to account for certain language phenomena such as pro-drop and wh-movement, a set of special tokens, called empty categories (EC), are used in Penn Treebanks (Marcus et al., 1993; Bies and Maamouri, 2003; Xue et al., 2005). Since empty categories do not exist in the surface form of a language, they are often deemed elusive and recovering ECs is even figuratively called “chasing the ghost” (Yang and Xue, 2010). In this work we demonstrate that, with the availability of large-scale EC annotations, it is feasible to predict and recover ECs with high accuracy. More importantly, with various approaches of modeling the recovered ECs in SMT, we are able to achieve significant improvements1. The contributions of this paper include the following: • Propose a novel structured approach to EC prediction,</context>
</contexts>
<marker>Bies, Maamouri, 2003</marker>
<rawString>Ann Bies and Mohamed Maamouri. 2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Penn</author>
</authors>
<title>Arabic treebank guidelines.</title>
<journal>In</journal>
<volume>03</volume>
<pages>1--28</pages>
<marker>Penn, </marker>
<rawString>Penn Arabic treebank guidelines. In http://www.ircs.upenn.edu/arabic/Jan03release/ guidelines-TB-1-28-03.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu Cai</author>
<author>David Chiang</author>
<author>Yoav Goldberg</author>
</authors>
<title>Language-independent parsing with empty elements.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:short papers,</booktitle>
<pages>212--216</pages>
<contexts>
<context position="21365" citStr="Cai et al., 2011" startWordPosition="3743" endWordPosition="3746">The training set includes CTB files 0081 through 0900. The development set includes files 0041 to 0080, and the test set contains files 0001-0040 and 0901-0931. We merge all empty categories into a single type in the training data before training our EC prediction model. To compare the performance on system-generated parse trees, we also train a Berkeley parser on the same training data and parse the test set. The prediction accuracy for such single type on the test set with gold or system-generated parse trees is shown in Table 7, compared to the numbers reported in (Yang and Xue, 2010) and (Cai et al., 2011). The model we proposed achieves 6% higher F1 score than that in (Yang and Xue, 2010) and 2.6% higher than that in (Cai et al., 2011), which is significant. This shows the effectiveness of our structured approach. Model T P R F1 (Yang and Xue, 2010) G 95.9 83.0 89.0 Structured (this work) G 96.5 93.6 95.0 (Yang and Xue, 2010) S 80.3 52.1 63.2 (Cai et al., 2011) S 74.0 61.3 67.0 Structured (this work) S 74.9 65.1 69.6 Table 7: Comparison with the previous results, using the same training and test data. T: parse trees. G: gold parse trees. S: system-generated parse trees. P: precision. R: recall</context>
<context position="27788" citStr="Cai et al., 2011" startWordPosition="4823" endWordPosition="4826">odifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where each word in the sentence receives a tag indicating whether there is an EC before it. A maximum entropy model is utilized to predict the tags, but different types of ECs are not distinguished. In (Cai et al., 2011), a language-independent method was proposed to integrate the recovery of empty elements into syntactic parsing. As shown in the previous section, our model outperforms the model in (Yang and Xue, 2010) and (Cai et al., 2011) significantly using the same training and test data. (Luo and Zhao, 2011) also tries to predict the existence of an EC in Chinese sentences, but the ECs in the middle of a tree constituent are lumped into a single position and are not uniquely recoverable. There exists only a handful of previous work on applying ECs explicitly to machine translation so far. One of them is</context>
</contexts>
<marker>Cai, Chiang, Goldberg, 2011</marker>
<rawString>Shu Cai, David Chiang, and Yoav Goldberg. 2011. Language-independent parsing with empty elements. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:short papers, pages 212–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. In</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University.</institution>
<contexts>
<context position="22950" citStr="Chen and Goodman, 1998" startWordPosition="4018" endWordPosition="4021">2 million sentence pairs from the parallel corpora released by 827 LDC over the years, with the data from United Nations and Hong Kong excluded 2. The Chinese text is segmented with a segmenter trained on the CTB data using conditional random field (CRF), followed by the longest-substring match segmentation in a second pass. Our language model (LM) training data consists of about 10 billion English words, which includes Gigaword and other newswire and web data released by LDC, as well as the English side of the parallel training corpus. We train a 6-gram LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Our tuning set for MT contains 1275 sentences from LDC2010E30. We test our system on the NIST MT08 Newswire (691 sentences) and Weblog (666 sentences) sets. Both tuning and test sets have 4 sets of references for each sentence. The MT systems are optimized with pairwise ranking optimization (Hopkins and May, 2011) to maximize BLEU (Papineni et al., 2002). We first predict *pro* and *PRO* with our annotation model for all Chinese sentences in the parallel training data, with *pro* and *PRO* inserted between the original Chinese words. Then we run GIZA++ (Och and Ney, 2000) to generate the wor</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. In Technical Report TR-10-98, Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="13725" citStr="Chiang, 2005" startWordPosition="2419" endWordPosition="2420">nted Data With the pre-processed MT training corpus, an unsupervised word aligner, such as GIZA++, can be used to generate automatic word alignment, as the first step of a system training pipeline. The effect of inserting ECs is two-fold: first, it can impact the automatic word alignment since now it allows the target-side words, especially the function words, to align to the inserted ECs and fix some errors in the original word alignment; second, new phrases and rules can be extracted from the preprocessed training data. For example, for a hierarchical MT system, some phrase pairs and Hiero (Chiang, 2005) rules can be extracted with recovered *pro* and *PRO* at the Chinese side. In this work we also take advantages of the augmented Chinese parse trees (with ECs projected to the surface) and extract tree-to-string grammar (Liu et al., 2006) for a tree-to-string MT system. Due to the recovered ECs in the source parse trees, the tree-to-string grammar extracted from such trees can be more discriminative, with an increased capability of distinguishing different context. An example of an augmented Chinese parse tree aligned to an English string is shown in Figure 3, in which the incorrect alignment</context>
<context position="22115" citStr="Chiang, 2005" startWordPosition="3878" endWordPosition="3879">ich is significant. This shows the effectiveness of our structured approach. Model T P R F1 (Yang and Xue, 2010) G 95.9 83.0 89.0 Structured (this work) G 96.5 93.6 95.0 (Yang and Xue, 2010) S 80.3 52.1 63.2 (Cai et al., 2011) S 74.0 61.3 67.0 Structured (this work) S 74.9 65.1 69.6 Table 7: Comparison with the previous results, using the same training and test data. T: parse trees. G: gold parse trees. S: system-generated parse trees. P: precision. R: recall. 4.2 MT Results In the Chinese-to-English MT experiments, we test two state-of-the-art MT systems. One is an reimplementation of Hiero (Chiang, 2005), and the other is a hybrid syntax-based tree-to-string system (Zhao and Al-onaizan, 2008), where normal phrase pairs and Hiero rules are used as a backoff for tree-to-string rules. The MT training data includes 2 million sentence pairs from the parallel corpora released by 827 LDC over the years, with the data from United Nations and Hong Kong excluded 2. The Chinese text is segmented with a segmenter trained on the CTB data using conditional random field (CRF), followed by the longest-substring match segmentation in a second pass. Our language model (LM) training data consists of about 10 bi</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Effects of empty categories on machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>636--645</pages>
<contexts>
<context position="12152" citStr="Chung and Gildea, 2010" startWordPosition="2140" endWordPosition="2143">ode 16 head word of the parent node 17 is the current node head child of its parent? EC Features 18 predicted EC of the left sibling 19 the set of predicted ECs of child nodes Table 2: List of features. 3 Integrating Empty Categories in Machine Translation In this section, we explore multiple approaches of utilizing recovered ECs in machine translation. 3.1 Explicit Recovery of ECs in MT We conducted some initial error analysis on our MT system output and found that most of the errors that are related to ECs are due to the missing *pro* and *PRO*. This is also consistent with the findings in (Chung and Gildea, 2010). One of the other frequent ECs, *OP*, appears in the Chinese relative clauses, which usually have a Chinese word “De” aligned to the target side “that” or “which”. And the trace, *T*, exists in both Chinese and English sides. For MT we want to focus on the places where there exist mismatches between the source and target languages. A straightforward way of utilizing the recovered *pro* and *PRO* is to pre-process the MT training and test data by inserting ECs into the original source text (i.e. Chinese in this case). As mentioned in the previous section, the output of our EC predictor is a ne</context>
<context position="28434" citStr="Chung and Gildea, 2010" startWordPosition="4935" endWordPosition="4938">t method was proposed to integrate the recovery of empty elements into syntactic parsing. As shown in the previous section, our model outperforms the model in (Yang and Xue, 2010) and (Cai et al., 2011) significantly using the same training and test data. (Luo and Zhao, 2011) also tries to predict the existence of an EC in Chinese sentences, but the ECs in the middle of a tree constituent are lumped into a single position and are not uniquely recoverable. There exists only a handful of previous work on applying ECs explicitly to machine translation so far. One of them is the work reported in (Chung and Gildea, 2010), where three approaches are compared, based on either pattern matching, CRF, or parsing. However, there is no comparison between using gold trees and automatic trees. There also exist a few major differences on the MT part between our work and theirs. First, in addition to the pre-processing of training data and inserting recovered empty categories, we implement sparse features to further boost the performance, and tune the feature weights directly towards maximizing the machine translation metric. Second, there is no discussion on the quality of word alignment in (Chung and Gildea, 2010), wh</context>
</contexts>
<marker>Chung, Gildea, 2010</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2010. Effects of empty categories on machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 636– 645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Dienes</author>
<author>Amit Dubey</author>
</authors>
<title>Deep syntactic processing by combining shallow methods.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26951" citStr="Dienes and Dubey, 2003" startWordPosition="4687" endWordPosition="4691"> in Table 12, which show the effectiveness of the recovered ECs in MT. System MT08-nw MT08-wb Hiero 33.99 25.40 +prediction 34.62* 25.63 +prediction+sparse 34.95* 25.80* Table 10: BLEU scores in the Hiero system. 828 System MT08-nw MT08-wb T2S+Hiero 34.53 25.80 +prediction 35.17* 26.08 +prediction+sparse 35.51* 26.53* Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang</context>
</contexts>
<marker>Dienes, Dubey, 2003</marker>
<rawString>Peter Dienes and Amit Dubey. 2003. Deep syntactic processing by combining shallow methods. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Gabbard</author>
<author>Seth Kulick</author>
<author>Mitchell Marcus</author>
</authors>
<title>Fully parsing the penn treebank.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</booktitle>
<contexts>
<context position="26974" citStr="Gabbard et al., 2006" startWordPosition="4692" endWordPosition="4695"> the effectiveness of the recovered ECs in MT. System MT08-nw MT08-wb Hiero 33.99 25.40 +prediction 34.62* 25.63 +prediction+sparse 34.95* 25.80* Table 10: BLEU scores in the Hiero system. 828 System MT08-nw MT08-wb T2S+Hiero 34.53 25.80 +prediction 35.17* 26.08 +prediction+sparse 35.51* 26.53* Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where </context>
</contexts>
<marker>Gabbard, Kulick, Marcus, 2006</marker>
<rawString>Ryan Gabbard, Seth Kulick, and Mitchell Marcus. 2006. Fully parsing the penn treebank. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Haifeng Wang</author>
<author>Josef van Genabith</author>
</authors>
<title>Recovering non-local dependencies for Chinese.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<marker>Guo, Wang, van Genabith, 2007</marker>
<rawString>Yuqing Guo, Haifeng Wang, and Josef van Genabith. 2007. Recovering non-local dependencies for Chinese. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<contexts>
<context position="23267" citStr="Hopkins and May, 2011" startWordPosition="4071" endWordPosition="4074">nd pass. Our language model (LM) training data consists of about 10 billion English words, which includes Gigaword and other newswire and web data released by LDC, as well as the English side of the parallel training corpus. We train a 6-gram LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Our tuning set for MT contains 1275 sentences from LDC2010E30. We test our system on the NIST MT08 Newswire (691 sentences) and Weblog (666 sentences) sets. Both tuning and test sets have 4 sets of references for each sentence. The MT systems are optimized with pairwise ranking optimization (Hopkins and May, 2011) to maximize BLEU (Papineni et al., 2002). We first predict *pro* and *PRO* with our annotation model for all Chinese sentences in the parallel training data, with *pro* and *PRO* inserted between the original Chinese words. Then we run GIZA++ (Och and Ney, 2000) to generate the word alignment for each direction and apply grow-diagonal-final (Koehn et al., 2003), same as in the baseline. We want to measure the impact on the word alignment, which is an important step for the system building. We append a 300-sentence set, which we have human hand alignment available as reference, to the 2M train</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>A simple pattern-matching algorithm for recovering empty nodes and their antecedents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26927" citStr="Johnson, 2002" startWordPosition="4685" endWordPosition="4686">mples are given in Table 12, which show the effectiveness of the recovered ECs in MT. System MT08-nw MT08-wb Hiero 33.99 25.40 +prediction 34.62* 25.63 +prediction+sparse 34.95* 25.80* Table 10: BLEU scores in the Hiero system. 828 System MT08-nw MT08-wb T2S+Hiero 34.53 25.80 +prediction 35.17* 26.08 +prediction+sparse 35.51* 26.53* Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a </context>
</contexts>
<marker>Johnson, 2002</marker>
<rawString>Mark Johnson. 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="23631" citStr="Koehn et al., 2003" startWordPosition="4131" endWordPosition="4134">0. We test our system on the NIST MT08 Newswire (691 sentences) and Weblog (666 sentences) sets. Both tuning and test sets have 4 sets of references for each sentence. The MT systems are optimized with pairwise ranking optimization (Hopkins and May, 2011) to maximize BLEU (Papineni et al., 2002). We first predict *pro* and *PRO* with our annotation model for all Chinese sentences in the parallel training data, with *pro* and *PRO* inserted between the original Chinese words. Then we run GIZA++ (Och and Ney, 2000) to generate the word alignment for each direction and apply grow-diagonal-final (Koehn et al., 2003), same as in the baseline. We want to measure the impact on the word alignment, which is an important step for the system building. We append a 300-sentence set, which we have human hand alignment available as reference, to the 2M training sentence pairs before running GIZA++. The alignment accuracy measured on this alignment test set, with or without *pro* and *PRO* inserted before running GIZA++, is shown in Table 8. To make a fair comparison with the baseline alignment, any target words aligned to ECs are deemed as unaligned during scoring. We observe 1.2% improvement on function word relat</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT-NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="13964" citStr="Liu et al., 2006" startWordPosition="2459" endWordPosition="2462">ld: first, it can impact the automatic word alignment since now it allows the target-side words, especially the function words, to align to the inserted ECs and fix some errors in the original word alignment; second, new phrases and rules can be extracted from the preprocessed training data. For example, for a hierarchical MT system, some phrase pairs and Hiero (Chiang, 2005) rules can be extracted with recovered *pro* and *PRO* at the Chinese side. In this work we also take advantages of the augmented Chinese parse trees (with ECs projected to the surface) and extract tree-to-string grammar (Liu et al., 2006) for a tree-to-string MT system. Due to the recovered ECs in the source parse trees, the tree-to-string grammar extracted from such trees can be more discriminative, with an increased capability of distinguishing different context. An example of an augmented Chinese parse tree aligned to an English string is shown in Figure 3, in which the incorrect alignment in Figure 1 is fixed. A few examples of the extracted Hiero rules and tree-to-string rules are also listed, which we would not have been able to extract from the original incorrect word alignment when the *pro* was missing. 3.3 Soft Recov</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Bing Zhao</author>
</authors>
<title>A statistical tree annotator and its applications.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1230--1238</pages>
<contexts>
<context position="5073" citStr="Luo and Zhao (2011)" startWordPosition="821" endWordPosition="824">ith their intended usages. Readers are referred to the documentation (Xue et al., 2005) of CTB for detailed discussions about the characterization of empty categories. EC Meaning *T* trace of A’-movement * trace of A-movement *PRO* big PRO in control structures *pro* pro-drop *OP* operator in relative clauses *RNR* for right node raising Table 1: List of empty categories in the CTB. In this section, we tackle the problem of recovering Chinese ECs. The problem has been studied before in the literature. For instance, Yang and Xue (2010) attempted to predict the existence of an EC before a word; Luo and Zhao (2011) predicted ECs on parse trees, but the position information of some ECs is partially lost in their representation. Furthermore, Luo and Zhao (2011) conducted experiments on gold parse trees only. In our opinion, recovering ECs from machine parse trees is more meaningful since that is what one would encounter when developing a downstream application such as machine translation. In this paper, we aim to have a more comprehensive treatment of the problem: all EC types along with their locations are predicted, and we will report the results on both human parse trees and machinegenerated parse tree</context>
<context position="28087" citStr="Luo and Zhao, 2011" startWordPosition="4873" endWordPosition="4876">ienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where each word in the sentence receives a tag indicating whether there is an EC before it. A maximum entropy model is utilized to predict the tags, but different types of ECs are not distinguished. In (Cai et al., 2011), a language-independent method was proposed to integrate the recovery of empty elements into syntactic parsing. As shown in the previous section, our model outperforms the model in (Yang and Xue, 2010) and (Cai et al., 2011) significantly using the same training and test data. (Luo and Zhao, 2011) also tries to predict the existence of an EC in Chinese sentences, but the ECs in the middle of a tree constituent are lumped into a single position and are not uniquely recoverable. There exists only a handful of previous work on applying ECs explicitly to machine translation so far. One of them is the work reported in (Chung and Gildea, 2010), where three approaches are compared, based on either pattern matching, CRF, or parsing. However, there is no comparison between using gold trees and automatic trees. There also exist a few major differences on the MT part between our work and theirs. </context>
</contexts>
<marker>Luo, Zhao, 2011</marker>
<rawString>Xiaoqiang Luo and Bing Zhao. 2011. A statistical tree annotator and its applications. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1230–1238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="2467" citStr="Marcus et al., 1993" startWordPosition="392" endWordPosition="395"> “that” is missing on the Chinese side. Consequently, “that” is incorrectly aligned to the second to the last Chinese word “De”, due to their high co-occurrence frequency in the training data. If the dropped pronoun were recovered, “that” would have been aligned with the dropped-pro (cf. Figure 3), which is a much more sensible alignment. Figure 1: Example of incorrect word alignment due to missing pronouns on the Chinese side. In order to account for certain language phenomena such as pro-drop and wh-movement, a set of special tokens, called empty categories (EC), are used in Penn Treebanks (Marcus et al., 1993; Bies and Maamouri, 2003; Xue et al., 2005). Since empty categories do not exist in the surface form of a language, they are often deemed elusive and recovering ECs is even figuratively called “chasing the ghost” (Yang and Xue, 2010). In this work we demonstrate that, with the availability of large-scale EC annotations, it is feasible to predict and recover ECs with high accuracy. More importantly, with various approaches of modeling the recovered ECs in SMT, we are able to achieve significant improvements1. The contributions of this paper include the following: • Propose a novel structured a</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics, volume 19(2), pages 313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="23530" citStr="Och and Ney, 2000" startWordPosition="4116" endWordPosition="4119">Ney smoothing (Chen and Goodman, 1998). Our tuning set for MT contains 1275 sentences from LDC2010E30. We test our system on the NIST MT08 Newswire (691 sentences) and Weblog (666 sentences) sets. Both tuning and test sets have 4 sets of references for each sentence. The MT systems are optimized with pairwise ranking optimization (Hopkins and May, 2011) to maximize BLEU (Papineni et al., 2002). We first predict *pro* and *PRO* with our annotation model for all Chinese sentences in the parallel training data, with *pro* and *PRO* inserted between the original Chinese words. Then we run GIZA++ (Och and Ney, 2000) to generate the word alignment for each direction and apply grow-diagonal-final (Koehn et al., 2003), same as in the baseline. We want to measure the impact on the word alignment, which is an important step for the system building. We append a 300-sentence set, which we have human hand alignment available as reference, to the 2M training sentence pairs before running GIZA++. The alignment accuracy measured on this alignment test set, with or without *pro* and *PRO* inserted before running GIZA++, is shown in Table 8. To make a fair comparison with the baseline alignment, any target words alig</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="23308" citStr="Papineni et al., 2002" startWordPosition="4078" endWordPosition="4081"> data consists of about 10 billion English words, which includes Gigaword and other newswire and web data released by LDC, as well as the English side of the parallel training corpus. We train a 6-gram LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Our tuning set for MT contains 1275 sentences from LDC2010E30. We test our system on the NIST MT08 Newswire (691 sentences) and Weblog (666 sentences) sets. Both tuning and test sets have 4 sets of references for each sentence. The MT systems are optimized with pairwise ranking optimization (Hopkins and May, 2011) to maximize BLEU (Papineni et al., 2002). We first predict *pro* and *PRO* with our annotation model for all Chinese sentences in the parallel training data, with *pro* and *PRO* inserted between the original Chinese words. Then we run GIZA++ (Och and Ney, 2000) to generate the word alignment for each direction and apply grow-diagonal-final (Koehn et al., 2003), same as in the baseline. We want to measure the impact on the word alignment, which is an important step for the system building. We append a 300-sentence set, which we have human hand alignment available as reference, to the 2M training sentence pairs before running GIZA++.</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="18501" citStr="Ratnaparkhi, 1997" startWordPosition="3226" endWordPosition="3228">1 score for this category, with limited number of missing or false positives. The F1 scores for majority of the ECs are above 70%, except for “*”, which is relatively rare in the data. For the two categories that are interesting to MT, *pro* and *PRO*, the predictor achieves 74.3% and 81.5% in F1 scores, respectively. The results reported above are based on the LDC gold parse trees. To apply the EC prediction to NLP applications, such as MT, it is impossible to always rely on the gold trees due to its limited availability. We parse our test set with a maximum entropy based statistical parser (Ratnaparkhi, 1997) first. The parser accuracy is around 84% on the test set. Then we extract features based on the system-generated parse trees, and decode with the previously trained model. The results are shown in Table 5. Compared to those in Table 4, the F1 scores dropped by different degrees for dif826 Tag Ref Sys P R F1 NULL 75159 75508 99.3 99.7 99.5 *pro* 1692 1442 80.8 68.9 74.3 *PRO* 1410 1282 85.6 77.8 81.5 *T* 1851 1845 82.8 82.5 82.7 *OP* 1721 1853 90.9 97.9 94.2 *RNR* 51 39 87.2 66.7 75.6 * 156 96 63.5 39.1 48.4 Table 4: Prediction accuracy with gold parse trees, where NULL represents the cases wh</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of Second Conference on Empirical Methods in Natural Language Processing, pages 1– 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>In Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<pages>207--238</pages>
<contexts>
<context position="2511" citStr="Xue et al., 2005" startWordPosition="400" endWordPosition="403">uently, “that” is incorrectly aligned to the second to the last Chinese word “De”, due to their high co-occurrence frequency in the training data. If the dropped pronoun were recovered, “that” would have been aligned with the dropped-pro (cf. Figure 3), which is a much more sensible alignment. Figure 1: Example of incorrect word alignment due to missing pronouns on the Chinese side. In order to account for certain language phenomena such as pro-drop and wh-movement, a set of special tokens, called empty categories (EC), are used in Penn Treebanks (Marcus et al., 1993; Bies and Maamouri, 2003; Xue et al., 2005). Since empty categories do not exist in the surface form of a language, they are often deemed elusive and recovering ECs is even figuratively called “chasing the ghost” (Yang and Xue, 2010). In this work we demonstrate that, with the availability of large-scale EC annotations, it is feasible to predict and recover ECs with high accuracy. More importantly, with various approaches of modeling the recovered ECs in SMT, we are able to achieve significant improvements1. The contributions of this paper include the following: • Propose a novel structured approach to EC prediction, including the exac</context>
<context position="4541" citStr="Xue et al., 2005" startWordPosition="731" endWordPosition="734">d approach to EC prediction. In Section 3, we describe the integration of Chinese ECs in MT. The experimental results for both EC prediction and SMT are reported in Section 4. A survey on the related work is conducted in Section 5, and Section 6 summarizes the work and introduces some future work. 2 Chinese Empty Category Prediction The empty categories in the Chinese Treebank (CTB) include trace markers for A’- and Amovement, dropped pronoun, big PRO etc. A complete list of categories used in CTB is shown in Table 1 along with their intended usages. Readers are referred to the documentation (Xue et al., 2005) of CTB for detailed discussions about the characterization of empty categories. EC Meaning *T* trace of A’-movement * trace of A-movement *PRO* big PRO in control structures *pro* pro-drop *OP* operator in relative clauses *RNR* for right node raising Table 1: List of empty categories in the CTB. In this section, we tackle the problem of recovering Chinese ECs. The problem has been studied before in the literature. For instance, Yang and Xue (2010) attempted to predict the existence of an EC before a word; Luo and Zhao (2011) predicted ECs on parse trees, but the position information of some </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. In Natural Language Engineering, volume 11(2), pages 207– 238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaqin Yang</author>
<author>Nianwen Xue</author>
</authors>
<title>Chasing the ghost: Recovering empty categories in the Chinese Treebank.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1382--1390</pages>
<location>Beijing, China,</location>
<contexts>
<context position="2701" citStr="Yang and Xue, 2010" startWordPosition="433" endWordPosition="436">hat” would have been aligned with the dropped-pro (cf. Figure 3), which is a much more sensible alignment. Figure 1: Example of incorrect word alignment due to missing pronouns on the Chinese side. In order to account for certain language phenomena such as pro-drop and wh-movement, a set of special tokens, called empty categories (EC), are used in Penn Treebanks (Marcus et al., 1993; Bies and Maamouri, 2003; Xue et al., 2005). Since empty categories do not exist in the surface form of a language, they are often deemed elusive and recovering ECs is even figuratively called “chasing the ghost” (Yang and Xue, 2010). In this work we demonstrate that, with the availability of large-scale EC annotations, it is feasible to predict and recover ECs with high accuracy. More importantly, with various approaches of modeling the recovered ECs in SMT, we are able to achieve significant improvements1. The contributions of this paper include the following: • Propose a novel structured approach to EC prediction, including the exact word-level lo1Hence “Enlisting the ghost” in the title of this paper. 822 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 822–831, Sofia, Bul</context>
<context position="4994" citStr="Yang and Xue (2010)" startWordPosition="806" endWordPosition="809"> PRO etc. A complete list of categories used in CTB is shown in Table 1 along with their intended usages. Readers are referred to the documentation (Xue et al., 2005) of CTB for detailed discussions about the characterization of empty categories. EC Meaning *T* trace of A’-movement * trace of A-movement *PRO* big PRO in control structures *pro* pro-drop *OP* operator in relative clauses *RNR* for right node raising Table 1: List of empty categories in the CTB. In this section, we tackle the problem of recovering Chinese ECs. The problem has been studied before in the literature. For instance, Yang and Xue (2010) attempted to predict the existence of an EC before a word; Luo and Zhao (2011) predicted ECs on parse trees, but the position information of some ECs is partially lost in their representation. Furthermore, Luo and Zhao (2011) conducted experiments on gold parse trees only. In our opinion, recovering ECs from machine parse trees is more meaningful since that is what one would encounter when developing a downstream application such as machine translation. In this paper, we aim to have a more comprehensive treatment of the problem: all EC types along with their locations are predicted, and we wi</context>
<context position="7883" citStr="Yang and Xue, 2010" startWordPosition="1332" endWordPosition="1335"> position is encoded by @1 since the deleted NP is the second child of the IP node (we use 0- based indices). With this transformation, we are able to recover not only the position of an EC, but its type as well. A special tag NULL is attached to non-terminal nodes without EC. Since an EC is introduced to express the structure of a sentence, it is a good practice to associate it with the syn823 Figure 2: Example of tree transformation on training data to encode an empty category and its position information. tactic tree, as opposed to simply attaching it to a neighboring word, as was done in (Yang and Xue, 2010). We believe this is one of the reasons why our model has better accuracy than that of (Yang and Xue, 2010) (cf. Table 7). In summary, a projected tag consists of an EC type (such as *pro*) and the EC’s position information. The problem of predicting ECs is then cast into predicting an EC tag at each non-terminal node. Notice that the input to such a predictor is a syntactic tree without ECs, e.g., the parse tree on the right hand of Figure 2 without the EC tag *pro*@1 is such an example. 2.2 A Structured Empty Category Model We propose a structured MaxEnt model for predicting ECs. Specially, </context>
<context position="20503" citStr="Yang and Xue, 2010" startWordPosition="3594" endWordPosition="3597">Cs in the training data except *pro* and *PRO*. So the model only predicts NULL, *pro* or *PRO*. The results on the test set are listed in Table 6. There is 0.8% and 0.5% increase on NULL and *pro*, respectively. The F1 score for *PRO* drops by 0.2% slightly. As mentioned earlier, for MT we focus on recovering *pro* and *PRO* only. The model generating the results in Table 6 is the one we applied in our MT experiments reported later. In order to compare to the state-of-the-art models to see where our model stands, we switch our training, development and test data to those used in the work of (Yang and Xue, 2010) and (Cai et Tag Precision Recall F1 NULL 98.5 98.9 98.7 *pro* 51.0 51.1 51.1 *PRO* 66.0 50.4 57.1 Table 6: Prediction accuracy with systemgenerated parse trees, modeling *pro* and *PRO* only. al., 2011), for the purpose of a direct comparison. The training set includes CTB files 0081 through 0900. The development set includes files 0041 to 0080, and the test set contains files 0001-0040 and 0901-0931. We merge all empty categories into a single type in the training data before training our EC prediction model. To compare the performance on system-generated parse trees, we also train a Berkele</context>
<context position="27566" citStr="Yang and Xue, 2010" startWordPosition="4783" endWordPosition="4786">2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where each word in the sentence receives a tag indicating whether there is an EC before it. A maximum entropy model is utilized to predict the tags, but different types of ECs are not distinguished. In (Cai et al., 2011), a language-independent method was proposed to integrate the recovery of empty elements into syntactic parsing. As shown in the previous section, our model outperforms the model in (Yang and Xue, 2010) and (Cai et al., 2011) significantly using the same training and test data. (Luo and Zhao, 2011) also tries to predict the existence of an EC in Chinese sentences, but the ECs</context>
</contexts>
<marker>Yang, Xue, 2010</marker>
<rawString>Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost: Recovering empty categories in the Chinese Treebank. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1382–1390, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Yaser Al-onaizan</author>
</authors>
<title>Generalizing local and non-local word-reordering patterns for syntax-based machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>572--581</pages>
<contexts>
<context position="22205" citStr="Zhao and Al-onaizan, 2008" startWordPosition="3890" endWordPosition="3893">Model T P R F1 (Yang and Xue, 2010) G 95.9 83.0 89.0 Structured (this work) G 96.5 93.6 95.0 (Yang and Xue, 2010) S 80.3 52.1 63.2 (Cai et al., 2011) S 74.0 61.3 67.0 Structured (this work) S 74.9 65.1 69.6 Table 7: Comparison with the previous results, using the same training and test data. T: parse trees. G: gold parse trees. S: system-generated parse trees. P: precision. R: recall. 4.2 MT Results In the Chinese-to-English MT experiments, we test two state-of-the-art MT systems. One is an reimplementation of Hiero (Chiang, 2005), and the other is a hybrid syntax-based tree-to-string system (Zhao and Al-onaizan, 2008), where normal phrase pairs and Hiero rules are used as a backoff for tree-to-string rules. The MT training data includes 2 million sentence pairs from the parallel corpora released by 827 LDC over the years, with the data from United Nations and Hong Kong excluded 2. The Chinese text is segmented with a segmenter trained on the CTB data using conditional random field (CRF), followed by the longest-substring match segmentation in a second pass. Our language model (LM) training data consists of about 10 billion English words, which includes Gigaword and other newswire and web data released by L</context>
</contexts>
<marker>Zhao, Al-onaizan, 2008</marker>
<rawString>Bing Zhao and Yaser Al-onaizan. 2008. Generalizing local and non-local word-reordering patterns for syntax-based machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 572–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shanheng Zhao</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Identification and resolution of Chinese zero pronouns: A machine learning approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="27262" citStr="Zhao and Ng, 2007" startWordPosition="4734" endWordPosition="4737">.53* Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where each word in the sentence receives a tag indicating whether there is an EC before it. A maximum entropy model is utilized to predict the tags, but different types of ECs are not distinguished. In (Cai et al., 2011), a language-independent method was proposed to integrate the recovery of </context>
</contexts>
<marker>Zhao, Ng, 2007</marker>
<rawString>Shanheng Zhao and Hwee Tou Ng. 2007. Identification and resolution of Chinese zero pronouns: A machine learning approach. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>