<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000050">
<title confidence="0.987011">
Mildly Context-Sensitive Dependency Languages
</title>
<author confidence="0.900137">
Marco Kuhlmann
</author>
<affiliation confidence="0.776741333333333">
Programming Systems Lab
Saarland University
Saarbrücken, Germany
</affiliation>
<email confidence="0.989103">
kuhlmann@ps.uni-sb.de
</email>
<author confidence="0.787962">
Mathias Möhl
</author>
<affiliation confidence="0.746205333333333">
Programming Systems Lab
Saarland University
Saarbrücken, Germany
</affiliation>
<email confidence="0.995676">
mmohl@ps.uni-sb.de
</email>
<sectionHeader confidence="0.995599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999748777777778">
Dependency-based representations of natu-
ral language syntax require a fine balance
between structural flexibility and computa-
tional complexity. In previous work, several
constraints have been proposed to identify
classes of dependency structures that are well-
balanced in this sense; the best-known but
also most restrictive of these is projectivity.
Most constraints are formulated on fully spec-
ified structures, which makes them hard to in-
tegrate into models where structures are com-
posed from lexical information. In this paper,
we show how two empirically relevant relax-
ations of projectivity can be lexicalized, and
how combining the resulting lexicons with a
regular means of syntactic composition gives
rise to a hierarchy of mildly context-sensitive
dependency languages.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996380723404256">
Syntactic representations based on word-to-word de-
pendencies have a long tradition in descriptive lin-
guistics. Lately, they have also been used in many
computational tasks, such as relation extraction (Cu-
lotta and Sorensen, 2004), parsing (McDonald et al.,
2005), and machine translation (Quirk et al., 2005).
Especially in recent work on parsing, there is a par-
ticular interest in non-projective dependency struc-
tures, in which a word and its dependents may be
spread out over a discontinuous region of the sen-
tence. These structures naturally arise in the syntactic
analysis of languages with flexible word order, such
160
as Czech (Veselá et al., 2004). Unfortunately, most
formal results on non-projectivity are discouraging:
While grammar-driven dependency parsers that are
restricted to projective structures can be as efficient
as parsers for lexicalized context-free grammar (Eis-
ner and Satta, 1999), parsing is prohibitively expen-
sive when unrestricted forms of non-projectivity are
permitted (Neuhaus and Bröker, 1997). Data-driven
dependency parsing with non-projective structures is
quadratic when all attachment decisions are assumed
to be independent of one another (McDonald et al.,
2005), but becomes intractable when this assumption
is abandoned (McDonald and Pereira, 2006).
In search of a balance between structural flexibility
and computational complexity, several authors have
proposed constraints to identify classes of non-projec-
tive dependency structures that are computationally
well-behaved (Bodirsky et al., 2005; Nivre, 2006).
In this paper, we focus on two of these proposals:
the gap-degree restriction, which puts a bound on
the number of discontinuities in the region of a sen-
tence covered by a word and its dependents, and the
well-nestedness condition, which constrains the ar-
rangement of dependency subtrees. Both constraints
have been shown to be in very good fit with data from
dependency treebanks (Kuhlmann and Nivre, 2006).
However, like all other such proposals, they are for-
mulated on fully specified structures, which makes it
hard to integrate them into a generative model, where
dependency structures are composed from elemen-
tary units of lexicalized information. Consequently,
little is known about the generative capacity and com-
putational complexity of languages over restricted
non-projective dependency structures.
</bodyText>
<note confidence="0.9026995">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 160–167,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99990014893617">
Contents of the paper In this paper, we show how
the gap-degree restriction and the well-nestedness
condition can be captured in dependency lexicons,
and how combining such lexicons with a regular
means of syntactic composition gives rise to an infi-
nite hierarchy of mildly context-sensitive languages.
The technical key to these results is a procedure
to encode arbitrary, even non-projective dependency
structures into trees (terms) over a signature of local
order-annotations. The constructors of these trees
can be read as lexical entries, and both the gap-de-
gree restriction and the well-nestedness condition
can be couched as syntactic properties of these en-
tries. Sets of gap-restricted dependency structures
can be described using regular tree grammars. This
gives rise to a notion of regular dependency lan-
guages, and allows us to establish a formal relation
between the structural constraints and mildly con-
text-sensitive grammar formalisms (Joshi, 1985): We
show that regular dependency languages correspond
to the sets of derivations of lexicalized Linear Con-
text-Free Rewriting Systems (lcfrs) (Vijay-Shanker
et al., 1987), and that the gap-degree measure is the
structural correspondent of the concept of ‘fan-out’
in this formalism (Satta, 1992). We also show that
adding the well-nestedness condition corresponds
to the restriction of lcfrs to Coupled Context-Free
Grammars (Hotz and Pitsch, 1996), and that regu-
lar sets of well-nested structures with a gap-degree
of at most 1 are exactly the class of sets of deriva-
tions of Lexicalized Tree Adjoining Grammar (ltag).
This result generalizes previous work on the relation
between ltag and dependency representations (Ram-
bow and Joshi, 1997; Bodirsky et al., 2005).
Structure of the paper The remainder of this pa-
per is structured as follows. Section 2 contains some
basic notions related to trees and dependency struc-
tures. In Section 3 we present the encoding of depen-
dency structures as order-annotated trees, and show
how this encoding allows us to give a lexicalized re-
formulation of both the gap-degree restriction and the
well-nestedness condition. Section 4 introduces the
notion of regular dependency languages. In Section 5
we show how different combinations of restrictions
on non-projectivity in these languages correspond
to different mildly context-sensitive grammar for-
malisms. Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.965834" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.999874">
Throughout the paper, we write [n] for the set of all
positive natural numbers up to and including n. The
set of all strings over a set A is denoted by A*, the
empty string is denoted by &amp;quot;, and the concatenation
of two strings x and y is denoted either by xy, or,
where this is ambiguous, by x • y.
</bodyText>
<subsectionHeader confidence="0.97741">
2.1 Trees
</subsectionHeader>
<bodyText confidence="0.999086857142857">
In this paper, we regard trees as terms. We expect the
reader to be familiar with the basic concepts related
to this framework, and only introduce our particular
notation. Let E be a set of labels. The set of (finite,
unranked) trees over E is defined recursively by the
equation TE := { a(x) I a E E, x E TE* }. The set
of nodes of a tree t E TE is defined as
</bodyText>
<equation confidence="0.940355">
N(a(t1 • • • t,,)) := {&amp;quot;} U { iu I i E [n], u E N(ti) }:
</equation>
<bodyText confidence="0.999875">
For two nodes u, v E N(t), we say that u governs v,
and write u a v, if v can be written as v = ux, for
some sequence x E ICY*. Note that the governance
relation is both reflexive and transitive. The converse
of government is called dependency, so u a v can
also be read as ‘v depends on u’. The yield of a
node u E N(t), cul, is the set of all dependents of u
in t: cul := { v E N(t) I u a v }. We also use the
notations t(u) for the label at the node u of t, and
t=u for the subtree of t rooted at u. A tree language
over E is a subset of TE.
</bodyText>
<subsectionHeader confidence="0.999335">
2.2 Dependency structures
</subsectionHeader>
<bodyText confidence="0.9990639375">
For the purposes of this paper, a dependency structure
over E is a pair d = (t, x), where t E TE is a tree,
and x is a list of the nodes in t. We write DE to
refer to the set of all dependency structures over E.
Independently of the governance relation in d, the
list x defines a total order on the nodes in t; we
write u -&lt; v to denote that u precedes v in this order.
Note that, like governance, the precedence relation is
both reflexive and transitive. A dependency language
over E is a subset of DE.
Example. The left half of Figure 1 shows how we
visualize dependency structures: circles represent
nodes, arrows represent the relation of (immediate)
governance, the left-to-right order of the nodes repre-
sents their order in the precedence relation, and the
dotted lines indicate the labelling. ❑
</bodyText>
<page confidence="0.987291">
161
</page>
<figure confidence="0.989467272727273">
a b c d e f
(a; 012)
(b; 01)
(d; 10)
(c; 0)
tive dependency structures into order-annotated trees,
an
d for reversing this encoding.
(e; 01)
162 We now present procedures for encoding projec-
(f; 0)
</figure>
<figureCaption confidence="0.999965">
Figure 1: A projective dependency structure
</figureCaption>
<bodyText confidence="0.9381246">
:=
• 0. If u = vi
for some i E N (that is, if u is an inner node), we
also update the order annotation of the parent v of u
through the assignment
</bodyText>
<equation confidence="0.998544666666667">
w(u)
w(u)
w(v) := w(v) • i.
</equation>
<bodyText confidence="0.97636847826087">
sponds to a yield
decomposes into the singleton
interval [u, u], and the collection of the intervals that
correspond to the yields of the immediate dependents
of u. To reconstruct the global precedence relation,
it suffices to annotate each node u with the relative
precedences among the constituent parts of its yield.
We represent this
order as a string over the
alphabet
where the symbol 0 represents the sin-
gleton interval [u, u], and a symbol i # 0 represents
the interval that corresponds to the yield of the
direct dependent of u. An order-annotated tree is a
tree labelled with pairs (Q, w), where Q is the label
proper, and w is a local order annotation. In what
follows, we will use the functional notations Q(u)
and
to refer to the label and order annotation
of u, respectively.
EXAMPLE. Figure 1 shows a projective dependency
structure together with its representation as an
LuJ
</bodyText>
<equation confidence="0.953030857142857">
‘local’
N0,
ith
w(u)
order-
annotated tree. ❑
lin(u)
• • • in)
= lin0(u,w(u))lin0(u,i1
= lin00(u, i1) • • • lin00(u, in)
i) = if i = 0 then u else li
lin00(u,
n(ui)
ee.
</equation>
<sectionHeader confidence="0.902668" genericHeader="method">
3 Lexicalizing the precedence relation
</sectionHeader>
<bodyText confidence="0.999977428571428">
In this section, we show how the precedence relation
of dependency structures can be encoded as, and
decoded from, a collection of node-specific order
annotations. Under the assumption that the nodes of
a dependency structure correspond to lexemic units,
this result demonstrates how word-order information
can be captured in a dependency lexicon.
</bodyText>
<subsectionHeader confidence="0.999469">
3.1 Projective structures
</subsectionHeader>
<bodyText confidence="0.998386125">
Lexicalizing the precedence relation of a dependency
structure is particularly easy if the structure under
consideration meets the condition of projectivity. A
dependency structure is projective, if each of its
yields forms an interval with respect to the prece-
dence order (Kuhlmann and Nivre, 2006).
In a projective structure, the interval that corre-
Encoding The representation of a projective depen-
dency structure (t, x) as an order-annotated tree can
be computed in a single left-to-right sweep over x.
Starting with a copy of the tree t in which every
node is annotated with the empty string, for each new
node u in x, we update the order annotation of u
through the assignment
Decoding To decode an order-annotated tree t, we
first linearize the nodes of t into a sequence x, and
then remove all order annotations. Linearization pro-
ceeds in a way that is very close to a pre-order traver-
sal of the tree, except that the relative position of
the root node of a subtree is explicitly specified in
the order annotation. Specifically, to linearize an or-
der-annotated tree, we look into the local order
annotated at the root node of the tree, and concatenate
the linearizations of its constituent part
</bodyText>
<equation confidence="0.6898275">
w(u)
s. A symbol i
</equation>
<bodyText confidence="0.918277375">
in w(u) represents either the singleton interval [u, u]
(i = 0), or the interval corresponding to some direct
dependent ui of u (i # 0), in which case we pro-
ceed recursively. Formally, the linearization of u is
captured by the following three equations:
Both encoding and decoding can be done in time
linear in the number of nodes of the dependency
structure or order-annotated tr
</bodyText>
<subsectionHeader confidence="0.984439">
3.2 Non-projective structures
</subsectionHeader>
<bodyText confidence="0.968849777777778">
Itis straightforward to see that our representation of
dependency structures is insufficient if the str
uctures
under consideration are non-projective. To witness,
consider the structure shown in Figure 2. Encoding
this structure using the procedure presented above
yields the same order-annotated tree as the one shown
in Figure 1, which demonstrates that the encoding is
not reversible.
</bodyText>
<equation confidence="0.985619">
(a; (01212))
(e, (0, 1))
(f; (0))
</equation>
<figureCaption confidence="0.999435">
Figure 2: A non-projective dependency structure
</figureCaption>
<bodyText confidence="0.996912285714286">
Blocks In a non-projective dependency structure,
the yield of a node may be spread out over more than
one interval; we will refer to these intervals as blocks.
Two nodes v, w belong to the same block of a node u,
if all nodes between v and w are governed by u.
EXAMPLE. Consider the nodes b, c, d in the struc-
tures depicted in Figures 1 and 2. In Figure 1, these
nodes belong to the same block of b. In Figure 2,
the three nodes are spread out over two blocks of b
(marked by the boxes): c and d are separated by a
node (e) not governed by b. ❑
Blocks have a recursive structure that is closely re-
lated to the recursive structure of yields: the blocks of
a node u can be decomposed into the singleton [u, u],
and the blocks of the direct dependents of u. Just as
a projective dependency structure can be represented
by annotating each yield with an order on its con-
stituents, an unrestricted structure can be represented
by annotating each block.
Extended order annotations To represent orders
on blocks, we extend our annotation scheme as fol-
lows. First, instead of a single string, an annotation
!(u) now is a tuple of strings, where the kth com-
ponent specifies the order among the constituents of
the kth block of u. Second, instead of one, the an-
notation may now contain multiple occurrences of
the same dependent; the kth occurrence of i in !(u)
represents the kth block of the node ui.
We write !(u)k to refer to the kth component of
the order annotation of u. We also use the notation
(i#k)u to refer to the kth occurrence of i in !(u),
and omit the subscript when the node u is implicit.
EXAMPLE. In the annotated tree shown in Figure 2,
!(b)1 = (0#1)(1#1), and !(b)2 = (1#2). ❑
Encoding To encode a dependency structure (t, x)
as an extended order-annotated tree, we do a post-
order traversal of t as follows. For a given node u, let
us represent a constituent of a block of u as a triple
i : [vl, v,], where i denotes the node that contributes
the constituent, and vl and v, denote the constituent’s
leftmost and rightmost elements. At each node u, we
have access to the singleton block 0 : [u, u], and the
constituent blocks of the immediate dependents of u.
We say that two blocks i : [vl, v,], j : [wl, w,] can
be merged, if the node v, immediately precedes the
node wl. The result of the merger is a new block ij :
[vl, w,] that represents the information that the two
merged constituents belong to the same block of u.
By exhaustive merging, we obtain the constituent
structure of all blocks of u. From this structure, we
can read off the order annotation !(u).
EXAMPLE. The yield of the node b in Figure 2 de-
composes into 0 : [b, b], 1 : [c, c], and 1 : [d, d].
Since b and c are adjacent, the first two of these con-
stituents can be merged into a new block 01 : [b, c];
the third constituent remains unchanged. This gives
rise to the order annotation (01,1) for b. ❑
When using a global data-structure to keep track
of the constituent blocks, the encoding procedure can
be implemented to run in time linear in the number
of blocks in the dependency structure. In particular,
for projective dependency structures, it still runs in
time linear in the number of nodes.
Decoding To linearize the kth block of a node u,
we look into the kth component of the order anno-
tated at u, and concatenate the linearizations of its
constituent parts. Each occurrence (i#k) in a com-
ponent of !(u) represents either the node u itself
(i = 0), or the kth block of some direct dependent ui
of u (i # 0), in which case we proceed recursively:
</bodyText>
<equation confidence="0.998963666666667">
lin(u, k) = lin0(u, !(u)k)
lin0(u, i1 • • • in) = lin00(u, i1) • • • lin00(u, in)
lin00(u, (i#k)u) = if i = 0 then u else lin(ui, k)
</equation>
<bodyText confidence="0.98983525">
The root node of a dependency structure has only
one block. Therefore, to linearize a tree t, we only
need to linearize the first block of the tree’s root node:
lin(t) = lin(s, 1).
</bodyText>
<figure confidence="0.896548875">
e
a
f
d
b c
(b, (01, 1))
(d, (1, 0))
(c; (0))
</figure>
<page confidence="0.996267">
163
</page>
<bodyText confidence="0.941601444444444">
Consistent order annotations Every dependency
structure over E can be encoded as a tree over the set
E x SL, where SL is the set of all order annotations.
The converse of this statement does not hold: to be
interpretable as a dependency structure, tree structure
and order annotation in an order-annotated tree must
be consistent, in the following sense.
PROPERTY C1: Every annotation co(u) in a tree t
contains all and only the symbols in the collection
{0} U { i I ui E N(t) }, i.e., one symbol for u, and
one symbol for every direct dependent of u.
PROPERTY C2: The number of occurrences of a
symbol i # 0 in co(u) is identical to the number of
components in the annotation of the node ui. Further-
more, the number of components in the annotation
of the root node is 1.
With this notion of consistency, we can prove the
following technical result about the relation between
dependency structures and annotated trees. We write
7rz(s) for the tree obtained from a tree s E Tzxg
by re-labelling every node u with a(u).
PROPOSITION 1. For every dependency structure
(t, x) over E, there exists a tree s over E x SL such
that 7rz(s) = t and lin(s) = x. Conversely, for
every consistently order-annotated tree s E Tzxg,
there exists a uniquely determined dependency struc-
ture (t, x) with these properties. ❑
</bodyText>
<subsectionHeader confidence="0.997789">
3.3 Local versions of structural constraints
</subsectionHeader>
<bodyText confidence="0.987852068965517">
The encoding of dependency structures as order-an-
notated trees allows us to reformulate two constraints
on non-projectivity originally defined on fully speci-
fied dependency structures (Bodirsky et al., 2005) in
terms of syntactic properties of the order annotations
that they induce:
Gap-degree The gap-degree of a dependency
structure is the maximum over the number of dis-
continuities in any yield of that structure.
EXAMPLE. The structure depicted in Figure 2 has
gap-degree 1: the yield of b has one discontinuity,
marked by the node e, and this is the maximal number
of discontinuities in any yield of the structure. ❑
Since a discontinuity in a yield is delimited by two
blocks, and since the number of blocks of a node u
equals the number of components in the order anno-
tation of u, the following result is obvious:
PROPOSITION 2. A dependency structure has gap-de-
gree k if and only if the maximal number of compo-
nents among the annotations co(u) is k + 1. ❑
In particular, a dependency structure is projective iff
all of its annotations consist of just one component.
Well-nestedness The well-nestedness condition
constrains the arrangement of subtrees in a depen-
dency structure. Two subtrees t/u1, t/u2 interleave,
if there are nodes v1l , v,1. E t/u1 and v2l , v2 � E t/u2
such that v1l -&lt; v2l -&lt; v1� -&lt; v2. A dependency struc-
ture is well-nested, if no two of its disjoint subtrees
interleave. We can prove the following result:
</bodyText>
<figureCaption confidence="0.6375145">
PROPOSITION 3. A dependency structure is well-
nested if and only if no annotation co(u) contains
</figureCaption>
<bodyText confidence="0.864818">
a substring i • • • j • • • i • • • j, for i, j E N. ❑
EXAMPLE. The dependency structure in Figure 1 is
well-nested, the structure depicted in Figure 2 is not:
the subtrees rooted at the nodes b and e interleave.
To see this, notice that b -&lt; e -&lt; d -&lt; f . Also notice
that co(a) contains the substring 1212. ❑
</bodyText>
<sectionHeader confidence="0.966161" genericHeader="method">
4 Regular dependency languages
</sectionHeader>
<bodyText confidence="0.999903272727273">
The encoding of dependency structures as order-an-
notated trees gives rise to an encoding of dependency
languages as tree languages. More specifically, de-
pendency languages over a set E can be encoded
as tree languages over the set E x SL, where SL is
the set of all order annotations. Via this encoding,
we can study dependency languages using the tools
and results of the well-developed formal theory of
tree languages. In this section, we discuss depen-
dency languages that can be encoded as regular tree
languages.
</bodyText>
<subsectionHeader confidence="0.997413">
4.1 Regular tree grammars
</subsectionHeader>
<bodyText confidence="0.999955181818182">
The class of regular tree languages, REGT for short,
is a very natural class with many characterizations
(Gécseg and Steinby, 1997): it is generated by regular
tree grammars, recognized by finite tree automata,
and expressible in monadic second-order logic. Here
we use the characterization in terms of grammars.
Regular tree grammars are natural candidates for the
formalization of dependency lexicons, as each rule
in such a grammar can be seen as the specification of
a word and the syntactic categories or grammatical
functions of its immediate dependents.
</bodyText>
<page confidence="0.996843">
164
</page>
<bodyText confidence="0.998617444444444">
Formally, a (normalized) regular tree grammar is
a construct G D (NG, EG, SG, PG), in which NG
and EG are finite sets of non-terminal and termi-
nal symbols, respectively, SG 2 NG is a dedicated
start symbol, and PG is a finite set of productions
of the form A ! a(A1 • • • An), where a 2 EG,
A 2 NG, and Ai 2 NG, for every i 2 [n]. The (di-
rect) derivation relation associated to G is the binary
relation )G on the set T˙GUNG defined as follows:
</bodyText>
<equation confidence="0.9681775">
t 2 T˙GUNG tlu D A (A ! s) 2 PG
t )G t[u 7! s]
</equation>
<bodyText confidence="0.991729666666667">
Informally, each step in a derivation replaces a non-
terminal-labelled leaf by the right-hand side of a
matching production. The tree language generated
by G is the set of all terminal trees that can eventu-
ally be derived from the trivial tree formed by its start
symbol: L(G) D ft 2 T˙G j SG )G t g.
</bodyText>
<subsectionHeader confidence="0.996769">
4.2 Regular dependency grammars
</subsectionHeader>
<bodyText confidence="0.968604074074074">
We call a dependency language regular, if its encod-
ing as a set of trees over E x SL forms a regular tree
language, and write REGD for the class of all regular
dependency languages. For every regular dependency
language L, there is a regular tree grammar with ter-
minal alphabet E x SL that generates the encoding
of L. Similar to the situation with individual struc-
tures, the converse of this statement does not hold:
the consistency properties mentioned above impose
corresponding syntactic restrictions on the rules of
grammars G that generate the encoding of L.
PROPERTY C1&apos;: The !-component of every pro-
duction A ! ha, !i(A1 • • • An) in G contains all and
only symbols in the set f0g [ f i j i 2 [n] g.
PROPERTY C2&apos;: For every non-terminal X 2 NG,
there is a uniquely determined integer dX such that
for every production A ! ha, !i(A1 • • • An) in G,
dAi gives the number of occurrences of i in !, dA
gives the number of components in !, and dSG D 1.
It turns out that these properties are in fact sufficient
to characterize the class of regular tree grammars that
generate encodings of dependency languages. In but
slight abuse of terminology, we will refer to such
grammars as regular dependency grammars.
EXAMPLE. Figure 3 shows a regular tree grammar
that generates a set of non-projective dependency
structures with string language f anbn j n &gt; 1g. ❑
</bodyText>
<figure confidence="0.875013666666667">
S ! ha, h01ii(B) j ha, h0121ii(A, B)
A ! ha, h0, 1ii(B) j ha, h01, 21ii(A, B)
B ! hb, h0ii
</figure>
<figureCaption confidence="0.998929">
Figure 3: A grammar for a language in REGD(1)
</figureCaption>
<sectionHeader confidence="0.404911" genericHeader="conclusions">
5 Structural constraints and formal power
</sectionHeader>
<bodyText confidence="0.999782">
In this section, we present our results on the genera-
tive capacity of regular dependency languages, link-
ing them to a large class of mildly context-sensitive
grammar formalisms.
</bodyText>
<subsectionHeader confidence="0.996228">
5.1 Gap-restricted dependency languages
</subsectionHeader>
<bodyText confidence="0.9999753">
A dependency language L is called gap-restricted, if
there is a constant cL &gt; 0 such that no structure in L
has a gap-degree higher than cL. It is plain to see that
every regular dependency language is gap-restricted:
the gap-degree of a structure is directly reflected in
the number of components of its order annotations,
and every regular dependency grammar makes use of
only a finite number of these annotations. We write
REGD(k) to refer to the class of regular dependency
languages with a gap-degree bounded by k.
</bodyText>
<subsectionHeader confidence="0.697166">
Linear Context-Free Rewriting Systems Gap-re-
</subsectionHeader>
<bodyText confidence="0.997872466666667">
stricted dependency languages are closely related
to Linear Context-Free Rewriting Systems (LCFRS)
(Vijay-Shanker et al., 1987), a class of formal sys-
tems that generalizes several mildly context-sensitive
grammar formalisms. An LCFRS consists of a regular
tree grammar G and an interpretation of the terminal
symbols of this grammar as linear, non-erasing func-
tions into tuples of strings. By these functions, each
tree in L(G) can be evaluated to a string.
EXAMPLE. Here is an example for a function:
f (hx11, x21i,hx12i) D hax11, x12x21i
This function states that in order to compute the pair
of strings that corresponds to a tree whose root node
is labelled with the symbol f , one first has to com-
pute the pair of strings corresponding to the first child
</bodyText>
<figure confidence="0.999763714285714">
a
a a b b b
B
B
B
A
A
</figure>
<page confidence="0.990688">
165
</page>
<bodyText confidence="0.999127825396825">
of the root node ((x11; x21)) and the single string cor- 5.2 Well-nested dependency languages
responding to the second child ((x12)), and then con- The absence of the substring i • • • j • • • i • • • j in the
catenate the individual components in the specified order annotations of well-nested dependency struc-
order, preceded by the terminal symbol a. ❑ tures corresponds to a restriction to ‘well-bracketed’
We call a function lexicalized, if it contributes ex- compositions of sub-structures. This restriction is
actly one terminal symbol. In an LcFRS in which all central to the formalism of Coupled-Context-Free
functions are lexicalized, there is a one-to-one cor- Grammar (ccFG) (Hotz and Pitsch, 1996).
respondence between the nodes in an evaluated tree It is straightforward to see that every ccFG can
and the positions in the string that the tree evaluates be translated into an equivalent LcFRS. We can also
to. Therefore, tree and string implicitly form a depen- prove that every LcFRS obtained from a regular depen-
dency structure, and we can speak of the dependency dency grammar with well-nested order annotations
language generated by a lexicalized LcFRS. can be translated back into an equivalent ccFG. We
Equivalence We can prove that every regular de- write REGDwn.k/ for the well-nested subclass of
pendency grammar can be transformed into a lexi- REGD.k/, and LCCFL.k/ for the class of all depen-
calized LcFRS that generates the same dependency dency languages generated by lexicalized ccFGs with
language, and vice versa. The basic insight in this a fan-out of at most k.
proof is that every order annotation in a regular de- PROPOSITION 5. REGDwn.k/ = LCCFL.k + 1/ ❑
pendency grammar can be interpreted as a compact As a special case, Coupled-Context-Free Grammars
description of a function in the corresponding LcFRS. with fan-out 2 are equivalent to Tree Adjoining Gram-
The number of components in the order-annotation, mars (TAGs) (Hotz and Pitsch, 1996). This enables
and hence, the gap-degree of the resulting depen- us to generalize a previous result on the class of de-
dency language, corresponds to the fan-out of the pendency structures generated by lexicalized TAGs
function: the highest number of components among (Bodirsky et al., 2005) to the class of generated de-
the arguments of the function (Satta, 1992).1 A tech- pendency languages, LTAL.
nical difficulty is caused by the fact that LcFRS can PROPOSITION 6. REGDwn.1/ = LTAL ❑
swap components: f .(x11; x21)/ = (ax21; x11). This 6 Conclusion
commutativity needs to be compiled out during the In this paper, we have presented a lexicalized refor-
translation into a regular dependency grammar. mulation of two structural constraints on non-pro-
We write LLCFRL.k/ for the class of all depen- jective dependency representations, and shown that
dency languages generated by lexicalized LcFRS with combining dependency lexicons that satisfy these
a fan-out of at most k. constraints with a regular means of syntactic com-
PROPOSITION 4. REGD.k/ = LLCFRL.k + 1/ ❑ position yields classes of mildly context-sensitive
In particular, the class REGD.0/ of regular depen- dependency languages. Our results make a signif-
dency languages over projective structures is exactly icant contribution to a better understanding of the
the class of dependency languages generated by lexi- relation between the phenomenon of non-projectivity
calized context-free grammars. and notions of formal power.
EXAMPLE. The gap-degree of the language generated The close link between restricted forms of non-
by the grammar in Figure 3 is bounded by 1. The projective dependency languages and mildly context-
rules for the non-terminal A can be translated into sensitive grammar formalisms provides a promising
the following functions of an equivalent LcFRS: starting point for future work. On the practical side,
it should allow us to benefit from the experience
in building parsers for mildly context-sensitive for-
malisms when addressing the task of efficient non-
projective dependency parsing, at least in the frame-
f(a;(0;1)).(x11)/ = (a; x11)
f(a;(01;21)).(x11; x21); (x12)/ = (ax11; x12x21)
The fan-out of these functions is 2. ❑
1More precisely, gap-degree = fan-out — 1.
166
work of grammar-driven parsing. This may even-
tually lead to a better trade-off between structural
flexibility and computational efficiency than that ob-
tained with current systems. On a more theoretical
level, our results provide a basis for comparing a va-
riety of formally rather distinct grammar formalisms
with respect to the sets of dependency structures that
they can generate. Such a comparison may be empir-
ically more adequate than one based on traditional
notions of generative capacity (Kallmeyer, 2006).
Acknowledgements We thank Guido Tack, Stefan
Thater, and the anonymous reviewers of this paper
for their detailed comments. The work of the authors
is funded by the German Research Foundation.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999423">
Manuel Bodirsky, Marco Kuhlmann, and Mathias Möhl.
2005. Well-nested drawings as models of syntactic
structure. In Tenth Conference on Formal Grammar
and Ninth Meeting on Mathematics of Language, Edin-
burgh, Scotland, UK.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 423–429, Barcelona, Spain.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and head automa-
ton grammars. In 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
457–464, College Park, Maryland, USA.
Ferenc Gécseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, volume 3,
pages 1–68. Springer-Verlag, New York, USA.
Günter Hotz and Gisela Pitsch. 1996. On parsing coupled-
context-free languages. Theoretical Computer Science,
161:205–233.
Aravind K. Joshi. 1985. Tree adjoining grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In David R. Dowty, Lauri
Karttunen, and Arnold M. Zwicky, editors, Natural Lan-
guage Parsing, pages 206–250. Cambridge University
Press, Cambridge, UK.
Laura Kallmeyer. 2006. Comparing lexicalized grammar
formalisms in an empirically adequate way: The notion
of generative attachment capacity. In International
Conference on Linguistic Evidence, pages 154–156,
Tübingen, Germany.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics (COLING-ACL) Main Conference Poster Ses-
sions, pages 507–514, Sydney, Australia.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 81–88, Trento, Italy.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajiˇc. 2005. Non-projective dependency parsing using
spanning tree algorithms. In Human Language Technol-
ogy Conference (HLT) and Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 523–530, Vancouver, British Columbia, Canada.
Peter Neuhaus and Norbert Bröker. 1997. The complexity
of recognition of linguistically adequate dependency
grammars. In 35th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 337–343,
Madrid, Spain.
Joakim Nivre. 2006. Constraints on non-projective depen-
dency parsing. In Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 73–80, Trento, Italy.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically informed
phrasal smt. In 43rd Annual Meeting of the Association
for Computational Linguistics (ACL), pages 271–279,
Ann Arbor, USA.
Owen Rambow and Aravind K. Joshi. 1997. A for-
mal look at dependency grammars and phrase-structure
grammars. In Leo Wanner, editor, Recent Trends in
Meaning-Text Theory, volume 39 of Studies in Lan-
guage, Companion Series, pages 167–190. John Ben-
jamins, Amsterdam, The Netherlands.
Giorgio Satta. 1992. Recognition of linear context-free
rewriting systems. In 30th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
89–95, Newark, Delaware, USA.
Katerina Veselá, Jiˇri Havelka, and Eva Hajiˇcova. 2004.
Condition of projectivity in the underlying depen-
dency structures. In 20th International Conference on
Computational Linguistics (COLING), pages 289–295,
Geneva, Switzerland.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 104–111, Stanford, California, USA.
</reference>
<page confidence="0.997687">
167
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928202">
<title confidence="0.999941">Mildly Context-Sensitive Dependency Languages</title>
<author confidence="0.999992">Marco Kuhlmann</author>
<affiliation confidence="0.997745">Programming Systems Lab Saarland University</affiliation>
<address confidence="0.994596">Saarbrücken, Germany</address>
<email confidence="0.998371">kuhlmann@ps.uni-sb.de</email>
<author confidence="0.990928">Mathias Möhl</author>
<affiliation confidence="0.9887155">Programming Systems Lab Saarland University</affiliation>
<address confidence="0.983271">Saarbrücken, Germany</address>
<email confidence="0.999313">mmohl@ps.uni-sb.de</email>
<abstract confidence="0.999264105263158">Dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity. In previous work, several constraints have been proposed to identify classes of dependency structures that are wellbalanced in this sense; the best-known but also most restrictive of these is projectivity. Most constraints are formulated on fully specified structures, which makes them hard to integrate into models where structures are composed from lexical information. In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Manuel Bodirsky</author>
<author>Marco Kuhlmann</author>
<author>Mathias Möhl</author>
</authors>
<title>Well-nested drawings as models of syntactic structure.</title>
<date>2005</date>
<booktitle>In Tenth Conference on Formal Grammar and Ninth Meeting on Mathematics of Language,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="2587" citStr="Bodirsky et al., 2005" startWordPosition="358" endWordPosition="361">s prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. Both constraints have been shown to be in very good fit with data from dependency treebanks (Kuhlmann and Nivre, 2006). However, like all other such proposals, they are formulated on fully specified structures, which makes it hard to integrate them into a generative model, where dependency stru</context>
<context position="5330" citStr="Bodirsky et al., 2005" startWordPosition="770" endWordPosition="773">Vijay-Shanker et al., 1987), and that the gap-degree measure is the structural correspondent of the concept of ‘fan-out’ in this formalism (Satta, 1992). We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996), and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag). This result generalizes previous work on the relation between ltag and dependency representations (Rambow and Joshi, 1997; Bodirsky et al., 2005). Structure of the paper The remainder of this paper is structured as follows. Section 2 contains some basic notions related to trees and dependency structures. In Section 3 we present the encoding of dependency structures as order-annotated trees, and show how this encoding allows us to give a lexicalized reformulation of both the gap-degree restriction and the well-nestedness condition. Section 4 introduces the notion of regular dependency languages. In Section 5 we show how different combinations of restrictions on non-projectivity in these languages correspond to different mildly context-s</context>
<context position="17558" citStr="Bodirsky et al., 2005" startWordPosition="3016" endWordPosition="3019"> for the tree obtained from a tree s E Tzxg by re-labelling every node u with a(u). PROPOSITION 1. For every dependency structure (t, x) over E, there exists a tree s over E x SL such that 7rz(s) = t and lin(s) = x. Conversely, for every consistently order-annotated tree s E Tzxg, there exists a uniquely determined dependency structure (t, x) with these properties. ❑ 3.3 Local versions of structural constraints The encoding of dependency structures as order-annotated trees allows us to reformulate two constraints on non-projectivity originally defined on fully specified dependency structures (Bodirsky et al., 2005) in terms of syntactic properties of the order annotations that they induce: Gap-degree The gap-degree of a dependency structure is the maximum over the number of discontinuities in any yield of that structure. EXAMPLE. The structure depicted in Figure 2 has gap-degree 1: the yield of b has one discontinuity, marked by the node e, and this is the maximal number of discontinuities in any yield of the structure. ❑ Since a discontinuity in a yield is delimited by two blocks, and since the number of blocks of a node u equals the number of components in the order annotation of u, the following resu</context>
<context position="26532" citStr="Bodirsky et al., 2005" startWordPosition="4593" endWordPosition="4596"> PROPOSITION 5. REGDwn.k/ = LCCFL.k + 1/ ❑ pendency grammar can be interpreted as a compact As a special case, Coupled-Context-Free Grammars description of a function in the corresponding LcFRS. with fan-out 2 are equivalent to Tree Adjoining GramThe number of components in the order-annotation, mars (TAGs) (Hotz and Pitsch, 1996). This enables and hence, the gap-degree of the resulting depen- us to generalize a previous result on the class of dedency language, corresponds to the fan-out of the pendency structures generated by lexicalized TAGs function: the highest number of components among (Bodirsky et al., 2005) to the class of generated dethe arguments of the function (Satta, 1992).1 A tech- pendency languages, LTAL. nical difficulty is caused by the fact that LcFRS can PROPOSITION 6. REGDwn.1/ = LTAL ❑ swap components: f .(x11; x21)/ = (ax21; x11). This 6 Conclusion commutativity needs to be compiled out during the In this paper, we have presented a lexicalized refortranslation into a regular dependency grammar. mulation of two structural constraints on non-proWe write LLCFRL.k/ for the class of all depen- jective dependency representations, and shown that dency languages generated by lexicalized L</context>
</contexts>
<marker>Bodirsky, Kuhlmann, Möhl, 2005</marker>
<rawString>Manuel Bodirsky, Marco Kuhlmann, and Mathias Möhl. 2005. Well-nested drawings as models of syntactic structure. In Tenth Conference on Formal Grammar and Ninth Meeting on Mathematics of Language, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>423--429</pages>
<location>Barcelona,</location>
<contexts>
<context position="1277" citStr="Culotta and Sorensen, 2004" startWordPosition="170" endWordPosition="174">fied structures, which makes them hard to integrate into models where structures are composed from lexical information. In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages. 1 Introduction Syntactic representations based on word-to-word dependencies have a long tradition in descriptive linguistics. Lately, they have also been used in many computational tasks, such as relation extraction (Culotta and Sorensen, 2004), parsing (McDonald et al., 2005), and machine translation (Quirk et al., 2005). Especially in recent work on parsing, there is a particular interest in non-projective dependency structures, in which a word and its dependents may be spread out over a discontinuous region of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such 160 as Czech (Veselá et al., 2004). Unfortunately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as effi</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In 42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 423–429, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>457--464</pages>
<location>College Park, Maryland, USA.</location>
<contexts>
<context position="1955" citStr="Eisner and Satta, 1999" startWordPosition="273" endWordPosition="277">ion (Quirk et al., 2005). Especially in recent work on parsing, there is a particular interest in non-projective dependency structures, in which a word and its dependents may be spread out over a discontinuous region of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such 160 as Czech (Veselá et al., 2004). Unfortunately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999), parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally wel</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In 37th Annual Meeting of the Association for Computational Linguistics (ACL), pages 457–464, College Park, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc Gécseg</author>
<author>Magnus Steinby</author>
</authors>
<title>Tree languages.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>1--68</pages>
<publisher>Springer-Verlag,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="19901" citStr="Gécseg and Steinby, 1997" startWordPosition="3436" endWordPosition="3439">notated trees gives rise to an encoding of dependency languages as tree languages. More specifically, dependency languages over a set E can be encoded as tree languages over the set E x SL, where SL is the set of all order annotations. Via this encoding, we can study dependency languages using the tools and results of the well-developed formal theory of tree languages. In this section, we discuss dependency languages that can be encoded as regular tree languages. 4.1 Regular tree grammars The class of regular tree languages, REGT for short, is a very natural class with many characterizations (Gécseg and Steinby, 1997): it is generated by regular tree grammars, recognized by finite tree automata, and expressible in monadic second-order logic. Here we use the characterization in terms of grammars. Regular tree grammars are natural candidates for the formalization of dependency lexicons, as each rule in such a grammar can be seen as the specification of a word and the syntactic categories or grammatical functions of its immediate dependents. 164 Formally, a (normalized) regular tree grammar is a construct G D (NG, EG, SG, PG), in which NG and EG are finite sets of non-terminal and terminal symbols, respective</context>
</contexts>
<marker>Gécseg, Steinby, 1997</marker>
<rawString>Ferenc Gécseg and Magnus Steinby. 1997. Tree languages. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages, volume 3, pages 1–68. Springer-Verlag, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Günter Hotz</author>
<author>Gisela Pitsch</author>
</authors>
<title>On parsing coupledcontext-free languages.</title>
<date>1996</date>
<journal>Theoretical Computer Science,</journal>
<pages>161--205</pages>
<contexts>
<context position="5013" citStr="Hotz and Pitsch, 1996" startWordPosition="718" endWordPosition="721">f regular dependency languages, and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms (Joshi, 1985): We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al., 1987), and that the gap-degree measure is the structural correspondent of the concept of ‘fan-out’ in this formalism (Satta, 1992). We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996), and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag). This result generalizes previous work on the relation between ltag and dependency representations (Rambow and Joshi, 1997; Bodirsky et al., 2005). Structure of the paper The remainder of this paper is structured as follows. Section 2 contains some basic notions related to trees and dependency structures. In Section 3 we present the encoding of dependency structures as order-annotated trees, and show how this encoding allows </context>
<context position="24974" citStr="Hotz and Pitsch, 1996" startWordPosition="4340" endWordPosition="4343">guages responding to the second child ((x12)), and then con- The absence of the substring i • • • j • • • i • • • j in the catenate the individual components in the specified order annotations of well-nested dependency strucorder, preceded by the terminal symbol a. ❑ tures corresponds to a restriction to ‘well-bracketed’ We call a function lexicalized, if it contributes ex- compositions of sub-structures. This restriction is actly one terminal symbol. In an LcFRS in which all central to the formalism of Coupled-Context-Free functions are lexicalized, there is a one-to-one cor- Grammar (ccFG) (Hotz and Pitsch, 1996). respondence between the nodes in an evaluated tree It is straightforward to see that every ccFG can and the positions in the string that the tree evaluates be translated into an equivalent LcFRS. We can also to. Therefore, tree and string implicitly form a depen- prove that every LcFRS obtained from a regular dependency structure, and we can speak of the dependency dency grammar with well-nested order annotations language generated by a lexicalized LcFRS. can be translated back into an equivalent ccFG. We Equivalence We can prove that every regular de- write REGDwn.k/ for the well-nested sub</context>
<context position="26242" citStr="Hotz and Pitsch, 1996" startWordPosition="4547" endWordPosition="4550">nto a lexi- REGD.k/, and LCCFL.k/ for the class of all depencalized LcFRS that generates the same dependency dency languages generated by lexicalized ccFGs with language, and vice versa. The basic insight in this a fan-out of at most k. proof is that every order annotation in a regular de- PROPOSITION 5. REGDwn.k/ = LCCFL.k + 1/ ❑ pendency grammar can be interpreted as a compact As a special case, Coupled-Context-Free Grammars description of a function in the corresponding LcFRS. with fan-out 2 are equivalent to Tree Adjoining GramThe number of components in the order-annotation, mars (TAGs) (Hotz and Pitsch, 1996). This enables and hence, the gap-degree of the resulting depen- us to generalize a previous result on the class of dedency language, corresponds to the fan-out of the pendency structures generated by lexicalized TAGs function: the highest number of components among (Bodirsky et al., 2005) to the class of generated dethe arguments of the function (Satta, 1992).1 A tech- pendency languages, LTAL. nical difficulty is caused by the fact that LcFRS can PROPOSITION 6. REGDwn.1/ = LTAL ❑ swap components: f .(x11; x21)/ = (ax21; x11). This 6 Conclusion commutativity needs to be compiled out during th</context>
</contexts>
<marker>Hotz, Pitsch, 1996</marker>
<rawString>Günter Hotz and Gisela Pitsch. 1996. On parsing coupledcontext-free languages. Theoretical Computer Science, 161:205–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions? In</title>
<date>1985</date>
<booktitle>Natural Language Parsing,</booktitle>
<pages>206--250</pages>
<editor>David R. Dowty, Lauri Karttunen, and Arnold M. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="4564" citStr="Joshi, 1985" startWordPosition="655" endWordPosition="656"> to encode arbitrary, even non-projective dependency structures into trees (terms) over a signature of local order-annotations. The constructors of these trees can be read as lexical entries, and both the gap-degree restriction and the well-nestedness condition can be couched as syntactic properties of these entries. Sets of gap-restricted dependency structures can be described using regular tree grammars. This gives rise to a notion of regular dependency languages, and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms (Joshi, 1985): We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al., 1987), and that the gap-degree measure is the structural correspondent of the concept of ‘fan-out’ in this formalism (Satta, 1992). We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996), and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoi</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Aravind K. Joshi. 1985. Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions? In David R. Dowty, Lauri Karttunen, and Arnold M. Zwicky, editors, Natural Language Parsing, pages 206–250. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
</authors>
<title>Comparing lexicalized grammar formalisms in an empirically adequate way: The notion of generative attachment capacity.</title>
<date>2006</date>
<booktitle>In International Conference on Linguistic Evidence,</booktitle>
<pages>154--156</pages>
<location>Tübingen, Germany.</location>
<marker>Kallmeyer, 2006</marker>
<rawString>Laura Kallmeyer. 2006. Comparing lexicalized grammar formalisms in an empirically adequate way: The notion of generative attachment capacity. In International Conference on Linguistic Evidence, pages 154–156, Tübingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Mildly nonprojective dependency structures.</title>
<date>2006</date>
<booktitle>In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL) Main Conference Poster Sessions,</booktitle>
<pages>507--514</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3010" citStr="Kuhlmann and Nivre, 2006" startWordPosition="428" endWordPosition="431">exibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. Both constraints have been shown to be in very good fit with data from dependency treebanks (Kuhlmann and Nivre, 2006). However, like all other such proposals, they are formulated on fully specified structures, which makes it hard to integrate them into a generative model, where dependency structures are composed from elementary units of lexicalized information. Consequently, little is known about the generative capacity and computational complexity of languages over restricted non-projective dependency structures. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 160–167, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics Contents o</context>
<context position="10296" citStr="Kuhlmann and Nivre, 2006" startWordPosition="1691" endWordPosition="1694">tion of dependency structures can be encoded as, and decoded from, a collection of node-specific order annotations. Under the assumption that the nodes of a dependency structure correspond to lexemic units, this result demonstrates how word-order information can be captured in a dependency lexicon. 3.1 Projective structures Lexicalizing the precedence relation of a dependency structure is particularly easy if the structure under consideration meets the condition of projectivity. A dependency structure is projective, if each of its yields forms an interval with respect to the precedence order (Kuhlmann and Nivre, 2006). In a projective structure, the interval that correEncoding The representation of a projective dependency structure (t, x) as an order-annotated tree can be computed in a single left-to-right sweep over x. Starting with a copy of the tree t in which every node is annotated with the empty string, for each new node u in x, we update the order annotation of u through the assignment Decoding To decode an order-annotated tree t, we first linearize the nodes of t into a sequence x, and then remove all order annotations. Linearization proceeds in a way that is very close to a pre-order traversal of </context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Marco Kuhlmann and Joakim Nivre. 2006. Mildly nonprojective dependency structures. In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL) Main Conference Poster Sessions, pages 507–514, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>81--88</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="2339" citStr="McDonald and Pereira, 2006" startWordPosition="326" endWordPosition="329">nately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999), parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. Both constraints have been shown to be in very </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 81–88, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>523--530</pages>
<location>Vancouver, British Columbia, Canada.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–530, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Neuhaus</author>
<author>Norbert Bröker</author>
</authors>
<title>The complexity of recognition of linguistically adequate dependency grammars.</title>
<date>1997</date>
<booktitle>In 35th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>337--343</pages>
<location>Madrid,</location>
<contexts>
<context position="2076" citStr="Neuhaus and Bröker, 1997" startWordPosition="290" endWordPosition="293">ency structures, in which a word and its dependents may be spread out over a discontinuous region of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such 160 as Czech (Veselá et al., 2004). Unfortunately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999), parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restric</context>
</contexts>
<marker>Neuhaus, Bröker, 1997</marker>
<rawString>Peter Neuhaus and Norbert Bröker. 1997. The complexity of recognition of linguistically adequate dependency grammars. In 35th Annual Meeting of the Association for Computational Linguistics (ACL), pages 337–343, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Constraints on non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>73--80</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="2601" citStr="Nivre, 2006" startWordPosition="362" endWordPosition="363">ve when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. Both constraints have been shown to be in very good fit with data from dependency treebanks (Kuhlmann and Nivre, 2006). However, like all other such proposals, they are formulated on fully specified structures, which makes it hard to integrate them into a generative model, where dependency structures are com</context>
<context position="10296" citStr="Nivre, 2006" startWordPosition="1693" endWordPosition="1694">dency structures can be encoded as, and decoded from, a collection of node-specific order annotations. Under the assumption that the nodes of a dependency structure correspond to lexemic units, this result demonstrates how word-order information can be captured in a dependency lexicon. 3.1 Projective structures Lexicalizing the precedence relation of a dependency structure is particularly easy if the structure under consideration meets the condition of projectivity. A dependency structure is projective, if each of its yields forms an interval with respect to the precedence order (Kuhlmann and Nivre, 2006). In a projective structure, the interval that correEncoding The representation of a projective dependency structure (t, x) as an order-annotated tree can be computed in a single left-to-right sweep over x. Starting with a copy of the tree t in which every node is annotated with the empty string, for each new node u in x, we update the order annotation of u through the assignment Decoding To decode an order-annotated tree t, we first linearize the nodes of t into a sequence x, and then remove all order annotations. Linearization proceeds in a way that is very close to a pre-order traversal of </context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Constraints on non-projective dependency parsing. In Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 73–80, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1356" citStr="Quirk et al., 2005" startWordPosition="183" endWordPosition="186">mposed from lexical information. In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages. 1 Introduction Syntactic representations based on word-to-word dependencies have a long tradition in descriptive linguistics. Lately, they have also been used in many computational tasks, such as relation extraction (Culotta and Sorensen, 2004), parsing (McDonald et al., 2005), and machine translation (Quirk et al., 2005). Especially in recent work on parsing, there is a particular interest in non-projective dependency structures, in which a word and its dependents may be spread out over a discontinuous region of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such 160 as Czech (Veselá et al., 2004). Unfortunately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999),</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 271–279, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Aravind K Joshi</author>
</authors>
<title>A formal look at dependency grammars and phrase-structure grammars.</title>
<date>1997</date>
<booktitle>Recent Trends in Meaning-Text Theory,</booktitle>
<volume>39</volume>
<pages>167--190</pages>
<editor>In Leo Wanner, editor,</editor>
<publisher>John Benjamins,</publisher>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="5306" citStr="Rambow and Joshi, 1997" startWordPosition="765" endWordPosition="769">riting Systems (lcfrs) (Vijay-Shanker et al., 1987), and that the gap-degree measure is the structural correspondent of the concept of ‘fan-out’ in this formalism (Satta, 1992). We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996), and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag). This result generalizes previous work on the relation between ltag and dependency representations (Rambow and Joshi, 1997; Bodirsky et al., 2005). Structure of the paper The remainder of this paper is structured as follows. Section 2 contains some basic notions related to trees and dependency structures. In Section 3 we present the encoding of dependency structures as order-annotated trees, and show how this encoding allows us to give a lexicalized reformulation of both the gap-degree restriction and the well-nestedness condition. Section 4 introduces the notion of regular dependency languages. In Section 5 we show how different combinations of restrictions on non-projectivity in these languages correspond to di</context>
</contexts>
<marker>Rambow, Joshi, 1997</marker>
<rawString>Owen Rambow and Aravind K. Joshi. 1997. A formal look at dependency grammars and phrase-structure grammars. In Leo Wanner, editor, Recent Trends in Meaning-Text Theory, volume 39 of Studies in Language, Companion Series, pages 167–190. John Benjamins, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Satta</author>
</authors>
<title>Recognition of linear context-free rewriting systems.</title>
<date>1992</date>
<booktitle>In 30th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>89--95</pages>
<location>Newark, Delaware, USA.</location>
<contexts>
<context position="4860" citStr="Satta, 1992" startWordPosition="698" endWordPosition="699">ties of these entries. Sets of gap-restricted dependency structures can be described using regular tree grammars. This gives rise to a notion of regular dependency languages, and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms (Joshi, 1985): We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al., 1987), and that the gap-degree measure is the structural correspondent of the concept of ‘fan-out’ in this formalism (Satta, 1992). We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996), and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag). This result generalizes previous work on the relation between ltag and dependency representations (Rambow and Joshi, 1997; Bodirsky et al., 2005). Structure of the paper The remainder of this paper is structured as follows. Section 2 contains some basic notions related to tr</context>
<context position="26604" citStr="Satta, 1992" startWordPosition="4608" endWordPosition="4609">a compact As a special case, Coupled-Context-Free Grammars description of a function in the corresponding LcFRS. with fan-out 2 are equivalent to Tree Adjoining GramThe number of components in the order-annotation, mars (TAGs) (Hotz and Pitsch, 1996). This enables and hence, the gap-degree of the resulting depen- us to generalize a previous result on the class of dedency language, corresponds to the fan-out of the pendency structures generated by lexicalized TAGs function: the highest number of components among (Bodirsky et al., 2005) to the class of generated dethe arguments of the function (Satta, 1992).1 A tech- pendency languages, LTAL. nical difficulty is caused by the fact that LcFRS can PROPOSITION 6. REGDwn.1/ = LTAL ❑ swap components: f .(x11; x21)/ = (ax21; x11). This 6 Conclusion commutativity needs to be compiled out during the In this paper, we have presented a lexicalized refortranslation into a regular dependency grammar. mulation of two structural constraints on non-proWe write LLCFRL.k/ for the class of all depen- jective dependency representations, and shown that dency languages generated by lexicalized LcFRS with combining dependency lexicons that satisfy these a fan-out of </context>
</contexts>
<marker>Satta, 1992</marker>
<rawString>Giorgio Satta. 1992. Recognition of linear context-free rewriting systems. In 30th Annual Meeting of the Association for Computational Linguistics (ACL), pages 89–95, Newark, Delaware, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina Veselá</author>
<author>Jiˇri Havelka</author>
<author>Eva Hajiˇcova</author>
</authors>
<title>Condition of projectivity in the underlying dependency structures.</title>
<date>2004</date>
<booktitle>In 20th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>289--295</pages>
<location>Geneva, Switzerland.</location>
<marker>Veselá, Havelka, Hajiˇcova, 2004</marker>
<rawString>Katerina Veselá, Jiˇri Havelka, and Eva Hajiˇcova. 2004. Condition of projectivity in the underlying dependency structures. In 20th International Conference on Computational Linguistics (COLING), pages 289–295, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In 25th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>104--111</pages>
<location>Stanford, California, USA.</location>
<contexts>
<context position="4735" citStr="Vijay-Shanker et al., 1987" startWordPosition="677" endWordPosition="680">ees can be read as lexical entries, and both the gap-degree restriction and the well-nestedness condition can be couched as syntactic properties of these entries. Sets of gap-restricted dependency structures can be described using regular tree grammars. This gives rise to a notion of regular dependency languages, and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms (Joshi, 1985): We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al., 1987), and that the gap-degree measure is the structural correspondent of the concept of ‘fan-out’ in this formalism (Satta, 1992). We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996), and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag). This result generalizes previous work on the relation between ltag and dependency representations (Rambow and Joshi, 1997; Bodirsky et al., 2005). Str</context>
<context position="23607" citStr="Vijay-Shanker et al., 1987" startWordPosition="4101" endWordPosition="4104">constant cL &gt; 0 such that no structure in L has a gap-degree higher than cL. It is plain to see that every regular dependency language is gap-restricted: the gap-degree of a structure is directly reflected in the number of components of its order annotations, and every regular dependency grammar makes use of only a finite number of these annotations. We write REGD(k) to refer to the class of regular dependency languages with a gap-degree bounded by k. Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms. An LCFRS consists of a regular tree grammar G and an interpretation of the terminal symbols of this grammar as linear, non-erasing functions into tuples of strings. By these functions, each tree in L(G) can be evaluated to a string. EXAMPLE. Here is an example for a function: f (hx11, x21i,hx12i) D hax11, x12x21i This function states that in order to compute the pair of strings that corresponds to a tree whose root node is labelled with the symbol f , one first has to compute the pair of strings c</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In 25th Annual Meeting of the Association for Computational Linguistics (ACL), pages 104–111, Stanford, California, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>