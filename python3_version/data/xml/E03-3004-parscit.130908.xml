<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.975728">
Focusing on Scenario Recognition in Information Extraction
</title>
<author confidence="0.776117">
Milena Yankova
</author>
<affiliation confidence="0.715444">
Linguistic Modelling Depai talent,
Central Laboratory for Parallel Processing,
Bulgarian Academy of Sciences,
</affiliation>
<address confidence="0.72777">
25A Acad. G. Bonchev Str.,
1113 Sofia, Bulgaria
</address>
<email confidence="0.997236">
myankova@lml.bas.bg
</email>
<author confidence="0.986208">
Svetla Boytcheva
</author>
<affiliation confidence="0.999535666666667">
Department of Information Technologies,
Faculty of Mathematics &amp; Informatics
Sofia University &amp;quot;St. Kl. Ohridski&amp;quot;,
</affiliation>
<address confidence="0.801195">
5 James Baurchier Str.,
1164 Sofia, Bulgaria
</address>
<email confidence="0.99831">
svetla@fmi.uni-sofia.bg
</email>
<sectionHeader confidence="0.996615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999754833333334">
This paper reports a research effort in In-
formation Extraction, especially in tem-
plate pattern matching. Our approach uses
reach domain knowledge in the football
(soccer) area and logical form representa-
tion for necessary inferences of facts and
templates filling. Our system FRET&apos;
(Football Reports Extraction Templates) is
compatible to the language-engineering
environment GATE and handles its internal
representations and some intermediate
analysis results.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999514">
An enormous amount of information exists in
natural language texts only but to analyse and pro-
cess this information automatically, it has to be
first distilled into a more structured form. Informa-
tion Extraction (1E) systems extract pieces of in-
formation by mapping natural language texts into
predefined structured representation - linguistic
patterns, usually sets of attribute-value pairs. Some
of the attribute-value pairs are to be filled in by
results from morphological analysis, named-
entities recognition, and (partial) syntactic analy-
sis. These processes are relatively well studied and
most of the 1E systems report high precision and
recall. However, the semantic analysis - which in-
cludes building logical forms, recognition of refer-
</bodyText>
<footnote confidence="0.715367666666667">
1 This work is partially supported by the European
Commission via contract ICA1-2000-70016 &amp;quot;BIS-21
Centre of Excellence&amp;quot;
</footnote>
<bodyText confidence="0.999679444444445">
ences and template filling - is a complicated proc-
ess, which is still far from its ultimate solution.
This paper focuses on the semantic processing in
1E. Following the terminology established by the
Message Understanding Conferences (MUCs), we
shall call the specification of the particular events
or relations to be extracted SCENARIO and we
shall refer to the final, tabular output format of in-
formation extraction as TEMPLATE. The actual
structure of the templates used has varied from a
flat record structure at MUC-4 [9] to a more com-
plex object oriented definition which was used for
Tipster and MUC-5 [2], MUC-6 [7] and MUC-7
[3]. Once filled, templates represent an extract of
key information from the text [12]. Extracted in-
formation can be stored in databases for various
purposes such as text indexing, information high-
lighting, data mining, natural language summarisa-
tion, etc.
Different systems provide different approaches
for solving semantic problems in IE. The
CRYSTAL system [11], for example, is based on
machine-learning covering algorithm for building
expected rules for template filling. Large hand-
marked training corpus is needed. But the domain
is quite static - weather forecast - with explicitly
fully expressed information. The system creates a
formal representation of the text that is equivalent
to related database entries.
Another Information Extraction system is SMES
[10], which does not have semantic analysis im-
plemented in it. Fragments extracted by a lexically
driven parser are attached to anchors - lexical en-
tries (mainly verbs). If successful, the set of found
fragments together with the anchor build up an
instantiated template. Filling templates strongly
</bodyText>
<page confidence="0.998983">
41
</page>
<bodyText confidence="0.99940825">
depends on the words and relations between them,
as they appear in the text.
In our approach we use the IE system GATE 2.1
beta 1 (GATE - General Architecture for Text En-
gineering) [4], which provides lexical analysis,
named entity recognition, coreference resolution
and other NLP modules. The system has been used
for many language-processing projects; in particu-
lar for Information Extraction in several languages.
In this paper we present work in progress, aim-
ing at the implementation of the system FRET
(Football Reports Extraction Templates). FRET
provides syntactic analysis and template (scenario)
pattern matching from English text. The innova-
tive aspect in our considerations is the relative
weight of the semantic analysis, since we use logi-
cal forms, a lexical knowledge base and certain
inference to match text and templates.
The paper is organised as follows: section 2 pre-
sents a short overview of JE as a whole and some
difficulties with performing subtasks in the chosen
domain. Section 3 describes the structure of the
data resource bank integrated in FRET. Section 4
discuses our approach in translation to logical
form. Section 5 describes in details the templates&apos;
structure. Section 6 explains the algorithm for fill-
ing templates with information from the text. Sec-
tion 7 contains the conclusion.
</bodyText>
<sectionHeader confidence="0.789782" genericHeader="method">
2 Information extraction
</sectionHeader>
<bodyText confidence="0.996783314285714">
IE can be divided into the following subtasks [6]:
Lexical Analysis, which turns a text into a se-
quence of sentences, each of them is a sequence
of lexical items (tokens). Usually sentences are
not marked, so special techniques are required
to recognise sentence boundaries. Each token is
looked up in the dictionary to determine its pos-
sible features and part-of-speech types.
Named Entity (NE) Recognition, which takes
a sequence of lexical items and tries to identify
reliably determinable structures using a set of
regular expressions: proper names, locations,
organizations, dates, currency amounts and etc.
The max score result reported in MUC-3 [8]
trough MUC-7 in this task is f-measure &lt; 97%.
Cor efer en ce Resolution, which identifies dif-
ferent descriptions of the same entity in differ-
ent parts of a text (usually one-two neighbour
sentences). These descriptions are the ones
identified by NE recognition and their ana-
phoric references. The best result reported for
this task at MUC 3-7 is f-measure &lt; 67,5%.
Syntactic Analysis, which provides some as-
pects of syntactic analysis and simplifies the
phase of fact extraction. The arguments to be
extracted often correspond to noun phrases in
the text, and relationships to grammatical func-
tional relations. Note that for IE we are only in-
terested in the grammatical relations relevant to
the template; correctly determining the other re-
lations may be a waste of time [6].
Template (Scenario) Pattern Matching,
which maps the syntactic structures to semantic
structures related to the templates to be filled in.
This stage extracts the events or relationships
relevant to the scenario. The max score result
reported for this task is f-measure &lt; 57%.
One of the most important questions is how to
recognise the scenario, which we are looking for.
For this purpose one specifies a template, as a se-
quence of slots some of which are marked as
obligatory and the others are optional. When the
required (marked) slots are filled in then we say
that the scenario is matched and the slots in the
template represent the wanted information from the
processed text. If the information in the processed
text is not enough to fill in the necessary slots, the
text does not correspond to the scenario.
The domain chosen for tuning and testing FRET
is football. The corpus is composed from BBC re-
ports about 31 matches of the Euro2000 champion-
ship. These texts have a specific text structure and
FRET&apos; s parser is tailored to cover it. Match reports
and comments have paragraph structure and pro-
vide rich temporal information.
Most often, the preferred research domains in IF
are fully informative with explicit statically ex-
pressed facts, where every statement is true at least
in the current text. Such domains are news articles,
telegraphic military messages, weather forecast
etc., which are used in MUC competitions. On the
other hand football reports are dynamic with no
assurance, that when once facts are declared they
will not be negated later. The needed information
sometimes is not fully provided and inferences are
required for extracting the implicitly expressed
facts. Tuning in a domain that allows frequent
changes even in terminology is also an important
and actual difficulty. Details about further prob-
lems in this domain are given below.
</bodyText>
<page confidence="0.995079">
42
</page>
<subsectionHeader confidence="0.578077">
2.1. Named entity recognition
</subsectionHeader>
<bodyText confidence="0.999672636363636">
First problem is NE recognition for proper
names, especially for foreign names. The players
from different nationalities have specific names
that can be out of the database for recognising
NEs. This is due to the limited list of predefined
names. It is impossible to collect all names for all
nationalities and distinct ways for transcribing for-
eign names. Another difficulty are nicknames of
the players, which are used in the text. Sometimes
players&apos; team numbers are used instead of person
names.
</bodyText>
<figure confidence="0.723124">
Example 1:
Ronaldo - soccer superstar; the
Phenomenon
Example 2:
Number nine scores.
</figure>
<subsectionHeader confidence="0.694056">
2.2. Coreference resolution
</subsectionHeader>
<bodyText confidence="0.9939748">
NE recognition problems described above con-
tribute to the coreference resolution problem. In-
stances of player&apos;s designation by metaphoric
description of their performance are more or less
unrecognizable.
</bodyText>
<construct confidence="0.3684565">
Example 3:
The brazilian superstar rediscov-
ered his enchanting mix of regal
majesty and youthful wonder.
</construct>
<bodyText confidence="0.9999492">
For finding metaphors it is necessary to have
explicit semantic description for each word (based
on meaning postulates, conceptual graphs etc.) to
recognize usage of words in a way different from
the traditional one. This is a huge time consuming
task because of the large amount of words existing
in the corpus texts. Correct metaphors recognition
is quite a hard task even for most of humans.
FRET uses the results of GATE, which performs
the first three 1E tasks: Lexical analysis, NE recog-
nition (f-measure &lt; 96%), and Coreference resolu-
tion (f-measure&lt;51.9%). Therefore FRET&apos;s
performance in solving these tasks in football do-
main depends only on GATE&apos;s performance and
the built-in GATE data corpus.
</bodyText>
<sectionHeader confidence="0.888683" genericHeader="method">
3 Data resources in FRET
</sectionHeader>
<bodyText confidence="0.994873142857143">
The process of filling slots in a template doesn&apos;t
imply certain &amp;quot;full understanding&amp;quot;, but only recog-
nizes semantically equivalent representations of
the expected concepts. For most of the concepts in
the text we need only naive semantic information.
However to fulfil the template slots, a more de-
tailed lexical knowledge base is needed, including
the necessary information for all concepts and pos-
sible relations between them that can be referred in
some sense to the template. An expert in our spe-
cific domain — football reports, develops this lexi-
cal knowledge base in FRET.
FRET&apos;s resources, shown in Figure 1 include
three types of data:
</bodyText>
<listItem confidence="0.901869333333333">
- Static Resource Bank,
- Dynamic Resource Bank,
- Template&apos;s description (see section 5).
</listItem>
<bodyText confidence="0.6528006">
Static resource bank contains linguistic knowl-
edge (lexicon, grammar) as well as a knowledge
base that represents some main &amp;quot;action relations&amp;quot;:
effect causality: an action A causes effects
B1, B2, B. There are two types of effects
</bodyText>
<listItem confidence="0.9406288">
— intentional effects and side effects;
- preconditions-causality: an action A may
have preconditions B1, B2, ..., B.;
- enablement - action A enables action B;
- decomposition - action A is performed when
</listItem>
<bodyText confidence="0.7956735">
subactions B1, B2, ..., B„ are performed;
generation - action A generates action B.
The knowledge base also includes lists of syno-
nym concepts in the football domain. For example:
</bodyText>
<construct confidence="0.861693">
Example 4:
Synonym objects: [net, home...]
Synonym actions:
[head, shoot, stab, hit...]
</construct>
<bodyText confidence="0.9999607">
One of the more natural ways to attach required
semantic information to already syntactically
parsed sentences is to translate them into first—
order Logical Form (LF). For this purpose we need
grammar rules and rules for translation into LF.
These rules are kept in Static Resource Bank.
Since in the football reports most of the sen-
tences have quite complex syntax structure, in or-
der to simplify template matching we substitute
some of the concepts and relations between them
with their normal form (infinitives, base forms
etc.). So we use a lexicon including about 65 000
words&apos; base forms and their wordforms. For short-
ness we do not describe the lexicon into details
here, because the focus is on the semantic analysis
and resources closely related to it.
Texts in the football domain usually do not in-
clude all the information necessary for filling tem-
plates. That&apos;s why each text is associated with
another data resource that contains additional
</bodyText>
<page confidence="0.999687">
43
</page>
<figureCaption confidence="0.999895">
Figure 1: The matching algorithm of FRET
</figureCaption>
<figure confidence="0.999636192307692">
TF VT
ReS011rce Bank
6tatic Tenijrcr
Bank
11-)ynunic Resour ce
Bank
— -
L o Foi m
Ti All dation
lop
Infer (lice
tching
I\NiTo YES
Templates
Descril tion
:Liptipas-.
LF
SubEvenis LF
GATE 001
Lexical Analysis
NE Re:: o.g‘ niti on
Part-of-speech
tagger
AY
Coreference
Re s oluti cfl
</figure>
<page confidence="0.99486">
44
</page>
<bodyText confidence="0.9973907">
information. For example, team names, lists of
players in each of the teams, playing roles, penal-
ties etc. This is fast changing information and can-
not be stationary added in the system, but it is
reported in the processed texts and is automatically
extracted. For example the players in both teams
are usually presented in the beginning of the match
with their names, numbers and position in the
team. All such additional information is stored in
the Dynamic Resource Bank (Fig. 1).
</bodyText>
<sectionHeader confidence="0.968197" genericHeader="method">
4 Logical form translation
</sectionHeader>
<bodyText confidence="0.990697947368421">
A specially developed left-recursive, top-down,
depth-first parser, implemented in Sicstus Prolog,
is used in FRET for logical form translation. This
parser uses grammar rules and rules for translation
into LF from our resource bank. In LF we repre-
sent all words as predicates with predicate symbol
the corresponding base form of the word and one
argument. For example the word &amp;quot;squeezes&amp;quot; will
be represented in LF as squeeze (X). For the-
matic roles we also add predicates with predicate
symbol &amp;quot;theta&amp;quot; and three arguments. The second
argument is a constant and represents the thematic
role. The rest of the arguments are bound with the
corresponding predicates that represent related
concepts or constants to this thematic role (see ex-
amples). All proper names are represented as con-
stants that occur as arguments of the corresponding
thematic roles.
Example 5:
</bodyText>
<subsectionHeader confidence="0.50402">
Sentence:
</subsectionHeader>
<bodyText confidence="0.9919846">
53 mins: Beckham shoots the ball
across the penalty area to Alan
Shearer who heads into the back of
the net at the far post and scores.
Logical form: score( A) &amp;
</bodyText>
<equation confidence="0.927848454545455">
theta( A,agnt,&apos;Alan Shearer&apos;) &amp;
head( C) &amp; theta( C,agnt,&apos;Alan
Shearer&apos;) &amp; theta( C,obj, D) &amp;
ball( D) &amp; theta( C,into, E) &amp;
net(E) &amp; shoot(F) &amp;
theta( F,agnt,&apos;Beckham&apos;) &amp;
theta( F,obj, D) &amp;
theta( F,to,&apos;Alan Shearer&apos;) &amp; &amp;
theta( F,across, G) &amp; area( G) &amp;
theta( G,char, H) &amp; penalty( H)
&amp; time (53).
</equation>
<bodyText confidence="0.99983485">
Coreference solving provided by GATE in this
stage [5] helps for earlier binding of the variables
in LF and makes further matching processes easier
(especially future inferences).
Usually partial information about an event may
be spread over several sentences. This information
needs to be combined before a template can be
generated. In other cases, some of the information
is only implicit, and needs to be made explicit
through an inference process. That&apos;s why FRET
associates the time of the event to each produced
LF. Every LF is decomposed to its disjuncts and
each of them is marked with the associated time.
Some problems come out while parsing. One of
them is the interpretation of negations. As de-
scribed in [1] and taking into account the specific
domain texts, we distinguish explicit and implicit
negations.
In explicit usage, &amp;quot;NO&amp;quot; negates sentences im-
mediately preceding the current one.
</bodyText>
<figure confidence="0.950671727272728">
Example 6:
Sentence:
69 min: Jeep Stam will be next.
Surely he has to score. N000000!
He&apos;s blazed it way, way, over.
Logical Form:
not (be( A) &amp;
theta( Afagnt,&apos;Jaap Stem&apos;) &amp;
theta( A,char, B) &amp; next( B) &amp;
score( C)&amp; theta( C,agnt,&apos;Jaap
Stam&apos;)) &amp; time (69)
</figure>
<bodyText confidence="0.979175416666667">
In this case the negation is marked in the LF of
all previous sentences in the current paragraph,
which are bound trough their variables in the dis-
course.
In implicit usage of negation inside one sentence
(marked with words as &amp;quot;but&amp;quot;, &amp;quot;however&amp;quot;...), nega-
tion is inserted as in LF follows:
- in case of &amp;quot;but&amp;quot; and &amp;quot;however&amp;quot;, only pre-
ceding words in this sentence are negated;
- in case of &amp;quot;however&amp;quot;, used in the beginning
of the sentence, the preceded sentences re-
stricted by the discourse are negated.
</bodyText>
<figure confidence="0.740560454545454">
Example 7:
Sentence:
87 min:
Barker again came close to score
but his strike failed to hit the
target.
Logical Form:
not(score( A)&amp; be B)&amp;
theta( B,agnt,&apos;Barker&apos;)&amp;
theta( B,to, A)&amp; theta( B,char, E)&amp;
close( E)&amp;theta( A,agnt,&apos;Barker&apos;))&amp;
</figure>
<page confidence="0.933592">
45
</page>
<equation confidence="0.6885944">
btrrke ( C) &amp; theLa ( C, poss, &apos;Barker &apos; )
&amp; fail ( D) &amp; theta ( D, agnt, C) &amp;
thcta (_D, to,_G) &amp; hit (_G) &amp; thc,ta (_G,
agnt, C) &amp; theta (_G, obj, H) &amp;
target( H) &amp; time (87) .
</equation>
<bodyText confidence="0.99676">
In both cases we are paying attention to not hav-
ing double usage of negations. Note that we inter-
pret the negation in a rather domain-specific way,
which is motivated by our detailed study of the
available domain corpus.
</bodyText>
<sectionHeader confidence="0.994924" genericHeader="method">
5 Template format
</sectionHeader>
<bodyText confidence="0.9925389">
Template is described by a table with two types
of fields that have to be filled in:
- obligatory fields;
- optional fields ( see example in Table 1).
If the obligatory fields are filled in, the template
succeeds and the scenario is found and matched to
the text. Optional fields can be left empty if there
is no information for their filling in the processed
text. Both types of fields, taken as a whole, contain
the key information presented in the text.
</bodyText>
<listItem confidence="0.92372975">
Obligatory Optional
• Player o Assistance
• Time o Position
• Team o Type of action (by
head, by shoot, ...)
• Score o Player&apos;s penalties
(red, yellow cards,
minutes and etc.)
</listItem>
<tableCaption confidence="0.986974">
Table 1 Template table for the scenario Goal
</tableCaption>
<bodyText confidence="0.998214">
The template scenario also includes information
about two types of events:
</bodyText>
<equation confidence="0.316634666666667">
a) main event — LF of obligatory and optional
fields.
Example 8:
</equation>
<bodyText confidence="0.420539">
LF of the main event Goal:
</bodyText>
<table confidence="0.857728818181818">
Obligatory: Score ( A) &amp;
theta (_A, agnt, Player) &amp;
time (Minute)
Optional: Actionl ( C) &amp;
theta (_C, agnt, Player) &amp;
theta ( C, obj, D) &amp; ball ( D) &amp;
theta ( C, Loc, E) &amp; Location ( E) &amp;
Action2 ( F) &amp;
theta (_F, agnt,Assistant) &amp;
theta (_F&apos;, obj, D) &amp;
theta (_F, to, Player)
</table>
<bodyText confidence="0.938207857142857">
b) set of subevents — LFs of events related to the
main event and type of relations to the main
event.
The matching algorithm of FRET is based on re-
lations between events and we present here more
details about three special types of implications,
used in the next examples.
</bodyText>
<listItem confidence="0.8644776">
- Event E2 is a part of event El (Fig.2)
- Event El enables event E2, i.e. event El hap-
pens before the beginning of event E2 and
event El is a precondition for E2 (Fig. 3)
- Event El entails event E2, i.e. when El hap-
</listItem>
<bodyText confidence="0.965837285714286">
pens E2 always happens at the same time
(Fig. 4)
Note that in example 8 the predicate names are
capitalized because they are variables. This means
that practically the matching procedure is per-
formed in second order logic, further employing
the set of synonyms as possible predicate names.
</bodyText>
<sectionHeader confidence="0.94802" genericHeader="method">
6 Filling template
</sectionHeader>
<bodyText confidence="0.996203">
The matching algorithm of FRET (Fig. 1) has two
main steps:
</bodyText>
<listItem confidence="0.702856">
- matching LFs;
- filling templates.
</listItem>
<bodyText confidence="0.8748515">
Matching LFs step is based on the unification algo-
rithm.
</bodyText>
<subsectionHeader confidence="0.774998">
Direct matching:
</subsectionHeader>
<bodyText confidence="0.9839665">
Initially the matching algorithm tries to match LFs
produced from the text to the LF of the main event.
</bodyText>
<figure confidence="0.62196445">
4111.
• •
past future
Example 11:
El: Player&apos;s shot
hits the net.
E2: The player
scores.
Figure 4
Example 10:
El: The player
shoots the ball
E2: Player&apos;s
shot hits the
net.
Example 9:
El: Player&apos;s shot
hits the net.
E2: The ball is into the
net.
</figure>
<page confidence="0.993991">
46
</page>
<bodyText confidence="0.9992754">
We call this step direct matching. Each situation in
the text is described by a set of LFs marked by the
same moment of time. Direct matching algorithm
searches for necessary information consecutively
in each set of individual LFs. In this step we also
use synonyms lists and data structures representing
action relations from the knowledge base. Direct
matching algorithm succeeds when all main
event&apos;s LFs variables related to template&apos;s obliga-
tory fields are bound.
</bodyText>
<table confidence="0.5858122">
Example 12:
Sentence:
12 min:Pessotto steps up.He scores!
Logical form: score( A) &amp;
theta( Afagnt,&apos;Pessotto&apos;)&amp;time(12).
</table>
<bodyText confidence="0.997855285714286">
In example 12 we can fill in only the obligatory
template fields of &amp;quot;goal&amp;quot; (example 8), because we
have no additional information about any kind of
assistance, position and etc.
In contrast in Example 5, the direct matching al-
gorithm succeeds and all obligatory and optional
template fields will be replete (see Table 2).
</bodyText>
<sectionHeader confidence="0.494468" genericHeader="method">
Inference matching:
</sectionHeader>
<bodyText confidence="0.968857176470588">
If the direct matching algorithm fails then FRET
starts the inference-matching algorithm. Inference-
matching algorithm tries to match some of tem-
plate&apos;s subevents LFs with the text LFs similarly to
the direct matching algorithm. If we find the nec-
essary information about some subevent, we use
the corresponding additional information about the
type of relation between this subevent and the main
event. Using inference rules and the knowledge
base, FRET inference-matching algorithm derives
an inference from subevents LFs. If it is possible
successfully to match the inferred LFs to the main
event LF, then the inference-matching algorithm
succeeds.
Example 13:
SubEvent: Player shoots the ball
into the net.
</bodyText>
<subsectionHeader confidence="0.794093">
SubEvent&apos;s logical form:
</subsectionHeader>
<construct confidence="0.750358">
Action( A) &amp; theta( A,agnt,Player)
&amp; theta( Afobj, C) &amp; ball( C) &amp;
theta( Afinto, D) &amp; Net D).
7Sentence:
</construct>
<bodyText confidence="0.6746235">
41 min: From the resulting corner,
Micoud finds Sylvain Wiltord on the
edge of the area. He shoots the
ball into the net.
</bodyText>
<table confidence="0.9782789">
Logical form: time(41) &amp; shoot( A)
&amp; theta( A,agnt,&apos;Sylvain Wiltord&apos;)
&amp; theta( Afobj, C) &amp; ball( C) &amp;
theta( Afinto, D) &amp; net D) &amp;
find(_E) &amp; theta(_E,agnt,&apos;Micoud&apos;)
&amp; theta( E,obj, &apos;Sylvain Wiltord&apos;)
&amp; theta( Efloc, F) &amp; edge( F) &amp;
theta( G,poss, F) &amp; area( G) &amp; &amp;
theta( E,from, &amp; corner( H)
theta( H,char, &amp; resulting( I).
</table>
<bodyText confidence="0.967108411764706">
This subevent is matched to the &amp;quot;goal&amp;quot; scenario
applying inference as shown in example 11: &amp;quot;He
shoots the ball into the net&amp;quot; implies that &amp;quot;there is a
score&amp;quot;. Our current evaluation with available do-
main texts shows that simple relations between
events similar to those in examples 9, 10 and 11,
are sufficient for covering paraphrases and suc-
cessful matching of subevents.
Filling template form:
When the matching algorithm succeeds, then we
can fill in the template. First we fill the required
information in the obligatory fields. If necessary
we use some additional information from Dynamic
resource bank. At the next step we try to fill those
of the optional fields for which there is sufficient
information. Table 2 presents the result obtained
after filling in a template from Example 5.
</bodyText>
<listItem confidence="0.898477">
Obligatory Optional
• Player: Alan o Assistance: David
Shearer Beckham
• Time: 53 min 0 Position: penalty area
• Team: England 0 Type of action: heads
• Score: 4 o Player&apos;s penalties
cards( 1,yellow ,12 min)
</listItem>
<bodyText confidence="0.916511375">
Tab e 2
Texts from a total of 31 reports are tested. The
scenario templates are filled in with precision:
80%, recall: 50% and f-measure: 44,44%. We have
to mention that these measures are approximate,
because we report work in progress and FRET is
tested only for a few templates (goals — totally 89,
sent off— totally 8).
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.997671285714286">
In the world of high technologies, extracting in-
formation from &amp;quot;free&amp;quot; NL texts is very important.
Therefore we try to find an easy and effective way
for filling in templates, which may allow for real
semantic processing of large text collections.
In this paper we describe on-going work on se-
mantic analysis in JE: our main idea and core tech-
</bodyText>
<page confidence="0.997711">
47
</page>
<bodyText confidence="0.987406523809524">
nique for realization. We think that the inference is
an integral part of finding facts in texts, and that
for making inferences it is necessary to represent
sentences into LFs. However, not all the informa-
tion provided in the text is needed for simple tem-
plate filling; so we choose shallow parsing and
partial semantic analysis. Note that when the sim-
pler inference fails the more complicated one is
started. The knowledge database has the major role
in inferences from the logical forms. Because of
the fast changing domain terminology a regular
tuning of the database is required with the help of
domain experts. Even human beings are embar-
rassed to recognize domain specific usage of some
words, which are treated as terms in this domain.
The main innovative aspects of FRET are:
• usage of the specific temporal features in the
domain texts. Scenarios are matched to para-
graphs discussing certain important moments.
This simplifies the choice of sentences to be
parsed in order to fill in a template;
</bodyText>
<listItem confidence="0.907814166666667">
• clear and sound logical definitions of notions
like &amp;quot;template filling&amp;quot;, allowing application of
higher-order logic;
• elaborated inference mechanisms which provide
relatively deep NL understanding but only in
&amp;quot;certain points&amp;quot;. The de-facto fragmentation of
</listItem>
<bodyText confidence="0.950573060606061">
the knowledge base into scenario-relevant and
scenario-irrelevant facts allows relatively sim-
ple and very effective inference. Note that only
scenario-relevant relations between events are
linked in the inference chains;
• attempts for domain-specific treatment of the
negation.
However, many difficulties in the implementa-
tion are due to our decision to present sentences
into pure logical forms. One of them, that we plan
to work on, is a more precise resolution of nega-
tions&apos; scope. We hope to improve FRET perform-
ance in the next months when an extensive
evaluation with further unknown texts is planned.
The implementation of presented version of
FRET is in Prolog to make it clear and comprehen-
sible. Another advantage of the logical program-
ming language is easier realization of inferences
and knowledge representation. The next challenge
is to rewrite the system in Java, which is not a triv-
ial task. The reason of following this direction is a
better co-operation with GATE system and faster
performance in case of growing, real-scale linguis-
tic and knowledge resources.
The presented approach can easily be adapted to
a new domain, because it uses just a few domain
dependent resources: data structures and template
description. However we should keep in mind that
this approach is tailored only for text with a spe-
cific temporal structure. In our further work we
plan to test FRET system behaviour on another
domains of such type and we expect similar re-
sults.
</bodyText>
<sectionHeader confidence="0.999239" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863540540541">
[1] Boytcheva, Sv., A. Strupchanska and G.Angelova.
(July 2002), &amp;quot;Processing Negation in NL Interfaces
to Knowledge Bases&amp;quot; In Proceedings of. ICCS-2002,
pp.137-150
[2] Chinchor. N. (1993), &amp;quot;The statistical significance of
the MUC-5 results&amp;quot;, In Proceedings of MUC-5, pp.
79-83. Morgan Kaufmann,.
[3] Chinchor N., (1998), &amp;quot;Overview of MUC-7&amp;quot;, In Pro-
ceedings of MUC-7, http://www.muc.saic.com/
[4] Cunningaham, H., D. Mayard, K. Boncheva, V. Tab-
lan, C. Ursu and M. Dimitrov (2002) &amp;quot;The GATE
User Guide&amp;quot;. http://gate.ac.uk/.
[5] Dimitrov, Marin (2002), &amp;quot;A light-weight Approach
to Coreference Resolution for Named Entities in
Text&amp;quot;, MSc thesis, Sofia University
[6] Grishman, Ralph (1997), &amp;quot;Information Extraction:
Techniques and Challenges&amp;quot;, International Summer
School, SCIE-97
[7] Grishman, R. and B. Sundheim. (1996), &amp;quot;Message
Understanding Conference — 6 : A Brief History&amp;quot;. In
Proceedings of COLING-96, pp. 466--471.
[8] Lehnert, W., C. Cardie, D. Fisher, E. Riloff, and R.
Williams, (May 1991), ()University of Massachu-
setts: MUC-3 Test Results and Analysis, in Proceed-
ings of MUC-3, Morgan Kaufmann, pp. 116-119.
[9] Lehnert, W., D. Fisher, J. McCarthy, E. Riloff, and
S. Soderland, ()University of Massachusetts: MUC-4
Test Results and Analysis, in Proceedings of MUC-4
(June 1992), Morgan Kaufmann,. pp. 151-158.
[10] Neumann, G., R. Backofen, J. Baur, M. Becker, C,
Broun (1997) &amp;quot;An Information Extraction Core Sys-
tem for Real World German Text Processing&amp;quot;
[11] Soderland, Stephen (1997) &amp;quot;Learning to Extract
Text-based Information from the World Wide Web&amp;quot;
[12] Wilks, Yorick (1997) &amp;quot;Information Extraction as a
Core Language Technology&amp;quot;, International Summer
School, SCIE-97.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.641689">
<title confidence="0.999924">Focusing on Scenario Recognition in Information Extraction</title>
<author confidence="0.998776">Milena Yankova</author>
<affiliation confidence="0.911409333333333">Linguistic Modelling Depai talent, Central Laboratory for Parallel Processing, Bulgarian Academy of Sciences,</affiliation>
<address confidence="0.9909315">25A Acad. G. Bonchev Str., 1113 Sofia, Bulgaria</address>
<email confidence="0.983626">myankova@lml.bas.bg</email>
<author confidence="0.981145">Svetla Boytcheva</author>
<affiliation confidence="0.996969666666667">Department of Information Technologies, Faculty of Mathematics &amp; Informatics Sofia University &amp;quot;St. Kl. Ohridski&amp;quot;,</affiliation>
<address confidence="0.970381">5 James Baurchier Str., 1164 Sofia, Bulgaria</address>
<email confidence="0.993142">svetla@fmi.uni-sofia.bg</email>
<abstract confidence="0.999551076923077">This paper reports a research effort in Information Extraction, especially in template pattern matching. Our approach uses reach domain knowledge in the football (soccer) area and logical form representation for necessary inferences of facts and templates filling. Our system FRET&apos; (Football Reports Extraction Templates) is compatible to the language-engineering environment GATE and handles its internal representations and some intermediate analysis results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sv Boytcheva</author>
<author>A Strupchanska</author>
<author>G Angelova</author>
</authors>
<title>Processing Negation in NL Interfaces to Knowledge Bases&amp;quot;</title>
<date>2002</date>
<booktitle>In Proceedings of. ICCS-2002,</booktitle>
<pages>137--150</pages>
<contexts>
<context position="15132" citStr="[1]" startWordPosition="2417" endWordPosition="2417">her matching processes easier (especially future inferences). Usually partial information about an event may be spread over several sentences. This information needs to be combined before a template can be generated. In other cases, some of the information is only implicit, and needs to be made explicit through an inference process. That&apos;s why FRET associates the time of the event to each produced LF. Every LF is decomposed to its disjuncts and each of them is marked with the associated time. Some problems come out while parsing. One of them is the interpretation of negations. As described in [1] and taking into account the specific domain texts, we distinguish explicit and implicit negations. In explicit usage, &amp;quot;NO&amp;quot; negates sentences immediately preceding the current one. Example 6: Sentence: 69 min: Jeep Stam will be next. Surely he has to score. N000000! He&apos;s blazed it way, way, over. Logical Form: not (be( A) &amp; theta( Afagnt,&apos;Jaap Stem&apos;) &amp; theta( A,char, B) &amp; next( B) &amp; score( C)&amp; theta( C,agnt,&apos;Jaap Stam&apos;)) &amp; time (69) In this case the negation is marked in the LF of all previous sentences in the current paragraph, which are bound trough their variables in the discourse. In impli</context>
</contexts>
<marker>[1]</marker>
<rawString>Boytcheva, Sv., A. Strupchanska and G.Angelova. (July 2002), &amp;quot;Processing Negation in NL Interfaces to Knowledge Bases&amp;quot; In Proceedings of. ICCS-2002, pp.137-150</rawString>
</citation>
<citation valid="true">
<authors>
<author>N</author>
</authors>
<title>The statistical significance of the MUC-5 results&amp;quot;,</title>
<date>1993</date>
<booktitle>In Proceedings of MUC-5,</booktitle>
<pages>79--83</pages>
<publisher>Morgan Kaufmann,.</publisher>
<contexts>
<context position="2416" citStr="[2]" startWordPosition="351" endWordPosition="351"> template filling - is a complicated process, which is still far from its ultimate solution. This paper focuses on the semantic processing in 1E. Following the terminology established by the Message Understanding Conferences (MUCs), we shall call the specification of the particular events or relations to be extracted SCENARIO and we shall refer to the final, tabular output format of information extraction as TEMPLATE. The actual structure of the templates used has varied from a flat record structure at MUC-4 [9] to a more complex object oriented definition which was used for Tipster and MUC-5 [2], MUC-6 [7] and MUC-7 [3]. Once filled, templates represent an extract of key information from the text [12]. Extracted information can be stored in databases for various purposes such as text indexing, information highlighting, data mining, natural language summarisation, etc. Different systems provide different approaches for solving semantic problems in IE. The CRYSTAL system [11], for example, is based on machine-learning covering algorithm for building expected rules for template filling. Large handmarked training corpus is needed. But the domain is quite static - weather forecast - with </context>
</contexts>
<marker>[2]</marker>
<rawString>Chinchor. N. (1993), &amp;quot;The statistical significance of the MUC-5 results&amp;quot;, In Proceedings of MUC-5, pp. 79-83. Morgan Kaufmann,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
</authors>
<title>Overview of MUC-7&amp;quot;,</title>
<date>1998</date>
<booktitle>In Proceedings of MUC-7,</booktitle>
<location>http://www.muc.saic.com/</location>
<contexts>
<context position="2441" citStr="[3]" startWordPosition="356" endWordPosition="356">complicated process, which is still far from its ultimate solution. This paper focuses on the semantic processing in 1E. Following the terminology established by the Message Understanding Conferences (MUCs), we shall call the specification of the particular events or relations to be extracted SCENARIO and we shall refer to the final, tabular output format of information extraction as TEMPLATE. The actual structure of the templates used has varied from a flat record structure at MUC-4 [9] to a more complex object oriented definition which was used for Tipster and MUC-5 [2], MUC-6 [7] and MUC-7 [3]. Once filled, templates represent an extract of key information from the text [12]. Extracted information can be stored in databases for various purposes such as text indexing, information highlighting, data mining, natural language summarisation, etc. Different systems provide different approaches for solving semantic problems in IE. The CRYSTAL system [11], for example, is based on machine-learning covering algorithm for building expected rules for template filling. Large handmarked training corpus is needed. But the domain is quite static - weather forecast - with explicitly fully expresse</context>
</contexts>
<marker>[3]</marker>
<rawString>Chinchor N., (1998), &amp;quot;Overview of MUC-7&amp;quot;, In Proceedings of MUC-7, http://www.muc.saic.com/</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningaham</author>
<author>D Mayard</author>
<author>K Boncheva</author>
<author>V Tablan</author>
<author>C Ursu</author>
<author>M Dimitrov</author>
</authors>
<title>The GATE User Guide&amp;quot;.</title>
<date>2002</date>
<note>http://gate.ac.uk/.</note>
<contexts>
<context position="3691" citStr="[4]" startWordPosition="551" endWordPosition="551">esentation of the text that is equivalent to related database entries. Another Information Extraction system is SMES [10], which does not have semantic analysis implemented in it. Fragments extracted by a lexically driven parser are attached to anchors - lexical entries (mainly verbs). If successful, the set of found fragments together with the anchor build up an instantiated template. Filling templates strongly 41 depends on the words and relations between them, as they appear in the text. In our approach we use the IE system GATE 2.1 beta 1 (GATE - General Architecture for Text Engineering) [4], which provides lexical analysis, named entity recognition, coreference resolution and other NLP modules. The system has been used for many language-processing projects; in particular for Information Extraction in several languages. In this paper we present work in progress, aiming at the implementation of the system FRET (Football Reports Extraction Templates). FRET provides syntactic analysis and template (scenario) pattern matching from English text. The innovative aspect in our considerations is the relative weight of the semantic analysis, since we use logical forms, a lexical knowledge </context>
</contexts>
<marker>[4]</marker>
<rawString>Cunningaham, H., D. Mayard, K. Boncheva, V. Tablan, C. Ursu and M. Dimitrov (2002) &amp;quot;The GATE User Guide&amp;quot;. http://gate.ac.uk/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marin Dimitrov</author>
</authors>
<title>A light-weight Approach to Coreference Resolution for Named Entities in Text&amp;quot;, MSc thesis,</title>
<date>2002</date>
<institution>Sofia University</institution>
<contexts>
<context position="14465" citStr="[5]" startWordPosition="2306" endWordPosition="2306">nts that occur as arguments of the corresponding thematic roles. Example 5: Sentence: 53 mins: Beckham shoots the ball across the penalty area to Alan Shearer who heads into the back of the net at the far post and scores. Logical form: score( A) &amp; theta( A,agnt,&apos;Alan Shearer&apos;) &amp; head( C) &amp; theta( C,agnt,&apos;Alan Shearer&apos;) &amp; theta( C,obj, D) &amp; ball( D) &amp; theta( C,into, E) &amp; net(E) &amp; shoot(F) &amp; theta( F,agnt,&apos;Beckham&apos;) &amp; theta( F,obj, D) &amp; theta( F,to,&apos;Alan Shearer&apos;) &amp; &amp; theta( F,across, G) &amp; area( G) &amp; theta( G,char, H) &amp; penalty( H) &amp; time (53). Coreference solving provided by GATE in this stage [5] helps for earlier binding of the variables in LF and makes further matching processes easier (especially future inferences). Usually partial information about an event may be spread over several sentences. This information needs to be combined before a template can be generated. In other cases, some of the information is only implicit, and needs to be made explicit through an inference process. That&apos;s why FRET associates the time of the event to each produced LF. Every LF is decomposed to its disjuncts and each of them is marked with the associated time. Some problems come out while parsing. </context>
</contexts>
<marker>[5]</marker>
<rawString>Dimitrov, Marin (2002), &amp;quot;A light-weight Approach to Coreference Resolution for Named Entities in Text&amp;quot;, MSc thesis, Sofia University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Information Extraction: Techniques and Challenges&amp;quot;, International Summer School,</title>
<date>1997</date>
<contexts>
<context position="4901" citStr="[6]" startWordPosition="739" endWordPosition="739">d certain inference to match text and templates. The paper is organised as follows: section 2 presents a short overview of JE as a whole and some difficulties with performing subtasks in the chosen domain. Section 3 describes the structure of the data resource bank integrated in FRET. Section 4 discuses our approach in translation to logical form. Section 5 describes in details the templates&apos; structure. Section 6 explains the algorithm for filling templates with information from the text. Section 7 contains the conclusion. 2 Information extraction IE can be divided into the following subtasks [6]: Lexical Analysis, which turns a text into a sequence of sentences, each of them is a sequence of lexical items (tokens). Usually sentences are not marked, so special techniques are required to recognise sentence boundaries. Each token is looked up in the dictionary to determine its possible features and part-of-speech types. Named Entity (NE) Recognition, which takes a sequence of lexical items and tries to identify reliably determinable structures using a set of regular expressions: proper names, locations, organizations, dates, currency amounts and etc. The max score result reported in MUC</context>
<context position="6282" citStr="[6]" startWordPosition="962" endWordPosition="962">ne-two neighbour sentences). These descriptions are the ones identified by NE recognition and their anaphoric references. The best result reported for this task at MUC 3-7 is f-measure &lt; 67,5%. Syntactic Analysis, which provides some aspects of syntactic analysis and simplifies the phase of fact extraction. The arguments to be extracted often correspond to noun phrases in the text, and relationships to grammatical functional relations. Note that for IE we are only interested in the grammatical relations relevant to the template; correctly determining the other relations may be a waste of time [6]. Template (Scenario) Pattern Matching, which maps the syntactic structures to semantic structures related to the templates to be filled in. This stage extracts the events or relationships relevant to the scenario. The max score result reported for this task is f-measure &lt; 57%. One of the most important questions is how to recognise the scenario, which we are looking for. For this purpose one specifies a template, as a sequence of slots some of which are marked as obligatory and the others are optional. When the required (marked) slots are filled in then we say that the scenario is matched and</context>
</contexts>
<marker>[6]</marker>
<rawString>Grishman, Ralph (1997), &amp;quot;Information Extraction: Techniques and Challenges&amp;quot;, International Summer School, SCIE-97</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>B Sundheim</author>
</authors>
<title>Message Understanding Conference — 6 : A Brief History&amp;quot;.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96,</booktitle>
<pages>466--471</pages>
<contexts>
<context position="2427" citStr="[7]" startWordPosition="353" endWordPosition="353">illing - is a complicated process, which is still far from its ultimate solution. This paper focuses on the semantic processing in 1E. Following the terminology established by the Message Understanding Conferences (MUCs), we shall call the specification of the particular events or relations to be extracted SCENARIO and we shall refer to the final, tabular output format of information extraction as TEMPLATE. The actual structure of the templates used has varied from a flat record structure at MUC-4 [9] to a more complex object oriented definition which was used for Tipster and MUC-5 [2], MUC-6 [7] and MUC-7 [3]. Once filled, templates represent an extract of key information from the text [12]. Extracted information can be stored in databases for various purposes such as text indexing, information highlighting, data mining, natural language summarisation, etc. Different systems provide different approaches for solving semantic problems in IE. The CRYSTAL system [11], for example, is based on machine-learning covering algorithm for building expected rules for template filling. Large handmarked training corpus is needed. But the domain is quite static - weather forecast - with explicitly </context>
</contexts>
<marker>[7]</marker>
<rawString>Grishman, R. and B. Sundheim. (1996), &amp;quot;Message Understanding Conference — 6 : A Brief History&amp;quot;. In Proceedings of COLING-96, pp. 466--471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>C Cardie</author>
<author>D Fisher</author>
<author>E Riloff</author>
<author>R Williams</author>
</authors>
<date>1991</date>
<booktitle>University of Massachusetts: MUC-3 Test Results and Analysis, in Proceedings of MUC-3,</booktitle>
<pages>116--119</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="5507" citStr="[8]" startWordPosition="833" endWordPosition="833">exical Analysis, which turns a text into a sequence of sentences, each of them is a sequence of lexical items (tokens). Usually sentences are not marked, so special techniques are required to recognise sentence boundaries. Each token is looked up in the dictionary to determine its possible features and part-of-speech types. Named Entity (NE) Recognition, which takes a sequence of lexical items and tries to identify reliably determinable structures using a set of regular expressions: proper names, locations, organizations, dates, currency amounts and etc. The max score result reported in MUC-3 [8] trough MUC-7 in this task is f-measure &lt; 97%. Cor efer en ce Resolution, which identifies different descriptions of the same entity in different parts of a text (usually one-two neighbour sentences). These descriptions are the ones identified by NE recognition and their anaphoric references. The best result reported for this task at MUC 3-7 is f-measure &lt; 67,5%. Syntactic Analysis, which provides some aspects of syntactic analysis and simplifies the phase of fact extraction. The arguments to be extracted often correspond to noun phrases in the text, and relationships to grammatical functional</context>
</contexts>
<marker>[8]</marker>
<rawString>Lehnert, W., C. Cardie, D. Fisher, E. Riloff, and R. Williams, (May 1991), ()University of Massachusetts: MUC-3 Test Results and Analysis, in Proceedings of MUC-3, Morgan Kaufmann, pp. 116-119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>D Fisher</author>
<author>J McCarthy</author>
<author>E Riloff</author>
<author>S Soderland</author>
</authors>
<date>1992</date>
<booktitle>of Massachusetts: MUC-4 Test Results and Analysis, in Proceedings of MUC-4</booktitle>
<pages>151--158</pages>
<publisher>Morgan Kaufmann,.</publisher>
<location>University</location>
<contexts>
<context position="2330" citStr="[9]" startWordPosition="335" endWordPosition="335">ropean Commission via contract ICA1-2000-70016 &amp;quot;BIS-21 Centre of Excellence&amp;quot; ences and template filling - is a complicated process, which is still far from its ultimate solution. This paper focuses on the semantic processing in 1E. Following the terminology established by the Message Understanding Conferences (MUCs), we shall call the specification of the particular events or relations to be extracted SCENARIO and we shall refer to the final, tabular output format of information extraction as TEMPLATE. The actual structure of the templates used has varied from a flat record structure at MUC-4 [9] to a more complex object oriented definition which was used for Tipster and MUC-5 [2], MUC-6 [7] and MUC-7 [3]. Once filled, templates represent an extract of key information from the text [12]. Extracted information can be stored in databases for various purposes such as text indexing, information highlighting, data mining, natural language summarisation, etc. Different systems provide different approaches for solving semantic problems in IE. The CRYSTAL system [11], for example, is based on machine-learning covering algorithm for building expected rules for template filling. Large handmarke</context>
</contexts>
<marker>[9]</marker>
<rawString>Lehnert, W., D. Fisher, J. McCarthy, E. Riloff, and S. Soderland, ()University of Massachusetts: MUC-4 Test Results and Analysis, in Proceedings of MUC-4 (June 1992), Morgan Kaufmann,. pp. 151-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
<author>R Backofen</author>
<author>J Baur</author>
<author>M Becker</author>
<author>C Broun</author>
</authors>
<title>An Information Extraction Core System for Real World German Text Processing&amp;quot;</title>
<date>1997</date>
<contexts>
<context position="3209" citStr="[10]" startWordPosition="469" endWordPosition="469">ext indexing, information highlighting, data mining, natural language summarisation, etc. Different systems provide different approaches for solving semantic problems in IE. The CRYSTAL system [11], for example, is based on machine-learning covering algorithm for building expected rules for template filling. Large handmarked training corpus is needed. But the domain is quite static - weather forecast - with explicitly fully expressed information. The system creates a formal representation of the text that is equivalent to related database entries. Another Information Extraction system is SMES [10], which does not have semantic analysis implemented in it. Fragments extracted by a lexically driven parser are attached to anchors - lexical entries (mainly verbs). If successful, the set of found fragments together with the anchor build up an instantiated template. Filling templates strongly 41 depends on the words and relations between them, as they appear in the text. In our approach we use the IE system GATE 2.1 beta 1 (GATE - General Architecture for Text Engineering) [4], which provides lexical analysis, named entity recognition, coreference resolution and other NLP modules. The system </context>
</contexts>
<marker>[10]</marker>
<rawString>Neumann, G., R. Backofen, J. Baur, M. Becker, C, Broun (1997) &amp;quot;An Information Extraction Core System for Real World German Text Processing&amp;quot;</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
</authors>
<title>Learning to Extract Text-based Information from the World Wide Web&amp;quot;</title>
<date>1997</date>
<contexts>
<context position="2802" citStr="[11]" startWordPosition="409" endWordPosition="409"> information extraction as TEMPLATE. The actual structure of the templates used has varied from a flat record structure at MUC-4 [9] to a more complex object oriented definition which was used for Tipster and MUC-5 [2], MUC-6 [7] and MUC-7 [3]. Once filled, templates represent an extract of key information from the text [12]. Extracted information can be stored in databases for various purposes such as text indexing, information highlighting, data mining, natural language summarisation, etc. Different systems provide different approaches for solving semantic problems in IE. The CRYSTAL system [11], for example, is based on machine-learning covering algorithm for building expected rules for template filling. Large handmarked training corpus is needed. But the domain is quite static - weather forecast - with explicitly fully expressed information. The system creates a formal representation of the text that is equivalent to related database entries. Another Information Extraction system is SMES [10], which does not have semantic analysis implemented in it. Fragments extracted by a lexically driven parser are attached to anchors - lexical entries (mainly verbs). If successful, the set of f</context>
</contexts>
<marker>[11]</marker>
<rawString>Soderland, Stephen (1997) &amp;quot;Learning to Extract Text-based Information from the World Wide Web&amp;quot;</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Information Extraction as a Core Language Technology&amp;quot;, International Summer School,</title>
<date>1997</date>
<contexts>
<context position="2524" citStr="[12]" startWordPosition="369" endWordPosition="369">uses on the semantic processing in 1E. Following the terminology established by the Message Understanding Conferences (MUCs), we shall call the specification of the particular events or relations to be extracted SCENARIO and we shall refer to the final, tabular output format of information extraction as TEMPLATE. The actual structure of the templates used has varied from a flat record structure at MUC-4 [9] to a more complex object oriented definition which was used for Tipster and MUC-5 [2], MUC-6 [7] and MUC-7 [3]. Once filled, templates represent an extract of key information from the text [12]. Extracted information can be stored in databases for various purposes such as text indexing, information highlighting, data mining, natural language summarisation, etc. Different systems provide different approaches for solving semantic problems in IE. The CRYSTAL system [11], for example, is based on machine-learning covering algorithm for building expected rules for template filling. Large handmarked training corpus is needed. But the domain is quite static - weather forecast - with explicitly fully expressed information. The system creates a formal representation of the text that is equiv</context>
</contexts>
<marker>[12]</marker>
<rawString>Wilks, Yorick (1997) &amp;quot;Information Extraction as a Core Language Technology&amp;quot;, International Summer School, SCIE-97.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>