<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013647">
<title confidence="0.998824">
Improving Syntax-Augmented Machine Translation by
Coarsening the Label Set
</title>
<author confidence="0.99684">
Greg Hanneman and Alon Lavie
</author>
<affiliation confidence="0.840671">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
</affiliation>
<email confidence="0.998998">
{ghannema,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996525">
We present a new variant of the Syntax-
Augmented Machine Translation (SAMT) for-
malism with a category-coarsening algorithm
originally developed for tree-to-tree gram-
mars. We induce bilingual labels into the
SAMT grammar, use them for category coars-
ening, then project back to monolingual la-
beling as in standard SAMT. The result is a
“collapsed” grammar with the same expres-
sive power and format as the original, but
many fewer nonterminal labels. We show that
the smaller label set provides improved trans-
lation scores by 1.14 BLEU on two Chinese–
English test sets while reducing the occur-
rence of sparsity and ambiguity problems
common to large label sets.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924510204082">
The formulation of statistical machine translation in
terms of synchronous parsing has become both the-
oretically and practically successful. In a parsing-
based MT formalism, synchronous context-free
grammar rules that match a source-language input
can be hierarchically composed to produce a corre-
sponding target-language output. SCFG translation
grammars can be extracted automatically from data.
While formally syntactic approaches with a single
grammar nonterminal have often worked well (Chi-
ang, 2007), the desire to exploit linguistic knowl-
edge has motivated the use of translation grammars
with richer, linguistically syntactic nonterminal in-
ventories (Galley et al., 2004; Liu et al., 2006; Lavie
et al., 2008; Liu et al., 2009).
Linguistically syntactic MT systems can derive
their label sets, either monolingually or bilingually,
from parallel corpora that have been annotated with
source- and/or target-side parse trees provided by
a statistical parser. The MT system may exactly
adopt the parser’s label set or modify it in some way.
Larger label sets are able to represent more precise,
fine-grained categories. On the other hand, they also
exacerbate a number of computational and modeling
problems by increasing grammar size, derivational
ambiguity, and data sparsity.
In this paper, we focus on the Syntax-Augmented
MT formalism (Zollmann and Venugopal, 2006), a
monolingually labeled version of Hiero that can cre-
ate up to 4000 “extended” category labels based on
pairs of parse nodes. We take a standard SAMT
grammar with target-side labels and extend its label-
ing to a bilingual format (Zollmann, 2011). We then
coarsen the bilingual labels following the “label col-
lapsing” algorithm of Hanneman and Lavie (2011).
This represents a novel extension of the tree-to-tree
collapsing algorithm to the SAMT formalism. Af-
ter removing the source-side labels, we obtain a new
SAMT grammar with coarser target-side labels than
the original.
Coarsened grammars provide improvement of up
to 1.14 BLEU points over the baseline SAMT results
on two Chinese–English test sets; they also outper-
form a Hiero baseline by up to 0.60 BLEU on one
of the sets. Aside from improved translation quality,
in analysis we find significant reductions in deriva-
tional ambiguity and rule sparsity, two problems that
make large nonterminal sets difficult to work with.
Section 2 provides a survey of large syntax-based
</bodyText>
<page confidence="0.965152">
288
</page>
<note confidence="0.4718275">
Proceedings of NAACL-HLT 2013, pages 288–297,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9930963">
MT label sets, their associated problems of deriva-
tional ambiguity and rule sparsity, and previous at-
tempts at addressing those problems. The section
also summarizes the tree-to-tree label collapsing al-
gorithm and the process of SAMT rule extraction.
We then describe our method of label collapsing in
SAMT grammars in Section 3. Experimental results
are presented in Section 4 and analyzed in Section
5. Finally, Section 6 offers some conclusions and
avenues for future work.
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999847">
2.1 Working with Large Label Sets
</subsectionHeader>
<bodyText confidence="0.999899852459017">
Aside from the SAMT method of grammar extrac-
tion, which we treat more fully in Section 2.3, sev-
eral other lines of work have explored increasing
the nonterminal set for syntax-based MT. Huang and
Knight (2006), for example, augmented the standard
Penn Treebank labels for English by adding lexi-
calization to certain types of nodes. Chiang (2010)
and Zollmann (2011) worked with a bilingual exten-
sion of SAMT that used its notion of “extended cat-
egories” on both the source and target sides. Taking
standard monolingual SAMT as a baseline, Baker et
al. (2012) developed a tagger to augment syntactic
labels with some semantically derived information.
Ambati et al. (2009) extracted tree-to-tree rules with
similar extensions for sibling nodes, resulting again
in a large number of labels.
Extended categories allow for the extraction of
a larger number of rules, increasing coverage and
translation performance over systems that are lim-
ited to exact constituent matches only. However,
the gains in coverage come with a corresponding
increase in computational and modeling complexity
due to the larger label set involved.
Derivational ambiguity — the condition of hav-
ing multiple derivations for the same output string
— is a particular problem for parsing-based MT sys-
tems. The same phrase pair may be represented with
a large number of different syntactic labels. Fur-
ther, new hierarchical rules are created by abstract-
ing smaller phrase pairs out of larger ones; each of
these substitutions must also be marked by a label
of some kind. Keeping variantly labeled copies of
the same rules fragments probabilities during gram-
mar scoring and creates redundant hypotheses in the
decoder at run time.
A complementary problem — when a desired rule
application is impossible because its labels do not
match — has been variously identified as “data spar-
sity,” the “matching constraint,” and “rule sparsity”
in the grammar. It arises from the definition of
SCFG rule application: in order to compose two
rules, the left-hand-side label of the smaller rule
must match a right-hand-side label in the larger rule
it is being plugged in to. With large label sets, it
becomes less likely that two arbitrarily chosen rules
can compose, making the grammar less flexible for
representing new sentences.
Previous research has attempted to address both
of these problems in different ways. Preference
grammars (Venugopal et al., 2009) are a technique
for reducing derivational ambiguity by summing
scores over labeled variants of the same deriva-
tion during decoding. Chiang (2010) addressed rule
sparsity by introducing a soft matching constraint:
the decoder may pay a learned label-pair-specific
penalty for substituting a rule headed by one label
into a substitution slot marked for another. Combin-
ing properties of both of the above methods, Huang
et al. (2010) modeled monolingual labels as distribu-
tions over latent syntactic categories and calculated
similarity scores between them for rule composition.
</bodyText>
<subsectionHeader confidence="0.999834">
2.2 Label Collapsing in Tree-to-Tree Rules
</subsectionHeader>
<bodyText confidence="0.999964789473684">
Aiming to reduce both derivational ambiguity and
rule sparsity, we previously presented a “label col-
lapsing” algorithm for systems in which bilingual
labels are used (Hanneman and Lavie, 2011). It
coarsens the overall label set by clustering monolin-
gual labels based on which labels they appear joined
with in the other language.
The label collapsing algorithm takes as its input
a set of SCFG rule instances extracted from a par-
allel corpus. Each time a tree-to-tree rule is ex-
tracted, its left-hand side is a label of the form s::t,
where s is a label from the source-language cate-
gory set 5 and t is a label from the target-language
category set T. Operationally, the joint label means
that a source-side subtree rooted at s was the trans-
lational equivalent of a target-side subtree rooted at
t in a parallel sentence. Figure 1 shows several such
subtrees, highlighted in grey and numbered. Joint
left-hand-side labels for the collapsing algorithm,
</bodyText>
<page confidence="0.998861">
289
</page>
<figureCaption confidence="0.998052">
Figure 1: Sample extraction of bilingual nonterminals for
label collapsing. Labels extracted from this tree pair in-
clude VBD::VV and NP::AD.
</figureCaption>
<bodyText confidence="0.999053">
such as VBD::VV and NP::AD, can be assembled
by matching co-numbered nodes.
From the counts of the extracted rules, it is thus
straightforward to compute for all values of s and
t the observed P(s  |t) and P(t  |s), the probability
of one half of a joint nonterminal label appearing
in the grammar given the other half. In the figure,
for example, P(JJ  |NN) = 0.5. The conditional
probabilities accumulated over the whole grammar
give rise to a simple L1 distance metric over any pair
of monolingual labels:
</bodyText>
<equation confidence="0.9945025">
�d(s1, s2) = |P(t  |s1) − P(t  |s2) |(1)
tET
</equation>
<bodyText confidence="0.992060666666667">
and P(t  |s) values appropriately. The choice of la-
bel pair to collapse in each iteration can be expressed
formally as
</bodyText>
<equation confidence="0.882578">
arg min {d(sZ, sj), d(tk, t�)1 (3)
(si,sj)ES2,(tk,te)ET2
</equation>
<bodyText confidence="0.9607665">
That is, either a source label pair or a target label pair
may be chosen by the algorithm in each iteration.
</bodyText>
<subsectionHeader confidence="0.995838">
2.3 SAMT Rule Extraction
</subsectionHeader>
<bodyText confidence="0.999966714285714">
SAMT grammars pose a challenge to the label col-
lapsing algorithm described above because their la-
bel sets are usually monolingual. The classic SAMT
formulation (Zollmann and Venugopal, 2006) pro-
duces a grammar labeled on the target side only.
Nonterminal instances that exactly match a target-
language syntactic constituent in a parallel sentence
are given labels of the form t. Labels of the form
t1+t2 are assigned to nonterminals that span exactly
two contiguous parse nodes. Categorial grammar la-
bels such as t1/t2 and t1\t2 are given to nontermi-
nals that span an incomplete t1 constituent missing
a t2 node to its right or left, respectively. Any non-
terminal that cannot be labeled by one of the above
three schemes is assigned the default label X.
Figure 2(a) shows the extraction of a VP-level
SAMT grammar rule from part of a parallel sen-
tence. At the word level, the smaller English phrase
supported each other (and its Chinese equivalent) is
being abstracted as a nonterminal within the larger
phrase supported each other in international affairs.
The larger phrase corresponds to a parsed VP node
on the target side; this will become the label of
the extracted rule’s left-hand side. Since the ab-
stracted sub-phrase does not correspond to a single
constituent, the SAMT labeling conventions assign
it the label VBD+NP. We can thus write the ex-
tracted rule as:
</bodyText>
<equation confidence="0.9883832">
(4)
�
d(t1, t2) =
sES
|P(s  |t1) − P(s  |t2) |(2)
</equation>
<bodyText confidence="0.999977166666667">
An agglomerative clustering algorithm then com-
bines labels in a series of greedy iterations. At each
step, the algorithm finds the pair of labels that is cur-
rently the closest together according to the distance
metrics of Equations (1) and (2), combines those two
labels into a new one, and updates the set of P(s |t)
While the SAMT label formats can be trivially
converted into joint labels X::t, X::t1+t2, X::t1/t2,
X::t1\t2, and X::X, they cannot be usefully fed into
the label collapsing algorithm because the necessary
conditional label probabilities are meaningless. To
acquire meaningful source-side labels, we turn to a
</bodyText>
<page confidence="0.972676">
290
</page>
<figure confidence="0.999114">
(a) (b)
</figure>
<figureCaption confidence="0.999754">
Figure 2: Sample extraction of an SAMT grammar rule: (a) with monolingual syntax and (b) with bilingual syntax.
</figureCaption>
<bodyText confidence="0.9999475">
bilingual SAMT extension used by Chiang (2010)
and Zollmann (2011). Both a source- and a target-
side parse tree are used to extract rules from a par-
allel sentence; two SAMT-style labels are worked
out independently on each side for each nonterminal
instance, then packed into a joint label. It is there-
fore possible for a nonterminal instance to be labeled
s::t, s1\s2::t, s1+s2::t1/t2, or various other combi-
nations depending on what parse nodes the nonter-
minal spans in each tree.
Such a bilingually labeled rule is extracted in Fig-
ure 2(b). The target-side labels from Figure 2(a) are
now paired with source-side labels extracted from an
added Chinese parse tree. In this case, the abstracted
sub-phrase supported each other is given the joint
label VP::VBD+NP, while the rule’s left-hand side
becomes LCP+VP::VP.
We implement bilingual SAMT grammar extrac-
tion by modifying Thrax (Weese et al., 2011), an
open-source, Hadoop-based framework for extract-
ing standard SAMT grammars. By default, Thrax
can produce grammars labeled either on the source
or target side, but not both. It also outputs rules
that are already scored according to a user-specified
set of translation model features, meaning that the
raw rule counts needed to compute the label condi-
tional probabilities P(s  |t) and P(t  |s) are not di-
rectly available. We implement a new subclass of
grammar extractor with logic for independently la-
beling both sides of an SAMT rule in order to get the
necessary bilingual labels; an adaptation to the exist-
ing Thrax “rarity” feature provides the rule counts.
</bodyText>
<sectionHeader confidence="0.960062" genericHeader="method">
3 Label Collapsing in SAMT Rules
</sectionHeader>
<bodyText confidence="0.999721571428571">
Our method of producing label-collapsed SAMT
grammars is shown graphically in Figure 3.
We first obtain an SAMT grammar with bilingual
labels, together with the frequency count for each
rule, using the modified version of Thrax described
in Section 2.3. The rules can be grouped according
to the target-side label of their left-hand sides (Fig-
ure 3(a)).
The rule counts are then used to compute label-
ing probabilities P(s  |t) and P(t  |s) over left-hand-
side usages of each source label s and each target
label t. These are simple maximum-likelihood es-
timates: if #(si7 tj) represents the combined fre-
quency counts of all rules with si::tj on the left-hand
</bodyText>
<page confidence="0.981994">
291
</page>
<figure confidence="0.998901">
(a) (b) (c) (d)
</figure>
<figureCaption confidence="0.99863675">
Figure 3: Stages of preparing label-collapsed rules for SAMT grammars. (a) SAMT rules with bilingual nonterminals
are extracted and collected based on their target left-hand sides. (b) Probabiliites P(t  |s) and P(s  |s) are computed. (c)
Nonterminals are clustered according to the label collapsing algorithm. (d) Source sides of nonterminals are removed
to create a standard SAMT grammar.
</figureCaption>
<bodyText confidence="0.98953">
side, the source-given-target labeling probability is:
</bodyText>
<equation confidence="0.9985545">
P(si  |tj) = #(si::tj) (5)
EtET #(si::t)
</equation>
<bodyText confidence="0.999795333333333">
The computation for target given source is analo-
gous. Each monolingual label can thus be repre-
sented as a distribution over the labels it is aligned
to in the opposite language (Figure 3(b)).
Such distributions over labels are the input to the
label-collapsing algorithm, as described in Section
2.2. As shown in Figure 3(c), the algorithm results
in the original target-side labels being combined into
different groups, denoted in this case as new labels
CA and CB. We run label collapsing for varying
numbers of iterations to produce varying degrees of
coarsened label sets.
Given a mapping from original target-side labels
to collapsed groups, all nonterminals in the original
SAMT grammar are overwritten accordingly. The
source-side labels are dropped at this point: we use
them only for the purpose of label collapsing, but not
in assembling or scoring the final grammar. The re-
sulting monolingual SAMT-style grammar with col-
lapsed labels (Figure 3(d)) can now be scored and
used for decoding in the usual way.
For constructing a baseline SAMT grammar with-
out label collapsing, we merely extract a bilingual
grammar as in the first step of Figure 3, immediately
remove the source-side labels from it, and proceed
to grammar scoring.
All grammars are scored according to a set of
eight features. For an SCFG rule with left-hand-side
label t, source right-hand side f, and target right-
hand side e, they are:
</bodyText>
<listItem confidence="0.9541745">
• Standard maximum-likelihood phrasal transla-
tion probabilities P(f  |e) and P(e  |f)
• Maximum-likelihood labeling probability
P(t  |f, e)
• Lexical translation probabilities Plex(f  |e) and
Plex(e  |f), as calculated by Thrax
• Rarity score exp(i)−i for a rule with extracted
exp(count c
• Binary indicator features that mark phrase pair
(as opposed to hierarchical) rules and glue rules
</listItem>
<bodyText confidence="0.999107666666667">
Scored grammars are filtered down to the sen-
tence level, retaining only those rules whose source-
side terminals match an individual tuning or testing
sentence. In addition to losslessly filtering gram-
mars in this way, we also carry out two types of
lossy pruning in order to reduce overall grammar
</bodyText>
<page confidence="0.975145">
292
</page>
<table confidence="0.999794">
System Labels Rules Per Sent.
SAMT 4181 69,401,006 48,444
Collapse 1 913 64,596,618 35,004
Collapse 2 131 60,526,479 24,510
Collapse 3 72 58,483,310 20,445
Hiero 1 36,538,657 7,738
</table>
<tableCaption confidence="0.9544335">
Table 1: Grammar statistics for different degrees of label
collapsing: number of target-side labels, unique rules in
the whole grammar, and average number of pruned rules
after filtering to individual sentences.
</tableCaption>
<bodyText confidence="0.995332">
size. One pruning pass keeps only the 80 most fre-
quently observed target right-hand sides for each
source right-hand side. A second pass globally re-
moves hierarchical rules that were extracted fewer
than six times in the training data.
</bodyText>
<sectionHeader confidence="0.999883" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999934835616439">
We conduct experiments on Chinese-to-English MT,
using systems trained from the FBIS corpus of ap-
proximately 302,000 parallel sentence pairs. We
parse both sides of the training data with the Berke-
ley parsers (Petrov and Klein, 2007) for Chinese
and English. The English side is lowercased after
parsing; the Chinese side is segmented beforehand.
Unidirectional word alignments are obtained with
GIZA++ (Och and Ney, 2003) and symmetrized, re-
sulting in a parallel parsed corpus with Viterbi word
alignments for each sentence pair. Our modified ver-
sion of Thrax takes the parsed and aligned corpus as
input and returns a list of rules, which can then be
label-collapsed and scored as previously described.
In Thrax, we retain most of the default settings for
Hiero- and SAMT-style grammars as specified in the
extractor’s configuration file. Inheriting from Hiero,
we require the right-hand side of all rules to con-
tain at least one pair of aligned terminals, no more
than two nonterminals, and no more than five termi-
nals and nonterminal elements combined. Nonter-
minals are not allowed to be adjacent on the source
side, and they may not contain unaligned boundary
words. Rules themselves are not extracted from any
span in the training data longer than 10 tokens.
Our initial bilingual SAMT grammar uses 2699
unique source-side labels and 4181 unique target-
side labels, leading to the appearance of 29,088 joint
bilingual labels in the rule set. We provide the joint
labels (along with their counts) to the label collaps-
ing algorithm, while we strip out the source-side
labels to create the baseline SAMT grammar with
4181 unique target-side labels. Table 1 summarizes
how the number of target labels, unique extracted
rules, and the average number of pruned rules avail-
able per sentence change as the initial grammar is
label-collapsed to three progressively coarser de-
grees. Once the collapsing process has occurred ex-
haustively, the original SAMT grammar becomes a
Hiero-format grammar with a single nonterminal.
Each of the five grammars in Table 1 is used to
build an MT system. All systems are tuned and de-
coded with cdec (Dyer et al., 2010), an open-source
decoder for SCFG-based MT with arbitrary rule for-
mats and nonterminal labels. We tune the systems
on the 1664-sentence NIST Open MT 2006 data set,
optimizing towards the BLEU metric. Our test sets
are the NIST 2003 data set of 919 sentences and the
NIST 2008 data set of 1357 sentences. The tun-
ing set and both test sets all have four English ref-
erences.
We evaluate systems on BLEU (Papineni et al.,
2002), METEOR (Denkowski and Lavie, 2011), and
TER (Snover et al., 2006), as calculated in all three
cases by MultEval version 0.5.0.1 These scores for
the MT ’03 test set are shown in Table 2, and those
for the MT ’08 test set in Table 3, combined by Mult-
Eval over three optimization runs on the tuning set.
MultEval also implements statistical significance
testing between systems based on multiple optimizer
runs and approximate randomization. This process
(Clark et al., 2011) randomly swaps outputs between
systems and estimates the probability that the ob-
served score difference arose by chance. We report
these results in the tables as well for three MERT
runs and ap-value of 0.05. Systems that were judged
statistically different from the SAMT baseline have
triangles in the appropriate “Sig. SAMT?” columns;
systems judged different from the Hiero baseline
have triangles under the “Sig. Hiero?” columns. An
up-triangle (A) indicates that the system was better,
while a down-triangle (V) means that the baseline
was better.
</bodyText>
<footnote confidence="0.981454">
1https://github.com/jhclark/multeval
</footnote>
<page confidence="0.994682">
293
</page>
<table confidence="0.999089142857143">
Metric Scores Sig. SAMT? Sig. Hiero?
System BLEU MET TER B M T B M T
SAMT 31.18 30.64 61.02 O O O
Collapse 1 31.42 31.31 60.95 N O O
Collapse 2 31.90 31.73 60.98 N N O N O
Collapse 3 32.32 31.75 60.54 N N N N O
Hiero 32.30 31.42 60.10 N N N
</table>
<tableCaption confidence="0.940101">
Table 2: MT ’03 test set results. The first section gives automatic metric scores; the remaining sections indicate
whether each system is statistically significantly better (N) or worse (O) than the SAMT and Hiero baselines.
</tableCaption>
<table confidence="0.999721571428571">
Metric Scores Sig. SAMT? Sig. Hiero?
System BLEU MET TER B M T B M T
SAMT 22.10 24.94 63.78 O O O
Collapse 1 23.01 26.03 63.35 N N N N
Collapse 2 23.53 26.50 63.29 N N N N N
Collapse 3 23.61 26.37 63.07 N N N N N N
Hiero 23.01 25.72 63.53 N N N
</table>
<tableCaption confidence="0.826504">
Table 3: MT ’08 test set results. The first section gives automatic metric scores; the remaining sections indicate
whether each system is statistically significantly better (N) or worse (O) than the SAMT and Hiero baselines.
</tableCaption>
<figureCaption confidence="0.9026125">
Figure 4: Extracted frequency of each target-side label, with labels arranged in order of decreasing frequency count.
Note the log–log scale of the plot.
</figureCaption>
<page confidence="0.996891">
294
</page>
<sectionHeader confidence="0.989022" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999939883333333">
Tables 2 and 3 show that the coarsened grammars
significantly improve translation performance over
the SAMT baseline. This is especially true for the
“Collapse 3” setting of 72 labels, which scores 1.14
BLEU higher on MT ’03 and 1.51 BLEU higher on
MT ’08 than the uncollapsed system.
On the easier MT ’03 set, label-collapsed systems
do not generally outperform Hiero, although Col-
lapse 3 achieves a statistical tie according to BLEU
(+0.02) and a statistical improvement over Hiero ac-
cording to METEOR (+0.33). MT ’08 appears as
a significantly harder test set: metric scores for all
systems are drastically lower, and we find approxi-
mately 7% to 8% fewer phrase pair matches per sen-
tence. In this case the label-collapsed systems per-
form better, with all three of them achieving statisti-
cal significance over Hiero in at least one metric and
statistical ties in the other. The coarsened systems’
comparatively better performance on the harder test
set suggests that the linguistic information encoded
in multiple-nonterminal grammars helps the systems
more accurately parse new types of input.
Table 1 already showed at a global scale the strong
effect of label collapsing on reducing derivational
ambiguity, as labeled variants of the same basic
structural rule were progressively combined. Since
category coarsening is purely a relabeling operation,
any reordering pattern implemented in the original
SAMT grammar still exists in the collapsed ver-
sions; therefore, any reduction in the size of the
grammar is a reduction in variant labelings. Figure
4 shows this process in more detail for the baseline
SAMT grammar and the three collapsed grammars.
For each grammar, labels are arranged in decreas-
ing order of extracted frequency, and the frequency
count of each label is plotted. The long tail of rare
categories in the SAMT grammar (1950 labels seen
fewer than 100 times each) is combined into a pro-
gressively sharper distribution at each step. Not only
are there fewer rare labels, but these hard-to-model
categories consume a proportionally smaller fraction
of the total label set: from 47% in the baseline gram-
mar down to 26% in Collapse 3.
We find that label collapsing disproportionately
affects frequently extracted and hierarchical rules
over rarer rules and phrase pairs. The 15.7% re-
duction in total grammar size between the SAMT
baseline and the Collapse 3 system affects 18.0% of
the hierarchical rules, but only 1.6% of the phrase
pairs. If rules are counted separately each time they
match another source sentence, the average reduc-
tion in size of a sentence-filtered grammar is 57.8%.
Intuitively, hierarchical rules are more affected by
label collapsing because phrase pairs do not have
many variant left-hand-side labels to begin with,
while the same hierarchical rule pattern may be in-
stantiated in the grammar by a large number of vari-
ant labelings. We can see this situation in more de-
tail by counting variants of a particular set of rules.
Labeled forms of the Hiero-style rule
</bodyText>
<equation confidence="0.9745">
X _* [X1 X2] :: [the X2 of X1] (6)
</equation>
<bodyText confidence="0.999971">
are among the most frequently used rules in all five
of our systems. The way they are treated by label
collapsing thus has a strong impact on the results of
runtime decoding.
In the SAMT baseline, Rule (6) appears in the
grammar with 221 different labels in the X1 nonter-
minal slot, 53 labels for the X2 slot, and 90 choices
of left-hand side — a total of 1330 different label-
ings all together. More than three-fourths of these
variants were extracted three times or fewer from the
training data; even if they can be used in a test sen-
tence, statistical features for such low-count rules
are poorly estimated. During label collapsing, the
number of labeled variations of Rule (6) drops from
1330 to 325, to 96, and finally to 63 in the Collapse
3 grammar. There, the pattern is instantiated with 14
possible X1 labels, five X2 labels, and three different
left-hand sides.
It is difficult to measure rule sparsity directly (i.e.
to count the number of rules that are missing during
decoding), but a reduction in rule sparsity between
systems should be manifested as an increased num-
ber of hierarchical rule applications. Figure 5 shows
the average number of hierarchical rules applied per
sentence, distinguishing syntactic rules from glue
rules, on both test sets. The collapsed grammars al-
low for approximately one additional syntactic rule
application per sentence compared to the SAMT
baseline, or three additional applications compared
to Hiero. This shows an implicit reduction in miss-
ing syntactic rules in the collapsed grammars. In the
</bodyText>
<page confidence="0.993502">
295
</page>
<note confidence="0.975135">
MT 2003 MT 2008
</note>
<figureCaption confidence="0.999757">
Figure 5: Average number of hierarchical rules (both syntactic and glue rules) applied per sentence on each test set.
</figureCaption>
<bodyText confidence="0.99617475">
glue rule columns, we note that label collapsing also
promotes a shift away from generic glue rules, pos-
sibly via the creation of more permissive — but still
meaningfully labeled — syntactic rules.
</bodyText>
<sectionHeader confidence="0.999426" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999980139534884">
We demonstrated a viable technique for reducing the
label set size in SAMT grammars by temporarily in-
ducing bilingual syntax and using it in an existing
tree-to-tree category coarsening algorithm. In col-
lapsing SAMT category labels, we were able to sig-
nificantly improve translation quality while using a
grammar less than half the size of the original. We
believe it is also more robust to test-set or domain
variation than a single-nonterminal Hiero grammar.
Collapsed grammars confer practical benefits during
both model estimation and runtime decoding. We
showed that, in particular, they suffer less from rule
sparsity and derivational ambiguity problems that
are common to larger label sets.
We can highlight two areas for potential improve-
ments in future work. In our current implementation
of label collapsing, we indiscriminately allow either
source labels or target labels to be collapsed at each
iteration of the algorithm (see Equation 3). This is
an intuitively sensible setting when collapsing bilin-
gual labels, but it is perhaps less obviously so for a
monolingually labeled system such as SAMT. An al-
ternative would be to collapse target-side labels only,
leaving the source-side labels alone since they do not
appear in the final grammar anyway. In this case, the
target labels would be represented and clustered as
distributions over a static set of latent categories.
A larger area of future concern is the stopping
point of the collapsing algorithm. In our previ-
ous work (Hanneman and Lavie, 2011), we manu-
ally identified iterations in our run of the algorithm
where the Li distance between the most recently
collapsed label pair was markedly lower than the
Li difference of the pair in the previous iteration.
Such an approach is more feasible in our previous
runs of 120 iterations than in ours here of nearly
2100, where it is not likely that three manually cho-
sen stopping points represent the optimal collapsing
results. In future work, we plan to work towards the
development of an automatic stopping criterion, a
more principled test for whether each successive it-
eration of label collapsing provides some useful ben-
efit to the underlying grammar.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999841285714286">
This research work was supported in part by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
Thanks to Chris Dyer for providing the word-
aligned and preprocessed corpus we used in our ex-
periments. We also thank the anonymous reviewers
for helpful comments and suggestions for analysis.
</bodyText>
<sectionHeader confidence="0.997982" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.891247666666667">
Vamshi Ambati, Alon Lavie, and Jaime Carbonell. 2009.
Extraction of syntactic translation models from paral-
lel data using syntax from source and target languages.
</reference>
<page confidence="0.991454">
296
</page>
<reference confidence="0.998749457943926">
In Proceedings of the 12th Machine Translation Sum-
mit, pages 190–197, Ottawa, Canada, August.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Chris Callison-Burch, Nathaniel W. Filardo, Christine
Piatko, Lori Levin, and Scott Miller. 2012. Use of
modality and negation in semantically-informed syn-
tactic MT. Computational Linguistics, 38(2):411–
438.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443–1452, Uppsala, Sweden, July.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Crontrolling for optimizer insta-
bility. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Short
Papers, pages 176–181, Portland, OR, June.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 85–91, Edinburgh, United Kingdom, July.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7–12, Uppsala, Sweden, July.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In
HLT-NAACL 2004: Main Proceedings, pages 273–
280, Boston, MA, May.
Greg Hanneman and Alon Lavie. 2011. Automatic cate-
gory label coarsening for syntax-based machine trans-
lation. In Proceedings of SSST-5: Fifth Workshop on
Syntax, Semantics, and Structure in Statistical Trans-
lation, pages 98–106, Portland, OR, June.
Bryant Huang and Kevin Knight. 2006. Relabeling syn-
tax trees to improve syntax-based machine translation
quality. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the ACL, pages 240–247, New York, NY, June.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 138–147, Cambridge, MA, October.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87–95, Columbus, OH, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 609–616, Sydney, Aus-
tralia, July.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the 47th Annual Meeting of the ACL and
the Fourth IJCNLP of the AFNLP, pages 558–566,
Suntec, Singapore, August.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311–318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404–411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223–231, Cambridge, MA, August.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: Soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the ACL, pages 236–244, Boulder, CO,
June.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 478–
484, Edinburgh, United Kingdom, July.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138–141, New York, NY, June.
Andreas Zollmann. 2011. Learning Multiple-
Nonterminal Synchronous Grammars for Machine
Translation. Ph.D. thesis, Carnegie Mellon University.
</reference>
<page confidence="0.997429">
297
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550448">
<title confidence="0.999797">Improving Syntax-Augmented Machine Translation Coarsening the Label Set</title>
<author confidence="0.642738">Hanneman</author>
<affiliation confidence="0.7829715">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.999371">Pittsburgh, PA 15213</address>
<abstract confidence="0.998264882352941">We present a new variant of the Syntax- Augmented Machine Translation (SAMT) formalism with a category-coarsening algorithm originally developed for tree-to-tree grammars. We induce bilingual labels into the SAMT grammar, use them for category coarsening, then project back to monolingual labeling as in standard SAMT. The result is a “collapsed” grammar with the same expressive power and format as the original, but many fewer nonterminal labels. We show that the smaller label set provides improved translation scores by 1.14 BLEU on two Chinese– English test sets while reducing the occurrence of sparsity and ambiguity problems common to large label sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Alon Lavie</author>
<author>Jaime Carbonell</author>
</authors>
<title>Extraction of syntactic translation models from parallel data using syntax from source and target languages.</title>
<date>2009</date>
<contexts>
<context position="4631" citStr="Ambati et al. (2009)" startWordPosition="709" endWordPosition="712">which we treat more fully in Section 2.3, several other lines of work have explored increasing the nonterminal set for syntax-based MT. Huang and Knight (2006), for example, augmented the standard Penn Treebank labels for English by adding lexicalization to certain types of nodes. Chiang (2010) and Zollmann (2011) worked with a bilingual extension of SAMT that used its notion of “extended categories” on both the source and target sides. Taking standard monolingual SAMT as a baseline, Baker et al. (2012) developed a tagger to augment syntactic labels with some semantically derived information. Ambati et al. (2009) extracted tree-to-tree rules with similar extensions for sibling nodes, resulting again in a large number of labels. Extended categories allow for the extraction of a larger number of rules, increasing coverage and translation performance over systems that are limited to exact constituent matches only. However, the gains in coverage come with a corresponding increase in computational and modeling complexity due to the larger label set involved. Derivational ambiguity — the condition of having multiple derivations for the same output string — is a particular problem for parsing-based MT system</context>
</contexts>
<marker>Ambati, Lavie, Carbonell, 2009</marker>
<rawString>Vamshi Ambati, Alon Lavie, and Jaime Carbonell. 2009. Extraction of syntactic translation models from parallel data using syntax from source and target languages.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In Proceedings of the 12th Machine Translation Summit,</booktitle>
<pages>190--197</pages>
<location>Ottawa, Canada,</location>
<marker></marker>
<rawString>In Proceedings of the 12th Machine Translation Summit, pages 190–197, Ottawa, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathryn Baker</author>
<author>Michael Bloodgood</author>
<author>Bonnie J Dorr</author>
<author>Chris Callison-Burch</author>
<author>Nathaniel W Filardo</author>
<author>Christine Piatko</author>
<author>Lori Levin</author>
<author>Scott Miller</author>
</authors>
<title>Use of modality and negation in semantically-informed syntactic MT.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>2</issue>
<pages>438</pages>
<contexts>
<context position="4519" citStr="Baker et al. (2012)" startWordPosition="693" endWordPosition="696"> future work. 2 Background 2.1 Working with Large Label Sets Aside from the SAMT method of grammar extraction, which we treat more fully in Section 2.3, several other lines of work have explored increasing the nonterminal set for syntax-based MT. Huang and Knight (2006), for example, augmented the standard Penn Treebank labels for English by adding lexicalization to certain types of nodes. Chiang (2010) and Zollmann (2011) worked with a bilingual extension of SAMT that used its notion of “extended categories” on both the source and target sides. Taking standard monolingual SAMT as a baseline, Baker et al. (2012) developed a tagger to augment syntactic labels with some semantically derived information. Ambati et al. (2009) extracted tree-to-tree rules with similar extensions for sibling nodes, resulting again in a large number of labels. Extended categories allow for the extraction of a larger number of rules, increasing coverage and translation performance over systems that are limited to exact constituent matches only. However, the gains in coverage come with a corresponding increase in computational and modeling complexity due to the larger label set involved. Derivational ambiguity — the condition</context>
</contexts>
<marker>Baker, Bloodgood, Dorr, Callison-Burch, Filardo, Piatko, Levin, Miller, 2012</marker>
<rawString>Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris Callison-Burch, Nathaniel W. Filardo, Christine Piatko, Lori Levin, and Scott Miller. 2012. Use of modality and negation in semantically-informed syntactic MT. Computational Linguistics, 38(2):411– 438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1404" citStr="Chiang, 2007" startWordPosition="202" endWordPosition="204"> while reducing the occurrence of sparsity and ambiguity problems common to large label sets. 1 Introduction The formulation of statistical machine translation in terms of synchronous parsing has become both theoretically and practically successful. In a parsingbased MT formalism, synchronous context-free grammar rules that match a source-language input can be hierarchically composed to produce a corresponding target-language output. SCFG translation grammars can be extracted automatically from data. While formally syntactic approaches with a single grammar nonterminal have often worked well (Chiang, 2007), the desire to exploit linguistic knowledge has motivated the use of translation grammars with richer, linguistically syntactic nonterminal inventories (Galley et al., 2004; Liu et al., 2006; Lavie et al., 2008; Liu et al., 2009). Linguistically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-g</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1443--1452</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="4306" citStr="Chiang (2010)" startWordPosition="658" endWordPosition="659">en describe our method of label collapsing in SAMT grammars in Section 3. Experimental results are presented in Section 4 and analyzed in Section 5. Finally, Section 6 offers some conclusions and avenues for future work. 2 Background 2.1 Working with Large Label Sets Aside from the SAMT method of grammar extraction, which we treat more fully in Section 2.3, several other lines of work have explored increasing the nonterminal set for syntax-based MT. Huang and Knight (2006), for example, augmented the standard Penn Treebank labels for English by adding lexicalization to certain types of nodes. Chiang (2010) and Zollmann (2011) worked with a bilingual extension of SAMT that used its notion of “extended categories” on both the source and target sides. Taking standard monolingual SAMT as a baseline, Baker et al. (2012) developed a tagger to augment syntactic labels with some semantically derived information. Ambati et al. (2009) extracted tree-to-tree rules with similar extensions for sibling nodes, resulting again in a large number of labels. Extended categories allow for the extraction of a larger number of rules, increasing coverage and translation performance over systems that are limited to ex</context>
<context position="6522" citStr="Chiang (2010)" startWordPosition="1010" endWordPosition="1011">SCFG rule application: in order to compose two rules, the left-hand-side label of the smaller rule must match a right-hand-side label in the larger rule it is being plugged in to. With large label sets, it becomes less likely that two arbitrarily chosen rules can compose, making the grammar less flexible for representing new sentences. Previous research has attempted to address both of these problems in different ways. Preference grammars (Venugopal et al., 2009) are a technique for reducing derivational ambiguity by summing scores over labeled variants of the same derivation during decoding. Chiang (2010) addressed rule sparsity by introducing a soft matching constraint: the decoder may pay a learned label-pair-specific penalty for substituting a rule headed by one label into a substitution slot marked for another. Combining properties of both of the above methods, Huang et al. (2010) modeled monolingual labels as distributions over latent syntactic categories and calculated similarity scores between them for rule composition. 2.2 Label Collapsing in Tree-to-Tree Rules Aiming to reduce both derivational ambiguity and rule sparsity, we previously presented a “label collapsing” algorithm for sys</context>
<context position="11167" citStr="Chiang (2010)" startWordPosition="1783" endWordPosition="1784">according to the distance metrics of Equations (1) and (2), combines those two labels into a new one, and updates the set of P(s |t) While the SAMT label formats can be trivially converted into joint labels X::t, X::t1+t2, X::t1/t2, X::t1\t2, and X::X, they cannot be usefully fed into the label collapsing algorithm because the necessary conditional label probabilities are meaningless. To acquire meaningful source-side labels, we turn to a 290 (a) (b) Figure 2: Sample extraction of an SAMT grammar rule: (a) with monolingual syntax and (b) with bilingual syntax. bilingual SAMT extension used by Chiang (2010) and Zollmann (2011). Both a source- and a targetside parse tree are used to extract rules from a parallel sentence; two SAMT-style labels are worked out independently on each side for each nonterminal instance, then packed into a joint label. It is therefore possible for a nonterminal instance to be labeled s::t, s1\s2::t, s1+s2::t1/t2, or various other combinations depending on what parse nodes the nonterminal spans in each tree. Such a bilingually labeled rule is extracted in Figure 2(b). The target-side labels from Figure 2(a) are now paired with source-side labels extracted from an added </context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443–1452, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Crontrolling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Short Papers,</booktitle>
<pages>176--181</pages>
<location>Portland, OR,</location>
<contexts>
<context position="19667" citStr="Clark et al., 2011" startWordPosition="3171" endWordPosition="3174">a set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown in Table 2, and those for the MT ’08 test set in Table 3, combined by MultEval over three optimization runs on the tuning set. MultEval also implements statistical significance testing between systems based on multiple optimizer runs and approximate randomization. This process (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. We report these results in the tables as well for three MERT runs and ap-value of 0.05. Systems that were judged statistically different from the SAMT baseline have triangles in the appropriate “Sig. SAMT?” columns; systems judged different from the Hiero baseline have triangles under the “Sig. Hiero?” columns. An up-triangle (A) indicates that the system was better, while a down-triangle (V) means that the baseline was better. 1https://github.com/jhclark/multeval 293 Metri</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Crontrolling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Short Papers, pages 176–181, Portland, OR, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>85--91</pages>
<location>Edinburgh, United Kingdom,</location>
<contexts>
<context position="19228" citStr="Denkowski and Lavie, 2011" startWordPosition="3097" endWordPosition="3100"> grammar with a single nonterminal. Each of the five grammars in Table 1 is used to build an MT system. All systems are tuned and decoded with cdec (Dyer et al., 2010), an open-source decoder for SCFG-based MT with arbitrary rule formats and nonterminal labels. We tune the systems on the 1664-sentence NIST Open MT 2006 data set, optimizing towards the BLEU metric. Our test sets are the NIST 2003 data set of 919 sentences and the NIST 2008 data set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown in Table 2, and those for the MT ’08 test set in Table 3, combined by MultEval over three optimization runs on the tuning set. MultEval also implements statistical significance testing between systems based on multiple optimizer runs and approximate randomization. This process (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. We report these results in the tables a</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 85–91, Edinburgh, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="18769" citStr="Dyer et al., 2010" startWordPosition="3016" endWordPosition="3019">ithm, while we strip out the source-side labels to create the baseline SAMT grammar with 4181 unique target-side labels. Table 1 summarizes how the number of target labels, unique extracted rules, and the average number of pruned rules available per sentence change as the initial grammar is label-collapsed to three progressively coarser degrees. Once the collapsing process has occurred exhaustively, the original SAMT grammar becomes a Hiero-format grammar with a single nonterminal. Each of the five grammars in Table 1 is used to build an MT system. All systems are tuned and decoded with cdec (Dyer et al., 2010), an open-source decoder for SCFG-based MT with arbitrary rule formats and nonterminal labels. We tune the systems on the 1664-sentence NIST Open MT 2006 data set, optimizing towards the BLEU metric. Our test sets are the NIST 2003 data set of 919 sentences and the NIST 2008 data set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown </context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, pages 7–12, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Main Proceedings,</booktitle>
<pages>273--280</pages>
<location>Boston, MA,</location>
<contexts>
<context position="1577" citStr="Galley et al., 2004" startWordPosition="226" endWordPosition="229">s of synchronous parsing has become both theoretically and practically successful. In a parsingbased MT formalism, synchronous context-free grammar rules that match a source-language input can be hierarchically composed to produce a corresponding target-language output. SCFG translation grammars can be extracted automatically from data. While formally syntactic approaches with a single grammar nonterminal have often worked well (Chiang, 2007), the desire to exploit linguistic knowledge has motivated the use of translation grammars with richer, linguistically syntactic nonterminal inventories (Galley et al., 2004; Liu et al., 2006; Lavie et al., 2008; Liu et al., 2009). Linguistically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-grained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data spars</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLT-NAACL 2004: Main Proceedings, pages 273– 280, Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Hanneman</author>
<author>Alon Lavie</author>
</authors>
<title>Automatic category label coarsening for syntax-based machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of SSST-5: Fifth Workshop on Syntax, Semantics, and Structure in Statistical Translation,</booktitle>
<pages>98--106</pages>
<location>Portland, OR,</location>
<contexts>
<context position="2629" citStr="Hanneman and Lavie (2011)" startWordPosition="392" endWordPosition="395">rained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity. In this paper, we focus on the Syntax-Augmented MT formalism (Zollmann and Venugopal, 2006), a monolingually labeled version of Hiero that can create up to 4000 “extended” category labels based on pairs of parse nodes. We take a standard SAMT grammar with target-side labels and extend its labeling to a bilingual format (Zollmann, 2011). We then coarsen the bilingual labels following the “label collapsing” algorithm of Hanneman and Lavie (2011). This represents a novel extension of the tree-to-tree collapsing algorithm to the SAMT formalism. After removing the source-side labels, we obtain a new SAMT grammar with coarser target-side labels than the original. Coarsened grammars provide improvement of up to 1.14 BLEU points over the baseline SAMT results on two Chinese–English test sets; they also outperform a Hiero baseline by up to 0.60 BLEU on one of the sets. Aside from improved translation quality, in analysis we find significant reductions in derivational ambiguity and rule sparsity, two problems that make large nonterminal sets</context>
<context position="7188" citStr="Hanneman and Lavie, 2011" startWordPosition="1107" endWordPosition="1110"> soft matching constraint: the decoder may pay a learned label-pair-specific penalty for substituting a rule headed by one label into a substitution slot marked for another. Combining properties of both of the above methods, Huang et al. (2010) modeled monolingual labels as distributions over latent syntactic categories and calculated similarity scores between them for rule composition. 2.2 Label Collapsing in Tree-to-Tree Rules Aiming to reduce both derivational ambiguity and rule sparsity, we previously presented a “label collapsing” algorithm for systems in which bilingual labels are used (Hanneman and Lavie, 2011). It coarsens the overall label set by clustering monolingual labels based on which labels they appear joined with in the other language. The label collapsing algorithm takes as its input a set of SCFG rule instances extracted from a parallel corpus. Each time a tree-to-tree rule is extracted, its left-hand side is a label of the form s::t, where s is a label from the source-language category set 5 and t is a label from the target-language category set T. Operationally, the joint label means that a source-side subtree rooted at s was the translational equivalent of a target-side subtree rooted</context>
<context position="27818" citStr="Hanneman and Lavie, 2011" startWordPosition="4532" endWordPosition="4535">lapsed at each iteration of the algorithm (see Equation 3). This is an intuitively sensible setting when collapsing bilingual labels, but it is perhaps less obviously so for a monolingually labeled system such as SAMT. An alternative would be to collapse target-side labels only, leaving the source-side labels alone since they do not appear in the final grammar anyway. In this case, the target labels would be represented and clustered as distributions over a static set of latent categories. A larger area of future concern is the stopping point of the collapsing algorithm. In our previous work (Hanneman and Lavie, 2011), we manually identified iterations in our run of the algorithm where the Li distance between the most recently collapsed label pair was markedly lower than the Li difference of the pair in the previous iteration. Such an approach is more feasible in our previous runs of 120 iterations than in ours here of nearly 2100, where it is not likely that three manually chosen stopping points represent the optimal collapsing results. In future work, we plan to work towards the development of an automatic stopping criterion, a more principled test for whether each successive iteration of label collapsin</context>
</contexts>
<marker>Hanneman, Lavie, 2011</marker>
<rawString>Greg Hanneman and Alon Lavie. 2011. Automatic category label coarsening for syntax-based machine translation. In Proceedings of SSST-5: Fifth Workshop on Syntax, Semantics, and Structure in Statistical Translation, pages 98–106, Portland, OR, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryant Huang</author>
<author>Kevin Knight</author>
</authors>
<title>Relabeling syntax trees to improve syntax-based machine translation quality.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL,</booktitle>
<pages>240--247</pages>
<location>New York, NY,</location>
<contexts>
<context position="4170" citStr="Huang and Knight (2006)" startWordPosition="635" endWordPosition="638"> addressing those problems. The section also summarizes the tree-to-tree label collapsing algorithm and the process of SAMT rule extraction. We then describe our method of label collapsing in SAMT grammars in Section 3. Experimental results are presented in Section 4 and analyzed in Section 5. Finally, Section 6 offers some conclusions and avenues for future work. 2 Background 2.1 Working with Large Label Sets Aside from the SAMT method of grammar extraction, which we treat more fully in Section 2.3, several other lines of work have explored increasing the nonterminal set for syntax-based MT. Huang and Knight (2006), for example, augmented the standard Penn Treebank labels for English by adding lexicalization to certain types of nodes. Chiang (2010) and Zollmann (2011) worked with a bilingual extension of SAMT that used its notion of “extended categories” on both the source and target sides. Taking standard monolingual SAMT as a baseline, Baker et al. (2012) developed a tagger to augment syntactic labels with some semantically derived information. Ambati et al. (2009) extracted tree-to-tree rules with similar extensions for sibling nodes, resulting again in a large number of labels. Extended categories a</context>
</contexts>
<marker>Huang, Knight, 2006</marker>
<rawString>Bryant Huang and Kevin Knight. 2006. Relabeling syntax trees to improve syntax-based machine translation quality. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 240–247, New York, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Martin Cmejrek</author>
<author>Bowen Zhou</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>138--147</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="6807" citStr="Huang et al. (2010)" startWordPosition="1053" endWordPosition="1056"> the grammar less flexible for representing new sentences. Previous research has attempted to address both of these problems in different ways. Preference grammars (Venugopal et al., 2009) are a technique for reducing derivational ambiguity by summing scores over labeled variants of the same derivation during decoding. Chiang (2010) addressed rule sparsity by introducing a soft matching constraint: the decoder may pay a learned label-pair-specific penalty for substituting a rule headed by one label into a substitution slot marked for another. Combining properties of both of the above methods, Huang et al. (2010) modeled monolingual labels as distributions over latent syntactic categories and calculated similarity scores between them for rule composition. 2.2 Label Collapsing in Tree-to-Tree Rules Aiming to reduce both derivational ambiguity and rule sparsity, we previously presented a “label collapsing” algorithm for systems in which bilingual labels are used (Hanneman and Lavie, 2011). It coarsens the overall label set by clustering monolingual labels based on which labels they appear joined with in the other language. The label collapsing algorithm takes as its input a set of SCFG rule instances ex</context>
</contexts>
<marker>Huang, Cmejrek, Zhou, 2010</marker>
<rawString>Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou. 2010. Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Alok Parlikar</author>
<author>Vamshi Ambati</author>
</authors>
<title>Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>87--95</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="1615" citStr="Lavie et al., 2008" startWordPosition="234" endWordPosition="237">h theoretically and practically successful. In a parsingbased MT formalism, synchronous context-free grammar rules that match a source-language input can be hierarchically composed to produce a corresponding target-language output. SCFG translation grammars can be extracted automatically from data. While formally syntactic approaches with a single grammar nonterminal have often worked well (Chiang, 2007), the desire to exploit linguistic knowledge has motivated the use of translation grammars with richer, linguistically syntactic nonterminal inventories (Galley et al., 2004; Liu et al., 2006; Lavie et al., 2008; Liu et al., 2009). Linguistically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-grained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity. In this paper, we focus on the Sy</context>
</contexts>
<marker>Lavie, Parlikar, Ambati, 2008</marker>
<rawString>Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008. Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora. In Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation, pages 87–95, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>609--616</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1595" citStr="Liu et al., 2006" startWordPosition="230" endWordPosition="233">ing has become both theoretically and practically successful. In a parsingbased MT formalism, synchronous context-free grammar rules that match a source-language input can be hierarchically composed to produce a corresponding target-language output. SCFG translation grammars can be extracted automatically from data. While formally syntactic approaches with a single grammar nonterminal have often worked well (Chiang, 2007), the desire to exploit linguistic knowledge has motivated the use of translation grammars with richer, linguistically syntactic nonterminal inventories (Galley et al., 2004; Liu et al., 2006; Lavie et al., 2008; Liu et al., 2009). Linguistically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-grained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity. In this paper</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609–616, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the Fourth IJCNLP of the AFNLP,</booktitle>
<pages>558--566</pages>
<location>Suntec, Singapore,</location>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings of the 47th Annual Meeting of the ACL and the Fourth IJCNLP of the AFNLP, pages 558–566, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17043" citStr="Och and Ney, 2003" startWordPosition="2729" endWordPosition="2732">ly observed target right-hand sides for each source right-hand side. A second pass globally removes hierarchical rules that were extracted fewer than six times in the training data. 4 Experiments We conduct experiments on Chinese-to-English MT, using systems trained from the FBIS corpus of approximately 302,000 parallel sentence pairs. We parse both sides of the training data with the Berkeley parsers (Petrov and Klein, 2007) for Chinese and English. The English side is lowercased after parsing; the Chinese side is segmented beforehand. Unidirectional word alignments are obtained with GIZA++ (Och and Ney, 2003) and symmetrized, resulting in a parallel parsed corpus with Viterbi word alignments for each sentence pair. Our modified version of Thrax takes the parsed and aligned corpus as input and returns a list of rules, which can then be label-collapsed and scored as previously described. In Thrax, we retain most of the default settings for Hiero- and SAMT-style grammars as specified in the extractor’s configuration file. Inheriting from Hiero, we require the right-hand side of all rules to contain at least one pair of aligned terminals, no more than two nonterminals, and no more than five terminals </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evalution of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="19192" citStr="Papineni et al., 2002" startWordPosition="3092" endWordPosition="3095">T grammar becomes a Hiero-format grammar with a single nonterminal. Each of the five grammars in Table 1 is used to build an MT system. All systems are tuned and decoded with cdec (Dyer et al., 2010), an open-source decoder for SCFG-based MT with arbitrary rule formats and nonterminal labels. We tune the systems on the 1664-sentence NIST Open MT 2006 data set, optimizing towards the BLEU metric. Our test sets are the NIST 2003 data set of 919 sentences and the NIST 2008 data set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown in Table 2, and those for the MT ’08 test set in Table 3, combined by MultEval over three optimization runs on the tuning set. MultEval also implements statistical significance testing between systems based on multiple optimizer runs and approximate randomization. This process (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. We </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evalution of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>404--411</pages>
<location>Rochester, NY,</location>
<contexts>
<context position="16854" citStr="Petrov and Klein, 2007" startWordPosition="2701" endWordPosition="2704">umber of target-side labels, unique rules in the whole grammar, and average number of pruned rules after filtering to individual sentences. size. One pruning pass keeps only the 80 most frequently observed target right-hand sides for each source right-hand side. A second pass globally removes hierarchical rules that were extracted fewer than six times in the training data. 4 Experiments We conduct experiments on Chinese-to-English MT, using systems trained from the FBIS corpus of approximately 302,000 parallel sentence pairs. We parse both sides of the training data with the Berkeley parsers (Petrov and Klein, 2007) for Chinese and English. The English side is lowercased after parsing; the Chinese side is segmented beforehand. Unidirectional word alignments are obtained with GIZA++ (Och and Ney, 2003) and symmetrized, resulting in a parallel parsed corpus with Viterbi word alignments for each sentence pair. Our modified version of Thrax takes the parsed and aligned corpus as input and returns a list of rules, which can then be label-collapsed and scored as previously described. In Thrax, we retain most of the default settings for Hiero- and SAMT-style grammars as specified in the extractor’s configuratio</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL HLT 2007, pages 404–411, Rochester, NY, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Seventh Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="19259" citStr="Snover et al., 2006" startWordPosition="3103" endWordPosition="3106">ach of the five grammars in Table 1 is used to build an MT system. All systems are tuned and decoded with cdec (Dyer et al., 2010), an open-source decoder for SCFG-based MT with arbitrary rule formats and nonterminal labels. We tune the systems on the 1664-sentence NIST Open MT 2006 data set, optimizing towards the BLEU metric. Our test sets are the NIST 2003 data set of 919 sentences and the NIST 2008 data set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown in Table 2, and those for the MT ’08 test set in Table 3, combined by MultEval over three optimization runs on the tuning set. MultEval also implements statistical significance testing between systems based on multiple optimizer runs and approximate randomization. This process (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. We report these results in the tables as well for three MERT runs and </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the Seventh Conference of the Association for Machine Translation in the Americas, pages 223–231, Cambridge, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Preference grammars: Softening syntactic constraints to improve statistical machine translation.</title>
<date>2009</date>
<booktitle>In Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>236--244</pages>
<location>Boulder, CO,</location>
<contexts>
<context position="6376" citStr="Venugopal et al., 2009" startWordPosition="986" endWordPosition="989">t match — has been variously identified as “data sparsity,” the “matching constraint,” and “rule sparsity” in the grammar. It arises from the definition of SCFG rule application: in order to compose two rules, the left-hand-side label of the smaller rule must match a right-hand-side label in the larger rule it is being plugged in to. With large label sets, it becomes less likely that two arbitrarily chosen rules can compose, making the grammar less flexible for representing new sentences. Previous research has attempted to address both of these problems in different ways. Preference grammars (Venugopal et al., 2009) are a technique for reducing derivational ambiguity by summing scores over labeled variants of the same derivation during decoding. Chiang (2010) addressed rule sparsity by introducing a soft matching constraint: the decoder may pay a learned label-pair-specific penalty for substituting a rule headed by one label into a substitution slot marked for another. Combining properties of both of the above methods, Huang et al. (2010) modeled monolingual labels as distributions over latent syntactic categories and calculated similarity scores between them for rule composition. 2.2 Label Collapsing in</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2009. Preference grammars: Softening syntactic constraints to improve statistical machine translation. In Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236–244, Boulder, CO, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Weese</author>
<author>Juri Ganitkevitch</author>
<author>Chris CallisonBurch</author>
<author>Matt Post</author>
<author>Adam Lopez</author>
</authors>
<title>Joshua 3.0: Syntax-based machine translation with the Thrax grammar extractor.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>478--484</pages>
<location>Edinburgh, United Kingdom,</location>
<contexts>
<context position="12023" citStr="Weese et al., 2011" startWordPosition="1921" endWordPosition="1924">label. It is therefore possible for a nonterminal instance to be labeled s::t, s1\s2::t, s1+s2::t1/t2, or various other combinations depending on what parse nodes the nonterminal spans in each tree. Such a bilingually labeled rule is extracted in Figure 2(b). The target-side labels from Figure 2(a) are now paired with source-side labels extracted from an added Chinese parse tree. In this case, the abstracted sub-phrase supported each other is given the joint label VP::VBD+NP, while the rule’s left-hand side becomes LCP+VP::VP. We implement bilingual SAMT grammar extraction by modifying Thrax (Weese et al., 2011), an open-source, Hadoop-based framework for extracting standard SAMT grammars. By default, Thrax can produce grammars labeled either on the source or target side, but not both. It also outputs rules that are already scored according to a user-specified set of translation model features, meaning that the raw rule counts needed to compute the label conditional probabilities P(s |t) and P(t |s) are not directly available. We implement a new subclass of grammar extractor with logic for independently labeling both sides of an SAMT rule in order to get the necessary bilingual labels; an adaptation </context>
</contexts>
<marker>Weese, Ganitkevitch, CallisonBurch, Post, Lopez, 2011</marker>
<rawString>Jonathan Weese, Juri Ganitkevitch, Chris CallisonBurch, Matt Post, and Adam Lopez. 2011. Joshua 3.0: Syntax-based machine translation with the Thrax grammar extractor. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 478– 484, Edinburgh, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<location>New York, NY,</location>
<contexts>
<context position="2273" citStr="Zollmann and Venugopal, 2006" startWordPosition="333" endWordPosition="336">ically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-grained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity. In this paper, we focus on the Syntax-Augmented MT formalism (Zollmann and Venugopal, 2006), a monolingually labeled version of Hiero that can create up to 4000 “extended” category labels based on pairs of parse nodes. We take a standard SAMT grammar with target-side labels and extend its labeling to a bilingual format (Zollmann, 2011). We then coarsen the bilingual labels following the “label collapsing” algorithm of Hanneman and Lavie (2011). This represents a novel extension of the tree-to-tree collapsing algorithm to the SAMT formalism. After removing the source-side labels, we obtain a new SAMT grammar with coarser target-side labels than the original. Coarsened grammars provid</context>
<context position="9141" citStr="Zollmann and Venugopal, 2006" startWordPosition="1440" endWordPosition="1443"> grammar give rise to a simple L1 distance metric over any pair of monolingual labels: �d(s1, s2) = |P(t |s1) − P(t |s2) |(1) tET and P(t |s) values appropriately. The choice of label pair to collapse in each iteration can be expressed formally as arg min {d(sZ, sj), d(tk, t�)1 (3) (si,sj)ES2,(tk,te)ET2 That is, either a source label pair or a target label pair may be chosen by the algorithm in each iteration. 2.3 SAMT Rule Extraction SAMT grammars pose a challenge to the label collapsing algorithm described above because their label sets are usually monolingual. The classic SAMT formulation (Zollmann and Venugopal, 2006) produces a grammar labeled on the target side only. Nonterminal instances that exactly match a targetlanguage syntactic constituent in a parallel sentence are given labels of the form t. Labels of the form t1+t2 are assigned to nonterminals that span exactly two contiguous parse nodes. Categorial grammar labels such as t1/t2 and t1\t2 are given to nonterminals that span an incomplete t1 constituent missing a t2 node to its right or left, respectively. Any nonterminal that cannot be labeled by one of the above three schemes is assigned the default label X. Figure 2(a) shows the extraction of a</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, pages 138–141, New York, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
</authors>
<title>Learning MultipleNonterminal Synchronous Grammars for Machine Translation.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="2519" citStr="Zollmann, 2011" startWordPosition="377" endWordPosition="378">r’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-grained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity. In this paper, we focus on the Syntax-Augmented MT formalism (Zollmann and Venugopal, 2006), a monolingually labeled version of Hiero that can create up to 4000 “extended” category labels based on pairs of parse nodes. We take a standard SAMT grammar with target-side labels and extend its labeling to a bilingual format (Zollmann, 2011). We then coarsen the bilingual labels following the “label collapsing” algorithm of Hanneman and Lavie (2011). This represents a novel extension of the tree-to-tree collapsing algorithm to the SAMT formalism. After removing the source-side labels, we obtain a new SAMT grammar with coarser target-side labels than the original. Coarsened grammars provide improvement of up to 1.14 BLEU points over the baseline SAMT results on two Chinese–English test sets; they also outperform a Hiero baseline by up to 0.60 BLEU on one of the sets. Aside from improved translation quality, in analysis we find sig</context>
<context position="4326" citStr="Zollmann (2011)" startWordPosition="661" endWordPosition="662">thod of label collapsing in SAMT grammars in Section 3. Experimental results are presented in Section 4 and analyzed in Section 5. Finally, Section 6 offers some conclusions and avenues for future work. 2 Background 2.1 Working with Large Label Sets Aside from the SAMT method of grammar extraction, which we treat more fully in Section 2.3, several other lines of work have explored increasing the nonterminal set for syntax-based MT. Huang and Knight (2006), for example, augmented the standard Penn Treebank labels for English by adding lexicalization to certain types of nodes. Chiang (2010) and Zollmann (2011) worked with a bilingual extension of SAMT that used its notion of “extended categories” on both the source and target sides. Taking standard monolingual SAMT as a baseline, Baker et al. (2012) developed a tagger to augment syntactic labels with some semantically derived information. Ambati et al. (2009) extracted tree-to-tree rules with similar extensions for sibling nodes, resulting again in a large number of labels. Extended categories allow for the extraction of a larger number of rules, increasing coverage and translation performance over systems that are limited to exact constituent matc</context>
<context position="11187" citStr="Zollmann (2011)" startWordPosition="1786" endWordPosition="1787">istance metrics of Equations (1) and (2), combines those two labels into a new one, and updates the set of P(s |t) While the SAMT label formats can be trivially converted into joint labels X::t, X::t1+t2, X::t1/t2, X::t1\t2, and X::X, they cannot be usefully fed into the label collapsing algorithm because the necessary conditional label probabilities are meaningless. To acquire meaningful source-side labels, we turn to a 290 (a) (b) Figure 2: Sample extraction of an SAMT grammar rule: (a) with monolingual syntax and (b) with bilingual syntax. bilingual SAMT extension used by Chiang (2010) and Zollmann (2011). Both a source- and a targetside parse tree are used to extract rules from a parallel sentence; two SAMT-style labels are worked out independently on each side for each nonterminal instance, then packed into a joint label. It is therefore possible for a nonterminal instance to be labeled s::t, s1\s2::t, s1+s2::t1/t2, or various other combinations depending on what parse nodes the nonterminal spans in each tree. Such a bilingually labeled rule is extracted in Figure 2(b). The target-side labels from Figure 2(a) are now paired with source-side labels extracted from an added Chinese parse tree. </context>
</contexts>
<marker>Zollmann, 2011</marker>
<rawString>Andreas Zollmann. 2011. Learning MultipleNonterminal Synchronous Grammars for Machine Translation. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>