<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011494">
<title confidence="0.9966385">
Statistical Machine Translation through Global Lexical Selection and
Sentence Reconstruction
</title>
<author confidence="0.958486">
Srinivas Bangalore, Patrick Haffner, Stephan Kanthak
</author>
<affiliation confidence="0.834683">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.92734">
180 Park Ave, Florham Park, NJ 07932
</address>
<email confidence="0.999156">
{srini,haffner,skanthak}@research.att.com
</email>
<sectionHeader confidence="0.997393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917">
Machine translation of a source language
sentence involves selecting appropriate tar-
get language words and ordering the se-
lected words to form a well-formed tar-
get language sentence. Most of the pre-
vious work on statistical machine transla-
tion relies on (local) associations of target
words/phrases with source words/phrases
for lexical selection. In contrast, in this pa-
per, we present a novel approach to lexical
selection where the target words are associ-
ated with the entire source sentence (global)
without the need to compute local associa-
tions. Further, we present a technique for
reconstructing the target language sentence
from the selected words. We compare the re-
sults of this approach against those obtained
from a finite-state based statistical machine
translation system which relies on local lex-
ical associations.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958381818182">
Machine translation can be viewed as consisting of
two subproblems: (a) lexical selection, where appro-
priate target language lexical items are chosen for
each source language lexical item and (b) lexical re-
ordering, where the chosen target language lexical
items are rearranged to produce a meaningful target
language string. Most of the previous work on statis-
tical machine translation, as exemplified in (Brown
et al., 1993), employs word-alignment algorithm
(such as GIZA++ (Och and Ney, 2003)) that pro-
vides local associations between source and target
words. The source-to-target word alignments are
sometimes augmented with target-to-source word
alignments in order to improve precision. Further,
the word-level alignments are extended to phrase-
level alignments in order to increase the extent of
local associations. The phrasal associations compile
some amount of (local) lexical reordering of the tar-
get words – those permitted by the size of the phrase.
Most of the state-of-the-art machine translation sys-
tems use phrase-level associations in conjunction
with a target language model to produce sentences.
There is relatively little emphasis on (global) lexical
reordering other than the local reorderings permit-
ted within the phrasal alignments. A few exceptions
are the hierarchical (possibly syntax-based) trans-
duction models (Wu, 1997; Alshawi et al., 1998;
Yamada and Knight, 2001; Chiang, 2005) and the
string transduction models (Kanthak et al., 2005).
In this paper, we present an alternate approach to
lexical selection and lexical reordering. For lexical
selection, in contrast to the local approaches of as-
sociating target to source words, we associate tar-
get words to the entire source sentence. The intu-
ition is that there may be lexico-syntactic features of
the source sentence (not necessarily a single source
word) that might trigger the presence of a target
word in the target sentence. Furthermore, it might be
difficult to exactly associate a target word to a source
word in many situations – (a) when the translations
are not exact but paraphrases (b) when the target lan-
guage does not have one lexical item to express the
same concept that is expressed by a source word.
Extending word to phrase alignments attempts to ad-
dress some of these situations while alleviating the
noise in word-level alignments.
As a consequence of this global lexical selection
approach, we no longer have a tight association be-
tween source and target language words. The re-
sult of lexical selection is simply a bag of words in
the target language and the sentence has to be recon-
structed using this bag of words. The words in the
bag, however, might be enhanced with rich syntactic
information that could aid in reconstructing the tar-
get sentence. This approach to lexical selection and
</bodyText>
<page confidence="0.968469">
152
</page>
<note confidence="0.9243215">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 152–159,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<figure confidence="0.9931915625">
Phrase Segmented
Bilanguage Bilanguage Joint Language
Sentence Aligned
Corpus
Alignment
Word
Alignment Bilanguage
Transformation
Local Phrase
Reordering
Joint Language
Modeling
WFSA
FSA to FST
Translation model
WFST
</figure>
<figureCaption confidence="0.999987">
Figure 1: Training phases for our system
Figure 2: Decoding phases for our system
</figureCaption>
<bodyText confidence="0.999344090909091">
sentence reconstruction has the potential to circum-
vent limitations of word-alignment based methods
for translation between languages with significantly
different word order (e.g. English-Japanese).
In this paper, we present the details of training
a global lexical selection model using classifica-
tion techniques and sentence reconstruction mod-
els using permutation automata. We also present a
stochastic finite-state transducer (SFST) as an exam-
ple of an approach that relies on local associations
and use it to compare and contrast our approach.
</bodyText>
<sectionHeader confidence="0.85167" genericHeader="method">
2 SFST Training and Decoding
</sectionHeader>
<bodyText confidence="0.999898">
In this section, we describe each of the components
of our SFST system shown in Figure 1. The SFST
approach described here is similar to the one de-
scribed in (Bangalore and Riccardi, 2000) which has
subsequently been adopted by (Banchs et al., 2005).
</bodyText>
<subsectionHeader confidence="0.991474">
2.1 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999971333333333">
The first stage in the process of training a lexical se-
lection model is obtaining an alignment function (f)
that given a pair of source (s1s2 ... sn) and target
(t1t2 ... tm) language sentences, maps source lan-
guage word subsequences into target language word
subsequences, as shown below.
</bodyText>
<equation confidence="0.999218">
∀i∃j(f(si) = tj ∨ f(si) = 2) (1)
</equation>
<bodyText confidence="0.999954375">
For the work reported in this paper, we have used
the GIZA++ tool (Och and Ney, 2003) which im-
plements a string-alignment algorithm. GIZA++
alignment however is asymmetric in that the word
mappings are different depending on the direction
of alignment – source-to-target or target-to-source.
Hence in addition to the functions f as shown in
Equation 1 we train another alignment function g :
</bodyText>
<equation confidence="0.97625725">
∀j∃i(g(tj) = si ∨ g(tj) = 2) (2)
English: I need to make a collect call
Japanese:fj6j nVlF n—)SI- b,(t7D �LARI���bh1_t
Alignment: 1 5 0 3 0 2 4
</equation>
<figureCaption confidence="0.9539125">
Figure 3: Example bilingual texts with alignment in-
formation
</figureCaption>
<figure confidence="0.6389775">
I:fjj� need:,LVb,�)9t* to:2 make::2—iLi�
a:2 collect :2L i call b)(16
</figure>
<figureCaption confidence="0.9934095">
Figure 4: Bilanguage strings resulting from align-
ments shown in Figure 3.
</figureCaption>
<subsectionHeader confidence="0.999322">
2.2 Bilanguage Representation
</subsectionHeader>
<bodyText confidence="0.989668761904762">
From the alignment information (see Figure 3), we
construct a bilanguage representation of each sen-
tence in the bilingual corpus. The bilanguage string
consists of source-target symbol pair sequences as
shown in Equation 3. Note that the tokens of a bilan-
guage could be either ordered according to the word
order of the source language or ordered according to
the word order of the target language.
Bf = bf1 bf2 ... bf (3)
m
bfi = (si−1; si, f(si)) if f(si−1) = 2
= (si, f(si−1); f(si)) if si−1 = 2
= (si, f(si)) otherwise
Figure 4 shows an example alignment and the
source-word-ordered bilanguage strings correspond-
ing to the alignment shown in Figure 3.
We also construct a bilanguage using the align-
ment function g similar to the bilanguage using the
alignment function f as shown in Equation 3.
Thus, the bilanguage corpus obtained by combin-
ing the two alignment functions is B = Bf ∪ Bg.
</bodyText>
<subsectionHeader confidence="0.999925">
2.3 Bilingual Phrases and Local Reordering
</subsectionHeader>
<bodyText confidence="0.998931272727273">
While word-to-word translation only approximates
the lexical selection process, phrase-to-phrase map-
ping can greatly improve the translation of colloca-
tions, recurrent strings, etc. Using phrases also al-
lows words within the phrase to be reordered into the
correct target language order, thus partially solving
the reordering problem. Additionally, SFSTs can
take advantage of phrasal correlations to improve the
computation of the probability P(WS, WT).
The bilanguage representation could result in
some source language phrases to be mapped to 2
</bodyText>
<figure confidence="0.999309904761905">
Source
Sentence/
Weighted
Lattice
Lexical Choice
FST Composition
Translation
Decoding
Model
Permutation
Construction
Target
Model
Language
Decoding Lexical Reodering
Permutation
Lattice
FSA
Composition
Sentence
Target
</figure>
<page confidence="0.998401">
153
</page>
<bodyText confidence="0.999903625">
(empty target phrase). In addition to these phrases,
we compute subsequences of a given length k on the
bilanguage string and for each subsequence we re-
order the target words of the subsequence to be in
the same order as they are in the target language sen-
tence corresponding to that bilanguage string. This
results in a retokenization of the bilanguage into to-
kens of source-target phrase pairs.
</bodyText>
<subsectionHeader confidence="0.863847">
2.4 SFST Model
</subsectionHeader>
<bodyText confidence="0.999961555555556">
From the bilanguage corpus B, we train an n-gram
language model using standard tools (Goffin et al.,
2005). The resulting language model is represented
as a weighted finite-state automaton (S x T —*
[0,1]). The symbols on the arcs of this automaton
(si ti) are interpreted as having the source and target
symbols (si:ti), making it into a weighted finite-state
transducer (S —* T x[0, 1]) that provides a weighted
string-to-string transduction from S into T :
</bodyText>
<equation confidence="0.9958335">
T* = argmax P(si, ti|si−1, ti−1 ... si−n−1, ti−n−1)
T
</equation>
<subsectionHeader confidence="0.974152">
2.5 Decoding
</subsectionHeader>
<bodyText confidence="0.999979285714286">
Since we represent the translation model as a
weighted finite-state transducer (TransFST), the
decoding process of translating a new source in-
put (sentence or weighted lattice (I3)) amounts to
a transducer composition (o) and selection of the
best probability path (BestPath) resulting from the
composition and projecting the target sequence (7r1).
</bodyText>
<equation confidence="0.996434">
T* = 71(BestPath(I3 oTransFST)) (4)
</equation>
<bodyText confidence="0.999946444444444">
However, we have noticed that on the develop-
ment corpus, the decoded target sentence is typically
shorter than the intended target sentence. This mis-
match may be due to the incorrect estimation of the
back-off events and their probabilities in the train-
ing phase of the transducer. In order to alleviate
this mismatch, we introduce a negative word inser-
tion penalty model as a mechanism to produce more
words in the target sentence.
</bodyText>
<subsectionHeader confidence="0.973065">
2.6 Word Insertion Model
</subsectionHeader>
<bodyText confidence="0.999964">
The word insertion model is also encoded as a
weighted finite-state automaton and is included in
the decoding sequence as shown in Equation 5. The
word insertion FST has one state and  |ET  |number
of arcs each weighted with a A weight representing
the word insertion cost. On composition as shown
in Equation 5, the word insertion model penalizes or
rewards paths which have more words depending on
whether A is positive or negative value.
</bodyText>
<equation confidence="0.997901">
T* = 7r1(BestPath(I3oTransFSToWIP)) (5)
</equation>
<bodyText confidence="0.638917">
Figure 5: Locally constraint permutation automaton
for a sentence with 4 words and window size of 2.
</bodyText>
<subsectionHeader confidence="0.996672">
2.7 Global Reordering
</subsectionHeader>
<bodyText confidence="0.999983285714286">
Local reordering as described in Section 2.3 is re-
stricted by the window size k and accounts only for
different word order within phrases. As permuting
non-linear automata is too complex, we apply global
reordering by permuting the words of the best trans-
lation and weighting the result by an n-gram lan-
guage model (see also Figure 2):
</bodyText>
<equation confidence="0.99351">
T* = BestPath(perm(T�) o LMt) (6)
</equation>
<bodyText confidence="0.999971117647059">
Even the size of the minimal permutation automa-
ton of a linear automaton grows exponentially with
the length of the input sequence. While decoding by
composition simply resembles the principle of mem-
oization (i.e. here: all state hypotheses of a whole
sentence are kept in memory), it is necessary to ei-
ther use heuristic forward pruning or constrain per-
mutations to be within a local window of adjustable
size (also see (Kanthak et al., 2005)). We have cho-
sen to constrain permutations here. Figure 5 shows
the resulting minimal permutation automaton for an
input sequence of 4 words and a window size of 2.
Decoding ASR output in combination with global
reordering uses n-best lists or extracts them from lat-
tices first. Each entry of the n-best list is decoded
separately and the best target sentence is picked
from the union of the n intermediate results.
</bodyText>
<sectionHeader confidence="0.9325095" genericHeader="method">
3 Discriminant Models for Lexical
Selection
</sectionHeader>
<bodyText confidence="0.999927933333333">
The approach from the previous section is a genera-
tive model for statistical machine translation relying
on local associations between source and target sen-
tences. Now, we present our approach for a global
lexical selection model based on discriminatively
trained classification techniques. Discriminant mod-
eling techniques have become the dominant method
for resolving ambiguity in speech and other NLP
tasks, outperforming generative models. Discrimi-
native training has been used mainly for translation
model combination (Och and Ney, 2002) and with
the exception of (Wellington et al., 2006; Tillmann
and Zhang, 2006), has not been used to directly train
parameters of a translation model. We expect dis-
criminatively trained global lexical selection models
</bodyText>
<figure confidence="0.993280833333333">
0000
2
1
0100
1000
2
3
1
1010
1100
2
4
3
1110
1101
4
3
1111
</figure>
<page confidence="0.999578">
154
</page>
<bodyText confidence="0.999948761904762">
to outperform generatively trained local lexical se-
lection models as well as provide a framework for
incorporating rich morpho-syntactic information.
Statistical machine translation can be formulated
as a search for the best target sequence that maxi-
mizes P(T|S), where S is the source sentence and
T is the target sentence. Ideally, P(T|S) should
be estimated directly to maximize the conditional
likelihood on the training data (discriminant model).
However, T corresponds to a sequence with a ex-
ponentially large combination of possible labels,
and traditional classification approaches cannot be
used directly. Although Conditional Random Fields
(CRF) (Lafferty et al., 2001) train an exponential
model at the sequence level, in translation tasks such
as ours the computational requirements of training
such models are prohibitively expensive.
We investigate two approaches to approximating
the string level global classification problem, using
different independence assumptions. A comparison
of the two approaches is summarized in Table 1.
</bodyText>
<subsectionHeader confidence="0.999152">
3.1 Sequential Lexical Choice Model
</subsectionHeader>
<bodyText confidence="0.99999475">
In the first approach, we formulate a sequential lo-
cal classification problem as shown in Equations 7.
This approach is similar to the SFST approach in
that it relies on local associations between the source
and target words(phrases). We can use a conditional
model (instead of a joint model as before) and the
parameters are determined using discriminant train-
ing which allows for richer conditioning context.
</bodyText>
<equation confidence="0.9929965">
f1N
P(T |S) = i�1 P(ti|Φ(S, i)) (7)
</equation>
<bodyText confidence="0.999774666666667">
where Φ(S, i) is a set of features extracted from the
source string S (shortened as Φ in the rest of the
section).
</bodyText>
<subsectionHeader confidence="0.999688">
3.2 Bag-of-Words Lexical Choice Model
</subsectionHeader>
<bodyText confidence="0.99999141025641">
The sequential lexical choice model described in
the previous section treats the selection of a lexical
choice for a source word in the local lexical context
as a classification task. The data for training such
models is derived from word alignments obtained
by e.g. GIZA++. The decoded target lexical items
have to be further reordered, but for closely related
languages the reordering could be incorporated into
correctly ordered target phrases as discussed previ-
ously.
For pairs of languages with radically different
word order (e.g. English-Japanese), there needs to
be a global reordering of words similar to the case
in the SFST-based translation system. Also, for such
differing language pairs, the alignment algorithms
such as GIZA++ perform poorly.
These observations prompted us to formulate the
lexical choice problem without the need for word
alignment information. We require a sentence
aligned corpus as before, but we treat the target sen-
tence as a bag-of-words or BOW assigned to the
source sentence. The goal is, given a source sen-
tence, to estimate the probability that we find a given
word in the target sentence. This is why, instead of
producing a target sentence, what we initially obtain
is a target bag of words. Each word in the target vo-
cabulary is detected independently, so we have here
a very simple use of binary static classifiers. Train-
ing sentence pairs are considered as positive exam-
ples when the word appears in the target, and neg-
ative otherwise. Thus, the number of training ex-
amples equals the number of sentence pairs, in con-
trast to the sequential lexical choice model which
has one training example for each token in the bilin-
gual training corpus. The classifier is trained with n-
gram features (BOgrams(S)) from the source sen-
tence. During decoding the words with conditional
probability greater than a threshold 0 are considered
as the result of lexical choice decoding.
</bodyText>
<equation confidence="0.997829">
BOWT� = {t|P(t|BOgrams(S)) &gt; 01 (8)
</equation>
<bodyText confidence="0.999983833333333">
For reconstructing the proper order of words in
the target sentence we consider all permutations of
words in BOWT� and weight them by a target lan-
guage model. This step is similar to the one de-
scribed in Section 2.7. The BOW approach can also
be modified to allow for length adjustments of tar-
get sentences, if we add optional deletions in the fi-
nal step of permutation decoding. The parameter 0
and an additional word deletion penalty can then be
used to adjust the length of translated outputs. In
Section 6, we discuss several issues regarding this
model.
</bodyText>
<sectionHeader confidence="0.963923" genericHeader="method">
4 Choosing the classifier
</sectionHeader>
<bodyText confidence="0.9968444">
This section addresses the choice of the classifi-
cation technique, and argues that one technique
that yields excellent performance while scaling well
is binary maximum entropy (Maxent) with L1-
regularization.
</bodyText>
<subsectionHeader confidence="0.995151">
4.1 Multiclass vs. Binary Classification
</subsectionHeader>
<bodyText confidence="0.999746">
The Sequential and BOW models represent two dif-
ferent classification problems. In the sequential
model, we have a multiclass problem where each
class ti is exclusive, therefore, all the classifier out-
puts P(ti|Φ) must be jointly optimized such that
</bodyText>
<page confidence="0.999647">
155
</page>
<tableCaption confidence="0.999429">
Table 1: A comparison of the sequential and bag-of-words lexical choice models
</tableCaption>
<table confidence="0.998569">
Sequential Lexical Model Bag-of-Words Lexical Model
Output target Target word for each source position i Target word given a source sentence
Input features BOgram(S, i − d, i + d) : bag of n-grams BOgram(S, 0, |S|): bag of n-grams
in source sentence in the interval [i − d, i + d] in source sentence
Probabilities P(ti|BOgram(S, i − d, i + d)) P(BOW(T)|BOgram(S, 0, |S|))
Independence assumption between the labels
Number of classes One per target word or phrase
Training samples One per source token One per sentence
Preprocessing Source/Target word alignment Source/Target sentence alignment
</table>
<bodyText confidence="0.993299611111111">
Ei P (ti|Φ) = 1. This can be problematic: with
one classifier per word in the vocabulary, even allo-
cating the memory during training may exceed the
memory capacity of current computers.
In the BOW model, each class can be detected
independently, and two different classes can be de-
tected at the same time. This is known as the 1-vs-
other scheme. The key advantage over the multiclass
scheme is that not all classifiers have to reside in
memory at the same time during training which al-
lows for parallelization. Fortunately for the sequen-
tial model, we can decompose a multiclass classifi-
cation problem into separate 1-vs-other problems. In
theory, one has to make an additional independence
assumption and the problem statement becomes dif-
ferent. Each output label t is projected into a bit
string with components bj(t) where probability of
each component is estimated independently:
</bodyText>
<equation confidence="0.990226">
1 + e−(a;−a,)·4D
</equation>
<bodyText confidence="0.9999899">
In practice, despite the approximation, the 1-vs-
other scheme has been shown to perform as well as
the multiclass scheme (Rifkin and Klautau, 2004).
As a consequence, we use the same type of binary
classifier for the sequential and the BOW models.
The excellent results recently obtained with the
SEARN algorithm (Daume et al., 2007) also sug-
gest that binary classifiers, when properly trained
and combined, seem to be capable of matching more
complex structured output approaches.
</bodyText>
<subsectionHeader confidence="0.987801">
4.2 Geometric vs. Probabilistic Interpretation
</subsectionHeader>
<bodyText confidence="0.9998935">
We separate the most popular classification tech-
niques into two broad categories:
</bodyText>
<listItem confidence="0.998842142857143">
• Geometric approaches maximize the width of
a separation margin between the classes. The
most popular method is the Support Vector Ma-
chine (SVM) (Vapnik, 1998).
• Probabilistic approaches maximize the con-
ditional likelihood of the output class given
the input features. This logistic regression is
</listItem>
<bodyText confidence="0.9983166">
also called Maxent as it finds the distribution
with maximum entropy that properly estimates
the average of each feature over the training
data (Berger et al., 1996).
In previous studies, we found that the best accuracy
is achieved with non-linear (or kernel) SVMs, at the
expense of a high test time complexity, which is un-
acceptable for machine translation. Linear SVMs
and regularized Maxent yield similar performance.
In theory, Maxent training, which scales linearly
with the number of examples, is faster than SVM
training, which scales quadratically with the num-
ber of examples. In our first experiments with lexi-
cal choice models, we observed that Maxent slightly
outperformed SVMs. Using a single threshold with
SVMs, some classes of words were over-detected.
This suggests that, as theory predicts, SVMs do not
properly approximate the posterior probability. We
therefore chose to use Maxent as the best probability
approximator.
</bodyText>
<subsectionHeader confidence="0.780813">
4.3 L1 vs. L2 regularization
</subsectionHeader>
<bodyText confidence="0.999952214285715">
Traditionally, Maxent is regularized by imposing a
Gaussian prior on each weight: this L2 regulariza-
tion finds the solution with the smallest possible
weights. However, on tasks like machine translation
with a very large number of input features, a Lapla-
cian L1 regularization that also attempts to maxi-
mize the number of zero weights is highly desirable.
A new L1-regularized Maxent algorithms was
proposed for density estimation (Dudik et al., 2004)
and we adapted it to classification. We found this al-
gorithm to converge faster than the current state-of-
the-art in Maxent training, which is L2-regularized
L-BFGS (Malouf, 2002)1. Moreover, the number of
trained parameters is considerably smaller.
</bodyText>
<sectionHeader confidence="0.99208" genericHeader="method">
5 Data and Experiments
</sectionHeader>
<bodyText confidence="0.998847">
We have performed experiments on the IWSLT06
Chinese-English training and development sets from
</bodyText>
<footnote confidence="0.9987955">
1We used the implementation available at
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
</footnote>
<equation confidence="0.9548605">
1
P (bj(t)|Φ) = 1 − P (¯bj(t)|Φ) =
</equation>
<page confidence="0.998573">
156
</page>
<tableCaption confidence="0.998379">
Table 2: Statistics of training and development data from 2005/2006 (* = first of multiple translations only).
</tableCaption>
<table confidence="0.999314153846154">
Training (2005) Dev 2005 Dev 2006
Chinese English Chinese English Chinese English
Sentences 46,311 506 489
Running Words
Vocabulary
Singletons
351,060 376,615 3,826 3,897 5,214 6,362∗
11,178 11,232 931 898 1,136 1,134∗
4,348 4,866 600 538 619 574∗
OOVs [%] - - 0.6 0.3 0.9 1.0
ASR WER [%] - - - - 25.2 -
Perplexity - - 33 - 86 -
# References - - 16 7
</table>
<bodyText confidence="0.999662153846154">
2005 and 2006. The data are traveler task ex-
pressions such as seeking directions, expressions in
restaurants and travel reservations. Table 2 presents
some statistics on the data sets. It must be noted
that while the 2005 development set matches the
training data closely, the 2006 development set has
been collected separately and shows slightly differ-
ent statistics for average sentence length, vocabulary
size and out-of-vocabulary words. Also the 2006
development set contains no punctuation marks in
Chinese, but the corresponding English translations
have punctuation marks. We also evaluated our
models on the Chinese speech recognition output
and we report results using 1-best with a word er-
ror rate of 25.2%.
For the experiments, we tokenized the Chinese
sentences into character strings and trained the mod-
els discussed in the previous sections. Also, we
trained a punctuation prediction model using Max-
ent framework on the Chinese character strings in
order to insert punctuation marks into the 2006 de-
velopment data set. The resulting character string
with punctuation marks is used as input to the trans-
lation decoder. For the 2005 development set, punc-
tuation insertion was not needed since the Chinese
sentences already had the true punctuation marks.
In Table 3 we present the results of the three dif-
ferent translation models – FST, Sequential Maxent
and BOW Maxent. There are a few interesting ob-
servations that can be made based on these results.
First, on the 2005 development set, the sequential
Maxent model outperforms the FST model, even
though the two models were trained starting from
the same GIZA++ alignment. The difference, how-
ever, is due to the fact that Maxent models can cope
with increased lexical context2 and the parameters
of the model are discriminatively trained. The more
surprising result is that the BOW Maxent model sig-
nificantly outperforms the sequential Maxent model.
</bodyText>
<footnote confidence="0.968148666666667">
2We use 6 words to the left and right of a source word for
sequential Maxent, but only 2 preceding source and target words
for FST approach.
</footnote>
<bodyText confidence="0.973457285714286">
The reason is that the sequential Maxent model re-
lies on the word alignment, which, if erroneous, re-
sults in incorrect predictions by the sequential Max-
ent model. The BOW model does not rely on the
word-level alignment and can be interpreted as a dis-
criminatively trained model of dictionary lookup for
a target word in the context of a source sentence.
</bodyText>
<tableCaption confidence="0.77593875">
Table 3: Results (mBLEU) scores for the three dif-
ferent models on the transcriptions for development
set 2005 and 2006 and ASR 1-best for development
set 2006.
</tableCaption>
<table confidence="0.9937418">
Dev 2005 Dev 2006
Text Text ASR 1-best
FST 51.8 19.5 16.5
Seq. Maxent 53.5 19.4 16.3
BOW Maxent 59.9 19.3 16.6
</table>
<bodyText confidence="0.99985284">
As indicated in the data release document, the
2006 development set was collected differently com-
pared to the one from 2005. Due to this mis-
match, the performance of the Maxent models are
not very different from the FST model, indicating
the lack of good generalization across different gen-
res. However, we believe that the Maxent frame-
work allows for incorporation of linguistic features
that could potentially help in generalization across
genres. For translation of ASR 1-best, we see a sys-
tematic degradation of about 3% in mBLEU score
compared to translating the transcription.
In order to compensate for the mismatch between
the 2005 and 2006 data sets, we computed a 10-fold
average mBLEU score by including 90% of the 2006
development set into the training set and using 10%
of the 2006 development set for testing, each time.
The average mBLEU score across these 10 runs in-
creased to 22.8.
In Figure 6 we show the improvement of mBLEU
scores with the increase in permutation window size.
We had to limit to a permutation window size of 10
due to memory limitations, even though the curve
has not plateaued. We anticipate using pruning tech-
niques we can increase the window size further.
</bodyText>
<page confidence="0.97694">
157
</page>
<figure confidence="0.9959447">
0.6
0.58
0.56
0.54
0.52
0.5
0.48
0.46
6 6.5 7 7.5 8 8.5 9 9.5 10
Permutation Window Size
</figure>
<figureCaption confidence="0.9932885">
Figure 6: Improvement in mBLEU score with the
increase in size of the permutation window
</figureCaption>
<subsectionHeader confidence="0.995417">
5.1 United Nations and Hansard Corpora
</subsectionHeader>
<bodyText confidence="0.933077291666667">
In order to test the scalability of the global lexical
selection approach, we also performed lexical se-
lection experiments on the United Nations (Arabic-
English) corpus and the Hansard (French-English)
corpus using the SFST model and the BOW Maxent
model. We used 1,000,000 training sentence pairs
and tested on 994 test sentences for the UN corpus.
For the Hansard corpus we used the same training
and test split as in (Zens and Ney, 2004): 1.4 million
training sentence pairs and 5432 test sentences. The
vocabulary sizes for the two corpora are mentioned
in Table 4. Also in Table 4, are the results in terms of
F-measure between the words in the reference sen-
tence and the decoded sentences. We can see that the
BOW model outperforms the SFST model on both
corpora significantly. This is due to a systematic
10% relative improvement for open class words, as
they benefit from a much wider context. BOW per-
formance on close class words is higher for the UN
corpus but lower for the Hansard corpus.
Table 4: Lexical Selection results (F-measure) on
the Arabic-English UN Corpus and the French-
English Hansard Corpus. In parenthesis are F-
measures for open and closed class lexical items.
</bodyText>
<table confidence="0.9972595">
Corpus Vocabulary SFST BOW
Source Target
UN 252,571 53,005 64.6 69.5
(60.5/69.1) (66.2/72.6)
Hansard 100,270 78,333 57.4 60.8
(50.6/67.7) (56.5/63.4)
</table>
<sectionHeader confidence="0.999825" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999996474576271">
The BOW approach is promising as it performs rea-
sonably well despite considerable losses in the trans-
fer of information between source and target lan-
guage. The first and most obvious loss is about word
position. The only information we currently use to
restore the target word position is the target language
model. Information about the grammatical role of a
word in the source sentence is completely lost. The
language model might fortuitously recover this in-
formation if the sentence with the correct grammat-
ical role for the word happens to be the maximum
likelihood sentence in the permutation automaton.
We are currently working toward incorporating
syntactic information on the target words so as to be
able to recover some of the grammatical role infor-
mation lost in the classification process. In prelimi-
nary experiments, we have associated the target lex-
ical items with supertag information (Bangalore and
Joshi, 1999). Supertags are labels that provide linear
ordering constraints as well as grammatical relation
information. Although associating supertags to tar-
get words increases the class set for the classifier, we
have noticed that the degradation in the F-score is
on the order of 3% across different corpora. The su-
pertag information can then be exploited in the sen-
tence construction process. The use of supertags in
phrase-based SMT system has been shown to im-
prove results (Hassan et al., 2006).
A less obvious loss is the number of times a word
or concept appears in the target sentence. Func-
tion words like ”the” and ”of” can appear many
times in an English sentence. In the model dis-
cussed in this paper, we index each occurrence of the
function word with a counter. In order to improve
this method, we are currently exploring a technique
where the function words serve as attributes (e.g.
definiteness, tense, case) on the contentful lexical
items, thus enriching the lexical item with morpho-
syntactic information.
A third issue concerning the BOW model is the
problem of synonyms – target words which translate
the same source word. Suppose that in the training
data, target words t1 and t2 are, with equal probabil-
ity, translations of the same source word. Then, in
the presence of this source word, the probability to
detect the corresponding target word, which we as-
sume is 0.8, will be, because of discriminant learn-
ing, split equally between t1 and t2, that is 0.4 and
0.4. Because of this synonym problem, the BOW
threshold 0 has to be set lower than 0.5, which is
observed experimentally. However, if we set the
threshold to 0.3, both t1 and t2 will be detected in
the target sentence, and we found this to be a major
source of undesirable insertions.
The BOW approach is different from the pars-
ing based approaches (Melamed, 2004; Zhang and
Gildea, 2005; Cowan et al., 2006) where the transla-
tion model tightly couples the syntactic and lexical
items of the two languages. The decoupling of the
</bodyText>
<page confidence="0.994994">
158
</page>
<bodyText confidence="0.999852666666667">
two steps in our model has the potential for gener-
ating paraphrased sentences not necessarily isomor-
phic to the structure of the source sentence.
</bodyText>
<sectionHeader confidence="0.99949" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999957">
We view machine translation as consisting of lexi-
cal selection and lexical reordering steps. These two
steps need not necessarily be sequential and could be
tightly integrated. We have presented the weighted
finite-state transducer model of machine translation
where lexical choice and a limited amount of lexical
reordering are tightly integrated into a single trans-
duction. We have also presented a novel approach
to translation where these two steps are loosely cou-
pled and the parameters of the lexical choice model
are discriminatively trained using a maximum en-
tropy model. The lexical reordering model in this
approach is achieved using a permutation automa-
ton. We have evaluated these two approaches on the
2005 and 2006 IWSLT development sets and shown
that the techniques scale well to Hansard and UN
corpora.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999624345679012">
H. Alshawi, S. Bangalore, and S. Douglas. 1998. Automatic
acquisition of hierarchical transduction models for machine
translation. In ACL, Montreal, Canada.
R.E. Banchs, J.M. Crego, A. Gispert, P. Lambert, and J.B.
Marino. 2005. Statistical machine translation of euparl data
by using bilingual n-grams. In Workshop on Building and
Using Parallel Texts. ACL.
S. Bangalore and A. K. Joshi. 1999. Supertagging: An ap-
proach to almost parsing. Computational Linguistics, 25(2).
S. Bangalore and G. Riccardi. 2000. Stochastic finite-state
models for spoken language machine translation. In Pro-
ceedings of the Workshop on Embedded Machine Transla-
tion Systems, pages 52–59.
A.L. Berger, Stephen A. D. Pietra, D. Pietra, and J. Vincent.
1996. A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, 22(1):39–71.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The
Mathematics of Machine Translation: Parameter Estimation.
Computational Linguistics, 16(2):263–312.
D. Chiang. 2005. A hierarchical phrase-based model for statis-
tical machine translation. In Proceedings of the ACL Con-
ference, Ann Arbor, MI.
B. Cowan, I. Kucerova, and M. Collins. 2006. A discrimi-
native model for tree-to-tree translation. In Proceedings of
EMNLP.
H. Daume, J. Langford, and D. Marcu. 2007. Search-based
structure prediction. submitted to Machine Learning Jour-
nal.
M. Dudik, S. Phillips, and R.E. Schapire. 2004. Perfor-
mance Guarantees for Regularized Maximum Entropy Den-
sity Estimation. In Proceedings of COLT’04, Banff, Canada.
Springer Verlag.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur, A. Ljolje,
S. Parthasarathy, M. Rahim, G. Riccardi, and M. Saraclar.
2005. The AT&amp;T WATSON Speech Recognizer. In Pro-
ceedings ofICASSP, Philadelphia, PA.
H. Hassan, M. Hearne, K. Sima’an, and A. Way. 2006. Syntac-
tic phrase-based statistical machine translation. In Proceed-
ings of IEEE/ACL first International Workshop on Spoken
Language Technology (SLT), Aruba, December.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005.
Novel reordering approaches in phrase-based statistical ma-
chine translation. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 167–174, Ann Ar-
bor, Michigan.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of ICML, San Fran-
cisco, CA.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proceedings of CoNLL-
2002, pages 49–55. Taipei, Taiwan.
I. D. Melamed. 2004. Statistical machine translation by pars-
ing. In Proceedings ofACL.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proceedings ofACL.
F.J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19–51.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of one-
vs-all classification. Journal ofMachine Learning Research,
pages 101–141.
C. Tillmann and T. Zhang. 2006. A discriminative global train-
ing algorithm for statistical mt. In COLING-ACL.
V.N. Vapnik. 1998. Statistical Learning Theory. John Wiley &amp;
Sons.
B. Wellington, J. Turian, C. Pike, and D. Melamed. 2006. Scal-
able purely-discriminative training for word and tree trans-
ducers. In AMTA.
D. Wu. 1997. Stochastic Inversion Transduction Grammars
and Bilingual Parsing of Parallel Corpora. Computational
Linguistics, 23(3):377–404.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proceedings of 39th ACL.
R. Zens and H. Ney. 2004. Improvements in phrase-based sta-
tistical machine translation. In Proceedings ofHLT-NAACL,
pages 257–264, Boston, MA.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized inver-
sion transduction grammar for alignment. In Proceedings of
ACL.
</reference>
<page confidence="0.998833">
159
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937594">
<title confidence="0.996275">Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction</title>
<author confidence="0.999895">Srinivas Bangalore</author>
<author confidence="0.999895">Patrick Haffner</author>
<author confidence="0.999895">Stephan Kanthak</author>
<affiliation confidence="0.999663">AT&amp;T Labs - Research</affiliation>
<address confidence="0.999213">180 Park Ave, Florham Park, NJ 07932</address>
<abstract confidence="0.997411714285714">Machine translation of a source language sentence involves selecting appropriate target language words and ordering the selected words to form a well-formed target language sentence. Most of the previous work on statistical machine translarelies on associations of target words/phrases with source words/phrases for lexical selection. In contrast, in this paper, we present a novel approach to lexical selection where the target words are associwith the entire source sentence without the need to compute local associations. Further, we present a technique for reconstructing the target language sentence from the selected words. We compare the results of this approach against those obtained from a finite-state based statistical machine translation system which relies on local lexical associations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Automatic acquisition of hierarchical transduction models for machine translation. In ACL,</title>
<date>1998</date>
<location>Montreal, Canada.</location>
<contexts>
<context position="2469" citStr="Alshawi et al., 1998" startWordPosition="358" endWordPosition="361">elevel alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult to exactly associate </context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 1998</marker>
<rawString>H. Alshawi, S. Bangalore, and S. Douglas. 1998. Automatic acquisition of hierarchical transduction models for machine translation. In ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Banchs</author>
<author>J M Crego</author>
<author>A Gispert</author>
<author>P Lambert</author>
<author>J B Marino</author>
</authors>
<title>Statistical machine translation of euparl data by using bilingual n-grams.</title>
<date>2005</date>
<booktitle>In Workshop on Building and Using Parallel Texts. ACL.</booktitle>
<contexts>
<context position="5212" citStr="Banchs et al., 2005" startWordPosition="792" endWordPosition="795">paper, we present the details of training a global lexical selection model using classification techniques and sentence reconstruction models using permutation automata. We also present a stochastic finite-state transducer (SFST) as an example of an approach that relies on local associations and use it to compare and contrast our approach. 2 SFST Training and Decoding In this section, we describe each of the components of our SFST system shown in Figure 1. The SFST approach described here is similar to the one described in (Bangalore and Riccardi, 2000) which has subsequently been adopted by (Banchs et al., 2005). 2.1 Word Alignment The first stage in the process of training a lexical selection model is obtaining an alignment function (f) that given a pair of source (s1s2 ... sn) and target (t1t2 ... tm) language sentences, maps source language word subsequences into target language word subsequences, as shown below. ∀i∃j(f(si) = tj ∨ f(si) = 2) (1) For the work reported in this paper, we have used the GIZA++ tool (Och and Ney, 2003) which implements a string-alignment algorithm. GIZA++ alignment however is asymmetric in that the word mappings are different depending on the direction of alignment – so</context>
</contexts>
<marker>Banchs, Crego, Gispert, Lambert, Marino, 2005</marker>
<rawString>R.E. Banchs, J.M. Crego, A. Gispert, P. Lambert, and J.B. Marino. 2005. Statistical machine translation of euparl data by using bilingual n-grams. In Workshop on Building and Using Parallel Texts. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="28485" citStr="Bangalore and Joshi, 1999" startWordPosition="4580" endWordPosition="4583">nguage model. Information about the grammatical role of a word in the source sentence is completely lost. The language model might fortuitously recover this information if the sentence with the correct grammatical role for the word happens to be the maximum likelihood sentence in the permutation automaton. We are currently working toward incorporating syntactic information on the target words so as to be able to recover some of the grammatical role information lost in the classification process. In preliminary experiments, we have associated the target lexical items with supertag information (Bangalore and Joshi, 1999). Supertags are labels that provide linear ordering constraints as well as grammatical relation information. Although associating supertags to target words increases the class set for the classifier, we have noticed that the degradation in the F-score is on the order of 3% across different corpora. The supertag information can then be exploited in the sentence construction process. The use of supertags in phrase-based SMT system has been shown to improve results (Hassan et al., 2006). A less obvious loss is the number of times a word or concept appears in the target sentence. Function words li</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>S. Bangalore and A. K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Riccardi</author>
</authors>
<title>Stochastic finite-state models for spoken language machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Embedded Machine Translation Systems,</booktitle>
<pages>52--59</pages>
<contexts>
<context position="5151" citStr="Bangalore and Riccardi, 2000" startWordPosition="782" endWordPosition="785">h significantly different word order (e.g. English-Japanese). In this paper, we present the details of training a global lexical selection model using classification techniques and sentence reconstruction models using permutation automata. We also present a stochastic finite-state transducer (SFST) as an example of an approach that relies on local associations and use it to compare and contrast our approach. 2 SFST Training and Decoding In this section, we describe each of the components of our SFST system shown in Figure 1. The SFST approach described here is similar to the one described in (Bangalore and Riccardi, 2000) which has subsequently been adopted by (Banchs et al., 2005). 2.1 Word Alignment The first stage in the process of training a lexical selection model is obtaining an alignment function (f) that given a pair of source (s1s2 ... sn) and target (t1t2 ... tm) language sentences, maps source language word subsequences into target language word subsequences, as shown below. ∀i∃j(f(si) = tj ∨ f(si) = 2) (1) For the work reported in this paper, we have used the GIZA++ tool (Och and Ney, 2003) which implements a string-alignment algorithm. GIZA++ alignment however is asymmetric in that the word mappin</context>
</contexts>
<marker>Bangalore, Riccardi, 2000</marker>
<rawString>S. Bangalore and G. Riccardi. 2000. Stochastic finite-state models for spoken language machine translation. In Proceedings of the Workshop on Embedded Machine Translation Systems, pages 52–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>Stephen A D Pietra</author>
<author>D Pietra</author>
<author>J Vincent</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="19883" citStr="Berger et al., 1996" startWordPosition="3166" endWordPosition="3169">uctured output approaches. 4.2 Geometric vs. Probabilistic Interpretation We separate the most popular classification techniques into two broad categories: • Geometric approaches maximize the width of a separation margin between the classes. The most popular method is the Support Vector Machine (SVM) (Vapnik, 1998). • Probabilistic approaches maximize the conditional likelihood of the output class given the input features. This logistic regression is also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data (Berger et al., 1996). In previous studies, we found that the best accuracy is achieved with non-linear (or kernel) SVMs, at the expense of a high test time complexity, which is unacceptable for machine translation. Linear SVMs and regularized Maxent yield similar performance. In theory, Maxent training, which scales linearly with the number of examples, is faster than SVM training, which scales quadratically with the number of examples. In our first experiments with lexical choice models, we observed that Maxent slightly outperformed SVMs. Using a single threshold with SVMs, some classes of words were over-detect</context>
</contexts>
<marker>Berger, Pietra, Pietra, Vincent, 1996</marker>
<rawString>A.L. Berger, Stephen A. D. Pietra, D. Pietra, and J. Vincent. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>R Mercer</author>
</authors>
<title>The Mathematics of Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1525" citStr="Brown et al., 1993" startWordPosition="221" endWordPosition="224">ords. We compare the results of this approach against those obtained from a finite-state based statistical machine translation system which relies on local lexical associations. 1 Introduction Machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. Most of the previous work on statistical machine translation, as exemplified in (Brown et al., 1993), employs word-alignment algorithm (such as GIZA++ (Och and Ney, 2003)) that provides local associations between source and target words. The source-to-target word alignments are sometimes augmented with target-to-source word alignments in order to improve precision. Further, the word-level alignments are extended to phraselevel alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use ph</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics, 16(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Conference,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2509" citStr="Chiang, 2005" startWordPosition="366" endWordPosition="367">nt of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult to exactly associate a target word to a source word in many s</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the ACL Conference, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Cowan</author>
<author>I Kucerova</author>
<author>M Collins</author>
</authors>
<title>A discriminative model for tree-to-tree translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="30366" citStr="Cowan et al., 2006" startWordPosition="4903" endWordPosition="4906"> in the presence of this source word, the probability to detect the corresponding target word, which we assume is 0.8, will be, because of discriminant learning, split equally between t1 and t2, that is 0.4 and 0.4. Because of this synonym problem, the BOW threshold 0 has to be set lower than 0.5, which is observed experimentally. However, if we set the threshold to 0.3, both t1 and t2 will be detected in the target sentence, and we found this to be a major source of undesirable insertions. The BOW approach is different from the parsing based approaches (Melamed, 2004; Zhang and Gildea, 2005; Cowan et al., 2006) where the translation model tightly couples the syntactic and lexical items of the two languages. The decoupling of the 158 two steps in our model has the potential for generating paraphrased sentences not necessarily isomorphic to the structure of the source sentence. 7 Conclusions We view machine translation as consisting of lexical selection and lexical reordering steps. These two steps need not necessarily be sequential and could be tightly integrated. We have presented the weighted finite-state transducer model of machine translation where lexical choice and a limited amount of lexical r</context>
</contexts>
<marker>Cowan, Kucerova, Collins, 2006</marker>
<rawString>B. Cowan, I. Kucerova, and M. Collins. 2006. A discriminative model for tree-to-tree translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
<author>J Langford</author>
<author>D Marcu</author>
</authors>
<title>Search-based structure prediction. submitted to Machine Learning Journal.</title>
<date>2007</date>
<contexts>
<context position="19141" citStr="Daume et al., 2007" startWordPosition="3054" endWordPosition="3057">e 1-vs-other problems. In theory, one has to make an additional independence assumption and the problem statement becomes different. Each output label t is projected into a bit string with components bj(t) where probability of each component is estimated independently: 1 + e−(a;−a,)·4D In practice, despite the approximation, the 1-vsother scheme has been shown to perform as well as the multiclass scheme (Rifkin and Klautau, 2004). As a consequence, we use the same type of binary classifier for the sequential and the BOW models. The excellent results recently obtained with the SEARN algorithm (Daume et al., 2007) also suggest that binary classifiers, when properly trained and combined, seem to be capable of matching more complex structured output approaches. 4.2 Geometric vs. Probabilistic Interpretation We separate the most popular classification techniques into two broad categories: • Geometric approaches maximize the width of a separation margin between the classes. The most popular method is the Support Vector Machine (SVM) (Vapnik, 1998). • Probabilistic approaches maximize the conditional likelihood of the output class given the input features. This logistic regression is also called Maxent as i</context>
</contexts>
<marker>Daume, Langford, Marcu, 2007</marker>
<rawString>H. Daume, J. Langford, and D. Marcu. 2007. Search-based structure prediction. submitted to Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dudik</author>
<author>S Phillips</author>
<author>R E Schapire</author>
</authors>
<title>Performance Guarantees for Regularized Maximum Entropy Density Estimation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLT’04,</booktitle>
<publisher>Springer Verlag.</publisher>
<location>Banff, Canada.</location>
<contexts>
<context position="21138" citStr="Dudik et al., 2004" startWordPosition="3361" endWordPosition="3364">edicts, SVMs do not properly approximate the posterior probability. We therefore chose to use Maxent as the best probability approximator. 4.3 L1 vs. L2 regularization Traditionally, Maxent is regularized by imposing a Gaussian prior on each weight: this L2 regularization finds the solution with the smallest possible weights. However, on tasks like machine translation with a very large number of input features, a Laplacian L1 regularization that also attempts to maximize the number of zero weights is highly desirable. A new L1-regularized Maxent algorithms was proposed for density estimation (Dudik et al., 2004) and we adapted it to classification. We found this algorithm to converge faster than the current state-ofthe-art in Maxent training, which is L2-regularized L-BFGS (Malouf, 2002)1. Moreover, the number of trained parameters is considerably smaller. 5 Data and Experiments We have performed experiments on the IWSLT06 Chinese-English training and development sets from 1We used the implementation available at http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html 1 P (bj(t)|Φ) = 1 − P (¯bj(t)|Φ) = 156 Table 2: Statistics of training and development data from 2005/2006 (* = first of multiple t</context>
</contexts>
<marker>Dudik, Phillips, Schapire, 2004</marker>
<rawString>M. Dudik, S. Phillips, and R.E. Schapire. 2004. Performance Guarantees for Regularized Maximum Entropy Density Estimation. In Proceedings of COLT’04, Banff, Canada. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goffin</author>
<author>C Allauzen</author>
<author>E Bocchieri</author>
<author>D Hakkani-Tur</author>
<author>A Ljolje</author>
<author>S Parthasarathy</author>
<author>M Rahim</author>
<author>G Riccardi</author>
<author>M Saraclar</author>
</authors>
<title>The AT&amp;T WATSON Speech Recognizer.</title>
<date>2005</date>
<booktitle>In Proceedings ofICASSP,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8551" citStr="Goffin et al., 2005" startWordPosition="1337" endWordPosition="1340">get Model Language Decoding Lexical Reodering Permutation Lattice FSA Composition Sentence Target 153 (empty target phrase). In addition to these phrases, we compute subsequences of a given length k on the bilanguage string and for each subsequence we reorder the target words of the subsequence to be in the same order as they are in the target language sentence corresponding to that bilanguage string. This results in a retokenization of the bilanguage into tokens of source-target phrase pairs. 2.4 SFST Model From the bilanguage corpus B, we train an n-gram language model using standard tools (Goffin et al., 2005). The resulting language model is represented as a weighted finite-state automaton (S x T —* [0,1]). The symbols on the arcs of this automaton (si ti) are interpreted as having the source and target symbols (si:ti), making it into a weighted finite-state transducer (S —* T x[0, 1]) that provides a weighted string-to-string transduction from S into T : T* = argmax P(si, ti|si−1, ti−1 ... si−n−1, ti−n−1) T 2.5 Decoding Since we represent the translation model as a weighted finite-state transducer (TransFST), the decoding process of translating a new source input (sentence or weighted lattice (I3</context>
</contexts>
<marker>Goffin, Allauzen, Bocchieri, Hakkani-Tur, Ljolje, Parthasarathy, Rahim, Riccardi, Saraclar, 2005</marker>
<rawString>V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur, A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi, and M. Saraclar. 2005. The AT&amp;T WATSON Speech Recognizer. In Proceedings ofICASSP, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>M Hearne</author>
<author>K Sima’an</author>
<author>A Way</author>
</authors>
<title>Syntactic phrase-based statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of IEEE/ACL first International Workshop on Spoken Language Technology (SLT),</booktitle>
<location>Aruba,</location>
<marker>Hassan, Hearne, Sima’an, Way, 2006</marker>
<rawString>H. Hassan, M. Hearne, K. Sima’an, and A. Way. 2006. Syntactic phrase-based statistical machine translation. In Proceedings of IEEE/ACL first International Workshop on Spoken Language Technology (SLT), Aruba, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kanthak</author>
<author>D Vilar</author>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Novel reordering approaches in phrase-based statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>167--174</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2567" citStr="Kanthak et al., 2005" startWordPosition="373" endWordPosition="376"> compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult to exactly associate a target word to a source word in many situations – (a) when the translations are not exact but pa</context>
<context position="11230" citStr="Kanthak et al., 2005" startWordPosition="1779" endWordPosition="1782">pply global reordering by permuting the words of the best translation and weighting the result by an n-gram language model (see also Figure 2): T* = BestPath(perm(T�) o LMt) (6) Even the size of the minimal permutation automaton of a linear automaton grows exponentially with the length of the input sequence. While decoding by composition simply resembles the principle of memoization (i.e. here: all state hypotheses of a whole sentence are kept in memory), it is necessary to either use heuristic forward pruning or constrain permutations to be within a local window of adjustable size (also see (Kanthak et al., 2005)). We have chosen to constrain permutations here. Figure 5 shows the resulting minimal permutation automaton for an input sequence of 4 words and a window size of 2. Decoding ASR output in combination with global reordering uses n-best lists or extracts them from lattices first. Each entry of the n-best list is decoded separately and the best target sentence is picked from the union of the n intermediate results. 3 Discriminant Models for Lexical Selection The approach from the previous section is a generative model for statistical machine translation relying on local associations between sour</context>
</contexts>
<marker>Kanthak, Vilar, Matusov, Zens, Ney, 2005</marker>
<rawString>S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005. Novel reordering approaches in phrase-based statistical machine translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 167–174, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="13194" citStr="Lafferty et al., 2001" startWordPosition="2086" endWordPosition="2089">n models as well as provide a framework for incorporating rich morpho-syntactic information. Statistical machine translation can be formulated as a search for the best target sequence that maximizes P(T|S), where S is the source sentence and T is the target sentence. Ideally, P(T|S) should be estimated directly to maximize the conditional likelihood on the training data (discriminant model). However, T corresponds to a sequence with a exponentially large combination of possible labels, and traditional classification approaches cannot be used directly. Although Conditional Random Fields (CRF) (Lafferty et al., 2001) train an exponential model at the sequence level, in translation tasks such as ours the computational requirements of training such models are prohibitively expensive. We investigate two approaches to approximating the string level global classification problem, using different independence assumptions. A comparison of the two approaches is summarized in Table 1. 3.1 Sequential Lexical Choice Model In the first approach, we formulate a sequential local classification problem as shown in Equations 7. This approach is similar to the SFST approach in that it relies on local associations between </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL2002,</booktitle>
<pages>49--55</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="21317" citStr="Malouf, 2002" startWordPosition="3391" endWordPosition="3392">xent is regularized by imposing a Gaussian prior on each weight: this L2 regularization finds the solution with the smallest possible weights. However, on tasks like machine translation with a very large number of input features, a Laplacian L1 regularization that also attempts to maximize the number of zero weights is highly desirable. A new L1-regularized Maxent algorithms was proposed for density estimation (Dudik et al., 2004) and we adapted it to classification. We found this algorithm to converge faster than the current state-ofthe-art in Maxent training, which is L2-regularized L-BFGS (Malouf, 2002)1. Moreover, the number of trained parameters is considerably smaller. 5 Data and Experiments We have performed experiments on the IWSLT06 Chinese-English training and development sets from 1We used the implementation available at http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html 1 P (bj(t)|Φ) = 1 − P (¯bj(t)|Φ) = 156 Table 2: Statistics of training and development data from 2005/2006 (* = first of multiple translations only). Training (2005) Dev 2005 Dev 2006 Chinese English Chinese English Chinese English Sentences 46,311 506 489 Running Words Vocabulary Singletons 351,060 376,615 3</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>R. Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of CoNLL2002, pages 49–55. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="30321" citStr="Melamed, 2004" startWordPosition="4897" endWordPosition="4898">slations of the same source word. Then, in the presence of this source word, the probability to detect the corresponding target word, which we assume is 0.8, will be, because of discriminant learning, split equally between t1 and t2, that is 0.4 and 0.4. Because of this synonym problem, the BOW threshold 0 has to be set lower than 0.5, which is observed experimentally. However, if we set the threshold to 0.3, both t1 and t2 will be detected in the target sentence, and we found this to be a major source of undesirable insertions. The BOW approach is different from the parsing based approaches (Melamed, 2004; Zhang and Gildea, 2005; Cowan et al., 2006) where the translation model tightly couples the syntactic and lexical items of the two languages. The decoupling of the 158 two steps in our model has the potential for generating paraphrased sentences not necessarily isomorphic to the structure of the source sentence. 7 Conclusions We view machine translation as consisting of lexical selection and lexical reordering steps. These two steps need not necessarily be sequential and could be tightly integrated. We have presented the weighted finite-state transducer model of machine translation where lex</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I. D. Melamed. 2004. Statistical machine translation by parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="12233" citStr="Och and Ney, 2002" startWordPosition="1934" endWordPosition="1937"> of the n intermediate results. 3 Discriminant Models for Lexical Selection The approach from the previous section is a generative model for statistical machine translation relying on local associations between source and target sentences. Now, we present our approach for a global lexical selection model based on discriminatively trained classification techniques. Discriminant modeling techniques have become the dominant method for resolving ambiguity in speech and other NLP tasks, outperforming generative models. Discriminative training has been used mainly for translation model combination (Och and Ney, 2002) and with the exception of (Wellington et al., 2006; Tillmann and Zhang, 2006), has not been used to directly train parameters of a translation model. We expect discriminatively trained global lexical selection models 0000 2 1 0100 1000 2 3 1 1010 1100 2 4 3 1110 1101 4 3 1111 154 to outperform generatively trained local lexical selection models as well as provide a framework for incorporating rich morpho-syntactic information. Statistical machine translation can be formulated as a search for the best target sequence that maximizes P(T|S), where S is the source sentence and T is the target sen</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1595" citStr="Och and Ney, 2003" startWordPosition="231" endWordPosition="234">om a finite-state based statistical machine translation system which relies on local lexical associations. 1 Introduction Machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. Most of the previous work on statistical machine translation, as exemplified in (Brown et al., 1993), employs word-alignment algorithm (such as GIZA++ (Och and Ney, 2003)) that provides local associations between source and target words. The source-to-target word alignments are sometimes augmented with target-to-source word alignments in order to improve precision. Further, the word-level alignments are extended to phraselevel alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to</context>
<context position="5641" citStr="Och and Ney, 2003" startWordPosition="869" endWordPosition="872">FST system shown in Figure 1. The SFST approach described here is similar to the one described in (Bangalore and Riccardi, 2000) which has subsequently been adopted by (Banchs et al., 2005). 2.1 Word Alignment The first stage in the process of training a lexical selection model is obtaining an alignment function (f) that given a pair of source (s1s2 ... sn) and target (t1t2 ... tm) language sentences, maps source language word subsequences into target language word subsequences, as shown below. ∀i∃j(f(si) = tj ∨ f(si) = 2) (1) For the work reported in this paper, we have used the GIZA++ tool (Och and Ney, 2003) which implements a string-alignment algorithm. GIZA++ alignment however is asymmetric in that the word mappings are different depending on the direction of alignment – source-to-target or target-to-source. Hence in addition to the functions f as shown in Equation 1 we train another alignment function g : ∀j∃i(g(tj) = si ∨ g(tj) = 2) (2) English: I need to make a collect call Japanese:fj6j nVlF n—)SI- b,(t7D �LARI���bh1_t Alignment: 1 5 0 3 0 2 4 Figure 3: Example bilingual texts with alignment information I:fjj� need:,LVb,�)9t* to:2 make::2—iLi� a:2 collect :2L i call b)(16 Figure 4: Bilangua</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Rifkin</author>
<author>Aldebaro Klautau</author>
</authors>
<date>2004</date>
<booktitle>In defense of onevs-all classification. Journal ofMachine Learning Research,</booktitle>
<pages>101--141</pages>
<contexts>
<context position="18955" citStr="Rifkin and Klautau, 2004" startWordPosition="3023" endWordPosition="3026">e to reside in memory at the same time during training which allows for parallelization. Fortunately for the sequential model, we can decompose a multiclass classification problem into separate 1-vs-other problems. In theory, one has to make an additional independence assumption and the problem statement becomes different. Each output label t is projected into a bit string with components bj(t) where probability of each component is estimated independently: 1 + e−(a;−a,)·4D In practice, despite the approximation, the 1-vsother scheme has been shown to perform as well as the multiclass scheme (Rifkin and Klautau, 2004). As a consequence, we use the same type of binary classifier for the sequential and the BOW models. The excellent results recently obtained with the SEARN algorithm (Daume et al., 2007) also suggest that binary classifiers, when properly trained and combined, seem to be capable of matching more complex structured output approaches. 4.2 Geometric vs. Probabilistic Interpretation We separate the most popular classification techniques into two broad categories: • Geometric approaches maximize the width of a separation margin between the classes. The most popular method is the Support Vector Mach</context>
</contexts>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>Ryan Rifkin and Aldebaro Klautau. 2004. In defense of onevs-all classification. Journal ofMachine Learning Research, pages 101–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>T Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical mt.</title>
<date>2006</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="12311" citStr="Tillmann and Zhang, 2006" startWordPosition="1947" endWordPosition="1950">tion The approach from the previous section is a generative model for statistical machine translation relying on local associations between source and target sentences. Now, we present our approach for a global lexical selection model based on discriminatively trained classification techniques. Discriminant modeling techniques have become the dominant method for resolving ambiguity in speech and other NLP tasks, outperforming generative models. Discriminative training has been used mainly for translation model combination (Och and Ney, 2002) and with the exception of (Wellington et al., 2006; Tillmann and Zhang, 2006), has not been used to directly train parameters of a translation model. We expect discriminatively trained global lexical selection models 0000 2 1 0100 1000 2 3 1 1010 1100 2 4 3 1110 1101 4 3 1111 154 to outperform generatively trained local lexical selection models as well as provide a framework for incorporating rich morpho-syntactic information. Statistical machine translation can be formulated as a search for the best target sequence that maximizes P(T|S), where S is the source sentence and T is the target sentence. Ideally, P(T|S) should be estimated directly to maximize the conditiona</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>C. Tillmann and T. Zhang. 2006. A discriminative global training algorithm for statistical mt. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="19579" citStr="Vapnik, 1998" startWordPosition="3121" endWordPosition="3122">equence, we use the same type of binary classifier for the sequential and the BOW models. The excellent results recently obtained with the SEARN algorithm (Daume et al., 2007) also suggest that binary classifiers, when properly trained and combined, seem to be capable of matching more complex structured output approaches. 4.2 Geometric vs. Probabilistic Interpretation We separate the most popular classification techniques into two broad categories: • Geometric approaches maximize the width of a separation margin between the classes. The most popular method is the Support Vector Machine (SVM) (Vapnik, 1998). • Probabilistic approaches maximize the conditional likelihood of the output class given the input features. This logistic regression is also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data (Berger et al., 1996). In previous studies, we found that the best accuracy is achieved with non-linear (or kernel) SVMs, at the expense of a high test time complexity, which is unacceptable for machine translation. Linear SVMs and regularized Maxent yield similar performance. In theory, Maxent training, which scale</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V.N. Vapnik. 1998. Statistical Learning Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellington</author>
<author>J Turian</author>
<author>C Pike</author>
<author>D Melamed</author>
</authors>
<title>Scalable purely-discriminative training for word and tree transducers.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="12284" citStr="Wellington et al., 2006" startWordPosition="1943" endWordPosition="1946"> Models for Lexical Selection The approach from the previous section is a generative model for statistical machine translation relying on local associations between source and target sentences. Now, we present our approach for a global lexical selection model based on discriminatively trained classification techniques. Discriminant modeling techniques have become the dominant method for resolving ambiguity in speech and other NLP tasks, outperforming generative models. Discriminative training has been used mainly for translation model combination (Och and Ney, 2002) and with the exception of (Wellington et al., 2006; Tillmann and Zhang, 2006), has not been used to directly train parameters of a translation model. We expect discriminatively trained global lexical selection models 0000 2 1 0100 1000 2 3 1 1010 1100 2 4 3 1110 1101 4 3 1111 154 to outperform generatively trained local lexical selection models as well as provide a framework for incorporating rich morpho-syntactic information. Statistical machine translation can be formulated as a search for the best target sequence that maximizes P(T|S), where S is the source sentence and T is the target sentence. Ideally, P(T|S) should be estimated directly</context>
</contexts>
<marker>Wellington, Turian, Pike, Melamed, 2006</marker>
<rawString>B. Wellington, J. Turian, C. Pike, and D. Melamed. 2006. Scalable purely-discriminative training for word and tree transducers. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2447" citStr="Wu, 1997" startWordPosition="356" endWordPosition="357">d to phraselevel alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th ACL.</booktitle>
<contexts>
<context position="2494" citStr="Yamada and Knight, 2001" startWordPosition="362" endWordPosition="365">rder to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words – those permitted by the size of the phrase. Most of the state-of-the-art machine translation systems use phrase-level associations in conjunction with a target language model to produce sentences. There is relatively little emphasis on (global) lexical reordering other than the local reorderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax-based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present an alternate approach to lexical selection and lexical reordering. For lexical selection, in contrast to the local approaches of associating target to source words, we associate target words to the entire source sentence. The intuition is that there may be lexico-syntactic features of the source sentence (not necessarily a single source word) that might trigger the presence of a target word in the target sentence. Furthermore, it might be difficult to exactly associate a target word to a source</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proceedings of 39th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>257--264</pages>
<location>Boston, MA.</location>
<contexts>
<context position="26646" citStr="Zens and Ney, 2004" startWordPosition="4280" endWordPosition="4283"> 6 6.5 7 7.5 8 8.5 9 9.5 10 Permutation Window Size Figure 6: Improvement in mBLEU score with the increase in size of the permutation window 5.1 United Nations and Hansard Corpora In order to test the scalability of the global lexical selection approach, we also performed lexical selection experiments on the United Nations (ArabicEnglish) corpus and the Hansard (French-English) corpus using the SFST model and the BOW Maxent model. We used 1,000,000 training sentence pairs and tested on 994 test sentences for the UN corpus. For the Hansard corpus we used the same training and test split as in (Zens and Ney, 2004): 1.4 million training sentence pairs and 5432 test sentences. The vocabulary sizes for the two corpora are mentioned in Table 4. Also in Table 4, are the results in terms of F-measure between the words in the reference sentence and the decoded sentences. We can see that the BOW model outperforms the SFST model on both corpora significantly. This is due to a systematic 10% relative improvement for open class words, as they benefit from a much wider context. BOW performance on close class words is higher for the UN corpus but lower for the Hansard corpus. Table 4: Lexical Selection results (F-m</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens and H. Ney. 2004. Improvements in phrase-based statistical machine translation. In Proceedings ofHLT-NAACL, pages 257–264, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="30345" citStr="Zhang and Gildea, 2005" startWordPosition="4899" endWordPosition="4902"> same source word. Then, in the presence of this source word, the probability to detect the corresponding target word, which we assume is 0.8, will be, because of discriminant learning, split equally between t1 and t2, that is 0.4 and 0.4. Because of this synonym problem, the BOW threshold 0 has to be set lower than 0.5, which is observed experimentally. However, if we set the threshold to 0.3, both t1 and t2 will be detected in the target sentence, and we found this to be a major source of undesirable insertions. The BOW approach is different from the parsing based approaches (Melamed, 2004; Zhang and Gildea, 2005; Cowan et al., 2006) where the translation model tightly couples the syntactic and lexical items of the two languages. The decoupling of the 158 two steps in our model has the potential for generating paraphrased sentences not necessarily isomorphic to the structure of the source sentence. 7 Conclusions We view machine translation as consisting of lexical selection and lexical reordering steps. These two steps need not necessarily be sequential and could be tightly integrated. We have presented the weighted finite-state transducer model of machine translation where lexical choice and a limite</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>H. Zhang and D. Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>