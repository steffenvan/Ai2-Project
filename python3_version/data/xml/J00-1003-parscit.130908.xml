<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9994835">
Practical Experiments with Regular
Approximation of Context-Free Languages
</title>
<author confidence="0.950103">
Mark-Jan Nederhof*
</author>
<bodyText confidence="0.86073375">
German Research Center for Artificial
Intelligence
Several methods are discussed that construct a finite automaton given a context-free grammar,
including both methods that lead to subsets and those that lead to supersets of the original
context-free language. Some of these methods of regular approximation are new, and some others
are presented here in a more refined form with respect to existing literature. Practical experiments
with the different methods of regular approximation are performed for spoken-language input:
hypotheses from a speech recognizer are filtered through a finite automaton.
</bodyText>
<sectionHeader confidence="0.990092" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999850038461538">
Several methods of regular approximation of context-free languages have been pro-
posed in the literature. For some, the regular language is a superset of the context-free
language, and for others it is a subset. We have implemented a large number of meth-
ods, and where necessary, refined them with an analysis of the grammar. We also
propose a number of new methods.
The analysis of the grammar is based on a sufficient condition for context-free
grammars to generate regular languages. For an arbitrary grammar, this analysis iden-
tifies sets of rules that need to be processed in a special way in order to obtain a regular
language. The nature of this processing differs for the respective approximation meth-
ods. For other parts of the grammar, no special treatment is needed and the grammar
rules are translated to the states and transitions of a finite automaton without affecting
the language.
Few of the published articles on regular approximation have discussed the appli-
cation in practice. In particular, little attention has been given to the following two
questions: First, what happens when a context-free grammar grows in size? What is
then the increase of the sizes of the intermediate results and the obtained minimal de-
terministic automaton? Second, how &amp;quot;precise&amp;quot; are the approximations? That is, how
much larger than the original context-free language is the language obtained by a
superset approximation, and how much smaller is the language obtained by a subset
approximation? (How we measure the &amp;quot;sizes&amp;quot; of languages in a practical setting will
become clear in what follows.)
Some considerations with regard to theoretical upper bounds on the sizes of the
intermediate results and the finite automata have already been discussed in Nederhof
(1997). In this article we will try to answer the above two questions in a practical set-
ting, using practical linguistic grammars and sentences taken from a spoken-language
corpus.
</bodyText>
<note confidence="0.836903">
* DFKL Stuhlsatzenhausweg 3, D-66123 Saarbrticken, Germany. E-mail: nederhof@dflci.de
© 2000 Association for Computational Linguistics
Computational Linguistics Volume 26, Number 1
</note>
<bodyText confidence="0.99971475">
The structure of this paper is as follows: In Section 2 we recall some standard
definitions from language theory. Section 3 investigates a sufficient condition for a
context-free grammar to generate a regular language. We also present the construction
of a finite automaton from such a grammar. In Section 4, we discuss several meth-
ods to approximate the language generated by a grammar if the sufficient condition
mentioned above is not satisfied. These methods can be enhanced by a grammar trans-
formation presented in Section 5. Section 6 compares the respective methods, which
leads to conclusions in Section 7.
</bodyText>
<sectionHeader confidence="0.981225" genericHeader="keywords">
2. Preliminaries
</sectionHeader>
<bodyText confidence="0.998769916666667">
Throughout this paper we use standard formal language notation (see, for example,
Harrison [19781). In this section we review some basic definitions.
A context-free grammar G is a 4-tuple (E,N,P,S), where E and N are two finite
disjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, and
P is a finite set of rules. Each rule has the form A a with A E N and a E V*, where
V denotes N U E. The relation on N x V* is extended to a relation on V* x V* as
usual. The transitive and reflexive closure of is denoted by
The language generated by G is given by the set {w E E* I S wl. By definition,
such a set is a context-free language. By reduction of a grammar we mean the elimi-
nation from P of all rules A --+ such that S aA0 -4 cry -÷* w does not hold for
any a,/3 e V* and w E E*.
We generally use symbols A, B,C, . . . to range over N, symbols a, b, c, . . . to range
over E, symbols X, Y, Z to range over V. symbols a, 0,-y, . to range over V*, and
symbols v, w, x, . . to range over E*. We write c to denote the empty string.
A rule of the form A B is called a unit rule.
A (nondeterministic) finite automaton .T is a 5-tuple (K, E, A, s, F), where K is a
finite set of states, of which s is the initial state and those in F C K are the final states,
E is the input alphabet, and the transition relation A is a finite subset of K x E* x K.
We define a configuration to be an element of K x E. We define the binary relation
I- between configurations as: (q, ow) (q&apos;, w) if and only if (q, v, q&apos;) E A. The transitive
and reflexive closure of H is denoted by H*.
Some input v is recognized if (s, v) 1-* (q, €), for some q E F. The language accepted
by .F is defined to be the set of all strings v that are recognized. By definition, a
language accepted by a finite automaton is called a regular language.
</bodyText>
<sectionHeader confidence="0.527437" genericHeader="introduction">
3. Finite Automata in the Absence of Self-Embedding
</sectionHeader>
<bodyText confidence="0.999800583333333">
We define a spine in a parse tree to be a path that runs from the root down to some
leaf. Our main interest in spines lies in the sequences of grammar symbols at nodes
bordering on spines.
A simple example is the set of parse trees such as the one in Figure 1, for a
grammar of palindromes. It is intuitively clear that the language is not regular: the
grammar symbols to the left of the spine from the root to E &amp;quot;communicate&amp;quot; with those
to the right of the spine. More precisely, the prefix of the input up to the point where
it meets the final node c of the spine determines the suffix after that point, in such
a way that an unbounded quantity of symbols from the prefix need to be taken into
account.
A formal explanation for why the grammar may not generate a regular language
relies on the following definition (Chomsky 1959b):
</bodyText>
<page confidence="0.994371">
18
</page>
<figure confidence="0.7481206">
Nederhof Experiments with Regular Approximation
a S a
S --&gt; a S a
S — &gt; b S b
S E b S b
</figure>
<figureCaption confidence="0.976372">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.457746">
Grammar of palindromes, and a parse tree.
Definition
</subsectionHeader>
<bodyText confidence="0.999909">
A grammar is self-embedding if there is some A E N such that A —›* aA3, for some
a 0 c and
If a grammar is not self-embedding, this means that when a section of a spine in
a parse tree repeats itself, then either no grammar symbols occur to the left of that
section of the spine, or no grammar symbols occur to the right. This prevents the
&amp;quot;unbounded communication&amp;quot; between the two sides of the spine exemplified by the
palindrome grammar.
We now prove that grammars that are not self-embedding generate regular lan-
guages. For an arbitrary grammar, we define the set of recursive nonterminals as:
</bodyText>
<equation confidence="0.735422">
N = {A E N I /3[A —›* aA3]}
</equation>
<bodyText confidence="0.99937">
We determine the partition Al of N consisting of subsets N1, N2,. , Nk, for some k&gt; 0,
of mutually recursive nonterminals:
</bodyText>
<equation confidence="0.809606666666667">
= {N1, N2, • • • .Nic}
U N2 U . . . U Nk =N
Vi[N, 0]
Vi,j[i j n N, = 0]
and for all A,B E N:
E N, A B E — ]ai, A, az, 132[A —&gt;* air3/31 AB ct2A1321,
We now define the function recursive from Al to the set {left, right, self, cyclic} . For
1 &lt;i &lt; k:
recursive(Ni) - left, if -leftGenerating(Ni) A RightGenerating(Ni)
</equation>
<listItem confidence="0.479123333333333">
- right, if LeftGenerating(Ni) A -RightGenerating(Ni)
- self, if LeftGenerating(Ni) A RightGenerating(Ni)
- cyclic, if LeftGenerating(Ni) A -RightGenerating(Ni)
</listItem>
<bodyText confidence="0.980134">
where
</bodyText>
<equation confidence="0.9715005">
LeftGenerating(Ni) A(A aB,(3) E P[A E N, A B E N, A a 0 E]
RightGenerating(Ni) = 3(A —&gt; al30) E P[A E N, AB E N A13 E]
</equation>
<page confidence="0.998471">
19
</page>
<note confidence="0.880617">
Computational Linguistics Volume 26, Number 1
</note>
<bodyText confidence="0.99977648">
When recursive(N1) = left, Ni consists of only left-recursive nonterminals, which does
not mean it cannot also contain right-recursive nonterminals, but in that case right
recursion amounts to application of unit rules. When recursive(Ni) = cyclic, it is only
such unit rules that take part in the recursion.
That recursive(Ni) = self, for some i, is a sufficient and necessary condition for the
grammar to be self-embedding. Therefore, we have to prove that if recursive(Ni) E
{left, right, cyclic}, for all i, then the grammar generates a regular language. Our proof
differs from an existing proof (Chomsky 1959a) in that it is fully constructive: Fig-
ure 2 presents an algorithm for creating a finite automaton that accepts the language
generated by the grammar.
The process is initiated at the start symbol, and from there the process descends
the grammar in all ways until terminals are encountered, and then transitions are
created labeled with those terminals. Descending the grammar is straightforward in
the case of rules of which the left-hand side is not a recursive nonterminal: the sub-
automata found recursively for members in the right-hand side will be connected.
In the case of recursive nonterminals, the process depends on whether the nontermi-
nals in the corresponding set from Al are mutually left-recursive or right-recursive;
if they are both, which means they are cyclic, then either subprocess can be ap-
plied; in the code in Figure 2 cyclic and right-recursive subsets N, are treated uni-
formly.
We discuss the case in which the nonterminals are left-recursive. One new state is
created for each nonterminal in the set. The transitions that are created for terminals
and nonterminals not in N, are connected in a way that is reminiscent of the con-
struction of left-corner parsers (Rosertkrantz and Lewis 1970), and specifically of one
construction that focuses on sets of mutually recursive nonterminals (Nederhof 1994,
Section 5.8).
An example is given in Figure 3. Four states have been labeled according to the
names they are given in procedure make fa. There are two states that are labeled qB.
This can be explained by the fact that nonterminal B can be reached by descending
the grammar from S in two essentially distinct ways.
The code in Figure 2 differs from the actual implementation in that sometimes, for a
nonterminal, a separate finite automaton is constructed, namely, for those nonterminals
that occur as A in the code. A transition in such a subautomaton may be labeled by
another nonterminal B, which then represents the subautomaton corresponding to B.
The resulting representation is similar to extended context-free grammars (Purdom
and Brown 1981), with the exception that in our case recursion cannot occur, by virtue
of the construction.
The representation for the running example is indicated by Figure 4, which shows
two subautomata, labeled S and B. The one labeled S is the automaton on the top level,
and contains two transitions labeled B, which refer to the other subautomaton. Note
that this representation is more compact than that of Figure 3, since the transitions
that are involved in representing the sublanguage of strings generated by nonterminal
B are included only once.
The compact representation consisting of subautomata can be turned into a sin-
gle finite automaton by substituting subautomata A for transitions labeled A in other
automata. This comes down to regular substitution in the sense of Berstel (1979). The
advantage of this way of obtaining a finite automaton over a direct construction of a
nondeterministic automaton is that subautomata may be determinized and minimized
before they are substituted into larger subautomata. Since in many cases determinized
and minimized automata are much smaller, this process avoids much of the combina-
</bodyText>
<page confidence="0.966801">
20
</page>
<title confidence="0.290732">
Nederhof Experiments with Regular Approximation
</title>
<construct confidence="0.760459533333333">
let K =- 0, A = 0, s fresh_state, f = fresh_state, F = {f};
make_fa(s, S, f).
procedure make_fa(q0, a, q1):
if a = e
then let A = A U {(q0,e, qi)}
elseif a = a, some a E E
then let A = A U {(q0, a, qi)}
elseif a = X/3, some X E V, 3 E V* such that 101 &gt; 0
then let q = fresh_state;
make_fa(qo , X, q);
make_fa(q, 13, q1)
else let A = a; (* a must consist of a single nonterminal *)
if there exists i such that A E Ari
then for each B E Ni do let qB = fresh_state end;
if recursive(N) = left
</construct>
<bodyText confidence="0.433759">
then for each (C Xi • • • X,n) E P such that C E PT, A , , Xm Ni
</bodyText>
<equation confidence="0.987118653846154">
do make_fa(go, • • X,.,„ qc)
end;
for each (C # DX1 • • • _Km) E P such that
C,D E Ni A , , X&amp;quot;, V Ni
do make_fa(qD, • qc)
end;
let A A U {(qA, E,
else (* recursive(N) E {right, cyclic} *)
for each (C X1 • • • X,,,) E P such that CEN A Xi, ,XmNj
do make_fa(qc, X1 • • • Xm 7 ql)
end;
for each (C -4 Xi • • • X,,D) E P such that
C,D E Ni A X1, ... Ni
do make_fa(qc, • • X., qD)
end;
let A = A U {(q0, e,qA)}
end
else for each (A 13) E P do make_fa(q0, qi) end (* A is not recursive *)
end
end
end.
procedure fresh_state():
create some object q such that q K;
let K = K U {q};
return q
end.
</equation>
<figureCaption confidence="0.814105">
Figure 2
</figureCaption>
<bodyText confidence="0.9807715">
Transformation from a grammar G = (E, N, P, S) that is not self-embedding into an equivalent
finite automaton .F = (K, E, A, s, F).
</bodyText>
<page confidence="0.997499">
21
</page>
<figure confidence="0.882466222222222">
Computational Linguistics Volume 26, Number 1
S —&gt; Aa N = {S, A, B}
A —+ SB N2}
A -- Bb -= {S, Al recursive(Ni) = left
B —+ Bc N2 = {B} recursive(N2) = left
B -- d
C
Figure 3
Application of the code from Figure 2 on a small grammar.
</figure>
<figureCaption confidence="0.964461">
Figure 4
</figureCaption>
<bodyText confidence="0.990876181818182">
The automaton from Figure 3 in a compact representation.
tonal explosion that takes place upon naive construction of a single nondeterministic
finite automaton.&apos;
Assume we have a list of subautomata A1, . . , Am that is ordered from lower-level
to higher-level automata; i.e., if an automaton Ap occurs as the label of a transition
of automaton Aq, then p &lt; q; Am must be the start symbol S. This order is a natural
result of the way that subautomata are constructed during our depth-first traversal of
the grammar, which is actually postorder in the sense that a subautomaton is output
after all subautomata occurring at its transitions have been output.
Our implementation constructs a minimal deterministic automaton by repeating
the following for p = 1, . . . , m:
</bodyText>
<listItem confidence="0.998965">
1. Make a copy of A. Determinize and minimize the copy. If it has fewer
transitions labeled by nonterminals than the original, then replace Ap by
its copy.
2. Replace each transition in Ap of the form (q, A„ q&apos;) by (a copy of)
automaton A, in a straightforward way. This means that new e-transitions
connect q to the start state of A, and the final states of A, to q&apos;.
</listItem>
<footnote confidence="0.963982">
1 The representation in Mohri and Pereira (1998) is even more compact than ours for grammars that are
not self-embedding. However, in this paper we use our representation as an intermediate result in
approximating an unrestricted context-free grammar, with the final objective of obtaining a single
minimal deterministic automaton. For this purpose, Mohri and Pereira&apos;s representation offers little
advantage.
</footnote>
<page confidence="0.663134">
qB
22
</page>
<subsectionHeader confidence="0.72575">
Nederhof Experiments with Regular Approximation
</subsectionHeader>
<bodyText confidence="0.813948">
3. Again determinize and minimize Ap and store it for later reference.
The automaton obtained for An, after step 3 is the desired result.
</bodyText>
<subsectionHeader confidence="0.641949">
4. Methods of Regular Approximation
</subsectionHeader>
<bodyText confidence="0.999765384615385">
This section describes a number of methods for approximating a context-free gram-
mar by means of a finite automaton. Some published methods did not mention self-
embedding explicitly as the source of nonregularity for the language, and suggested
that approximations should be applied globally for the complete grammar. Where
this is the case, we adapt the method so that it is more selective and deals with
self-embedding locally.
The approximations are integrated into the construction of the finite automaton
from the grammar, which was described in the previous section. A separate incarnation
of the approximation process is activated upon finding a nonterminal A such that
A E N, and recursive(N,) = self, for some i. This incarnation then only pertains to
the set of rules of the form B a, where B E Ni. In other words, nonterminals not
in Ni are treated by this incarnation of the approximation process as if they were
terminals.
</bodyText>
<subsectionHeader confidence="0.999727">
4.1 Superset Approximation Based on RTNs
</subsectionHeader>
<bodyText confidence="0.986900965517241">
The following approximation was proposed in Nederhof (1997). The presentation
here, however, differs substantially from the earlier publication, which treated the ap-
proximation process entirely on the level of context-free grammars: a self-embedding
grammar was transformed in such a way that it was no longer self-embedding. A
finite automaton was then obtained from the grammar by the algorithm discussed
above.
The presentation here is based on recursive transition networks (RTNs) (Woods
1970). We can see a context-free grammar as an RTN as follows: We introduce two
states qA and q&apos;A for each nonterminal A, and m + 1 states qo, , q„, for each rule
A —&gt; Xi ... Xm. The states for a rule A Xm are connected with each other and
to the states for the left-hand side A by one transition (qA, Et q0), a transition qi)
for each i such that 1 &lt; i &lt; m, and one transition (qm, e, q). (Actually, some epsilon
transitions are avoided in our implementation, but we will not be concerned with such
optimizations here.)
In this way, we obtain a finite automaton with initial state qA and final state q&apos;A for
each nonterminal A and its defining rules A Xm. This automaton can be seen
as one component of the RTN. The complete RTN is obtained by the collection of all
such finite automata for different nonterminals.
An approximation now results if we join all the components in one big automaton,
and if we approximate the usual mechanism of recursion by replacing each transition
(q, A, q&apos;) by two transitions (q, c, qA) and (q&apos;A, E, q&apos;). The construction is illustrated in
Figure 5.
In terms of the original grammar, this approximation can be informally explained
as follows: Suppose we have three rules B aA,3, B&apos; a&apos; A,3&apos; , and A —&gt; 7. Top-down,
left-to-right parsing would proceed, for example, by recognizing a in the first rule;
it would then descend into rule A -4 -y, and recognize -y; it would then return to
the first rule and subsequently process 0. In the approximation, however, the finite
automaton &amp;quot;forgets&amp;quot; which rule it came from when it starts to recognize -y, so that it
may subsequently recognize 13&apos; in the second rule.
</bodyText>
<page confidence="0.994201">
23
</page>
<figure confidence="0.9990859">
Computational Linguistics Volume 26, Number 1
(b)
A—&gt;a B b
A — &gt; c A
B d A e
B &gt; f
(a)
A
.•
(c)
</figure>
<figureCaption confidence="0.993851">
Figure 5
</figureCaption>
<bodyText confidence="0.993970739130435">
Application of the RTN method for the grammar in (a). The RTN is given in (b), and (c)
presents the approximating finite automaton. We assume A is the start symbol and therefore
qA becomes the initial state and q&apos;A becomes the final state in the approximating automaton.
For the sake of presentational convenience, the above describes a construction
working on the complete grammar. However, our implementation applies the con-
struction separately for each nonterminal in a set N, such that recursive(N) = self,
which leads to a separate subautomaton of the compact representation (Section 3).
See Nederhof (1998) for a variant of this approximation that constructs finite trans-
ducers rather than finite automata.
We have further implemented a parameterized version of the RTN approximation.
A state of the nondeterministic automaton is now also associated to a list H of length
IHI strictly smaller than a number d, which is the parameter to the method. This list
represents a history of rule positions that were encountered in the computation leading
to the present state.
More precisely, we define an item to be an object of the form [A —&gt; a • 0],
where A —&gt; ai3 is a rule from the grammar. These are the same objects as the &amp;quot;dot-
ted&amp;quot; productions of Earley (1970). The dot indicates a position in the right-hand
side.
The unparameterized RTN method had one state qj for each item I, and two states
qA and q&apos;A for each nonterminal A. The parameterized RTN method has one state qm
for each item I and each list of items H that represents a valid history for reaching
I, and two states qAH and q&apos;AH for each nonterminal A and each list of items H that
represents a valid history for reaching A. Such a valid history is defined to be a list
</bodyText>
<page confidence="0.994808">
24
</page>
<subsectionHeader confidence="0.515474">
Nederhof Experiments with Regular Approximation
</subsectionHeader>
<bodyText confidence="0.990773918918919">
H with 0 &lt; IHI &lt;d that represents a series of positions in rules that could have been
invoked before reaching I or A, respectively. More precisely, if we set H I . . . In, then
each Im (1 &lt;m &lt;n) should be of the form [Am —&gt; am • B,n0m] and for 1 &lt;m &lt; n we
should have Am = B,±1. Furthermore, for a state q/H with I = [A —&gt; a • 13] we demand
A = B1 if n &gt; 0. For a state gill/ we demand A = B1 if n &gt; 0. (Strictly speaking, states
gill/ and qffi, with IHI &lt;d 1 and I = [A —&gt; a • 0], will only be needed if Aim is the
start symbol in the case 11-II &gt; 0, or if A is the start symbol in the case H = E.)
The transitions of the automaton that pertain to terminals in right-hand sides
of rules are very similar to those in the case of the unparameterized method: For a
state qm with I of the form [A a • a0], we create a transition (qm, a, qm), with
I&apos; = [A —&gt; aa • 0].
Similarly, we create epsilon transitions that connect left-hand sides and right-hand
sides of rules: For each state qAH there is a transition (01A11/ 6/ (3111) for each item I =
[A —&gt; • a], for some a, and for each state of the form qv&apos;&apos;, with I&apos; = [A —&gt; a .], there
is a transition (qr,H, q/AH).
For transitions that pertain to nonterminals in the right-hand sides of rules, we
need to manipulate the histories. For a state qIH with I of the form [A a • B0], we
create two epsilon transitions. One is (qM/ Et qBH,), where H&apos; is defined to be IH if
III/ I &lt; d, and to be the first d — 1 items of IH, otherwise. Informally, we extend the
history by the item I representing the rule position that we have just come from, but
the oldest information in the history is discarded if the history becomes too long. The
second transition is (q/Bw, E, qP11), with I&apos; = [A —&gt; aB • l3].
If the start symbol is S, the initial state is qs and the final state is g&apos;s (after the
symbol S in the subscripts we find empty lists of items). Note that the parameterized
method with d = 1 concurs with the unparameterized method, since the lists of items
then remain empty.
An example with parameter d = 2 is given in Figure 6. For the unparameterized
method, each I = [A —&gt; a • 0] corresponded to one state (Figure 5). Since reaching A
can have three different histories of length shorter than 2 (the empty history, since A is
the start symbol; the history of coming from the rule position given by item [A c • A];
and the history of coming from the rule position given by item [B d • Ae]), in Figure 6
we now have three states of the form gm for each I = [A —&gt; a • 0], as well as three
states of the form gAn and q&apos;A
The higher we choose d, the more precise the approximation is, since the histories
allow the automaton to simulate part of the mechanism of recursion from the original
grammar, and the maximum length of the histories corresponds to the number of
levels of recursion that can be simulated accurately.
</bodyText>
<subsectionHeader confidence="0.999969">
4.2 Refinement of RTN Superset Approximation
</subsectionHeader>
<bodyText confidence="0.99990875">
We rephrase the method of Grimley-Evans (1997) as follows: First, we construct the
approximating finite automaton according to the unparameterized RTN method above.
Then an additional mechanism is introduced that ensures for each rule A — Xi . X„,
separately that the list of visits to the states go, , gni satisfies some reasonable criteria:
a visit to g,, with 0 &lt; i &lt; m, should be followed by one to gi+i or go. The latter option
amounts to a nested incarnation of the rule. There is a complementary condition for
what should precede a visit to q,, with 0 &lt; i &lt; m.
Since only pairs of consecutive visits to states from the set {go, ...,grn} are consid-
ered, finite-state techniques suffice to implement such conditions. This can be realized
by attaching histories to the states as in the case of the parameterized RTN method
above, but now each history is a set rather than a list, and can contain at most one
item [A —&gt; a • 0] for each rule A a0. As reported by Grimley-Evans (1997) and con-
</bodyText>
<page confidence="0.996496">
25
</page>
<figureCaption confidence="0.963187">
Figure 6
</figureCaption>
<bodyText confidence="0.992211875">
Application of the parameterized RTN method with d =- 2. We again assume A is the start
symbol. States qm have not been labeled in order to avoid cluttering the picture.
firmed by our own experiments, the nondeterministic finite automata resulting from
this method may be quite large, even for small grammars. The explanation is that the
number of such histories is exponential in the number of rules.
We have refined the method with respect to the original publication by applying
the construction separately for each nonterminal in a set N, such that recursive(N) =
self.
</bodyText>
<subsectionHeader confidence="0.998557">
4.3 Subset Approximation by Transforming the Grammar
</subsectionHeader>
<bodyText confidence="0.999507">
Putting restrictions on spines is another way to obtain a regular language. Several
methods can be defined. The first method we present investigates spines in a very
detailed way. It eliminates from the language only those sentences for which a sub-
derivation is required of the form B ceB[3, for some a € and )3 E. The motivation
is that such sentences do not occur frequently in practice, since these subderivations
make them difficult for people to comprehend (Resnik 1992). Their exclusion will
therefore not lead to much loss of coverage of typical sentences, especially for simple
application domains.
We express the method in terms of a grammar transformation in Figure 7. The
effect of this transformation is that a nonterminal A is tagged with a set of pairs
(B, Q), where B is a nonterminal occurring higher in the spine; for any given B, at
most one such pair (B, Q) can be contained in the set. The set Q may contain the
element I to indicate that something to the left of the part of the spine from B to A
</bodyText>
<figure confidence="0.891512888888889">
Volume 26, Number 1
Computational Linguistics
H= [A—&gt;a•B {61B
A --&gt; a B b
A - &gt; c A
B &gt; d A
B --&gt; f
26
Nederhof Experiments with Regular Approximation
</figure>
<bodyText confidence="0.9404885">
We are given a grammar G = (E, N, P. S). The following is to be performed for each
set N, E Ai such that recursive(N) = self.
</bodyText>
<listItem confidence="0.949233555555556">
1. For each A E Ni and each F E 2(1/&apos; X 211&apos;r ), add the following nonterminal
to N.
• AF.
2. For each A E N,, add the following rule to P.
• A —&gt; A° .
3. For each (A -4 a0A1ct1A2 E P such that A, Ai, ,A,, E N
and no symbols from a0, ..., am are members of Ni, and each F such that
(A, {1, r}) F, add the following rule to P.
AF i
• ao4al 4Ba„„ where, for 1 &lt; j &lt; m,
- Fj = { (B, Q U (2; U Q1r) I (B, Q) F&apos;};
- F&apos; = U {(A, Oil if -,3(2[(A, Q) E F], and F&apos; = F
otherwise;
- (2; = 0 if a0Alai ...Aj_iaj_i = E, and Q = {l} otherwise;
- Qir = 0 if ajAj+iaj+i ...Amam = 6, and Qir = {r}
otherwise.
4. Remove from P the old rules of the form A —&gt; a, where A c N,.
5. Reduce the grammar.
</listItem>
<figureCaption confidence="0.999204">
Figure 7
</figureCaption>
<bodyText confidence="0.96531775">
Subset approximation by transforming the grammar.
was generated. Similarly, r E Q indicates that something to the right was generated. If
Q = {l, r} , then we have obtained a derivation B aA,B, for some a E and
and further occurrences of B below A should be blocked in order to avoid a derivation
with self-embedding.
An example is given in Figure 8. The original grammar is implicit in the depicted
parse tree on the left, and contains at least the rules S —› A a, A b B, B --4 C, and
C —&gt; S. This grammar is self-embedding, since we have a subderivation S -4* bSa.
We explain how FB is obtained from FA in the rule AFA b BFB. We first construct
F&apos; = {(S, {r}), (A, 0)1 from FA = {(S, {ITT by adding (A, 0), since no other pair of the
form (A, Q) was already present. To the left of the occurrence of B in the original rule
A—&gt;bBwe find a nonempty string b. This means that we have to add 1 to all second
components of pairs in which gives us FB = {(S, {l, r}), (A, {/})}.
In the transformed grammar, the lower occurrence of S in the tree is tagged with
the set {(S, {1, r}), (A, {4), (B, 0), (C, 0)1. The meaning is that higher up in the spine, we
will find the nonterminals S, A, B, and C. The pair (A, {1}) indicates that since we saw
A on the spine, something to the left has been generated, namely, b. The pair (B, 0)
indicates that nothing either to the left or to the right has been generated since we
saw B. The pair (S, {I, r}) indicates that both to the left and to the right something has
been generated (namely, b on the left and a on the right). Since this indicates that an
</bodyText>
<page confidence="0.976359">
27
</page>
<figure confidence="0.9991074">
Fs = Volume 26, Number 1
FA = {(S) {r})}
FB = {(S, {1, r}), (A, {/})}
FC = {(S, fi,r1), (A, {/}), (B, 0)}
= f(S, r}), (A, {/}), (B4O),(C,0)}
Computational Linguistics
(a) S (b)
A a
b
a
</figure>
<figureCaption confidence="0.997715">
Figure 8
</figureCaption>
<bodyText confidence="0.9695920625">
A parse tree in a self-embedding grammar (a), and the corresponding parse tree in the
transformed grammar (b), for the transformation from Figure 7. For the moment we ignore
step 5 of Figure 7, i.e., reduction of the transformed grammar.
offending subderivation S —&gt;* aSi3 has been found, further completion of the parse
tree is blocked: the transformed grammar will not have any rules with left-hand side
s I (S,{1,r}),(A,{1}),(B,(1),(C,0)} . In fact, after the grammar is reduced, any parse tree that is
{(s,m),(A,{1}),(B4O),(C,0)},
constructed can no longer even contain a node labeled by S or
any nodes with labels of the form AF such that (A, {l, r}) e F.
One could generalize this approximation in such a way that not all self-embedding
is blocked, but only self-embedding occurring, say, twice in a row, in the sense of a
subderivation of the form A —4* ct1A(31 c2A/32/3l. We will not do so here, because
already for the basic case above, the transformed grammar can be huge due to the
high number of nontermirtals of the form AF that may result; the number of such
nonterminals is exponential in the size of N,.
We therefore present, in Figure 9, an alternative approximation that has a lower
complexity. By parameter d, it restricts the number of rules along a spine that may
generate something to the left and to the right. We do not, however, restrict pure left
recursion and pure right recursion. Between two occurrences of an arbitrary rule, we
allow left recursion followed by right recursion (which leads to tag r followed by tag
rl), or right recursion followed by left recursion (which leads to tag I followed by
tag 1r).
An example is given in Figure 10. As before, the rules of the grammar are implicit
in the depicted parse tree. At the top of the derivation we find S. In the transformed
grammar, we first have to apply S —&gt; ST&apos;°. The derivation starts with a rule S —&gt; A a,
which generates a string (a) to the right of a nonterminal (A). Before we can apply zero
or more of such rules, we first have to apply a unit rule STA) sr,o in the transformed
grammar. For zero or more rules that subsequently generate something on the left,
such as A —+ b B, we have to obtain a superscript containing rl, and in the example
this is done by applying Ar&apos;C&apos; A&apos;1&apos;°. Now we are finished with pure left recursion and
pure right recursion, and apply Brl&apos;° —&gt; B-L°. This allows us to apply one unconstrained
rule, which appears in the transformed grammar as Bi&apos;° --&gt; c ST&apos;l d.
</bodyText>
<page confidence="0.968717">
28
</page>
<subsectionHeader confidence="0.308324">
Nederhof Experiments with Regular Approximation
</subsectionHeader>
<bodyText confidence="0.99995425">
We are given a grammar G = (E, N,P,S). The following is to be performed for each
set N, E .1■1 such that recursive(N,) = self. The value d stands for the maximum number
of unconstrained rules along a spine, possibly alternated with a series of left-recursive
rules followed by a series of right-recursive rules, or vice versa.
</bodyText>
<listItem confidence="0.922990423076923">
1. For each A eN1, each Q E {T, 1, r, lr, rl, I}, and each f such that
0 &lt;f &lt; d, add the following nonterminals to N.
•
2. For each A E N„ add the following rule to P.
• A_4 AT,°.
3. For each A E N, and f such that 0 &lt;f &lt; d, add the following rules to P.
• AT J —&gt; .
• AT ,f Arj.
• AU -4
• Arj Arij.
• Airj A--J.
• Arij Ai&apos;f.
4. For each (A —&gt; Ba) E P such that A, B e N, and no symbols from a are
members of Ni, each f such that 0 &lt;f &lt; d, and each Q E fr,lrl, add the
following rule to P.
• A(21 —&gt; BQ1a.
5. For each (A —&gt; aB) C P such that A, B E N, and no symbols from a are
members of Nu each f such that 0 &lt;f &lt; d, and each Q C {1, rib add the
following rule to P.
• AQ1 —&gt; aBQI.
6. For each (A —&gt; a0A1a1A2 ...am-iAmam) c P such that A, Ai, ,A„, E N,
and no symbols from an,. , am are members of N„ and each f such that
0 &lt;f &lt; d, add the following rule to P. provided m = 0 V f &lt; d.
• Ai/ ctoAl-rf-Fi al . .Am am.
7. Remove from P the old rules of the form A a, where A c N,.
8. Reduce the grammar.
</listItem>
<figureCaption confidence="0.907445">
Figure 9
</figureCaption>
<bodyText confidence="0.6751294">
A simpler subset approximation by transforming the grammar.
Now the counter f has been increased from 0 at the start of the subderivation to
1 at the end. Depending on the value d that we choose, we cannot build derivations
by repeating subderivation S bcSdaan unlimited number of times: at some
point the counter will exceed d. If we choose d = 0, then already the derivation at
</bodyText>
<page confidence="0.997364">
29
</page>
<figureCaption confidence="0.939138">
Figure 10
</figureCaption>
<bodyText confidence="0.991249571428572">
A parse tree in a self-embedding grammar (a), and the corresponding parse tree in the
transformed grammar (b), for the simple subset approximation from Figure 9.
Figure 10 (b) is no longer possible, since no nonterminal in the transformed grammar
would contain 1 in its superscript.
Because of the demonstrated increase of the counter f , this transformation is guar-
anteed to remove self-embedding from the grammar. However, it is not as selective as
the transformation we saw before, in the sense that it may also block subderivations
that are not of the form A —›* ceA13. Consider for example the subderivation from
Figure 10, but replacing the lower occurrence of S by any other nonterminal C that is
mutually recursive with S, A, and B. Such a subderivation S —›* bcCda would also
be blocked by choosing d = 0. In general, increasing d allows more of such derivations
that are not of the form A —›* aA13 but also allows more derivations that are of that
form.
The reason for considering this transformation rather than any other that elim-
inates self-embedding is purely pragmatic: of the many variants we have tried that
yield nontrivial subset approximations, this transformation has the lowest complex-
ity in terms of the sizes of intermediate structures and of the resulting finite au-
tomata.
In the actual implementation, we have integrated the grammar transformation and
the construction of the finite automaton, which avoids reanalysis of the grammar to
determine the partition of mutually recursive nonterminals after transformation. This
integration makes use, for example, of the fact that for fixed Ni and fixed f, the set of
nonterminals of the form A11, with A E N, is (potentially) mutually right-recursive.
A set of such nonterminals can therefore be treated as the corresponding case from
Figure 2, assuming the value right.
The full formulation of the integrated grammar transformation and construction
of the finite automaton is rather long and is therefore not given here. A very similar
formulation, for another grammar transformation, is given in Nederhof (1998).
</bodyText>
<figure confidence="0.9430833">
Computational Linguistics Volume 26, Number 1
S-C°
sr,0
a
Ar40
B&apos;0
0
(S d
30
Nederhof Experiments with Regular Approximation
</figure>
<subsectionHeader confidence="0.887596">
4.4 Superset Approximation through Pushdown Automata
</subsectionHeader>
<bodyText confidence="0.9998886">
The distinction between context-free languages and regular languages can be seen in
terms of the distinction between pushdown automata and finite automata. Pushdown
automata maintain a stack that is potentially unbounded in height, which allows more
complex languages to be recognized than in the case of finite automata. Regular ap-
proximation can be achieved by restricting the height of the stack, as we will see in
Section 4.5, or by ignoring the distinction between several stacks when they become
too high.
More specifically, the method proposed by Pereira and Wright (1997) first con-
structs an LR automaton, which is a special case of a pushdown automaton. Then,
stacks that may be constructed in the course of recognition of a string are computed
one by one. However, stacks that contain two occurrences of a stack symbol are iden-
tified with the shorter stack that results by removing the part of the stack between the
two occurrences, including one of the two occurrences. This process defines a congru-
ence relation on stacks, with a finite number of congruence classes. This congruence
relation directly defines a finite automaton: each class is translated to a unique state of
the nondeterministic finite automaton, shift actions are translated to transitions labeled
with terminals, and reduce actions are translated to epsilon transitions.
The method has a high complexity. First, construction of an LR automaton, of
which the size is exponential in the size of the grammar, may be a prohibitively ex-
pensive task (Nederhof and Satta 1996). This is, however, only a fraction of the effort
needed to compute the congruence classes, of which the number is in turn exponen-
tial in the size of the LR automaton. If the resulting nondeterministic automaton is
determinized, we obtain a third source of exponential behavior. The time and space
complexity of the method are thereby bounded by a triple exponential function in the
size of the grammar. This theoretical analysis seems to be in keeping with the high
costs of applying this method in practice, as will be shown later in this article.
As proposed by Pereira and Wright (1997), our implementation applies the ap-
proximation separately for each nonterminal occurring in a set N, that reveals self-
embedding.
A different superset approximation based on LR automata was proposed by Baker
(1981) and rediscovered by Heckert (1994). Each individual stack symbol is now trans-
lated to one state of the nondeterministic finite automaton. It can be argued theoret-
ically that this approximation differs from the unparameterized RTN approximation
from Section 4.1 only under certain conditions that are not likely to occur very often
in practice. This consideration is confirmed by our experiments to be discussed later.
Our implementation differs from the original algorithm in that the approximation is
applied separately for each nonterminal in a set Ni that reveals self-embedding.
A generalization of this method was suggested by Bermudez and Schimpf (1990).
For a fixed number d&gt; 0 we investigate sequences of d top-most elements of stacks
that may arise in the LR automaton, and we translate these to states of the finite
automaton. More precisely, we define another congruence relation on stacks, such that
we have one congruence class for each sequence of d stack symbols and this class
contains all stacks that have that sequence as d top-most elements; we have a separate
class for each stack that contains fewer than d elements. As before, each congruence
class is translated to one state of the nondeterministic finite automaton. Note that the
case d 1 is equivalent to the approximation in Baker (1981).
If we replace the LR automaton by a certain type of automaton that performs top-
down recognition, then the method in Bermudez and Schimpf (1990) amounts to the
parameterized RTN method from Section 4.1; note that the histories from Section 4.1
in fact function as stacks, the items being the stack symbols.
</bodyText>
<page confidence="0.999413">
31
</page>
<note confidence="0.570323">
Computational Linguistics Volume 26, Number 1
</note>
<subsectionHeader confidence="0.804113">
4.5 Subset Approximation through Pushdown Automata
</subsectionHeader>
<bodyText confidence="0.999691684210526">
By restricting the height of the stack of a pushdown automaton, one obstructs recogni-
tion of a set of strings in the context-free language, and therefore a subset approxima-
tion results. This idea was proposed by Krauwer and des Tombe (1981), Langendoen
and Langsam (1987), and Pulman (1986), and was rediscovered by Black (1989) and
recently by Johnson (1998). Since the latest publication in this area is more explicit in
its presentation, we will base our treatment on this, instead of going to the historical
roots of the method.
One first constructs a modified left-corner recognizer from the grammar, in the
form of a pushdown automaton. The stack height is bounded by a low number;
Johnson (1998) claims a suitable number would be 5. The motivation for using the
left-corner strategy is that the height of the stack maintained by a left-corner parser
is already bounded by a constant in the absence of self-embedding. If the artificial
bound imposed by the approximation method is chosen to be larger than or equal to
this natural bound, then the approximation may be exact.
Our own implementation is more refined than the published algorithms mentioned
above, in that it defines a separate left-corner recognizer for each nonterminal A such
that A E Ni and recursive(N) = self, some i. In the construction of one such recognizer,
nonterminals that do not belong to Ni are treated as terminals, as in all other methods
discussed here.
</bodyText>
<subsectionHeader confidence="0.956149">
4.6 Superset Approximation by N-grams
</subsectionHeader>
<bodyText confidence="0.977029846153846">
An approximation from Seyfarth and Bermudez (1995) can be explained as follows.
Define the set of all terminals reachable from nonterminal A to be EA = {a a,[3[A —4*
aa[3]}. We now approximate the set of strings derivable from A by EA, which is the
set of strings consisting of terminals from EA. Our implementation is made slightly
more sophisticated by taking EA to be {X I 3B, a, /3[B E N, A B aX0 AX N]}, for
each A such that A E N, and recursive(N) = self, for some i. That is, each X E EA is
a terminal, or a nonterminal not in the same set Ni as A, but immediately reachable
from set N„ through B E N.
This method can be generalized, inspired by Stolcke and Segal (1994), who derive
N-gram probabilities from stochastic context-free grammars. By ignoring the probabil-
ities, each N = 1, 2, 3, ... gives rise to a superset approximation that can be described
as follows: The set of strings derivable from a nonterminal A is approximated by the
set of strings al ... an such that
</bodyText>
<listItem confidence="0.9961574">
• for each substring v = ai±i ai+N (0 &lt; i &lt; n — N) we have A —&gt;* wvy, for
some w and y,
• for each prefix v =- al ... ai (0 &lt; &lt; n) such that i &lt; N we have A vy,
for some y, and
• for each suffix v = afro. . an (0 &lt; i &lt; n) such that n — i &lt; N we have
</listItem>
<bodyText confidence="0.9386805">
A —›* wv, for some w.
(Again, the algorithms that we actually implemented are more refined and take into
account the sets 1\11.)
The approximation from Seyfarth and Bermudez (1995) can be seen as the case N =
1, which will henceforth be called the unigram method. We have also experimented
with the cases N = 2 and N = 3, which will be called the bigram and trigram methods.
</bodyText>
<page confidence="0.990104">
32
</page>
<table confidence="0.330242">
Nederhof Experiments with Regular Approximation
</table>
<sectionHeader confidence="0.807863" genericHeader="method">
5. Increasing the Precision
</sectionHeader>
<bodyText confidence="0.999783266666667">
The methods of approximation described above take as input the parts of the grammar
that pertain to self-embedding. It is only for those parts that the language is affected.
This leads us to a way to increase the precision: before applying any of the above
methods of regular approximation, we first transform the grammar.
This grammar transformation copies grammar rules containing recursive nonter-
minals and, in the copies, it replaces these nonterminals by new nonrecursive nonter-
minals. The new rules take over part of the roles of the old rules, but since the new
rules do not contain recursion and therefore do not pertain to self-embedding, they
remain unaffected by the approximation process.
Consider for example the palindrome grammar from Figure 1. The RTN method
will yield a rather crude approximation, namely, the language {a, b}*. We transform
this grammar in order to keep the approximation process away from the first three
levels of recursion. We achieve this by introducing three new nonterminals S[1], S[2]
and S[3], and by adding modified copies of the original grammar rules, so that we
obtain:
</bodyText>
<equation confidence="0.9948075">
S[1] —&gt; a S[2] a IbS [2] b I c
S[2] —&gt; a S[3] a IbS [3] b I c
S[3] —&gt; aSa I 13Sb c
S —&gt; aSa I bSb I c
</equation>
<bodyText confidence="0.99741675">
The new start symbol is S[1].
The new grammar generates the same language as before, but the approximation
process leaves unaffected the nonterminals S[1], S[2], and S[3] and the rules defining
them, since these nonterminals are not recursive. These nonterminals amount to the
upper three levels of the parse trees, and therefore the effect of the approximation
on the language is limited to lower levels. If we apply the RTN method then we
obtain the language that consists of (grammatical) palindromes of the form une, where
w E {E, a, b} U {a, b}2 U {a, b}3, plus (possibly ungrammatical) strings of the form wywR,
where w E {a, b}3 and v E {a, b}*. (wR indicates the mirror image of w.)
The grammar transformation in its full generality is given by the following, which
is to be applied for fixed integer j &gt; 0, which is a parameter of the transformation,
and for each Ni such that recursive(N) = self.
</bodyText>
<equation confidence="0.903887">
For each nonterminal A E N, we introduce j new nonterminals A[1], , A [j]. For
each A —&gt; ... Xn, in P such that A E and h such that 1 h &lt; j, we add
A[h] —&gt; Xn, to P. where for 1 &lt;k &lt; m:
= Xk[h + 1] if Xk E N, h &lt; j
= Xk otherwise
</equation>
<bodyText confidence="0.82164">
Further, we replace all rules A —&gt; Xi . Xn, such that A V N, by A —&gt; Xn,&apos; , where
for 1 &lt;k &lt; m:
Xc, = X[1] if Xk E
= Xk otherwise
If the start symbol S was in Nu we let S[1] be the new start symbol.
A second transformation, which shares some characteristics with the one above,
was presented in Nederhof (1997). One of the earliest papers suggesting such transfor-
mations as a way to increase the precision of approximation is due to ulik and Cohen
(1973), who only discuss examples, however; no general algorithms were defined.
</bodyText>
<page confidence="0.992482">
33
</page>
<figure confidence="0.998076962962963">
Computational Linguistics Volume 26, Number 1
50 100 150 200 250 300 350
corpus size (# sentences)
5 10 15 20 25 30
length (# words)
180
160
140
120
100
80
60
40
20
0 o
grammar size (# rules)
550
500
450
400
350
300
250
200
150
100
50
</figure>
<figureCaption confidence="0.99599">
Figure 11
</figureCaption>
<bodyText confidence="0.991488666666667">
The test material. The left-hand curve refers to the construction of the grammar from 332
sentences, the right-hand curve refers to the corpus of 1,000 sentences used as input to the
finite automata.
</bodyText>
<sectionHeader confidence="0.950347" genericHeader="method">
6. Empirical Results
</sectionHeader>
<bodyText confidence="0.99802434375">
In this section we investigate empirically how the respective approximation methods
behave on grammars of different sizes and how much the approximated languages
differ from the original context-free languages. This last question is difficult to answer
precisely. Both an original context-free language and an approximating regular lan-
guage generally consist of an infinite number of strings, and the number of strings
that are introduced in a superset approximation or that are excluded in a subset ap-
proximation may also be infinite. This makes it difficult to attach numbers to the
&amp;quot;quality&amp;quot; of approximations.
We have opted for a pragmatic approach, which does not require investigation of
the entire infinite languages of the grammar and the finite automata, but looks at a
certain finite set of strings taken from a corpus, as discussed below. For this finite set
of strings, we measure the percentage that overlaps with the investigated languages.
For the experiments, we took context-free grammars for German, generated auto-
matically from an HPSG and a spoken-language corpus of 332 sentences. This corpus
consists of sentences possessing grammatical phenomena of interest, manually selected
from a larger corpus of actual dialogues. An HPSG parser was applied on these sen-
tences, and a form of context-free backbone was selected from the first derivation that
was found. (To take the first derivation is as good as any other strategy, given that we
have at present no mechanisms for relative ranking of derivations.) The label occur-
ring at a node together with the sequence of labels at the daughter nodes was then
taken to be a context-free rule. The collection of such rules for the complete corpus
forms a context-free grammar. Due to the incremental nature of this construction of
the grammar, we can consider the subgrammars obtained after processing the first p
sentences, where p = 1, 2, 3, ... ,332. See Figure 11 (left) for the relation between p and
the number of rules of the grammar. The construction is such that rules have at most
two members in the right-hand side.
As input, we considered a set of 1,000 sentences, obtained independently from the
332 sentences mentioned above. These 1,000 sentences were found by having a speech
recognizer provide a single hypothesis for each utterance, where utterances come from
actual dialogues. Figure 11 (right) shows how many sentences of different lengths the
corpus contains, up to length 30. Above length 25, this number quickly declines, but
still a fair quantity of longer strings can be found, e.g., 11 strings of a length between
</bodyText>
<page confidence="0.993313">
34
</page>
<bodyText confidence="0.984802653846154">
Nederhof Experiments with Regular Approximation
51 and 60 words. In most cases however such long strings are in fact composed of a
number of shorter sentences.
Each of the 1,000 sentences were input in their entirety to the automata, although
in practical spoken-language systems, often one is not interested in the grammaticality
of complete utterances, but tries to find substrings that form certain phrases bearing
information relevant to the understanding of the utterance. We will not be concerned
here with the exact way such recognition of substrings could be realized by means of
finite automata, since this is outside the scope of this paper.
For the respective methods of approximation, we measured the size of the com-
pact representation of the nondeterministic automaton, the number of states and the
number of transitions of the minimal deterministic automaton, and the percentage
of sentences that were recognized, in comparison to the percentage of grammatical
sentences. For the compact representation, we counted the number of lines, which is
roughly the sum of the numbers of transitions from all subautomata, not considering
about three additional lines per subautomaton for overhead.
We investigated the size of the compact representation because it is reasonably
implementation independent, barring optimizations of the approximation algorithms
themselves that affect the sizes of the subautomata. For some methods, we show that
there is a sharp increase in the size of the compact representation for a small increase
in the size of the grammar, which gives us a strong indication of how difficult it
would be to apply the method to much larger grammars. Note that the size of the
compact representation is a (very) rough indication of how much effort is involved in
determinization, minimization, and substitution of the subautomata into each other.
For determinization and minimization of automata, we have applied programs from
the FSM library described in Mohri, Pereira, and Riley (1998). This library is considered
to be competitive with respect to other tools for processing of finite-state machines.
When these programs cannot determinize or minimize in reasonable time and space
some subautomata constructed by a particular method of approximation, then this can
be regarded as an indication of the impracticality of the method.
We were not able to compute the compact representation for all the methods
and all the grammars. The refined RTN approximation from Section 4.2 proved to be
quite problematic. We were not able to compute the compact representation for any
of the automatically obtained grammars in our collection that were self-embedding.
We therefore eliminated individual rules by hand, starting from the smallest self-
embedding grammar in our collection, eventually finding grammars small enough to
be handled by this method. The results are given in Table 1. Note that the size of the
compact representation increases significantly for each additional grammar rule. The
sizes of the finite automata, after determinization and minimization, remain relatively
small.
Also problematic was the first approximation from Section 4.4, which was based
on LR parsing following Pereira and Wright (1997). Even for the grammar of 50 rules,
we were not able to determinize and minimize one of the subautomata according
to step 1 of Section 3: we stopped the process after it had reached a size of over 600
megabytes. Results, as far as we could obtain them, are given in Table 2. Note the sharp
increases in the size of the compact representation, resulting from small increases, from
44 to 47 and from 47 to 50, in the number of rules, and note an accompanying sharp
increase in the size of the finite automaton. For this method, we see no possibility
of accomplishing the complete approximation process, including determinization and
minimization, for grammars in our collection that are substantially larger than 50 rules.
Since no grammars of interest could be handled by them, the above two methods
will be left out of further consideration.
</bodyText>
<page confidence="0.997467">
35
</page>
<note confidence="0.532877">
Computational Linguistics Volume 26, Number 1
</note>
<tableCaption confidence="0.880189333333333">
Table 1
Size of the compact representation and number of states and transitions,
for the refined RTN approximation (Grimley-Evans 1997).
</tableCaption>
<table confidence="0.999914">
Grammar Size Compact Representation # of States # of Transitions
10 133 11 14
12 427 17 26
13 1,139 17 34
14 4,895 17 36
15 16,297 17 40
16 51,493 19 52
17 208,350 19 52
18 409,348 21 59
19 1,326,256 21 61
</table>
<tableCaption confidence="0.998887">
Table 2
</tableCaption>
<bodyText confidence="0.745375333333333">
Size of the compact representation and number of states and transitions,
for the superset approximation based on LR automata following Pereira
and Wright (1997).
</bodyText>
<table confidence="0.9975678">
Grammar Size Compact Representation # of States # of Transitions
35 15,921 350 2,125
44 24,651 499 4,352
47 151,226 5,112 35,754
50 646,419 ? ?
</table>
<bodyText confidence="0.99994428">
Below, we refer to the unparameterized and parameterized approximations based
on RTNs (Section 4.1) as RTN and RTNd, respectively, for d = 2,3; to the subset
approximation from Figure 9 as Subd, for d -= 1, 2, 3; and to the second and third
methods from Section 4.4, which were based on LR parsing following Baker (1981)
and Bermudez and Schimpf (1990), as LR and LRd, respectively, for d = 2,3. We refer
to the subset approximation based on left-corner parsing from Section 4.5 as LCd, for
the maximal stack height of d = 2, 3, 4; and to the methods discussed in Section 4.6 as
Unigram, Bigram, and Trigram.
We first discuss the compact representation of the nondeterministic automata. In
Figure 12 we use two different scales to be able to represent the large variety of values.
For the method Subd, the compact representation is of purely theoretical interest for
grammars larger than 156 rules in the case of Subl, for those larger than 62 rules
in the case of Sub2, and for those larger than 35 rules in the case of Sub3, since
the minimal deterministic automata could thereafter no longer be computed with a
reasonable bound on resources; we stopped the processes after they had consumed
over 400 megabytes. For LC3, LC4, RTN3, LR2, and LR3, this was also the case for
grammars larger than 139, 62, 156, 217, and 156 rules, respectively. The sizes of the
compact representation seem to grow moderately for LR and Bigram, in the upper
panel, yet the sizes are much larger than those for RTN and Unigram, which are
indicated in the lower panel.
The numbers of states for the respective methods are given in Figure 13, again
using two very different scales. As in the case of the grammars, the terminals of our
finite automata are parts of speech rather than words. This means that in general there
will be nondeterminism during application of an automaton on an input sentence due
to lexical ambiguity. This nondeterminism can be handled efficiently using tabular
</bodyText>
<page confidence="0.985871">
36
</page>
<figure confidence="0.999451466666667">
Nederhof Experiments with Regular Approximation
compact repr size
700000
600000
500000
400000
300000
200000
100000
50 100 150 200 250 300 350 400 450 500 550
grammar size
LC4 -9-
LR3
RTN3
LC3
LR2
Trigram
LC2
RTN2
LR
Bigram
9
20000
15000
0
Ni
(/3
a)
10000
as
0.
0
5000
0
RTN2
LC2 -0—
LR
Bigram
Sub3 -x
Sub2 A
Sub1 -
RTN
Unigram
0 50 100 150 200 250 300 350 400 450 500 550
grammar size
</figure>
<figureCaption confidence="0.953612">
Figure 12
</figureCaption>
<bodyText confidence="0.8628925">
Size of the compact representation.
techniques, provided the number of states is not too high. This consideration favors
methods that produce low numbers of states, such as Trigram, LR, RTN, Bigram, and
Unigram.
</bodyText>
<page confidence="0.99461">
37
</page>
<figure confidence="0.996312928571429">
Computational Linguistics Volume 26, Number 1
Sub2 A
LC4 -e—
Subl * -
LC3 4K—
RT N3
LC2 -0— -
10000
9000
8000
7000
6000
rJ
1g 5000
4000
3000
2000
1000
. 0
200
600
100
300 400 500
grammar size
i&apos;&apos;&apos;&apos;&apos;&apos;°-----e-----..-----e
I 0 0 I
100 200 300 400 500 600
grammar size
</figure>
<figureCaption confidence="0.957091">
Figure 13
</figureCaption>
<bodyText confidence="0.9201205">
Number of states of the determinized and minimized automata.
Note that the numbers of states for LR and RTN differ very little. In fact, for
some of the smallest and for some of the largest grammars in our collection, the
resulting automata were identical. Note, however, that the intermediate results for LR
</bodyText>
<figure confidence="0.991721611111111">
LC3
RTN3
LR3
LC2
RTN2
Trigram
LR2
LR
RTN
Bigram
Unigram
la 0
0 ----------- --
100
80
60
40
20
</figure>
<page confidence="0.915108">
38
</page>
<subsectionHeader confidence="0.334674">
Nederhof Experiments with Regular Approximation
</subsectionHeader>
<bodyText confidence="0.998726098039216">
(Figure 12) are much larger. It should therefore be concluded that the &amp;quot;sophistication&amp;quot;
of LR parsing is here merely an avoidable source of inefficiency.
The numbers of transitions for the respective methods are given in Figure 14.
Again, note the different scales used in the two panels. The numbers of transitions
roughly correspond to the storage requirements for the automata. It can be seen that,
again, Trigram, LR, RTN, Bigram, and Unigram perform well.
The precision of the respective approximations is measured in terms of the per-
centage of sentences in the corpus that are recognized by the automata, in comparison
to the percentage of sentences that are generated by the grammar, as presented by Fig-
ure 15. The lower panel represents an enlargement of a section from the upper panel.
Methods that could only be applied for the smaller grammars are only presented in
the lower panel; LC4 and Sub2 have been omitted entirely.
The curve labeled G represents the percentage of sentences generated by the gram-
mar. Note that since all approximation methods compute either supersets or subsets, a
particular automaton cannot both recognize some ungrammatical sentences and reject
some grammatical sentences.
Unigram and Bigram recognize very high percentages of ungrammatical sentences.
Much better results were obtained for RTN. The curve for LR would not be distin-
guishable from that for RTN in the figure, and is therefore omitted. (For only two of
the investigated grammars was there any difference, the largest difference occurring
for grammar size 217, where 34.1 versus 34.5 percent of sentences were recognized
in the cases of LR and RTN, respectively.) Trigram remains very close to RTN (and
LR); for some grammars a lower percentage is recognized, for others a higher per-
centage is recognized. LR2 seems to improve slightly over RTN and Trigram, but data
is available only for small grammars, due to the difficulty of applying the method to
larger grammars. A more substantial improvement is found for R&apos;TN2. Even smaller
percentages are recognized by LR3 and RTN3, but again, data is available only for
small grammars.
The subset approximations LC3 and Sub1 remain very close to G, but here again
only data for small grammars is available, since these two methods could not be
applied on larger grammars. Although application of LC2 on larger grammars required
relatively few resources, the approximation is very crude: only a small percentage of
the grammatical sentences are recognized.
We also performed experiments with the grammar transformation from Section 5,
in combination with the RTN method. We found that for increasing j, the interme-
diate automata soon became too large to be determinized and minimized, with a
bound on the memory consumption of 400 megabytes. The sizes of the automata that
we were able to compute are given in Figure 16. RTN+j, for j = 1, 2, 3, 4, 5, repre-
sents the (unparameterized) RTN method in combination with the grammar transfor-
mation with parameter j. This is not to be confused with the parameterized RTNd
method.
Figure 17 indicates the number of sentences in the corpus that are recognized by
an automaton divided by the number of sentences in the corpus that are generated
by the grammar. For comparison, the figure also includes curves for RTNd, where
d = 2,3 (cf. Figure 15). We see that j -= 1,2 has little effect. For j =- 3, 4, 5, however,
the approximating language becomes substantially smaller than that in the case of
RTN, but at the expense of large automata. In particular, if we compare the sizes of
the automata for RTN+j in Figure 16 with those for RTNd in Figures 13 and 14, then
Figure 17 suggests the large sizes of the automata for RTN+j are not compensated
adequately by a reduction of the percentage of sentences that are recognized. RTNd
seems therefore preferable to RTN+j.
</bodyText>
<page confidence="0.992308">
39
</page>
<figure confidence="0.999683613636364">
Volume 26, Number 1
Computational Linguistics
90000
80000 -
70000 -
60000 -
co
g 50000 -
Co
Cs
— 40000 -
*
30000 -
20000 -
10000 -
100 200 300 400 500 600
grammar size
Sub2
LC4 -e—
Su b 1 _
LC3
LC2 -e—
RTN2
LC3
RTN3
LR3 --
LC2 -e—
RTN2 -e—
Trigram
L --H -
RTN -e—
Bigram -9 --
Unigram
5000
4000
2 3000
0
Co
Is
2000
1000
0
0 100 200 300 400 500 600
grammar size
</figure>
<figureCaption confidence="0.947216">
Figure 14
</figureCaption>
<bodyText confidence="0.720115">
Number of transitions of the determinized and minimized automata.
</bodyText>
<sectionHeader confidence="0.896533" genericHeader="conclusions">
7. Conclusions
</sectionHeader>
<bodyText confidence="0.9830355">
If we apply the finite automata with the intention of filtering out incorrect sentences,
for example from the output from a speech recognizer, then it is allowed that a
</bodyText>
<page confidence="0.996789">
40
</page>
<figureCaption confidence="0.977144">
Figure 15
</figureCaption>
<bodyText confidence="0.9855282">
Percentage of sentences that are recognized.
certain percentage of ungrammatical input is recognized. Recognizing ungrammat-
ical input merely makes filtering less effective; it does not affect the functionality
of the system as a whole, provided we assume that the grammar specifies exactly
the set of sentences that can be successfully handled by a subsequent phase of pro-
</bodyText>
<figure confidence="0.998941813953488">
Experiments with Regular Approximation
Nederhof
100
200
300
grammar size
400
500
600
40 60 80 100 120
grammar size
140
160
100
80
00
112
40
20
Bigram
/ RTN(LR)
Trigram
LR2
RTN2
LR3
RTN3
LC3
Sub1
LC2
6
5
2
a)
0
92 3
4
Unigram -0- -
Bigram -0-
,0-- Trigram
RTN(LR) -9—
RTN2 -A—
G -
LC2 -0—
</figure>
<page confidence="0.507223">
41
</page>
<figure confidence="0.999423962962963">
Computational Linguistics Volume 26, Number 1
2 6000
as
10000
4000
2000
8000
0 F-
0 50 100 150 200 250 300 350 400 450 500 550 0 50 100 150 200 250 300 350 400 450 500 550
grammar size grammar size
RTN+5 --6.--
RTN-F4 -0 -
RTN+3 -+ --
RTN+2 -0--
RTN+1
;=). 150000
`)
100000
50000
250000
200000
I I 1111
RTN+5 -A - -
RTN+4 -0-- -
RTN+3 -+ - -
RTN+2 -
RTN+1
</figure>
<figureCaption confidence="0.948072333333333">
Figure 16
Number of states and number of transitions of the determinized and minimized automata.
Figure 17
</figureCaption>
<subsectionHeader confidence="0.444626">
Number of recognized sentences divided by number of grammatical sentences.
</subsectionHeader>
<bodyText confidence="0.997597142857143">
cessing. Also allowed is that &amp;quot;pathological&amp;quot; grammatical sentences are rejected that
seldom occur in practice; an example are sentences requiring multiple levels of self-
embedding.
Of the methods we considered that may lead to rejection of grammatical sen-
tences, i.e., the subset approximations, none seems of much practical value. The most
serious problem is the complexity of the construction of automata from the compact
representation for large grammars. Since the tools we used for obtaining the minimal
</bodyText>
<figure confidence="0.987214523809524">
50
100
150
300
350
400
200 250
grammar size
1.6
RTN -e—
RT N2 -4---
RTN3
RTN+1 -N-
RTN+2 - -
RTN+3 -+ --
RTN+4 - 0 --
RTN+5 - A
1.5
1.4
1.2
1.1
</figure>
<page confidence="0.857459">
42
</page>
<bodyText confidence="0.962052666666667">
Nederhof Experiments with Regular Approximation
deterministic automata are considered to be of high quality, it seems unlikely that
alternative implementations could succeed on much larger grammars, especially con-
sidering the sharp increases in the sizes of the automata for small increases in the size
of the grammar. Only LC2 could be applied with relatively few resources, but this is a
very crude approximation, which leads to rejection of many more sentences than just
those requiring self-embedding.
Similarly, some of the superset approximations are not applicable to large gram-
mars because of the high costs of obtaining the minimal deterministic automata. Some
others provide rather large languages, and therefore do not allow very effective fil-
tering of ungrammatical input. One method, however, seems to be excellently suited
for large grammars, namely, the RTN method, considering both the unparameterized
version and the parameterized version with d = 2. In both cases, the size of the au-
tomaton grows moderately in the grammar size. For the unparameterized version, the
compact representation also grows moderately. Furthermore, the percentage of recog-
nized sentences remains close to the percentage of grammatical sentences. It seems
therefore that, under the conditions of our experiments, this method is the most suit-
able regular approximation that is presently available.
</bodyText>
<sectionHeader confidence="0.99148" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999689571428571">
This paper could not have been written
without the wonderful help of Hans-Ulrich
Krieger, who created the series of grammars
that are used in the experiments. I also owe
to him many thanks for countless
discussions and for allowing me to pursue
this work. I am very grateful to the
anonymous referees for their inspiring
suggestions.
This work was funded by the German
Federal Ministry of Education, Science,
Research and Technology (BMBF) in the
framework of the VERBMOBIL Project under
Grant 01 IV 701 VO.
</bodyText>
<sectionHeader confidence="0.995529" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999599333333334">
Baker, Theodore P. 1981. Extending
lookahead for LR parsers. Journal of
Computer and System Sciences, 22:243-259.
Bermudez, Manuel E. and Karl M. Schimpf.
1990. Practical arbitrary lookahead LR
parsing. Journal of Computer and System
Sciences, 41:230-250.
Berstel, Jean. 1979. Transductions and
Context-Free Languages. B. G. Teubner,
Stuttgart.
Black, Alan W. 1989. Finite state machines
from feature grammars. In International
Workshop on Parsing Technologies, pages
277-285, Pittsburgh, PA.
Chomsky, Noam. 1959a. A note on phrase
structure grammars. Information and
Control, 2:393-395.
Chomsky, Noam. 1959b. On certain formal
properties of grammars. Information and
Control, 2:137-167.
Culik, Karel II and Rina Cohen. 1973.
LR-regular grammars—An extension of
LR(k) grammars. Journal of Computer and
System Sciences, 7:66-96.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. Communications of the
ACM, 13(2):94-102, February.
Grimley-Evans, Edmund. 1997.
Approximating context-free grammars
with a finite-state calculus. In Proceedings
of the 35th Annual Meeting of the Association
for Computational Linguistics an 8th
Conference of the European Chapter of the
Association for Computational Linguistics,
pages 452-459, Madrid, Spain.
Harrison, Michael A. 1978. Introduction to
Formal Language Theory. Addison-Wesley.
Heckert, Erik. 1994. Behandlung von
Syntaxfehlern far LR-Sprachen ohne
Korrekturversuche. Ph.D. thesis,
Ruhr-Universitat Bochum.
Johnson, Mark. 1998. Finite-state
approximation of constraint-based
grammars using left-corner grammar
transforms. In COLING-ACL &apos;98: 36th
Annual Meeting of the Association for
Computational Linguistics and 17th
International Conference on Computational
Linguistics, volume 1, pages 619-623,
Montreal, Quebec, Canada.
Krauwer, Steven and Louis des Tombe. 1981.
Transducers and grammars as theories of
language. Theoretical Linguistics, 8:173-202.
Langendoen, D. Terence and Yedidyah
Langsam. 1987. On the design of finite
transducers for parsing phrase-structure
languages. In Alexis Manaster-Ramer,
editor, Mathematics of Language. John
Benjamins, Amsterdam, pages 191-235.
Mohri, Mehryar and Fernando C. N.
</reference>
<page confidence="0.99104">
43
</page>
<note confidence="0.370022">
Computational Linguistics Volume 26, Number 1
</note>
<reference confidence="0.999481188405797">
Pereira. 1998. Dynamic compilation of
weighted context-free grammars. In
COLING-ACL &apos;98: 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, volume 2, pages
891-897, Montreal, Quebec, Canada.
Mohri, Mehryar, Fernando C. N. Pereira,
and Michael Riley. 1998. A rational design
for a weighted finite-state transducer
library. In Derick Wood and Sheng Yu,
editors, Automata Implementation. Lecture
Notes in Computer Science, Number 1436.
Springer Verlag, pages 144-158.
Nederhof, Mark-Jan. 1994. Linguistic Parsing
and Program Transformations. Ph.D. thesis,
University of Nijmegen.
Nederhof, Mark-Jan. 1997. Regular
approximations of CFLs: A grammatical
view. In Proceedings of the International
Workshop on Parsing Technologies,
pages 159-170, Massachusetts Institute of
Technology.
Nederhof, Mark-Jan. 1998. Context-free
parsing through regular approximation.
In Proceedings of the International Workshop
on Finite State Methods in Natural Language
Processing, pages 13-24, Ankara, Turkey.
Nederhof, Mark-Jan and Giorgio Satta. 1996.
Efficient tabular LR parsing. In Proceedings
of the 34th Annual Meeting, pages 239-246,
Santa Cruz, CA. Association for
Computational Linguistics.
Pereira, Fernando C. N. and Rebecca N.
Wright. 1997. Finite-state approximation
of phrase-structure grammars. In
Emmanuel Roche and Yves Schabes,
editors, Finite-State Language Processing.
MIT Press, pages 149-173.
Pulman, S. G. 1986. Grammars, parsers, and
memory limitations. Language and
Cognitive Processes, 1(3):197-225.
Purdom, Paul Walton, Jr. and Cynthia A.
Brown. 1981. Parsing extended LR(k)
grammars. Acta Infirmatica, 15:115-127.
Resnik, Philip. 1992. Left-corner parsing and
psychological plausibility. In COLING &apos;92:
Papers presented to the Fifteenth [sic]
International Conference on Computational
Linguistics, pages 191-197, Nantes, France.
Rosenkrantz, D. J. and P. M. Lewis, II. 1970.
Deterministic left comer parsing. In IEEE
Conference Record of the 11th Annual
Symposium on Switching and Automata
Theory, pages 139-152.
Seyfarth, Benjamin R. and Manuel E.
Bermudez. 1995. Suffix languages in LR
parsing. International Journal of Computer
Mathematics, 55:135-153.
Stolcke, Andreas and Jonathan Segal. 1994.
Precise N-gram probabilities from
stochastic context-free grammars. In
Proceedings of the 32nd Annual Meeting,
pages 74-79, Las Cruces, NM. Association
for Computational Linguistics.
Woods, W. A. 1970. Transition network
grammars for natural language analysis.
Communications of the ACM,
13(10):591-606.
</reference>
<page confidence="0.999234">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.516385">
<title confidence="0.897241">Practical Experiments with Regular Approximation of Context-Free Languages Mark-Jan Nederhof* German Research Center for Artificial</title>
<abstract confidence="0.953204">Intelligence Several methods are discussed that construct a finite automaton given a context-free grammar, including both methods that lead to subsets and those that lead to supersets of the original context-free language. Some of these methods of regular approximation are new, and some others are presented here in a more refined form with respect to existing literature. Practical experiments with the different methods of regular approximation are performed for spoken-language input: hypotheses from a speech recognizer are filtered through a finite automaton.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Theodore P Baker</author>
</authors>
<title>Extending lookahead for LR parsers.</title>
<date>1981</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>22--243</pages>
<contexts>
<context position="37108" citStr="Baker (1981)" startWordPosition="6795" endWordPosition="6796">erministic automaton is determinized, we obtain a third source of exponential behavior. The time and space complexity of the method are thereby bounded by a triple exponential function in the size of the grammar. This theoretical analysis seems to be in keeping with the high costs of applying this method in practice, as will be shown later in this article. As proposed by Pereira and Wright (1997), our implementation applies the approximation separately for each nonterminal occurring in a set N, that reveals selfembedding. A different superset approximation based on LR automata was proposed by Baker (1981) and rediscovered by Heckert (1994). Each individual stack symbol is now translated to one state of the nondeterministic finite automaton. It can be argued theoretically that this approximation differs from the unparameterized RTN approximation from Section 4.1 only under certain conditions that are not likely to occur very often in practice. This consideration is confirmed by our experiments to be discussed later. Our implementation differs from the original algorithm in that the approximation is applied separately for each nonterminal in a set Ni that reveals self-embedding. A generalization</context>
<context position="38422" citStr="Baker (1981)" startWordPosition="7007" endWordPosition="7008">equences of d top-most elements of stacks that may arise in the LR automaton, and we translate these to states of the finite automaton. More precisely, we define another congruence relation on stacks, such that we have one congruence class for each sequence of d stack symbols and this class contains all stacks that have that sequence as d top-most elements; we have a separate class for each stack that contains fewer than d elements. As before, each congruence class is translated to one state of the nondeterministic finite automaton. Note that the case d 1 is equivalent to the approximation in Baker (1981). If we replace the LR automaton by a certain type of automaton that performs topdown recognition, then the method in Bermudez and Schimpf (1990) amounts to the parameterized RTN method from Section 4.1; note that the histories from Section 4.1 in fact function as stacks, the items being the stack symbols. 31 Computational Linguistics Volume 26, Number 1 4.5 Subset Approximation through Pushdown Automata By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results. This i</context>
<context position="53061" citStr="Baker (1981)" startWordPosition="9537" endWordPosition="9538">mpact representation and number of states and transitions, for the superset approximation based on LR automata following Pereira and Wright (1997). Grammar Size Compact Representation # of States # of Transitions 35 15,921 350 2,125 44 24,651 499 4,352 47 151,226 5,112 35,754 50 646,419 ? ? Below, we refer to the unparameterized and parameterized approximations based on RTNs (Section 4.1) as RTN and RTNd, respectively, for d = 2,3; to the subset approximation from Figure 9 as Subd, for d -= 1, 2, 3; and to the second and third methods from Section 4.4, which were based on LR parsing following Baker (1981) and Bermudez and Schimpf (1990), as LR and LRd, respectively, for d = 2,3. We refer to the subset approximation based on left-corner parsing from Section 4.5 as LCd, for the maximal stack height of d = 2, 3, 4; and to the methods discussed in Section 4.6 as Unigram, Bigram, and Trigram. We first discuss the compact representation of the nondeterministic automata. In Figure 12 we use two different scales to be able to represent the large variety of values. For the method Subd, the compact representation is of purely theoretical interest for grammars larger than 156 rules in the case of Subl, f</context>
</contexts>
<marker>Baker, 1981</marker>
<rawString>Baker, Theodore P. 1981. Extending lookahead for LR parsers. Journal of Computer and System Sciences, 22:243-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel E Bermudez</author>
<author>Karl M Schimpf</author>
</authors>
<title>Practical arbitrary lookahead LR parsing.</title>
<date>1990</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>41--230</pages>
<contexts>
<context position="37768" citStr="Bermudez and Schimpf (1990)" startWordPosition="6893" endWordPosition="6896">4). Each individual stack symbol is now translated to one state of the nondeterministic finite automaton. It can be argued theoretically that this approximation differs from the unparameterized RTN approximation from Section 4.1 only under certain conditions that are not likely to occur very often in practice. This consideration is confirmed by our experiments to be discussed later. Our implementation differs from the original algorithm in that the approximation is applied separately for each nonterminal in a set Ni that reveals self-embedding. A generalization of this method was suggested by Bermudez and Schimpf (1990). For a fixed number d&gt; 0 we investigate sequences of d top-most elements of stacks that may arise in the LR automaton, and we translate these to states of the finite automaton. More precisely, we define another congruence relation on stacks, such that we have one congruence class for each sequence of d stack symbols and this class contains all stacks that have that sequence as d top-most elements; we have a separate class for each stack that contains fewer than d elements. As before, each congruence class is translated to one state of the nondeterministic finite automaton. Note that the case </context>
<context position="53093" citStr="Bermudez and Schimpf (1990)" startWordPosition="9540" endWordPosition="9543">ion and number of states and transitions, for the superset approximation based on LR automata following Pereira and Wright (1997). Grammar Size Compact Representation # of States # of Transitions 35 15,921 350 2,125 44 24,651 499 4,352 47 151,226 5,112 35,754 50 646,419 ? ? Below, we refer to the unparameterized and parameterized approximations based on RTNs (Section 4.1) as RTN and RTNd, respectively, for d = 2,3; to the subset approximation from Figure 9 as Subd, for d -= 1, 2, 3; and to the second and third methods from Section 4.4, which were based on LR parsing following Baker (1981) and Bermudez and Schimpf (1990), as LR and LRd, respectively, for d = 2,3. We refer to the subset approximation based on left-corner parsing from Section 4.5 as LCd, for the maximal stack height of d = 2, 3, 4; and to the methods discussed in Section 4.6 as Unigram, Bigram, and Trigram. We first discuss the compact representation of the nondeterministic automata. In Figure 12 we use two different scales to be able to represent the large variety of values. For the method Subd, the compact representation is of purely theoretical interest for grammars larger than 156 rules in the case of Subl, for those larger than 62 rules in</context>
</contexts>
<marker>Bermudez, Schimpf, 1990</marker>
<rawString>Bermudez, Manuel E. and Karl M. Schimpf. 1990. Practical arbitrary lookahead LR parsing. Journal of Computer and System Sciences, 41:230-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Berstel</author>
</authors>
<date>1979</date>
<journal>Transductions and Context-Free Languages. B. G. Teubner,</journal>
<location>Stuttgart.</location>
<contexts>
<context position="11215" citStr="Berstel (1979)" startWordPosition="1951" endWordPosition="1952">s two subautomata, labeled S and B. The one labeled S is the automaton on the top level, and contains two transitions labeled B, which refer to the other subautomaton. Note that this representation is more compact than that of Figure 3, since the transitions that are involved in representing the sublanguage of strings generated by nonterminal B are included only once. The compact representation consisting of subautomata can be turned into a single finite automaton by substituting subautomata A for transitions labeled A in other automata. This comes down to regular substitution in the sense of Berstel (1979). The advantage of this way of obtaining a finite automaton over a direct construction of a nondeterministic automaton is that subautomata may be determinized and minimized before they are substituted into larger subautomata. Since in many cases determinized and minimized automata are much smaller, this process avoids much of the combina20 Nederhof Experiments with Regular Approximation let K =- 0, A = 0, s fresh_state, f = fresh_state, F = {f}; make_fa(s, S, f). procedure make_fa(q0, a, q1): if a = e then let A = A U {(q0,e, qi)} elseif a = a, some a E E then let A = A U {(q0, a, qi)} elseif </context>
</contexts>
<marker>Berstel, 1979</marker>
<rawString>Berstel, Jean. 1979. Transductions and Context-Free Languages. B. G. Teubner, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan W Black</author>
</authors>
<title>Finite state machines from feature grammars.</title>
<date>1989</date>
<booktitle>In International Workshop on Parsing Technologies,</booktitle>
<pages>277--285</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="39158" citStr="Black (1989)" startWordPosition="7126" endWordPosition="7127">ez and Schimpf (1990) amounts to the parameterized RTN method from Section 4.1; note that the histories from Section 4.1 in fact function as stacks, the items being the stack symbols. 31 Computational Linguistics Volume 26, Number 1 4.5 Subset Approximation through Pushdown Automata By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results. This idea was proposed by Krauwer and des Tombe (1981), Langendoen and Langsam (1987), and Pulman (1986), and was rediscovered by Black (1989) and recently by Johnson (1998). Since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method. One first constructs a modified left-corner recognizer from the grammar, in the form of a pushdown automaton. The stack height is bounded by a low number; Johnson (1998) claims a suitable number would be 5. The motivation for using the left-corner strategy is that the height of the stack maintained by a left-corner parser is already bounded by a constant in the absence of self-embedding. If t</context>
</contexts>
<marker>Black, 1989</marker>
<rawString>Black, Alan W. 1989. Finite state machines from feature grammars. In International Workshop on Parsing Technologies, pages 277-285, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>A note on phrase structure grammars.</title>
<date>1959</date>
<journal>Information and Control,</journal>
<pages>2--393</pages>
<contexts>
<context position="6183" citStr="Chomsky 1959" startWordPosition="1087" endWordPosition="1088">f parse trees such as the one in Figure 1, for a grammar of palindromes. It is intuitively clear that the language is not regular: the grammar symbols to the left of the spine from the root to E &amp;quot;communicate&amp;quot; with those to the right of the spine. More precisely, the prefix of the input up to the point where it meets the final node c of the spine determines the suffix after that point, in such a way that an unbounded quantity of symbols from the prefix need to be taken into account. A formal explanation for why the grammar may not generate a regular language relies on the following definition (Chomsky 1959b): 18 Nederhof Experiments with Regular Approximation a S a S --&gt; a S a S — &gt; b S b S E b S b Figure 1 Grammar of palindromes, and a parse tree. Definition A grammar is self-embedding if there is some A E N such that A —›* aA3, for some a 0 c and If a grammar is not self-embedding, this means that when a section of a spine in a parse tree repeats itself, then either no grammar symbols occur to the left of that section of the spine, or no grammar symbols occur to the right. This prevents the &amp;quot;unbounded communication&amp;quot; between the two sides of the spine exemplified by the palindrome grammar. We </context>
<context position="8352" citStr="Chomsky 1959" startWordPosition="1486" endWordPosition="1487">hen recursive(N1) = left, Ni consists of only left-recursive nonterminals, which does not mean it cannot also contain right-recursive nonterminals, but in that case right recursion amounts to application of unit rules. When recursive(Ni) = cyclic, it is only such unit rules that take part in the recursion. That recursive(Ni) = self, for some i, is a sufficient and necessary condition for the grammar to be self-embedding. Therefore, we have to prove that if recursive(Ni) E {left, right, cyclic}, for all i, then the grammar generates a regular language. Our proof differs from an existing proof (Chomsky 1959a) in that it is fully constructive: Figure 2 presents an algorithm for creating a finite automaton that accepts the language generated by the grammar. The process is initiated at the start symbol, and from there the process descends the grammar in all ways until terminals are encountered, and then transitions are created labeled with those terminals. Descending the grammar is straightforward in the case of rules of which the left-hand side is not a recursive nonterminal: the subautomata found recursively for members in the right-hand side will be connected. In the case of recursive nontermina</context>
</contexts>
<marker>Chomsky, 1959</marker>
<rawString>Chomsky, Noam. 1959a. A note on phrase structure grammars. Information and Control, 2:393-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>On certain formal properties of grammars.</title>
<date>1959</date>
<journal>Information and Control,</journal>
<pages>2--137</pages>
<contexts>
<context position="6183" citStr="Chomsky 1959" startWordPosition="1087" endWordPosition="1088">f parse trees such as the one in Figure 1, for a grammar of palindromes. It is intuitively clear that the language is not regular: the grammar symbols to the left of the spine from the root to E &amp;quot;communicate&amp;quot; with those to the right of the spine. More precisely, the prefix of the input up to the point where it meets the final node c of the spine determines the suffix after that point, in such a way that an unbounded quantity of symbols from the prefix need to be taken into account. A formal explanation for why the grammar may not generate a regular language relies on the following definition (Chomsky 1959b): 18 Nederhof Experiments with Regular Approximation a S a S --&gt; a S a S — &gt; b S b S E b S b Figure 1 Grammar of palindromes, and a parse tree. Definition A grammar is self-embedding if there is some A E N such that A —›* aA3, for some a 0 c and If a grammar is not self-embedding, this means that when a section of a spine in a parse tree repeats itself, then either no grammar symbols occur to the left of that section of the spine, or no grammar symbols occur to the right. This prevents the &amp;quot;unbounded communication&amp;quot; between the two sides of the spine exemplified by the palindrome grammar. We </context>
<context position="8352" citStr="Chomsky 1959" startWordPosition="1486" endWordPosition="1487">hen recursive(N1) = left, Ni consists of only left-recursive nonterminals, which does not mean it cannot also contain right-recursive nonterminals, but in that case right recursion amounts to application of unit rules. When recursive(Ni) = cyclic, it is only such unit rules that take part in the recursion. That recursive(Ni) = self, for some i, is a sufficient and necessary condition for the grammar to be self-embedding. Therefore, we have to prove that if recursive(Ni) E {left, right, cyclic}, for all i, then the grammar generates a regular language. Our proof differs from an existing proof (Chomsky 1959a) in that it is fully constructive: Figure 2 presents an algorithm for creating a finite automaton that accepts the language generated by the grammar. The process is initiated at the start symbol, and from there the process descends the grammar in all ways until terminals are encountered, and then transitions are created labeled with those terminals. Descending the grammar is straightforward in the case of rules of which the left-hand side is not a recursive nonterminal: the subautomata found recursively for members in the right-hand side will be connected. In the case of recursive nontermina</context>
</contexts>
<marker>Chomsky, 1959</marker>
<rawString>Chomsky, Noam. 1959b. On certain formal properties of grammars. Information and Control, 2:137-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karel Culik</author>
<author>Rina Cohen</author>
</authors>
<title>LR-regular grammars—An extension of LR(k) grammars.</title>
<date>1973</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>7--66</pages>
<marker>Culik, Cohen, 1973</marker>
<rawString>Culik, Karel II and Rina Cohen. 1973. LR-regular grammars—An extension of LR(k) grammars. Journal of Computer and System Sciences, 7:66-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>13--2</pages>
<contexts>
<context position="19391" citStr="Earley (1970)" startWordPosition="3451" endWordPosition="3452">at constructs finite transducers rather than finite automata. We have further implemented a parameterized version of the RTN approximation. A state of the nondeterministic automaton is now also associated to a list H of length IHI strictly smaller than a number d, which is the parameter to the method. This list represents a history of rule positions that were encountered in the computation leading to the present state. More precisely, we define an item to be an object of the form [A —&gt; a • 0], where A —&gt; ai3 is a rule from the grammar. These are the same objects as the &amp;quot;dotted&amp;quot; productions of Earley (1970). The dot indicates a position in the right-hand side. The unparameterized RTN method had one state qj for each item I, and two states qA and q&apos;A for each nonterminal A. The parameterized RTN method has one state qm for each item I and each list of items H that represents a valid history for reaching I, and two states qAH and q&apos;AH for each nonterminal A and each list of items H that represents a valid history for reaching A. Such a valid history is defined to be a list 24 Nederhof Experiments with Regular Approximation H with 0 &lt; IHI &lt;d that represents a series of positions in rules that could</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94-102, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edmund Grimley-Evans</author>
</authors>
<title>Approximating context-free grammars with a finite-state calculus.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics an 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>452--459</pages>
<location>Madrid,</location>
<contexts>
<context position="22868" citStr="Grimley-Evans (1997)" startWordPosition="4149" endWordPosition="4150"> given by item [A c • A]; and the history of coming from the rule position given by item [B d • Ae]), in Figure 6 we now have three states of the form gm for each I = [A —&gt; a • 0], as well as three states of the form gAn and q&apos;A The higher we choose d, the more precise the approximation is, since the histories allow the automaton to simulate part of the mechanism of recursion from the original grammar, and the maximum length of the histories corresponds to the number of levels of recursion that can be simulated accurately. 4.2 Refinement of RTN Superset Approximation We rephrase the method of Grimley-Evans (1997) as follows: First, we construct the approximating finite automaton according to the unparameterized RTN method above. Then an additional mechanism is introduced that ensures for each rule A — Xi . X„, separately that the list of visits to the states go, , gni satisfies some reasonable criteria: a visit to g,, with 0 &lt; i &lt; m, should be followed by one to gi+i or go. The latter option amounts to a nested incarnation of the rule. There is a complementary condition for what should precede a visit to q,, with 0 &lt; i &lt; m. Since only pairs of consecutive visits to states from the set {go, ...,grn} ar</context>
<context position="52219" citStr="Grimley-Evans 1997" startWordPosition="9382" endWordPosition="9383"> in the number of rules, and note an accompanying sharp increase in the size of the finite automaton. For this method, we see no possibility of accomplishing the complete approximation process, including determinization and minimization, for grammars in our collection that are substantially larger than 50 rules. Since no grammars of interest could be handled by them, the above two methods will be left out of further consideration. 35 Computational Linguistics Volume 26, Number 1 Table 1 Size of the compact representation and number of states and transitions, for the refined RTN approximation (Grimley-Evans 1997). Grammar Size Compact Representation # of States # of Transitions 10 133 11 14 12 427 17 26 13 1,139 17 34 14 4,895 17 36 15 16,297 17 40 16 51,493 19 52 17 208,350 19 52 18 409,348 21 59 19 1,326,256 21 61 Table 2 Size of the compact representation and number of states and transitions, for the superset approximation based on LR automata following Pereira and Wright (1997). Grammar Size Compact Representation # of States # of Transitions 35 15,921 350 2,125 44 24,651 499 4,352 47 151,226 5,112 35,754 50 646,419 ? ? Below, we refer to the unparameterized and parameterized approximations based </context>
</contexts>
<marker>Grimley-Evans, 1997</marker>
<rawString>Grimley-Evans, Edmund. 1997. Approximating context-free grammars with a finite-state calculus. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics an 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 452-459, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Harrison</author>
</authors>
<title>Introduction to Formal Language Theory.</title>
<date>1978</date>
<publisher>Addison-Wesley.</publisher>
<marker>Harrison, 1978</marker>
<rawString>Harrison, Michael A. 1978. Introduction to Formal Language Theory. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Heckert</author>
</authors>
<title>Behandlung von Syntaxfehlern far LR-Sprachen ohne Korrekturversuche.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Ruhr-Universitat Bochum.</institution>
<contexts>
<context position="37143" citStr="Heckert (1994)" startWordPosition="6800" endWordPosition="6801">zed, we obtain a third source of exponential behavior. The time and space complexity of the method are thereby bounded by a triple exponential function in the size of the grammar. This theoretical analysis seems to be in keeping with the high costs of applying this method in practice, as will be shown later in this article. As proposed by Pereira and Wright (1997), our implementation applies the approximation separately for each nonterminal occurring in a set N, that reveals selfembedding. A different superset approximation based on LR automata was proposed by Baker (1981) and rediscovered by Heckert (1994). Each individual stack symbol is now translated to one state of the nondeterministic finite automaton. It can be argued theoretically that this approximation differs from the unparameterized RTN approximation from Section 4.1 only under certain conditions that are not likely to occur very often in practice. This consideration is confirmed by our experiments to be discussed later. Our implementation differs from the original algorithm in that the approximation is applied separately for each nonterminal in a set Ni that reveals self-embedding. A generalization of this method was suggested by Be</context>
</contexts>
<marker>Heckert, 1994</marker>
<rawString>Heckert, Erik. 1994. Behandlung von Syntaxfehlern far LR-Sprachen ohne Korrekturversuche. Ph.D. thesis, Ruhr-Universitat Bochum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Finite-state approximation of constraint-based grammars using left-corner grammar transforms.</title>
<date>1998</date>
<booktitle>In COLING-ACL &apos;98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>619--623</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="39189" citStr="Johnson (1998)" startWordPosition="7131" endWordPosition="7132"> to the parameterized RTN method from Section 4.1; note that the histories from Section 4.1 in fact function as stacks, the items being the stack symbols. 31 Computational Linguistics Volume 26, Number 1 4.5 Subset Approximation through Pushdown Automata By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results. This idea was proposed by Krauwer and des Tombe (1981), Langendoen and Langsam (1987), and Pulman (1986), and was rediscovered by Black (1989) and recently by Johnson (1998). Since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method. One first constructs a modified left-corner recognizer from the grammar, in the form of a pushdown automaton. The stack height is bounded by a low number; Johnson (1998) claims a suitable number would be 5. The motivation for using the left-corner strategy is that the height of the stack maintained by a left-corner parser is already bounded by a constant in the absence of self-embedding. If the artificial bound imposed by </context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Johnson, Mark. 1998. Finite-state approximation of constraint-based grammars using left-corner grammar transforms. In COLING-ACL &apos;98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, volume 1, pages 619-623, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Krauwer</author>
<author>Louis des Tombe</author>
</authors>
<title>Transducers and grammars as theories of language. Theoretical Linguistics,</title>
<date>1981</date>
<pages>8--173</pages>
<marker>Krauwer, Tombe, 1981</marker>
<rawString>Krauwer, Steven and Louis des Tombe. 1981. Transducers and grammars as theories of language. Theoretical Linguistics, 8:173-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Terence Langendoen</author>
<author>Yedidyah Langsam</author>
</authors>
<title>On the design of finite transducers for parsing phrase-structure languages.</title>
<date>1987</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<pages>191--235</pages>
<editor>In Alexis Manaster-Ramer, editor,</editor>
<location>Amsterdam,</location>
<contexts>
<context position="39101" citStr="Langendoen and Langsam (1987)" startWordPosition="7115" endWordPosition="7118"> of automaton that performs topdown recognition, then the method in Bermudez and Schimpf (1990) amounts to the parameterized RTN method from Section 4.1; note that the histories from Section 4.1 in fact function as stacks, the items being the stack symbols. 31 Computational Linguistics Volume 26, Number 1 4.5 Subset Approximation through Pushdown Automata By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results. This idea was proposed by Krauwer and des Tombe (1981), Langendoen and Langsam (1987), and Pulman (1986), and was rediscovered by Black (1989) and recently by Johnson (1998). Since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method. One first constructs a modified left-corner recognizer from the grammar, in the form of a pushdown automaton. The stack height is bounded by a low number; Johnson (1998) claims a suitable number would be 5. The motivation for using the left-corner strategy is that the height of the stack maintained by a left-corner parser is already bou</context>
</contexts>
<marker>Langendoen, Langsam, 1987</marker>
<rawString>Langendoen, D. Terence and Yedidyah Langsam. 1987. On the design of finite transducers for parsing phrase-structure languages. In Alexis Manaster-Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam, pages 191-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Dynamic compilation of weighted context-free grammars.</title>
<date>1998</date>
<booktitle>In COLING-ACL &apos;98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>891--897</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="14321" citStr="Mohri and Pereira (1998)" startWordPosition="2586" endWordPosition="2589">a subautomaton is output after all subautomata occurring at its transitions have been output. Our implementation constructs a minimal deterministic automaton by repeating the following for p = 1, . . . , m: 1. Make a copy of A. Determinize and minimize the copy. If it has fewer transitions labeled by nonterminals than the original, then replace Ap by its copy. 2. Replace each transition in Ap of the form (q, A„ q&apos;) by (a copy of) automaton A, in a straightforward way. This means that new e-transitions connect q to the start state of A, and the final states of A, to q&apos;. 1 The representation in Mohri and Pereira (1998) is even more compact than ours for grammars that are not self-embedding. However, in this paper we use our representation as an intermediate result in approximating an unrestricted context-free grammar, with the final objective of obtaining a single minimal deterministic automaton. For this purpose, Mohri and Pereira&apos;s representation offers little advantage. qB 22 Nederhof Experiments with Regular Approximation 3. Again determinize and minimize Ap and store it for later reference. The automaton obtained for An, after step 3 is the desired result. 4. Methods of Regular Approximation This secti</context>
</contexts>
<marker>Mohri, Pereira, 1998</marker>
<rawString>Mohri, Mehryar and Fernando C. N. Pereira. 1998. Dynamic compilation of weighted context-free grammars. In COLING-ACL &apos;98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, volume 2, pages 891-897, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>A rational design for a weighted finite-state transducer library.</title>
<date>1998</date>
<booktitle>Automata Implementation. Lecture Notes in Computer Science, Number 1436.</booktitle>
<pages>144--158</pages>
<editor>In Derick Wood and Sheng Yu, editors,</editor>
<publisher>Springer Verlag,</publisher>
<marker>Mohri, Pereira, Riley, 1998</marker>
<rawString>Mohri, Mehryar, Fernando C. N. Pereira, and Michael Riley. 1998. A rational design for a weighted finite-state transducer library. In Derick Wood and Sheng Yu, editors, Automata Implementation. Lecture Notes in Computer Science, Number 1436. Springer Verlag, pages 144-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Linguistic Parsing and Program Transformations.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Nijmegen.</institution>
<contexts>
<context position="9678" citStr="Nederhof 1994" startWordPosition="1700" endWordPosition="1701">or right-recursive; if they are both, which means they are cyclic, then either subprocess can be applied; in the code in Figure 2 cyclic and right-recursive subsets N, are treated uniformly. We discuss the case in which the nonterminals are left-recursive. One new state is created for each nonterminal in the set. The transitions that are created for terminals and nonterminals not in N, are connected in a way that is reminiscent of the construction of left-corner parsers (Rosertkrantz and Lewis 1970), and specifically of one construction that focuses on sets of mutually recursive nonterminals (Nederhof 1994, Section 5.8). An example is given in Figure 3. Four states have been labeled according to the names they are given in procedure make fa. There are two states that are labeled qB. This can be explained by the fact that nonterminal B can be reached by descending the grammar from S in two essentially distinct ways. The code in Figure 2 differs from the actual implementation in that sometimes, for a nonterminal, a separate finite automaton is constructed, namely, for those nonterminals that occur as A in the code. A transition in such a subautomaton may be labeled by another nonterminal B, which</context>
</contexts>
<marker>Nederhof, 1994</marker>
<rawString>Nederhof, Mark-Jan. 1994. Linguistic Parsing and Program Transformations. Ph.D. thesis, University of Nijmegen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Regular approximations of CFLs: A grammatical view.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<pages>159--170</pages>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="2478" citStr="Nederhof (1997)" startWordPosition="381" endWordPosition="382">n the increase of the sizes of the intermediate results and the obtained minimal deterministic automaton? Second, how &amp;quot;precise&amp;quot; are the approximations? That is, how much larger than the original context-free language is the language obtained by a superset approximation, and how much smaller is the language obtained by a subset approximation? (How we measure the &amp;quot;sizes&amp;quot; of languages in a practical setting will become clear in what follows.) Some considerations with regard to theoretical upper bounds on the sizes of the intermediate results and the finite automata have already been discussed in Nederhof (1997). In this article we will try to answer the above two questions in a practical setting, using practical linguistic grammars and sentences taken from a spoken-language corpus. * DFKL Stuhlsatzenhausweg 3, D-66123 Saarbrticken, Germany. E-mail: nederhof@dflci.de © 2000 Association for Computational Linguistics Computational Linguistics Volume 26, Number 1 The structure of this paper is as follows: In Section 2 we recall some standard definitions from language theory. Section 3 investigates a sufficient condition for a context-free grammar to generate a regular language. We also present the const</context>
<context position="15945" citStr="Nederhof (1997)" startWordPosition="2844" endWordPosition="2845">locally. The approximations are integrated into the construction of the finite automaton from the grammar, which was described in the previous section. A separate incarnation of the approximation process is activated upon finding a nonterminal A such that A E N, and recursive(N,) = self, for some i. This incarnation then only pertains to the set of rules of the form B a, where B E Ni. In other words, nonterminals not in Ni are treated by this incarnation of the approximation process as if they were terminals. 4.1 Superset Approximation Based on RTNs The following approximation was proposed in Nederhof (1997). The presentation here, however, differs substantially from the earlier publication, which treated the approximation process entirely on the level of context-free grammars: a self-embedding grammar was transformed in such a way that it was no longer self-embedding. A finite automaton was then obtained from the grammar by the algorithm discussed above. The presentation here is based on recursive transition networks (RTNs) (Woods 1970). We can see a context-free grammar as an RTN as follows: We introduce two states qA and q&apos;A for each nonterminal A, and m + 1 states qo, , q„, for each rule A —&gt;</context>
<context position="44663" citStr="Nederhof (1997)" startWordPosition="8162" endWordPosition="8163">eter of the transformation, and for each Ni such that recursive(N) = self. For each nonterminal A E N, we introduce j new nonterminals A[1], , A [j]. For each A —&gt; ... Xn, in P such that A E and h such that 1 h &lt; j, we add A[h] —&gt; Xn, to P. where for 1 &lt;k &lt; m: = Xk[h + 1] if Xk E N, h &lt; j = Xk otherwise Further, we replace all rules A —&gt; Xi . Xn, such that A V N, by A —&gt; Xn,&apos; , where for 1 &lt;k &lt; m: Xc, = X[1] if Xk E = Xk otherwise If the start symbol S was in Nu we let S[1] be the new start symbol. A second transformation, which shares some characteristics with the one above, was presented in Nederhof (1997). One of the earliest papers suggesting such transformations as a way to increase the precision of approximation is due to ulik and Cohen (1973), who only discuss examples, however; no general algorithms were defined. 33 Computational Linguistics Volume 26, Number 1 50 100 150 200 250 300 350 corpus size (# sentences) 5 10 15 20 25 30 length (# words) 180 160 140 120 100 80 60 40 20 0 o grammar size (# rules) 550 500 450 400 350 300 250 200 150 100 50 Figure 11 The test material. The left-hand curve refers to the construction of the grammar from 332 sentences, the right-hand curve refers to th</context>
</contexts>
<marker>Nederhof, 1997</marker>
<rawString>Nederhof, Mark-Jan. 1997. Regular approximations of CFLs: A grammatical view. In Proceedings of the International Workshop on Parsing Technologies, pages 159-170, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Context-free parsing through regular approximation.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Workshop on Finite State Methods in Natural Language Processing,</booktitle>
<pages>13--24</pages>
<location>Ankara, Turkey.</location>
<contexts>
<context position="18739" citStr="Nederhof (1998)" startWordPosition="3335" endWordPosition="3336"> 5 Application of the RTN method for the grammar in (a). The RTN is given in (b), and (c) presents the approximating finite automaton. We assume A is the start symbol and therefore qA becomes the initial state and q&apos;A becomes the final state in the approximating automaton. For the sake of presentational convenience, the above describes a construction working on the complete grammar. However, our implementation applies the construction separately for each nonterminal in a set N, such that recursive(N) = self, which leads to a separate subautomaton of the compact representation (Section 3). See Nederhof (1998) for a variant of this approximation that constructs finite transducers rather than finite automata. We have further implemented a parameterized version of the RTN approximation. A state of the nondeterministic automaton is now also associated to a list H of length IHI strictly smaller than a number d, which is the parameter to the method. This list represents a history of rule positions that were encountered in the computation leading to the present state. More precisely, we define an item to be an object of the form [A —&gt; a • 0], where A —&gt; ai3 is a rule from the grammar. These are the same </context>
<context position="34575" citStr="Nederhof (1998)" startWordPosition="6386" endWordPosition="6387">ne the partition of mutually recursive nonterminals after transformation. This integration makes use, for example, of the fact that for fixed Ni and fixed f, the set of nonterminals of the form A11, with A E N, is (potentially) mutually right-recursive. A set of such nonterminals can therefore be treated as the corresponding case from Figure 2, assuming the value right. The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here. A very similar formulation, for another grammar transformation, is given in Nederhof (1998). Computational Linguistics Volume 26, Number 1 S-C° sr,0 a Ar40 B&apos;0 0 (S d 30 Nederhof Experiments with Regular Approximation 4.4 Superset Approximation through Pushdown Automata The distinction between context-free languages and regular languages can be seen in terms of the distinction between pushdown automata and finite automata. Pushdown automata maintain a stack that is potentially unbounded in height, which allows more complex languages to be recognized than in the case of finite automata. Regular approximation can be achieved by restricting the height of the stack, as we will see in Se</context>
</contexts>
<marker>Nederhof, 1998</marker>
<rawString>Nederhof, Mark-Jan. 1998. Context-free parsing through regular approximation. In Proceedings of the International Workshop on Finite State Methods in Natural Language Processing, pages 13-24, Ankara, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient tabular LR parsing.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting,</booktitle>
<pages>239--246</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="36305" citStr="Nederhof and Satta 1996" startWordPosition="6660" endWordPosition="6663">two occurrences, including one of the two occurrences. This process defines a congruence relation on stacks, with a finite number of congruence classes. This congruence relation directly defines a finite automaton: each class is translated to a unique state of the nondeterministic finite automaton, shift actions are translated to transitions labeled with terminals, and reduce actions are translated to epsilon transitions. The method has a high complexity. First, construction of an LR automaton, of which the size is exponential in the size of the grammar, may be a prohibitively expensive task (Nederhof and Satta 1996). This is, however, only a fraction of the effort needed to compute the congruence classes, of which the number is in turn exponential in the size of the LR automaton. If the resulting nondeterministic automaton is determinized, we obtain a third source of exponential behavior. The time and space complexity of the method are thereby bounded by a triple exponential function in the size of the grammar. This theoretical analysis seems to be in keeping with the high costs of applying this method in practice, as will be shown later in this article. As proposed by Pereira and Wright (1997), our impl</context>
</contexts>
<marker>Nederhof, Satta, 1996</marker>
<rawString>Nederhof, Mark-Jan and Giorgio Satta. 1996. Efficient tabular LR parsing. In Proceedings of the 34th Annual Meeting, pages 239-246, Santa Cruz, CA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Rebecca N Wright</author>
</authors>
<title>Finite-state approximation of phrase-structure grammars.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing.</booktitle>
<pages>149--173</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="35334" citStr="Pereira and Wright (1997)" startWordPosition="6501" endWordPosition="6504">erset Approximation through Pushdown Automata The distinction between context-free languages and regular languages can be seen in terms of the distinction between pushdown automata and finite automata. Pushdown automata maintain a stack that is potentially unbounded in height, which allows more complex languages to be recognized than in the case of finite automata. Regular approximation can be achieved by restricting the height of the stack, as we will see in Section 4.5, or by ignoring the distinction between several stacks when they become too high. More specifically, the method proposed by Pereira and Wright (1997) first constructs an LR automaton, which is a special case of a pushdown automaton. Then, stacks that may be constructed in the course of recognition of a string are computed one by one. However, stacks that contain two occurrences of a stack symbol are identified with the shorter stack that results by removing the part of the stack between the two occurrences, including one of the two occurrences. This process defines a congruence relation on stacks, with a finite number of congruence classes. This congruence relation directly defines a finite automaton: each class is translated to a unique s</context>
<context position="36895" citStr="Pereira and Wright (1997)" startWordPosition="6761" endWordPosition="6764">sive task (Nederhof and Satta 1996). This is, however, only a fraction of the effort needed to compute the congruence classes, of which the number is in turn exponential in the size of the LR automaton. If the resulting nondeterministic automaton is determinized, we obtain a third source of exponential behavior. The time and space complexity of the method are thereby bounded by a triple exponential function in the size of the grammar. This theoretical analysis seems to be in keeping with the high costs of applying this method in practice, as will be shown later in this article. As proposed by Pereira and Wright (1997), our implementation applies the approximation separately for each nonterminal occurring in a set N, that reveals selfembedding. A different superset approximation based on LR automata was proposed by Baker (1981) and rediscovered by Heckert (1994). Each individual stack symbol is now translated to one state of the nondeterministic finite automaton. It can be argued theoretically that this approximation differs from the unparameterized RTN approximation from Section 4.1 only under certain conditions that are not likely to occur very often in practice. This consideration is confirmed by our exp</context>
<context position="51193" citStr="Pereira and Wright (1997)" startWordPosition="9208" endWordPosition="9211">cally obtained grammars in our collection that were self-embedding. We therefore eliminated individual rules by hand, starting from the smallest selfembedding grammar in our collection, eventually finding grammars small enough to be handled by this method. The results are given in Table 1. Note that the size of the compact representation increases significantly for each additional grammar rule. The sizes of the finite automata, after determinization and minimization, remain relatively small. Also problematic was the first approximation from Section 4.4, which was based on LR parsing following Pereira and Wright (1997). Even for the grammar of 50 rules, we were not able to determinize and minimize one of the subautomata according to step 1 of Section 3: we stopped the process after it had reached a size of over 600 megabytes. Results, as far as we could obtain them, are given in Table 2. Note the sharp increases in the size of the compact representation, resulting from small increases, from 44 to 47 and from 47 to 50, in the number of rules, and note an accompanying sharp increase in the size of the finite automaton. For this method, we see no possibility of accomplishing the complete approximation process,</context>
<context position="52595" citStr="Pereira and Wright (1997)" startWordPosition="9452" endWordPosition="9455">the above two methods will be left out of further consideration. 35 Computational Linguistics Volume 26, Number 1 Table 1 Size of the compact representation and number of states and transitions, for the refined RTN approximation (Grimley-Evans 1997). Grammar Size Compact Representation # of States # of Transitions 10 133 11 14 12 427 17 26 13 1,139 17 34 14 4,895 17 36 15 16,297 17 40 16 51,493 19 52 17 208,350 19 52 18 409,348 21 59 19 1,326,256 21 61 Table 2 Size of the compact representation and number of states and transitions, for the superset approximation based on LR automata following Pereira and Wright (1997). Grammar Size Compact Representation # of States # of Transitions 35 15,921 350 2,125 44 24,651 499 4,352 47 151,226 5,112 35,754 50 646,419 ? ? Below, we refer to the unparameterized and parameterized approximations based on RTNs (Section 4.1) as RTN and RTNd, respectively, for d = 2,3; to the subset approximation from Figure 9 as Subd, for d -= 1, 2, 3; and to the second and third methods from Section 4.4, which were based on LR parsing following Baker (1981) and Bermudez and Schimpf (1990), as LR and LRd, respectively, for d = 2,3. We refer to the subset approximation based on left-corner </context>
</contexts>
<marker>Pereira, Wright, 1997</marker>
<rawString>Pereira, Fernando C. N. and Rebecca N. Wright. 1997. Finite-state approximation of phrase-structure grammars. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing. MIT Press, pages 149-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Pulman</author>
</authors>
<title>Grammars, parsers, and memory limitations.</title>
<date>1986</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>1--3</pages>
<contexts>
<context position="39120" citStr="Pulman (1986)" startWordPosition="7120" endWordPosition="7121"> recognition, then the method in Bermudez and Schimpf (1990) amounts to the parameterized RTN method from Section 4.1; note that the histories from Section 4.1 in fact function as stacks, the items being the stack symbols. 31 Computational Linguistics Volume 26, Number 1 4.5 Subset Approximation through Pushdown Automata By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results. This idea was proposed by Krauwer and des Tombe (1981), Langendoen and Langsam (1987), and Pulman (1986), and was rediscovered by Black (1989) and recently by Johnson (1998). Since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method. One first constructs a modified left-corner recognizer from the grammar, in the form of a pushdown automaton. The stack height is bounded by a low number; Johnson (1998) claims a suitable number would be 5. The motivation for using the left-corner strategy is that the height of the stack maintained by a left-corner parser is already bounded by a constant </context>
</contexts>
<marker>Pulman, 1986</marker>
<rawString>Pulman, S. G. 1986. Grammars, parsers, and memory limitations. Language and Cognitive Processes, 1(3):197-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Brown</author>
</authors>
<title>Parsing extended LR(k) grammars.</title>
<date>1981</date>
<journal>Acta Infirmatica,</journal>
<pages>15--115</pages>
<contexts>
<context position="10429" citStr="Brown 1981" startWordPosition="1824" endWordPosition="1825">here are two states that are labeled qB. This can be explained by the fact that nonterminal B can be reached by descending the grammar from S in two essentially distinct ways. The code in Figure 2 differs from the actual implementation in that sometimes, for a nonterminal, a separate finite automaton is constructed, namely, for those nonterminals that occur as A in the code. A transition in such a subautomaton may be labeled by another nonterminal B, which then represents the subautomaton corresponding to B. The resulting representation is similar to extended context-free grammars (Purdom and Brown 1981), with the exception that in our case recursion cannot occur, by virtue of the construction. The representation for the running example is indicated by Figure 4, which shows two subautomata, labeled S and B. The one labeled S is the automaton on the top level, and contains two transitions labeled B, which refer to the other subautomaton. Note that this representation is more compact than that of Figure 3, since the transitions that are involved in representing the sublanguage of strings generated by nonterminal B are included only once. The compact representation consisting of subautomata can </context>
</contexts>
<marker>Brown, 1981</marker>
<rawString>Purdom, Paul Walton, Jr. and Cynthia A. Brown. 1981. Parsing extended LR(k) grammars. Acta Infirmatica, 15:115-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Left-corner parsing and psychological plausibility.</title>
<date>1992</date>
<booktitle>In COLING &apos;92: Papers presented to the Fifteenth [sic] International Conference on Computational Linguistics,</booktitle>
<pages>191--197</pages>
<location>Nantes, France.</location>
<contexts>
<context position="24929" citStr="Resnik 1992" startWordPosition="4510" endWordPosition="4511">truction separately for each nonterminal in a set N, such that recursive(N) = self. 4.3 Subset Approximation by Transforming the Grammar Putting restrictions on spines is another way to obtain a regular language. Several methods can be defined. The first method we present investigates spines in a very detailed way. It eliminates from the language only those sentences for which a subderivation is required of the form B ceB[3, for some a € and )3 E. The motivation is that such sentences do not occur frequently in practice, since these subderivations make them difficult for people to comprehend (Resnik 1992). Their exclusion will therefore not lead to much loss of coverage of typical sentences, especially for simple application domains. We express the method in terms of a grammar transformation in Figure 7. The effect of this transformation is that a nonterminal A is tagged with a set of pairs (B, Q), where B is a nonterminal occurring higher in the spine; for any given B, at most one such pair (B, Q) can be contained in the set. The set Q may contain the element I to indicate that something to the left of the part of the spine from B to A Volume 26, Number 1 Computational Linguistics H= [A—&gt;a•B </context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Resnik, Philip. 1992. Left-corner parsing and psychological plausibility. In COLING &apos;92: Papers presented to the Fifteenth [sic] International Conference on Computational Linguistics, pages 191-197, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Rosenkrantz</author>
<author>P M Lewis</author>
</authors>
<title>Deterministic left comer parsing.</title>
<date>1970</date>
<booktitle>In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata Theory,</booktitle>
<pages>139--152</pages>
<marker>Rosenkrantz, Lewis, 1970</marker>
<rawString>Rosenkrantz, D. J. and P. M. Lewis, II. 1970. Deterministic left comer parsing. In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata Theory, pages 139-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin R Seyfarth</author>
<author>Manuel E Bermudez</author>
</authors>
<title>Suffix languages in LR parsing.</title>
<date>1995</date>
<journal>International Journal of Computer Mathematics,</journal>
<pages>55--135</pages>
<contexts>
<context position="40359" citStr="Seyfarth and Bermudez (1995)" startWordPosition="7323" endWordPosition="7326">nce of self-embedding. If the artificial bound imposed by the approximation method is chosen to be larger than or equal to this natural bound, then the approximation may be exact. Our own implementation is more refined than the published algorithms mentioned above, in that it defines a separate left-corner recognizer for each nonterminal A such that A E Ni and recursive(N) = self, some i. In the construction of one such recognizer, nonterminals that do not belong to Ni are treated as terminals, as in all other methods discussed here. 4.6 Superset Approximation by N-grams An approximation from Seyfarth and Bermudez (1995) can be explained as follows. Define the set of all terminals reachable from nonterminal A to be EA = {a a,[3[A —4* aa[3]}. We now approximate the set of strings derivable from A by EA, which is the set of strings consisting of terminals from EA. Our implementation is made slightly more sophisticated by taking EA to be {X I 3B, a, /3[B E N, A B aX0 AX N]}, for each A such that A E N, and recursive(N) = self, for some i. That is, each X E EA is a terminal, or a nonterminal not in the same set Ni as A, but immediately reachable from set N„ through B E N. This method can be generalized, inspired </context>
<context position="41723" citStr="Seyfarth and Bermudez (1995)" startWordPosition="7606" endWordPosition="7609">N = 1, 2, 3, ... gives rise to a superset approximation that can be described as follows: The set of strings derivable from a nonterminal A is approximated by the set of strings al ... an such that • for each substring v = ai±i ai+N (0 &lt; i &lt; n — N) we have A —&gt;* wvy, for some w and y, • for each prefix v =- al ... ai (0 &lt; &lt; n) such that i &lt; N we have A vy, for some y, and • for each suffix v = afro. . an (0 &lt; i &lt; n) such that n — i &lt; N we have A —›* wv, for some w. (Again, the algorithms that we actually implemented are more refined and take into account the sets 1\11.) The approximation from Seyfarth and Bermudez (1995) can be seen as the case N = 1, which will henceforth be called the unigram method. We have also experimented with the cases N = 2 and N = 3, which will be called the bigram and trigram methods. 32 Nederhof Experiments with Regular Approximation 5. Increasing the Precision The methods of approximation described above take as input the parts of the grammar that pertain to self-embedding. It is only for those parts that the language is affected. This leads us to a way to increase the precision: before applying any of the above methods of regular approximation, we first transform the grammar. Thi</context>
</contexts>
<marker>Seyfarth, Bermudez, 1995</marker>
<rawString>Seyfarth, Benjamin R. and Manuel E. Bermudez. 1995. Suffix languages in LR parsing. International Journal of Computer Mathematics, 55:135-153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jonathan Segal</author>
</authors>
<title>Precise N-gram probabilities from stochastic context-free grammars.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting,</booktitle>
<pages>74--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Las Cruces, NM.</location>
<contexts>
<context position="40986" citStr="Stolcke and Segal (1994)" startWordPosition="7449" endWordPosition="7452">n be explained as follows. Define the set of all terminals reachable from nonterminal A to be EA = {a a,[3[A —4* aa[3]}. We now approximate the set of strings derivable from A by EA, which is the set of strings consisting of terminals from EA. Our implementation is made slightly more sophisticated by taking EA to be {X I 3B, a, /3[B E N, A B aX0 AX N]}, for each A such that A E N, and recursive(N) = self, for some i. That is, each X E EA is a terminal, or a nonterminal not in the same set Ni as A, but immediately reachable from set N„ through B E N. This method can be generalized, inspired by Stolcke and Segal (1994), who derive N-gram probabilities from stochastic context-free grammars. By ignoring the probabilities, each N = 1, 2, 3, ... gives rise to a superset approximation that can be described as follows: The set of strings derivable from a nonterminal A is approximated by the set of strings al ... an such that • for each substring v = ai±i ai+N (0 &lt; i &lt; n — N) we have A —&gt;* wvy, for some w and y, • for each prefix v =- al ... ai (0 &lt; &lt; n) such that i &lt; N we have A vy, for some y, and • for each suffix v = afro. . an (0 &lt; i &lt; n) such that n — i &lt; N we have A —›* wv, for some w. (Again, the algorithm</context>
</contexts>
<marker>Stolcke, Segal, 1994</marker>
<rawString>Stolcke, Andreas and Jonathan Segal. 1994. Precise N-gram probabilities from stochastic context-free grammars. In Proceedings of the 32nd Annual Meeting, pages 74-79, Las Cruces, NM. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Transition network grammars for natural language analysis.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>13--10</pages>
<contexts>
<context position="16383" citStr="Woods 1970" startWordPosition="2908" endWordPosition="2909">this incarnation of the approximation process as if they were terminals. 4.1 Superset Approximation Based on RTNs The following approximation was proposed in Nederhof (1997). The presentation here, however, differs substantially from the earlier publication, which treated the approximation process entirely on the level of context-free grammars: a self-embedding grammar was transformed in such a way that it was no longer self-embedding. A finite automaton was then obtained from the grammar by the algorithm discussed above. The presentation here is based on recursive transition networks (RTNs) (Woods 1970). We can see a context-free grammar as an RTN as follows: We introduce two states qA and q&apos;A for each nonterminal A, and m + 1 states qo, , q„, for each rule A —&gt; Xi ... Xm. The states for a rule A Xm are connected with each other and to the states for the left-hand side A by one transition (qA, Et q0), a transition qi) for each i such that 1 &lt; i &lt; m, and one transition (qm, e, q). (Actually, some epsilon transitions are avoided in our implementation, but we will not be concerned with such optimizations here.) In this way, we obtain a finite automaton with initial state qA and final state q&apos;A </context>
</contexts>
<marker>Woods, 1970</marker>
<rawString>Woods, W. A. 1970. Transition network grammars for natural language analysis. Communications of the ACM, 13(10):591-606.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>