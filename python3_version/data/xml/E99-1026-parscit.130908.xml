<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<note confidence="0.713828">
Proceedings of EACL &apos;99
</note>
<title confidence="0.9911115">
Japanese Dependency Structure Analysis
Based on Maximum Entropy Models
</title>
<author confidence="0.981513">
Kiyotalca Uchimotot Satoshi Sekine Hitoshi Isaharat
</author>
<affiliation confidence="0.993933">
tCommunications Research Laboratory
Ministry of Posts and Telecommunications
</affiliation>
<address confidence="0.725008">
588-2, Iwaoka, Iwaoka-cho, Nishi-ku
Kobe, Hyogo, 651-2401, Japan
[uchimot o I isahara]@crl.go . jp
New York University
715 Broadway, 7th floor
New York, NY 10003, USA
</address>
<email confidence="0.994823">
sekine@cs.nyu.edu
</email>
<sectionHeader confidence="0.998561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999762769230769">
This paper describes a dependency
structure analysis of Japanese sentences
based on the maximum entropy mod-
els. Our model is created by learning
the weights of some features from a train-
ing corpus to predict the dependency be-
tween bunsetsus or phrasal units. The
dependency accuracy of our system is
87.2% using the Kyoto University cor-
pus. We discuss the contribution of each
feature set and the relationship between
the number of training data and the ac-
curacy.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99764405">
Dependency structure analysis is one of the ba-
sic techniques in Japanese sentence analysis. The
Japanese dependency structure is usually repre-
sented by the relationship between phrasal units
called `bunsetsu.&apos; The analysis has two concep-
tual steps. In the first step, a dependency matrix
is prepared. Each element of the matrix repre-
sents how likely one bunsetsu is to depend on the
other. In the second step, an optimal set of de-
pendencies for the entire sentence is found. In
this paper, we will mainly discuss the first step, a
model for estimating dependency likelihood.
So far there have been two different approaches
to estimating the dependency likelihood. One is
the rule-based approach, in which the rules are
created by experts and likelihoods are calculated
by some means, including semiautomatic corpus-
based methods but also by manual assignment of
scores for rules. However, hand-crafted rules have
the following problems.
</bodyText>
<listItem confidence="0.960276538461538">
• They have a problem with their coverage. Be-
cause there are many features to find correct
dependencies, it is difficult to find them man-
ually.
• They also have a problem with their consis-
tency, since many of the features compete
with each other and humans cannot create
consistent rules or assign consistent scores.
• As syntactic characteristics differ across dif-
ferent domains, the rules have to be changed
when the target domain changes. It is costly
to create a new hand-made rule for each do-
main.
</listItem>
<bodyText confidence="0.999974142857143">
Adother approach is a fully automatic corpus-
based approach. This approach has the poten-
tial to overcome the problems of the rule-based
approach. It automatically learns the likelihoods
of dependencies from a tagged corpus and calcu-
lates the best dependencies for an input sentence.
We take this approach. This approach is taken by
some other systems (Collins, 1996; Fujio and Mat-
sumoto, 1998; Haruno et al., 1998). The parser
proposed by Ratnaparkhi (Ratnaparkhi, 1997) is
considered to be one of the most accurate parsers
in English. Its probability estimation is based on
the maximum entropy models. We also use the
maximum entropy model. This model learns the
weights of given features from a training corpus.
The weights are calculated based on the frequen-
cies of the features in the training data. The set of
features is defined by a human. In our model, we
use features of bunsetsu, such as character strings,
parts of speech, and inflection types of bunsetsu,
as well as information between bunsetsus, such as
the existence of punctuation, and the distance be-
tween bunsetsus. The probabilities of dependen-
cies are estimated from the model by using those
features in input sentences. We assume that the
overall dependencies in a whole sentence can be
determined as the product of the probabilities of
all the dependencies in the sentence.
</bodyText>
<page confidence="0.997366">
196
</page>
<bodyText confidence="0.9247885">
Proceedings of EACL &apos;99
Now, we briefly describe the algorithm of de-
pendency analysis. It is said that Japanese de-
pendencies have the following characteristics.
</bodyText>
<listItem confidence="0.980906">
(1) Dependencies are directed from left to right
(2) Dependencies do not cross
(3) A bunsetsu, except for the rightmost one, de-
pends on only one bunsetsu
(4) In many cases, the left context is not neces-
sary to determine a dependencyl
</listItem>
<bodyText confidence="0.9999546">
The analysis method proposed in this paper is de-
signed to utilize these features. Based on these
properties, we detect the dependencies in a sen-
tence by analyzing it backwards (from right to
left). In the past, such a backward algorithm has
been used with rule-based parsers (e.g., (Fujita,
1988)). We applied it to our statistically based
approach. Because of the statistical property, we
can incorporate a beam search, an effective way of
limiting the search space in a backward analysis.
</bodyText>
<sectionHeader confidence="0.994217" genericHeader="method">
2 The Probability Model
</sectionHeader>
<bodyText confidence="0.99994975">
Given a tokenization of a test corpus, the prob-
lem of dependency structure analysis in Japanese
can be reduced to the problem of assigning one
of two tags to each relationship which consists of
two bunsetsus. A relationship could be tagged as
&amp;quot;0&amp;quot; or &amp;quot;1&amp;quot; to indicate whether or not there is a
dependency between the bunsetsus, respectively.
The two tags form the space of &amp;quot;futures&amp;quot; for a
maximum entropy formulation of our dependency
problem between bunsetsus. A maximum entropy
solution to this, or any other similar problem al-
lows the computation of P(fili) for any f from the
space of possible futures, F, for every h from the
space of possible histories, H. A &amp;quot;history&amp;quot; in max-
imum entropy is all of the conditioning data which
enables you to make a decision among the space
of futures. In the dependency problem, we could
reformulate this in terms of finding the probabil-
ity of f associated with the relationship at index
tin the test corpus as:
</bodyText>
<equation confidence="0.988656">
P(flhs) = P(fI Information derivable
</equation>
<bodyText confidence="0.946778833333333">
from the test corpus
related to relationship 1)
The computation of P(f1h) in M.E. is depen-
dent on a set of &amp;quot;features&amp;quot; which, hopefully, are
helpful in making a prediction about the future.
Like most current M.E. modeling efforts in com-
putational linguistics, we restrict ourselves to fea-
tures which are binary functions of the history and
lAssumption (4) has not been discussed very much,
but our investigation with humans showed that it is
true in more than 90% of cases.
future. For instance, one of our features is
</bodyText>
<equation confidence="0.9631088">
if has(h, x) = ture,
= &amp;quot;Posterior - Head-
POS(Major) : fal(verb)&amp;quot; (1)
&amp; f = 1
otherwise.
</equation>
<bodyText confidence="0.994650090909091">
Here &amp;quot;has(h,x)&amp;quot; is a binary function which re-
turns true if the history h has an attribute x. We
focus on attributes on a bunsetsu itself and those
between bunsetsus. Section 3 will mention these
attributes.
Given a set of features and some training data,
the maximum entropy estimation process pro-
duces a model in which every feature gi has as-
sociated with it a parameter ai. This allows us
to compute the conditional probability as follows
(Berger et al., 1996):
</bodyText>
<equation confidence="0.99999">
P(f1h) —
Z(h) =
</equation>
<bodyText confidence="0.9977064">
The maximum entropy estimation technique
guarantees that for every feature gi, the expected
value of gi according to the M.E. model will equal
the empirical expectation of gi in the training cor-
pus. In other words:
</bodyText>
<equation confidence="0.998143333333333">
E f) • g,(h, f)
h, f
E ) • E PmE(flh ) • gi(h, f). (4)
</equation>
<bodyText confidence="0.999540428571429">
Here P is an empirical probability and PmE is
the probability assigned by the M.E. model.
We assume that dependencies in a sentence are
independent of each other and the overall depen-
dencies in a sentence can be determined based on
the product of probability of all dependencies in
the sentence.
</bodyText>
<sectionHeader confidence="0.997314" genericHeader="method">
3 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999991235294118">
In our experiment, we used the Kyoto University
text corpus (version 2) (Kurohashi and Nagao,
1997), a tagged corpus of the Mainichi newspaper.
For training we used 7,958 sentences from news-
paper articles appearing from January 1st to Jan-
uary 8th, and for testing we used 1,246 sentences
from articles appearing on January 9th. The input
sentences were morphologically analyzed and their
bunsetsus were identified. We assumed that this
preprocessing was done correctly before parsing
input sentences. If we used automatic morpholog-
ical analysis and bunsetsu identification, the pars-
ing accuracy would not decrease so much because
the rightmost element in a bunsetsu is usually a
case marker, a verb ending, or a adjective end-
ing, and each of these is easily recognized. The
automatic preprocessing by using public domain
</bodyText>
<equation confidence="0.547021">
g(h, f) 01
</equation>
<page confidence="0.938069">
197
</page>
<bodyText confidence="0.983381461538462">
Proceedings of EACL &apos;99
tools, for example, can achieve 97% for morpho-
logical analysis (Kitauchi et al., 1998) and 99% for
bunsetsu identification (Murata et al., 1998).
We employed the Maximum Entropy tool made
by Ristad (Ristad, 1998), which requires one to
specify the number of iterations for learning. We
set this number to 400 in all our experiments.
In the following sections, we show the features
used in our experiments and the results. Then we
describe some interesting statistics that we found
in our experiments. Finally, we compare our work
with some related systems.
</bodyText>
<subsectionHeader confidence="0.998687">
3.1 Results of Experiments
</subsectionHeader>
<bodyText confidence="0.975904237623763">
The features used in our experiments are listed in
Tables 1 and 2. Each row in Table 1 contains a
feature type, feature values, and an experimental
result that will be explained later. Each feature
consists of a type and a value. The features are
basically some attributes of a bunsetsu itself or
those between bunsetsus. We call them &apos;basic fea-
tures.&apos; The list is expanded from Haruno&apos;s list
(Haruno et al., 1998). The features in the list are
classified into five categories that are related to
the &amp;quot;Head&amp;quot; part of the anterior bunsetsu (cate-
gory &amp;quot;a&amp;quot;), the &amp;quot;Type&amp;quot; part of the anterior bun-
setsu (category &amp;quot;b&amp;quot;), the &amp;quot;Head&amp;quot; part of the pos-
terior bunsetsu (category &amp;quot;c&amp;quot;), the &amp;quot;Type&amp;quot; part
of the posterior bunsetsu (category &amp;quot;d&amp;quot;), and the
features between bunsetsus (category &amp;quot;e&amp;quot;) respec-
tively. The term &amp;quot;Head&amp;quot; basically means a right-
most content word in a bunsetsu, and the term
&amp;quot;Type&amp;quot; basically means a function word following
a &amp;quot;Head&amp;quot; word or an inflection type of a &amp;quot;Head&amp;quot;
word. The terms are defined in the following para-
graph. The features in Table 2 are combinations
of basic features (&apos;combined features&apos;). They are
represented by the corresponding category name
of basic features, and each feature set is repre-
sented by the feature numbers of the correspond-
ing basic features. They are classified into nine
categories we constructed manually. For exam-
ple, twin features are combinations of the features
related to the categories &amp;quot;b&amp;quot; and &amp;quot;c.&amp;quot; Triplet,
quadruplet and quintuplet features basically con-
sist of the twin features plus the features of the
remainder categories &amp;quot;a,&amp;quot; &amp;quot;d&amp;quot; and &amp;quot;e.&amp;quot; The to-
tal number of features is about 600,000. Among
them, 40,893 were observed in the training corpus,
and we used them in our experiment.
The terms used in the table are the following:
Anterior: left bunsetsu of the dependency
Posterior: right bunsetsu of the dependency
Head: the rightmost word in a bunsetsu other
than those whose major part-of-speech2 cat-
egory is &amp;quot;4* (special marks),&amp;quot; &amp;quot;MN (post-
positional particles),&amp;quot; or &amp;quot;4fMn (suffix)&amp;quot;
Part-of-speech categories follow those of JU-
MAN(Kurohashi and Nagao, 1998).
Head-Lex: the fundamental form (uninflected
form) of the head word. Only words with
a frequency of three or more are used.
Head-Inf: the inflection type of a head
Type: the rightmost word other than those
whose major part-of-speech category is &amp;quot;4
fk (special marks).&amp;quot; If the major category of
the word is neither &amp;quot;MN (post-positional par-
ticles)&amp;quot; nor &amp;quot;tows (suffix),&amp;quot; and the word is
inflectable3, then the type is represented by
the inflection type.
JOSHI1: the rightmost post-positional particle
in the bunsetsu
JOSHI2: the second rightmost post-positional
particle in the bunsetsu if there are two or
more post-positional particles in the bunsetsu
TOUTEN, WA: TOUTEN means if a comma
(Touten) exists in the bunsetsu. WA means
if the word WA (a topic marker) exists in the
bunsetsu
BW: BW means &amp;quot;between bunsetsus&amp;quot;
BW-Distance: the distance between the bunset-
sus
BW-TOUTEN: if TOUTEN exists between
bunsetsus
BW-IDto-Anterior-Type:
BW-IDto-Anterior-Type means if there is a
bunsetsu whose type is identical to that of
the anterior bunsetsu between bunsetsus
BW-IDto-Anterior-Type-Head-POS: the
part-of-speech category of the head word of
the bunsetsu of &amp;quot;BW-IDto-Anterior-Type&amp;quot;
BW-IDto-Posterior-Head: if there is between
bunsetsus a bunsetsu whose head is identical
to that of the posterior bunsetsu
BW-IDto-Posterior-Head-Type(String):
the lexical information of the bunsetsu &amp;quot;BW-
IDto-Posterior-Head&amp;quot;
The results of our experiment are listed in Ta-
ble 3. The dependency accuracy means the per-
centage of correct dependencies out of all depen-
dencies. The sentence accuracy means the per-
centage of sentences in which all dependencies
were analyzed correctly. We used input sentences
that had already been morphologically analyzed
and for which bunsetsus had been identified. The
first line in Table 3 (deterministic) shows the ac-
curacy achieved when the test sentences were an-
alyzed deterministically (beam width Jr = 1). The
second line in Table 3 (best beam search) shows
the best accuracy among the experiments when
changing the beam breadth k from 1 to 20. The
best accuracy was achieved when k = 11, although
the variation in accuracy was very small. This re-
sult supports assumption (4) in Chapter 1 because
&apos;The inflection types follow those of JUMAN.
</bodyText>
<page confidence="0.992678">
198
</page>
<tableCaption confidence="0.6542005">
Proceedings of EACL &apos;99
Table 1: Features basic features
</tableCaption>
<table confidence="0.999577618181818">
Basic features (5 categories, 43 types)
Category Feature Feature type Feature values ... (Number of values) Accuracy without
number each feature
1 Anterior-Head-Lex (2204) 86.98% (-0.16%)
2 Anterior-Head-POS(Major) WA+ (verb), •)*V44 (adjective), ZPJ (noun), ... (11) 86.43% (-0.71%)
a 3 Anterior-Head-POS(Minor) *ift5=4 (common noun), NCR (quantifier), ... (24)
4 Anterior-Head-Inf(Major) Sigirg (vowel verb), ... (30) 87.14% (±0%)
5 Anterior-Head-Inf(Minor) ,IF:,* (stem), M„:2M (fundamental form), ... (60)
6 Anterior-Type(String) Ct, t a, -t- 1..-C, telt , IC, 4), • • - (73) 69.73% (-17.41%)
7 Anterior-Type(Major) Itn. (post-positional particle), ... (43)
8 Anterior-Type(Minor) 41tral (case marker), 0.. (imperative form)
... (102)
9 Anterior-JOSHI1(String) hs‘7, t &amp;quot;t, (0;2‘., ^, *15,, • • - (63) 87.11% (-0.03%)
b 10 Anterior-JOSHI1(Minor) [nil], 46,IVIM (case marker), ... (5)
11 Anterior-JOSHI2(String) efE, tt, +, X, 21&apos;, • • • (63) 87.08% (-0.06%)
12 Anterior-JOSHI2(Minor) gsVA (case marker), ... (4)
13 Anterior-punctuation [nil, comma, period (3) 85.47% -1.67%
14 Anterior-bracket-open [nil], I , &apos; , C - , 1, &lt;, r, 4, ... (14) 87.12% -0.02%
15 Anterior-bracket-close [nil], _1 , &apos; , ) , - , J , &gt;, J , , ... (14) 87.10% -0.04%
16 Posterior-Head-Lex The same values as those of feature number 1. 86.31% -0.83%
17 Posterior-Head-POS(Major) The same values as those of feature number 2. 76.15% -10.99 0)
c 18 Posterior-Head-POS(Minor) The same values as those of feature number 3.
19 Posterior-Head-Inf(Major) The same values as those of feature number 4. 87.14% (±-0%)
20 Posterior-Head-Inf(Minor) The same values as those of feature number 5.
21 Posterior-Type(String) The same values as those of feature number 6. 86.06% (-1.08%)
22 Posterior-Type(Major) The same values as those of feature number 7.
23 Posterior-Type(Minor) The same values as those of feature number 8.
24 Posterior-JOSHIl(String) The same values as those of feature number 9. 87.16% (+0.02%)
d 25 Posterior-JOSHIl(Minor) The same values as those of feature number 10.
26 Posterior-JOSHI2(String) The same values as those of feature number 11. 87.11% (-0.03%)
27 Posterior-JOSHI2(Minor) The same values as those of feature number 12.
28 Posterior-punctuation The same values as those of feature number 13. 84.62% -2.52%)
29 Posterior-bracket-open The same values as those of feature number 14. 86.87% -0.27%)
30 Posterior-bracket-close The same values as those of feature number 15. 86.85% -0.29%)
31 BW-Distance A(1), B(2 --- 5, C(6 or more) (3) 84.64% -2.50%
32 BW-TO()TEN nilj, [exist] (2 86.81% -0.33%
33 BW-WA ,nil , xist] (2 86.96% -0.18%
{e
34 BW-brackets nil l , close, open, open-close (4) 86.08% -1.06%
35 BW-IDto-Anterior- type nil , [exist.] (2) 86.99% -0.15%
36 BW-IDto-Anterior-Type- The same values as those of feature number 2.
Head-POS(Major)
37 BW-IDto-Anterior-Type- The same values as those of feature number 3.
Head-POS(Minor)
e 38 BW-IDto-Anterior-Type- The same values as those of feature number 4.
Head-Inf(Major)
39 BW-IDto-Anterior-Type- The same values as those of feature number 5.
Head-Inf(Minor)
40 BW-IDto-Posterior-Head truli, lexistj (2) 86.75% (-0.39%)
41 BW-IDto-Posterior-Head- The same values as those of feature number 6.
Type(String)
42 BW-IDto-Posterior-Head- The same values as those of feature number 7.
Type(Major)
43 BW-IDto-Posterior-Head- The same values as those of feature number 8.
Type(Minor)
</table>
<tableCaption confidence="0.960136">
Table 2: Features combined features
</tableCaption>
<bodyText confidence="0.98733252173913">
Combined features (9 categories, 134 types)
Combination type Combinations Accuracy without
Category Feature set the feature
Twin features: (b, c) b = {6, 7, 81, c = {16, 17, 181 86.99% (-0.15%)
related to the &amp;quot;Type&amp;quot; part of
the anterior bunsetsu and the
&amp;quot;Head&amp;quot; part of the posterior
bunsetsu.
Triplet features: (b1, b2, c) (135, b2) = {(9, 11), (10, 12)}, c = {17, 18} 86.47% (-0.67%)
basically consist of the twin (b, c, e) b = {6, 7, 8}, c = {17, 18},
features plus the features e = {31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43}
between bunsetsus. (d1, d2, e) (d1, d1, e) = (29, 30, 34)
Quadruplet features: (b1, b2, c, d) b1 = {6, 7,8), c = {17, 18), (b2, d) = (13, 28) 85.65% (-1.49%)
basically consist of the twin (b, c, el, e2) b = {6, 7, 8}, c = {17, 18}, (el, e2) = (35, 40)
features plus the features (a, b, c, d) (a, c) = {(1, 16), (2, 17), (3, 18)1,
related to the &amp;quot;Head&amp;quot; part of (b, d) = {(6, 21), (7, 22), (8, 23)}
the anterior bunsetsu, and the
&amp;quot;Type&amp;quot; part of the posterior
bunsetsu.
Quintuplet features: (a, b1, b2, c, d) (a, c) = {(2, 17), (3, 18)}, 86.96% (-0.18%)
basically consist of the (b1, b2) = {(9, 11), (10, 12)), d = {21,22,23}
quadruplet features plus the (a, b, c, d, e) (a, c) = {(1, 16), (2, 17), (3, 18)},
features between bunsetsus. (b, d) = {(6, 21), (7, 22), (8, 23), e = 31
</bodyText>
<page confidence="0.998289">
199
</page>
<tableCaption confidence="0.939292">
Proceedings of EACL &apos;99
Table 3: Results of dependencyanalysis
</tableCaption>
<table confidence="0.696304">
Dependency accuracy Sentence accuracy
Deterministic (k = 1) 87.14% (9814/11263) 40.60% (503/1239)
Best beam search(k = 11) 87.21% (9822/11263) 40.60% (503/1239)
Baseline 64.09% (7219/11263) 6.38% ( 79/1239)
</table>
<figure confidence="0.987597">
1.0 -
0.8714
0.8 -
0.6 -
Dependency accuracy
0.4 -
0.2 -
0
10 20 30
Number of bunsetsus in a sentence
</figure>
<figureCaption confidence="0.989773">
Figure 1: Relationship between the number of bunsetsus in a sentence and dependency accuracy.
</figureCaption>
<bodyText confidence="0.641073142857143">
it shows that the previous context has almost no
effect on the accuracy. The last line in Table 3 rep-
resents the accuracy when we assumed that every
bunsetsu depended on the next one (baseline).
Figure 1 shows the relationship between the
sentence length (the number of bunsetsus) and
the dependency accuracy. The data for sentences
longer than 28 segments are not shown, because
there was at most one sentence of each length.
Figure 1 shows that the accuracy degradation due
to increasing sentence length is not significant.
For the entire test corpus the average running time
on a SUN Sparc Station 20 was 0.08 seconds per
sentence.
</bodyText>
<subsectionHeader confidence="0.993284">
3.2 Features and Accuracy
</subsectionHeader>
<bodyText confidence="0.9999465">
This section describes how much each feature set
contributes to improve the accuracy.
The rightmost column in Tables 1 and 2 shows
the performance of the analysis without each fea-
ture set. In parenthesis, the percentage of im-
provement or degradation to the formal experi-
ment is shown. In the experiments, when a basic
feature was deleted, the combined features that
included the basic feature were also deleted.
We also conducted some experiments in which
several types of features were deleted together.
The results are shown in Table 4. All of the results
in the experiments were carried out deterministi-
cally (beam width k = 1).
The results shown in Table 1 were very close
to our expectation. The most useful features are
the type of the anterior bunsetsu and the part-
of-speech tag of the head word on the posterior
bunsetsu. Next important features are the dis-
tance between bunsetsus, the existence of punctu-
ation in the bunsetsu, and the existence of brack-
ets. These results indicate preferential rules with
respect to the features.
The accuracy obtained with the lexical fea-
tures of the head word was better than that
without them. In the experiment with the fea-
tures, we found many idiomatic expressions, for
example, &amp;quot;11;-C (oujite, according to)— 1A.N5 6
(kimeru, decide)&amp;quot; and &amp;quot;3V-C. (katachi_de, in the
form of)— HMI 6 (okonawareru, be held).&amp;quot; We
would expect to collect more of such expressions
if we use more training data.
The experiments without some combined fea-
tures are reported in Tables 2 and 4. As can
be seen from the results, the combined features
are very useful to improve the accuracy. We used
these combined features in addition to the basic
features because we thought that the basic fea-
tures were actually related to each other. With-
out the combined features, the features are inde-
pendent of each other in the maximum entropy
framework.
We manually selected combined features, which
are shown in Table 2. If we had used all combi-
</bodyText>
<page confidence="0.996896">
200
</page>
<tableCaption confidence="0.931071">
Proceedings of EACL &apos;99
Table 4: Accuracy without several types of features
</tableCaption>
<table confidence="0.948126333333333">
Features Accuracy
Without features 1 and 16 (lexical information about the head word) 86.30% (-0.84%)
Without features 35 to 43 86.83% (-0.31%)
Without quadruplet and quintuplet features 84.27% (-2.87%)
Without triplet, quadruplet, and quintuplet features 81.28% (-5.86%)
Without all combinations 68.83% (-18.31%)
</table>
<bodyText confidence="0.999946689655173">
nations, the number of combined features would
have been very large, and the training would
not have been completed on the available ma-
chine. Furthermore, we found that the accuracy
decreased when several new features were added
in our preliminary experiments. So, we should
not use all combinations of the basic features. We
selected the combined features based on our intu-
ition.
In our future work, we believe some methods
for automatic feature selection should be studied.
One of the simplest ways of selecting features is
to select features according to their frequencies in
the training corpus. But using this method in our
current experiments, the accuracy decreased in all
of the experiments. Other methods that have been
proposed are one based on using the gain (Berger
et al., 1996) and an approximate method for se-
lecting informative features (Shirai et al., 1998a),
and several criteria for feature selection were pro-
posed and compared with other criteria (Berger
and Printz, 1998). We would like to try these
methods.
Investigating the sentences which could not be
analyzed correctly, we found that many of those
sentences included coordinate structures. We be-
lieve that coordinate structures can be detected to
a certain extent by considering new features which
take a wide range of information into account.
</bodyText>
<subsectionHeader confidence="0.9814965">
3.3 Number of Training Data and
Accuracy
</subsectionHeader>
<bodyText confidence="0.999924818181818">
Figure 2 shows the relationship between the num-
ber of training data (the number of sentences) and
the accuracy. This figure shows dependency accu-
racies for the training corpus and the test corpus.
Accuracy of 81.84% was achieved even with a very
small training set (250 sentences). We believe that
this is due to the strong characteristic of the max-
imum entropy framework to the data sparseness
problem. From the learning curve, we can expect
a certain amount of improvement if we have more
training data.
</bodyText>
<subsectionHeader confidence="0.998896">
3.4 Comparison with Related Works
</subsectionHeader>
<bodyText confidence="0.976537666666667">
This section compares our work with related
statistical dependency structure analyses in
Japanese.
</bodyText>
<subsectionHeader confidence="0.455178">
Comparison with
</subsectionHeader>
<bodyText confidence="0.995128619047619">
Shirai&apos;s work (Shirai et al., 1998b)
Shirai proposed a framework of statistical lan-
guage modeling using several corpora: the EDR
corpus, RWC corpus, and Kyoto University cor-
pus. He combines a parser based on a hand-made
CFG and a probabilistic dependency model. He
also used the maximum entropy model to estimate
the dependency probabilities between two or three
post-positional particles and a verb. Accuracy of
84.34% was achieved using 500 test sentences of
length 7 to 9 bunsetsus. In both his and our ex-
periments, the input sentences were morphologi-
cally analyzed and their bunsetsus were identified.
The comparison of the results cannot strictly be
done because the conditions were different. How-
ever, it should be noted that the accuracy achieved
by our model using sentences of the same length
was about 3% higher than that of Shirai&apos;s model,
although we used a much smaller set of training
data. We believe that it is because his approach
is based on a hand-made CFG.
</bodyText>
<sectionHeader confidence="0.404559" genericHeader="method">
Comparison with Ehara&apos;s work (Ehara, 1998)
</sectionHeader>
<bodyText confidence="0.988290291666667">
Ehara also used the Maximum Entropy model,
and a set of similar kinds of features to ours. How-
ever, there is a big difference in the number of fea-
tures between Ehara&apos;s model and ours. Besides
the difference in the number of basic features,
Ehara uses only the combination of two features,
but we also use triplet, quadruplet, and quintuplet
features. As shown in Section 3.2, the accuracy in-
creased more than 5% using triplet or larger com-
binations. We believe that the difference in the
combination features between Ehara&apos;s model and
ours may have led to the difference in the accuracy.
The accuracy of his system was about 10% lower
than ours. Note that Ehara used TV news articles
for training and testing, which are different from
our corpus. The average sentence length in those
articles was 17.8, much longer than that (average:
10.0) in the Kyoto University text corpus.
Comparison with
Fujio&apos;s work (Fujio and Matsumoto, 1998)
and Haruno&apos;s work (Haruno et al., 1998)
Fujio used the Maximum Likelihood model
with similar features to our model in his parser.
Haruno proposed a parser that uses decision tree
</bodyText>
<page confidence="0.989515">
201
</page>
<figure confidence="0.982282666666667">
Proceedings of EACL &apos;99
94
92
90
88
86
84
82
80
0
1000 2000 3000 4000 5000 6000 7000 8000
Number of Training Data (sentences)
Figure 2: Relationship between the number of training data and the parsing accuracy. (beam breadth
k =1)
&amp;quot;training&amp;quot; -0—
&amp;quot;testing&amp;quot;
............. ............ ............ ------------
Parsing Accuracy (0/0)
</figure>
<bodyText confidence="0.99981496">
models and a boosting method. It is difficult to
directly compare these models with ours because
they use a different corpus, the EDR corpus which
is ten times as large as our corpus, for training
and testing, and the way of collecting test data
is also different. But they reported an accuracy
of around 85%, which is slightly worse than our
model.
We carried out two experiments using almost
the same attributes as those used in their exper-
iments. The results are shown in Table 5, where
the lines &amp;quot;Feature set(1)&amp;quot; and &amp;quot;Feature set(2)&amp;quot;
show the accuracies achieved by using Fujio&apos;s
attributes and Haruno&apos;s attributes respectively.
Considering that both results are around 85% to
86%, which is about the same as ours. From these
experiments, we believe that the important factor
in the statistical approaches is not the model, i.e.
Maximum Entropy, Maximum Likelihood, or De-
cision Tree, but the feature selection. However,
it may be interesting to compare these models
in terms of the number of training data, as we
can imagine that some models are better at cop-
ing with the data sparseness problem than others.
This is our future work.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999991391304348">
This paper described a Japanese dependency
structure analysis based on the maximum en-
tropy model. Our model is created by learning
the weights of some features from a training cor-
pus to predict the dependency between bunset-
sus or phrasal units. The probabilities of depen-
dencies between bunsetsus are estimated by this
model. The dependency accuracy of our system
was 87.2% using the Kyoto University corpus.
In our experiments without the feature sets
shown in Tables 1 and 2, we found that some basic
and combined features strongly contribute to im-
prove the accuracy. Investigating the relationship
between the number of training data and the accu-
racy, we found that good accuracy can be achieved
even with a very small set of training data. We
believe that the maximum entropy framework has
suitable characteristics for overcoming the data
sparseness problem.
There are several future directions. In particu-
lar, we are interested in how to deal with coordi-
nate structures, since that seems to be the largest
problem at the moment.
</bodyText>
<sectionHeader confidence="0.999329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996679647058824">
Adam Berger and Harry Printz. 1998. A com-
parison of criteria for maximum entropy / min-
imum divergence feature selection. Proceedings
of Third Conference on Empirical Methods in
Natural Language Processing, pages 97-106.
Adam L. Berger, Stephen A. Della Pietra, and
Vincent J. Della Pietra. 1996. A maximum en-
tropy approach to natural language processing.
Computational Linguistics, 22(1):39-71.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. Proceed-
ings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL),
pages 184-191.
Terumasa Ehara. 1998. Japanese bunsetsu de-
pendency estimation using maximum entropy
method. Proceedings of The Fourth Annual
</reference>
<page confidence="0.998006">
202
</page>
<tableCaption confidence="0.9297875">
Proceedings of EACL &apos;99
Table 5: Simulation of Fu io&apos;s and Haruno&apos;s experiments
</tableCaption>
<bodyText confidence="0.693728666666667">
_ Accuracy
Feature set
Feature set (1) 85.71% (-1.43%)
(Without features 4, 5, 9-12, 14, 15, 19, 20, 24-27, 29, 30, 34-43.)
Feature set (2) 86.47% (-0.67%)
(Without features 4, 5, 9-12, 19, 20, 24-27, 34-43.)
</bodyText>
<reference confidence="0.972775595744681">
Meeting of The Association for Natural Lan- ural Language Processing, 5(3):85-106. (in
guage Processing, pages 382-385. (in Japanese). Japanese).
Masakazu Fujio and Yuuji Matsumoto. 1998.
Japanese dependency structure analysis based
on lexicalized statistics. Proceedings of Third
Conference on Empirical Methods in Natural
Language Processing, pages 87-96.
Katsuhiko Fujita. 1988. A deterministic parser
based on karari-uke grammar. pages 399-402.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1998. Using decision trees to con-
struct a practical parser. Proceedings of the
COLING-ACL &apos;98.
Akira Kitauchi, Takehito Utsuro, and Yuji Mat-
sumoto. 1998. Error-driven model learning
of Japanese morphological analysis. IPSJ-
WGNL, NL124-6:41-48. (in Japanese).
Sadao Kurohashi and Makoto Nagao. 1997. Ky-
oto university text corpus project. pages 115-
118. (in Japanese).
Sadao Kurohashi and Makoto Nagao, 1998.
Japanese Morphological Analysis System JU-
MAN version 3.5. Department of Informatics,
Kyoto University.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma,
and Hitoshi Isahara. 1998. Machine learning
approach to bunsetsu identification — compar-
ison of decision tree, maximum entropy model,
example-based approach, and a new method us-
ing category-exclusive rules —. IPSJ-WGNL,
NL128-4:23-30. (in Japanese).
Adwait Ratnaparkhi. 1997. A linear observed
time statistical parser based on maximum en-
tropy models. Conference on Empirical Meth-
ods in Natural Language Processing.
Eric Sven Ristad. 1998. Maximum en-
tropy modeling toolkit, release 1.6 beta.
http://www.mnemonic.com/software/memt.
Kiyoaki Shirai, Kentaro Inui, Takenobu Toku-
naga, and Hozumi Tanaka. 1998a. Learning
dependencies between case frames using max-
imum entropy method. pages 356-359. (in
Japanese).
Kiyoaki Shirai, Kentaro Inui, Takenobu Toku-
naga, and Hozumi Tanaka. 1998b. A frame-
work of integrating syntactic and lexical statis-
tics in statistical parsing. Journal of Nat-
</reference>
<page confidence="0.999173">
203
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.353267">
<note confidence="0.840229">Proceedings of EACL &apos;99</note>
<title confidence="0.9958695">Japanese Dependency Structure Analysis Based on Maximum Entropy Models</title>
<author confidence="0.978627">Kiyotalca Uchimotot Satoshi Sekine Hitoshi Isaharat</author>
<affiliation confidence="0.997213">tCommunications Research Laboratory Ministry of Posts and Telecommunications</affiliation>
<address confidence="0.979903">588-2, Iwaoka, Iwaoka-cho, Nishi-ku Kobe, Hyogo, 651-2401, Japan</address>
<author confidence="0.616985">o I jp</author>
<affiliation confidence="0.999284">New York University</affiliation>
<address confidence="0.9985505">715 Broadway, 7th floor New York, NY 10003, USA</address>
<email confidence="0.999839">sekine@cs.nyu.edu</email>
<abstract confidence="0.980251571428571">This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models. Our model is created by learning the weights of some features from a training corpus to predict the dependency between bunsetsus or phrasal units. The dependency accuracy of our system is 87.2% using the Kyoto University corpus. We discuss the contribution of each feature set and the relationship between the number of training data and the accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Harry Printz</author>
</authors>
<title>A comparison of criteria for maximum entropy / minimum divergence feature selection.</title>
<date>1998</date>
<booktitle>Proceedings of Third Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>97--106</pages>
<contexts>
<context position="22391" citStr="Berger and Printz, 1998" startWordPosition="3675" endWordPosition="3678">r intuition. In our future work, we believe some methods for automatic feature selection should be studied. One of the simplest ways of selecting features is to select features according to their frequencies in the training corpus. But using this method in our current experiments, the accuracy decreased in all of the experiments. Other methods that have been proposed are one based on using the gain (Berger et al., 1996) and an approximate method for selecting informative features (Shirai et al., 1998a), and several criteria for feature selection were proposed and compared with other criteria (Berger and Printz, 1998). We would like to try these methods. Investigating the sentences which could not be analyzed correctly, we found that many of those sentences included coordinate structures. We believe that coordinate structures can be detected to a certain extent by considering new features which take a wide range of information into account. 3.3 Number of Training Data and Accuracy Figure 2 shows the relationship between the number of training data (the number of sentences) and the accuracy. This figure shows dependency accuracies for the training corpus and the test corpus. Accuracy of 81.84% was achieved </context>
</contexts>
<marker>Berger, Printz, 1998</marker>
<rawString>Adam Berger and Harry Printz. 1998. A comparison of criteria for maximum entropy / minimum divergence feature selection. Proceedings of Third Conference on Empirical Methods in Natural Language Processing, pages 97-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="6648" citStr="Berger et al., 1996" startWordPosition="1101" endWordPosition="1104">n more than 90% of cases. future. For instance, one of our features is if has(h, x) = ture, = &amp;quot;Posterior - HeadPOS(Major) : fal(verb)&amp;quot; (1) &amp; f = 1 otherwise. Here &amp;quot;has(h,x)&amp;quot; is a binary function which returns true if the history h has an attribute x. We focus on attributes on a bunsetsu itself and those between bunsetsus. Section 3 will mention these attributes. Given a set of features and some training data, the maximum entropy estimation process produces a model in which every feature gi has associated with it a parameter ai. This allows us to compute the conditional probability as follows (Berger et al., 1996): P(f1h) — Z(h) = The maximum entropy estimation technique guarantees that for every feature gi, the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus. In other words: E f) • g,(h, f) h, f E ) • E PmE(flh ) • gi(h, f). (4) Here P is an empirical probability and PmE is the probability assigned by the M.E. model. We assume that dependencies in a sentence are independent of each other and the overall dependencies in a sentence can be determined based on the product of probability of all dependencies in the sentence. 3 Experiments an</context>
<context position="22190" citStr="Berger et al., 1996" startWordPosition="3644" endWordPosition="3647">e accuracy decreased when several new features were added in our preliminary experiments. So, we should not use all combinations of the basic features. We selected the combined features based on our intuition. In our future work, we believe some methods for automatic feature selection should be studied. One of the simplest ways of selecting features is to select features according to their frequencies in the training corpus. But using this method in our current experiments, the accuracy decreased in all of the experiments. Other methods that have been proposed are one based on using the gain (Berger et al., 1996) and an approximate method for selecting informative features (Shirai et al., 1998a), and several criteria for feature selection were proposed and compared with other criteria (Berger and Printz, 1998). We would like to try these methods. Investigating the sentences which could not be analyzed correctly, we found that many of those sentences included coordinate structures. We believe that coordinate structures can be detected to a certain extent by considering new features which take a wide range of information into account. 3.3 Number of Training Data and Accuracy Figure 2 shows the relations</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>184--191</pages>
<contexts>
<context position="2700" citStr="Collins, 1996" startWordPosition="428" endWordPosition="429">ns cannot create consistent rules or assign consistent scores. • As syntactic characteristics differ across different domains, the rules have to be changed when the target domain changes. It is costly to create a new hand-made rule for each domain. Adother approach is a fully automatic corpusbased approach. This approach has the potential to overcome the problems of the rule-based approach. It automatically learns the likelihoods of dependencies from a tagged corpus and calculates the best dependencies for an input sentence. We take this approach. This approach is taken by some other systems (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998). The parser proposed by Ratnaparkhi (Ratnaparkhi, 1997) is considered to be one of the most accurate parsers in English. Its probability estimation is based on the maximum entropy models. We also use the maximum entropy model. This model learns the weights of given features from a training corpus. The weights are calculated based on the frequencies of the features in the training data. The set of features is defined by a human. In our model, we use features of bunsetsu, such as character strings, parts of speech, and inflection types of bunsets</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), pages 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terumasa Ehara</author>
</authors>
<title>Japanese bunsetsu dependency estimation using maximum entropy method.</title>
<date>1998</date>
<booktitle>Proceedings of The Fourth Annual Meeting of The Association for Natural Lan- ural Language Processing, 5(3):85-106. (in guage Processing,</booktitle>
<pages>382--385</pages>
<note>(in Japanese). Japanese).</note>
<contexts>
<context position="24436" citStr="Ehara, 1998" startWordPosition="4011" endWordPosition="4012">cy of 84.34% was achieved using 500 test sentences of length 7 to 9 bunsetsus. In both his and our experiments, the input sentences were morphologically analyzed and their bunsetsus were identified. The comparison of the results cannot strictly be done because the conditions were different. However, it should be noted that the accuracy achieved by our model using sentences of the same length was about 3% higher than that of Shirai&apos;s model, although we used a much smaller set of training data. We believe that it is because his approach is based on a hand-made CFG. Comparison with Ehara&apos;s work (Ehara, 1998) Ehara also used the Maximum Entropy model, and a set of similar kinds of features to ours. However, there is a big difference in the number of features between Ehara&apos;s model and ours. Besides the difference in the number of basic features, Ehara uses only the combination of two features, but we also use triplet, quadruplet, and quintuplet features. As shown in Section 3.2, the accuracy increased more than 5% using triplet or larger combinations. We believe that the difference in the combination features between Ehara&apos;s model and ours may have led to the difference in the accuracy. The accurac</context>
</contexts>
<marker>Ehara, 1998</marker>
<rawString>Terumasa Ehara. 1998. Japanese bunsetsu dependency estimation using maximum entropy method. Proceedings of The Fourth Annual Meeting of The Association for Natural Lan- ural Language Processing, 5(3):85-106. (in guage Processing, pages 382-385. (in Japanese). Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masakazu Fujio</author>
<author>Yuuji Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on lexicalized statistics.</title>
<date>1998</date>
<booktitle>Proceedings of Third Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>87--96</pages>
<contexts>
<context position="2727" citStr="Fujio and Matsumoto, 1998" startWordPosition="430" endWordPosition="434">e consistent rules or assign consistent scores. • As syntactic characteristics differ across different domains, the rules have to be changed when the target domain changes. It is costly to create a new hand-made rule for each domain. Adother approach is a fully automatic corpusbased approach. This approach has the potential to overcome the problems of the rule-based approach. It automatically learns the likelihoods of dependencies from a tagged corpus and calculates the best dependencies for an input sentence. We take this approach. This approach is taken by some other systems (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998). The parser proposed by Ratnaparkhi (Ratnaparkhi, 1997) is considered to be one of the most accurate parsers in English. Its probability estimation is based on the maximum entropy models. We also use the maximum entropy model. This model learns the weights of given features from a training corpus. The weights are calculated based on the frequencies of the features in the training data. The set of features is defined by a human. In our model, we use features of bunsetsu, such as character strings, parts of speech, and inflection types of bunsetsu, as well as information b</context>
<context position="25371" citStr="Fujio and Matsumoto, 1998" startWordPosition="4168" endWordPosition="4171">plet, quadruplet, and quintuplet features. As shown in Section 3.2, the accuracy increased more than 5% using triplet or larger combinations. We believe that the difference in the combination features between Ehara&apos;s model and ours may have led to the difference in the accuracy. The accuracy of his system was about 10% lower than ours. Note that Ehara used TV news articles for training and testing, which are different from our corpus. The average sentence length in those articles was 17.8, much longer than that (average: 10.0) in the Kyoto University text corpus. Comparison with Fujio&apos;s work (Fujio and Matsumoto, 1998) and Haruno&apos;s work (Haruno et al., 1998) Fujio used the Maximum Likelihood model with similar features to our model in his parser. Haruno proposed a parser that uses decision tree 201 Proceedings of EACL &apos;99 94 92 90 88 86 84 82 80 0 1000 2000 3000 4000 5000 6000 7000 8000 Number of Training Data (sentences) Figure 2: Relationship between the number of training data and the parsing accuracy. (beam breadth k =1) &amp;quot;training&amp;quot; -0— &amp;quot;testing&amp;quot; ............. ............ ............ ------------ Parsing Accuracy (0/0) models and a boosting method. It is difficult to directly compare these models with </context>
</contexts>
<marker>Fujio, Matsumoto, 1998</marker>
<rawString>Masakazu Fujio and Yuuji Matsumoto. 1998. Japanese dependency structure analysis based on lexicalized statistics. Proceedings of Third Conference on Empirical Methods in Natural Language Processing, pages 87-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhiko Fujita</author>
</authors>
<title>A deterministic parser based on karari-uke grammar.</title>
<date>1988</date>
<pages>399--402</pages>
<contexts>
<context position="4376" citStr="Fujita, 1988" startWordPosition="709" endWordPosition="710">nalysis. It is said that Japanese dependencies have the following characteristics. (1) Dependencies are directed from left to right (2) Dependencies do not cross (3) A bunsetsu, except for the rightmost one, depends on only one bunsetsu (4) In many cases, the left context is not necessary to determine a dependencyl The analysis method proposed in this paper is designed to utilize these features. Based on these properties, we detect the dependencies in a sentence by analyzing it backwards (from right to left). In the past, such a backward algorithm has been used with rule-based parsers (e.g., (Fujita, 1988)). We applied it to our statistically based approach. Because of the statistical property, we can incorporate a beam search, an effective way of limiting the search space in a backward analysis. 2 The Probability Model Given a tokenization of a test corpus, the problem of dependency structure analysis in Japanese can be reduced to the problem of assigning one of two tags to each relationship which consists of two bunsetsus. A relationship could be tagged as &amp;quot;0&amp;quot; or &amp;quot;1&amp;quot; to indicate whether or not there is a dependency between the bunsetsus, respectively. The two tags form the space of &amp;quot;futures&amp;quot; </context>
</contexts>
<marker>Fujita, 1988</marker>
<rawString>Katsuhiko Fujita. 1988. A deterministic parser based on karari-uke grammar. pages 399-402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiko Haruno</author>
<author>Satoshi Shirai</author>
<author>Yoshifumi Ooyama</author>
</authors>
<title>Using decision trees to construct a practical parser.</title>
<date>1998</date>
<booktitle>Proceedings of the COLING-ACL &apos;98.</booktitle>
<contexts>
<context position="2749" citStr="Haruno et al., 1998" startWordPosition="435" endWordPosition="438">n consistent scores. • As syntactic characteristics differ across different domains, the rules have to be changed when the target domain changes. It is costly to create a new hand-made rule for each domain. Adother approach is a fully automatic corpusbased approach. This approach has the potential to overcome the problems of the rule-based approach. It automatically learns the likelihoods of dependencies from a tagged corpus and calculates the best dependencies for an input sentence. We take this approach. This approach is taken by some other systems (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998). The parser proposed by Ratnaparkhi (Ratnaparkhi, 1997) is considered to be one of the most accurate parsers in English. Its probability estimation is based on the maximum entropy models. We also use the maximum entropy model. This model learns the weights of given features from a training corpus. The weights are calculated based on the frequencies of the features in the training data. The set of features is defined by a human. In our model, we use features of bunsetsu, such as character strings, parts of speech, and inflection types of bunsetsu, as well as information between bunsetsus, such</context>
<context position="9119" citStr="Haruno et al., 1998" startWordPosition="1519" endWordPosition="1522">es used in our experiments and the results. Then we describe some interesting statistics that we found in our experiments. Finally, we compare our work with some related systems. 3.1 Results of Experiments The features used in our experiments are listed in Tables 1 and 2. Each row in Table 1 contains a feature type, feature values, and an experimental result that will be explained later. Each feature consists of a type and a value. The features are basically some attributes of a bunsetsu itself or those between bunsetsus. We call them &apos;basic features.&apos; The list is expanded from Haruno&apos;s list (Haruno et al., 1998). The features in the list are classified into five categories that are related to the &amp;quot;Head&amp;quot; part of the anterior bunsetsu (category &amp;quot;a&amp;quot;), the &amp;quot;Type&amp;quot; part of the anterior bunsetsu (category &amp;quot;b&amp;quot;), the &amp;quot;Head&amp;quot; part of the posterior bunsetsu (category &amp;quot;c&amp;quot;), the &amp;quot;Type&amp;quot; part of the posterior bunsetsu (category &amp;quot;d&amp;quot;), and the features between bunsetsus (category &amp;quot;e&amp;quot;) respectively. The term &amp;quot;Head&amp;quot; basically means a rightmost content word in a bunsetsu, and the term &amp;quot;Type&amp;quot; basically means a function word following a &amp;quot;Head&amp;quot; word or an inflection type of a &amp;quot;Head&amp;quot; word. The terms are defined in the follow</context>
<context position="25411" citStr="Haruno et al., 1998" startWordPosition="4175" endWordPosition="4178">shown in Section 3.2, the accuracy increased more than 5% using triplet or larger combinations. We believe that the difference in the combination features between Ehara&apos;s model and ours may have led to the difference in the accuracy. The accuracy of his system was about 10% lower than ours. Note that Ehara used TV news articles for training and testing, which are different from our corpus. The average sentence length in those articles was 17.8, much longer than that (average: 10.0) in the Kyoto University text corpus. Comparison with Fujio&apos;s work (Fujio and Matsumoto, 1998) and Haruno&apos;s work (Haruno et al., 1998) Fujio used the Maximum Likelihood model with similar features to our model in his parser. Haruno proposed a parser that uses decision tree 201 Proceedings of EACL &apos;99 94 92 90 88 86 84 82 80 0 1000 2000 3000 4000 5000 6000 7000 8000 Number of Training Data (sentences) Figure 2: Relationship between the number of training data and the parsing accuracy. (beam breadth k =1) &amp;quot;training&amp;quot; -0— &amp;quot;testing&amp;quot; ............. ............ ............ ------------ Parsing Accuracy (0/0) models and a boosting method. It is difficult to directly compare these models with ours because they use a different corpus</context>
</contexts>
<marker>Haruno, Shirai, Ooyama, 1998</marker>
<rawString>Masahiko Haruno, Satoshi Shirai, and Yoshifumi Ooyama. 1998. Using decision trees to construct a practical parser. Proceedings of the COLING-ACL &apos;98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Kitauchi</author>
<author>Takehito Utsuro</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Error-driven model learning of Japanese morphological analysis.</title>
<date>1998</date>
<journal>IPSJWGNL,</journal>
<pages>124--6</pages>
<note>(in Japanese).</note>
<contexts>
<context position="8207" citStr="Kitauchi et al., 1998" startWordPosition="1365" endWordPosition="1368">th. The input sentences were morphologically analyzed and their bunsetsus were identified. We assumed that this preprocessing was done correctly before parsing input sentences. If we used automatic morphological analysis and bunsetsu identification, the parsing accuracy would not decrease so much because the rightmost element in a bunsetsu is usually a case marker, a verb ending, or a adjective ending, and each of these is easily recognized. The automatic preprocessing by using public domain g(h, f) 01 197 Proceedings of EACL &apos;99 tools, for example, can achieve 97% for morphological analysis (Kitauchi et al., 1998) and 99% for bunsetsu identification (Murata et al., 1998). We employed the Maximum Entropy tool made by Ristad (Ristad, 1998), which requires one to specify the number of iterations for learning. We set this number to 400 in all our experiments. In the following sections, we show the features used in our experiments and the results. Then we describe some interesting statistics that we found in our experiments. Finally, we compare our work with some related systems. 3.1 Results of Experiments The features used in our experiments are listed in Tables 1 and 2. Each row in Table 1 contains a feat</context>
</contexts>
<marker>Kitauchi, Utsuro, Matsumoto, 1998</marker>
<rawString>Akira Kitauchi, Takehito Utsuro, and Yuji Matsumoto. 1998. Error-driven model learning of Japanese morphological analysis. IPSJWGNL, NL124-6:41-48. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Kyoto university text corpus project.</title>
<date>1997</date>
<pages>115--118</pages>
<note>(in Japanese).</note>
<contexts>
<context position="7360" citStr="Kurohashi and Nagao, 1997" startWordPosition="1229" endWordPosition="1232">eature gi, the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus. In other words: E f) • g,(h, f) h, f E ) • E PmE(flh ) • gi(h, f). (4) Here P is an empirical probability and PmE is the probability assigned by the M.E. model. We assume that dependencies in a sentence are independent of each other and the overall dependencies in a sentence can be determined based on the product of probability of all dependencies in the sentence. 3 Experiments and Discussion In our experiment, we used the Kyoto University text corpus (version 2) (Kurohashi and Nagao, 1997), a tagged corpus of the Mainichi newspaper. For training we used 7,958 sentences from newspaper articles appearing from January 1st to January 8th, and for testing we used 1,246 sentences from articles appearing on January 9th. The input sentences were morphologically analyzed and their bunsetsus were identified. We assumed that this preprocessing was done correctly before parsing input sentences. If we used automatic morphological analysis and bunsetsu identification, the parsing accuracy would not decrease so much because the rightmost element in a bunsetsu is usually a case marker, a verb </context>
</contexts>
<marker>Kurohashi, Nagao, 1997</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1997. Kyoto university text corpus project. pages 115-118. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<date>1998</date>
<booktitle>Japanese Morphological Analysis System JUMAN version 3.5.</booktitle>
<institution>Department of Informatics, Kyoto University.</institution>
<contexts>
<context position="10816" citStr="Kurohashi and Nagao, 1998" startWordPosition="1794" endWordPosition="1798">tures basically consist of the twin features plus the features of the remainder categories &amp;quot;a,&amp;quot; &amp;quot;d&amp;quot; and &amp;quot;e.&amp;quot; The total number of features is about 600,000. Among them, 40,893 were observed in the training corpus, and we used them in our experiment. The terms used in the table are the following: Anterior: left bunsetsu of the dependency Posterior: right bunsetsu of the dependency Head: the rightmost word in a bunsetsu other than those whose major part-of-speech2 category is &amp;quot;4* (special marks),&amp;quot; &amp;quot;MN (postpositional particles),&amp;quot; or &amp;quot;4fMn (suffix)&amp;quot; Part-of-speech categories follow those of JUMAN(Kurohashi and Nagao, 1998). Head-Lex: the fundamental form (uninflected form) of the head word. Only words with a frequency of three or more are used. Head-Inf: the inflection type of a head Type: the rightmost word other than those whose major part-of-speech category is &amp;quot;4 fk (special marks).&amp;quot; If the major category of the word is neither &amp;quot;MN (post-positional particles)&amp;quot; nor &amp;quot;tows (suffix),&amp;quot; and the word is inflectable3, then the type is represented by the inflection type. JOSHI1: the rightmost post-positional particle in the bunsetsu JOSHI2: the second rightmost post-positional particle in the bunsetsu if there are tw</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>Sadao Kurohashi and Makoto Nagao, 1998. Japanese Morphological Analysis System JUMAN version 3.5. Department of Informatics, Kyoto University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Machine learning approach to bunsetsu identification — comparison of decision tree, maximum entropy model, example-based approach, and a new method using category-exclusive rules —.</title>
<date>1998</date>
<journal>IPSJ-WGNL,</journal>
<pages>128--4</pages>
<note>(in Japanese).</note>
<contexts>
<context position="8265" citStr="Murata et al., 1998" startWordPosition="1374" endWordPosition="1377">eir bunsetsus were identified. We assumed that this preprocessing was done correctly before parsing input sentences. If we used automatic morphological analysis and bunsetsu identification, the parsing accuracy would not decrease so much because the rightmost element in a bunsetsu is usually a case marker, a verb ending, or a adjective ending, and each of these is easily recognized. The automatic preprocessing by using public domain g(h, f) 01 197 Proceedings of EACL &apos;99 tools, for example, can achieve 97% for morphological analysis (Kitauchi et al., 1998) and 99% for bunsetsu identification (Murata et al., 1998). We employed the Maximum Entropy tool made by Ristad (Ristad, 1998), which requires one to specify the number of iterations for learning. We set this number to 400 in all our experiments. In the following sections, we show the features used in our experiments and the results. Then we describe some interesting statistics that we found in our experiments. Finally, we compare our work with some related systems. 3.1 Results of Experiments The features used in our experiments are listed in Tables 1 and 2. Each row in Table 1 contains a feature type, feature values, and an experimental result that </context>
</contexts>
<marker>Murata, Uchimoto, Ma, Isahara, 1998</marker>
<rawString>Masaki Murata, Kiyotaka Uchimoto, Qing Ma, and Hitoshi Isahara. 1998. Machine learning approach to bunsetsu identification — comparison of decision tree, maximum entropy model, example-based approach, and a new method using category-exclusive rules —. IPSJ-WGNL, NL128-4:23-30. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2805" citStr="Ratnaparkhi, 1997" startWordPosition="444" endWordPosition="445"> across different domains, the rules have to be changed when the target domain changes. It is costly to create a new hand-made rule for each domain. Adother approach is a fully automatic corpusbased approach. This approach has the potential to overcome the problems of the rule-based approach. It automatically learns the likelihoods of dependencies from a tagged corpus and calculates the best dependencies for an input sentence. We take this approach. This approach is taken by some other systems (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998). The parser proposed by Ratnaparkhi (Ratnaparkhi, 1997) is considered to be one of the most accurate parsers in English. Its probability estimation is based on the maximum entropy models. We also use the maximum entropy model. This model learns the weights of given features from a training corpus. The weights are calculated based on the frequencies of the features in the training data. The set of features is defined by a human. In our model, we use features of bunsetsu, such as character strings, parts of speech, and inflection types of bunsetsu, as well as information between bunsetsus, such as the existence of punctuation, and the distance betwe</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
</authors>
<title>Maximum entropy modeling toolkit, release 1.6</title>
<date>1998</date>
<note>beta. http://www.mnemonic.com/software/memt.</note>
<contexts>
<context position="8333" citStr="Ristad, 1998" startWordPosition="1387" endWordPosition="1388">correctly before parsing input sentences. If we used automatic morphological analysis and bunsetsu identification, the parsing accuracy would not decrease so much because the rightmost element in a bunsetsu is usually a case marker, a verb ending, or a adjective ending, and each of these is easily recognized. The automatic preprocessing by using public domain g(h, f) 01 197 Proceedings of EACL &apos;99 tools, for example, can achieve 97% for morphological analysis (Kitauchi et al., 1998) and 99% for bunsetsu identification (Murata et al., 1998). We employed the Maximum Entropy tool made by Ristad (Ristad, 1998), which requires one to specify the number of iterations for learning. We set this number to 400 in all our experiments. In the following sections, we show the features used in our experiments and the results. Then we describe some interesting statistics that we found in our experiments. Finally, we compare our work with some related systems. 3.1 Results of Experiments The features used in our experiments are listed in Tables 1 and 2. Each row in Table 1 contains a feature type, feature values, and an experimental result that will be explained later. Each feature consists of a type and a value</context>
</contexts>
<marker>Ristad, 1998</marker>
<rawString>Eric Sven Ristad. 1998. Maximum entropy modeling toolkit, release 1.6 beta. http://www.mnemonic.com/software/memt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
</authors>
<title>Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka.</title>
<date>1998</date>
<pages>356--359</pages>
<note>(in Japanese).</note>
<marker>Shirai, 1998</marker>
<rawString>Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1998a. Learning dependencies between case frames using maximum entropy method. pages 356-359. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
<author>Kentaro Inui</author>
<author>Takenobu Tokunaga</author>
<author>Hozumi Tanaka</author>
</authors>
<title>A framework of integrating syntactic and lexical statistics in statistical parsing.</title>
<date>1998</date>
<journal>Journal of Nat-</journal>
<contexts>
<context position="22272" citStr="Shirai et al., 1998" startWordPosition="3657" endWordPosition="3660">ments. So, we should not use all combinations of the basic features. We selected the combined features based on our intuition. In our future work, we believe some methods for automatic feature selection should be studied. One of the simplest ways of selecting features is to select features according to their frequencies in the training corpus. But using this method in our current experiments, the accuracy decreased in all of the experiments. Other methods that have been proposed are one based on using the gain (Berger et al., 1996) and an approximate method for selecting informative features (Shirai et al., 1998a), and several criteria for feature selection were proposed and compared with other criteria (Berger and Printz, 1998). We would like to try these methods. Investigating the sentences which could not be analyzed correctly, we found that many of those sentences included coordinate structures. We believe that coordinate structures can be detected to a certain extent by considering new features which take a wide range of information into account. 3.3 Number of Training Data and Accuracy Figure 2 shows the relationship between the number of training data (the number of sentences) and the accuracy</context>
</contexts>
<marker>Shirai, Inui, Tokunaga, Tanaka, 1998</marker>
<rawString>Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1998b. A framework of integrating syntactic and lexical statistics in statistical parsing. Journal of Nat-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>