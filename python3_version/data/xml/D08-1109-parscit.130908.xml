<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002097">
<title confidence="0.985794">
Unsupervised Multilingual Learning for POS Tagging
</title>
<author confidence="0.994699">
Benjamin Snyder and Tahira Naseem and Jacob Eisenstein and Regina Barzilay
</author>
<affiliation confidence="0.845826">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
77 Massachusetts Ave., Cambridge MA 02139
</affiliation>
<email confidence="0.99426">
{bsnyder, tahira, jacobe, regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.996596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984894736842">
We demonstrate the effectiveness of multilin-
gual learning for unsupervised part-of-speech
tagging. The key hypothesis of multilin-
gual learning is that by combining cues from
multiple languages, the structure of each be-
comes more apparent. We formulate a hier-
archical Bayesian model for jointly predicting
bilingual streams of part-of-speech tags. The
model learns language-specific features while
capturing cross-lingual patterns in tag distri-
bution for aligned words. Once the parame-
ters of our model have been learned on bilin-
gual parallel data, we evaluate its performance
on a held-out monolingual test set. Our evalu-
ation on six pairs of languages shows consis-
tent and significant performance gains over a
state-of-the-art monolingual baseline. For one
language pair, we observe a relative reduction
in error of 53%.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980804347826">
In this paper, we explore the application of multilin-
gual learning to part-of-speech tagging when no an-
notation is available. This core task has been studied
in an unsupervised monolingual framework for over
a decade and is still an active area of research. In this
paper, we demonstrate the effectiveness of multilin-
gual learning when applied to both closely related
and distantly related language pairs. We further ana-
lyze the language features which lead to robust bilin-
gual performance.
The fundamental idea upon which our work is
based is that the patterns of ambiguity inherent in
part-of-speech tag assignments differ across lan-
guages. At the lexical level, a word with part-of-
speech tag ambiguity in one language may corre-
spond to an unambiguous word in the other lan-
guage. For example, the word “can” in English may
function as an auxiliary verb, a noun, or a regular
verb. However, each of the corresponding functions
in Serbian is expressed with a distinct lexical item.
Languages also differ in their patterns of structural
ambiguity. For example, the presence of an article
in English greatly reduces the ambiguity of the suc-
ceeding tag. In Serbian, a language without articles,
this constraint is obviously absent. The key idea of
multilingual learning is that by combining cues from
multiple languages, the structure of each becomes
more apparent.
While multilingual learning can address ambigu-
ities in each language, it must be flexible enough
to accommodate cross-lingual variations such as tag
inventory and syntactic structure. As a result of
such variations, two languages often select and order
their tags differently even when expressing the same
meaning. A key challenge of multilingual learning
is to model language-specific structure while allow-
ing information to flow between languages.
We jointly model bilingual part-of-speech tag se-
quences in a hierarchical Bayesian framework. For
each word, we posit a hidden tag state which gen-
erates the word as well as the succeeding tag. In
addition, the tags of words with common seman-
tic or syntactic function in parallel sentences are
combined into bilingual nodes representing the tag
pair. These joined nodes serve as anchors that cre-
ate probabilistic dependencies between the tag se-
</bodyText>
<page confidence="0.940686">
1041
</page>
<note confidence="0.880064">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1041–1050,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999960766666667">
quences in each language. We use standard tools
from machine translation to discover aligned word-
pairs, and thereafter our model treats the alignments
as observed data.
Our model structure allows language-specific tag
inventories. Additionally, it assumes only that the
tags at joined nodes are correlated; they need not be
identical. We factor the conditional probabilities of
joined nodes into two individual transition probabil-
ities as well as a coupling probability. We define
priors over the transition, emission, and coupling
parameters and perform Bayesian inference using
Gibbs sampling and the Metropolis-Hastings algo-
rithm.
We evaluate our model on a parallel corpus of
four languages: English, Bulgarian, Serbian, and
Slovene. For each of the six language pairs, we
train a bilingual model on this corpus, and evaluate it
on held-out monolingual test sets. Our results show
consistent improvement over a monolingual baseline
for all languages and all pairings. In fact, for one
language pair – Serbian and Slovene – the error is
reduced by over 53%. Moreover, the multilingual
model significantly reduces the gap between unsu-
pervised and supervised performance. For instance,
in the case of Slovene this gap is reduced by 71%.
We also observe significant variation in the level of
improvement across language pairs. We show that a
cross-lingual entropy measure corresponds with the
observed differentials in performance.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.986088068965517">
Multilingual Learning A number of approaches
for multilingual learning have focused on induc-
ing cross-lingual structures, with applications to
machine translation. Examples of such efforts
include work on the induction of synchronous
grammars (Wu and Wong, 1998; Chiang, 2005)
and learning multilingual lexical resources (Genzel,
2005).
Another thread of work using cross-lingual links
has been in word-sense disambiguation, where
senses of words can be defined based on their trans-
lations (Brown et al., 1991; Dagan et al., 1991;
Resnik and Yarowsky, 1997; Ng et al., 2003).
When annotations for a task of interest are avail-
able in a source language but are missing in the
target language, the annotations can be projected
across a parallel corpus (Yarowsky et al., 2000;
Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi
and Hwa, 2005). In fact, projection methods have
been used to train highly accurate part-of-speech
taggers (Yarowsky and Ngai, 2001; Feldman et al.,
2006). In contrast, our own work assumes that an-
notations exist for neither language.
Finally, there has been recent work on applying
unsupervised multilingual learning to morphologi-
cal segmentation (Snyder and Barzilay, 2008). In
this paper, we demonstrate that unsupervised mul-
tilingual learning can be successfully applied to the
sentence-level task of part-of-speech tagging.
</bodyText>
<subsectionHeader confidence="0.634875">
Unsupervised Part-of-Speech Tagging Since
</subsectionHeader>
<bodyText confidence="0.999891">
the work of Merialdo (1994), the HMM has been the
model of choice for unsupervised tagging (Banko
and Moore, 2004). Recent advances in these
approaches include the use of a fully Bayesian
HMM (Johnson, 2007; Goldwater and Griffiths,
2007). In very recent work, Toutanova and John-
son (2008) depart from this framework and propose
an LDA-based generative model that groups words
through a latent layer of ambiguity classes thereby
leveraging morphological features. In addition, a
number of approaches have focused on develop-
ing discriminative approaches for unsupervised and
semi-supervised tagging (Smith and Eisner, 2005;
Haghighi and Klein, 2006).
Our focus is on developing a simple model that
effectively incorporates multilingual evidence. We
view this direction as orthogonal to refining mono-
lingual tagging models for any particular language.
</bodyText>
<sectionHeader confidence="0.988987" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999944083333333">
We propose a bilingual model for unsupervised part-
of-speech tagging that jointly tags parallel streams
of text in two languages. Once the parameters have
been learned using an untagged bilingual parallel
text, the model is applied to a held-out monolingual
test set.
Our key hypothesis is that the patterns of ambigu-
ity found in each language at the part-of-speech level
will differ in systematic ways; by considering multi-
ple language simultaneously, the total inherent am-
biguity can be reduced in each language. The model
is designed to permit information to flow across the
</bodyText>
<page confidence="0.990476">
1042
</page>
<figure confidence="0.999315461538461">
(a)
XI
J&apos; adore les po/ssons
YI
I love fish
X2
Y2 Y3 Y4
X3
(b)
XI/YI XI/YI XI/YI
J&apos; adore les po/ssons
I love fish
Y3
</figure>
<figureCaption confidence="0.9958455">
Figure 1: (a) Graphical structure of two standard monolingual HMM’s. (b) Graphical structure of our bilingual model
based on word alignments.
</figureCaption>
<bodyText confidence="0.999929461538462">
language barrier, while respecting language-specific
idiosyncrasies such as tag inventory, selection, and
order. We assume that for pairs of words that share
similar semantic or syntactic function, the associ-
ated tags will be statistically correlated, though not
necessarily identical. We use such word pairs as
the bilingual anchors of our model, allowing cross-
lingual information to be shared via joint tagging de-
cisions. We use standard tools from machine trans-
lation to identify these aligned words, and thereafter
our model treats them as fixed and observed data.
To avoid cycles, we remove crossing edges from the
alignments.
For unaligned parts of the sentence, the tag and
word selections are identical to standard monolin-
gual HMM’s. Figure 1 shows an example of the
bilingual graphical structure we use, in comparison
to two independent monolingual HMM’s.
We formulate a hierarchical Bayesian model that
exploits both language-specific and cross-lingual
patterns to explain the observed bilingual sentences.
We present a generative story in which the observed
words are produced by the hidden tags and model
parameters. In Section 4, we describe how to in-
fer the posterior distribution over these hidden vari-
ables, given the observations.
</bodyText>
<subsectionHeader confidence="0.829331">
3.1 Generative Model
</subsectionHeader>
<bodyText confidence="0.993546285714286">
Our generative model assumes the existence of two
tagsets, T and T′, and two vocabularies W and W′,
one of each for each language. For ease of exposi-
tion, we formulate our model with bigram tag de-
pendencies. However, in our experiments we used
a trigram model, which is a trivial extension of the
model discussed here and in the next section.
</bodyText>
<listItem confidence="0.998754444444445">
1. For each tag t E T, draw a transition distri-
bution φt over tags T, and an emission distri-
bution θt over words W, both from symmetric
Dirichlet priors.1
2. For each tag t E T′, draw a transition distri-
bution φ′t over tags T′, and an emission distri-
bution θ′t over words W′, both from symmetric
Dirichlet priors.
3. Draw a bilingual coupling distribution ω over
tag pairs T x T′ from a symmetric Dirichlet
prior.
4. For each bilingual parallel sentence:
(a) Draw an alignment a from an alignment
distribution A (see the following para-
graph for formal definitions of a and A),
(b) Draw a bilingual sequence of part-of-
speech tags (xi, ..., xm), (yi, ..., yn) ac-
cording to:
</listItem>
<bodyText confidence="0.723693666666667">
P(xi, ..., xm, yi, ..., yn|a, φ, φ′, ω). 2
This joint distribution is given in equa-
tion 1.
</bodyText>
<footnote confidence="0.9909166">
1The Dirichlet is a probability distribution over the simplex,
and is conjugate to the multinomial (Gelman et al., 2004).
2Note that we use a special end state rather than explicitly
modeling sentence length. Thus the values of m and n depend
on the draw.
</footnote>
<page confidence="0.957448">
1043
</page>
<listItem confidence="0.734701">
(c) For each part-of-speech tag xi in the first
language, emit a word from W: ei ∼ θxi,
(d) For each part-of-speech tag yj in the sec-
ond language, emit a word from W′: fj ∼
θ′ yj.
</listItem>
<bodyText confidence="0.999875136363637">
We define an alignment a to be a set of one-to-
one integer pairs with no crossing edges. Intuitively,
each pair (i, j) ∈ a indicates that the words ei and
fj share some common role in the bilingual paral-
lel sentences. In our experiments, we assume that
alignments are directly observed and we hold them
fixed. From the perspective of our generative model,
we treat alignments as drawn from a distribution A,
about which we remain largely agnostic. We only
require that A assign zero probability to alignments
which either: (i) align a single index in one language
to multiple indices in the other language or (ii) con-
tain crossing edges. The resulting alignments are
thus one-to-one, contain no crossing edges, and may
be sparse or even possibly empty. Our technique for
obtaining alignments that display these properties is
described in Section 5.
Given an alignment a and sets of transition param-
eters φ and φ′, we factor the conditional probability
of a bilingual tag sequence (x1, ...xm), (y1, ..., yn)
into transition probabilities for unaligned tags, and
joint probabilities over aligned tag pairs:
</bodyText>
<equation confidence="0.9881305">
P(x1, ..., xm, y1, ..., yn|a, φ, φ′, ω) =
rl rl
φxi−1(xi) ·
unaligned i unaligned j
ri P(xi, yj|xi−1, yj−1, φ, φ′, ω)
(i,j)∈a
</equation>
<bodyText confidence="0.996095">
Because the alignment contains no crossing
edges, we can model the tags as generated sequen-
tially by a stochastic process. We define the dis-
tribution over aligned tag pairs to be a product of
each language’s transition probability and the cou-
pling probability:
The normalization constant here is defined as:
</bodyText>
<equation confidence="0.8393315">
Z =E φxi−1(x) φ′yj−1(y) ω(x, y)
x,y
</equation>
<bodyText confidence="0.999972842105263">
This factorization allows the language-specific tran-
sition probabilities to be shared across aligned and
unaligned tags. In the latter case, the addition of
the coupling parameter ω gives the tag pair an addi-
tional role: that of multilingual anchor. In essence,
the probability of the aligned tag pair is a product
of three experts: the two transition parameters and
the coupling parameter. Thus, the combination of
a high probability transition in one language and a
high probability coupling can resolve cases of inher-
ent transition uncertainty in the other language. In
addition, any one of the three parameters can “veto”
a tag pair to which it assigns low probability.
To perform inference in this model, we predict
the bilingual tag sequences with maximal probabil-
ity given the observed words and alignments, while
integrating over the transition, emission, and cou-
pling parameters. To do so, we use a combination of
sampling-based techniques.
</bodyText>
<sectionHeader confidence="0.999547" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999954857142857">
The core element of our inference procedure is
Gibbs sampling (Geman and Geman, 1984). Gibbs
sampling begins by randomly initializing all unob-
served random variables; at each iteration, each ran-
dom variable zi is sampled from the conditional dis-
tribution P(zi|z−i), where z−i refers to all variables
other than zi. Eventually, the distribution over sam-
ples drawn from this process will converge to the
unconditional joint distribution P(z) of the unob-
served variables. When possible, we avoid explic-
itly sampling variables which are not of direct inter-
est, but rather integrate over them—this technique
is known as “collapsed sampling,” and can reduce
variance (Liu, 1994).
We sample: (i) the bilingual tag sequences (x, y),
(ii) the two sets of transition parameters φ and φ′,
and (iii) the coupling parameter ω. We integrate over
the emission parameters θ and θ′, whose priors are
Dirichlet distributions with hyperparameters θ0 and
θ′0. The resulting emission distribution over words
ei, given the other words e−i, the tag sequences x
</bodyText>
<equation confidence="0.9967376">
φ′yj−1(yj) ·
(1)
P(xi, yj|xi−1, yj−1, φ, φ′, ω) =
φxi−1(xi) φ′yj−1(yj) ω(xi,yj) (2)
Z
</equation>
<page confidence="0.865307">
1044
</page>
<bodyText confidence="0.981116">
and the emission prior θ0, can easily be derived as:
</bodyText>
<equation confidence="0.99881225">
P(ei |x, e−i, θ0) = 10x θxi (ei) P(θxi |θ0) dθxi (3)
i
n(xi, ei) + θ0
n(xi) + Wxiθ0
</equation>
<bodyText confidence="0.999090333333333">
Here, n(xi) is the number of occurrences of the
tag xi in x−i, n(xi, ei) is the number of occurrences
of the tag-word pair (xi, ei) in (x−i, e−i), and Wxi
is the number of word types in the vocabulary W
that can take tag xi. The integral is tractable due
to Dirichlet-multinomial conjugacy (Gelman et al.,
2004).
We will now discuss, in turn, each of the variables
that we sample. Note that in all cases we condi-
tion on the other sampled variables as well as the
observed words and alignments, e, f and a, which
are kept fixed throughout.
</bodyText>
<subsectionHeader confidence="0.999264">
4.1 Sampling Part-of-speech Tags
</subsectionHeader>
<bodyText confidence="0.9997553">
This section presents the conditional distributions
that we sample from to obtain the part-of-speech
tags. Depending on the alignment, there are several
scenarios. In the simplest case, both the tag to be
sampled and its succeeding tag are not aligned to
any tag in the other language. If so, the sampling
distribution is identical to the monolingual case, in-
cluding only terms for the emission (defined in equa-
tion 3), and the preceding and succeeding transi-
tions:
</bodyText>
<equation confidence="0.9856275">
P(xi|x−i, y, e, f, a, φ, φ′, ω, θ0, θ′0) ∝
P(ei|x, e−i, θ0) φxi−1(xi) φxi(xi+1).
</equation>
<bodyText confidence="0.9992136">
For an aligned tag pair (xi, yj), we sample the
identity of the tags jointly. By applying the chain
rule we obtain terms for the emissions in both lan-
guages and a joint term for the transition probabili-
ties:
</bodyText>
<equation confidence="0.929715">
P(xi, yj|x−i, y−j, e, f, a, φ, φ′, ω, θ0, θ′0) ∝
P(ei|x, e−i, θ0)P(fj|y, f−j, θ′0)
P(xi, yj|x−i, y−j, a, φ, φ′,ω)
</equation>
<bodyText confidence="0.9877625">
The expansion of the joint term depends on the
alignment of the succeeding tags. In the case that
the successors are not aligned, we have a product of
the bilingual coupling probability and four transition
probabilities (preceding and succeeding transitions
in each language):
</bodyText>
<equation confidence="0.9992125">
P(xi,yj|x−i,y−j, a,φ,φ′,ω) ∝
ω(xi,yj)φxi−1(xi) φ′yj−1(yj) φxi(xi+1) φ′yj(yj+1)
</equation>
<bodyText confidence="0.9983325">
Whenever one or more of the succeeding tags is
aligned, the sampling formulas must account for the
effect of the sampled tag on the joint probability
of the succeeding tags, which is no longer a sim-
ple multinomial transition probability. We give the
formula for one such case—when we are sampling
an aligned tag pair (xi, yj), whose succeeding tags
(xi+1, yj+1) are also aligned to one another:
</bodyText>
<equation confidence="0.90108025">
P (xi, yj|x−i,y−j, a, φ, φ′,ω) ∝ ω(xi,yj)
�φxi(xi+1) ��Oyj (yj+1)
Ex,y φxi (x) 0&apos; (y) ω(x, y)
Similar equations can be derived for cases where
</equation>
<bodyText confidence="0.989554">
the succeeding tags are not aligned to each other, but
to other tags.
</bodyText>
<subsectionHeader confidence="0.9972755">
4.2 Sampling Transition Parameters and the
Coupling Parameter
</subsectionHeader>
<bodyText confidence="0.999830409090909">
When computing the joint probability of an aligned
tag pair (Equation 2), we employ the transition pa-
rameters φ, φ′ and the coupling parameter ω in a nor-
malized product. Because of this, we can no longer
regard these parameters as simple multinomials, and
thus can no longer sample them using the standard
closed formulas.
Instead, to resample these parameters, we re-
sort to the Metropolis-Hastings algorithm as a sub-
routine within Gibbs sampling (Hastings, 1970).
Metropolis-Hastings is a Markov chain sampling
technique that can be used when it is impossible to
directly sample from the posterior. Instead, sam-
ples are drawn from a proposal distribution and then
stochastically accepted or rejected on the basis of:
their likelihood, their probability under the proposal
distribution, and the likelihood and proposal proba-
bility of the previous sample.
We use a form of Metropolis-Hastings known as
an independent sampler. In this setup, the proposal
distribution does not depend on the value of the
previous sample, although the accept/reject decision
</bodyText>
<equation confidence="0.885055">
· φxi−1(xi) φ′yj−1(yj)
</equation>
<page confidence="0.893301">
1045
</page>
<bodyText confidence="0.9995418">
does depend on the previous model likelihood. More
formally, if we denote the proposal distribution as
Q(z), the target distribution as P(z), and the previ-
ous sample as z, then the probability of accepting a
new sample z∗ — Q is set at:
</bodyText>
<equation confidence="0.636877">
min{ 1, P(z) Q((∗) }
</equation>
<bodyText confidence="0.999688888888889">
Theoretically any non-degenerate proposal distri-
bution may be used. However, a higher acceptance
rate and faster convergence is achieved when the
proposal Q is a close approximation of P. For a par-
ticular transition parameter φx, we define our pro-
posal distribution Q to be Dirichlet with parameters
set to the bigram counts of the tags following x in
the sampled tag data. Thus, the proposal distribu-
tion for φx has a mean proportional to these counts,
and is thus likely to be a good approximation to the
target distribution.
Likewise for the coupling parameter ω, we de-
fine a Dirichlet proposal distribution. This Dirichlet
is parameterized by the counts of aligned tag pairs
(x, y) in the current set of tag samples. Since this
sets the mean of the proposal to be proportional to
these counts, this too is likely to be a good approxi-
mation to the target distribution.
</bodyText>
<subsectionHeader confidence="0.98745">
4.3 Hyperparameter Re-estimation
</subsectionHeader>
<bodyText confidence="0.9998734">
After every iteration of Gibbs sampling the hyper-
parameters θ0 and θ′ 0 are re-estimated using a single
Metropolis-Hastings move. The proposal distribu-
tion is set to a Gaussian with mean at the current
value and variance equal to one tenth of the mean.
</bodyText>
<sectionHeader confidence="0.99904" genericHeader="method">
5 Experimental Set-Up
</sectionHeader>
<bodyText confidence="0.999920619047619">
Our evaluation framework follows the standard pro-
cedures established for unsupervised part-of-speech
tagging. Given a tag dictionary (i.e., a set of possi-
ble tags for each word type), the model has to select
the appropriate tag for each token occurring in a text.
We also evaluate tagger performance when only in-
complete dictionaries are available (Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007). In both
scenarios, the model is trained only using untagged
text.
In this section, we first describe the parallel data
and part-of-speech annotations used for system eval-
uation. Next we describe a monolingual base-
line and our procedures for initialization and hyper-
parameter setting.
Data As a source of parallel data, we use Orwell’s
novel “Nineteen Eighty Four” in the original English
as well as translations to three Slavic languages —
Bulgarian, Serbian and Slovene. This data is dis-
tributed as part of the Multext-East corpus which
is publicly available. The corpus provides detailed
morphological annotation at the world level, includ-
ing part-of-speech tags. In addition a lexicon for
each language is provided.
We obtain six parallel corpora by considering
all pairings of the four languages. We compute
word level alignments for each language pair using
Giza++. To generate one-to-one alignments at the
word level, we intersect the one-to-many alignments
going in each direction and automatically remove
crossing edges in the order in which they appear left
to right. This process results in alignment of about
half the tokens in each bilingual parallel corpus. We
treat the alignments as fixed and observed variables
throughout the training procedure.
The corpus consists of 94,725 English words (see
Table 2). For every language, a random three quar-
ters of the data are used for learning the model while
the remaining quarter is used for testing. In the test
set, only monolingual information is made available
to the model, in order to simulate future performance
on non-parallel data.
</bodyText>
<table confidence="0.994083">
Tokens Tags/Token
SR 89,051 1.41
SL 91,724 1.40
BG 80,757 1.34
EN 94,725 2.58
</table>
<tableCaption confidence="0.996093">
Table 2: Corpus statistics: SR=Serbian, SL=Slovene,
EN=English, BG=Bulgarian
</tableCaption>
<bodyText confidence="0.999664166666667">
Tagset The Multext-East corpus is manually an-
notated with detailed morphosyntactic information.
In our experiments, we focus on the main syntac-
tic category encoded as a first letter of the labels.
The annotation distinguishes between 13 parts-of-
speech, of which 11 are common for all languages
</bodyText>
<page confidence="0.968917">
1046
</page>
<table confidence="0.9995702">
Random Monolingual Unsupervised Monolingual Supervised Trigram Entropy
EN 56.24 90.71 96.97 1.558
BG 82.68 88.88 96.96 1.708
SL 84.70 87.41 97.31 1.703
SR 83.41 85.05 96.72 1.789
</table>
<tableCaption confidence="0.984137666666667">
Table 1: Monolingual tagging accuracy for English, Bulgarian, Slovene, and Serbian for two unsupervised baselines
(random tag selection and a Bayesian HMM (Goldwater and Griffiths, 2007)) as well as a supervised HMM. In
addition, the trigram part-of-speech tag entropy is given for each language.
</tableCaption>
<bodyText confidence="0.9994708125">
in our experiments.3
In the Multext-East corpus, punctuation marks are
not annotated. We expand the tag repository by
defining a separate tag for all punctuation marks.
This allows the model to make use of any transition
or coupling patterns involving punctuation marks.
We do not consider punctuation tokens when com-
puting model accuracy.
Table 2 shows the tag/token ratio for these lan-
guages. For Slavic languages, we use the tag dic-
tionaries provided with the corpus. For English,
we use a different process for dictionary construc-
tion. Using the original dictionary would result in
the tag/token ratio of 1.5, in comparison to the ra-
tio of 2.3 observed in the Wall Street Journal (WSJ)
corpus. To make our results on English tagging more
comparable to previous benchmarks, we expand the
original dictionary of English tags by merging it
with the tags from the WSJ dictionary. This process
results in a tag/token ratio of 2.58, yielding a slightly
more ambiguous dictionary than the one used in pre-
vious tagging work. 4
Monolingual Baseline As our monolingual base-
line we use the unsupervised Bayesian HMM model
of Goldwater and Griffiths (2007) (BHMM1). This
model modifies the standard HMM by adding pri-
ors and by performing Bayesian inference. Its is in
line with state-of-the-art unsupervised models. This
model is a particulary informative baseline, since
our model reduces to this baseline model when there
are no alignments in the data. This implies that any
performance gain over the baseline can only be at-
</bodyText>
<footnote confidence="0.995737333333333">
3The remaining two tags are Particle and Determiner; The
English tagset does not include Particle while the other three
languages Serbian, Slovene and Bulgarian do not have Deter-
miner in their tagset.
4We couldn’t perform the same dictionary expansion for the
Slavic languages due to a lack of additional annotated resources.
</footnote>
<bodyText confidence="0.999620810810811">
tributed to the multilingual aspect of our model. We
used our own implementation after verifying that its
performance on WSJ was identical to that reported
in (Goldwater and Griffiths, 2007).
Supervised Performance In order to provide a
point of comparison, we also provide supervised re-
sults when an annotated corpus is provided. We use
the standard supervised HMM with Viterbi decod-
ing.
Training and Testing Framework Initially, all
words are assigned tags randomly from their tag
dictionaries. During each iteration of the sam-
pler, aligned tag pairs and unaligned tags are sam-
pled from their respective distributions given in Sec-
tion 4.1 above. The hyperparameters 00 and 0′ 0 are
initialized with the values learned during monolin-
gual training. They are re-estimated after every iter-
ation of the sampler using the Metropolis Hastings
algorithm. The parameters 0 and 0′ are initially
set to trigram counts and the w parameter is set to
tag pair counts of aligned pairs. After every 40 it-
erations of the sampler, a Metropolis Hastings sub-
routine is invoked that re-estimates these parameters
based on the current counts. Overall, the algorithm
is run for 1000 iterations of tag sampling, by which
time the resulting log-likelihood converges to stable
values. Each Metropolis Hastings subroutine sam-
ples 20 values, with an acceptance ratio of around
1/6, in line with the standard recommended values.
After training, trigram and word emission prob-
abilities are computed based on the counts of tags
assigned in the final iteration. For smoothing, the
final sampled values of the hyperparameters are
used. The highest probability tag sequences for each
monolingual test set are then predicted using trigram
Viterbi decoding. We report results averaged over
five complete runs of all experiments.
</bodyText>
<page confidence="0.995849">
1047
</page>
<sectionHeader confidence="0.999658" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999566153846154">
Complete Tag Dictionary In our first experiment,
we assume that a complete dictionary listing the pos-
sible tags for every word is provided in each lan-
guage. Table 1 shows the monolingual results of a
random baseline, an unsupervised Bayesian HMM
and a supervised HMM. Table 3 show the results
of our bilingual models for different language pair-
ings while repeating the monolingual unsupervised
results from Table 1 for easy comparison. The final
column indicates the absolute gain in performance
over this monolingual baseline.
Across all language pairs, the bilingual model
consistently outperforms the monolingual baseline.
All the improvements are statistically significant by
a Fisher sign test at p &lt; 0.05. For some lan-
guage pairs, the gains are quite high. For instance,
the pairing of Serbian and Slovene (two closely re-
lated languages) yields absolute improvements of
6.7 and 7.7 percentage points, corresponding to rel-
ative reductions in error of 51.4% and 53.2%. Pair-
ing Bulgarian and English (two distantly related lan-
guages) also yields large gains: 5.6 and 1.3 percent-
age points, corresponding to relative reductions in
error of 50% and 14%, respectively.5
When we compare the best bilingual result for
each language (Table 3, in bold) to the monolin-
gual supervised results (Table 1), we find that for
all languages the gap between supervised and un-
supervised learning is reduced significantly. For En-
glish, this gap is reduced by 21%. For the Slavic lan-
guages, the supervised-unsupervised gap is reduced
by even larger amounts: 57%, 69%, and 78% for
Serbian, Bulgarian, and Slovene respectively.
While all the languages benefit from the bilin-
gual learning framework, some language combina-
tions are more effective than others. Slovene, for in-
stance, achieves a large improvement when paired
with Serbian (+7.7), a closely related Slavic lan-
guage, but only a minor improvement when coupled
</bodyText>
<footnote confidence="0.9981215">
5The accuracy of the monolingual English tagger is rela-
tively high compared to the 87% reported by (Goldwater and
Griffiths, 2007) on the WSJ corpus. We attribute this discrep-
ancy to the slight differences in tag inventory used in our data-
set. For example, when Particles and Prepositions are merged
in the WSJ corpus (as they happen to be in our tag inventory
and corpus), the performance of Goldwater’s model on WSJ is
similar to what we report here.
</footnote>
<table confidence="0.999904285714286">
Entropy Mono- Bilingual Absolute
lingual Gain
EN 0.566 90.71 91.01 +0.30
SR 0.554 85.05 90.06 +5.03
EN 0.578 90.71 92.00 +1.29
BG 0.543 88.88 94.48 +5.61
EN 0.571 90.71 92.01 +1.30
SL 0.568 87.41 88.54 +1.13
SL 0.494 87.41 95.10 +7.69
SR 0.478 85.05 91.75 +6.70
BG 0.568 88.88 91.95 +3.08
SR 0.588 85.05 86.58 +1.53
BG 0.579 88.88 90.91 +2.04
SL 0.609 87.41 88.20 +0.79
</table>
<tableCaption confidence="0.98163775">
Table 3: The tagging accuracy of our bilingual models
on different language pairs, when a full tag dictionary is
provided. The Monolingual Unsupervised results from
Table 1 are repeated for easy comparison. The first col-
</tableCaption>
<bodyText confidence="0.996133642857143">
umn shows the cross-lingual entropy of a tag when the
tag of the aligned word in the other language is known.
The final column shows the absolute improvement over
the monolingual Bayesian HMM. The best result for each
language is shown in boldface.
with English (+1.3). On the other hand, for Bulgar-
ian, the best performance is achieved when coupling
with English (+5.6) rather than with closely related
Slavic languages (+3.1 and +2.4). As these results
show, an optimal pairing cannot be predicted based
solely on the family connection of paired languages.
To gain a better understanding of this variation
in performance, we measured the internal tag en-
tropy of each language as well as the cross-lingual
tag entropy of language pairs. For the first measure,
we computed the conditional entropy of a tag de-
cision given the previous two tags. Intuitively, this
should correspond to the inherent structural uncer-
tainty of part-of-speech decisions in a language. In
fact, as Table 1 shows, the trigram entropy is a good
indicator of the relative performance of the mono-
lingual baseline. To measure the cross-lingual tag
entropies of language pairs, we considered all bilin-
gual aligned tag pairs, and computed the conditional
entropy of the tags in one language given the tags
in the other language. This measure should indi-
cate the amount of information that one language in
a pair can provide the other. The results of this anal-
</bodyText>
<page confidence="0.942487">
1048
</page>
<table confidence="0.999864142857143">
Mono- Bilingual Absolute
lingual Gain
EN 63.57 68.22 +4.66
SR 41.14 54.73 +13.59
EN 63.57 71.34 +7.78
BG 53.19 62.55 +9.37
EN 63.57 66.48 +2.91
SL 49.90 53.77 +3.88
SL 49.90 59.68 +9.78
SR 41.14 54.08 +12.94
BG 53.19 54.22 +1.04
SR 41.14 56.91 +15.77
BG 53.19 55.88 +2.70
SL 49.90 58.50 +8.60
</table>
<tableCaption confidence="0.984435">
Table 4: Tagging accuracy for Bilingual models with re-
</tableCaption>
<figureCaption confidence="0.899604">
duced dictionary: Lexicon entries are available for only
the 100 most frequent words, while all other words be-
come fully ambiguous. The improvement over the mono-
lingual Bayesian HMM trained under similar circum-
stances is shown. The best result for each language is
shown in boldface.
</figureCaption>
<bodyText confidence="0.9998813">
ysis are given in the first column of Table 3. We ob-
serve that the cross-lingual entropy is lowest for the
Serbian and Slovene pair, corresponding with their
large gain in performance. Bulgarian, on the other
hand, has lowest cross-lingual entropy when paired
with English. This corresponds with the fact that
English provides Bulgarian with its largest perfor-
mance gain. In general, we find that the largest per-
formance gain for any language is achieved when
minimizing its cross-lingual entropy.
Reduced Tag Dictionary We also conducted ex-
periments to investigate the impact of the dictio-
nary size on the performance of the bilingual model.
Here, we provide results for the realistic scenario
where only a very small dictionary is present. Ta-
ble 4 shows the performance when a tag dictionary
for the 100 most frequent words is present in each
language. The bilingual model’s results are consis-
tently and significantly better than the monolingual
baseline for all language pairs.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999987909090909">
We have demonstrated the effectiveness of multilin-
gual learning for unsupervised part-of-speech tag-
ging. The key hypothesis of multilingual learn-
ing is that by combining cues from multiple lan-
guages, the structure of each becomes more appar-
ent. We formulated a hierarchical Bayesian model
for jointly predicting bilingual streams of tags. The
model learns language-specific features while cap-
turing cross-lingual patterns in tag distribution. Our
evaluation shows significant performance gains over
a state-of-the-art monolingual baseline.
</bodyText>
<sectionHeader confidence="0.998352" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.882257333333333">
The authors acknowledge the support of the National
Science Foundation (CAREER grant IIS-0448168 and
grant IIS-0835445) and the Microsoft Research Faculty
Fellowship. Thanks to Michael Collins, Amir Glober-
son, Lillian Lee, Yoong Keok Lee, Maria Polinsky and
the anonymous reviewers for helpful comments and sug-
gestions. Any opinions, findings, and conclusions or rec-
ommendations expressed above are those of the authors
and do not necessarily reflect the views of the NSF.
</bodyText>
<sectionHeader confidence="0.990626" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999225777777777">
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of the COL-
ING, pages 556–561.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-sense dis-
ambiguation using statistical methods. In Proceedings
of the ACL, pages 264–270.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the ACL, pages 263–270.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proceed-
ings of the ACL, pages 130–137.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the ACL, pages 255–262.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of new
morpho-syntactically annotated resources. In Pro-
ceedings ofLREC, pages 549–554.
Andrew Gelman, John B. Carlin, Hal .S. Stern, and Don-
ald .B. Rubin. 2004. Bayesian data analysis. Chap-
man and Hall/CRC.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721–741.
</reference>
<page confidence="0.842011">
1049
</page>
<reference confidence="0.99982695">
Dmitriy Genzel. 2005. Inducing a multilingual dictio-
nary from a parallel multitext in related languages. In
Proceedings ofHLT/EMNLP, pages 875–882.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744–751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. Proceedings of HLT-
NAACL, pages 320–327.
W. K. Hastings. 1970. Monte carlo sampling meth-
ods using Markov chains and their applications.
Biometrika, 57:97–109.
Mark Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Proceedings of EMNLP/CoNLL,
pages 296–305.
Jun S. Liu. 1994. The collapsed Gibbs sampler in
Bayesian computations with applications to a gene
regulation problem. Journal of the American Statis-
tical Association, 89(427):958–966.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):155–171.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In Proceedings of the ACL, pages
455–462.
Sebastian Pad´o and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings ofACL, pages 1161 – 1168.
Philip Resnik and David Yarowsky. 1997. A perspective
on word sense disambiguation methods and their eval-
uation. In Proceedings of the ACL SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, What,
and How?, pages 79–86.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the ACL, pages 354–362.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the ACL/HLT, pages 737–
745.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian lda-based model for semi-supervised part-
of-speech tagging. In Advances in Neural Information
Processing Systems 20, pages 1521–1528. MIT Press.
Dekai Wu and Hongsing Wong. 1998. Machine trans-
lation with a stochastic grammatical channel. In Pro-
ceedings of the ACL/COLING, pages 1408–1415.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of EMNLP, pages 851 – 858.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
NAACL, pages 1–8.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
ofHLT, pages 161–168.
</reference>
<page confidence="0.99016">
1050
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.830917">
<title confidence="0.997416">Unsupervised Multilingual Learning for POS Tagging</title>
<author confidence="0.995408">Snyder Naseem Eisenstein</author>
<affiliation confidence="0.991955">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<address confidence="0.965263">77 Massachusetts Ave., Cambridge MA</address>
<email confidence="0.973172">tahira,jacobe,</email>
<abstract confidence="0.99456425">We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The key hypothesis of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent. We formulate a hierarchical Bayesian model for jointly predicting bilingual streams of part-of-speech tags. The model learns language-specific features while capturing cross-lingual patterns in tag distribution for aligned words. Once the parameters of our model have been learned on bilingual parallel data, we evaluate its performance on a held-out monolingual test set. Our evaluation on six pairs of languages shows consistent and significant performance gains over a state-of-the-art monolingual baseline. For one language pair, we observe a relative reduction in error of 53%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Robert C Moore</author>
</authors>
<title>Part-ofspeech tagging in context.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING,</booktitle>
<pages>556--561</pages>
<contexts>
<context position="6550" citStr="Banko and Moore, 2004" startWordPosition="995" endWordPosition="998">rain highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incor</context>
</contexts>
<marker>Banko, Moore, 2004</marker>
<rawString>Michele Banko and Robert C. Moore. 2004. Part-ofspeech tagging in context. In Proceedings of the COLING, pages 556–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>Word-sense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>264--270</pages>
<contexts>
<context position="5551" citStr="Brown et al., 1991" startWordPosition="839" endWordPosition="842">t a cross-lingual entropy measure corresponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying uns</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1991. Word-sense disambiguation using statistical methods. In Proceedings of the ACL, pages 264–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="5318" citStr="Chiang, 2005" startWordPosition="806" endWordPosition="807">duces the gap between unsupervised and supervised performance. For instance, in the case of Slovene this gap is reduced by 71%. We also observe significant variation in the level of improvement across language pairs. We show that a cross-lingual entropy measure corresponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
<author>Ulrike Schwall</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>130--137</pages>
<contexts>
<context position="5571" citStr="Dagan et al., 1991" startWordPosition="843" endWordPosition="846">tropy measure corresponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingu</context>
</contexts>
<marker>Dagan, Itai, Schwall, 1991</marker>
<rawString>Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two languages are more informative than one. In Proceedings of the ACL, pages 130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>255--262</pages>
<contexts>
<context position="5836" citStr="Diab and Resnik, 2002" startWordPosition="889" endWordPosition="892">of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since </context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of the ACL, pages 255–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Feldman</author>
<author>Jirka Hana</author>
<author>Chris Brew</author>
</authors>
<title>A cross-language approach to rapid creation of new morpho-syntactically annotated resources.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<pages>549--554</pages>
<contexts>
<context position="6019" citStr="Feldman et al., 2006" startWordPosition="918" endWordPosition="921">ork using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Baye</context>
</contexts>
<marker>Feldman, Hana, Brew, 2006</marker>
<rawString>Anna Feldman, Jirka Hana, and Chris Brew. 2006. A cross-language approach to rapid creation of new morpho-syntactically annotated resources. In Proceedings ofLREC, pages 549–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gelman</author>
<author>John B Carlin</author>
<author>Hal S Stern</author>
<author>Donald B Rubin</author>
</authors>
<title>Bayesian data analysis. Chapman and Hall/CRC.</title>
<date>2004</date>
<contexts>
<context position="10631" citStr="Gelman et al., 2004" startWordPosition="1665" endWordPosition="1668">er words W′, both from symmetric Dirichlet priors. 3. Draw a bilingual coupling distribution ω over tag pairs T x T′ from a symmetric Dirichlet prior. 4. For each bilingual parallel sentence: (a) Draw an alignment a from an alignment distribution A (see the following paragraph for formal definitions of a and A), (b) Draw a bilingual sequence of part-ofspeech tags (xi, ..., xm), (yi, ..., yn) according to: P(xi, ..., xm, yi, ..., yn|a, φ, φ′, ω). 2 This joint distribution is given in equation 1. 1The Dirichlet is a probability distribution over the simplex, and is conjugate to the multinomial (Gelman et al., 2004). 2Note that we use a special end state rather than explicitly modeling sentence length. Thus the values of m and n depend on the draw. 1043 (c) For each part-of-speech tag xi in the first language, emit a word from W: ei ∼ θxi, (d) For each part-of-speech tag yj in the second language, emit a word from W′: fj ∼ θ′ yj. We define an alignment a to be a set of one-toone integer pairs with no crossing edges. Intuitively, each pair (i, j) ∈ a indicates that the words ei and fj share some common role in the bilingual parallel sentences. In our experiments, we assume that alignments are directly obs</context>
<context position="15065" citStr="Gelman et al., 2004" startWordPosition="2419" endWordPosition="2422">ssion distribution over words ei, given the other words e−i, the tag sequences x φ′yj−1(yj) · (1) P(xi, yj|xi−1, yj−1, φ, φ′, ω) = φxi−1(xi) φ′yj−1(yj) ω(xi,yj) (2) Z 1044 and the emission prior θ0, can easily be derived as: P(ei |x, e−i, θ0) = 10x θxi (ei) P(θxi |θ0) dθxi (3) i n(xi, ei) + θ0 n(xi) + Wxiθ0 Here, n(xi) is the number of occurrences of the tag xi in x−i, n(xi, ei) is the number of occurrences of the tag-word pair (xi, ei) in (x−i, e−i), and Wxi is the number of word types in the vocabulary W that can take tag xi. The integral is tractable due to Dirichlet-multinomial conjugacy (Gelman et al., 2004). We will now discuss, in turn, each of the variables that we sample. Note that in all cases we condition on the other sampled variables as well as the observed words and alignments, e, f and a, which are kept fixed throughout. 4.1 Sampling Part-of-speech Tags This section presents the conditional distributions that we sample from to obtain the part-of-speech tags. Depending on the alignment, there are several scenarios. In the simplest case, both the tag to be sampled and its succeeding tag are not aligned to any tag in the other language. If so, the sampling distribution is identical to the </context>
</contexts>
<marker>Gelman, Carlin, Stern, Rubin, 2004</marker>
<rawString>Andrew Gelman, John B. Carlin, Hal .S. Stern, and Donald .B. Rubin. 2004. Bayesian data analysis. Chapman and Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="13573" citStr="Geman and Geman, 1984" startWordPosition="2164" endWordPosition="2167">obability transition in one language and a high probability coupling can resolve cases of inherent transition uncertainty in the other language. In addition, any one of the three parameters can “veto” a tag pair to which it assigns low probability. To perform inference in this model, we predict the bilingual tag sequences with maximal probability given the observed words and alignments, while integrating over the transition, emission, and coupling parameters. To do so, we use a combination of sampling-based techniques. 4 Inference The core element of our inference procedure is Gibbs sampling (Geman and Geman, 1984). Gibbs sampling begins by randomly initializing all unobserved random variables; at each iteration, each random variable zi is sampled from the conditional distribution P(zi|z−i), where z−i refers to all variables other than zi. Eventually, the distribution over samples drawn from this process will converge to the unconditional joint distribution P(z) of the unobserved variables. When possible, we avoid explicitly sampling variables which are not of direct interest, but rather integrate over them—this technique is known as “collapsed sampling,” and can reduce variance (Liu, 1994). We sample: </context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Inducing a multilingual dictionary from a parallel multitext in related languages.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT/EMNLP,</booktitle>
<pages>875--882</pages>
<contexts>
<context position="5377" citStr="Genzel, 2005" startWordPosition="813" endWordPosition="814">ce. For instance, in the case of Slovene this gap is reduced by 71%. We also observe significant variation in the level of improvement across language pairs. We show that a cross-lingual entropy measure corresponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yaro</context>
</contexts>
<marker>Genzel, 2005</marker>
<rawString>Dmitriy Genzel. 2005. Inducing a multilingual dictionary from a parallel multitext in related languages. In Proceedings ofHLT/EMNLP, pages 875–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="6674" citStr="Goldwater and Griffiths, 2007" startWordPosition="1014" endWordPosition="1017">rk assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particula</context>
<context position="20145" citStr="Goldwater and Griffiths, 2007" startWordPosition="3269" endWordPosition="3272">rparameters θ0 and θ′ 0 are re-estimated using a single Metropolis-Hastings move. The proposal distribution is set to a Gaussian with mean at the current value and variance equal to one tenth of the mean. 5 Experimental Set-Up Our evaluation framework follows the standard procedures established for unsupervised part-of-speech tagging. Given a tag dictionary (i.e., a set of possible tags for each word type), the model has to select the appropriate tag for each token occurring in a text. We also evaluate tagger performance when only incomplete dictionaries are available (Smith and Eisner, 2005; Goldwater and Griffiths, 2007). In both scenarios, the model is trained only using untagged text. In this section, we first describe the parallel data and part-of-speech annotations used for system evaluation. Next we describe a monolingual baseline and our procedures for initialization and hyperparameter setting. Data As a source of parallel data, we use Orwell’s novel “Nineteen Eighty Four” in the original English as well as translations to three Slavic languages — Bulgarian, Serbian and Slovene. This data is distributed as part of the Multext-East corpus which is publicly available. The corpus provides detailed morpholo</context>
<context position="22564" citStr="Goldwater and Griffiths, 2007" startWordPosition="3645" endWordPosition="3648">manually annotated with detailed morphosyntactic information. In our experiments, we focus on the main syntactic category encoded as a first letter of the labels. The annotation distinguishes between 13 parts-ofspeech, of which 11 are common for all languages 1046 Random Monolingual Unsupervised Monolingual Supervised Trigram Entropy EN 56.24 90.71 96.97 1.558 BG 82.68 88.88 96.96 1.708 SL 84.70 87.41 97.31 1.703 SR 83.41 85.05 96.72 1.789 Table 1: Monolingual tagging accuracy for English, Bulgarian, Slovene, and Serbian for two unsupervised baselines (random tag selection and a Bayesian HMM (Goldwater and Griffiths, 2007)) as well as a supervised HMM. In addition, the trigram part-of-speech tag entropy is given for each language. in our experiments.3 In the Multext-East corpus, punctuation marks are not annotated. We expand the tag repository by defining a separate tag for all punctuation marks. This allows the model to make use of any transition or coupling patterns involving punctuation marks. We do not consider punctuation tokens when computing model accuracy. Table 2 shows the tag/token ratio for these languages. For Slavic languages, we use the tag dictionaries provided with the corpus. For English, we us</context>
<context position="23823" citStr="Goldwater and Griffiths (2007)" startWordPosition="3852" endWordPosition="3855">tionary construction. Using the original dictionary would result in the tag/token ratio of 1.5, in comparison to the ratio of 2.3 observed in the Wall Street Journal (WSJ) corpus. To make our results on English tagging more comparable to previous benchmarks, we expand the original dictionary of English tags by merging it with the tags from the WSJ dictionary. This process results in a tag/token ratio of 2.58, yielding a slightly more ambiguous dictionary than the one used in previous tagging work. 4 Monolingual Baseline As our monolingual baseline we use the unsupervised Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1). This model modifies the standard HMM by adding priors and by performing Bayesian inference. Its is in line with state-of-the-art unsupervised models. This model is a particulary informative baseline, since our model reduces to this baseline model when there are no alignments in the data. This implies that any performance gain over the baseline can only be at3The remaining two tags are Particle and Determiner; The English tagset does not include Particle while the other three languages Serbian, Slovene and Bulgarian do not have Determiner in their tagset. 4We couldn’t perform the same</context>
<context position="28361" citStr="Goldwater and Griffiths, 2007" startWordPosition="4575" endWordPosition="4578">ntly. For English, this gap is reduced by 21%. For the Slavic languages, the supervised-unsupervised gap is reduced by even larger amounts: 57%, 69%, and 78% for Serbian, Bulgarian, and Slovene respectively. While all the languages benefit from the bilingual learning framework, some language combinations are more effective than others. Slovene, for instance, achieves a large improvement when paired with Serbian (+7.7), a closely related Slavic language, but only a minor improvement when coupled 5The accuracy of the monolingual English tagger is relatively high compared to the 87% reported by (Goldwater and Griffiths, 2007) on the WSJ corpus. We attribute this discrepancy to the slight differences in tag inventory used in our dataset. For example, when Particles and Prepositions are merged in the WSJ corpus (as they happen to be in our tag inventory and corpus), the performance of Goldwater’s model on WSJ is similar to what we report here. Entropy Mono- Bilingual Absolute lingual Gain EN 0.566 90.71 91.01 +0.30 SR 0.554 85.05 90.06 +5.03 EN 0.578 90.71 92.00 +1.29 BG 0.543 88.88 94.48 +5.61 EN 0.571 90.71 92.01 +1.30 SL 0.568 87.41 88.54 +1.13 SL 0.494 87.41 95.10 +7.69 SR 0.478 85.05 91.75 +6.70 BG 0.568 88.88 </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-ofspeech tagging. In Proceedings of the ACL, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>Proceedings of HLTNAACL,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="7084" citStr="Haghighi and Klein, 2006" startWordPosition="1073" endWordPosition="1076">994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particular language. 3 Model We propose a bilingual model for unsupervised partof-speech tagging that jointly tags parallel streams of text in two languages. Once the parameters have been learned using an untagged bilingual parallel text, the model is applied to a held-out monolingual test set. Our key hypothesis is that the patterns of ambiguity found in each language at the part-of-speech level will differ in syst</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. Proceedings of HLTNAACL, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W K Hastings</author>
</authors>
<title>Monte carlo sampling methods using Markov chains and their applications.</title>
<date>1970</date>
<journal>Biometrika,</journal>
<pages>57--97</pages>
<contexts>
<context position="17684" citStr="Hastings, 1970" startWordPosition="2864" endWordPosition="2865">ved for cases where the succeeding tags are not aligned to each other, but to other tags. 4.2 Sampling Transition Parameters and the Coupling Parameter When computing the joint probability of an aligned tag pair (Equation 2), we employ the transition parameters φ, φ′ and the coupling parameter ω in a normalized product. Because of this, we can no longer regard these parameters as simple multinomials, and thus can no longer sample them using the standard closed formulas. Instead, to resample these parameters, we resort to the Metropolis-Hastings algorithm as a subroutine within Gibbs sampling (Hastings, 1970). Metropolis-Hastings is a Markov chain sampling technique that can be used when it is impossible to directly sample from the posterior. Instead, samples are drawn from a proposal distribution and then stochastically accepted or rejected on the basis of: their likelihood, their probability under the proposal distribution, and the likelihood and proposal probability of the previous sample. We use a form of Metropolis-Hastings known as an independent sampler. In this setup, the proposal distribution does not depend on the value of the previous sample, although the accept/reject decision · φxi−1(</context>
</contexts>
<marker>Hastings, 1970</marker>
<rawString>W. K. Hastings. 1970. Monte carlo sampling methods using Markov chains and their applications. Biometrika, 57:97–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP/CoNLL,</booktitle>
<pages>296--305</pages>
<contexts>
<context position="6642" citStr="Johnson, 2007" startWordPosition="1012" endWordPosition="1013">ast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual </context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of EMNLP/CoNLL, pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun S Liu</author>
</authors>
<title>The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation problem.</title>
<date>1994</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>89</volume>
<issue>427</issue>
<contexts>
<context position="14160" citStr="Liu, 1994" startWordPosition="2257" endWordPosition="2258">(Geman and Geman, 1984). Gibbs sampling begins by randomly initializing all unobserved random variables; at each iteration, each random variable zi is sampled from the conditional distribution P(zi|z−i), where z−i refers to all variables other than zi. Eventually, the distribution over samples drawn from this process will converge to the unconditional joint distribution P(z) of the unobserved variables. When possible, we avoid explicitly sampling variables which are not of direct interest, but rather integrate over them—this technique is known as “collapsed sampling,” and can reduce variance (Liu, 1994). We sample: (i) the bilingual tag sequences (x, y), (ii) the two sets of transition parameters φ and φ′, and (iii) the coupling parameter ω. We integrate over the emission parameters θ and θ′, whose priors are Dirichlet distributions with hyperparameters θ0 and θ′0. The resulting emission distribution over words ei, given the other words e−i, the tag sequences x φ′yj−1(yj) · (1) P(xi, yj|xi−1, yj−1, φ, φ′, ω) = φxi−1(xi) φ′yj−1(yj) ω(xi,yj) (2) Z 1044 and the emission prior θ0, can easily be derived as: P(ei |x, e−i, θ0) = 10x θxi (ei) P(θxi |θ0) dθxi (3) i n(xi, ei) + θ0 n(xi) + Wxiθ0 Here, </context>
</contexts>
<marker>Liu, 1994</marker>
<rawString>Jun S. Liu. 1994. The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation problem. Journal of the American Statistical Association, 89(427):958–966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging english text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="6463" citStr="Merialdo (1994)" startWordPosition="982" endWordPosition="983">Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Hagh</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging english text with a probabilistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Bin Wang</author>
<author>Yee Seng Chan</author>
</authors>
<title>Exploiting parallel texts for word sense disambiguation: an empirical study.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="5616" citStr="Ng et al., 2003" startWordPosition="851" endWordPosition="854">ferentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Sn</context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Exploiting parallel texts for word sense disambiguation: an empirical study. In Proceedings of the ACL, pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Optimal constituent alignment with edge covers for semantic projection.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1161--1168</pages>
<marker>Pad´o, Lapata, 2006</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2006. Optimal constituent alignment with edge covers for semantic projection. In Proceedings ofACL, pages 1161 – 1168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>A perspective on word sense disambiguation methods and their evaluation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="5598" citStr="Resnik and Yarowsky, 1997" startWordPosition="847" endWordPosition="850">ponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphologica</context>
</contexts>
<marker>Resnik, Yarowsky, 1997</marker>
<rawString>Philip Resnik and David Yarowsky. 1997. A perspective on word sense disambiguation methods and their evaluation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>354--362</pages>
<contexts>
<context position="7057" citStr="Smith and Eisner, 2005" startWordPosition="1069" endWordPosition="1072"> the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particular language. 3 Model We propose a bilingual model for unsupervised partof-speech tagging that jointly tags parallel streams of text in two languages. Once the parameters have been learned using an untagged bilingual parallel text, the model is applied to a held-out monolingual test set. Our key hypothesis is that the patterns of ambiguity found in each language at the part-of-speec</context>
<context position="20113" citStr="Smith and Eisner, 2005" startWordPosition="3264" endWordPosition="3268"> Gibbs sampling the hyperparameters θ0 and θ′ 0 are re-estimated using a single Metropolis-Hastings move. The proposal distribution is set to a Gaussian with mean at the current value and variance equal to one tenth of the mean. 5 Experimental Set-Up Our evaluation framework follows the standard procedures established for unsupervised part-of-speech tagging. Given a tag dictionary (i.e., a set of possible tags for each word type), the model has to select the appropriate tag for each token occurring in a text. We also evaluate tagger performance when only incomplete dictionaries are available (Smith and Eisner, 2005; Goldwater and Griffiths, 2007). In both scenarios, the model is trained only using untagged text. In this section, we first describe the parallel data and part-of-speech annotations used for system evaluation. Next we describe a monolingual baseline and our procedures for initialization and hyperparameter setting. Data As a source of parallel data, we use Orwell’s novel “Nineteen Eighty Four” in the original English as well as translations to three Slavic languages — Bulgarian, Serbian and Slovene. This data is distributed as part of the Multext-East corpus which is publicly available. The c</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the ACL, pages 354–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL/HLT,</booktitle>
<pages>737--745</pages>
<contexts>
<context position="6240" citStr="Snyder and Barzilay, 2008" startWordPosition="950" endWordPosition="953">3). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of a</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In Proceedings of the ACL/HLT, pages 737– 745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian lda-based model for semi-supervised partof-speech tagging.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20,</booktitle>
<pages>1521--1528</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6725" citStr="Toutanova and Johnson (2008)" startWordPosition="1022" endWordPosition="1026">e. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particular language. 3 Model We propose a bilingual model fo</context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2008. A Bayesian lda-based model for semi-supervised partof-speech tagging. In Advances in Neural Information Processing Systems 20, pages 1521–1528. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Hongsing Wong</author>
</authors>
<title>Machine translation with a stochastic grammatical channel.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACL/COLING,</booktitle>
<pages>1408--1415</pages>
<contexts>
<context position="5303" citStr="Wu and Wong, 1998" startWordPosition="802" endWordPosition="805">el significantly reduces the gap between unsupervised and supervised performance. For instance, in the case of Slovene this gap is reduced by 71%. We also observe significant variation in the level of improvement across language pairs. We show that a cross-lingual entropy measure corresponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection me</context>
</contexts>
<marker>Wu, Wong, 1998</marker>
<rawString>Dekai Wu and Hongsing Wong. 1998. Machine translation with a stochastic grammatical channel. In Proceedings of the ACL/COLING, pages 1408–1415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhai Xi</author>
<author>Rebecca Hwa</author>
</authors>
<title>A backoff model for bootstrapping resources for non-english languages.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>851--858</pages>
<contexts>
<context position="5879" citStr="Xi and Hwa, 2005" startWordPosition="897" endWordPosition="900">f synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has be</context>
</contexts>
<marker>Xi, Hwa, 2005</marker>
<rawString>Chenhai Xi and Rebecca Hwa. 2005. A backoff model for bootstrapping resources for non-english languages. In Proceedings of EMNLP, pages 851 – 858.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="5996" citStr="Yarowsky and Ngai, 2001" startWordPosition="914" endWordPosition="917">005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include </context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In Proceedings of the NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora. In</title>
<date>2000</date>
<booktitle>Proceedings ofHLT,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="5813" citStr="Yarowsky et al., 2000" startWordPosition="885" endWordPosition="888"> translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-o</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2000</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2000. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings ofHLT, pages 161–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>