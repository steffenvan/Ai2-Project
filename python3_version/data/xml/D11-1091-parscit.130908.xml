<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000389">
<title confidence="0.9964985">
Unsupervised Learning of Selectional Restrictions and Detection of
Argument Coercions
</title>
<author confidence="0.99546">
Kirk Roberts and Sanda M. Harabagiu
</author>
<affiliation confidence="0.995607">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.744307">
Richardson, TX 75083, USA
</address>
<email confidence="0.999709">
{kirk,sanda}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999539142857143">
Metonymic language is a pervasive phe-
nomenon. Metonymic type shifting, or ar-
gument type coercion, results in a selectional
restriction violation where the argument’s se-
mantic class differs from the class the predi-
cate expects. In this paper we present an un-
supervised method that learns the selectional
restriction of arguments and enables the de-
tection of argument coercion. This method
also generates an enhanced probabilistic reso-
lution of logical metonymies. The experimen-
tal results indicate substantial improvements
the detection of coercions and the ranking of
metonymic interpretations.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999858666666667">
Metonymic language is pervasive in today’s social
interactions. For example, it is typical to find ques-
tions that require metonymic resolution:
</bodyText>
<listItem confidence="0.975294">
(Q1) Did you enjoy War and Peace?
(Q2) Does anyone have any advice on how to start
a bowling team?1
</listItem>
<bodyText confidence="0.9998962">
In order to process such questions and capture the
intention of the person that posed them, coercions
are needed. Question (Q1) is interpreted as whether
you enjoyed reading “War and Peace”, while (Q2)
is interpreted as asking for advice on organizing,
forming, or registering a bowling team. The qual-
ity of the answers therefore depends on the ability
to (1) recognize when metonymic language is used,
and (2) to produce coercions that capture the user’s
intention. One important step in this direction was
</bodyText>
<footnote confidence="0.405124">
1Both questions taken from Yahoo Answers.
</footnote>
<bodyText confidence="0.999760882352941">
taken by SemEval-2010 Task 7, which focused on
the ability to recognize (a) an argument’s selectional
restriction for predicates such as arrive at, cancel,
or hear, and (b) the type of coercion that licensed
a correct interpretation of the metonymy. Details of
the task are reported in (Pustejovsky et al., 2010).
Approaches to metonymy based on this task are lim-
ited, however, because (a) the task is focused only on
semantically non-ambiguous predicates and (b) the
selectional restrictions of the arguments were cho-
sen from a pre-defined set of six semantic classes
(artifact, document, event, location, proposition, and
sound). However, metonymy coercion systems ca-
pable of providing the interpretations of questions
(Q1) and (Q2) clearly cannot operate with the sim-
plifications designed for this task.
Inspired by recent advances in modeling selec-
tional preferences with latent-variable models (Rit-
ter et al., 2010; O´ S´eaghdha, 2010), we propose
an unsupervised model for learning selectional re-
strictions. The model assumes that (1) arguments
have a single selected class exemplified by the se-
lectional restriction, and (2) the selected class can
be inferred from the data, in part by modeling how
coercive each predicate is. The model is capable of
operating with both ambiguous and disambiguated
predicates, producing superior results for predicates
that have been disambiguated. The selectional re-
strictions and coercions detected by the model re-
ported in this paper can be used to enhance the logi-
cal metonymy approach reported in Lapata and Las-
carides (2003). The experimental results show a sig-
nificant improvement in the ranking of interpreta-
tions.
</bodyText>
<page confidence="0.956704">
980
</page>
<note confidence="0.957954">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 980–990,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9992145">
The rest of this paper is organized as follows.
Section 2 discusses related work. Section 3 de-
tails unsupervised models that inform detection of
metonymies. Section 4 outlines a method for disam-
biguating ambiguous predicates. Section 5 describes
the enhanced interpretation of logical metonymies
when conventional constraints are known. Section 6
outlines our implementation and experimental de-
sign. Section 7 presents our experimental results in
three broad tasks: (i) semantic class induction, (ii)
coercion detection, and (iii) logical metonymy inter-
pretation. Section 8 summarizes the conclusions.
</bodyText>
<sectionHeader confidence="0.998457" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999792763157895">
Lapata and Lascarides (2003) propose a probabilis-
tic ranking model for logical metonymies. They es-
timate these probabilities using co-occurrence fre-
quencies of predicate-argument pairs in a corpus.
Shutova (2009) extends this approach to provide
sense-disambiguated interpretations from WordNet
(Fellbaum, 1998) by using the alternative interpre-
tations to disambiguate polysemous words. Shutova
and Teufel (2009) extend this approach further by
clustering these sense-disambiguated interpretations
into distinct groups of meaning (e.g., {read, browse,
look through} and {write, produce, work on} for
“enjoy book”). Not only do these approaches as-
sume logical metonymies have already been iden-
tified, but they are susceptible to providing interpre-
tations that are themselves logical metonymies (e.g.,
finish book). In this paper, we propose an enhance-
ment to resolving logical metonymies by ruling out
event-invoking predicates in order to provide more
semantically valid interpretations.
Recently, the resolution of several linguistic prob-
lems has benefited from Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003) models. O´ S´eaghdha
(2010) examines several selectional preference mod-
els based on LDA in predicting human judgements
on predicate-argument plausibility. Both LDA and
an extension, ROOTH-LDA (based on Rooth et al.
(1999)), perform well at predicting plausibility on
unseen predicate-argument pairs. Inspired by these
results, we propose to extend selectional preference
models in order to learn selectional restrictions.
Alternatively, unsupervised algorithms exist that
both induce semantic classes (Rooth et al., 1999;
Lin and Pantel, 2001) and cluster predicates by their
selectional restrictions (Rumshisky et al., 2007) but
none of these provide a sufficient framework for de-
termining if a specific argument violates its predi-
cate’s selectional restriction.
</bodyText>
<sectionHeader confidence="0.9898695" genericHeader="method">
3 Unsupervised Learning of Selectional
Restrictions
</sectionHeader>
<bodyText confidence="0.9999045">
In predicate-argument structures, predicates impose
selectional restrictions in the form of semantic ex-
pectations on their arguments. Whenever the seman-
tic class of the argument meets these constraints a
selection occurs. For example, the predicate “hear”
imposes the semantics related to sound on the ar-
gument “voice”. Because the semantic class for
“voice” conforms to these constraints, we call its se-
mantic class the selected class. However, when the
semantic class of the argument violates these con-
straints, we follow Pustejovsky et al. (2010) and re-
fer to this as a coercion. In this case, we call the
argument’s semantic class the coerced class. For ex-
ample, “hear speaker” is a coercion where the ar-
gument class, person, is implicitly coerced into the
voice of the speaker, a sound.
</bodyText>
<subsectionHeader confidence="0.997829">
3.1 A Baseline Model
</subsectionHeader>
<bodyText confidence="0.987141769230769">
We consider the LDA-based selectional prefer-
ence model reported in O´ S´eaghdha (2010) as a
baseline for modeling selectional restrictions. For-
mally, we define our LDA baseline model as follows.
Let V be the predicate vocabulary size, let A be the
argument vocabulary size, and let K be the number
of argument classes. Let a&amp;quot;� be the ith (non-unique)
argument realized by predicate v. Let c&amp;quot;� be the class
for a&amp;quot;� . Let B&amp;quot; be the class distribution for predicate
v and Ok be the argument distribution for class k.
The graphical model for this LDA is shown in Fig-
ure 1(a). The generative process for LDA is:
For each argument class k = 1..K:
</bodyText>
<listItem confidence="0.976694166666667">
1. Choose Ok — Dirichlet(β)
For each unique predicate v = 1..V :
2. Choose B&amp;quot; — Dirichlet(α)
For every argument i = 1..n&amp;quot;:
3. Choose c&amp;quot;� — Multinomial(B&amp;quot;)
4. Choose a&amp;quot;� — Multinomial(O&apos;i )
</listItem>
<bodyText confidence="0.6407575">
Following Griffiths and Steyvers (2004), we col-
lapse B and O and estimate the model using Gibbs
</bodyText>
<page confidence="0.992701">
981
</page>
<figure confidence="0.996634428571429">
(a)
α θ
Q φ
K
c a
N
V
(b)
70
71
α
e
s
T
Q �
K
x
c
a
N
V
</figure>
<figureCaption confidence="0.999744">
Figure 1: Graphical models for (a) LDA, and (b) coercion LDA (cLDA).
</figureCaption>
<bodyText confidence="0.929918">
Sampling. This yields the update equation:
</bodyText>
<equation confidence="0.992831333333333">
fak + β
p(cv i = k|av; α, β) ∝ fvk + α (1)
fv + Kα fk + Aβ
</equation>
<bodyText confidence="0.998699333333333">
Where fak is the frequency of argument a being as-
signed class k; fk is the frequency of class k being
assigned to any argument; fvk is the frequency of
predicate v having an argument of class k; and fv is
the total number of non-unique arguments for pred-
icate v.
</bodyText>
<subsectionHeader confidence="0.995264">
3.2 A Coercion Model
</subsectionHeader>
<bodyText confidence="0.959283333333333">
We now incorporate our assumptions for selec-
tional restriction modeling. Namely: (1) there is
one selected class per predicate, and (2) the predi-
cate’s selected class can be chosen from the classes
of its arguments. To accomplish this, we must also
account for the coerciveness of each predicate. We
assign a latent variable τv for each predicate v that
controls how coercive v should be. The additional
hyper-parameters γ0 and γ1 act as priors on τv. The
generative process for this coercion LDA model,
which we denote cLDA, is:
For each argument class k = 1..K:
</bodyText>
<listItem confidence="0.9705547">
1. Choose φk ∼ Dirichlet(β)
For each unique predicate v = 1..V :
2. Choose sv ∼ Uniform(1,K)
3. Choose θv ∼ Dirichlet(α)2
4. Choose τv ∼ Beta(γ0, γ1)
For every argument i = 1..nv:
5. Choose cvi ∼ Multinomial(θv)
6. Choose xvi ∼ Bernoulli(τv)
7. If xvi = 1, Choose avi ∼ Multinomial(φc&amp;quot;i )
Else Choose avi ∼ Multinomial(φs&amp;quot;)
</listItem>
<bodyText confidence="0.9982255">
The model variable sv represents the selected class
for predicate v. The coerced class is represented
</bodyText>
<footnote confidence="0.605367666666667">
2With the exception that the probability of drawing the se-
lected class sv is zero. This can be seen as drawing the multi-
nomial θv from a Dirichlet distribution with K-1 components.
</footnote>
<bodyText confidence="0.999737952380952">
for each argument i by cvi , where xvi chooses be-
tween the selected and coerced class. The variable
xvi is similar to switching variables in other graph-
ical models such as Chemudugunta et al. (2007)
and Reisinger and Mooney (2010), where switch-
ing variables are used to choose between a back-
ground distribution and a document-specific distri-
bution. In this case, the switching variable chooses
between a specific class and a predicate-specific dis-
tribution. The graphical model for cLDA is shown
in Figure 1(b). Note that cLDA is virtually equiva-
lent to LDA when τv is 1 and γ1 is small because the
selected class will be ignored. In this way, highly co-
ercive predicates have less of an impact on the argu-
ment clustering because they are more reliant on the
multinomial θ. We use Gibbs sampling to perform
model inference and collapse θ, φ, and τ, integrat-
ing them out using multinomial-Dirichlet conjugacy
(the Beta distribution used by τ is just a special case
of the Dirichlet with only two parameters).
The update formula for the selected class sv is:
</bodyText>
<equation confidence="0.982627285714286">
p(sv = k|av, cv, xv; α, β)
nv
�∝ P(avi |sv = k; β)
i
�∝
i∈Sv favi k + β
fk + Aβ (2)
</equation>
<bodyText confidence="0.9953494">
Where nv is the number of argument observations
for predicate v; Sv is the set of arguments of v that
are selections; and fav i k is the frequency of word av i
being assigned to class k for any predicate. We then
sample cvi and xvi jointly:
</bodyText>
<equation confidence="0.996355428571429">
p(cvi = k, xvi = q|sv, cv ¯i , xv¯i , av; α, β,γ)
∝ p(cvi =k; α)p(xvi =q; γ)p(avi |sv, cv¯i , xv¯i , av; β)
faz + β (3)
fz + Aβ
∝ fvk + α fvq + γq
fv0 + γ0 + fv1 + γ1
fv + Kα
</equation>
<page confidence="0.982404">
982
</page>
<bodyText confidence="0.9999903125">
Where fvq, fv0, and fv1 is the frequency of x values
that equal q, 0, and 1, respectively, for predicate v;
faz is the frequency of word a being in class z and
fz is the frequency all words being in class z, where
z is defined as being equal to k when xvi = 1, or sv
when xvi = 0.
Note that Equation (2) results in a sampling of
the selected class for v proportional to the number
of arguments in each class for v, fulfilling our sec-
ond assumption. Also note from Equation (3), the
second term corresponds to the coerciveness of the
predicate. When the predicate is very coercive, the
marginal probability associated with xvi = 0 will be
very low. If all predicates become entirely coercive,
most x values will become 1 and the cLDA will be-
come almost equivalent to an LDA model.
</bodyText>
<subsectionHeader confidence="0.998486">
3.3 Coercion Detection
</subsectionHeader>
<bodyText confidence="0.999840777777778">
After the latent parameters have been estimated,
we still require a method to determine if a given
predicate-argument pair is a coercion or not. We
assign a score in [0, 1] instead of a binary value.
Higher scores (near 1) indicate high likelihood of
selection, while lower scores (near 0) indicate coer-
cion. The LDA model must rely on a scoring method
using the predicate-class and argument-class mix-
tures:
</bodyText>
<equation confidence="0.971939666666667">
P(k|v)P(a|k)
θv kφk (4)
a
</equation>
<bodyText confidence="0.999864769230769">
Where θvk represents the probability of any argument
of v being in the class k and φka represents the prob-
ability of the argument a being in class k for any
predicate. C1 is also available as a scoring method
for cLDA by including the proportion of the selected
class sv in θ. Note that since θ and φ are integrated
out for both LDA and cLDA, we instead use their
frequencies smoothed with α and β, respectively,
which is their maximum likelihood estimate.
The cLDA model contains two useful parameters
that can identify selections and coercions: the se-
lected class s and the coercion indicator x. This
yields two more coercion scoring metrics:
</bodyText>
<equation confidence="0.995143857142857">
C2(v,a) = P(a|sv)
= φsv (5)
a
C3(v, a) = P(xva = 0|v, a)
�i∈Iv a xv i
= 1.0 − (6)
|Iva|
</equation>
<bodyText confidence="0.999352666666667">
Where sv is the selected class for predicate v; Iva
is the set of predicate-argument instances for pred-
icate v and argument a; and xvi is 0 for a selection
and 1 for a coercion. Of the three metrics, C3 is the
most direct measure of a coercion as it represents
the average decision the model learned on the same
predicate-argument pair. However, C3 requires a
large sample of instances for a particular predicate
and argument, and so may be quite sparse. In prac-
tice, these different metrics have their own strengths
and weaknesses and the best performing method of-
ten depends on the final task.
</bodyText>
<sectionHeader confidence="0.968617" genericHeader="method">
4 Predicate Sense Induction
</sectionHeader>
<bodyText confidence="0.999992592592592">
Our assumption of a single selected class per predi-
cate ignores predicate polysemy. However, the same
lexical item may have multiple meanings, each with
a separate selected class. We therefore propose a
method of partitioning a predicate’s arguments by
the induced senses of the predicate. This allows sep-
arate induced predicates to each select a separate ar-
gument class. Consider the verb fire, which has at
least two distinct common senses: (1) to shoot or
propel an object (e.g., to fire a gun), and (2) to lay
someone off (e.g., to fire an employee). The first
sense selects a weapon (e.g., gun, bullet, rocket),
while the second sense selects a person (e.g., em-
ployee, coach, apprentice).
Specifically, we employ tiered clustering
(Reisinger and Mooney, 2010) using the words
in the predicate’s context. Tiered clustering is a
discrete clustering method, as opposed to methods
such as (Brody and Lapata, 2009) that assign a
distribution of word senses to each word instance.
Tiered clustering has several advantages over
other discrete clustering approaches. First, tiered
clustering learns a background word distribution in
addition to the clusters. This reduces the impact that
words common to most senses have on the cluster-
ing process and allow clusters to form around only
the most salient words. Second, tiered clustering
</bodyText>
<equation confidence="0.9988284">
K
C1(v, a) =
k
K
k
</equation>
<page confidence="0.99693">
983
</page>
<table confidence="0.999534666666667">
Cluster 1 Cluster 2 Cluster 3 Cluster 4
(18,391) (16,651) (18,749) (11,833)
shots ball hire gun
gun puck letter imagination
Israeli hired Yeltsin grill
missiles owner minister laser
rockets shots workforce cells
officers coaches executives engine
soldiers net employee brain
rounds circle managers !
bullets Johnson hired engines
weapons Williams union fire
</table>
<tableCaption confidence="0.992580333333333">
Table 1: Context word clusters resulting from tiered clus-
tering for the verb fire (includes the number of unique
words belonging to each cluster).
</tableCaption>
<bodyText confidence="0.998985157894737">
uses a Chinese Restaurant Process (CRP) prior to
control both the formation of new clusters (senses)
and the bias toward larger clusters (more common
senses). This conforms with our intuition of how
word senses are distributed: a few common senses
with a gradual transition to a long tail of rare senses.
When deciding which cluster to use for a given
predicate-argument pair, we use the cluster most
associated with the argument.
We use a 10-token window around the predicate
as features. The result of predicate induction on the
verb fire is shown in Table 4. The first three clusters
can be interpreted to be about (1) firing weapons, (2)
sport-related shots (e.g., “fired the puck”), and (3)
lay-offs. One must be careful in choosing the param-
eters for induction, however, as it is possible to par-
tition a unique word sense such that coercions and
selections are placed in a separate clusters. Section 6
discusses our parameter selection experiments.
</bodyText>
<sectionHeader confidence="0.994318" genericHeader="method">
5 Logical Metonymy Interpretation
</sectionHeader>
<bodyText confidence="0.999988605263158">
Logical metonymies are a unique class of coercions
due to the fact that their eventive interpretation can
be derived from verbal predicates. For instance, for
the logical metonymy “enjoy book”, we know that
read is a good candidate interpretation because (1)
books are objects whose purpose is to be read and
(2) reading is an event that may be enjoyed. We
therefore expect to see many instances of both “read
book” and “enjoy reading” (Lapata and Lascarides,
2003). Conversely, for coercions with non-eventive
interpretations, such as “arrive at meeting”, the in-
terpretation (location of) is more dependent on the
predicate (arrive) than the function of its argument
(meeting).
In this section, we limit our discussion of logical
metonymy to the verb-object case, its correspond-
ing baseline for ranking interpretations, and our pro-
posed enhancements. However, similar baselines
exist for other types of logical metonymy, such as
adjective-noun and noun-noun. Since our enhance-
ment does not depend on any syntactic information
beyond the predicate-argument instances needed for
Section 3.2, it could easily be applied to those as
well.
Lapata and Lascarides (2003) propose a proba-
bilistic ranking model where the probability of an
interpretation e for a verb-object pair (v, o) is pro-
portional to the probability of all three in a verb-
interpretation-object pattern.3 For example, the
probability that read is the correct interpretation of
“enjoy book” is proportional to the likelihood of see-
ing “enjoy reading book” expressed as a syntactic
dependency in a sufficiently large corpus. Due to
data sparsity, they approximate this likelihood of
seeing the object given the verb and interpretation
to simply the likelihood of seeing the object given
the interpretation. We denote this logical metonymy
ranking method as LMLL, formally defined as:
</bodyText>
<equation confidence="0.999307333333333">
LMLL(e; v, o) = Pc(v, e, o)
= Pc(e)Pc(v|e)Pc(o|e,v)
≈ Pc(e)Pc(v|e)Pc(o|e)
fc(v, e)fc(o, e)
≈ (7)
Nfc(e)
</equation>
<bodyText confidence="0.999909181818182">
Where Pc and fc indicate probability and frequency,
respectively, derived from corpus counts. See Lap-
ata and Lascarides (2003) for a detailed explanation
of how these frequencies are obtained.
This model, which we consider our baseline, is
only partially correct as the corpus will contain co-
ercions that form invalid interpretations. Consider
the phrases “enjoy finishing a book” and “enjoy
discussing a book”. Both “finish book” and “dis-
cuss book” are coercions (and logical metonymies)
themselves, and do not form a valid interpretation.4
</bodyText>
<footnote confidence="0.966868333333333">
3They use two patterns: “v e-ing o” and “v to e o”, where e
is tagged as a verb.
4For evidence of the frequency of these phrases, at the time
of this writing, “enjoy finishing a book” and “enjoy finishing
the book” have a combined 728 Google hits, while “enjoy dis-
cussing a book” and “enjoy discussing the book” have a com-
</footnote>
<page confidence="0.989786">
984
</page>
<bodyText confidence="0.987426074074074">
Thus, when discovering interpretations for logical discover the relative value of P, and Px:
metonymies, we must be aware of the selectional re- LMWT(e; v, o) = w1P,(v, e, o)+w2Px(e, o) (11)
strictions of candidate interpretations.
We propose to incorporate the coercion probabil-
ity learned by our cLDA model in order to rank only
those interpretations that are considered selections:
LM′(e; v, o) = P(v, e, o, xeo = 0) (8)
However, due to the approximations made to esti-
mate P,(v, e, o), this probability cannot be directly
calculated as not all the frequencies reflect verb-
object counts. Instead, we can combine the corpus
probability P,(v, e, o) with the probability that the
verb-object pair (e, o) is a coercion in our model.
We denote this probability Px(e, o), and it may be
derived from the scoring metrics in Equations (4),
(5), or (6) above. We further propose three methods
for enhancing the LMLL baseline using Px(e, o) to
approximate Equation 8.
A naive method for including information from
our cLDA model is to consider the corpus prob-
ability, P,(v, e, o) and the coercion probability,
Px(e, o), to be independent:
LMIND(e; v, o) = P,(v, e, o)Px(e, o) (9)
In other words, the rank of an interpretation is dic-
tated by the unweighted combination of its corpus
probability P, and its coercion probability Px. How-
ever, these two quantities are not likely to be inde-
pendent. Most instances where e is used with either
v or o are in fact selective.5 We therefore experiment
with two shallow learning methods for combining
these two quantities.
The first method is a filtering approach where a
threshold is learned for Px:
LMTH (e; v, o) = r Pc(v, e, o) if Px (e, o) ≥ S (10)
l 0 otherwise
Where the threshold S is learned from a development
set. We expect this model could suffer from noisy Px
values or to simply choose a threshold of zero due to
the prominence of P,.
Finally, we include a weighted linear model to
Where w1 and w2 are learned weights. We dis-
cuss how the parameters for LMTH and LMWT are
learned in the experimental setup below.
6 Experimental Setup
We use the NYT subsection of the English Gigaword
Fourth Edition (Parker et al., 2009) for a total of
1.8M newswire articles. The Stanford Dependency
Parser (de Marneffe et al., 2006) is used to extract
verb-object relations (dobj) that form the input to our
model. To reduce noise, we keep only verbs listed in
VerbNet (Kipper et al., 1998) with at least 100 ar-
gument instances, discarding have and say, which
are too semantically flexible to select from clear se-
mantic classes and so common they distort the class
distributions. This results in 4,145 unique verbs with
51M argument instances (388K unique arguments).
Additionally we use the dependency parser to ex-
tract open clausal complements of verbs (e.g., “like
to swim”) for use in logical metonymy interpreta-
tion. We believe this to be a more reliable alter-
native to the phrase chunk extraction patterns used
in Lapata and Lascarides (2003). We keep clausal
complements (xcomp) where the dependent is either
a gerund or infinitive in order to estimate P,(v|e) in
Equation (7).
For tiered clustering we use the same implemen-
tation as Reisinger and Mooney (2010)6 to partition
the surface form of the verb into one or more in-
duced forms. Instead of using a fixed number of
iterations, the clustering was run for 100 iterations
past the best recorded log-likelihood in order to find
the best possible fit to the data. We tuned the hyper-
parameters by maximizing the log-likelihood on a
small held-out set of 20 predicate-argument pairs
(10 selections, 10 coercions). The resulting parti-
tions were fairly conservative, yielding 12,332 in-
duced verbs or about 3 induced verb forms for every
surface form, with 305 verbs not being partitioned at
all.
We implemented both LDA and cLDA as de-
scribed in Sections 3.1 and 3.2. For the α and β
bined 7,040 Google hits.
5For comparison, “enjoy reading a book” and “enjoy read-
ing the book” have a combined 6.5 million Google hits
6Available at http://github.com/joeraii/UTML-Latent-
Variable-Modeling-Toolkit
985
hyper-parameters, we used the MALLET (McCal-
lum, 2002) defaults of 1.0 and 0.1, respectively, for
both LDA and cLDA. We used the 20 predicate-
argument pairs mentioned above to tune the &apos;y hyper-
parameters as well as the number of iterations. Both
&apos;y0 and &apos;y1 were set to 100. We observed that for
both LDA and cLDA, longer runs (in iterations) re-
sulted in improved model log-likelihood but infe-
rior results in terms of detecting coercions. It is
not uncommon in topic modeling for model likeli-
hood to not be completely correlated with the score
on the task for which the topic model was intended
(see Chang et al. (2009)). Both LDA and cLDA
were found to perform best at 50 iterations on this
data, after which their class distributions were less
“smooth” and became rigidly associated with just a
few classes, thus having a negative impact on coer-
cion detection. While further iterations hurt coer-
cion detection, only minor gains in model likelihood
are seen. We believe the small number of iterations
necessary for the model to converge is therefore a
function of the data. In traditional topic modeling,
documents are generally of similar size (i.e., within
an order of magnitude). But in our data, many pred-
icates have 10,000 times more instances than others.
We have not yet empirically explored the impact of
using a more uniform number of arguments for each
predicate. This issue also makes it difficult to take
multiple samples, which we experimented with un-
successfully.
Our a priori intuition was that as the number
of classes was increased, LDA would improve and
cLDA would degrade due to its assumption of a sin-
gle selected class. However, this did not always bear
out in the results for every task described below.
As such, instead of choosing a specific number of
classes for each model, we describe results for each
model with K = 10, 25, and 50.
For logical metonymy, both LMTH and LMWT
require learned parameters. LMTH needs a learned
threshold while LMWT needs two learned weights.
For both, we split the data set into two partitions,
learn the optimal threshold/weights on one partition,
and use it as the parameters for the other partition.
Both methods are trained on the final scoring metric,
described in Section 7.3. For threshold learning, this
involves finding the optimal cut-off to maximize the
score. For weight learning, we use an exhaustive
</bodyText>
<table confidence="0.998918071428572">
induced predicates? N Y
# classes 10 25 50 10 25 50
LDA NMI .382 .448 .389 .435 .391 .383
Rand .717 .731 .721 .760 .723 .730
F1 .425 .319 .192 .543 .311 .205
B3 (C) .553 .513 .444 .525 .476 .341
B3 (E) .453 .351 .223 .521 .324 .234
MUC .545 .545 .531 .500 .532 .544
cLDA NMI .446 .403 .360 .510 .430 .366
Rand .736 .719 .716 .788 .734 .711
F1 .448 .291 .183 .567 .329 .184
B3 (C) .575 .484 .312 .593 .495 .313
B3 (E) .473 .321 .205 .556 .346 .205
MUC .500 .521 .507 .595 .541 .571
</table>
<tableCaption confidence="0.998619">
Table 2: Clustering scores for induced classes.
</tableCaption>
<bodyText confidence="0.832534">
search over the range {1.0, 0.9, ... , 0.2, 0.1, 10−2,
10−3, ..., 10−14} for both w1 and w2.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="evaluation">
7 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.997402">
7.1 Semantic Class Induction
</subsectionHeader>
<bodyText confidence="0.999983">
For the evaluation of the argument classes in-
duced by our method, we use a subset of the Word-
Net lexicographer files, which correspond to coarse-
grained semantic classes. We chose this form of
evaluation because, unlike a named entity corpus,
no sentential context is required and is therefore
more consistent with the information available to
our model. We use six of the larger, more seman-
tically coherent WordNet classes: artifact, person,
plant, animal, location, and food. We consider each
of these a cluster and compare them to clusters com-
posed of the top ten non-polysemous words (accord-
ing to WordNet) in each of the classes generated
by both the baseline (LDA) and our model (cLDA).
Words not in both sets of clusters are removed. The
result of this evaluation, compared with six cluster-
ing metrics, is shown in Table 2. For descriptions of
NMI, Rand, and cluster F-measure, see Manning et
al. (2008); for the B3 metrics (Cluster and Element),
see Bagga and Baldwin (1998); for the MUC met-
ric, see Vilain et al. (1995). Each metric has differ-
ent strengths and biases in regards to the number and
distribution of clusters, so all are provided to give a
general picture of class induction performance.
The best performing model on all metrics is cLDA
with induced predicates using 10 classes. However,
as the number of classes is increased and the gran-
ularity of the induced classes becomes more fine-
grained, LDA (predictably) outperforms cLDA on
most metrics. This is consistent with our intuition
</bodyText>
<page confidence="0.994898">
986
</page>
<table confidence="0.998897166666667">
induced predicates? N Y
# classes 10 25 50 10 25 50
LDA C1 74.4 78.7 80.5 69.7 70.1 73.4
cLDA C1 80.6 81.2 80.9 76.2 78.4 77.5
C2 75.4 75.9 78.9 73.5 68.3 80.8
C3 67.8 70.8 67.4 70.9 67.4 74.1
</table>
<tableCaption confidence="0.999892">
Table 3: Accuracy on SemEval-2010 Task 7 data.
</tableCaption>
<bodyText confidence="0.991537333333333">
that a single-class assumption degrades as the num-
ber of classes increases.
For this evaluation, predicate induction also im-
proved LDA for smaller numbers of classes, but not
to the degree that it improved cLDA. Without pred-
icate induction, LDA outperforms cLDA on all six
metrics for 25 and 50 classes. With predicate in-
duction, LDA outperforms cLDA on only one metric
for 25 classes and five metrics for 50 classes. Thus
the induced predicates do reduce the negative im-
pact caused by the single selected class assumption
for semantic class induction.
</bodyText>
<subsectionHeader confidence="0.997728">
7.2 Coercion Detection
</subsectionHeader>
<bodyText confidence="0.999980734375">
For the evaluation of coercion detection, we use
the SemEval-2010 Task 7 data (Pustejovsky et al.,
2010). This data uses the most common sense for
each of five predicates (arrive, cancel, deny, fin-
ish, and hear) with a total of 2,070 sentences an-
notated with the argument’s source type (the argu-
ment’s semantic class) and target type (the predi-
cate’s selected class for that argument). We ignore
the actual argument classes and evaluate on the coer-
cion type, which is a selection when the source and
target type match, and a coercion otherwise.
In order to evaluate unsupervised systems on this
data, we use the corresponding training set (1,031
examples) to learn a threshold for coercion detec-
tion. At test time, if the model output is below the
threshold, a coercion is inferred. Otherwise it is con-
sidered a selection. Therefore, the better a model
can rank selections over coercions, the more accu-
rate threshold it will learn. The results for this eval-
uation are shown in Table 3. The baseline for this
task (threshold = 0, or all selections) is 67.4.
The best overall model on this data is cLDA us-
ing the C1 coercion scoring method (Equation (4)).
This method consistently outperforms the baseline
LDA, especially for smaller numbers of classes, per-
forming best with K = 25. The second metric, C2,
was not as reliable. The third metric, C3, performed
poorly on the task. As discussed in Section 3.3, C3
is a direct result of the sampling for the predicate-
argument pair in question and can thus be expected
to perform poorly on rare predicate-argument pairs.
Given that many of the arguments in this data are
rare or unseen in the Gigaword data (e.g., “cancel
Renault”), C3’s poor performance is understandable.
The use of predicate sense induction based on
tiered clustering to overcome the single-class as-
sumption caused significant degradation in perfor-
mance on this task. Using automatically induced
predicates instead of the surface form caused an av-
erage degradation of 2.6 points across the twelve
tests. A potential explanation for this is that
the evaluated predicates have a single dominant
sense, meaning the single class assumption may be
valid for these predicates (the task-defined selected
classes are: location for arrive, event for cancel and
finish, proposition for deny, and sound for hear).
Therefore it would be interesting to evaluate it on
a set of highly polysemous predicates with multi-
ple dominant senses. Furthermore, the introduction
of predicate sense induction was designed to help
cLDA, and the performance degradation for these
nine tests was not as large as it was for LDA. For
cLDA, C1 had an average degradation of 3.5 points
compared to LDA’s C1 average degradation of 6.5
points. cLDA’s C2 had an average degradation of
only 2.5 points and C3 was actually improved by 2.1
points. This suggests that there is value in assign-
ing different selected classes via sense induction, but
that the two-step approach is not beneficial for these
common predicates. This could be overcome by a
joint approach of inducing predicate classes while
simultaneously detecting coercions, as the presence
of many coercions would be an indicator that more
induced predicates are necessary.
</bodyText>
<subsectionHeader confidence="0.999036">
7.3 Logical Metonymy Interpretation
</subsectionHeader>
<bodyText confidence="0.9999705">
For the evaluation of logical metonymy, we use
both an existing data set and a newly created data
set. Shutova and Teufel (2009) annotated 10 verb-
object logical metonymies from Lapata and Las-
carides (2003) with sense-disambiguated interpreta-
tions and organized the interpretations into clusters
representing different possible meanings. For evalu-
ation purposes we ignore the sense annotations and
clusters and consider all lexical matchings of one
of the annotated interpretations to be correct. The
</bodyText>
<page confidence="0.991843">
987
</page>
<table confidence="0.9995416">
induced predicates? N Y
# classes 10 25 50 10 25 50
LMLL 0.381 0.365
LMIND LDA C1 0.415 0.406 0.383 0.386 0.412 0.395
cLDA C1 0.408 0.412 0.412 0.407 0.468 0.439
C2 0.415 0.447 0.419 0.414 0.415 0.434
C3 0.416 0.453 0.455 0.395 0.416 0.402
LMTH LDA C1 0.599 0.568 0.588 0.479 0.520 0.551
cLDA C1 0.571 0.644 0.751 0.497 0.620 0.708
C2 0.544 0.496 0.633 0.457 0.635 0.660
C3 0.601 0.677 0.767 0.472 0.622 0.571
LMWT LDA C1 0.383 0.381 0.379 0.365 0.356 0.361
cLDA C1 0.380 0.387 0.381 0.386 0.377 0.321
C2 0.317 0.342 0.350 0.338 0.340 0.345
C3 0.378 0.370 0.350 0.387 0.382 0.384
</table>
<tableCaption confidence="0.9916735">
Table 4: Mean average precision (MAP) scores on the Shutova and Teufel (2009) data set. The bold items indicate the
best scores with/without induced predicates as well as using/not using a threshold-based interpretation method.
</tableCaption>
<table confidence="0.999699266666667">
induced predicates? N Y
# classes 10 25 50 10 25 50
LMLL 0.274 0.248
LMIND LDA C1 0.291 0.286 0.294 0.263 0.267 0.255
cLDA C1 0.296 0.298 0.285 0.280 0.274 0.288
C2 0.291 0.287 0.288 0.283 0.271 0.285
C3 0.318 0.317 0.333 0.298 0.285 0.307
LMTH LDA C1 0.478 0.534 0.534 0.414 0.495 0.479
cLDA C1 0.449 0.504 0.541 0.391 0.495 0.513
C2 0.505 0.478 0.456 0.398 0.429 0.440
C3 0.449 0.496 0.577 0.382 0.439 0.446
LMWT LDA C1 0.276 0.270 0.271 0.248 0.251 0.249
cLDA C1 0.271 0.272 0.270 0.257 0.259 0.265
C2 0.274 0.274 0.266 0.250 0.259 0.261
C3 0.271 0.273 0.274 0.253 0.262 0.259
</table>
<tableCaption confidence="0.985307">
Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations.
The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-based
interpretation method.
</tableCaption>
<bodyText confidence="0.999609117647059">
data contains an average of 11 interpretations per
metonymy and has a reported 70% recall.
In order to create a larger data set, we identified
100 verb-object logical metonymies, including those
used in Lapata and Lascarides (2003). Three anno-
tators were asked to provide up to five interpreta-
tions for each metonymy (they were not provided
with any verbs from which to choose, only the verb-
object pair). The annotators provided an average of
4.6 interpretations per metonymy. Because our goal
was recall, inter-annotator agreement was necessar-
ily low, and each logical metonymy had an average
of 11.7 unique interpretations. All annotators agreed
on at least one interpretation for 40 metonymies,
while for 14 they had no interpretations in common.7
Since logical metonymy interpretation is usually
evaluated as a ranking task, we score our methods
</bodyText>
<footnote confidence="0.380331">
7 Data available at
http://www.hlt.utdallas.edu/∼kirk/data/lmet.zip
</footnote>
<bodyText confidence="0.636449">
using mean average precision (MAP):
</bodyText>
<equation confidence="0.894056">
�Nn=1 prec(n) × rel(n) (12)
interps(q)
</equation>
<bodyText confidence="0.999441666666667">
Where Q is the number of metonymies evaluated;
N is the number of interpretations ranked; prec(n)
is the precision at rank n; rel(n) = 1 if interpreta-
tion n is valid, 0 otherwise; and interps(q) is the
number of valid interpretations for the metonymy q.
We rank all 4,145 verbs as interpretations except for
those removed by the threshold technique, as they
have a score of zero. This can give LMTH artifi-
cially high MAP scores since it may remove some
valid interpretations that are low-ranking. However,
since a smaller, higher precision list may be useful
for many applications we still consider MAP a valid
metric and indicate both the highest scoring method
and the highest scoring non-threshold method. The
results on the Shutova and Teufel (2009) data are
</bodyText>
<figure confidence="0.8449585">
1
MAP = Q
� Q
q=1
</figure>
<page confidence="0.992734">
988
</page>
<bodyText confidence="0.999984053571429">
shown in Table 4. The results on our own data are
shown in Table 5.
The scores reported in the Shutova and Teufel
(2009) data are noticeably higher than the data we
annotated. Since the metonymies in our data are a
super-set of those in their data, and since for those
metonymies our annotators provided approximately
the same number of interpretations (110 versus 120),
this likely indicates the remaining metonymies in
our data are more difficult.
In all cases the best reported scores use cLDA.
Unlike coercion detection on the SemEval data, C3
performs very well, achieving the highest scores
when no predicate sense induction is used. Also un-
like coercion detection, LDA scores do not increase
as the number of classes increase. We suspect both
these differences have to do with the fact that the ar-
guments in this data are far more common. Since
LDA is a selectional preference model and its co-
ercion scores correspond roughly to the plausibility
of seeing a predicate-argument pair, it is less able to
distinguish coercions in common arguments.
Of the logical metonymy ranking methods,
LMTH consistently produces the highest MAP
scores. However, as stated before, by using a cut-off
and removing low-ranking valid interpretations, the
MAP score is increased, which might not be applica-
ble to some applications. The best non-thresholded
ranking method is LMIND, which naively combines
the LMLL score with the coercion probability. In
almost every case this beats out LMWT. Upon in-
spection, we observed that the range of scale for the
LMLL scores are very inconsistent. This can make
it difficult to learn a linear model using these scores
as features, and as a result the learned weights were
forced to ignore the coercion score and rely entirely
on LMLL. We attempted other scaling methods,
such as a rank-based method, but these had poor re-
sults as well, so we leave the problem of the super-
vised learning these weights to future work.
Using induced senses did not result in the dras-
tic and consistent degradation in performance seen
on the SemEval data, and the highest non-threshold
result for the Shutova and Teufel (2009) data used
predicate induction. Both metonymy data sets were
limited to the verbs found in Lapata and Lascarides
(2003), which are still quite common (attempt, be-
gin, enjoy, expect, finish, prefer, start, survive, try,
want). However, the verbs used in our data set had a
greater number of WordNet senses attested in a cor-
pus than the SemEval data (an average of 4.4 senses
for our data versus 3.0 senses for the SemEval data).
This suggests the potential value of sense induction
for highly polysemous predicates and further moti-
vates the integration of sense induction within a se-
lectional restriction model.
</bodyText>
<sectionHeader confidence="0.998507" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99998788">
We have presented a novel topic model that ex-
tends an unsupervised selectional preference model
(LDA) to an unsupervised selectional restriction
model (cLDA) using two assumptions. For the first
assumption, that each predicate has a single selected
class, we proposed a predicate induction method to
overcome predicate polysemy. This improved re-
sults for semantic class induction but proved harmful
for detecting coercions on common predicates with
a single, dominant sense. For the second assump-
tion, that the selected class can be inferred from the
data, we proposed a sampling method based on the
classes of the predicate’s arguments. Superior per-
formance on coercion detection shows the merit of
this assumption.
Additionally, we proposed methods for improving
an existing task, logical metonymy interpretation,
using the learned parameters of our model, showing
positive results.
It is clear that our model may be improved by
more accurate predicate sense induction. To this
end, we plan to develop a model that simultane-
ously induces predicates and learns coercions, using
knowledge of a predicate’s coerciveness to inform
the induction mechanism.
</bodyText>
<sectionHeader confidence="0.995989" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999980727272727">
We would like to thank Diarmuid O´ S´eaghdha,
Bryan Rink, and Anna Rumshisky for several help-
ful conversations during the course of this work.
We thank Mirella Lapata and Ekaterina Shutova for
making the data from their experiments available
as well as the organizers of SemEval-2010 Task 7
for the associated data set. Additionally, we thank
Srikanth Gullapalli, Aileen McDermott, and Bryan
Rink for annotating the data set used in our exper-
iment. Finally, we thank the anonymous reviewers
for their suggestions on improving this work.
</bodyText>
<page confidence="0.998506">
989
</page>
<sectionHeader confidence="0.998318" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999431146067416">
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In In Proceedings of
the First International Conference on Language Re-
sources and Evaluation Workshop on Linguistic Coref-
erence.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal ofMachine
Learning Research, 3:993–1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 103–111.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Neural
Information Processing Systems, pages 1–9.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling General and Specific As-
pects of Documents with a Probabilistic Topic Model.
In Advances in Neural Information Processing Sys-
tems 19.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Language Re-
sources and Evaluation.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States ofAmerica,
101(Suppl 1):5228.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
1998. Class-based construction of a verb lexicon. In
Proceedings ofAAAI/IAAI.
Maria Lapata and Alex Lascarides. 2003. A Probabilistic
Account of Logical Metonymy. Computational Lin-
guistics, 21(2):261–315.
Dekang Lin and Patrick Pantel. 2001. Induction of Se-
mantic Classes from Natural Language Text. In Pro-
ceedings ofACM SIGKDD Conference on Knowledge
Discovery and Data Mining, pages 317–322.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit.
Diarmuid O´ S´eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435–444.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth Edi-
tion. The LDC Corpus Catalog., LDC2009T13.
James Pustejovsky, Anna Rumshisky, Alex Plotnick,
Elisabetta Jezek, Olga Batiukova, and Valeria Quochi.
2010. SemEval-2010 Task 7: Argument Selection
and Coercion. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 27–32.
Joseph Reisinger and Raymond J. Mooney. 2010. A
Mixture Model with Sharing for Lexical Semantics. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1173–1182.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation method for Selectional Pref-
erences. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
424–434.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings ofthe 37th Annual Meeting ofthe Association
for Computational Linguistics.
Anna Rumshisky, Victor A. Grinberg, and James Puste-
jovsky. 2007. Detecting selectional behavior of com-
plex types in text. In Fourth International Workshop
on Generative Approaches to the Lexicon.
Ekaterina Shutova and Simone Teufel. 2009. Logical
Metonymy: Discovering Classes of Meaning. In Pro-
ceedings of the CogSci 2009 Workshop on Semantic
Space Models.
Ekaterina Shutova. 2009. Sense-based interpretation of
logical metonymy using a statistical method. In Pro-
ceedings of the ACL 2009 Student Workshop.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
fo the 6th Message Understanding Conference, pages
45–52.
</reference>
<page confidence="0.997413">
990
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.763667">
<title confidence="0.9995595">Unsupervised Learning of Selectional Restrictions and Detection Argument Coercions</title>
<author confidence="0.993246">Kirk Roberts</author>
<author confidence="0.993246">M Sanda</author>
<affiliation confidence="0.9869305">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.794752">Richardson, TX 75083,</address>
<abstract confidence="0.9995436">Metonymic language is a pervasive phenomenon. Metonymic type shifting, or arresults in a selectional restriction violation where the argument’s semantic class differs from the class the predicate expects. In this paper we present an unsupervised method that learns the selectional restriction of arguments and enables the detection of argument coercion. This method also generates an enhanced probabilistic resolution of logical metonymies. The experimental results indicate substantial improvements the detection of coercions and the ranking of metonymic interpretations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains. In</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation Workshop on Linguistic Coreference.</booktitle>
<contexts>
<context position="27432" citStr="Bagga and Baldwin (1998)" startWordPosition="4610" endWordPosition="4613">use six of the larger, more semantically coherent WordNet classes: artifact, person, plant, animal, location, and food. We consider each of these a cluster and compare them to clusters composed of the top ten non-polysemous words (according to WordNet) in each of the classes generated by both the baseline (LDA) and our model (cLDA). Words not in both sets of clusters are removed. The result of this evaluation, compared with six clustering metrics, is shown in Table 2. For descriptions of NMI, Rand, and cluster F-measure, see Manning et al. (2008); for the B3 metrics (Cluster and Element), see Bagga and Baldwin (1998); for the MUC metric, see Vilain et al. (1995). Each metric has different strengths and biases in regards to the number and distribution of clusters, so all are provided to give a general picture of class induction performance. The best performing model on all metrics is cLDA with induced predicates using 10 classes. However, as the number of classes is increased and the granularity of the induced classes becomes more finegrained, LDA (predictably) outperforms cLDA on most metrics. This is consistent with our intuition 986 induced predicates? N Y # classes 10 25 50 10 25 50 LDA C1 74.4 78.7 80</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In In Proceedings of the First International Conference on Language Resources and Evaluation Workshop on Linguistic Coreference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5263" citStr="Blei et al., 2003" startWordPosition="779" endWordPosition="782">into distinct groups of meaning (e.g., {read, browse, look through} and {write, produce, work on} for “enjoy book”). Not only do these approaches assume logical metonymies have already been identified, but they are susceptible to providing interpretations that are themselves logical metonymies (e.g., finish book). In this paper, we propose an enhancement to resolving logical metonymies by ruling out event-invoking predicates in order to provide more semantically valid interpretations. Recently, the resolution of several linguistic problems has benefited from Latent Dirichlet Allocation (LDA) (Blei et al., 2003) models. O´ S´eaghdha (2010) examines several selectional preference models based on LDA in predicting human judgements on predicate-argument plausibility. Both LDA and an extension, ROOTH-LDA (based on Rooth et al. (1999)), perform well at predicting plausibility on unseen predicate-argument pairs. Inspired by these results, we propose to extend selectional preference models in order to learn selectional restrictions. Alternatively, unsupervised algorithms exist that both induce semantic classes (Rooth et al., 1999; Lin and Pantel, 2001) and cluster predicates by their selectional restriction</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal ofMachine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>103--111</pages>
<contexts>
<context position="14573" citStr="Brody and Lapata, 2009" startWordPosition="2445" endWordPosition="2448">his allows separate induced predicates to each select a separate argument class. Consider the verb fire, which has at least two distinct common senses: (1) to shoot or propel an object (e.g., to fire a gun), and (2) to lay someone off (e.g., to fire an employee). The first sense selects a weapon (e.g., gun, bullet, rocket), while the second sense selects a person (e.g., employee, coach, apprentice). Specifically, we employ tiered clustering (Reisinger and Mooney, 2010) using the words in the predicate’s context. Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. Tiered clustering has several advantages over other discrete clustering approaches. First, tiered clustering learns a background word distribution in addition to the clusters. This reduces the impact that words common to most senses have on the clustering process and allow clusters to form around only the most salient words. Second, tiered clustering K C1(v, a) = k K k 983 Cluster 1 Cluster 2 Cluster 3 Cluster 4 (18,391) (16,651) (18,749) (11,833) shots ball hire gun gun puck letter imagination Israeli hired Yeltsin grill missil</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="24023" citStr="Chang et al. (2009)" startWordPosition="4015" endWordPosition="4018">5 hyper-parameters, we used the MALLET (McCallum, 2002) defaults of 1.0 and 0.1, respectively, for both LDA and cLDA. We used the 20 predicateargument pairs mentioned above to tune the &apos;y hyperparameters as well as the number of iterations. Both &apos;y0 and &apos;y1 were set to 100. We observed that for both LDA and cLDA, longer runs (in iterations) resulted in improved model log-likelihood but inferior results in terms of detecting coercions. It is not uncommon in topic modeling for model likelihood to not be completely correlated with the score on the task for which the topic model was intended (see Chang et al. (2009)). Both LDA and cLDA were found to perform best at 50 iterations on this data, after which their class distributions were less “smooth” and became rigidly associated with just a few classes, thus having a negative impact on coercion detection. While further iterations hurt coercion detection, only minor gains in model likelihood are seen. We believe the small number of iterations necessary for the model to converge is therefore a function of the data. In traditional topic modeling, documents are generally of similar size (i.e., within an order of magnitude). But in our data, many predicates ha</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Neural Information Processing Systems, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chaitanya Chemudugunta</author>
<author>Padhraic Smyth</author>
<author>Mark Steyvers</author>
</authors>
<title>Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19.</booktitle>
<contexts>
<context position="9703" citStr="Chemudugunta et al. (2007)" startWordPosition="1551" endWordPosition="1554"> 5. Choose cvi ∼ Multinomial(θv) 6. Choose xvi ∼ Bernoulli(τv) 7. If xvi = 1, Choose avi ∼ Multinomial(φc&amp;quot;i ) Else Choose avi ∼ Multinomial(φs&amp;quot;) The model variable sv represents the selected class for predicate v. The coerced class is represented 2With the exception that the probability of drawing the selected class sv is zero. This can be seen as drawing the multinomial θv from a Dirichlet distribution with K-1 components. for each argument i by cvi , where xvi chooses between the selected and coerced class. The variable xvi is similar to switching variables in other graphical models such as Chemudugunta et al. (2007) and Reisinger and Mooney (2010), where switching variables are used to choose between a background distribution and a document-specific distribution. In this case, the switching variable chooses between a specific class and a predicate-specific distribution. The graphical model for cLDA is shown in Figure 1(b). Note that cLDA is virtually equivalent to LDA when τv is 1 and γ1 is small because the selected class will be ignored. In this way, highly coercive predicates have less of an impact on the argument clustering because they are more reliant on the multinomial θ. We use Gibbs sampling to </context>
</contexts>
<marker>Chemudugunta, Smyth, Steyvers, 2007</marker>
<rawString>Chaitanya Chemudugunta, Padhraic Smyth, and Mark Steyvers. 2007. Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model. In Advances in Neural Information Processing Systems 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Language Resources and Evaluation.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the Fifth International Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="27432" citStr="(1998)" startWordPosition="4613" endWordPosition="4613">ger, more semantically coherent WordNet classes: artifact, person, plant, animal, location, and food. We consider each of these a cluster and compare them to clusters composed of the top ten non-polysemous words (according to WordNet) in each of the classes generated by both the baseline (LDA) and our model (cLDA). Words not in both sets of clusters are removed. The result of this evaluation, compared with six clustering metrics, is shown in Table 2. For descriptions of NMI, Rand, and cluster F-measure, see Manning et al. (2008); for the B3 metrics (Cluster and Element), see Bagga and Baldwin (1998); for the MUC metric, see Vilain et al. (1995). Each metric has different strengths and biases in regards to the number and distribution of clusters, so all are provided to give a general picture of class induction performance. The best performing model on all metrics is cLDA with induced predicates using 10 classes. However, as the number of classes is increased and the granularity of the induced classes becomes more finegrained, LDA (predictably) outperforms cLDA on most metrics. This is consistent with our intuition 986 induced predicates? N Y # classes 10 25 50 10 25 50 LDA C1 74.4 78.7 80</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States ofAmerica, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="7762" citStr="Griffiths and Steyvers (2004)" startWordPosition="1180" endWordPosition="1183"> be the argument vocabulary size, and let K be the number of argument classes. Let a&amp;quot;� be the ith (non-unique) argument realized by predicate v. Let c&amp;quot;� be the class for a&amp;quot;� . Let B&amp;quot; be the class distribution for predicate v and Ok be the argument distribution for class k. The graphical model for this LDA is shown in Figure 1(a). The generative process for LDA is: For each argument class k = 1..K: 1. Choose Ok — Dirichlet(β) For each unique predicate v = 1..V : 2. Choose B&amp;quot; — Dirichlet(α) For every argument i = 1..n&amp;quot;: 3. Choose c&amp;quot;� — Multinomial(B&amp;quot;) 4. Choose a&amp;quot;� — Multinomial(O&apos;i ) Following Griffiths and Steyvers (2004), we collapse B and O and estimate the model using Gibbs 981 (a) α θ Q φ K c a N V (b) 70 71 α e s T Q � K x c a N V Figure 1: Graphical models for (a) LDA, and (b) coercion LDA (cLDA). Sampling. This yields the update equation: fak + β p(cv i = k|av; α, β) ∝ fvk + α (1) fv + Kα fk + Aβ Where fak is the frequency of argument a being assigned class k; fk is the frequency of class k being assigned to any argument; fvk is the frequency of predicate v having an argument of class k; and fv is the total number of non-unique arguments for predicate v. 3.2 A Coercion Model We now incorporate our assum</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States ofAmerica, 101(Suppl 1):5228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>1998</date>
<booktitle>In Proceedings ofAAAI/IAAI.</booktitle>
<contexts>
<context position="21743" citStr="Kipper et al., 1998" startWordPosition="3633" endWordPosition="3636">y Px values or to simply choose a threshold of zero due to the prominence of P,. Finally, we include a weighted linear model to Where w1 and w2 are learned weights. We discuss how the parameters for LMTH and LMWT are learned in the experimental setup below. 6 Experimental Setup We use the NYT subsection of the English Gigaword Fourth Edition (Parker et al., 2009) for a total of 1.8M newswire articles. The Stanford Dependency Parser (de Marneffe et al., 2006) is used to extract verb-object relations (dobj) that form the input to our model. To reduce noise, we keep only verbs listed in VerbNet (Kipper et al., 1998) with at least 100 argument instances, discarding have and say, which are too semantically flexible to select from clear semantic classes and so common they distort the class distributions. This results in 4,145 unique verbs with 51M argument instances (388K unique arguments). Additionally we use the dependency parser to extract open clausal complements of verbs (e.g., “like to swim”) for use in logical metonymy interpretation. We believe this to be a more reliable alternative to the phrase chunk extraction patterns used in Lapata and Lascarides (2003). We keep clausal complements (xcomp) wher</context>
</contexts>
<marker>Kipper, Dang, Palmer, 1998</marker>
<rawString>Karin Kipper, Hoa Trang Dang, and Martha Palmer. 1998. Class-based construction of a verb lexicon. In Proceedings ofAAAI/IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Alex Lascarides</author>
</authors>
<title>A Probabilistic Account of Logical Metonymy.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="3237" citStr="Lapata and Lascarides (2003)" startWordPosition="492" endWordPosition="496">e propose an unsupervised model for learning selectional restrictions. The model assumes that (1) arguments have a single selected class exemplified by the selectional restriction, and (2) the selected class can be inferred from the data, in part by modeling how coercive each predicate is. The model is capable of operating with both ambiguous and disambiguated predicates, producing superior results for predicates that have been disambiguated. The selectional restrictions and coercions detected by the model reported in this paper can be used to enhance the logical metonymy approach reported in Lapata and Lascarides (2003). The experimental results show a significant improvement in the ranking of interpretations. 980 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 980–990, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics The rest of this paper is organized as follows. Section 2 discusses related work. Section 3 details unsupervised models that inform detection of metonymies. Section 4 outlines a method for disambiguating ambiguous predicates. Section 5 describes the enhanced interpretation of logical metonymies when convent</context>
<context position="16974" citStr="Lapata and Lascarides, 2003" startWordPosition="2834" endWordPosition="2837">e such that coercions and selections are placed in a separate clusters. Section 6 discusses our parameter selection experiments. 5 Logical Metonymy Interpretation Logical metonymies are a unique class of coercions due to the fact that their eventive interpretation can be derived from verbal predicates. For instance, for the logical metonymy “enjoy book”, we know that read is a good candidate interpretation because (1) books are objects whose purpose is to be read and (2) reading is an event that may be enjoyed. We therefore expect to see many instances of both “read book” and “enjoy reading” (Lapata and Lascarides, 2003). Conversely, for coercions with non-eventive interpretations, such as “arrive at meeting”, the interpretation (location of) is more dependent on the predicate (arrive) than the function of its argument (meeting). In this section, we limit our discussion of logical metonymy to the verb-object case, its corresponding baseline for ranking interpretations, and our proposed enhancements. However, similar baselines exist for other types of logical metonymy, such as adjective-noun and noun-noun. Since our enhancement does not depend on any syntactic information beyond the predicate-argument instance</context>
<context position="18583" citStr="Lapata and Lascarides (2003)" startWordPosition="3079" endWordPosition="3083">book” is proportional to the likelihood of seeing “enjoy reading book” expressed as a syntactic dependency in a sufficiently large corpus. Due to data sparsity, they approximate this likelihood of seeing the object given the verb and interpretation to simply the likelihood of seeing the object given the interpretation. We denote this logical metonymy ranking method as LMLL, formally defined as: LMLL(e; v, o) = Pc(v, e, o) = Pc(e)Pc(v|e)Pc(o|e,v) ≈ Pc(e)Pc(v|e)Pc(o|e) fc(v, e)fc(o, e) ≈ (7) Nfc(e) Where Pc and fc indicate probability and frequency, respectively, derived from corpus counts. See Lapata and Lascarides (2003) for a detailed explanation of how these frequencies are obtained. This model, which we consider our baseline, is only partially correct as the corpus will contain coercions that form invalid interpretations. Consider the phrases “enjoy finishing a book” and “enjoy discussing a book”. Both “finish book” and “discuss book” are coercions (and logical metonymies) themselves, and do not form a valid interpretation.4 3They use two patterns: “v e-ing o” and “v to e o”, where e is tagged as a verb. 4For evidence of the frequency of these phrases, at the time of this writing, “enjoy finishing a book” </context>
<context position="22301" citStr="Lapata and Lascarides (2003)" startWordPosition="3724" endWordPosition="3727">duce noise, we keep only verbs listed in VerbNet (Kipper et al., 1998) with at least 100 argument instances, discarding have and say, which are too semantically flexible to select from clear semantic classes and so common they distort the class distributions. This results in 4,145 unique verbs with 51M argument instances (388K unique arguments). Additionally we use the dependency parser to extract open clausal complements of verbs (e.g., “like to swim”) for use in logical metonymy interpretation. We believe this to be a more reliable alternative to the phrase chunk extraction patterns used in Lapata and Lascarides (2003). We keep clausal complements (xcomp) where the dependent is either a gerund or infinitive in order to estimate P,(v|e) in Equation (7). For tiered clustering we use the same implementation as Reisinger and Mooney (2010)6 to partition the surface form of the verb into one or more induced forms. Instead of using a fixed number of iterations, the clustering was run for 100 iterations past the best recorded log-likelihood in order to find the best possible fit to the data. We tuned the hyperparameters by maximizing the log-likelihood on a small held-out set of 20 predicate-argument pairs (10 sele</context>
<context position="32228" citStr="Lapata and Lascarides (2003)" startWordPosition="5413" endWordPosition="5417">his suggests that there is value in assigning different selected classes via sense induction, but that the two-step approach is not beneficial for these common predicates. This could be overcome by a joint approach of inducing predicate classes while simultaneously detecting coercions, as the presence of many coercions would be an indicator that more induced predicates are necessary. 7.3 Logical Metonymy Interpretation For the evaluation of logical metonymy, we use both an existing data set and a newly created data set. Shutova and Teufel (2009) annotated 10 verbobject logical metonymies from Lapata and Lascarides (2003) with sense-disambiguated interpretations and organized the interpretations into clusters representing different possible meanings. For evaluation purposes we ignore the sense annotations and clusters and consider all lexical matchings of one of the annotated interpretations to be correct. The 987 induced predicates? N Y # classes 10 25 50 10 25 50 LMLL 0.381 0.365 LMIND LDA C1 0.415 0.406 0.383 0.386 0.412 0.395 cLDA C1 0.408 0.412 0.412 0.407 0.468 0.439 C2 0.415 0.447 0.419 0.414 0.415 0.434 C3 0.416 0.453 0.455 0.395 0.416 0.402 LMTH LDA C1 0.599 0.568 0.588 0.479 0.520 0.551 cLDA C1 0.571</context>
<context position="34398" citStr="Lapata and Lascarides (2003)" startWordPosition="5776" endWordPosition="5779">.271 0.248 0.251 0.249 cLDA C1 0.271 0.272 0.270 0.257 0.259 0.265 C2 0.274 0.274 0.266 0.250 0.259 0.261 C3 0.271 0.273 0.274 0.253 0.262 0.259 Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations. The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-based interpretation method. data contains an average of 11 interpretations per metonymy and has a reported 70% recall. In order to create a larger data set, we identified 100 verb-object logical metonymies, including those used in Lapata and Lascarides (2003). Three annotators were asked to provide up to five interpretations for each metonymy (they were not provided with any verbs from which to choose, only the verbobject pair). The annotators provided an average of 4.6 interpretations per metonymy. Because our goal was recall, inter-annotator agreement was necessarily low, and each logical metonymy had an average of 11.7 unique interpretations. All annotators agreed on at least one interpretation for 40 metonymies, while for 14 they had no interpretations in common.7 Since logical metonymy interpretation is usually evaluated as a ranking task, we</context>
<context position="38197" citStr="Lapata and Lascarides (2003)" startWordPosition="6403" endWordPosition="6406">l using these scores as features, and as a result the learned weights were forced to ignore the coercion score and rely entirely on LMLL. We attempted other scaling methods, such as a rank-based method, but these had poor results as well, so we leave the problem of the supervised learning these weights to future work. Using induced senses did not result in the drastic and consistent degradation in performance seen on the SemEval data, and the highest non-threshold result for the Shutova and Teufel (2009) data used predicate induction. Both metonymy data sets were limited to the verbs found in Lapata and Lascarides (2003), which are still quite common (attempt, begin, enjoy, expect, finish, prefer, start, survive, try, want). However, the verbs used in our data set had a greater number of WordNet senses attested in a corpus than the SemEval data (an average of 4.4 senses for our data versus 3.0 senses for the SemEval data). This suggests the potential value of sense induction for highly polysemous predicates and further motivates the integration of sense induction within a selectional restriction model. 8 Conclusion We have presented a novel topic model that extends an unsupervised selectional preference model</context>
</contexts>
<marker>Lapata, Lascarides, 2003</marker>
<rawString>Maria Lapata and Alex Lascarides. 2003. A Probabilistic Account of Logical Metonymy. Computational Linguistics, 21(2):261–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Induction of Semantic Classes from Natural Language Text.</title>
<date>2001</date>
<booktitle>In Proceedings ofACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>317--322</pages>
<contexts>
<context position="5807" citStr="Lin and Pantel, 2001" startWordPosition="854" endWordPosition="857">lems has benefited from Latent Dirichlet Allocation (LDA) (Blei et al., 2003) models. O´ S´eaghdha (2010) examines several selectional preference models based on LDA in predicting human judgements on predicate-argument plausibility. Both LDA and an extension, ROOTH-LDA (based on Rooth et al. (1999)), perform well at predicting plausibility on unseen predicate-argument pairs. Inspired by these results, we propose to extend selectional preference models in order to learn selectional restrictions. Alternatively, unsupervised algorithms exist that both induce semantic classes (Rooth et al., 1999; Lin and Pantel, 2001) and cluster predicates by their selectional restrictions (Rumshisky et al., 2007) but none of these provide a sufficient framework for determining if a specific argument violates its predicate’s selectional restriction. 3 Unsupervised Learning of Selectional Restrictions In predicate-argument structures, predicates impose selectional restrictions in the form of semantic expectations on their arguments. Whenever the semantic class of the argument meets these constraints a selection occurs. For example, the predicate “hear” imposes the semantics related to sound on the argument “voice”. Because</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Induction of Semantic Classes from Natural Language Text. In Proceedings ofACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 317–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<contexts>
<context position="23459" citStr="McCallum, 2002" startWordPosition="3914" endWordPosition="3916">mall held-out set of 20 predicate-argument pairs (10 selections, 10 coercions). The resulting partitions were fairly conservative, yielding 12,332 induced verbs or about 3 induced verb forms for every surface form, with 305 verbs not being partitioned at all. We implemented both LDA and cLDA as described in Sections 3.1 and 3.2. For the α and β bined 7,040 Google hits. 5For comparison, “enjoy reading a book” and “enjoy reading the book” have a combined 6.5 million Google hits 6Available at http://github.com/joeraii/UTML-LatentVariable-Modeling-Toolkit 985 hyper-parameters, we used the MALLET (McCallum, 2002) defaults of 1.0 and 0.1, respectively, for both LDA and cLDA. We used the 20 predicateargument pairs mentioned above to tune the &apos;y hyperparameters as well as the number of iterations. Both &apos;y0 and &apos;y1 were set to 100. We observed that for both LDA and cLDA, longer runs (in iterations) resulted in improved model log-likelihood but inferior results in terms of detecting coercions. It is not uncommon in topic modeling for model likelihood to not be completely correlated with the score on the task for which the topic model was intended (see Chang et al. (2009)). Both LDA and cLDA were found to p</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>435--444</pages>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 435–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2009</date>
<booktitle>English Gigaword Fourth Edition. The LDC Corpus Catalog., LDC2009T13.</booktitle>
<contexts>
<context position="21488" citStr="Parker et al., 2009" startWordPosition="3589" endWordPosition="3592">antities. The first method is a filtering approach where a threshold is learned for Px: LMTH (e; v, o) = r Pc(v, e, o) if Px (e, o) ≥ S (10) l 0 otherwise Where the threshold S is learned from a development set. We expect this model could suffer from noisy Px values or to simply choose a threshold of zero due to the prominence of P,. Finally, we include a weighted linear model to Where w1 and w2 are learned weights. We discuss how the parameters for LMTH and LMWT are learned in the experimental setup below. 6 Experimental Setup We use the NYT subsection of the English Gigaword Fourth Edition (Parker et al., 2009) for a total of 1.8M newswire articles. The Stanford Dependency Parser (de Marneffe et al., 2006) is used to extract verb-object relations (dobj) that form the input to our model. To reduce noise, we keep only verbs listed in VerbNet (Kipper et al., 1998) with at least 100 argument instances, discarding have and say, which are too semantically flexible to select from clear semantic classes and so common they distort the class distributions. This results in 4,145 unique verbs with 51M argument instances (388K unique arguments). Additionally we use the dependency parser to extract open clausal c</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2009</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2009. English Gigaword Fourth Edition. The LDC Corpus Catalog., LDC2009T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Anna Rumshisky</author>
<author>Alex Plotnick</author>
<author>Elisabetta Jezek</author>
<author>Olga Batiukova</author>
<author>Valeria Quochi</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 7: Argument Selection and Coercion. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>27--32</pages>
<contexts>
<context position="1978" citStr="Pustejovsky et al., 2010" startWordPosition="299" endWordPosition="302"> on organizing, forming, or registering a bowling team. The quality of the answers therefore depends on the ability to (1) recognize when metonymic language is used, and (2) to produce coercions that capture the user’s intention. One important step in this direction was 1Both questions taken from Yahoo Answers. taken by SemEval-2010 Task 7, which focused on the ability to recognize (a) an argument’s selectional restriction for predicates such as arrive at, cancel, or hear, and (b) the type of coercion that licensed a correct interpretation of the metonymy. Details of the task are reported in (Pustejovsky et al., 2010). Approaches to metonymy based on this task are limited, however, because (a) the task is focused only on semantically non-ambiguous predicates and (b) the selectional restrictions of the arguments were chosen from a pre-defined set of six semantic classes (artifact, document, event, location, proposition, and sound). However, metonymy coercion systems capable of providing the interpretations of questions (Q1) and (Q2) clearly cannot operate with the simplifications designed for this task. Inspired by recent advances in modeling selectional preferences with latent-variable models (Ritter et al</context>
<context position="6629" citStr="Pustejovsky et al. (2010)" startWordPosition="976" endWordPosition="979">electional restriction. 3 Unsupervised Learning of Selectional Restrictions In predicate-argument structures, predicates impose selectional restrictions in the form of semantic expectations on their arguments. Whenever the semantic class of the argument meets these constraints a selection occurs. For example, the predicate “hear” imposes the semantics related to sound on the argument “voice”. Because the semantic class for “voice” conforms to these constraints, we call its semantic class the selected class. However, when the semantic class of the argument violates these constraints, we follow Pustejovsky et al. (2010) and refer to this as a coercion. In this case, we call the argument’s semantic class the coerced class. For example, “hear speaker” is a coercion where the argument class, person, is implicitly coerced into the voice of the speaker, a sound. 3.1 A Baseline Model We consider the LDA-based selectional preference model reported in O´ S´eaghdha (2010) as a baseline for modeling selectional restrictions. Formally, we define our LDA baseline model as follows. Let V be the predicate vocabulary size, let A be the argument vocabulary size, and let K be the number of argument classes. Let a&amp;quot;� be the it</context>
<context position="28881" citStr="Pustejovsky et al., 2010" startWordPosition="4861" endWordPosition="4864">f classes increases. For this evaluation, predicate induction also improved LDA for smaller numbers of classes, but not to the degree that it improved cLDA. Without predicate induction, LDA outperforms cLDA on all six metrics for 25 and 50 classes. With predicate induction, LDA outperforms cLDA on only one metric for 25 classes and five metrics for 50 classes. Thus the induced predicates do reduce the negative impact caused by the single selected class assumption for semantic class induction. 7.2 Coercion Detection For the evaluation of coercion detection, we use the SemEval-2010 Task 7 data (Pustejovsky et al., 2010). This data uses the most common sense for each of five predicates (arrive, cancel, deny, finish, and hear) with a total of 2,070 sentences annotated with the argument’s source type (the argument’s semantic class) and target type (the predicate’s selected class for that argument). We ignore the actual argument classes and evaluate on the coercion type, which is a selection when the source and target type match, and a coercion otherwise. In order to evaluate unsupervised systems on this data, we use the corresponding training set (1,031 examples) to learn a threshold for coercion detection. At </context>
</contexts>
<marker>Pustejovsky, Rumshisky, Plotnick, Jezek, Batiukova, Quochi, 2010</marker>
<rawString>James Pustejovsky, Anna Rumshisky, Alex Plotnick, Elisabetta Jezek, Olga Batiukova, and Valeria Quochi. 2010. SemEval-2010 Task 7: Argument Selection and Coercion. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 27–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>A Mixture Model with Sharing for Lexical Semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1173--1182</pages>
<contexts>
<context position="9735" citStr="Reisinger and Mooney (2010)" startWordPosition="1556" endWordPosition="1559">) 6. Choose xvi ∼ Bernoulli(τv) 7. If xvi = 1, Choose avi ∼ Multinomial(φc&amp;quot;i ) Else Choose avi ∼ Multinomial(φs&amp;quot;) The model variable sv represents the selected class for predicate v. The coerced class is represented 2With the exception that the probability of drawing the selected class sv is zero. This can be seen as drawing the multinomial θv from a Dirichlet distribution with K-1 components. for each argument i by cvi , where xvi chooses between the selected and coerced class. The variable xvi is similar to switching variables in other graphical models such as Chemudugunta et al. (2007) and Reisinger and Mooney (2010), where switching variables are used to choose between a background distribution and a document-specific distribution. In this case, the switching variable chooses between a specific class and a predicate-specific distribution. The graphical model for cLDA is shown in Figure 1(b). Note that cLDA is virtually equivalent to LDA when τv is 1 and γ1 is small because the selected class will be ignored. In this way, highly coercive predicates have less of an impact on the argument clustering because they are more reliant on the multinomial θ. We use Gibbs sampling to perform model inference and coll</context>
<context position="14423" citStr="Reisinger and Mooney, 2010" startWordPosition="2421" endWordPosition="2424">ings, each with a separate selected class. We therefore propose a method of partitioning a predicate’s arguments by the induced senses of the predicate. This allows separate induced predicates to each select a separate argument class. Consider the verb fire, which has at least two distinct common senses: (1) to shoot or propel an object (e.g., to fire a gun), and (2) to lay someone off (e.g., to fire an employee). The first sense selects a weapon (e.g., gun, bullet, rocket), while the second sense selects a person (e.g., employee, coach, apprentice). Specifically, we employ tiered clustering (Reisinger and Mooney, 2010) using the words in the predicate’s context. Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. Tiered clustering has several advantages over other discrete clustering approaches. First, tiered clustering learns a background word distribution in addition to the clusters. This reduces the impact that words common to most senses have on the clustering process and allow clusters to form around only the most salient words. Second, tiered clustering K C1(v, a) = k K k 983 Cluster </context>
<context position="22521" citStr="Reisinger and Mooney (2010)" startWordPosition="3760" endWordPosition="3763"> they distort the class distributions. This results in 4,145 unique verbs with 51M argument instances (388K unique arguments). Additionally we use the dependency parser to extract open clausal complements of verbs (e.g., “like to swim”) for use in logical metonymy interpretation. We believe this to be a more reliable alternative to the phrase chunk extraction patterns used in Lapata and Lascarides (2003). We keep clausal complements (xcomp) where the dependent is either a gerund or infinitive in order to estimate P,(v|e) in Equation (7). For tiered clustering we use the same implementation as Reisinger and Mooney (2010)6 to partition the surface form of the verb into one or more induced forms. Instead of using a fixed number of iterations, the clustering was run for 100 iterations past the best recorded log-likelihood in order to find the best possible fit to the data. We tuned the hyperparameters by maximizing the log-likelihood on a small held-out set of 20 predicate-argument pairs (10 selections, 10 coercions). The resulting partitions were fairly conservative, yielding 12,332 induced verbs or about 3 induced verb forms for every surface form, with 305 verbs not being partitioned at all. We implemented bo</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. A Mixture Model with Sharing for Lexical Semantics. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1173–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A Latent Dirichlet Allocation method for Selectional Preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>424--434</pages>
<contexts>
<context position="2585" citStr="Ritter et al., 2010" startWordPosition="389" endWordPosition="393">t al., 2010). Approaches to metonymy based on this task are limited, however, because (a) the task is focused only on semantically non-ambiguous predicates and (b) the selectional restrictions of the arguments were chosen from a pre-defined set of six semantic classes (artifact, document, event, location, proposition, and sound). However, metonymy coercion systems capable of providing the interpretations of questions (Q1) and (Q2) clearly cannot operate with the simplifications designed for this task. Inspired by recent advances in modeling selectional preferences with latent-variable models (Ritter et al., 2010; O´ S´eaghdha, 2010), we propose an unsupervised model for learning selectional restrictions. The model assumes that (1) arguments have a single selected class exemplified by the selectional restriction, and (2) the selected class can be inferred from the data, in part by modeling how coercive each predicate is. The model is capable of operating with both ambiguous and disambiguated predicates, producing superior results for predicates that have been disambiguated. The selectional restrictions and coercions detected by the model reported in this paper can be used to enhance the logical metony</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A Latent Dirichlet Allocation method for Selectional Preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings ofthe 37th Annual Meeting ofthe Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5485" citStr="Rooth et al. (1999)" startWordPosition="811" endWordPosition="814">tible to providing interpretations that are themselves logical metonymies (e.g., finish book). In this paper, we propose an enhancement to resolving logical metonymies by ruling out event-invoking predicates in order to provide more semantically valid interpretations. Recently, the resolution of several linguistic problems has benefited from Latent Dirichlet Allocation (LDA) (Blei et al., 2003) models. O´ S´eaghdha (2010) examines several selectional preference models based on LDA in predicting human judgements on predicate-argument plausibility. Both LDA and an extension, ROOTH-LDA (based on Rooth et al. (1999)), perform well at predicting plausibility on unseen predicate-argument pairs. Inspired by these results, we propose to extend selectional preference models in order to learn selectional restrictions. Alternatively, unsupervised algorithms exist that both induce semantic classes (Rooth et al., 1999; Lin and Pantel, 2001) and cluster predicates by their selectional restrictions (Rumshisky et al., 2007) but none of these provide a sufficient framework for determining if a specific argument violates its predicate’s selectional restriction. 3 Unsupervised Learning of Selectional Restrictions In pr</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proceedings ofthe 37th Annual Meeting ofthe Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rumshisky</author>
<author>Victor A Grinberg</author>
<author>James Pustejovsky</author>
</authors>
<title>Detecting selectional behavior of complex types in text.</title>
<date>2007</date>
<booktitle>In Fourth International Workshop on Generative Approaches to the Lexicon.</booktitle>
<contexts>
<context position="5889" citStr="Rumshisky et al., 2007" startWordPosition="865" endWordPosition="868">dels. O´ S´eaghdha (2010) examines several selectional preference models based on LDA in predicting human judgements on predicate-argument plausibility. Both LDA and an extension, ROOTH-LDA (based on Rooth et al. (1999)), perform well at predicting plausibility on unseen predicate-argument pairs. Inspired by these results, we propose to extend selectional preference models in order to learn selectional restrictions. Alternatively, unsupervised algorithms exist that both induce semantic classes (Rooth et al., 1999; Lin and Pantel, 2001) and cluster predicates by their selectional restrictions (Rumshisky et al., 2007) but none of these provide a sufficient framework for determining if a specific argument violates its predicate’s selectional restriction. 3 Unsupervised Learning of Selectional Restrictions In predicate-argument structures, predicates impose selectional restrictions in the form of semantic expectations on their arguments. Whenever the semantic class of the argument meets these constraints a selection occurs. For example, the predicate “hear” imposes the semantics related to sound on the argument “voice”. Because the semantic class for “voice” conforms to these constraints, we call its semanti</context>
</contexts>
<marker>Rumshisky, Grinberg, Pustejovsky, 2007</marker>
<rawString>Anna Rumshisky, Victor A. Grinberg, and James Pustejovsky. 2007. Detecting selectional behavior of complex types in text. In Fourth International Workshop on Generative Approaches to the Lexicon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Simone Teufel</author>
</authors>
<title>Logical Metonymy: Discovering Classes of Meaning.</title>
<date>2009</date>
<booktitle>In Proceedings of the CogSci 2009 Workshop on Semantic Space Models.</booktitle>
<contexts>
<context position="4559" citStr="Shutova and Teufel (2009)" startWordPosition="677" endWordPosition="680">on 7 presents our experimental results in three broad tasks: (i) semantic class induction, (ii) coercion detection, and (iii) logical metonymy interpretation. Section 8 summarizes the conclusions. 2 Previous Work Lapata and Lascarides (2003) propose a probabilistic ranking model for logical metonymies. They estimate these probabilities using co-occurrence frequencies of predicate-argument pairs in a corpus. Shutova (2009) extends this approach to provide sense-disambiguated interpretations from WordNet (Fellbaum, 1998) by using the alternative interpretations to disambiguate polysemous words. Shutova and Teufel (2009) extend this approach further by clustering these sense-disambiguated interpretations into distinct groups of meaning (e.g., {read, browse, look through} and {write, produce, work on} for “enjoy book”). Not only do these approaches assume logical metonymies have already been identified, but they are susceptible to providing interpretations that are themselves logical metonymies (e.g., finish book). In this paper, we propose an enhancement to resolving logical metonymies by ruling out event-invoking predicates in order to provide more semantically valid interpretations. Recently, the resolution</context>
<context position="32151" citStr="Shutova and Teufel (2009)" startWordPosition="5402" endWordPosition="5405">gradation of only 2.5 points and C3 was actually improved by 2.1 points. This suggests that there is value in assigning different selected classes via sense induction, but that the two-step approach is not beneficial for these common predicates. This could be overcome by a joint approach of inducing predicate classes while simultaneously detecting coercions, as the presence of many coercions would be an indicator that more induced predicates are necessary. 7.3 Logical Metonymy Interpretation For the evaluation of logical metonymy, we use both an existing data set and a newly created data set. Shutova and Teufel (2009) annotated 10 verbobject logical metonymies from Lapata and Lascarides (2003) with sense-disambiguated interpretations and organized the interpretations into clusters representing different possible meanings. For evaluation purposes we ignore the sense annotations and clusters and consider all lexical matchings of one of the annotated interpretations to be correct. The 987 induced predicates? N Y # classes 10 25 50 10 25 50 LMLL 0.381 0.365 LMIND LDA C1 0.415 0.406 0.383 0.386 0.412 0.395 cLDA C1 0.408 0.412 0.412 0.407 0.468 0.439 C2 0.415 0.447 0.419 0.414 0.415 0.434 C3 0.416 0.453 0.455 0.</context>
<context position="35913" citStr="Shutova and Teufel (2009)" startWordPosition="6017" endWordPosition="6020">l(n) = 1 if interpretation n is valid, 0 otherwise; and interps(q) is the number of valid interpretations for the metonymy q. We rank all 4,145 verbs as interpretations except for those removed by the threshold technique, as they have a score of zero. This can give LMTH artificially high MAP scores since it may remove some valid interpretations that are low-ranking. However, since a smaller, higher precision list may be useful for many applications we still consider MAP a valid metric and indicate both the highest scoring method and the highest scoring non-threshold method. The results on the Shutova and Teufel (2009) data are 1 MAP = Q � Q q=1 988 shown in Table 4. The results on our own data are shown in Table 5. The scores reported in the Shutova and Teufel (2009) data are noticeably higher than the data we annotated. Since the metonymies in our data are a super-set of those in their data, and since for those metonymies our annotators provided approximately the same number of interpretations (110 versus 120), this likely indicates the remaining metonymies in our data are more difficult. In all cases the best reported scores use cLDA. Unlike coercion detection on the SemEval data, C3 performs very well, </context>
<context position="38078" citStr="Shutova and Teufel (2009)" startWordPosition="6384" endWordPosition="6387">that the range of scale for the LMLL scores are very inconsistent. This can make it difficult to learn a linear model using these scores as features, and as a result the learned weights were forced to ignore the coercion score and rely entirely on LMLL. We attempted other scaling methods, such as a rank-based method, but these had poor results as well, so we leave the problem of the supervised learning these weights to future work. Using induced senses did not result in the drastic and consistent degradation in performance seen on the SemEval data, and the highest non-threshold result for the Shutova and Teufel (2009) data used predicate induction. Both metonymy data sets were limited to the verbs found in Lapata and Lascarides (2003), which are still quite common (attempt, begin, enjoy, expect, finish, prefer, start, survive, try, want). However, the verbs used in our data set had a greater number of WordNet senses attested in a corpus than the SemEval data (an average of 4.4 senses for our data versus 3.0 senses for the SemEval data). This suggests the potential value of sense induction for highly polysemous predicates and further motivates the integration of sense induction within a selectional restrict</context>
</contexts>
<marker>Shutova, Teufel, 2009</marker>
<rawString>Ekaterina Shutova and Simone Teufel. 2009. Logical Metonymy: Discovering Classes of Meaning. In Proceedings of the CogSci 2009 Workshop on Semantic Space Models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Sense-based interpretation of logical metonymy using a statistical method.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL 2009 Student Workshop.</booktitle>
<contexts>
<context position="4359" citStr="Shutova (2009)" startWordPosition="654" endWordPosition="655">edicates. Section 5 describes the enhanced interpretation of logical metonymies when conventional constraints are known. Section 6 outlines our implementation and experimental design. Section 7 presents our experimental results in three broad tasks: (i) semantic class induction, (ii) coercion detection, and (iii) logical metonymy interpretation. Section 8 summarizes the conclusions. 2 Previous Work Lapata and Lascarides (2003) propose a probabilistic ranking model for logical metonymies. They estimate these probabilities using co-occurrence frequencies of predicate-argument pairs in a corpus. Shutova (2009) extends this approach to provide sense-disambiguated interpretations from WordNet (Fellbaum, 1998) by using the alternative interpretations to disambiguate polysemous words. Shutova and Teufel (2009) extend this approach further by clustering these sense-disambiguated interpretations into distinct groups of meaning (e.g., {read, browse, look through} and {write, produce, work on} for “enjoy book”). Not only do these approaches assume logical metonymies have already been identified, but they are susceptible to providing interpretations that are themselves logical metonymies (e.g., finish book)</context>
</contexts>
<marker>Shutova, 2009</marker>
<rawString>Ekaterina Shutova. 2009. Sense-based interpretation of logical metonymy using a statistical method. In Proceedings of the ACL 2009 Student Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings fo the 6th Message Understanding Conference,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="27478" citStr="Vilain et al. (1995)" startWordPosition="4620" endWordPosition="4623">WordNet classes: artifact, person, plant, animal, location, and food. We consider each of these a cluster and compare them to clusters composed of the top ten non-polysemous words (according to WordNet) in each of the classes generated by both the baseline (LDA) and our model (cLDA). Words not in both sets of clusters are removed. The result of this evaluation, compared with six clustering metrics, is shown in Table 2. For descriptions of NMI, Rand, and cluster F-measure, see Manning et al. (2008); for the B3 metrics (Cluster and Element), see Bagga and Baldwin (1998); for the MUC metric, see Vilain et al. (1995). Each metric has different strengths and biases in regards to the number and distribution of clusters, so all are provided to give a general picture of class induction performance. The best performing model on all metrics is cLDA with induced predicates using 10 classes. However, as the number of classes is increased and the granularity of the induced classes becomes more finegrained, LDA (predictably) outperforms cLDA on most metrics. This is consistent with our intuition 986 induced predicates? N Y # classes 10 25 50 10 25 50 LDA C1 74.4 78.7 80.5 69.7 70.1 73.4 cLDA C1 80.6 81.2 80.9 76.2 </context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings fo the 6th Message Understanding Conference, pages 45–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>