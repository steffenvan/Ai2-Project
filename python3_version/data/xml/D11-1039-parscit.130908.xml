<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000139">
<title confidence="0.990783">
Bootstrapping Semantic Parsers from Conversations
</title>
<author confidence="0.986426">
Yoav Artzi and Luke Zettlemoyer
</author>
<affiliation confidence="0.993188">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.956846">
Seattle, WA 98195
</address>
<email confidence="0.999581">
{yoav,lsz}@cs.washington.edu
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993540476190476">
Conversations provide rich opportunities for
interactive, continuous learning. When some-
thing goes wrong, a system can ask for clari-
fication, rewording, or otherwise redirect the
interaction to achieve its goals. In this pa-
per, we present an approach for using con-
versational interactions of this type to induce
semantic parsers. We demonstrate learning
without any explicit annotation of the mean-
ings of user utterances. Instead, we model
meaning with latent variables, and introduce
a loss function to measure how well potential
meanings match the conversation. This loss
drives the overall learning approach, which in-
duces a weighted CCG grammar that could be
used to automatically bootstrap the semantic
analysis component in a complete dialog sys-
tem. Experiments on DARPA Communica-
tor conversational logs demonstrate effective
learning, despite requiring no explicit mean-
ing annotations.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996252068181818">
Conversational interactions provide significant op-
portunities for autonomous learning. A well-defined
goal allows a system to engage in remediations when
confused, such as asking for clarification, reword-
ing, or additional explanation. The user’s response
to such requests provides a strong, if often indirect,
signal that can be used to learn to avoid the orig-
inal confusion in future conversations. In this pa-
per, we show how to use this type of conversational
feedback to learn to better recover the meaning of
user utterances, by inducing semantic parsers from
unannotated conversational logs. We believe that
this style of learning will contribute to the long term
goal of building self-improving dialog systems that
continually learn from their mistakes, with little or
no human intervention.
Many dialog systems use a semantic parsing com-
ponent to analyze user utterances (e.g., Allen et al.,
2007; Litman et al., 2009; Young et al., 2010). For
example, in a flight booking system, the sentence
Sent: I want to go to Seattle on Friday
LF: λx.to(x, SEA) ∧ date(x, FRI)
might be mapped to the logical form (LF) meaning
representation above, a lambda-calculus expression
defining the set of flights that match the user’s de-
sired constraints. This LF is a representation of the
semantic content that comes from the sentence, and
would be input to a context-dependent understand-
ing component in a full dialog system, for example
to find the date that the symbol FRI refers to.
To induce semantic parsers from interactions, we
consider user statements in conversational logs and
model their meaning with latent variables. We
demonstrate that it is often possible to use the dia-
log that follows a statement (including remediations
such as clarifications, simplifications, etc.) to learn
the meaning of the original sentence. For example,
consider the first user utterance in Figure 1, where
the system failed to understand the user’s request.
To complete the task, the system must use a reme-
diation strategy. Here, it takes the initiative by ask-
ing for and confirming each flight constraint in turn.
This strategy produces an unnatural conversation but
provides supervision for learning the meaning of the
</bodyText>
<page confidence="0.982636">
421
</page>
<note confidence="0.9583225">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 421–432,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999336914285715">
original utterance. We can easily record representa-
tions of the meanings the system intended to convey
at each step, as seen in Figure 1, and use this indirect
supervision for learning.
Learning from this weak signal is challenging. In
any specific conversation, the system’s remediations
can fail to recover aspects of the original user mean-
ing and can introduce spurious constraints, for ex-
ample when users change their goals mid conversa-
tion. To learn effectively, the model must accumu-
late evidence from many interactions to best recover
the meaning of each specific sentence.
We will learn semantic parsers defined by prob-
abilistic Combinatory Categorial Grammars (PC-
CGs), which include both a lexicon and a weighted
linear model for parse selection. The lexicon speci-
fies the meanings of individual words and phrases,
while the parameters of a parsing model define
how to best combine word- and phrase-level mean-
ings to analyze complete sentences. To learn with-
out labeled meaning representations, we make use
of a variant of the loss-sensitive Perceptron algo-
rithm (Singh-Miller and Collins, 2007). We define
loss functions to provide a rough measure of (1) how
well a candidate meaning for a utterance matches
the conversation that follows it and (2) how well the
candidate matches our expectations about the types
of things that are often said in the dialog’s domain.
These notions of loss drive not only the parameter
estimation but also the grammar induction process
that constructs the CCG lexicon. Experiments on
conversation logs from the DARPA Communicator
corpus (Walker et al., 2002) demonstrate the feasi-
bility of our approach.
This paper makes the following contributions:
</bodyText>
<listItem confidence="0.9993071">
• A formalization of the problem of learning
the meaning of user statements from conversa-
tional feedback, without requiring annotation.
• A new loss-sensitive learning algorithm for this
problem that induces semantic parsers from
conversation logs.
• Loss functions to measure the quality of hypo-
thetical utterance meanings within the conver-
sation in which they appear.
• An evaluation on logs from two dialog systems
</listItem>
<note confidence="0.881393222222222">
SYSTEM: how can i help you? (OPEN TASK)
USER: i would like to fly from atlanta georgia to lon-
don england on september twenty fourth in the early
evening i would like to return on october first departing
from london in the late morning
SYSTEM: leaving what city? (ASK:λx.from(fl, x))
USER: atlanta georgia
SYSTEM: leaving atlanta. (CONFIRM:from(fl, ATL))
going to which city? (ASK:λx.to(fl, x))
</note>
<figure confidence="0.730923285714286">
USER: london
SYSTEM: arriving in london england.
(CONFIRM:to(fl, LON)) what date would you like to
depart atlanta? (ASK:λx.from(fl, ATL) ∧ depart-
date(fl, x))
USER: september twenty fourth in the early evening
[conversation continues]
</figure>
<figureCaption confidence="0.992023">
Figure 1: Conversational excerpt from a DARPA Com-
municator travel-planning dialog. Each system statement
is labeled with representations of its speech act and log-
ical meaning, in parentheses. The user utterances have
no labels. Conversations of this type provide the training
data to learn semantic parsers for user utterances.
</figureCaption>
<bodyText confidence="0.9666945">
that demonstrate effective learning from con-
versations alone.
</bodyText>
<sectionHeader confidence="0.989332" genericHeader="introduction">
2 Problem
</sectionHeader>
<bodyText confidence="0.995617565217391">
Our goal is to learn a function that maps a sentence
x to a lambda-calculus expression z. We assume ac-
cess to logs of conversations with automatically gen-
erated annotation of system utterance meanings, but
no explicit labeling of each user utterance meaning.
We define a conversation C = (U, O) to be a se-
quence of utterances U� = [uo, ... , um] and a set
of conversational objects O. An object o E O
is an entity that is being discussed, for example
there would be a unique object for each flight leg
discussed in a travel planning conversation. Each
utterance ui = (s, x, a, z) represents the speaker
s E {User, System} producing the natural lan-
guage statement x which asserts a speech act a E
{ASK, CONFIRM, ...} with meaning represen-
tation z. For example, from the second system ut-
terance in Figure 1 the question x =“Leaving what
city?” is an a=ASK speech act with lambda-calculus
meaning z = λx.from(fl, x). This meaning repre-
sents the fact that the system asked for the departure
city for the conversational object o = fl represent-
ing the flight leg that is currently being discussed.
We will learn from conversations where the speech
</bodyText>
<page confidence="0.997742">
422
</page>
<bodyText confidence="0.999762923076923">
acts a and logical forms z for user utterances are un-
labeled. Such data can be generated by recording
interactions, along with each system’s internal rep-
resentation of its own utterances.
Finally, since we will be analyzing sentences at a
specific point in a complete conversation, we define
our training data as a set {(ji, Ci)|i = 1... n}. Each
pair is a conversation Ci and the index ji of the user
utterance x in Ci whose meaning we will attempt to
learn to recover. In general, the same conversation
C can be used in multiple examples, each with a dif-
ferent sentence index. Section 8 provides the details
of how the data was gathered for our experiments.
</bodyText>
<sectionHeader confidence="0.952172" genericHeader="method">
3 Overview of Approach
</sectionHeader>
<bodyText confidence="0.9997765">
We will present an algorithm for learning a weighted
CCG parser, as defined in Section 5, that can be used
to map sentences to logical forms. The approach
induces a lexicon to represent the meanings of words
and phrases while also estimating the parameters of
a weighted linear model for selecting the best parse
given the lexicon.
Learning As defined in Section 2, the algorithm
takes a set of n training examples, {(ji, Ci) : i =
1, ... , n}. For each example, our goal is to learn to
parse the user utterance x at position ji in Ci. The
training data contains no direct evidence about the
logical form z that should be paired with x, or the
CCG analysis that would be used to construct z. We
model all of these choices as latent variables.
To learn effectively in this complex, latent space,
we introduce a loss function L(z, j, C) E R that
measures how well a logical form z models the
meaning for the user utterance at position j in C. In
Section 6, we will present the details of the loss we
use, which is designed to be sensitive to remedia-
tions in C (system requests for clarification, etc.) but
also be robust to the fact that conversations often do
not uniquely determine which z should be selected,
for example when the user prematurely ends the dis-
cussion. Then, in Section 7, we present an approach
for incorporating this loss function into a complete
algorithm that induces a CCG lexicon and estimates
the parameters of the parsing model.
This learning setup focuses on a subproblem in
dialog; semantic interpretation. We do not yet learn
to recover user speech acts or integrate the logical
form into the context of the conversation. These are
important areas for future work.
Evaluation We will evaluate performance on a
test set {(xi, zi)|i = 1, ... , m} of m sentences xi
that have been explicitly labeled with logical forms
zi. This data will allow us to directly evaluate the
quality of the learned model. Each sentence is an-
alyzed with the learned model alone; the loss func-
tion and any conversational context are not used dur-
ing evaluation. Parsers that perform well in this set-
ting will be strong candidates for inclusion in a more
complete dialog system, as motivated in Section 1.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999962545454546">
Most previous work on learning from conversational
interactions has focused on the dialog sub-problems
of response planning (e.g., Levin et al., 2000; Singh
et al., 2002) and natural language generation (e.g.,
Lemon, 2011). We are not aware of previous work
on inducing semantic parsers from conversations.
There has been significant work on supervised
learning for inducing semantic parsers. Various
techniques were applied to the problem includ-
ing machine translation (Papineni et al., 1997;
Ramaswamy and Kleindienst, 2000; Wong and
Mooney, 2006; 2007; Matuszek et al., 2010), higher-
order unification (Kwiatkowski et al., 2010), parsing
(Ruifang and Mooney, 2006; Lu et al., 2008), induc-
tive logic programming (Zelle and Mooney, 1996;
Thompson and Mooney, 2003; Tang and Mooney,
2000), probabilistic push-down automata (He and
Young, 2005; 2006) and ideas from support vec-
tor machines and string kernels (Kate and Mooney,
2006; Nguyen et al., 2006). The algorithms we de-
velop in this paper build on previous work on su-
pervised learning of CCG parsers (Zettlemoyer and
Collins, 2005; 2007), as we describe in Section 5.3.
There is also work on learning to do semantic
analysis with alternate forms of supervision. Clarke
et al. (2010) and Liang et al. (2011) describe ap-
proaches for learning semantic parsers from ques-
tions paired with database answers, while Gold-
wasser et al. (2011) presents work on unsuper-
vised learning. Our approach provides an alterna-
tive method of supervision that could complement
these approaches. Additionally, there has been sig-
nificant recent work on learning to do other, re-
</bodyText>
<page confidence="0.995108">
423
</page>
<bodyText confidence="0.705389">
I want to go from Boston to New York and then to Chicago
</bodyText>
<equation confidence="0.979530272727273">
S/N (N\N)/NP NP (N\N)/NP NP CONJ[] (N\N)/NP NP
λf.f λy.λf.λx.f(x) ∧ from(x, y) BOS λy.λf.λx.f(x) ∧ to(x, y) NYC λy.λf.λx.f(x) ∧ to(x, y) CHI
&gt; &gt; &gt;
(N\N) (N\N) (N\N)
λf.λx.f(x) ∧ from(x, BOS) λf.λx.f(x) ∧ to(x, NY C) λf.λx.f(x) ∧ to(x, CHI)
&lt;B
&lt;Φ&gt;
N
λx[].from(x[1], BOS) ∧ to(x[1], NYC) ∧ before(x[1], x[2]) ∧ to(x[2], CHI)
S
λx[].from(x[1], BOS) ∧ to(x[1], NYC) ∧ before(x[1], x[2]) ∧ to(x[2], CHI)
</equation>
<figureCaption confidence="0.991109333333333">
Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[]
that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the
lower ones create new nonterminals according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details.
</figureCaption>
<equation confidence="0.890638">
(N\N)
λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC)
(N\N)
λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI)
&gt;
</equation>
<bodyText confidence="0.999448333333333">
lated, natural language semantic analysis tasks from
context-dependent database queries (Miller et al.,
1996; Zettlemoyer and Collins, 2009), grounded
event streams (Chen et al., 2010; Liang et al., 2009),
environment interactions (Branavan et al., 2009;
2010; Vogel and Jurafsky, 2010), and even unanno-
tated text (Poon and Domingos, 2009; 2010).
Finally, the DARPA Communicator data (Walker
et al., 2002) has been previously studied. Walker and
Passonneau (2001) introduced a schema of speech
acts for evaluation of the DARPA Communicator
system performance. Georgila et al. (2009) extended
this annotation schema to user utterances using an
automatic process. Our speech acts extend this work
to additionally include full meaning representations.
</bodyText>
<sectionHeader confidence="0.932807" genericHeader="method">
5 Mapping Sentences to Logical Form
</sectionHeader>
<bodyText confidence="0.9999425">
We will use a weighted linear CCG grammar for se-
mantic parsing, as briefly reviewed in this section.
</bodyText>
<subsectionHeader confidence="0.990358">
5.1 Combinatory Categorial Grammars
</subsectionHeader>
<bodyText confidence="0.993631731707317">
Combinatory categorial grammars (CCGs) are a
linguistically-motivated model for a wide range of
language phenomena (Steedman, 1996; 2000). A
CCG is defined by a lexicon and a set of combina-
tors. The grammar defines a set of possible parse
trees, where each tree includes syntactic and seman-
tic information that can be used to construct logical
forms for sentences.
The lexicon contains entries that define categories
for words or phrases. For example, the second
lexical entry in the parse in Figure 2 is:
from := (N\N)/NP : Ay.Af.Ax.f(x) n from(x, y)
Each category includes both syntactic and seman-
tic information. For example, the phrase “from”
is assigned the category with syntax (N\N)/NP
and semantics Ay.Af.Ax.f(x) n from(x, y). The
outermost syntactic forward slash specifies that the
entry must first be combined with an NP to the
right (the departure city), while the inner back slash
specifies that it will later modify a noun N to the
left (to add a constraint to a set of flights). The
lambda-calculus semantic expression is designed
to build the appropriate meaning representation at
each of these steps, as seen in the parse in Figure 2.
In general, we make use of typed lambda cal-
culus to represent meaning (Carpenter, 1997), both
in the lexicon and in intermediate parse tree nodes.
We also introduce an extension for modeling array-
typed variables to represent lists of individual en-
tries. These constructions are used, for example, to
model sentences describing a sequence of segments
while specifying flight preferences.
Figure 2 shows how a CCG parse builds a logical
form for a complete sentence with an array-typed
variable. Each intermediate node in the tree is con-
structed with one of a small set of CCG combina-
tor rules, see the explanation from Steedman (1996;
2000). We make use of the standard application,
composition and coordination combinators, as well
as type-shifting rules introduced by Zettlemoyer and
Collins (2007) to model spontaneous, unedited text.
</bodyText>
<subsectionHeader confidence="0.998035">
5.2 Weighted Linear CCGs
</subsectionHeader>
<bodyText confidence="0.999789166666667">
A weighted linear CCG (Clark and Curran, 2007)
provides a ranking on the space of possible parses
under the grammar, which can be used to select
the best logical form for a sentence. This type of
model is closely related to several other approaches
(Ratnaparkhi et al., 1994; Johnson et al., 1999;
</bodyText>
<page confidence="0.995059">
424
</page>
<bodyText confidence="0.997730142857143">
Lafferty et al., 2001; Collins, 2004; Taskar et al.,
2004). Let x be a sentence, y be a CCG parse, and
GEN(x; A) be the set of all possible CCG parses for
x given the lexicon A. Define O(x, y) E Rd to be
a d-dimensional feature–vector representation and
0 E Rd to be a parameter vector. The optimal parse
for sentence x is
</bodyText>
<equation confidence="0.9948215">
y*(x) = arg max
yEGEN(x;A)
</equation>
<bodyText confidence="0.999952636363636">
and the final output logical form z is the lambda-
calculus expression at the root of y*(x).
We compute y*(x) with a CKY-style chart pars-
ing algorithm. Since each chart entry contains a
full lambda-calculus meaning expression, we use
N-best pruning to control the number of options we
consider at each span. Learning a model of this form
involves learning the parameters 0 and the lexicon
A. We will show that this is possible from conversa-
tional logs that do not contain any explicit labeling
of the logical forms for user utterances.
</bodyText>
<subsectionHeader confidence="0.9836">
5.3 Supervised learning with GENLEX
</subsectionHeader>
<bodyText confidence="0.99821896">
Previous work on lexical induction, including the
GENLEX approach which we briefly review here,
has required labeled logical meaning representa-
tions. In Section 7, we will introduce a new way
of using GENLEX to learn from unannotated con-
versation logs.
The supervised CCG learning algorithms of
Zettlemoyer and Collins (2005; 2007) induce a
weighted CCG from training examples (xi, zi) for
i = 1... n, where xi is a sentence and zi is the
corresponding lambda-calculus meaning representa-
tion. The output from the algorithm is a pair (0, A)
containing the learned parameters and CCG lexicon.
They defined the function GENLEX(x, z) to map
a sentence x and its meaning z to a large set of poten-
tial lexical entries. These entries are generated with
rules that consider the logical form z and guess po-
tential CCG categories. For example, the rule p →
(N\N)/NP : Ay.Af.Ax.f(x) n p(x, y) introduces
categories commonly used to model certain types of
prepositions. This rule would, for example, intro-
duce the category (N\N)/NP : Ay.Af.Ax.f(x) n
to(x, y) for any logical form z that contains the con-
stant to. GENLEX uses a small set of rules of this
type to generate categories that are paired with all
</bodyText>
<listItem confidence="0.9530818125">
Example Conversation:
(1) SYSTEM: how can i help you? (OPEN TASK)
(2) USER: i would like to fly from boston arriving to new york
and back from new york to boston
(3) SYSTEM: leaving boston (CONFIRM:from(fl1, BOS))
on what date? (ASK:Ax.departdate(fl1, x))
(4) USER: september twenty four
(5) SYSTEM: leaving boston (CONFIRM:from(fl1, BOS))
on september twenty four (CONFIRM:depart-
date(fl1, sept 24)) to new york
(CONFIRM:to(fl1, NY C)) is new york your final
destination? (ASK:-3x.fr(x, NY C))
(6) USER: yes i want to return to boston
(7) SYSTEM: sorry, i didn’t understand. (APOL-
OGY) where are you flying to from new york?
(ASK:Ax.fr(fl2, NYC) n to(fl2, x))
</listItem>
<figure confidence="0.930534">
[conversation ends]
Candidate Logical Expressions for Utterance #2:
(a) Ax.to(x, BOS) n from(x, NYC)
(b) Ax.from(x, BOS) n to(x, NYC)
(c) Ax.to(x, BOS) n to(x, NYC)
</figure>
<construct confidence="0.845161833333333">
(d) Ax[].from(x[1], BOS) n to(x[1], NYC)
n before(x[1], x[2]) n return(x[2])
n from(x[2], NYC) n to(x[2], BOS))
(e) Ax[].from(x[1], BOS) n to(x[1], NYC)
n before(x[1], x[2]) n return(x[2])
n from(x[2], BOS) n to(x[2], NYC)
</construct>
<figureCaption confidence="0.997772">
Figure 3: Conversation reflecting an interaction as seen
in the DARPA Communicator travel-planning dialogs.
</figureCaption>
<bodyText confidence="0.9984205">
possible substrings in x to form an overly general
lexicon. The complete learning algorithm then si-
multaneously selects a small subset of all entries
generated by GENLEX and estimates parameter val-
ues 0. Zettlemoyer and Collins (2005) present a
more detailed explanation.
</bodyText>
<sectionHeader confidence="0.989745" genericHeader="method">
6 Measuring Loss
</sectionHeader>
<bodyText confidence="0.999797444444444">
In Section 7, we will present a loss-sensitive learn-
ing algorithm that models the meaning of user utter-
ances as latent variables to be estimated from con-
versational interactions.
We first introduce a loss function to measure the
quality of potential meaning representations. This
loss function L(z, j, C) E R indicates how well a
logical expression z represents the meaning of the
j-th user utterance in conversation C. For example,
</bodyText>
<equation confidence="0.910225">
0 · O(x, y)
</equation>
<page confidence="0.989207">
425
</page>
<bodyText confidence="0.999828869565217">
consider the first user utterance (j = 2) in Figure 3,
which is a request for a return trip from Boston to
New York. We would like to assign the lowest loss
to the meaning representation (d) in Figure 3 that
correctly encodes all of the stated constraints.
We make use of a loss function with two parts:
L(z, j,C) = Lc(z, j,C) + Ld(z). The conversa-
tion loss Lc (defined in Section 6.1) measures how
well the candidate meaning representation fits the
conversation, for example incorporating informa-
tion recovered through conversational remediations
as motivated in Section 1. The domain loss Ld (de-
scribed in Section 6.2) measures how well a logi-
cal form z matches domain expectations, such as
the fact that flights can only have a single origin.
These functions guide the types of meaning repre-
sentations we expect to see, but in many cases will
fail to specify a unique best option, for example
in conversations where the user prematurely termi-
nates the interaction. In Section 7, we will present a
complete, loss-driven learning algorithm that is ro-
bust to these types of ambiguities while inducing a
weighted CCG parser from conversations.
</bodyText>
<subsectionHeader confidence="0.997492">
6.1 Conversation Loss
</subsectionHeader>
<bodyText confidence="0.984405683333333">
We will use a conversation loss function Lc(z, j, C)
that provides a rough indication of how well the log-
ical expression z represents a potential meaning for
the user utterance at position j in C. For example,
the first user utterance (j = 2) in Figure 3 is a re-
quest for a return trip from Boston to New York
where the user has explicitly mentioned both legs.
The figure also shows five options (a-e) for the logi-
cal form z. We want to assign the lowest loss to op-
tion (d), which includes all of the stated constraints.
The loss is computed in four steps for a user ut-
terance x at position j by (1) selecting a subset of
system utterances in the conversation C, (2) extract-
ing and computing loss for semantic content from
selected system utterances, (3) aligning the subex-
pressions in z to the extracted semantic content, and
(4) computing the minimal loss value from the best
alignment. In Figure 3, the loss for the candidate
logical forms is computed by considering the seg-
ment of system utterances up until the conversation
end. Within this segment, the matching for expres-
sion (d) involves mapping the origin and departure
constraints for the first leg (Boston - New York) onto
the earlier system confirmations while also align-
ing the ones for the second leg to system utterances
later in the selected portion of the conversation. Fi-
nally, the overall score depends on the quality of the
alignment, for example how many of the constraints
match to confirmations. This section presents the
full approach.
Segmentation For a user utterance at position j,
we select all system utterances from j − 1 until the
system believes it has completed the current subtask,
as indicated by a reset action or final offer. We call
this selected segment ¯C. In Figure 3, C¯ ends with a
reset, but in a successful interaction it would have
ended with the offer of a specific flight.
Extracting Properties A property is a predicate-
entity-value triplet, where the entity can be a vari-
able from z or a conversational object. For example,
(from, fl, BOS) is a property where fl is a ob-
ject from C¯ and (from, x, BOS) is a property from
z = Ax.from(x, BOS). We define P&amp; to be the
set of properties from logical forms for system ut-
terances in ¯C. Similarly, we define Pz to be the set
of properties in z.
Scoring System Properties For each system
property p E Pj we compute its position value
pos(p), which is a normalized weighted average
over all the positions where it appears in a logi-
cal form. For each mention the weight is obtained
from its speech act. For example, properties that are
explicitly confirmed contribute more to the average
than those that were merely offered to the user in a
select statement.
We use pos(p) to compute a loss loss(p) for
each property p E Pj. We first define Pff&apos; to be all
properties in P&amp; with entity e. For entity e and po-
sition d, we define the entity-normalization function:
d − minp∈PC¯&apos; pos(p)
</bodyText>
<equation confidence="0.997122">
C¯ pos(p) .
maxp∈PC¯ &apos; pos(p) − minp∈P &apos;
</equation>
<bodyText confidence="0.993623">
For a given property p E Pj with an entity e we
compute the loss value:
</bodyText>
<equation confidence="0.9995425">
loss(p) = n−1
e (1 − ne(pos(p))) − 1 .
</equation>
<bodyText confidence="0.9013384">
Where n�1
e is the inverse of ne. This loss value is de-
signed to, first, provide less loss for later properties
so that it, for example, favors the last property in a
series of statements that finally resolves a confusion
</bodyText>
<equation confidence="0.952771">
ne(d) =
</equation>
<page confidence="0.978444">
426
</page>
<bodyText confidence="0.964145225806451">
in the conversation. Second, the loss value is lower
for objects mentioned closer to the user utterance x,
thereby preferring objects discussed sooner.
Matching Properties An alignment A maps vari-
ables in z to conversational objects in ¯C, for exam-
ple the flight legs fl1 and fl2 being discussed in
Figure 3. We will use alignments to match prop-
erties of z and ¯C. To do this we extend the align-
ment function A to apply to properties, for example
A((from, x, BOS)) = (from, A(x), BOS).
Scoring Alignments Finally, we compute the
conversation loss L,(z, j, C) as follows:
The function s(A(p.), ps) E R computes the com-
patibility of the two input properties. It is zero if
A(p.) =� ps. Otherwise, it returns loss(ps).
We approximate the min computation in L, over
alignments A as follows. For a logical form z at
position j, we align the outer-most variable to the
conversational object in C¯ that is being discussed at
j. The remaining variables are aligned greedily to
minimize the loss, by selecting a single conversa-
tional object for each in turn.
Finally, for each aligned variable, we increase the
loss by one for each unmatched property from Pz.
This increases the loss of logical forms that include
spurious information. However, since a conversation
might stop prematurely and therefore won’t discuss
the entire user request, we only increase the loss for
variables that are already aligned. For this purpose,
we define an aligned variable to be one that has at
least one property matched successfully.
</bodyText>
<subsectionHeader confidence="0.997789">
6.2 Domain Loss
</subsectionHeader>
<bodyText confidence="0.995288076923077">
We also make use of a domain loss function Ld(z) E
R. The function takes a logical form z and returns
the number of violations there are in z to a set of
constraints on logical forms that occur commonly in
the dialog domain. For example, in a travel domain,
a violation might occur if a flight leg has two differ-
ent destination cities. The set of possible violations
must be specified for each dialog system, but can of-
ten be compiled from existing resources, such as a
database of valid flight ticketing options.
In our experiments, we will use a set of eight
simple constraints to check for violations in flight
Inputs: Training set {(ji, Ci) : i = 1 ... n} where each exam-
ple includes the index ji of a sentence xi in the conversation
Ci. Initial lexicon A0. Number of iterations T. Margin ry.
Beam size k for lexicon generation. Loss function L(x, j, C),
as described in Section 6.
Definitions: GENLEX(x, C) takes as input a sentence and a
conversation and returns a set of lexical items as described in
Section 7. GEN(x; A) is the set of all possible CCG parses
for x given the lexicon A. LF(y) returns the logical form
z at the root of the parse tree y. Let 4)i(y) be shorthand for
the feature function 4)(xi, y) defined in Section 5. Define
LEX(y) to be the set of lexical entries used in parse y. Fi-
nally, let MINLi(Y) be {y|∀y0 E Y, L(LF(y), ji, Ci) ≤
L(LF(y0), ji, Ci)}, the set of minimal loss parses in Y .
</bodyText>
<figure confidence="0.849575695652174">
Algorithm:
θ= 0¯, A = A0
Fort = 1 ... T, i = 1 ... n :
Step 1: (Lexical generation)
a. Set A = A U GENLEX(xi, Ci)
b. Let Y be the k highest scoring parses of xi using A
c. Select new lexical entries from the lowest loss parses
Ai = Uy∈MINL%(Y ){l|l E LEX(y)}
d. Set lexicon to A = A U Ai
Step 2: (Update parameters)
a. Define Gi = MINLi(GEN(xi, A, θ)) and
Lmin to be the minimal loss
b. Set Bi = GEN(xi, A, θ) − Gi
c. Set the relative loss function: Δi(y) = L(y, Ci)−Lmin
d. Construct sets of margin violating good and bad parses:
Ri = {r|r E Gi ∧
Ely0 E Bi s.t. (θ, 4)i(r) − 4)i(y0)) &lt; ryΔi(r)}
Ei = {e|e E Bi ∧
Ely0 E Gi s.t. (θ, 4)i(y0) − 4)i(e)) &lt; ryΔi(e)}
e. Apply the additive update:
1
θ = θ + Er∈R% |R � % 4)i(r) − �e∈E% |E% |4)i(e)
Output: Parameters θ and lexicon A
</figure>
<figureCaption confidence="0.99998">
Figure 4: The learning algorithm.
</figureCaption>
<bodyText confidence="0.999961">
itineraries, which can have multiple legs. These
include, for example, checking that the legs have
unique origins and destinations that match across the
entire itinerary. For example, in Figure 3 the logical
forms (a), (b) and (d) will have no violations; they
describe valid flights. Example (c) has a single vio-
lation: a flight has two origins. Example (e) violates
a more complex constraint: the second flight’s origin
is different from the first flight’s destination.
</bodyText>
<sectionHeader confidence="0.992805" genericHeader="method">
7 Learning
</sectionHeader>
<bodyText confidence="0.986726">
Figure 4 presents the complete learning algorithm.
We assume access to training examples, {(ji,Ci) :
</bodyText>
<equation confidence="0.856193714285714">
i = 1, ... , n}, where each example includes the in-
s(A(pu),ps) .
Lc(z, j, C) = min
A
�
E
p.∈P. p.∈PC¯
</equation>
<page confidence="0.991661">
427
</page>
<bodyText confidence="0.999846533333333">
dex ji of a sentence xi in the conversation Ci. The al-
gorithm learns a weighted CCG parser, described in
Section 5, including both a lexicon Λ and parameters
θ. The approach is online, considering each example
in turn and performing two steps: (1) expanding the
lexicon and (2) updating the parameters.
Step 1: Lexical Induction We introduce new lex-
ical items by selecting candidates from the function
GENLEX, following previous work (Zettlemoyer
and Collins, 2005; 2007) as reviewed in Section 5.3.
However, we face the new challenge that there is
no labeled logical-form meaning z. Instead, let ZC¯
be set of all logical forms that appear in system
utterances in the relevant conversation segment ¯C.
We will now define the conversational lexicon set:
</bodyText>
<equation confidence="0.9919745">
GENLEX(x, ¯C) = � GENLEX(x, z)
z∈ZC¯
</equation>
<bodyText confidence="0.999879630434783">
where we use logical forms from system utterances
to guess possible CCG categories for analyzing the
user utterance. This approach will overgeneralize,
when the system talks about things that are unrelated
to what the user said, and will also often be incom-
plete, for example when the system does not repeat
parts of the original content. However, it provides a
way of guessing lexical items that can be combined
with previously learned ones, which can fill in any
gaps and help select the best analysis.
Step 1(a) in Figure 4 uses GENLEX to tem-
porarily create a large set of potential categories
based on the conversation. Steps (b-d) select a small
subset of these entries to add to the current lexicon
Λ: we find the k-best parses under the model, re-
rank them according to loss, find the lexical items
used in the best trees, and add them to Λ. This
approach favors lexical items that are used in high-
scoring but low-loss analyses, as computed given the
current model.
Step 2: Parameter Updates Given the loss func-
tion G(x, i, C), we use a variant of a loss-sensitive
perceptron to update the parameters (Singh-Miller
and Collins, 2007). In Steps (a-c), for the current
example i, we compute the relative loss function Δi
that scales with the loss achieved by the best and
worst possible parses under the model. In contrast
to previous work, we do not only compute the loss
over a fixed n-best list of possible outputs, but in-
stead use the current model score to recompute the
options at each update. Then, Steps (d-e) find the set
Ri of least loss analyses and Ei of higher-loss can-
didates whose models scores are not separated by at
least -yΔi, where -y is a margin scale constant. The
final update (Step f) is additive and increases the pa-
rameters for features indicative of the analyses with
less loss while down weighting those for parses that
were not sufficiently separated.
Discussion This algorithm uses the conversation
to drive learning in two ways: it guides the lexi-
cal items that are proposed while also providing the
conversational feedback that defines the loss used to
update the parameters. The resulting approach is,
at every step, using information about how the con-
versation progressed after a user utterance to recon-
struct the meaning of the original statement.
</bodyText>
<sectionHeader confidence="0.94935" genericHeader="method">
8 Data Sets
</sectionHeader>
<bodyText confidence="0.999973740740741">
For evaluation, we used conversation logs from the
Lucent and BBN dialog systems in the DARPA
Communicator corpus (Walker et al., 2002). We se-
lected these systems since they provide significant
opportunities for learning. They asked relatively
open ended questions, allowing for more complex
user responses, while also using a number of sim-
ple remediating strategies to recover from misun-
derstandings. The original conversational logs in-
cluded unannotated transcripts of system and user
utterances. Inspired by the speech act labeling ap-
proach of Walker and Passonneau (2001), we wrote
a set of scripts to label the speech acts and logical
forms for system statements. This could be done
with high accuracy since the original text was gener-
ated with templates. These labels represent what the
system explicitly said and do not require complex,
potentially error-prone annotation of the full state of
the original dialog system. The set of speech acts in-
cludes confirmations, information requests, selects,
offers, instructions, and a miscellaneous category.
The data sets include a total of 376 conversations,
divided into training and testing sets. Table 1 pro-
vides details about the training and testing sets, as
well as general data set statistics. We developed our
system using 4-fold cross validation on the training
sets. Although there are approximately 12,000 user
</bodyText>
<page confidence="0.996076">
428
</page>
<table confidence="0.999397125">
Lucent BBN
# Conversations 214 162
Total # of utterances 11,974 12,579
Avg. utterances per conversation 55.95 77.65
Avg. tokens per user utterance 3.24 2.39
Total # of training utterances 208 67
Total # of testing utterances 96 67
Avg. tokens per selected utterance 11.72 9.53
</table>
<tableCaption confidence="0.999933">
Table 1: Data set statistics for Lucent and BBN systems.
</tableCaption>
<bodyText confidence="0.9998594">
utterances in the data sets, the vast majority are sim-
ple, short phrases (such as “yes” or “no”) which are
not useful for learning a semantic parser. We se-
lect user utterances with a small set of heuristics, in-
cluding a threshold (6 for Lucent, 4 for BBN) on the
number of words and requiring that at least one noun
phrase is present from our initial lexicon. This ap-
proach was manually developed to perform well on
the training sets, but is not perfect and does intro-
duce a small amount of noise into the data.
</bodyText>
<sectionHeader confidence="0.97817" genericHeader="method">
9 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999946666666667">
This section describes our experimental setup and
comparisons. We follow the setup of Zettlemoyer
and Collins (2007) where possible, including fea-
ture design, initialization of the semantic parser, and
evaluation metrics, as reviewed below.
Features and Parser The features include indica-
tors for lexical item use, properties of the logical
form that is being constructed, and indicators for
parsing operators used to build the tree. The parser
attempts to boost recall with a two-pass strategy that
allows for word skipping if the initial parse fails.
Initialization and Parameters We use an initial
lexicon that includes a list of domain-specific noun
phrases, such as city and airport names, and a list
of domain-independent categories for closed-class
words such as “the” and “and”. We also used a time
and number parser to expand this lexicon for each
input sentence with the BIU Number Normalizer.1
The learning parameters were tuned using the devel-
opment sets: the margin constant y is set to 0.5, we
use 6 iterations and take the top 30 parses for lexical
generation (step 1, figure 4). The parser used for pa-
rameter update (step 2, figure 4) has a beam of 250.
The parameter vector is initialized to ¯0.
</bodyText>
<footnote confidence="0.947125">
1http://www.cs.biu.ac.il/˜nlp/downloads/
</footnote>
<bodyText confidence="0.9982474">
Evaluation Metrics For evaluation, we measure
performance against gold standard labels. We report
both the number of exact matches, fully correct log-
ical forms, and a partial-credit number. We measure
partial-credit accuracy by mapping logical forms to
attribute-value pairs (for example, the expression
from(x, LA) will be mapped to from = LA) and
report precision and recall on attribute sets. This
more lenient measure does not test the overall struc-
ture of the logical expression, only its components.
Systems We compare performance with the fol-
lowing systems:
Full Supervision: We measured how a fully super-
vised approach would perform on our data by hand-
labeling the training data and using a 0-1 loss func-
tion that tests if the output logical form matches the
labeled one. For lexicon generation, the labels were
used instead of the conversation.
No Conversation Baseline: We also report results
for a no conversation baseline. This baseline sys-
tem is constructed by making two modifications to
the full approach. We remove the conversation loss
function and apply the GENLEX templates to every
possible logical constant, instead of only those in the
conversation. This baseline allows us to measure the
importance of having access to the conversations by
completely ignoring the context for each sentence.
Ablations: In addition to the baseline above, we
also do ablation tests by turning off various individ-
ual components of the complete algorithm.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="evaluation">
10 Results
</sectionHeader>
<bodyText confidence="0.999889466666667">
Table 2 shows exact match results for the develop-
ment sets, including different system configurations.
We report mean results across four folds. To ver-
ify their contributions, we include results where we
ablate the conversational loss and domain loss func-
tions. Both are essential.
The test results are listed in Table 3. The full
method significantly outperforms the baseline, indi-
cating that we are making effective use of the con-
versational feedback, although we do not yet match
the fully supervised result. The poor baseline per-
formance is not surprising, given the difficulty of the
task and lack of guidance when the conversations are
removed. The partial-credit numbers also demon-
strate an empirical trend that we observed; in many
</bodyText>
<page confidence="0.997314">
429
</page>
<table confidence="0.99975">
Exact Match Metric Lucent BBN
Prec. Rec. F1 Prec. Rec. F1
Without conversational loss 0.35 0.34 0.35 0.66 0.54 0.59
Without domain loss 0.42 0.42 0.42 0.69 0.56 0.61
Our Approach 0.63 0.61 0.62 0.77 0.64 0.69
Supervised method 0.76 0.75 0.75 0.81 0.67 0.73
</table>
<tableCaption confidence="0.978259">
Table 2: Mean exact-match results for cross fold evaluation on the development sets.
</tableCaption>
<table confidence="0.9998805">
Exact Match Metric Lucent BBN
Prec. Rec. F1 Prec. Rec. F1
No Conversations Baseline 0 0 0 0.16 0.15 0.15
Our Approach 0.58 0.55 0.56 0.85 0.75 0.79
Supervised method 0.7 0.68 0.69 0.87 0.78 0.82
Partial Credit Metric Lucent BBN
Prec. Rec. F1 Prec. Rec. F1
No Conversations Baseline 0.26 0.35 0.29 0.26 0.33 0.29
Our Approach 0.68 0.63 0.65 0.97 0.57 0.72
Supervised method 0.75 0.68 0.72 0.96 0.68 0.79
</table>
<tableCaption confidence="0.999884">
Table 3: Exact- and partial-match results on the test sets.
</tableCaption>
<bodyText confidence="0.999983571428572">
cases where we do not produce the correct logical
form, the output is often close to correct, with only
one or two missed flight constraints.
The difference between the two systems is evi-
dent. The BBN system presents a simpler approach
to the dialog problem by creating a more constrained
conversation. This is done by handling one flight
at a time, in the case of flight planing, and pos-
ing simple and close ended questions to the user.
Such an approach encourages the user to make sim-
pler requests, with relatively few constraints in each
request. In contrast, the Lucent system presents a
less-constrained approach: interactions start with an
open ended prompt and the conversations flow in a
more natural, less constrained fashion. BBN’s sim-
plified approach makes it easier for learning, giving
us superior performance when compared to the Lu-
cent system, despite the smaller training set. This is
true for both our approach and supervised learning.
We compared the logical forms recovered by the
best conversational model to the labeled ones in the
training set. Many of the errors came from cases
where the dialog system never fully recovered from
confusions in the conversation. For example, the Lu-
cent system almost never understood user utterances
that specified flight arrival times. Since it was unable
to consistently recover and introduce this constraint,
the user would often just recalculate and specify a
departure time that would achieve the original goal.
This type of failure provides no signal for our learn-
ing algorithm, whereas the fully supervised algo-
rithm would use labeled logical forms to resolve the
confusion. Interestingly, the test set had more sen-
tences that suffered such failures than the develop-
ment set, which contributed to the performance gap.
</bodyText>
<sectionHeader confidence="0.995905" genericHeader="discussions">
11 Discussion
</sectionHeader>
<bodyText confidence="0.999983058823529">
We presented a loss-driven learning approach that
induces the lexicon and parameters of a CCG parser
for mapping sentences to logical forms. The loss
was defined over the conversational context, without
requiring annotation of user utterances meaning.
The overall approach assumes that, in aggregate,
the conversations contain sufficient signal (remedia-
tions such as clarification, etc.) to learn effectively.
In this paper, we satisfied this requirement by us-
ing logs from automated systems that deployed rea-
sonably effective recovery strategies. An important
area for future work is to consider how this learning
can be best integrated into a complete dialog system.
This would include designing remediation strategies
that allow for the most effective learning and consid-
ering how similar techniques could be used simulta-
neously for other dialog subproblems.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998572833333333">
The research was supported by funding from the
DARPA Computer Science Study Group. Thanks
to Dan Weld, Raphael Hoffmann, Jonathan Berant,
Hoifung Poon and Mark Yatskar for their sugges-
tions and comments. We also thank Shachar Mirkin
for providing access to the BIU Normalizer.
</bodyText>
<page confidence="0.997105">
430
</page>
<sectionHeader confidence="0.996415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995661131313132">
Allen, J., M. Manshadi, M. Dzikovska, and M. Swift.
2007. Deep linguistic processing for spoken dialogue
systems. In Proceedings of the Workshop on Deep Lin-
guistic Processing.
Branavan, SRK, H. Chen, L.S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint Con-
ference of the Association for Computational Linguis-
tics and the International Joint Conference on Natural
Language Processing.
Branavan, SRK, L.S. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: learning to map high-level
instructions to commands. In Proceedings of the Asso-
ciation for Computational Linguistics.
Carpenter, B. 1997. Type-Logical Semantics. The MIT
Press.
Chen, D.L., J. Kim, and R.J. Mooney. 2010. Training a
multilingual sportscaster: using perceptual context to
learn language. Journal of Artificial Intelligence Re-
search 37(1):397–436.
Clark, S. and J.R. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models.
Computational Linguistics 33(4):493–552.
Clarke, J., D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world’s response.
In Proceedings of the Conference on Computational
Natural Language Learning.
Collins, M. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-
free methods. In New Developments in Parsing Tech-
nology.
Georgila, K., O. Lemon, J. Henderson, and J.D. Moore.
2009. Automatic annotation of context and speech acts
for dialogue corpora. Natural Language Engineering
15(03):315–353.
Goldwasser, D., R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In Proceedings. of the Association of Computa-
tional Linguistics.
He, Y. and S. Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech and
Language 19:85–106.
He, Y. and S. Young. 2006. Spoken language understand-
ing using the hidden vector state model. Speech Com-
munication 48(3-4).
Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999. Estimators for stochastic “unification-based”
grammars. In Proceedings of the Association for Com-
putational Linguistics.
Kate, R.J. and R.J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proceedings of the
Association for Computational Linguistics.
Kwiatkowski, T., L.S. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic ccg gram-
mars from logical form with higher-order unification.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning.
Lemon, O. 2011. Learning what to say and how to say
it: Joint optimisation of spoken dialogue management
and natural language generation. Computer Speech &amp;
Language 25(2):210–221.
Levin, E., R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing 8(1):11–23.
Liang, P., M.I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In Pro-
ceedings of the Joint Conference of the Association
for Computational Linguistics the International Joint
Conference on Natural Language Processing.
Liang, P., M.I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Pro-
ceedings of the Association for Computational Lin-
guistics.
Litman, D., J. Moore, M.O. Dzikovska, and E. Farrow.
2009. Using Natural Language Processing to Analyze
Tutorial Dialogue Corpora Across Domains Modali-
ties. In Proceeding of the Conference on Artificial In-
telligence in Education.
Lu, W., H.T. Ng, W.S. Lee, and L.S. Zettlemoyer. 2008.
A generative model for parsing natural language to
meaning representations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Matuszek, C., D. Fox, and K. Koscher. 2010. Follow-
ing directions using statistical machine translation. In
Proceeding of the international conference on Human-
robot interaction.
Miller, S., D. Stallard, R.J. Bobrow, and R.L. Schwartz.
1996. A fully statistical approach to natural language
interfaces. In Proceedings of the Association for Com-
putational Linguistics.
Nguyen, L., A. Shimazu, and X. Phan. 2006. Seman-
tic parsing with structured SVM ensemble classifica-
tion models. In Proceedings of the joint conference
</reference>
<page confidence="0.984426">
431
</page>
<reference confidence="0.999015767676768">
of the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics.
Papineni, K.A., S. Roukos, and T.R. Ward. 1997. Feature-
based language understanding. In Proceedings of the
European Conference on Speech Communication and
Technology.
Poon, H. and P. Domingos. 2009. Unsupervised semantic
parsing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Poon, H. and P. Domingos. 2010. Unsupervised ontology
induction from text. In Proceedings of the Association
for Computational Linguistics.
Ramaswamy, G.N. and J. Kleindienst. 2000. Hierarchi-
cal feature-based translation for scalable natural lan-
guage understanding. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Ratnaparkhi, A., S. Roukos, and R.T. Ward. 1994. A
maximum entropy model for parsing. In Proceedings
of the International Conference on Spoken Language
Processing.
Ruifang, G. and R.J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Porceedings of the
Association for Computational Linguistics.
Singh, S.P., D.J. Litman, M.J. Kearns, and M.A. Walker.
2002. Optimizing dialogue management with re-
inforcement learning: Experiments with the NJFun
system. Journal of Artificial Intelligence Research
16(1):105–133.
Singh-Miller, N. and M. Collins. 2007. Trigger-based
language modeling using a loss-sensitive perceptron
algorithm. In IEEE International Conference on
Acoustics, Speech and Signal Processing.
Steedman, M. 1996. Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. 2000. The Syntactic Process. The MIT
Press.
Tang, L.R. and R.J. Mooney. 2000. Automated construc-
tion of database interfaces: Integrating statistical and
relational learning for semantic parsing. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora.
Taskar, B., D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Thompson, C.A. and R.J. Mooney. 2003. Acquiring
word-meaning mappings for natural language inter-
faces. Journal ofArtificial Intelligence Research 18:1–
44.
Vogel, A. and D. Jurafsky. 2010. Learning to follow nav-
igational directions. In Proceedings of the Association
for Computational Linguistics.
Walker, M. and R. Passonneau. 2001. DATE: a dia-
logue act tagging scheme for evaluation of spoken di-
alogue systems. In Proceedings of the First Inter-
national Conference on Human Language Technology
Research.
Walker, M., A. Rudnicky, R. Prasad, J. Aberdeen,
E. Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom,
A. Potamianos, et al. 2002. DARPA Communicator:
Cross-system results for the 2001 evaluation. In Pro-
ceedings of the International Conference on Spoken
Language Processing.
Wong, Y.W. and R.J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the North American Association for Compu-
tational Linguistics.
Wong, Y.W. and R.J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Association for Com-
putational Linguistics.
Young, S., M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
POMDP-based spoken dialogue management. Com-
puter Speech &amp; Language 24(2):150–174.
Zelle, J.M. and R.J. Mooney. 1996. Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence.
Zettlemoyer, L.S. and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artificial
Intelligence.
Zettlemoyer, L.S. and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Zettlemoyer, L.S. and Michael Collins. 2009. Learning
context-dependent mappings from sentences to logical
form. In Proceedings of the Joint Conference of the
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing.
</reference>
<page confidence="0.99854">
432
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.927595">
<title confidence="0.999485">Bootstrapping Semantic Parsers from Conversations</title>
<author confidence="0.95988">Artzi</author>
<affiliation confidence="0.999786">Computer Science &amp; University of</affiliation>
<address confidence="0.996339">Seattle, WA</address>
<abstract confidence="0.998643">Conversations provide rich opportunities for interactive, continuous learning. When something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals. In this paper, we present an approach for using conversational interactions of this type to induce semantic parsers. We demonstrate learning without any explicit annotation of the meanings of user utterances. Instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation. This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system. Experiments on DARPA Communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>M Manshadi</author>
<author>M Dzikovska</author>
<author>M Swift</author>
</authors>
<title>Deep linguistic processing for spoken dialogue systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Deep Linguistic Processing.</booktitle>
<contexts>
<context position="2006" citStr="Allen et al., 2007" startWordPosition="293" endWordPosition="296">rong, if often indirect, signal that can be used to learn to avoid the original confusion in future conversations. In this paper, we show how to use this type of conversational feedback to learn to better recover the meaning of user utterances, by inducing semantic parsers from unannotated conversational logs. We believe that this style of learning will contribute to the long term goal of building self-improving dialog systems that continually learn from their mistakes, with little or no human intervention. Many dialog systems use a semantic parsing component to analyze user utterances (e.g., Allen et al., 2007; Litman et al., 2009; Young et al., 2010). For example, in a flight booking system, the sentence Sent: I want to go to Seattle on Friday LF: λx.to(x, SEA) ∧ date(x, FRI) might be mapped to the logical form (LF) meaning representation above, a lambda-calculus expression defining the set of flights that match the user’s desired constraints. This LF is a representation of the semantic content that comes from the sentence, and would be input to a context-dependent understanding component in a full dialog system, for example to find the date that the symbol FRI refers to. To induce semantic parser</context>
</contexts>
<marker>Allen, Manshadi, Dzikovska, Swift, 2007</marker>
<rawString>Allen, J., M. Manshadi, M. Dzikovska, and M. Swift. 2007. Deep linguistic processing for spoken dialogue systems. In Proceedings of the Workshop on Deep Linguistic Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>H Chen</author>
<author>L S Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="13504" citStr="Branavan et al., 2009" startWordPosition="2200" endWordPosition="2203">specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as b</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>Branavan, SRK, H. Chen, L.S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>L S Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reading between the lines: learning to map high-level instructions to commands.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>Branavan, SRK, L.S. Zettlemoyer, and R. Barzilay. 2010. Reading between the lines: learning to map high-level instructions to commands. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>Type-Logical Semantics.</title>
<date>1997</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="15412" citStr="Carpenter, 1997" startWordPosition="2509" endWordPosition="2510">mple, the phrase “from” is assigned the category with syntax (N\N)/NP and semantics Ay.Af.Ax.f(x) n from(x, y). The outermost syntactic forward slash specifies that the entry must first be combined with an NP to the right (the departure city), while the inner back slash specifies that it will later modify a noun N to the left (to add a constraint to a set of flights). The lambda-calculus semantic expression is designed to build the appropriate meaning representation at each of these steps, as seen in the parse in Figure 2. In general, we make use of typed lambda calculus to represent meaning (Carpenter, 1997), both in the lexicon and in intermediate parse tree nodes. We also introduce an extension for modeling arraytyped variables to represent lists of individual entries. These constructions are used, for example, to model sentences describing a sequence of segments while specifying flight preferences. Figure 2 shows how a CCG parse builds a logical form for a complete sentence with an array-typed variable. Each intermediate node in the tree is constructed with one of a small set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, compo</context>
</contexts>
<marker>Carpenter, 1997</marker>
<rawString>Carpenter, B. 1997. Type-Logical Semantics. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
<author>J Kim</author>
<author>R J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: using perceptual context to learn language.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="13434" citStr="Chen et al., 2010" startWordPosition="2190" endWordPosition="2193">struction of a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical For</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>Chen, D.L., J. Kim, and R.J. Mooney. 2010. Training a multilingual sportscaster: using perceptual context to learn language. Journal of Artificial Intelligence Research 37(1):397–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="16233" citStr="Clark and Curran, 2007" startWordPosition="2637" endWordPosition="2640">or example, to model sentences describing a sequence of segments while specifying flight preferences. Figure 2 shows how a CCG parse builds a logical form for a complete sentence with an array-typed variable. Each intermediate node in the tree is constructed with one of a small set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; 424 Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Clark, S. and J.R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>M Chang</author>
<author>D Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="11931" citStr="Clarke et al. (2010)" startWordPosition="1941" endWordPosition="1944"> et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Additionally, there has been significant recent work on learning to do other, re423 I want to go from Boston to New York and then to Chicago S/N (N\N)/NP NP (N\N)/NP NP CONJ[] (N\N)/NP NP λf.f λy.λf.λx.f(x) ∧ from(x, y) BOS λy.λf.λx.f(x) ∧ to(x, y) NYC λy.λf.λx.f(x) ∧ to(x, y) CHI &gt; &gt; &gt; (N\N) (N\N) (N\N) λf.λ</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>Clarke, J., D. Goldwasser, M. Chang, and D. Roth. 2010. Driving semantic parsing from the world’s response. In Proceedings of the Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distributionfree methods.</title>
<date>2004</date>
<booktitle>In New Developments in Parsing Technology.</booktitle>
<contexts>
<context position="16525" citStr="Collins, 2004" startWordPosition="2690" endWordPosition="2691">ules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; 424 Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the lambdacalculus expression at the root of y*(x). We compute y*(x) with a CKY-style chart parsing algorithm. Since each chart entry contains a full lambda-calculus meaning expression, we use N-best pruning to control the number of options we con</context>
</contexts>
<marker>Collins, 2004</marker>
<rawString>Collins, M. 2004. Parameter estimation for statistical parsing models: Theory and practice of distributionfree methods. In New Developments in Parsing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>O Lemon</author>
<author>J Henderson</author>
<author>J D Moore</author>
</authors>
<title>Automatic annotation of context and speech acts for dialogue corpora.</title>
<date>2009</date>
<journal>Natural Language Engineering</journal>
<volume>15</volume>
<issue>03</issue>
<contexts>
<context position="13833" citStr="Georgila et al. (2009)" startWordPosition="2250" endWordPosition="2253"> ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. 5.1 Combinatory Categorial Grammars Combinatory categorial grammars (CCGs) are a linguistically-motivated model for a wide range of language phenomena (Steedman, 1996; 2000). A CCG is defined by a lexicon and a set of combinators. The grammar defines a set of possible parse trees, where each tre</context>
</contexts>
<marker>Georgila, Lemon, Henderson, Moore, 2009</marker>
<rawString>Georgila, K., O. Lemon, J. Henderson, and J.D. Moore. 2009. Automatic annotation of context and speech acts for dialogue corpora. Natural Language Engineering 15(03):315–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>R Reichart</author>
<author>J Clarke</author>
<author>D Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings. of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="12081" citStr="Goldwasser et al. (2011)" startWordPosition="1965" endWordPosition="1969">3; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Additionally, there has been significant recent work on learning to do other, re423 I want to go from Boston to New York and then to Chicago S/N (N\N)/NP NP (N\N)/NP NP CONJ[] (N\N)/NP NP λf.f λy.λf.λx.f(x) ∧ from(x, y) BOS λy.λf.λx.f(x) ∧ to(x, y) NYC λy.λf.λx.f(x) ∧ to(x, y) CHI &gt; &gt; &gt; (N\N) (N\N) (N\N) λf.λx.f(x) ∧ from(x, BOS) λf.λx.f(x) ∧ to(x, NY C) λf.λx.f(x) ∧ to(x, CHI) &lt;B &lt;Φ&gt; N λx[].from(x[1], BOS) ∧ to(x[1], NYC) ∧ before(x[1], x[2]) ∧ to(x[2], C</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Goldwasser, D., R. Reichart, J. Clarke, and D. Roth. 2011. Confidence driven unsupervised semantic parsing. In Proceedings. of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>Semantic processing using the hidden vector state model.</title>
<date>2005</date>
<journal>Computer Speech and Language</journal>
<pages>19--85</pages>
<contexts>
<context position="11536" citStr="He and Young, 2005" startWordPosition="1873" endWordPosition="1876"> We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach p</context>
</contexts>
<marker>He, Young, 2005</marker>
<rawString>He, Y. and S. Young. 2005. Semantic processing using the hidden vector state model. Computer Speech and Language 19:85–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>Spoken language understanding using the hidden vector state model.</title>
<date>2006</date>
<journal>Speech Communication</journal>
<pages>48--3</pages>
<marker>He, Young, 2006</marker>
<rawString>He, Y. and S. Young. 2006. Spoken language understanding using the hidden vector state model. Speech Communication 48(3-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Geman</author>
<author>S Canon</author>
<author>Z Chi</author>
<author>S Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="16483" citStr="Johnson et al., 1999" startWordPosition="2681" endWordPosition="2684">ucted with one of a small set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; 424 Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the lambdacalculus expression at the root of y*(x). We compute y*(x) with a CKY-style chart parsing algorithm. Since each chart entry contains a full lambda-calculus meaning expression, we use N-best pruni</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>R J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11624" citStr="Kate and Mooney, 2006" startWordPosition="1888" endWordPosition="1891">here has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Add</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Kate, R.J. and R.J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L S Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Inducing probabilistic ccg grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11325" citStr="Kwiatkowski et al., 2010" startWordPosition="1841" endWordPosition="1844">previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. </context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Kwiatkowski, T., L.S. Zettlemoyer, S. Goldwater, and M. Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higher-order unification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="16510" citStr="Lafferty et al., 2001" startWordPosition="2686" endWordPosition="2689">set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; 424 Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the lambdacalculus expression at the root of y*(x). We compute y*(x) with a CKY-style chart parsing algorithm. Since each chart entry contains a full lambda-calculus meaning expression, we use N-best pruning to control the number of</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
</authors>
<title>Learning what to say and how to say it: Joint optimisation of spoken dialogue management and natural language generation.</title>
<date>2011</date>
<journal>Computer Speech &amp; Language</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="10917" citStr="Lemon, 2011" startWordPosition="1783" endWordPosition="1784">gical forms zi. This data will allow us to directly evaluate the quality of the learned model. Each sentence is analyzed with the learned model alone; the loss function and any conversational context are not used during evaluation. Parsers that perform well in this setting will be strong candidates for inclusion in a more complete dialog system, as motivated in Section 1. 4 Related Work Most previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata </context>
</contexts>
<marker>Lemon, 2011</marker>
<rawString>Lemon, O. 2011. Learning what to say and how to say it: Joint optimisation of spoken dialogue management and natural language generation. Computer Speech &amp; Language 25(2):210–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="10844" citStr="Levin et al., 2000" startWordPosition="1770" endWordPosition="1773">zi)|i = 1, ... , m} of m sentences xi that have been explicitly labeled with logical forms zi. This data will allow us to directly evaluate the quality of the learned model. Each sentence is analyzed with the learned model alone; the loss function and any conversational context are not used during evaluation. Parsers that perform well in this setting will be strong candidates for inclusion in a more complete dialog system, as motivated in Section 1. 4 Related Work Most previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson an</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>Levin, E., R. Pieraccini, and W. Eckert. 2000. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing 8(1):11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics the International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="13455" citStr="Liang et al., 2009" startWordPosition="2194" endWordPosition="2197">cal form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weigh</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Liang, P., M.I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the Association for Computational Linguistics the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11955" citStr="Liang et al. (2011)" startWordPosition="1946" endWordPosition="1949">Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Additionally, there has been significant recent work on learning to do other, re423 I want to go from Boston to New York and then to Chicago S/N (N\N)/NP NP (N\N)/NP NP CONJ[] (N\N)/NP NP λf.f λy.λf.λx.f(x) ∧ from(x, y) BOS λy.λf.λx.f(x) ∧ to(x, y) NYC λy.λf.λx.f(x) ∧ to(x, y) CHI &gt; &gt; &gt; (N\N) (N\N) (N\N) λf.λx.f(x) ∧ from(x, BOS) λf</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Liang, P., M.I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>J Moore</author>
<author>M O Dzikovska</author>
<author>E Farrow</author>
</authors>
<title>Using Natural Language Processing to Analyze Tutorial Dialogue Corpora Across Domains Modalities.</title>
<date>2009</date>
<booktitle>In Proceeding of the Conference on Artificial Intelligence in Education.</booktitle>
<contexts>
<context position="2027" citStr="Litman et al., 2009" startWordPosition="297" endWordPosition="300">ect, signal that can be used to learn to avoid the original confusion in future conversations. In this paper, we show how to use this type of conversational feedback to learn to better recover the meaning of user utterances, by inducing semantic parsers from unannotated conversational logs. We believe that this style of learning will contribute to the long term goal of building self-improving dialog systems that continually learn from their mistakes, with little or no human intervention. Many dialog systems use a semantic parsing component to analyze user utterances (e.g., Allen et al., 2007; Litman et al., 2009; Young et al., 2010). For example, in a flight booking system, the sentence Sent: I want to go to Seattle on Friday LF: λx.to(x, SEA) ∧ date(x, FRI) might be mapped to the logical form (LF) meaning representation above, a lambda-calculus expression defining the set of flights that match the user’s desired constraints. This LF is a representation of the semantic content that comes from the sentence, and would be input to a context-dependent understanding component in a full dialog system, for example to find the date that the symbol FRI refers to. To induce semantic parsers from interactions, </context>
</contexts>
<marker>Litman, Moore, Dzikovska, Farrow, 2009</marker>
<rawString>Litman, D., J. Moore, M.O. Dzikovska, and E. Farrow. 2009. Using Natural Language Processing to Analyze Tutorial Dialogue Corpora Across Domains Modalities. In Proceeding of the Conference on Artificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lu</author>
<author>H T Ng</author>
<author>W S Lee</author>
<author>L S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11378" citStr="Lu et al., 2008" startWordPosition="1850" endWordPosition="1853"> focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches fo</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Lu, W., H.T. Ng, W.S. Lee, and L.S. Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>D Fox</author>
<author>K Koscher</author>
</authors>
<title>Following directions using statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceeding of the international conference on Humanrobot interaction.</booktitle>
<contexts>
<context position="11273" citStr="Matuszek et al., 2010" startWordPosition="1834" endWordPosition="1837">, as motivated in Section 1. 4 Related Work Most previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis</context>
</contexts>
<marker>Matuszek, Fox, Koscher, 2010</marker>
<rawString>Matuszek, C., D. Fox, and K. Koscher. 2010. Following directions using statistical machine translation. In Proceeding of the international conference on Humanrobot interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>D Stallard</author>
<author>R J Bobrow</author>
<author>R L Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13359" citStr="Miller et al., 1996" startWordPosition="2179" endWordPosition="2182">[2]) ∧ to(x[2], CHI) Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additional</context>
</contexts>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>Miller, S., D. Stallard, R.J. Bobrow, and R.L. Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Nguyen</author>
<author>A Shimazu</author>
<author>X Phan</author>
</authors>
<title>Semantic parsing with structured SVM ensemble classification models.</title>
<date>2006</date>
<booktitle>In Proceedings of the joint conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11646" citStr="Nguyen et al., 2006" startWordPosition="1892" endWordPosition="1895">nt work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Additionally, there has b</context>
</contexts>
<marker>Nguyen, Shimazu, Phan, 2006</marker>
<rawString>Nguyen, L., A. Shimazu, and X. Phan. 2006. Semantic parsing with structured SVM ensemble classification models. In Proceedings of the joint conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>T R Ward</author>
</authors>
<title>Featurebased language understanding.</title>
<date>1997</date>
<booktitle>In Proceedings of the European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="11187" citStr="Papineni et al., 1997" startWordPosition="1821" endWordPosition="1824">this setting will be strong candidates for inclusion in a more complete dialog system, as motivated in Section 1. 4 Related Work Most previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007),</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>Papineni, K.A., S. Roukos, and T.R. Ward. 1997. Featurebased language understanding. In Proceedings of the European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="13589" citStr="Poon and Domingos, 2009" startWordPosition="2214" endWordPosition="2217"> introduce lexical items while the lower ones create new nonterminals according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. 5.1 Combinatory Categorial Grammars Combinatory cate</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Poon, H. and P. Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Unsupervised ontology induction from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>Poon, Domingos, 2010</marker>
<rawString>Poon, H. and P. Domingos. 2010. Unsupervised ontology induction from text. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G N Ramaswamy</author>
<author>J Kleindienst</author>
</authors>
<title>Hierarchical feature-based translation for scalable natural language understanding.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="11220" citStr="Ramaswamy and Kleindienst, 2000" startWordPosition="1825" endWordPosition="1828">rong candidates for inclusion in a more complete dialog system, as motivated in Section 1. 4 Related Work Most previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. T</context>
</contexts>
<marker>Ramaswamy, Kleindienst, 2000</marker>
<rawString>Ramaswamy, G.N. and J. Kleindienst. 2000. Hierarchical feature-based translation for scalable natural language understanding. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>A maximum entropy model for parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="16461" citStr="Ratnaparkhi et al., 1994" startWordPosition="2677" endWordPosition="2680">node in the tree is constructed with one of a small set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; 424 Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the lambdacalculus expression at the root of y*(x). We compute y*(x) with a CKY-style chart parsing algorithm. Since each chart entry contains a full lambda-calculus meaning expressio</context>
</contexts>
<marker>Ratnaparkhi, Roukos, Ward, 1994</marker>
<rawString>Ratnaparkhi, A., S. Roukos, and R.T. Ward. 1994. A maximum entropy model for parsing. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ruifang</author>
<author>R J Mooney</author>
</authors>
<title>Discriminative reranking for semantic parsing.</title>
<date>2006</date>
<booktitle>In Porceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11360" citStr="Ruifang and Mooney, 2006" startWordPosition="1846" endWordPosition="1849">rsational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) desc</context>
</contexts>
<marker>Ruifang, Mooney, 2006</marker>
<rawString>Ruifang, G. and R.J. Mooney. 2006. Discriminative reranking for semantic parsing. In Porceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Singh</author>
<author>D J Litman</author>
<author>M J Kearns</author>
<author>M A Walker</author>
</authors>
<title>Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="10865" citStr="Singh et al., 2002" startWordPosition="1774" endWordPosition="1777">of m sentences xi that have been explicitly labeled with logical forms zi. This data will allow us to directly evaluate the quality of the learned model. Each sentence is analyzed with the learned model alone; the loss function and any conversational context are not used during evaluation. Parsers that perform well in this setting will be strong candidates for inclusion in a more complete dialog system, as motivated in Section 1. 4 Related Work Most previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang </context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>Singh, S.P., D.J. Litman, M.J. Kearns, and M.A. Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. Journal of Artificial Intelligence Research 16(1):105–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Singh-Miller</author>
<author>M Collins</author>
</authors>
<title>Trigger-based language modeling using a loss-sensitive perceptron algorithm.</title>
<date>2007</date>
<booktitle>In IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="4611" citStr="Singh-Miller and Collins, 2007" startWordPosition="706" endWordPosition="709">odel must accumulate evidence from many interactions to best recover the meaning of each specific sentence. We will learn semantic parsers defined by probabilistic Combinatory Categorial Grammars (PCCGs), which include both a lexicon and a weighted linear model for parse selection. The lexicon specifies the meanings of individual words and phrases, while the parameters of a parsing model define how to best combine word- and phrase-level meanings to analyze complete sentences. To learn without labeled meaning representations, we make use of a variant of the loss-sensitive Perceptron algorithm (Singh-Miller and Collins, 2007). We define loss functions to provide a rough measure of (1) how well a candidate meaning for a utterance matches the conversation that follows it and (2) how well the candidate matches our expectations about the types of things that are often said in the dialog’s domain. These notions of loss drive not only the parameter estimation but also the grammar induction process that constructs the CCG lexicon. Experiments on conversation logs from the DARPA Communicator corpus (Walker et al., 2002) demonstrate the feasibility of our approach. This paper makes the following contributions: • A formaliz</context>
<context position="31346" citStr="Singh-Miller and Collins, 2007" startWordPosition="5307" endWordPosition="5310">p 1(a) in Figure 4 uses GENLEX to temporarily create a large set of potential categories based on the conversation. Steps (b-d) select a small subset of these entries to add to the current lexicon Λ: we find the k-best parses under the model, rerank them according to loss, find the lexical items used in the best trees, and add them to Λ. This approach favors lexical items that are used in highscoring but low-loss analyses, as computed given the current model. Step 2: Parameter Updates Given the loss function G(x, i, C), we use a variant of a loss-sensitive perceptron to update the parameters (Singh-Miller and Collins, 2007). In Steps (a-c), for the current example i, we compute the relative loss function Δi that scales with the loss achieved by the best and worst possible parses under the model. In contrast to previous work, we do not only compute the loss over a fixed n-best list of possible outputs, but instead use the current model score to recompute the options at each update. Then, Steps (d-e) find the set Ri of least loss analyses and Ei of higher-loss candidates whose models scores are not separated by at least -yΔi, where -y is a margin scale constant. The final update (Step f) is additive and increases </context>
</contexts>
<marker>Singh-Miller, Collins, 2007</marker>
<rawString>Singh-Miller, N. and M. Collins. 2007. Trigger-based language modeling using a loss-sensitive perceptron algorithm. In IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="14303" citStr="Steedman, 1996" startWordPosition="2320" endWordPosition="2321">ker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. 5.1 Combinatory Categorial Grammars Combinatory categorial grammars (CCGs) are a linguistically-motivated model for a wide range of language phenomena (Steedman, 1996; 2000). A CCG is defined by a lexicon and a set of combinators. The grammar defines a set of possible parse trees, where each tree includes syntactic and semantic information that can be used to construct logical forms for sentences. The lexicon contains entries that define categories for words or phrases. For example, the second lexical entry in the parse in Figure 2 is: from := (N\N)/NP : Ay.Af.Ax.f(x) n from(x, y) Each category includes both syntactic and semantic information. For example, the phrase “from” is assigned the category with syntax (N\N)/NP and semantics Ay.Af.Ax.f(x) n from(x,</context>
<context position="15957" citStr="Steedman (1996" startWordPosition="2599" endWordPosition="2600">e use of typed lambda calculus to represent meaning (Carpenter, 1997), both in the lexicon and in intermediate parse tree nodes. We also introduce an extension for modeling arraytyped variables to represent lists of individual entries. These constructions are used, for example, to model sentences describing a sequence of segments while specifying flight preferences. Figure 2 shows how a CCG parse builds a logical form for a complete sentence with an array-typed variable. Each intermediate node in the tree is constructed with one of a small set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; 424 Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Steedman, M. 1996. Surface Structure and Interpretation. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="13099" citStr="Steedman (2000)" startWordPosition="2141" endWordPosition="2142">(x) ∧ to(x, y) CHI &gt; &gt; &gt; (N\N) (N\N) (N\N) λf.λx.f(x) ∧ from(x, BOS) λf.λx.f(x) ∧ to(x, NY C) λf.λx.f(x) ∧ to(x, CHI) &lt;B &lt;Φ&gt; N λx[].from(x[1], BOS) ∧ to(x[1], NYC) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) S λx[].from(x[1], BOS) ∧ to(x[1], NYC) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Pa</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Steedman, M. 2000. The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tang</author>
<author>R J Mooney</author>
</authors>
<title>Automated construction of database interfaces: Integrating statistical and relational learning for semantic parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="11482" citStr="Tang and Mooney, 2000" startWordPosition="1866" endWordPosition="1869">2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) </context>
</contexts>
<marker>Tang, Mooney, 2000</marker>
<rawString>Tang, L.R. and R.J. Mooney. 2000. Automated construction of database interfaces: Integrating statistical and relational learning for semantic parsing. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="16547" citStr="Taskar et al., 2004" startWordPosition="2692" endWordPosition="2695">xplanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; 424 Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the lambdacalculus expression at the root of y*(x). We compute y*(x) with a CKY-style chart parsing algorithm. Since each chart entry contains a full lambda-calculus meaning expression, we use N-best pruning to control the number of options we consider at each span. Le</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Taskar, B., D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Thompson</author>
<author>R J Mooney</author>
</authors>
<title>Acquiring word-meaning mappings for natural language interfaces.</title>
<date>2003</date>
<journal>Journal ofArtificial Intelligence Research</journal>
<volume>18</volume>
<pages>44</pages>
<contexts>
<context position="11458" citStr="Thompson and Mooney, 2003" startWordPosition="1862" endWordPosition="1865">t al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while G</context>
</contexts>
<marker>Thompson, Mooney, 2003</marker>
<rawString>Thompson, C.A. and R.J. Mooney. 2003. Acquiring word-meaning mappings for natural language interfaces. Journal ofArtificial Intelligence Research 18:1– 44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vogel</author>
<author>D Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13537" citStr="Vogel and Jurafsky, 2010" startWordPosition="2205" endWordPosition="2208">gs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. </context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Vogel, A. and D. Jurafsky. 2010. Learning to follow navigational directions. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>R Passonneau</author>
</authors>
<title>DATE: a dialogue act tagging scheme for evaluation of spoken dialogue systems.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research.</booktitle>
<contexts>
<context position="13714" citStr="Walker and Passonneau (2001)" startWordPosition="2232" endWordPosition="2235">eedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. 5.1 Combinatory Categorial Grammars Combinatory categorial grammars (CCGs) are a linguistically-motivated model for a wide range of language phenomena (Steedman, 1996; 2000). A </context>
<context position="33081" citStr="Walker and Passonneau (2001)" startWordPosition="5593" endWordPosition="5596">t the meaning of the original statement. 8 Data Sets For evaluation, we used conversation logs from the Lucent and BBN dialog systems in the DARPA Communicator corpus (Walker et al., 2002). We selected these systems since they provide significant opportunities for learning. They asked relatively open ended questions, allowing for more complex user responses, while also using a number of simple remediating strategies to recover from misunderstandings. The original conversational logs included unannotated transcripts of system and user utterances. Inspired by the speech act labeling approach of Walker and Passonneau (2001), we wrote a set of scripts to label the speech acts and logical forms for system statements. This could be done with high accuracy since the original text was generated with templates. These labels represent what the system explicitly said and do not require complex, potentially error-prone annotation of the full state of the original dialog system. The set of speech acts includes confirmations, information requests, selects, offers, instructions, and a miscellaneous category. The data sets include a total of 376 conversations, divided into training and testing sets. Table 1 provides details </context>
</contexts>
<marker>Walker, Passonneau, 2001</marker>
<rawString>Walker, M. and R. Passonneau. 2001. DATE: a dialogue act tagging scheme for evaluation of spoken dialogue systems. In Proceedings of the First International Conference on Human Language Technology Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>A Rudnicky</author>
<author>R Prasad</author>
<author>J Aberdeen</author>
<author>E Bratt</author>
<author>J Garofolo</author>
<author>H Hastie</author>
<author>A Le</author>
<author>B Pellom</author>
<author>A Potamianos</author>
</authors>
<title>DARPA Communicator: Cross-system results for the 2001 evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="5107" citStr="Walker et al., 2002" startWordPosition="787" endWordPosition="790">eaning representations, we make use of a variant of the loss-sensitive Perceptron algorithm (Singh-Miller and Collins, 2007). We define loss functions to provide a rough measure of (1) how well a candidate meaning for a utterance matches the conversation that follows it and (2) how well the candidate matches our expectations about the types of things that are often said in the dialog’s domain. These notions of loss drive not only the parameter estimation but also the grammar induction process that constructs the CCG lexicon. Experiments on conversation logs from the DARPA Communicator corpus (Walker et al., 2002) demonstrate the feasibility of our approach. This paper makes the following contributions: • A formalization of the problem of learning the meaning of user statements from conversational feedback, without requiring annotation. • A new loss-sensitive learning algorithm for this problem that induces semantic parsers from conversation logs. • Loss functions to measure the quality of hypothetical utterance meanings within the conversation in which they appear. • An evaluation on logs from two dialog systems SYSTEM: how can i help you? (OPEN TASK) USER: i would like to fly from atlanta georgia to </context>
<context position="13656" citStr="Walker et al., 2002" startWordPosition="2224" endWordPosition="2227">according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. 5.1 Combinatory Categorial Grammars Combinatory categorial grammars (CCGs) are a linguistically-motivated model for a w</context>
<context position="32641" citStr="Walker et al., 2002" startWordPosition="5528" endWordPosition="5531"> down weighting those for parses that were not sufficiently separated. Discussion This algorithm uses the conversation to drive learning in two ways: it guides the lexical items that are proposed while also providing the conversational feedback that defines the loss used to update the parameters. The resulting approach is, at every step, using information about how the conversation progressed after a user utterance to reconstruct the meaning of the original statement. 8 Data Sets For evaluation, we used conversation logs from the Lucent and BBN dialog systems in the DARPA Communicator corpus (Walker et al., 2002). We selected these systems since they provide significant opportunities for learning. They asked relatively open ended questions, allowing for more complex user responses, while also using a number of simple remediating strategies to recover from misunderstandings. The original conversational logs included unannotated transcripts of system and user utterances. Inspired by the speech act labeling approach of Walker and Passonneau (2001), we wrote a set of scripts to label the speech acts and logical forms for system statements. This could be done with high accuracy since the original text was </context>
</contexts>
<marker>Walker, Rudnicky, Prasad, Aberdeen, Bratt, Garofolo, Hastie, Le, Pellom, Potamianos, 2002</marker>
<rawString>Walker, M., A. Rudnicky, R. Prasad, J. Aberdeen, E. Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom, A. Potamianos, et al. 2002. DARPA Communicator: Cross-system results for the 2001 evaluation. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11243" citStr="Wong and Mooney, 2006" startWordPosition="1829" endWordPosition="1832">a more complete dialog system, as motivated in Section 1. 4 Related Work Most previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on le</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Wong, Y.W. and R.J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Y.W. and R.J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>M Gasic</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The hidden information state model: A practical framework for POMDP-based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="2048" citStr="Young et al., 2010" startWordPosition="301" endWordPosition="304">be used to learn to avoid the original confusion in future conversations. In this paper, we show how to use this type of conversational feedback to learn to better recover the meaning of user utterances, by inducing semantic parsers from unannotated conversational logs. We believe that this style of learning will contribute to the long term goal of building self-improving dialog systems that continually learn from their mistakes, with little or no human intervention. Many dialog systems use a semantic parsing component to analyze user utterances (e.g., Allen et al., 2007; Litman et al., 2009; Young et al., 2010). For example, in a flight booking system, the sentence Sent: I want to go to Seattle on Friday LF: λx.to(x, SEA) ∧ date(x, FRI) might be mapped to the logical form (LF) meaning representation above, a lambda-calculus expression defining the set of flights that match the user’s desired constraints. This LF is a representation of the semantic content that comes from the sentence, and would be input to a context-dependent understanding component in a full dialog system, for example to find the date that the symbol FRI refers to. To induce semantic parsers from interactions, we consider user stat</context>
</contexts>
<marker>Young, Gasic, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>Young, S., M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu. 2010. The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech &amp; Language 24(2):150–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="11431" citStr="Zelle and Mooney, 1996" startWordPosition="1858" endWordPosition="1861"> planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired wit</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>Zelle, J.M. and R.J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="11779" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1915" endWordPosition="1918">ne translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Additionally, there has been significant recent work on learning to do other, re423 I want to go from Boston to New York and then to Chicago S/N (N\N)/NP NP (</context>
<context position="17733" citStr="Zettlemoyer and Collins (2005" startWordPosition="2898" endWordPosition="2901">ber of options we consider at each span. Learning a model of this form involves learning the parameters 0 and the lexicon A. We will show that this is possible from conversational logs that do not contain any explicit labeling of the logical forms for user utterances. 5.3 Supervised learning with GENLEX Previous work on lexical induction, including the GENLEX approach which we briefly review here, has required labeled logical meaning representations. In Section 7, we will introduce a new way of using GENLEX to learn from unannotated conversation logs. The supervised CCG learning algorithms of Zettlemoyer and Collins (2005; 2007) induce a weighted CCG from training examples (xi, zi) for i = 1... n, where xi is a sentence and zi is the corresponding lambda-calculus meaning representation. The output from the algorithm is a pair (0, A) containing the learned parameters and CCG lexicon. They defined the function GENLEX(x, z) to map a sentence x and its meaning z to a large set of potential lexical entries. These entries are generated with rules that consider the logical form z and guess potential CCG categories. For example, the rule p → (N\N)/NP : Ay.Af.Ax.f(x) n p(x, y) introduces categories commonly used to mod</context>
<context position="19993" citStr="Zettlemoyer and Collins (2005)" startWordPosition="3269" endWordPosition="3272"> (b) Ax.from(x, BOS) n to(x, NYC) (c) Ax.to(x, BOS) n to(x, NYC) (d) Ax[].from(x[1], BOS) n to(x[1], NYC) n before(x[1], x[2]) n return(x[2]) n from(x[2], NYC) n to(x[2], BOS)) (e) Ax[].from(x[1], BOS) n to(x[1], NYC) n before(x[1], x[2]) n return(x[2]) n from(x[2], BOS) n to(x[2], NYC) Figure 3: Conversation reflecting an interaction as seen in the DARPA Communicator travel-planning dialogs. possible substrings in x to form an overly general lexicon. The complete learning algorithm then simultaneously selects a small subset of all entries generated by GENLEX and estimates parameter values 0. Zettlemoyer and Collins (2005) present a more detailed explanation. 6 Measuring Loss In Section 7, we will present a loss-sensitive learning algorithm that models the meaning of user utterances as latent variables to be estimated from conversational interactions. We first introduce a loss function to measure the quality of potential meaning representations. This loss function L(z, j, C) E R indicates how well a logical expression z represents the meaning of the j-th user utterance in conversation C. For example, 0 · O(x, y) 425 consider the first user utterance (j = 2) in Figure 3, which is a request for a return trip from</context>
<context position="29880" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5054" endWordPosition="5057">gorithm. We assume access to training examples, {(ji,Ci) : i = 1, ... , n}, where each example includes the ins(A(pu),ps) . Lc(z, j, C) = min A � E p.∈P. p.∈PC¯ 427 dex ji of a sentence xi in the conversation Ci. The algorithm learns a weighted CCG parser, described in Section 5, including both a lexicon Λ and parameters θ. The approach is online, considering each example in turn and performing two steps: (1) expanding the lexicon and (2) updating the parameters. Step 1: Lexical Induction We introduce new lexical items by selecting candidates from the function GENLEX, following previous work (Zettlemoyer and Collins, 2005; 2007) as reviewed in Section 5.3. However, we face the new challenge that there is no labeled logical-form meaning z. Instead, let ZC¯ be set of all logical forms that appear in system utterances in the relevant conversation segment ¯C. We will now define the conversational lexicon set: GENLEX(x, ¯C) = � GENLEX(x, z) z∈ZC¯ where we use logical forms from system utterances to guess possible CCG categories for analyzing the user utterance. This approach will overgeneralize, when the system talks about things that are unrelated to what the user said, and will also often be incomplete, for examp</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Zettlemoyer, L.S. and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="16124" citStr="Zettlemoyer and Collins (2007)" startWordPosition="2620" endWordPosition="2623">xtension for modeling arraytyped variables to represent lists of individual entries. These constructions are used, for example, to model sentences describing a sequence of segments while specifying flight preferences. Figure 2 shows how a CCG parse builds a logical form for a complete sentence with an array-typed variable. Each intermediate node in the tree is constructed with one of a small set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; 424 Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector </context>
<context position="34866" citStr="Zettlemoyer and Collins (2007)" startWordPosition="5892" endWordPosition="5895">ata sets, the vast majority are simple, short phrases (such as “yes” or “no”) which are not useful for learning a semantic parser. We select user utterances with a small set of heuristics, including a threshold (6 for Lucent, 4 for BBN) on the number of words and requiring that at least one noun phrase is present from our initial lexicon. This approach was manually developed to perform well on the training sets, but is not perfect and does introduce a small amount of noise into the data. 9 Experimental Setup This section describes our experimental setup and comparisons. We follow the setup of Zettlemoyer and Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. Features and Parser The features include indicators for lexical item use, properties of the logical form that is being constructed, and indicators for parsing operators used to build the tree. The parser attempts to boost recall with a two-pass strategy that allows for word skipping if the initial parse fails. Initialization and Parameters We use an initial lexicon that includes a list of domain-specific noun phrases, such as city and airport names, and a list of domain-</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Zettlemoyer, L.S. and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="13391" citStr="Zettlemoyer and Collins, 2009" startWordPosition="2183" endWordPosition="2186">Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (&gt;, &lt;, etc.), see Steedman (2000) for details. (N\N) λf.λx.f(x) ∧ from(x, BOS) ∧ to(x, NYC) (N\N) λf.λx[].f(x) ∧ from(x[1], BOS) ∧ to(x[1], NY C) ∧ before(x[1], x[2]) ∧ to(x[2], CHI) &gt; lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning represen</context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Zettlemoyer, L.S. and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>