<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000033">
<title confidence="0.72817">
A Syntactified Direct Translation Model with Linear-time Decoding
Hany Hassan
</title>
<author confidence="0.56249">
Cairo TDC
</author>
<affiliation confidence="0.473422">
IBM
</affiliation>
<address confidence="0.763904">
Cairo, Egypt
</address>
<email confidence="0.996079">
hanyh@eg.ibm.com
</email>
<author confidence="0.96934">
Khalil Sima’an
</author>
<affiliation confidence="0.907396">
Language and Computation
University of Amsterdam
Amsterdam, The Netherlands
</affiliation>
<email confidence="0.990853">
k.simaan@uva.nl
</email>
<author confidence="0.997234">
Andy Way
</author>
<affiliation confidence="0.830488333333333">
School of Computing
Dublin City University
Dublin, Ireland
</affiliation>
<email confidence="0.99553">
away@computing.dcu.ie
</email>
<sectionHeader confidence="0.993825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999385">
Recent syntactic extensions of statisti-
cal translation models work with a syn-
chronous context-free or tree-substitution
grammar extracted from an automatically
parsed parallel corpus. The decoders ac-
companying these extensions typically ex-
ceed quadratic time complexity.
This paper extends the Direct Transla-
tion Model 2 (DTM2) with syntax while
maintaining linear-time decoding. We
employ a linear-time parsing algorithm
based on an eager, incremental interpre-
tation of Combinatory Categorial Gram-
mar (CCG). As every input word is pro-
cessed, the local parsing decisions resolve
ambiguity eagerly, by selecting a single
supertag–operator pair for extending the
dependency parse incrementally. Along-
side translation features extracted from
the derived parse tree, we explore syn-
tactic features extracted from the incre-
mental derivation process. Our empiri-
cal experiments show that our model sig-
nificantly outperforms the state-of-the art
DTM2 system.
</bodyText>
<sectionHeader confidence="0.999124" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953471698114">
Syntactic structure is gradually showing itself to
constitute a promising enrichment of state-of-the-
art Statistical Machine Translation (SMT) models.
However, it would appear that the decoding algo-
rithms are bearing the brunt of this improvement in
terms of time and space complexity. Most recent
extensions work with a synchronous context-free
or tree-substitution grammar extracted from an au-
tomatically parsed parallel corpus. While attrac-
tive in many ways, the decoders that are needed
for these types of grammars usually have time
and space complexities that are far beyond linear.
Leaving pruning aside, there is a genuine ques-
tion as to whether syntactic structure necessarily
implies more complex decoding algorithms. This
paper shows that this need not necessarily be the
case.
In this paper we extend the Direct Translation
Model (DTM2) (Ittycheriah and Roukos, 2007)
with target language syntax while maintaining
linear-time decoding. With this extension we
make three novel contributions to SMT. Our first
contribution is to define a linear-time syntactic
parser that works as incrementally as standard
SMT decoders (Tillmann and Ney, 2003; Koehn,
2004a). At every word position in the target lan-
guage string, this parser spans at most a single
parse-state to augment the translation states in
the decoder. The parse state summarizes previ-
ous parsing decisions and imposes constraints on
the set of valid future extensions such that a well-
formed sequence of parse states unambiguously
defines a dependency structure. This approach
is based on an incremental interpretation of the
mechanisms of Combinatory Categorial Grammar
(CCG) (Steedman, 2000).
Our second contribution lies in extending the
DMT2 model with a novel set of syntactically-
oriented feature functions. Crucially, these feature
functions concern the derived (partial) dependency
structure as well as local aspects of the derivation
process, including such information as the CCG
lexical categories (supertag), the CCG operators
and the intermediate parse states. This accom-
plishment is interesting both from a linguistic and
technical point of view.
Our third contribution is the extension of the
standard phrase-based decoder with the syntactic
structure and definition of new grammar-specific
pruning techniques that control the size of the
search space. Interestingly, because it is eager,
the incremental parser used in this work is hard
pushed to perform at a parsing level close to state-
</bodyText>
<page confidence="0.959874">
1182
</page>
<note confidence="0.996586">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182–1191,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999784615384615">
of-the-art cubic-time parsers. Nevertheless, the
parsing information it provides allows for signif-
icant improvement in translation quality.
We test the new model, called the Dependency-
based Direct Translation Model (DDTM), on stan-
dard Arabic–English translation tasks used in the
community, including LDC and GALE data. We
show that our DDTM system provides significant
improvements in BLEU (Papineni et al., 2002) and
TER (Snover et al., 2006) scores over the already
extremely competitive DTM2 system. We also
provide results of manual, qualitative analysis of
the system output to provide insight into the quan-
titative results.
This paper is organized as follows. Section 2
reviews the related work. Section 3 discusses the
DTM2 baseline model. Section 4 presents the gen-
eral workings of the incremental CCG parser lay-
ing the foundations for integrating it into DTM2.
Section 5 details our own DDTM, the dependency-
based extension of the DTM2 model. Section 6
reports on extensive experiments and their results.
Section 7 provides translation output to shed fur-
ther detailed insight into the characteristics of the
systems. Finally, Section 8 concludes, and dis-
cusses future work.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999987986486487">
In (Marcu et al., 2006), it is demonstrated that
‘syntactified’ target language phrases can im-
prove translation quality for Chinese–English. A
stochastic, top-down transduction process is em-
ployed that assigns a joint probability to a source
sentence and each of its alternative syntactified
translations; this is done by specifying a rewrit-
ing process of the target parse-tree into a source
sentence.
Likewise, the model in (Zollmann and Venu-
gopal, 2006) extends (Chiang, 2005) by augment-
ing the hierarchical phrases with syntactic cate-
gories derived from parsing the target side of a
parallel corpus. They use an existing parser to
parse the target side of the parallel corpus in or-
der to extract a syntactically motivated, bilingual
synchronous grammar as in (Chiang, 2005).
The above-mentioned approaches for incor-
porating syntax into Phrase-based SMT (Marcu
et al., 2006; Zollmann and Venugopal, 2006)
share common drawbacks. Firstly, they are
based on syntactic phrase-structure parse trees
incorporated into a Synchronous CFG or Tree-
Substitution Grammar, which makes for a diffi-
cult match with non-constituent phrases that are
common within Phrase-based SMT. These ap-
proaches usually resort to ad hoc solutions to
enrich the non-constituent phrases with syntactic
structures. Secondly, they deploy chart-based de-
coders with a high computational cost compared
with the phrase-based beam search decoders, e.g.,
(Tillmann and Ney, 2003; Koehn, 2004a). Thirdly,
due to the large parse space, some of the pro-
posed approaches are forced to employ small lan-
guage models compared to what is usually used
in phrase-based systems. To circumvent these
computational limitations, various pruning tech-
niques are usually needed, e.g., (Huang and Chi-
ang, 2007).
Other recent approaches, e.g., (Birch et al.,
2007; Hassan et al., 2007; Hassan et al., 2008a)
incorporate a linear-time supertagger into SMT to
take the role of a syntactic language model along-
side the standard language model. While these ap-
proaches share with our work the use of lexical-
ized grammars, they never seek to build a full de-
pendency tree or employ syntactic features in or-
der to directly influence the reordering probabili-
ties in the decoder. In the current work, we ex-
pand our previous work in (Hassan et al., 2007;
Hassan et al., 2008a) to introduce the capabilities
of building a full dependency structure and em-
ploying syntactic features to influence the decod-
ing process.
Recently, (Shen et al., 2008) introduced an ap-
proach for incorporating a dependency-based lan-
guage model into SMT. They proposed to extract
String-to-Dependency trees from the parallel cor-
pus. As the dependency trees are not constituents
by nature, they handle non-constituent phrases as
well. While this work is in the same general
direction as our work, namely aiming at incor-
porating dependency parsing into SMT, there re-
main three major differences. Firstly, (Shen et al.,
2008) resorted to heuristics to extract the String-
to-Dependency trees, whereas our approach em-
ploys the well formalized CCG grammatical the-
ory. Secondly, their decoder works bottom-up
and uses a chart parser with a limited language
model capability (3-grams), while we build on the
efficient, linear-time decoder commonly used in
phrase-based SMT. Thirdly, (Shen et al., 2008)
deploys the dependency language model to aug-
ment the lexical language model probability be-
</bodyText>
<page confidence="0.97886">
1183
</page>
<bodyText confidence="0.9999193">
tween two head words but never seek a full de-
pendency graph. In contrast, our approach inte-
grates an incremental parsing capability, that pro-
duces the partial dependency structures incremen-
tally while decoding, and thus provides for better
guidance for the search of the decoder for more
grammatical output. To the best of our knowledge,
our approach is the first to incorporate incremental
dependency parsing capabilities into SMT while
maintaining the linear-time and -space decoder.
</bodyText>
<sectionHeader confidence="0.993609" genericHeader="method">
3 Baseline: Direct Translation Model 2
</sectionHeader>
<bodyText confidence="0.9990014">
The Direct Translation Model (DTM) (Papineni
et al., 1997) employs the a posteriori conditional
distribution P(T|S) of a target sentence T given
a source sentence S, instead of the common in-
version into P (S|T) based on the source chan-
nel approach (Brown et al., 1990). DTM2, in-
troduced in (Ittycheriah and Roukos, 2007), ex-
presses the phrase-based translation task in a uni-
fied log-linear probabilistic framework consisting
of three components: (i) a prior conditional dis-
tribution P0(.|S), (ii) a number of feature func-
tions Oi() that capture the translation and language
model effects, and (iii) the weights of the features
Ai that are estimated under MaxEnt (Berger et al.,
1996), as in (1):
</bodyText>
<equation confidence="0.9967565">
P0(T, J|S) � AiOi(T, J, S) (1)
P(T|S) = exp
Z
i
</equation>
<bodyText confidence="0.997365941176471">
Here J is the skip reordering factor for the phrase
pair captured by Oi() and represents the jump from
the previous source word, and Z is the per source
sentence normalization term. The prior probabil-
ity P0 is the prior distribution for the phrase prob-
ability which is estimated using the phrase nor-
malized counts commonly used in conventional
Phrase–based SMT systems, e.g., (Koehn et al.,
2003).
DTM2 differs from other Phrase–based SMT
models in that it extracts from a word-aligned par-
allel corpus only a non-redundant set of minimal
phrases in the sense that no two phrases overlap
with each other.
Baseline DTM2 Features: The baseline em-
ploys the following five types of features (beside
the language model):
</bodyText>
<listItem confidence="0.944952545454545">
• Lexical Micro Features examining source
and target words of the phrases,
• Lexical Context Features encoding the
source and target phrase context (i.e. previ-
ous and next source and previous target),
• Source Morphological Features encoding
morphological and segmentation characteris-
tics of source words.
• Part-of-Speech Features encoding source and
target POS tags as well as the POS tags of the
surrounding contexts of phrases.
</listItem>
<bodyText confidence="0.993465333333333">
The DTM2 approach based on MaxEnt provides
a flexible framework for incorporating other avail-
able feature types as we demonstrate below.
DTM2 Decoder: The decoder for the baseline is
a beam search decoder similar to decoders used in
standard phrase-based log-linear systems such as
(Tillmann and Ney, 2003) and (Koehn, 2004a).
The main difference between the DTM2 decoder
and the standard Phrase–based SMT decoders is
that DTM2 deploys Maximum Entropy probabilis-
tic models to obtain the translation costs and var-
ious feature costs by deploying the features de-
scribed above in a discriminative MaxEnt fashion.
In the rest of this paper we adopt the DTM2 for-
malization of translation as a discriminative task,
and we describe the CCG-based incremental de-
pendency parser that we use for extending the
DTM2 decoder, and then list a new set of syntac-
tic dependency feature functions that extend the
DTM2 feature set. We also discuss pruning and
other details of the approach.
</bodyText>
<sectionHeader confidence="0.991224" genericHeader="method">
4 The Incremental Dependency Parser
</sectionHeader>
<bodyText confidence="0.9997536">
As it processes an input sentence left-to-right
word-by-word, the incremental dependency model
builds—for each prefix of the input sentence—a
partial parse that is a subgraph of the partial parse
that it builds for a longer prefix. The dependency
graph is constructed incrementally, in that the sub-
graph constructed at a preceding step is never al-
tered or revised in any later steps. The following
schematic view in (2) exhibits the general work-
ings of this parser:
</bodyText>
<equation confidence="0.963113666666667">
o1
S0 w1 stl S1
w2,s
</equation>
<bodyText confidence="0.993359">
The syntactic process is represented by a sequence
of transitions between adjacent syntactic states Si.
</bodyText>
<equation confidence="0.9805555">
o2 -S
i
o
t&gt;
2 Si
wi,
��Si+1 Sn (2)
sti
</equation>
<page confidence="0.940078">
1184
</page>
<bodyText confidence="0.999935294117647">
A transition from state Si−1 to Si scans the cur-
rent word wi and stochastically selects a com-
plex lexical descriptor/category sti and an oper-
ator oi given the local context in the transition se-
quence. The syntactic state Si summarizes all the
syntactic information about fragments that have
already been processed and registers the syntac-
tic arguments which are to be expected next. Only
an impoverished deterministic procedure (called a
‘State-Realizer’) is needed in order to compose a
state Si with the previous states S0 ... Si−1 in or-
der to obtain afully connected intermediate depen-
dency structure at every position in the input.
To implement the incremental parsing scheme
described above we use the parser described in
(Hassan et al., 2008b; Hassan et al., 2009), which
is based on Combinatory Categorial Grammar
(CCG) (Steedman, 2000). We only briefly de-
scribe this parser as its full description is beyond
the scope of this paper. The notions of a supertag
as a lexical category and the process of supertag-
ging are both crucial here (Bangalore and Joshi,
1999). Fortunately, CCG specifies the desired kind
of lexical categories (supertags) sti for every word
and a small set of combinatory operators oi that
combine the supertag sti with a previous parse
state Si−1 into the next parse state Si. In terms
of CCG representations, the parse state is a CCG
composite category which specifies either a func-
tor and the arguments it expects to the right of the
current word, or is itself an argument for a functor
that will follow it to the right. At the first word in
the sentence, the parse state consists solely of the
supertag of that word.
</bodyText>
<figure confidence="0.469765166666667">
Attacks rocked Riyadh
S0 NP (S\NP)/NP NP
&gt; NOP
S1: NP
&gt; TRFC
S2: S/NP
</figure>
<figureCaption confidence="0.5004959">
Figure 1: A sentence and possible supertag-,
operator- and state-sequences. NOP: No Oper-
ation; TRFC: Type Raise-Forward Composition;
FA: Forward Application. The CCG operators
used show that Attacks and Riyadh are both
dependents of rocked.
Figure 1 exhibits an example of the workings of
this parser. Practically speaking, after POS tag-
ging the input sentence, the parser employs two
components:
</figureCaption>
<listItem confidence="0.8478535">
• A Supertag-Operator Tagger which proposes
a supertag–operator pair for the current word,
• A deterministic State-Realizer, which real-
izes the current state by applying the current
operator to the previous state and the current
supertag.
</listItem>
<bodyText confidence="0.998861846153846">
The Supertag-Operator Tagger is a probablistic
component while the State-Realizer is a determin-
istic component. The generative model underlying
this component concerns the probability P(W, S)
of a word sequence W = wn1 and a parse-state
sequence S = Sn1 , with associated supertag se-
quence ST = stn1 and operator sequence O = on1,
which represents a possible derivation. Note that
given the choice of supertags sti and operator oi,
the state Si is calculated deterministically by the
State-Realizer.
A generative version of this model is described
in (3):
</bodyText>
<equation confidence="0.85979675">
Word Predictor
z } |{
P(wi|Wi−1Si−1)
In (3):
</equation>
<listItem confidence="0.9951143125">
• P(W, S) represents the product of the pro-
duction probabilities at each parse-state and
is similar to the structured language model
representation introduced in (Chelba, 2000).
• P(wi|Wi−1Si−1) is the probability of wi
given the previous sequence of words Wi−1
and the previous sequence of states Si−1,
• P(sti|Wi): is the supertag sti probability
given the word sequence Wi up to the cur-
rent position. Basically, this represents a se-
quence tagger (a ‘supertagger’).
• P(oi|Wi, Si−1, STi) represents the probabil-
ity of the operator oi given the previous
words, supertags and state sequences up to
the current position. This represents a CCG
operator tagger.
</listItem>
<bodyText confidence="0.9106252">
The different local conditional components (for
every i) in (3) are estimated as discriminative
MaxEnt submodels trained on a corpus of incre-
mental CCG derivations. This corpus was ex-
tracted from the CCGbank (Hockenmaier, 2003)
</bodyText>
<equation confidence="0.886998285714286">
&gt; FA
S3: S
P(W, S) = Yn
i=1
Supertagger Operator Tagger
z}|{ z } |{
.P(sti|Wi) . P(oi|Wi, Si−1, STi) (3)
</equation>
<page confidence="0.932877">
1185
</page>
<bodyText confidence="0.99995692">
by transforming every normal form derivation into
strictly left-to-right CCG derivations, with the
CCG operators only slightly redesigned to allow
incrementality while still satisfying the dependen-
cies in the CCGbank (cf. (Hassan et al., 2008b;
Hassan et al., 2009)).
As mentioned before, the State-Realizer is a
deterministic function. Starting at the first word
with (obviously) a null previous state, the realizer
performs the following deterministic steps for
each word in turn: (i) set the current supertag
and operator to those of the current word; (ii) at
the current state, apply the current operator to the
previous state and current supertag; (iii) add edges
to the dependency graphs between words that are
linked as CCG arguments; and (iv) if not at the
end of the sentence, set the previous state to the
current one, then set the current word to the next
one, and iterate from (i).
It is worth noting that the proposed dependency
parser is deterministic in the sense that it maintains
only one parse state per word. This characteris-
tic is crucial for its incorporation into a large-scale
SMT system to avoid explosion of the translation
space during decoding.
</bodyText>
<sectionHeader confidence="0.994052" genericHeader="method">
5 Dependency-based DTM (DDTM)
</sectionHeader>
<bodyText confidence="0.999894090909091">
In this section we extend the DTM2 model with
incremental target dependency-based syntax. We
call the resulting model the Dependency-based Di-
rect Translation Model (DDTM). This extension
takes place by (i) extracting syntactically enriched
minimal phrase pairs, (ii) including a new set of
syntactic feature functions among the exponen-
tial model features, and (iii) adapting the decoder
for dealing with syntax, including various pruning
strategies and enhancements. Next we describe
each extension in turn.
</bodyText>
<subsectionHeader confidence="0.989742">
5.1 Phrase Table: Incremental Syntax
</subsectionHeader>
<bodyText confidence="0.985846323529412">
The target-side sentences in the word-aligned par-
allel corpus used for training are parsed using
the incremental dependency parser described in
section 4. This results in a word-aligned par-
allel corpus where the words of the target sen-
tences are tagged with supertags and operators.
From this corpus we extract the set of minimal
phrase pairs using the method described in (Itty-
cheriah and Roukos, 2007), extracting along with
every target phrase the associated sequences of su-
pertags and operators. As shown in (4), a source
phrase s1, ... , sn translates into a target phrase
w1, ... , wm where every word wi is labeled with
a supertag sti, and a possible parsing operator oi
appearing with it in the parsed parallel corpus:
s1...sn &gt;[w1, st1, o1]...[wm, stm, om] (4)
Hence, our phrase table associates with every
target phrase an incremental parsing subgraph.
These subgraphs along with their probabilities
represent our phrase table augmented with incre-
mental dependency parsing structure.
This representation turns the complicated prob-
lem of MT with incremental parsing into a sequen-
tial classification problem in which the classifier
deploys various features from the source sentence
and the candidate target translations to specify a
sequence of decisions that finally results in an out-
put target string along with its associated depen-
dency graph. The classification decisions are per-
formed in sequence step-by-step while traversing
the input string to provide decisions on possible
words, supertags, operators and states. A beam
search decoder simultaneously decides which se-
quence is the most probable.
</bodyText>
<subsectionHeader confidence="0.978647">
5.2 DDTM Features
</subsectionHeader>
<bodyText confidence="0.9826111">
The exponential model and the MaxEnt frame-
work used in DTM2 and DDTM enabled us to ex-
plore the utility of incremental syntactic parsing
within a rich feature space. In our DDTM sys-
tem, we implemented a set of features alongside
the baseline DTM2 features that were discussed in
Section 3. The features described here encode all
the probabilistic components in (3) within a log
linear interpretation along with some more empir-
ically intuitive features.
</bodyText>
<listItem confidence="0.998374769230769">
• Supertag-Word features: these features ex-
amine the target phrase words with their as-
sociated supertags and is related to the Su-
pertagger component in (3).
• Supertag sequence features: these features
encode n-gram supertags (equivalent to the n-
gram supertags Language Model). This fea-
ture is related to the supertagger component
as well.
• Supertag-Operator features: these features
encode supertags and associated operators
which is related to the Operator Tagger com-
ponent in (3).
</listItem>
<page confidence="0.753279">
1186
</page>
<listItem confidence="0.955557272727273">
• Supertag-State features: these features regis-
ter state and supertag co-occurrences.
• State sequence features: these features en-
code n-gram state features and are equiva-
lent to an n-gram Language Model over parse
state sequences which is related to the multi-
plication in (3).
• Word-State sequence features: these fea-
tures encode words and states co-occurrences
which is related to the Word Predictor com-
ponent in (3).
</listItem>
<bodyText confidence="0.9997996">
The exponential model and the MaxEnt frame-
work used in DTM2 and DDTM enable us to ex-
plore the utility of incremental syntactic parsing
with the use of minimal phrases within a rich fea-
ture space.
</bodyText>
<subsectionHeader confidence="0.976606">
5.3 DDTM Decoder
</subsectionHeader>
<bodyText confidence="0.999936814814815">
In order to support incremental dependency pars-
ing, we extend the DTM2 decoder in three ways:
firstly, by constructing the syntactic states during
decoding; secondly, by extending the hypothesis
structures to incorporate the syntactic states and
the partial dependency derivations; and thirdly, by
modifying the pruning strategy to handle the large
search space.
At decoding time, each hypothesis state is as-
sociated with a parse-state which is constructed
while decoding using the incremental parsing ap-
proach introduced in ((Hassan et al., 2008b; Has-
san et al., 2009)). The previous state, the se-
quences of supertags and CCG incremental opera-
tors are deployed in a deterministic manner to re-
alize the parse-states as well as the intermediate
dependency graphs between words.
Figure 2 shows the DDTM decoder while de-
coding a sentence with the English translation “At-
tacks rocked Riyadh”. Each hypothesis is asso-
ciated with a parse-state Si and a partial depen-
dency graph (shown for some states only). More-
over, each transition is associated with an opera-
tor o that combines the previous state and the cur-
rent supertag st to construct the next state Si. The
decoder starts from a null state S1 and then pro-
ceeds with a possible expansion with the word “at-
tacks”, supertag NP and operator NOP to pro-
duce the next hypothesis with state S2 and cate-
gory NP. Further expansion for that path with the
verb “rocked”, supertag ‘(S\NP)/NP and oper-
ator TRFC will produce the state S5 with cat-
egory S/NP. The partial dependency graph for
state S5 is shown above the state where a depen-
dency relation between the two words is estab-
lished. Furthermore, another expansion with the
word “Riyadh”, supertag NP and operator FA
produces state S7 with category S and a completed
dependency graph as shown above the state. An-
other path which spans the states S1, S3 , S6 and
S8 ends with a state category S/NP and a partial
dependency graph as shown under state S8 where
the dependency graph is still missing its object
(e.g. “Riyadh attacks rocked the Saudi Govt.”).
The addition of parse-states may result in a very
large search space due to the fact that the same
phrase/word may have many possible supertags
and many possible operators. Moreover, the same
word sequences may have many parse-state se-
quences and, therefore, many hypotheses that rep-
resent the same word sequence. The search space
is definitely larger than the baseline search space.
We adopt the following three pruning heuristics to
limit the search space.
</bodyText>
<subsubsectionHeader confidence="0.668903">
5.3.1 Grammatical Pruning
</subsubsectionHeader>
<bodyText confidence="0.999969764705882">
Any hypothesis which does not constitute a valid
parse-state is discarded, i.e. if the previous parse-
state and the current supertag sequence cannot
construct a valid state using the associated oper-
ator sequence, then the expansion is discarded.
Therefore, this pruning strategy maintains only
fully connected graphs and discards any partially
connected graphs that might result during the de-
coding process.
As shown in Figure 2, the expansion from state
S1 to state S4 (with the dotted line) is pruned and
not expanded further because the proposed expan-
sion is the verb “attacks”, supertag (S\NP)/NP
and operator TRFC. Since the previous state is
NULL, it cannot be combined with the verb using
the TRFC operator. This would produce an un-
defined state and thus the hypothesis is discarded.
</bodyText>
<subsubsectionHeader confidence="0.554863">
5.3.2 Supertags and Operators Threshold
</subsubsectionHeader>
<bodyText confidence="0.999977333333333">
We limit the supertag and operator variants per tar-
get phrase to a predefined number of alternatives.
We tuned this on the MT03 DevSet for the best
accuracy while maintaining a manageable search
space. The supertags limit was set to four alterna-
tives while the operators limit was set to three.
As shown in Figure 2, each word can have many
alternatives with different supertags. In this exam-
ple the word “attacks” has two forms, namely a
</bodyText>
<page confidence="0.996918">
1187
</page>
<figureCaption confidence="0.998025">
Figure 2: DDTM Decoder: each hypothesis has a parse state and a partial dependency structure.
</figureCaption>
<figure confidence="0.99803464">
O:NOP
e: Riyadh
a: -*------
P:=.142
ST=NP/NP
S3=NP/NP
O:TRFC
e: attacks
a: *-------
P:=.092
ST=(S\NP)/NP
S4= UNDEF
attacks rocked
attacks rocked Riyadh
attacks
e: attacks
a: *----
P:=.162
ST=NP
S2=NP
O:NOP
e: rocked
a: --*--
P:=.083
ST=(S\NP)/NP
S5=S/NP
O:TRFC
e:
a: --------
P:1
S1:NULL
Riyadh attacks rocked
O:TRFC
e: rocked
a: --*------
P:=.01
ST=(S\NP)/NP
S8=S/NP
O:FC
e: attacks
a: *-------
P:=.07
ST=NP
S6=NP
O:FA
e: Riyadh
a: --*--
P:=.04
ST=NP
S7=S
</figure>
<bodyText confidence="0.752588">
noun and a verb, with different supertags and op-
erators. The proposed thresholds limit the possible
alternatives to a reasonable number.
</bodyText>
<subsectionHeader confidence="0.941921">
5.3.3 Merging Hypotheses
</subsectionHeader>
<bodyText confidence="0.9997904">
Standard Phrase–based SMT decoders merge
translation hypotheses if they cover the same
source words and share the same n-gram lan-
guage model history. Similarly, DDTM decoder
merges translation hypotheses if they cover the
same source words, share the same n-gram lan-
guage model history and share the same parse-
state history. This helps in reducing the search
space by merging paths that will not constitute a
part of the best path.
</bodyText>
<sectionHeader confidence="0.998807" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999962892857143">
We conducted experiments on an Arabic-to-
English translation task using LDC parallel data
and GALE parallel data. We used the UN paral-
lel corpus and LDC news corpus together with the
GALE parallel corpus, totaling 7.8M parallel sen-
tences. The 5-gram Language Model was trained
on the English Gigaword Corpus and the English
part of the parallel corpus. Our baseline system is
similar to the system described in (Ittycheriah and
Roukos, 2007). We report results on NIST MT05
and NIST MT06 evaluations test sets using BLEU
and TER as automatic evaluation metrics.
To train the DDTM model, we use the incre-
mental parser introduced in (Hassan et al., 2008b;
Hassan et al., 2009) to parse the target side of the
parallel training data. Each sentence is associated
with supertag, operator and parse-state sequences.
We then train models with different feature sets.
Results: We compared the baseline DTM2 (It-
tycheriah and Roukos, 2007) with our DDTM sys-
tem with the features listed above. We examine
the effect of all features on system performance.
In this set of experiments we used LDC parallel
data only which is composed of 3.7M sentences
and the results are reported on MT05 test set. Each
of the examined systems deploys DTM2 features
in addition to a number of newly added syntactic
features. The systems examined are:
</bodyText>
<listItem confidence="0.998851285714286">
• DTM2: Direct Translation model 2 baseline.
• D-SW: DTM2 + Supertag-Word features.
• D-SLM: DTM2 + Supertag-Word and su-
pertag n-gram features.
• D-SO: DTM2+ Supertag-Operator features.
• D-SS : DTM2 + supertags and states features
with parse-state construction.
• D-WS : DTM2 + words and states features
with parse-state construction.
• D-STLM: DTM2 + state n-gram features
with parse-state construction.
• DDTM: fully fledged system with all fea-
tures that proved useful above which are:
Supertag-Word features, supertag n-gram
</listItem>
<page confidence="0.992418">
1188
</page>
<bodyText confidence="0.8256835">
features, supertags and states features and
state n-gram features .
</bodyText>
<table confidence="0.999170666666667">
System BLEU Score on MT05
DTM2-Baseline 52.24
D-SW 52.28
D-SLM 52.29
D-SO 52.01
D-SS 52.39
D-WS 52.03
D-STLM 52.53
DDTM 52.61
</table>
<tableCaption confidence="0.999726">
Table 1: DDTM Results with various features.
</tableCaption>
<bodyText confidence="0.99983075">
As shown in Table 1, the DTM baseline system
demonstrates a very high BLEU score, unsurpris-
ingly given its top-ranked performance in two re-
cent major MT evaluation campaigns. Among the
features we tried, supertags and n-gram supertags
systems (D-SW and D-SLM systems) give slight
yet statistically insignificant improvements. On
the other hand, the states n-gram sequence features
(D-SS and DDTM systems) give small yet statis-
tically significant improvements (as calculated via
bootstrap resampling (Koehn, 2004b)). The D-WS
system shows a small degradation in performance,
probably due to the fact that the states-words inter-
actions are quite sparse and could not be estimated
with good evidence. Similarly, the D-SO system
shows a small degradation in performance. When
we investigated the features types, we found out
that all features that deploy the operators had bad
effect on the model. We think this is due to the fact
that the operator set is a small set with high evi-
dence in many training instances such that it has
low discriminative power on it is own. However,
it implicitly helps in producing the state sequence
which proved useful.
</bodyText>
<table confidence="0.9989736">
System DTM2-Baseline DDTM
MT05 (BLEU) 55.28 55.66
MT05 (TER) 38.79 38.48
MT06 (BLEU) 43.56 43.91
MT06 (TER) 49.08 48.65
</table>
<tableCaption confidence="0.998091">
Table 2: DDTM Results on MT05 and MT06.
</tableCaption>
<bodyText confidence="0.999987833333333">
We examined a combination of the best fea-
tures in our DDTM system on a larger training
data comprising 7.8M sentences from both NIST
and GALE parallel corpora. Table 2 shows the
results on both MT05 and MT06 test sets. As
shown, DDTM significantly outperforms the state-
of-the-art baseline system. It is worth noting that
DDTM outperforms this baseline even when very
large amounts of training data are used. Despite
the fact that the actual scores are not so different,
we found that the baseline translation output and
the DDTM translation outout are significantly dif-
ferent. We measured this by calculating the TER
between the baseline translation and the DDTM
translation for the MT05 test set, and found this
to be 25.9%. This large difference has not been
realized by the BLEU or TER scores in compari-
son to the baseline. We believe that this is due to
the fact that most changes that match the syntac-
tic constraints do not bring about the best match
where the automatic evaluation metrics are con-
cerned. Accordingly, in the next section we de-
scribe the outcome of a detailed manual analysis
of the output translations.
</bodyText>
<sectionHeader confidence="0.98856" genericHeader="method">
7 Manual Analysis of Results
</sectionHeader>
<bodyText confidence="0.999981586206897">
Although the BLEU score does not mark a large
improvement by the dependency-based system
over the baseline system, human inspection of the
data gives us important insights into the pros and
cons of the dependency-based model. We ana-
lyzed a randomly selected set of 100 sentences
from the MT05 test set. In this sample, the base-
line and the DDTM system perform similarly in
68% of the sentences. The outputs of both system
are similar though not identical. In these cases,
the systems may choose equivalent paraphrases.
However, the translations using syntactic struc-
tures are rather similar. It is worth noting that the
DDTM system tends to produce more concise sys-
ntactic structures which may lead to less BLUE
score due to penalizing the translation length al-
though the translation might be equivelent to the
baseline if not better.
In 28% of the sentences, the DDTM system pro-
duces remarkably better translations. The exam-
ples here illustrate the behaviour of the baseline
and the DDTM systems which can be observed
consistently throughout the test set. We only high-
light some of the examples for illustration pur-
poses. DDTM manages to insert verbs which are
deleted by any standard phrase-based SMT sys-
tem. DDTM prefers to deploy verbs since they
have complex and more detailed syntactic struc-
tures which give better and more likely state se-
</bodyText>
<page confidence="0.986204">
1189
</page>
<bodyText confidence="0.99991315">
quences. Furthermore, the DDTM system avoids
longer noun phrases and instead uses some prepo-
sitions in-between. Again, this is probably due to
the fact that like verbs, prepositions have a com-
plex syntactic description that give rise to more
likely state sequences.
On the other hand, the baseline produced better
translation in 8% of the analysis sample. We ob-
served that the baseline is doing better mainly in
two cases. The first when the produced translation
is very poor and producing poor sysntatctic struc-
ture due to out of vocabularies or hard to trans-
late sentences. The second case is with sentences
with long noun phrases, in such cases the DDTM
system prefres to introduce verbs or prepositions
in the middle of long noun phrase and thus the
baseline would produce better translations. This
is maybe due to the fact that noun phrases have
relatively simple structure in CCG such that it did
not help in constructing long noun phrases.
</bodyText>
<figure confidence="0.951608166666666">
Source:G£Qå�„Ë1ZAJ.£1YglA�apl�HA“ñj�®Ë1/2Ë�XYªK.©�’�ð
Reference: He then underwent medical examinations by a po-
lice doctor.
Baseline: He was subjected after that tests conducted by doc-
tors of the police .
DDTM: Then he underwent tests conducted by doctors of the
police.
á~
Source:L:rr�7PAJ�‚jJAÓñj.ëXñJ�Ë1ZA‚ÓLAK�QË1JëYsð
�J�j�
j ®Ó
�
Reference: Riyadh was rocked tonight by two car bomb at-
tacks..
Baseline: Riyadh rocked today night attacks by two booby -
trapped cars.
DDTM: Attacks rocked Riyadh today evening in two car
bombs.
</figure>
<figureCaption confidence="0.960246">
Figure 3: DDTM provides better syntactic struc-
ture with more concise translations.
</figureCaption>
<bodyText confidence="0.9809839">
Figure 3 shows two examples where DDTM
provides better and more concise syntactic struc-
ture. As we can see, there is not much agree-
ment between the reference and the proposed
translation. However, longer translations enhance
the possibility of picking more common n-gram
matches via the BLEU score and so increases the
chance of better scores. This well-known bias
does not favour the more concise output derived
by our DDTM system, of course.
</bodyText>
<sectionHeader confidence="0.986474" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999955742857143">
In this paper, we presented a novel model of de-
pendency phrase-based SMT which integrates in-
cremental dependency parsing into the transla-
tion model while retaining the linear decoding as-
sumed in conventional Phrase–based SMT sys-
tems. To the best of our knowledge, this model
constitutes the first effective attempt at integrating
a linear-time dependency parser that builds a con-
nected tree incrementally into SMT systems with
linear-time decoding. Crucially, it turns out that
incremental dependency parsing based on lexical-
ized grammars such as CCG and LTAG can pro-
vide valuable incremental parsing information to
the decoder even if their output is imperfect. We
believe this robustness in the face of imperfect
parser output to be a property of the probabilistic
formulation and statistical estimation used in the
Direct Translation Model. A noteworthy aspect of
our proposed approach is that it integrates features
from the derivation process as well as the derived
tree. We think that this is possible due to the im-
portance of the notion of a derivation in linguistic
frameworks such as CCG and LTAG.
Future work will attempt further extensions of
our DDTM system to allow for the exploitation
of long-range aspects of the dependency struc-
ture. We will work on expanding the features
set of DDTM system to leverage features from
the constructed dependency structure itself. Fi-
nally, we will work on enabling the deployment
of source side dependency structures to influence
the construction of the target dependency structure
based on a bilingually enabled dependency pars-
ing mechanism using the discriminative modeling
capabilities.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999878888888889">
We would like to thank Salim Roukos, IBM TJ
Watson Research Center, for fruitful, insightful
discussions and for his support during this work.
We also would like to thank Abe Ittycheriah, IBM
TJ Watson Research Center, for providing the
DTM2 baseline and for his support during de-
veloping this system. Finally, we would like to
thank the anonymous reviewers for their helpful
and constructive comments.
</bodyText>
<page confidence="0.988299">
1190
</page>
<sectionHeader confidence="0.995858" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889376146789">
Bangalore, S. and Joshi, A. (1999). “Supertagging: An
Approach to Almost Parsing”, Computational Lin-
guistics 25(2):237–265, 1999.
Berger, A. and Della Pietra, S. and Della Pietra, V.J.
(1996). Maximum Entropy Approach to Natural
Language Processing Computational Linguistics,
22(1): 39–71, 1996.
Birch, A., Osborne, M. and Koehn, P. (2007). CCG Su-
pertags in Factored Statistical Machine Translation.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, ACL-07, pp.9–16, 2007.
Brown, P., Cocke,J., Della Pietra, S., Jelinek, F., Della
Pietra, V.J. Lafferty, R. Mercer and Roossin, P. “A
Statistical Approach to Machine Translation” Com-
putational Linguistics 16(2):79–85, 1990.
Chelba, C. (2000). Exploiting Syntactic Structure for
Natural Language Modeling. PhD thesis, Johns
Hopkins University, Baltimore, MD.
Chiang, D. (2005). A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL05), pp.263–270, Ann Arbor,
MI.
Hassan, H., Sima’an, K., and Way, A.. (2009). Lex-
icalized Semi-Incremental Dependency Parsing. In
Proceedings ofRANLP 2009, the International Con-
ference on Recent Advances in Natural Language
Processing, Borovets, Bulgaria (to appear).
Hassan, H., Sima’an, K., and Way, A. (2008a). Syntac-
tically Lexicalized Phrase-Based Statistical Transla-
tion. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 6(7):1260–1273.
Hassan, H., Sima’an, K., and Way, A.. (2008b). A Syn-
tactic Language Model Based on Incremental CCG
Parsing. In Proceedings IEEE Workshop on Spoken
Language Technology (SLT) 2008, Goa, India.
Hassan, H., Sima’an, K., and Way, A. (2007). Inte-
grating Supertags into Phrase-based Statistical Ma-
chine Translation. In Proceedings of the ACL-2007,
Prague, Czech Republic, pp.288–295, 2007.
Hockenmaier, J. (2003). Data and Models for Statisti-
cal Parsing with Combinatory Categorial Grammar,
Ph.D Thesis, University of Edinburgh, UK, 2003.
Huang, L. and Chiang, D. (2007). Forest Rescoring:
Faster Decoding with Integrated Language Models.
In Proceedings of the ACL-2007, Prague, Czech Re-
public, 2007.
Ittycheriah, A. and Roukos, S. (2007). Direct trans-
lation model 2. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pp.57–64,
Rochester, NY.
Koehn, P. (2004a). Pharaoh: A Beam Search De-
coder for phrase-based Statistical Machine Transla-
tion Models. Machine Translation: From Real Users
to Research. In Proceedings of 6th Conference ofthe
Association for Machine Translation in the Ameri-
cas, AMTA 2004, pp.115–124, Washington, DC.
Koehn, P. (2004b). Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pp.388–395,
Edmonton, AB, Canada.
Koehn, P. Och, F.J. and Marcu, D. (2003). Statisti-
cal phrase-based translation. In Proceedings of the
Joint Human Language Technology Conference and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2003), pp.127–133, Edmonton, AL,
Canada.
Marcu, D., Wang, W., Echihabi, A., and Knight, K.
(2006). SPMT: Statistical Machine Translation with
Syntactified Target Language Phrases. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2006),
pp.44–52, Sydney, Australia.
Papineni, K., Roukos, S., and Ward, T. (1997).
Feature-Based Language Understanding. In Pro-
ceedings of 5th European Conference on Speech
Communication and Technology EUROSPEECH
’97 , pp.1435–1438, Rhodes, Greece.
Papineni, K., Roukos, S., Ward, T. and Zhu, W-J.
(2002). BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL’02), pp.311–318, Philadelphia, PA.
Shen, L., Xu, J., and Weischedel, R. (2008). A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings of ACL-08: HLT, pp.577–585, Columbus,
OH.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L. and
Makhoul, J. (2006) A Study of Translation Edit Rate
with Targeted Human Annotation. In AMTA 2006:
Proceedings ofthe 7th Conference ofthe Association
for Machine Translation in the Americas, pp.223–
231, Cambridege, MA.
Steedman, M. (2000). The Syntactic Process. MIT
Press, Cambridge, MA.
Tillmann, C. and Ney, H. (2003). Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97–133.
Zollmann, A. and Venugopal, A. (2006). Syntax aug-
mented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL, pp.138–141, New York,
NY.
</reference>
<page confidence="0.994497">
1191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.101783">
<title confidence="0.999871">A Syntactified Direct Translation Model with Linear-time Decoding</title>
<author confidence="0.691815">Hany</author>
<affiliation confidence="0.694804">Cairo</affiliation>
<address confidence="0.50355">Cairo,</address>
<email confidence="0.999848">hanyh@eg.ibm.com</email>
<author confidence="0.919241">Khalil</author>
<affiliation confidence="0.98838">Language and University of</affiliation>
<address confidence="0.620065">Amsterdam, The</address>
<email confidence="0.963431">k.simaan@uva.nl</email>
<author confidence="0.988841">Andy</author>
<affiliation confidence="0.997">School of Dublin City</affiliation>
<address confidence="0.695862">Dublin,</address>
<email confidence="0.991629">away@computing.dcu.ie</email>
<abstract confidence="0.995507692307692">Recent syntactic extensions of statistical translation models work with a synchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus. The decoders accompanying these extensions typically exceed quadratic time complexity. This paper extends the Direct Translation Model 2 (DTM2) with syntax while maintaining linear-time decoding. We employ a linear-time parsing algorithm based on an eager, incremental interpretation of Combinatory Categorial Grammar (CCG). As every input word is processed, the local parsing decisions resolve ambiguity eagerly, by selecting a single supertag–operator pair for extending the dependency parse incrementally. Alongside translation features extracted from the derived parse tree, we explore syntactic features extracted from the incremental derivation process. Our empirical experiments show that our model significantly outperforms the state-of-the art DTM2 system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A Joshi</author>
</authors>
<title>Supertagging: An Approach to Almost Parsing”,</title>
<date>1999</date>
<journal>Computational Linguistics</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="13642" citStr="Bangalore and Joshi, 1999" startWordPosition="2130" endWordPosition="2133">-Realizer’) is needed in order to compose a state Si with the previous states S0 ... Si−1 in order to obtain afully connected intermediate dependency structure at every position in the input. To implement the incremental parsing scheme described above we use the parser described in (Hassan et al., 2008b; Hassan et al., 2009), which is based on Combinatory Categorial Grammar (CCG) (Steedman, 2000). We only briefly describe this parser as its full description is beyond the scope of this paper. The notions of a supertag as a lexical category and the process of supertagging are both crucial here (Bangalore and Joshi, 1999). Fortunately, CCG specifies the desired kind of lexical categories (supertags) sti for every word and a small set of combinatory operators oi that combine the supertag sti with a previous parse state Si−1 into the next parse state Si. In terms of CCG representations, the parse state is a CCG composite category which specifies either a functor and the arguments it expects to the right of the current word, or is itself an argument for a functor that will follow it to the right. At the first word in the sentence, the parse state consists solely of the supertag of that word. Attacks rocked Riyadh</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Bangalore, S. and Joshi, A. (1999). “Supertagging: An Approach to Almost Parsing”, Computational Linguistics 25(2):237–265, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>Della Pietra</author>
<author>S</author>
<author>Della Pietra</author>
<author>V J</author>
</authors>
<date>1996</date>
<booktitle>Maximum Entropy Approach to Natural Language Processing Computational Linguistics,</booktitle>
<volume>22</volume>
<issue>1</issue>
<pages>39--71</pages>
<contexts>
<context position="9721" citStr="Berger et al., 1996" startWordPosition="1479" endWordPosition="1482">oys the a posteriori conditional distribution P(T|S) of a target sentence T given a source sentence S, instead of the common inversion into P (S|T) based on the source channel approach (Brown et al., 1990). DTM2, introduced in (Ittycheriah and Roukos, 2007), expresses the phrase-based translation task in a unified log-linear probabilistic framework consisting of three components: (i) a prior conditional distribution P0(.|S), (ii) a number of feature functions Oi() that capture the translation and language model effects, and (iii) the weights of the features Ai that are estimated under MaxEnt (Berger et al., 1996), as in (1): P0(T, J|S) � AiOi(T, J, S) (1) P(T|S) = exp Z i Here J is the skip reordering factor for the phrase pair captured by Oi() and represents the jump from the previous source word, and Z is the per source sentence normalization term. The prior probability P0 is the prior distribution for the phrase probability which is estimated using the phrase normalized counts commonly used in conventional Phrase–based SMT systems, e.g., (Koehn et al., 2003). DTM2 differs from other Phrase–based SMT models in that it extracts from a word-aligned parallel corpus only a non-redundant set of minimal p</context>
</contexts>
<marker>Berger, Pietra, S, Pietra, J, 1996</marker>
<rawString>Berger, A. and Della Pietra, S. and Della Pietra, V.J. (1996). Maximum Entropy Approach to Natural Language Processing Computational Linguistics, 22(1): 39–71, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Birch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>CCG Supertags in Factored Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, ACL-07,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="6923" citStr="Birch et al., 2007" startWordPosition="1030" endWordPosition="1033">ally resort to ad hoc solutions to enrich the non-constituent phrases with syntactic structures. Secondly, they deploy chart-based decoders with a high computational cost compared with the phrase-based beam search decoders, e.g., (Tillmann and Ney, 2003; Koehn, 2004a). Thirdly, due to the large parse space, some of the proposed approaches are forced to employ small language models compared to what is usually used in phrase-based systems. To circumvent these computational limitations, various pruning techniques are usually needed, e.g., (Huang and Chiang, 2007). Other recent approaches, e.g., (Birch et al., 2007; Hassan et al., 2007; Hassan et al., 2008a) incorporate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lexicalized grammars, they never seek to build a full dependency tree or employ syntactic features in order to directly influence the reordering probabilities in the decoder. In the current work, we expand our previous work in (Hassan et al., 2007; Hassan et al., 2008a) to introduce the capabilities of building a full dependency structure and employing syntactic fea</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Birch, A., Osborne, M. and Koehn, P. (2007). CCG Supertags in Factored Statistical Machine Translation. In Proceedings of the Second Workshop on Statistical Machine Translation, ACL-07, pp.9–16, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>Della Pietra</author>
<author>S Jelinek</author>
<author>Della Pietra F</author>
<author>V J Lafferty</author>
<author>R Mercer</author>
<author>P Roossin</author>
</authors>
<date>1990</date>
<journal>A Statistical Approach to Machine Translation” Computational Linguistics</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="9306" citStr="Brown et al., 1990" startWordPosition="1414" endWordPosition="1417">ctures incrementally while decoding, and thus provides for better guidance for the search of the decoder for more grammatical output. To the best of our knowledge, our approach is the first to incorporate incremental dependency parsing capabilities into SMT while maintaining the linear-time and -space decoder. 3 Baseline: Direct Translation Model 2 The Direct Translation Model (DTM) (Papineni et al., 1997) employs the a posteriori conditional distribution P(T|S) of a target sentence T given a source sentence S, instead of the common inversion into P (S|T) based on the source channel approach (Brown et al., 1990). DTM2, introduced in (Ittycheriah and Roukos, 2007), expresses the phrase-based translation task in a unified log-linear probabilistic framework consisting of three components: (i) a prior conditional distribution P0(.|S), (ii) a number of feature functions Oi() that capture the translation and language model effects, and (iii) the weights of the features Ai that are estimated under MaxEnt (Berger et al., 1996), as in (1): P0(T, J|S) � AiOi(T, J, S) (1) P(T|S) = exp Z i Here J is the skip reordering factor for the phrase pair captured by Oi() and represents the jump from the previous source w</context>
</contexts>
<marker>Brown, Cocke, Pietra, Jelinek, F, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Brown, P., Cocke,J., Della Pietra, S., Jelinek, F., Della Pietra, V.J. Lafferty, R. Mercer and Roossin, P. “A Statistical Approach to Machine Translation” Computational Linguistics 16(2):79–85, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<tech>PhD thesis,</tech>
<institution>Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<contexts>
<context position="15704" citStr="Chelba, 2000" startWordPosition="2471" endWordPosition="2472">rns the probability P(W, S) of a word sequence W = wn1 and a parse-state sequence S = Sn1 , with associated supertag sequence ST = stn1 and operator sequence O = on1, which represents a possible derivation. Note that given the choice of supertags sti and operator oi, the state Si is calculated deterministically by the State-Realizer. A generative version of this model is described in (3): Word Predictor z } |{ P(wi|Wi−1Si−1) In (3): • P(W, S) represents the product of the production probabilities at each parse-state and is similar to the structured language model representation introduced in (Chelba, 2000). • P(wi|Wi−1Si−1) is the probability of wi given the previous sequence of words Wi−1 and the previous sequence of states Si−1, • P(sti|Wi): is the supertag sti probability given the word sequence Wi up to the current position. Basically, this represents a sequence tagger (a ‘supertagger’). • P(oi|Wi, Si−1, STi) represents the probability of the operator oi given the previous words, supertags and state sequences up to the current position. This represents a CCG operator tagger. The different local conditional components (for every i) in (3) are estimated as discriminative MaxEnt submodels trai</context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>Chelba, C. (2000). Exploiting Syntactic Structure for Natural Language Modeling. PhD thesis, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pp.263–270,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="5597" citStr="Chiang, 2005" startWordPosition="830" endWordPosition="831">ther detailed insight into the characteristics of the systems. Finally, Section 8 concludes, and discusses future work. 2 Related Work In (Marcu et al., 2006), it is demonstrated that ‘syntactified’ target language phrases can improve translation quality for Chinese–English. A stochastic, top-down transduction process is employed that assigns a joint probability to a source sentence and each of its alternative syntactified translations; this is done by specifying a rewriting process of the target parse-tree into a source sentence. Likewise, the model in (Zollmann and Venugopal, 2006) extends (Chiang, 2005) by augmenting the hierarchical phrases with syntactic categories derived from parsing the target side of a parallel corpus. They use an existing parser to parse the target side of the parallel corpus in order to extract a syntactically motivated, bilingual synchronous grammar as in (Chiang, 2005). The above-mentioned approaches for incorporating syntax into Phrase-based SMT (Marcu et al., 2006; Zollmann and Venugopal, 2006) share common drawbacks. Firstly, they are based on syntactic phrase-structure parse trees incorporated into a Synchronous CFG or TreeSubstitution Grammar, which makes for </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, D. (2005). A Hierarchical Phrase-Based Model for Statistical Machine Translation. In 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pp.263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>K Sima’an</author>
<author>A Way</author>
</authors>
<title>Lexicalized Semi-Incremental Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings ofRANLP 2009, the International Conference on Recent Advances in Natural Language Processing,</booktitle>
<location>Borovets, Bulgaria</location>
<note>(to appear).</note>
<marker>Hassan, Sima’an, Way, 2009</marker>
<rawString>Hassan, H., Sima’an, K., and Way, A.. (2009). Lexicalized Semi-Incremental Dependency Parsing. In Proceedings ofRANLP 2009, the International Conference on Recent Advances in Natural Language Processing, Borovets, Bulgaria (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>K Sima’an</author>
<author>A Way</author>
</authors>
<title>Syntactically Lexicalized Phrase-Based Statistical Translation.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>6</volume>
<issue>7</issue>
<marker>Hassan, Sima’an, Way, 2008</marker>
<rawString>Hassan, H., Sima’an, K., and Way, A. (2008a). Syntactically Lexicalized Phrase-Based Statistical Translation. IEEE Transactions on Audio, Speech and Language Processing, 6(7):1260–1273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>K Sima’an</author>
<author>A Way</author>
</authors>
<title>A Syntactic Language Model Based on Incremental CCG Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings IEEE Workshop on Spoken Language Technology (SLT)</booktitle>
<location>Goa, India.</location>
<marker>Hassan, Sima’an, Way, 2008</marker>
<rawString>Hassan, H., Sima’an, K., and Way, A.. (2008b). A Syntactic Language Model Based on Incremental CCG Parsing. In Proceedings IEEE Workshop on Spoken Language Technology (SLT) 2008, Goa, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>K Sima’an</author>
<author>A Way</author>
</authors>
<title>Integrating Supertags into Phrase-based Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-2007,</booktitle>
<location>Prague, Czech Republic, pp.288–295,</location>
<marker>Hassan, Sima’an, Way, 2007</marker>
<rawString>Hassan, H., Sima’an, K., and Way, A. (2007). Integrating Supertags into Phrase-based Statistical Machine Translation. In Proceedings of the ACL-2007, Prague, Czech Republic, pp.288–295, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar, Ph.D Thesis,</title>
<date>2003</date>
<location>University of Edinburgh, UK,</location>
<contexts>
<context position="16414" citStr="Hockenmaier, 2003" startWordPosition="2585" endWordPosition="2586">e previous sequence of states Si−1, • P(sti|Wi): is the supertag sti probability given the word sequence Wi up to the current position. Basically, this represents a sequence tagger (a ‘supertagger’). • P(oi|Wi, Si−1, STi) represents the probability of the operator oi given the previous words, supertags and state sequences up to the current position. This represents a CCG operator tagger. The different local conditional components (for every i) in (3) are estimated as discriminative MaxEnt submodels trained on a corpus of incremental CCG derivations. This corpus was extracted from the CCGbank (Hockenmaier, 2003) &gt; FA S3: S P(W, S) = Yn i=1 Supertagger Operator Tagger z}|{ z } |{ .P(sti|Wi) . P(oi|Wi, Si−1, STi) (3) 1185 by transforming every normal form derivation into strictly left-to-right CCG derivations, with the CCG operators only slightly redesigned to allow incrementality while still satisfying the dependencies in the CCGbank (cf. (Hassan et al., 2008b; Hassan et al., 2009)). As mentioned before, the State-Realizer is a deterministic function. Starting at the first word with (obviously) a null previous state, the realizer performs the following deterministic steps for each word in turn: (i) se</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Hockenmaier, J. (2003). Data and Models for Statistical Parsing with Combinatory Categorial Grammar, Ph.D Thesis, University of Edinburgh, UK, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-2007,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6871" citStr="Huang and Chiang, 2007" startWordPosition="1021" endWordPosition="1025"> are common within Phrase-based SMT. These approaches usually resort to ad hoc solutions to enrich the non-constituent phrases with syntactic structures. Secondly, they deploy chart-based decoders with a high computational cost compared with the phrase-based beam search decoders, e.g., (Tillmann and Ney, 2003; Koehn, 2004a). Thirdly, due to the large parse space, some of the proposed approaches are forced to employ small language models compared to what is usually used in phrase-based systems. To circumvent these computational limitations, various pruning techniques are usually needed, e.g., (Huang and Chiang, 2007). Other recent approaches, e.g., (Birch et al., 2007; Hassan et al., 2007; Hassan et al., 2008a) incorporate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lexicalized grammars, they never seek to build a full dependency tree or employ syntactic features in order to directly influence the reordering probabilities in the decoder. In the current work, we expand our previous work in (Hassan et al., 2007; Hassan et al., 2008a) to introduce the capabilities of building a f</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Huang, L. and Chiang, D. (2007). Forest Rescoring: Faster Decoding with Integrated Language Models. In Proceedings of the ACL-2007, Prague, Czech Republic, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>Direct translation model 2. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pp.57–64,</booktitle>
<location>Rochester, NY.</location>
<contexts>
<context position="2170" citStr="Ittycheriah and Roukos, 2007" startWordPosition="306" endWordPosition="309"> terms of time and space complexity. Most recent extensions work with a synchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus. While attractive in many ways, the decoders that are needed for these types of grammars usually have time and space complexities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse s</context>
<context position="9358" citStr="Ittycheriah and Roukos, 2007" startWordPosition="1422" endWordPosition="1425">hus provides for better guidance for the search of the decoder for more grammatical output. To the best of our knowledge, our approach is the first to incorporate incremental dependency parsing capabilities into SMT while maintaining the linear-time and -space decoder. 3 Baseline: Direct Translation Model 2 The Direct Translation Model (DTM) (Papineni et al., 1997) employs the a posteriori conditional distribution P(T|S) of a target sentence T given a source sentence S, instead of the common inversion into P (S|T) based on the source channel approach (Brown et al., 1990). DTM2, introduced in (Ittycheriah and Roukos, 2007), expresses the phrase-based translation task in a unified log-linear probabilistic framework consisting of three components: (i) a prior conditional distribution P0(.|S), (ii) a number of feature functions Oi() that capture the translation and language model effects, and (iii) the weights of the features Ai that are estimated under MaxEnt (Berger et al., 1996), as in (1): P0(T, J|S) � AiOi(T, J, S) (1) P(T|S) = exp Z i Here J is the skip reordering factor for the phrase pair captured by Oi() and represents the jump from the previous source word, and Z is the per source sentence normalization </context>
<context position="18675" citStr="Ittycheriah and Roukos, 2007" startWordPosition="2945" endWordPosition="2949">mong the exponential model features, and (iii) adapting the decoder for dealing with syntax, including various pruning strategies and enhancements. Next we describe each extension in turn. 5.1 Phrase Table: Incremental Syntax The target-side sentences in the word-aligned parallel corpus used for training are parsed using the incremental dependency parser described in section 4. This results in a word-aligned parallel corpus where the words of the target sentences are tagged with supertags and operators. From this corpus we extract the set of minimal phrase pairs using the method described in (Ittycheriah and Roukos, 2007), extracting along with every target phrase the associated sequences of supertags and operators. As shown in (4), a source phrase s1, ... , sn translates into a target phrase w1, ... , wm where every word wi is labeled with a supertag sti, and a possible parsing operator oi appearing with it in the parsed parallel corpus: s1...sn &gt;[w1, st1, o1]...[wm, stm, om] (4) Hence, our phrase table associates with every target phrase an incremental parsing subgraph. These subgraphs along with their probabilities represent our phrase table augmented with incremental dependency parsing structure. This repr</context>
<context position="26884" citStr="Ittycheriah and Roukos, 2007" startWordPosition="4284" endWordPosition="4287">-gram language model history and share the same parsestate history. This helps in reducing the search space by merging paths that will not constitute a part of the best path. 6 Experiments We conducted experiments on an Arabic-toEnglish translation task using LDC parallel data and GALE parallel data. We used the UN parallel corpus and LDC news corpus together with the GALE parallel corpus, totaling 7.8M parallel sentences. The 5-gram Language Model was trained on the English Gigaword Corpus and the English part of the parallel corpus. Our baseline system is similar to the system described in (Ittycheriah and Roukos, 2007). We report results on NIST MT05 and NIST MT06 evaluations test sets using BLEU and TER as automatic evaluation metrics. To train the DDTM model, we use the incremental parser introduced in (Hassan et al., 2008b; Hassan et al., 2009) to parse the target side of the parallel training data. Each sentence is associated with supertag, operator and parse-state sequences. We then train models with different feature sets. Results: We compared the baseline DTM2 (Ittycheriah and Roukos, 2007) with our DDTM system with the features listed above. We examine the effect of all features on system performanc</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>Ittycheriah, A. and Roukos, S. (2007). Direct translation model 2. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pp.57–64, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: A Beam Search Decoder for phrase-based Statistical Machine Translation Models. Machine Translation: From Real Users to Research.</title>
<date>2004</date>
<booktitle>In Proceedings of 6th Conference ofthe Association for Machine Translation in the Americas, AMTA 2004, pp.115–124,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="2457" citStr="Koehn, 2004" startWordPosition="350" endWordPosition="351">ities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntacticallyoriented feat</context>
<context position="6571" citStr="Koehn, 2004" startWordPosition="976" endWordPosition="977">rase-based SMT (Marcu et al., 2006; Zollmann and Venugopal, 2006) share common drawbacks. Firstly, they are based on syntactic phrase-structure parse trees incorporated into a Synchronous CFG or TreeSubstitution Grammar, which makes for a difficult match with non-constituent phrases that are common within Phrase-based SMT. These approaches usually resort to ad hoc solutions to enrich the non-constituent phrases with syntactic structures. Secondly, they deploy chart-based decoders with a high computational cost compared with the phrase-based beam search decoders, e.g., (Tillmann and Ney, 2003; Koehn, 2004a). Thirdly, due to the large parse space, some of the proposed approaches are forced to employ small language models compared to what is usually used in phrase-based systems. To circumvent these computational limitations, various pruning techniques are usually needed, e.g., (Huang and Chiang, 2007). Other recent approaches, e.g., (Birch et al., 2007; Hassan et al., 2007; Hassan et al., 2008a) incorporate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lexicalized gram</context>
<context position="11252" citStr="Koehn, 2004" startWordPosition="1732" endWordPosition="1733">ext (i.e. previous and next source and previous target), • Source Morphological Features encoding morphological and segmentation characteristics of source words. • Part-of-Speech Features encoding source and target POS tags as well as the POS tags of the surrounding contexts of phrases. The DTM2 approach based on MaxEnt provides a flexible framework for incorporating other available feature types as we demonstrate below. DTM2 Decoder: The decoder for the baseline is a beam search decoder similar to decoders used in standard phrase-based log-linear systems such as (Tillmann and Ney, 2003) and (Koehn, 2004a). The main difference between the DTM2 decoder and the standard Phrase–based SMT decoders is that DTM2 deploys Maximum Entropy probabilistic models to obtain the translation costs and various feature costs by deploying the features described above in a discriminative MaxEnt fashion. In the rest of this paper we adopt the DTM2 formalization of translation as a discriminative task, and we describe the CCG-based incremental dependency parser that we use for extending the DTM2 decoder, and then list a new set of syntactic dependency feature functions that extend the DTM2 feature set. We also dis</context>
<context position="29048" citStr="Koehn, 2004" startWordPosition="4630" endWordPosition="4631">SO 52.01 D-SS 52.39 D-WS 52.03 D-STLM 52.53 DDTM 52.61 Table 1: DDTM Results with various features. As shown in Table 1, the DTM baseline system demonstrates a very high BLEU score, unsurprisingly given its top-ranked performance in two recent major MT evaluation campaigns. Among the features we tried, supertags and n-gram supertags systems (D-SW and D-SLM systems) give slight yet statistically insignificant improvements. On the other hand, the states n-gram sequence features (D-SS and DDTM systems) give small yet statistically significant improvements (as calculated via bootstrap resampling (Koehn, 2004b)). The D-WS system shows a small degradation in performance, probably due to the fact that the states-words interactions are quite sparse and could not be estimated with good evidence. Similarly, the D-SO system shows a small degradation in performance. When we investigated the features types, we found out that all features that deploy the operators had bad effect on the model. We think this is due to the fact that the operator set is a small set with high evidence in many training instances such that it has low discriminative power on it is own. However, it implicitly helps in producing the</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Koehn, P. (2004a). Pharaoh: A Beam Search Decoder for phrase-based Statistical Machine Translation Models. Machine Translation: From Real Users to Research. In Proceedings of 6th Conference ofthe Association for Machine Translation in the Americas, AMTA 2004, pp.115–124, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.388–395,</booktitle>
<location>Edmonton, AB,</location>
<contexts>
<context position="2457" citStr="Koehn, 2004" startWordPosition="350" endWordPosition="351">ities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntacticallyoriented feat</context>
<context position="6571" citStr="Koehn, 2004" startWordPosition="976" endWordPosition="977">rase-based SMT (Marcu et al., 2006; Zollmann and Venugopal, 2006) share common drawbacks. Firstly, they are based on syntactic phrase-structure parse trees incorporated into a Synchronous CFG or TreeSubstitution Grammar, which makes for a difficult match with non-constituent phrases that are common within Phrase-based SMT. These approaches usually resort to ad hoc solutions to enrich the non-constituent phrases with syntactic structures. Secondly, they deploy chart-based decoders with a high computational cost compared with the phrase-based beam search decoders, e.g., (Tillmann and Ney, 2003; Koehn, 2004a). Thirdly, due to the large parse space, some of the proposed approaches are forced to employ small language models compared to what is usually used in phrase-based systems. To circumvent these computational limitations, various pruning techniques are usually needed, e.g., (Huang and Chiang, 2007). Other recent approaches, e.g., (Birch et al., 2007; Hassan et al., 2007; Hassan et al., 2008a) incorporate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lexicalized gram</context>
<context position="11252" citStr="Koehn, 2004" startWordPosition="1732" endWordPosition="1733">ext (i.e. previous and next source and previous target), • Source Morphological Features encoding morphological and segmentation characteristics of source words. • Part-of-Speech Features encoding source and target POS tags as well as the POS tags of the surrounding contexts of phrases. The DTM2 approach based on MaxEnt provides a flexible framework for incorporating other available feature types as we demonstrate below. DTM2 Decoder: The decoder for the baseline is a beam search decoder similar to decoders used in standard phrase-based log-linear systems such as (Tillmann and Ney, 2003) and (Koehn, 2004a). The main difference between the DTM2 decoder and the standard Phrase–based SMT decoders is that DTM2 deploys Maximum Entropy probabilistic models to obtain the translation costs and various feature costs by deploying the features described above in a discriminative MaxEnt fashion. In the rest of this paper we adopt the DTM2 formalization of translation as a discriminative task, and we describe the CCG-based incremental dependency parser that we use for extending the DTM2 decoder, and then list a new set of syntactic dependency feature functions that extend the DTM2 feature set. We also dis</context>
<context position="29048" citStr="Koehn, 2004" startWordPosition="4630" endWordPosition="4631">SO 52.01 D-SS 52.39 D-WS 52.03 D-STLM 52.53 DDTM 52.61 Table 1: DDTM Results with various features. As shown in Table 1, the DTM baseline system demonstrates a very high BLEU score, unsurprisingly given its top-ranked performance in two recent major MT evaluation campaigns. Among the features we tried, supertags and n-gram supertags systems (D-SW and D-SLM systems) give slight yet statistically insignificant improvements. On the other hand, the states n-gram sequence features (D-SS and DDTM systems) give small yet statistically significant improvements (as calculated via bootstrap resampling (Koehn, 2004b)). The D-WS system shows a small degradation in performance, probably due to the fact that the states-words interactions are quite sparse and could not be estimated with good evidence. Similarly, the D-SO system shows a small degradation in performance. When we investigated the features types, we found out that all features that deploy the operators had bad effect on the model. We think this is due to the fact that the operator set is a small set with high evidence in many training instances such that it has low discriminative power on it is own. However, it implicitly helps in producing the</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Koehn, P. (2004b). Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.388–395, Edmonton, AB, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Och Koehn</author>
<author>F J</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2003), pp.127–133,</booktitle>
<location>Edmonton, AL,</location>
<contexts>
<context position="10178" citStr="Koehn et al., 2003" startWordPosition="1561" endWordPosition="1564">tions Oi() that capture the translation and language model effects, and (iii) the weights of the features Ai that are estimated under MaxEnt (Berger et al., 1996), as in (1): P0(T, J|S) � AiOi(T, J, S) (1) P(T|S) = exp Z i Here J is the skip reordering factor for the phrase pair captured by Oi() and represents the jump from the previous source word, and Z is the per source sentence normalization term. The prior probability P0 is the prior distribution for the phrase probability which is estimated using the phrase normalized counts commonly used in conventional Phrase–based SMT systems, e.g., (Koehn et al., 2003). DTM2 differs from other Phrase–based SMT models in that it extracts from a word-aligned parallel corpus only a non-redundant set of minimal phrases in the sense that no two phrases overlap with each other. Baseline DTM2 Features: The baseline employs the following five types of features (beside the language model): • Lexical Micro Features examining source and target words of the phrases, • Lexical Context Features encoding the source and target phrase context (i.e. previous and next source and previous target), • Source Morphological Features encoding morphological and segmentation characte</context>
</contexts>
<marker>Koehn, J, Marcu, 2003</marker>
<rawString>Koehn, P. Och, F.J. and Marcu, D. (2003). Statistical phrase-based translation. In Proceedings of the Joint Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2003), pp.127–133, Edmonton, AL, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: Statistical Machine Translation with Syntactified Target Language Phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pp.44–52,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="5142" citStr="Marcu et al., 2006" startWordPosition="760" endWordPosition="763">to the quantitative results. This paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the DTM2 baseline model. Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2. Section 5 details our own DDTM, the dependencybased extension of the DTM2 model. Section 6 reports on extensive experiments and their results. Section 7 provides translation output to shed further detailed insight into the characteristics of the systems. Finally, Section 8 concludes, and discusses future work. 2 Related Work In (Marcu et al., 2006), it is demonstrated that ‘syntactified’ target language phrases can improve translation quality for Chinese–English. A stochastic, top-down transduction process is employed that assigns a joint probability to a source sentence and each of its alternative syntactified translations; this is done by specifying a rewriting process of the target parse-tree into a source sentence. Likewise, the model in (Zollmann and Venugopal, 2006) extends (Chiang, 2005) by augmenting the hierarchical phrases with syntactic categories derived from parsing the target side of a parallel corpus. They use an existing</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Marcu, D., Wang, W., Echihabi, A., and Knight, K. (2006). SPMT: Statistical Machine Translation with Syntactified Target Language Phrases. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pp.44–52, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
</authors>
<title>Feature-Based Language Understanding.</title>
<date>1997</date>
<booktitle>In Proceedings of 5th European Conference on Speech Communication and Technology EUROSPEECH ’97 , pp.1435–1438,</booktitle>
<location>Rhodes, Greece.</location>
<contexts>
<context position="9096" citStr="Papineni et al., 1997" startWordPosition="1377" endWordPosition="1380">ical language model probability be1183 tween two head words but never seek a full dependency graph. In contrast, our approach integrates an incremental parsing capability, that produces the partial dependency structures incrementally while decoding, and thus provides for better guidance for the search of the decoder for more grammatical output. To the best of our knowledge, our approach is the first to incorporate incremental dependency parsing capabilities into SMT while maintaining the linear-time and -space decoder. 3 Baseline: Direct Translation Model 2 The Direct Translation Model (DTM) (Papineni et al., 1997) employs the a posteriori conditional distribution P(T|S) of a target sentence T given a source sentence S, instead of the common inversion into P (S|T) based on the source channel approach (Brown et al., 1990). DTM2, introduced in (Ittycheriah and Roukos, 2007), expresses the phrase-based translation task in a unified log-linear probabilistic framework consisting of three components: (i) a prior conditional distribution P0(.|S), (ii) a number of feature functions Oi() that capture the translation and language model effects, and (iii) the weights of the features Ai that are estimated under Max</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>Papineni, K., Roukos, S., and Ward, T. (1997). Feature-Based Language Understanding. In Proceedings of 5th European Conference on Speech Communication and Technology EUROSPEECH ’97 , pp.1435–1438, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), pp.311–318,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4335" citStr="Papineni et al., 2002" startWordPosition="629" endWordPosition="632">shed to perform at a parsing level close to state1182 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182–1191, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP of-the-art cubic-time parsers. Nevertheless, the parsing information it provides allows for significant improvement in translation quality. We test the new model, called the Dependencybased Direct Translation Model (DDTM), on standard Arabic–English translation tasks used in the community, including LDC and GALE data. We show that our DDTM system provides significant improvements in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores over the already extremely competitive DTM2 system. We also provide results of manual, qualitative analysis of the system output to provide insight into the quantitative results. This paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the DTM2 baseline model. Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2. Section 5 details our own DDTM, the dependencybased extension of the DTM2 model. Section 6 reports on extensive experiments and their results. </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T. and Zhu, W-J. (2002). BLEU: a Method for Automatic Evaluation of Machine Translation. In 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), pp.311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>J Xu</author>
<author>R Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, pp.577–585,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="7593" citStr="Shen et al., 2008" startWordPosition="1145" endWordPosition="1148">rate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lexicalized grammars, they never seek to build a full dependency tree or employ syntactic features in order to directly influence the reordering probabilities in the decoder. In the current work, we expand our previous work in (Hassan et al., 2007; Hassan et al., 2008a) to introduce the capabilities of building a full dependency structure and employing syntactic features to influence the decoding process. Recently, (Shen et al., 2008) introduced an approach for incorporating a dependency-based language model into SMT. They proposed to extract String-to-Dependency trees from the parallel corpus. As the dependency trees are not constituents by nature, they handle non-constituent phrases as well. While this work is in the same general direction as our work, namely aiming at incorporating dependency parsing into SMT, there remain three major differences. Firstly, (Shen et al., 2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas our approach employs the well formalized CCG grammatical theory. Secondly</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Shen, L., Xu, J., and Weischedel, R. (2008). A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL-08: HLT, pp.577–585, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In AMTA 2006: Proceedings ofthe 7th Conference ofthe Association for Machine Translation in the Americas, pp.223– 231,</booktitle>
<location>Cambridege, MA.</location>
<contexts>
<context position="4365" citStr="Snover et al., 2006" startWordPosition="635" endWordPosition="638">el close to state1182 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182–1191, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP of-the-art cubic-time parsers. Nevertheless, the parsing information it provides allows for significant improvement in translation quality. We test the new model, called the Dependencybased Direct Translation Model (DDTM), on standard Arabic–English translation tasks used in the community, including LDC and GALE data. We show that our DDTM system provides significant improvements in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores over the already extremely competitive DTM2 system. We also provide results of manual, qualitative analysis of the system output to provide insight into the quantitative results. This paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the DTM2 baseline model. Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2. Section 5 details our own DDTM, the dependencybased extension of the DTM2 model. Section 6 reports on extensive experiments and their results. Section 7 provides translation</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Snover, M., Dorr, B., Schwartz, R., Micciulla, L. and Makhoul, J. (2006) A Study of Translation Edit Rate with Targeted Human Annotation. In AMTA 2006: Proceedings ofthe 7th Conference ofthe Association for Machine Translation in the Americas, pp.223– 231, Cambridege, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2952" citStr="Steedman, 2000" startWordPosition="426" endWordPosition="427">a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntacticallyoriented feature functions. Crucially, these feature functions concern the derived (partial) dependency structure as well as local aspects of the derivation process, including such information as the CCG lexical categories (supertag), the CCG operators and the intermediate parse states. This accomplishment is interesting both from a linguistic and technical point of view. Our third contribution is the extension of the standard phrase-based decoder with the syntactic structure and definition of new gramm</context>
<context position="13415" citStr="Steedman, 2000" startWordPosition="2091" endWordPosition="2092">arizes all the syntactic information about fragments that have already been processed and registers the syntactic arguments which are to be expected next. Only an impoverished deterministic procedure (called a ‘State-Realizer’) is needed in order to compose a state Si with the previous states S0 ... Si−1 in order to obtain afully connected intermediate dependency structure at every position in the input. To implement the incremental parsing scheme described above we use the parser described in (Hassan et al., 2008b; Hassan et al., 2009), which is based on Combinatory Categorial Grammar (CCG) (Steedman, 2000). We only briefly describe this parser as its full description is beyond the scope of this paper. The notions of a supertag as a lexical category and the process of supertagging are both crucial here (Bangalore and Joshi, 1999). Fortunately, CCG specifies the desired kind of lexical categories (supertags) sti for every word and a small set of combinatory operators oi that combine the supertag sti with a previous parse state Si−1 into the next parse state Si. In terms of CCG representations, the parse state is a CCG composite category which specifies either a functor and the arguments it expect</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Steedman, M. (2000). The Syntactic Process. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Word reordering and a dynamic programming beam search algorithm for statistical machine translation.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2444" citStr="Tillmann and Ney, 2003" startWordPosition="346" endWordPosition="349">e time and space complexities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntactically</context>
<context position="6558" citStr="Tillmann and Ney, 2003" startWordPosition="972" endWordPosition="975">rporating syntax into Phrase-based SMT (Marcu et al., 2006; Zollmann and Venugopal, 2006) share common drawbacks. Firstly, they are based on syntactic phrase-structure parse trees incorporated into a Synchronous CFG or TreeSubstitution Grammar, which makes for a difficult match with non-constituent phrases that are common within Phrase-based SMT. These approaches usually resort to ad hoc solutions to enrich the non-constituent phrases with syntactic structures. Secondly, they deploy chart-based decoders with a high computational cost compared with the phrase-based beam search decoders, e.g., (Tillmann and Ney, 2003; Koehn, 2004a). Thirdly, due to the large parse space, some of the proposed approaches are forced to employ small language models compared to what is usually used in phrase-based systems. To circumvent these computational limitations, various pruning techniques are usually needed, e.g., (Huang and Chiang, 2007). Other recent approaches, e.g., (Birch et al., 2007; Hassan et al., 2007; Hassan et al., 2008a) incorporate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lex</context>
<context position="11235" citStr="Tillmann and Ney, 2003" startWordPosition="1727" endWordPosition="1730">source and target phrase context (i.e. previous and next source and previous target), • Source Morphological Features encoding morphological and segmentation characteristics of source words. • Part-of-Speech Features encoding source and target POS tags as well as the POS tags of the surrounding contexts of phrases. The DTM2 approach based on MaxEnt provides a flexible framework for incorporating other available feature types as we demonstrate below. DTM2 Decoder: The decoder for the baseline is a beam search decoder similar to decoders used in standard phrase-based log-linear systems such as (Tillmann and Ney, 2003) and (Koehn, 2004a). The main difference between the DTM2 decoder and the standard Phrase–based SMT decoders is that DTM2 deploys Maximum Entropy probabilistic models to obtain the translation costs and various feature costs by deploying the features described above in a discriminative MaxEnt fashion. In the rest of this paper we adopt the DTM2 formalization of translation as a discriminative task, and we describe the CCG-based incremental dependency parser that we use for extending the DTM2 decoder, and then list a new set of syntactic dependency feature functions that extend the DTM2 feature</context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>Tillmann, C. and Ney, H. (2003). Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational Linguistics, 29(1):97–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation, HLT/NAACL, pp.138–141,</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="5574" citStr="Zollmann and Venugopal, 2006" startWordPosition="824" endWordPosition="828">provides translation output to shed further detailed insight into the characteristics of the systems. Finally, Section 8 concludes, and discusses future work. 2 Related Work In (Marcu et al., 2006), it is demonstrated that ‘syntactified’ target language phrases can improve translation quality for Chinese–English. A stochastic, top-down transduction process is employed that assigns a joint probability to a source sentence and each of its alternative syntactified translations; this is done by specifying a rewriting process of the target parse-tree into a source sentence. Likewise, the model in (Zollmann and Venugopal, 2006) extends (Chiang, 2005) by augmenting the hierarchical phrases with syntactic categories derived from parsing the target side of a parallel corpus. They use an existing parser to parse the target side of the parallel corpus in order to extract a syntactically motivated, bilingual synchronous grammar as in (Chiang, 2005). The above-mentioned approaches for incorporating syntax into Phrase-based SMT (Marcu et al., 2006; Zollmann and Venugopal, 2006) share common drawbacks. Firstly, they are based on syntactic phrase-structure parse trees incorporated into a Synchronous CFG or TreeSubstitution Gr</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Zollmann, A. and Venugopal, A. (2006). Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, HLT/NAACL, pp.138–141, New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>