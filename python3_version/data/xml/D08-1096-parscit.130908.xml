<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000552">
<title confidence="0.99506">
A graph-theoretic model of lexical syntactic acquisition
</title>
<author confidence="0.990319">
Hinrich Sch¨utze and Michael Walsh
</author>
<affiliation confidence="0.996017">
Institute for Natural Language Processing
University of Stuttgart, Germany
</affiliation>
<email confidence="0.981852">
{hs999,walsh}@ifnlp.org
</email>
<sectionHeader confidence="0.994445" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999288">
This paper presents a graph-theoretic model of
the acquisition of lexical syntactic representa-
tions. The representations the model learns
are non-categorical or graded. We propose a
new evaluation methodology of syntactic ac-
quisition in the framework of exemplar theory.
When applied to the CHILDES corpus, the
evaluation shows that the model’s graded syn-
tactic representations perform better than pre-
viously proposed categorical representations.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999832">
In recent years, exemplar theory has had great ex-
planatory success in phonetics. Exemplar theory
posits that linguistic production and perception are
not mediated via abstract categories, but that instead
each production and perception of a linguistic unit
is stored and retained. Linguistic inference then di-
rectly operates on these stored exemplars. In this pa-
per, we propose a new approach to lexical syntactic
acquisition in the framework of exemplar theory.
Our approach uses an evaluation measure that
is different from previous work. Lexical syntac-
tic acquisition is most often evaluated with respect
to standard syntactic categories like verb and noun.
Our first contribution in this paper is that we instead
evaluate learned representations in the context of a
syntactic task. This task is the determination of an
aspect of grammaticality that we call local syntactic
coherence.
Our second contribution is a graph-theoretic
model of the acquisition of lexical syntactic rep-
resentations that is more rigorous than previous
heuristic proposals. The graph-theoretic model
can learn both categorical and non-categorical (or
graded) representations. The model is also a unified
framework for syntagmatic and paradigmatic rela-
tions (as will be discussed below), and for lower-
order syntactic relations (those that can be directly
observed from the input) and higher-order syntac-
tic relations (those that require some generalization
from what is directly observable).
Redington et al. (1998) give an influential account
of the acquisition of lexical syntactic representations
in which a standard syntactic category like verb or
noun is assigned to each word. Our third contribu-
tion is to show that, in the context of acquisition,
graded representations are superior to standard cat-
egorical representations in supporting judgments of
local syntactic coherence. A graded representation
formalism is one that, for any two words, can rep-
resent a third word whose syntactic properties are
intermediate between the two words (Manning and
Sch¨utze, 1999).
Clearly exemplar theory is not the only frame-
work in which lexical acquisition has been explored.
Gleitman (1990) for example argues for syntactic
bootstrapping to infer lexical semantics, work not at
odds with our own (see discussion on the role of se-
mantics below). Our argument for the importance
of distributional evidence does not call into question
the large body of work in child language acquisition
that demonstrates that “part of the capacity to learn
languages must be ’innate’ ” (Gleitman and New-
port, 1995). Tabula rasa learning is not possible. Our
goal is not to show that language acquisition pro-
ceeds with a minimum of inductive bias. Rather, we
attempt to formalize one aspect of language acquisi-
tion, the use of distributional information.
The paper is organized as follows. Section 2 moti-
vates the exemplar-theoretic approach by reviewing
its success in phonetics. Section 3 defines local syn-
tactic coherence, which is the basis for a new evalu-
ation methodology for the acquisition of lexical rep-
resentations. Section 4 develops the graph-theoretic
model. Section 5 compares graded and categorical
representations for the task of inferring local syn-
</bodyText>
<page confidence="0.939115">
917
</page>
<note confidence="0.9632925">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 917–926,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.957707333333333">
tactic coherence. Section 6 presents our evaluation.
Sections 7 and 8 discuss related and future work, and
present our conclusions.
</bodyText>
<sectionHeader confidence="0.95635" genericHeader="method">
2 Exemplar theory
</sectionHeader>
<bodyText confidence="0.999924857142857">
The general idea of research into exemplars in
speech production and perception is that encoun-
tered items (segments, words, sentences etc.) are
stored in great detail in memory along with rich
linguistic and extra-linguistic context information.
These exemplars are organized into clouds of mem-
ory traces with similar traces lying close to each
other while dissimilar traces are more distant. A
number of such models have had great success in
accounting for production and perception phenom-
ena in phonetics. E.g., Johnson (1997) offers an
exemplar model which challenges the notion that
speech is perceived through a process of normal-
ization whereby a speaker-specific representation is
mapped or normalized into a speaker-neutral cate-
gorical abstraction. Johnson’s model successfully
treats aspects of vowel perception, sex identifica-
tion, and speaker variability. Crucially, no normal-
ization of percepts into categorical representations
takes place. The correct identification of phonemes
and words in his model is a function of direct com-
parison to richly detailed exemplars stored in mem-
ory. Other examples of exemplar-theoretic phonetic
accounts include (Goldinger, 1997), (Pierrehumbert,
2001), and our own work (Sch¨utze et al., 2007). Ex-
emplar theory’s success in phonetics motivates us to
investigate its use as a model for local syntactic phe-
nomena.
</bodyText>
<sectionHeader confidence="0.973454" genericHeader="method">
3 Local syntactic coherence
</sectionHeader>
<bodyText confidence="0.999942946428572">
In the context sequence model for exemplar-
theoretic phonetics (Wade et al., 2008), we represent
speech using amplitude envelopes derived from the
acoustic signal and then compute similarity as the
integral over the correlation of the two acoustic sig-
nals.
For the syntactic level, we need a representa-
tion that has two key properties of the represen-
tation we use in phonetics in order to support an
exemplar-theoretic account. First, the representa-
tion must be directly derivable from the perceived
input. In particular, it cannot rely on the results of
any disambiguation that would occur either as part
of exemplar-theoretic perception or in further down-
stream processing. Second, it must support similar-
ity computations. Accordingly, we first motivate the
representation we use and then introduce a similarity
measure on these representations.
Representation. There are two main sources1 of
directly observable information about the syntactic
properties of words: semantic cues (e.g., things are
often referred to with nouns) and the neighbors of
a word in sentences that it is used in. In this pa-
per, we only consider the second source of informa-
tion for acquisition, lexical neighbors.2 We further
limit ourselves to the immediate left and right lexical
neighbors (see discussion in Section 7).
When using lexical neighbors as the basis of rep-
resentation, we have to make a basic choice as to
whether we look at left and right neighbors sepa-
rately or whether we only look at the “correlated”
neighborhood information of left and right neigh-
bors jointly. Our approach is based on the first alter-
native: we separate the processing of left and right
neighbors. We do this for two reasons. First, gener-
alization improves and model complexity decreases
if left-neighbor information and right-neighbor in-
formation are looked at separately. E.g., the right
neighbors of to, might and not are similar because
all three words can be followed by base verbs like
dance: to dance, might dance, (might) not dance.
But their left neighbors are very different.
Second, exemplar-theoretic similarity is best de-
fined at the smallest possible scale in order to allow
optimal matching between parts of the stimulus and
parts of memory. In phonetics, we use a time scale
of 10s of milliseconds or even less. Conceivably,
one could also use segments (e.g., consonants and
vowels) as the smallest unit; however, this would
presume a segmented signal. And segmentation is
part of the perception task we want to explain in the
first place.
Separating left and right neighbors – which
amounts to looking at left and right local contexts
of each word separately – is the smallest scale we
can operate at when doing syntactic matching. We
</bodyText>
<footnote confidence="0.998273">
1A comprehensive account of acquisition must also include
morphology. See Christiansen et al. (2004).
2Psycholinguistic evidence for the importance of neighbor
information for learning categories includes (Mintz, 2002).
</footnote>
<page confidence="0.993563">
918
</page>
<bodyText confidence="0.997002686567164">
choose this small scale for the same reasons as we
choose a small scale in phonetics: to ensure maxi-
mum flexibility when matching parts of the stimulus
with exemplars in memory. Using words, bigrams or
larger units would reduce the flexibility in matching
and require a larger amount of experience (or train-
ing data) to learn a particular generalization.
We refer to the representations of left and right
contexts of a given word as half-words. In other
words, we split a word into two entities, a left half-
word that characterizes its behavior to the left and
a right half-word that characterizes its behavior to
the right. Thus left-context and right-context com-
ponents of the representation of a given focus word
are defined, where a left (right) half-word consists
of a probability distribution over all words that oc-
cur to the left (right) of the focus word and the
dimensionality of the vector for each word is de-
pendent on the number of distinct neighbors (left
and right). For example, having experienced take
doll twice and drop doll once, then the left con-
text distribution, or left half-word of doll, dolll, is
P(take) = 2/3, P(drop) = 1/3. By extension, the
phrase take the doll is represented as the following
six half-words: takel, taker, thel, ther, dolll, and
dollr.
Distance measure. The basic intuition behind lo-
cal syntactic coherence is that an important compo-
nent of syntactic wellformedness – and a compo-
nent that is of particular importance in acquisition
– is whether a similar sequence has already been
stored as grammatical in memory. The same way
that a phonetic signal that is well-formed in a partic-
ular language has many similar exemplars in mem-
ory, a syntactic sequence should also be licensed by
similar, previously perceived sequences in memory.
To operationalize this notion, we need to be able to
compute the similarity or distance between an in-
put stimulus and exemplars in memory. We do this
by first defining a distance measure for sequences of
fixed length.
The distance 0 between two sequences of half-
words &lt; g1, ... , gn &gt; and &lt; h1, ... , hn &gt; is de-
fined to be the sum of the distances of their half-
words:
0(&lt;g1, ... , gn&gt;,&lt;h1, ... , hn&gt;) = Eni=1 0(gi, hi)
This definition presupposes a definition of the dis-
tance of two half-words which will be given below.
We then call a sequence of n half-words
g1, ... , gn locally coherent if there is a sequence
h1, ... , hn in memory with 0(&lt; g1, ... , gn &gt;, &lt;
h1, ... , hn &gt;) &lt; B where B is a parameter.
Finally, we define a sentence to be locally n-
coherent if all of its subsequences of length n are
locally coherent.
The graph-theoretic model that is introduced in
the next section will be evaluated with respect to
how well it captures local syntactic coherence. This
enables us to evaluate the model with respect to a
task as opposed to its ability to reproduce a particu-
lar linguistic representation of syntactic categories.3
Obviously, the notion of local syntactic coherence
only captures some aspects of syntax – e.g., it does
not capture long-distance dependencies. However,
it is a plausible component of syntactic competence
and a plausible intermediate step in the acquisition
of syntax.
</bodyText>
<sectionHeader confidence="0.999294" genericHeader="method">
4 Graph-theoretic model
</sectionHeader>
<bodyText confidence="0.987570192307692">
We briefly review the structuralist notions of syntag-
matic and paradigmatic relationships that have been
frequently used in prior work in NLP (e.g., (Church
et al., 1994)). De Saussure defined a syntagmatic
relationship between two words as their contigu-
ous occurrence in a sentence and a paradigmatic re-
lationship as mutual substitutability (de Saussure,
1962) (although he used the term rapport associ-
atif instead of paradigmatic). E.g., brown and dog
stand in a syntagmatic relationship with each other
in the phrase brown dog; brown and black stand in a
paradigmatic relationship with each other with re-
spect to the position between the and dog in the
phrase the X dog. De Saussure’s conceptualization
of syntactic relationships captures the fact that both
admissible neighbors and admissible substitutes in
language are an important part of the characteriza-
tion of the syntactic properties of a word.
We formalize the two relations as distribu-
tions over words, where we assume a vocabulary
{w1, ... , wV } and V is the number of words in the
vocabulary.
We denote the left syntagmatic distribution of wi
3Freudenthal et al. (2004) have much the same motivation
in introducing an evaluation measure of syntactic acquisition
based on chunking.
</bodyText>
<page confidence="0.993902">
919
</page>
<bodyText confidence="0.999041678571429">
by pi,s,l,m where i is the vocabulary index of wi, s
stands for syntagmatic, l for left and m is the order
of the distribution as discussed below. Intuitively,
pi,s,l,m(wj) is the probability that word wj occurs to
the left of wi. Similarly, for the left paradigmatic
distribution of wi, pi,p,l,m(wj) is the probability that
wj can be substituted for wi without changing local
syntactic coherence as far as the context to the left
is concerned. Note that we distinguish between left
and right paradigmatic distributions. A word wj can
be a perfect substitute for wi as far as the context to
the left is concerned, but a very unlikely substitute as
far as the context to the right is concerned. E.g., in
the phrase She loves her job, the word him is a good
left-context substitute for her, but a terrible right-
context substitute for her.
We will now show how the syntag-
matic/paradigmatic (henceforth: syn/para) dis-
tributions are defined iteratively, based on the
bigram distribution pww, and grounded by defining
pi,p,l,1 and pi,p,r,1.
pww(wiwj) is the probability that the bigram
wiwj occurs, that is, that wi and wj occur next to
each other (and in that order). We define the V × V
joint probability matrix J by Jij = pww(wiwj).
Denote by N the diagonal V × V matrix that con-
tains in Nii the reciprocal of pw(wi) where pw is the
marginal distribution of pww:
</bodyText>
<equation confidence="0.939412">
1
pww(wjwi) = pw(wi) =
Nii
</equation>
<bodyText confidence="0.9997695">
The conditional probability pleft of the fol-
lowing word and the conditional probability
pright of the preceding word can be computed
by multiplying (the transpose of) J and N:
</bodyText>
<equation confidence="0.9886375">
pleft(wi|wj) = pww(wiwj)/pw(wj) = (JN)ij; and
pright(wi|wj) = (JTN)ij.
</equation>
<bodyText confidence="0.987263">
The “grounding” paradigmatic distributions of or-
der 1 are defined as follows.
</bodyText>
<equation confidence="0.856992">
�
0 if wi =6 wj
pi,p,l,1(wj) = pi,p,r,1(wj) =1 if wi = wj
</equation>
<bodyText confidence="0.99949175">
In other words, each word has only one perfect left
/ right substitute and that perfect substitute is itself.
We define the syn/para distributions of higher order
recursively:
</bodyText>
<equation confidence="0.933891">
pi,s,l,m = JNpi,p,l,m (1)
pi,p,r,m pi,s,r,m
</equation>
<figureCaption confidence="0.995352">
Figure 1: The distribution of typical right neighbors (the
right syntagmatic distribution pi,,,r,m) is computed from
the distribution of typical “right substitutes” (the right
paradigmatic distribution pi,p,r,m).
</figureCaption>
<equation confidence="0.986496333333333">
pi,p,l,m = JT Npi,s,l,m−1
pi,s,r,m = JT Npi,p,r,m
pi,p,r,m = JNpi,s,r,m−1
</equation>
<bodyText confidence="0.998271529411765">
Basic matrix arithmetic shows that pi,s,l,1 is sim-
ply pleft(.|wi) and pi,s,r,1 is pright(.|wi).
For higher orders, the principle underlying Eq.s
1–4 is that when moving from left to right, we use
pright (that is, JT N), the conditional distribution that
characterizes right neighbors; when moving from
right to left, we use pleft (that is, JN), the condi-
tional distribution that characterizes left neighbors.
This is graphically shown in Fig. 1.
As illustrated by Fig. 1, the underlying graph for
pi,s,r,m and pi,p,r,m is a weighted bipartite directed
graph that connects the vocabulary on the left with
the vocabulary on the right. A directed edge from
wi on the left to wj on the right is weighted with
pww(wiwj)/pw(wi). A directed edge from wj on
the right to wi on the left (not shown) is weighted
with pww(wiwj)/pw(wj).
</bodyText>
<equation confidence="0.950095">
Eq.s 1–4 define four Markov chains:
pi,s,l,m = (JNJT N)pi,s,l,m−1 (5)
pi,p,l,m = (JT NJN)pi,p,l,m−1 (6)
pi,s,r,m = (JT NJN)pi,s,r,m−1 (7)
pi,p,r,m = (JNJT N)pi,p,r,m−1 (8)
</equation>
<bodyText confidence="0.999639">
It is easy to see that pw is a stationary distribution
for Eq. 1–4. Writing x~ for pw, we have:
</bodyText>
<equation confidence="0.921745260869565">
pww(wiwj)pw(wj) = pw(wi) = xi
pw(wj) pw(wj) = pw(wi) = xi
woman
girl
boy
man
sang
laughed
cried
ran
V V
E pww(wiwj) = E
j=1 j=1
V
(JN~x)i = E
j=1
V
(JT N~x)i = E
j=1
pw(wj)
pww(wjwi)
920
Hence, pw is a solution for Eq.s (5)–(8).
</equation>
<bodyText confidence="0.999765714285715">
The series converge if JNJT N and JT NJN
are ergodic, i.e., if the chain is aperiodic and irre-
ducible (Kemeny and Snell, 1976). Observe that
for many simple probabilistic context-free gram-
mars (PCFGs) the series in Eq. 1–4 will not con-
verge. For simple PCFGs, the alternation between
syntagmatic and paradigmatic distributions is peri-
odic. E.g., if inflected verb forms only occur after
nouns and nouns only before inflected verb forms,
then the right syntagmatic distributions of nouns will
have non-zero activation only for verbs and the right
paradigmatic distributions of nouns will have non-
zero activation only for nouns, thus preventing con-
vergence.4
The key difference between a simple PCFG and
natural language is ambiguity and noise. Because
of ambiguity and noise, JNJT N and JT NJN are
likely to be ergodic – there is always a small non-
zero probability that two words can occur next to
each other. Ambiguity and noise have the same ef-
fect as teleportation for PageRank (Brin and Page,
1998) in the sense that we can jump from each word
to each other word with non-zero probability.
Assuming that the Markov chains are ergodic, all
four converge to pw: pi,p,r,∞ = pi,p,l,∞ = pi,s,r,∞ =
pi,s,l,∞ = pw, for 1 G i G V .
Thus, in this formalization, given enough itera-
tions, syntagmatic and paradigmatic distributions of
words eventually all become identical with the prior
distribution pw. This is surprising because linguisti-
cally and computationally syntagmatic and paradig-
matic relations are fundamentally different.
However, on closer inspection, we observe that
limiting the number of iterations is often beneficial
when computing solutions to a problem iteratively.
E.g., the expectation-maximization algorithm is of-
ten stopped early because results close to conver-
gence are worse than results obtained after a small
number of iterations. From the point of view of
modeling human language acquisition, early stop-
ping is perhaps also more realistic since humans are
unlikely to perform a large number of iterations.
</bodyText>
<footnote confidence="0.750112333333333">
4However, non-ergodicity of JN does not imply non-
ergodicity of JNJT N and JT NJN, so Eq. (5)–(8) can con-
verge even for non-ergodic JN.
</footnote>
<figureCaption confidence="0.941919">
Figure 2: The distance between elephant and giraffe
(measured by the Jensen-Shannon divergence) is accu-
rately represented after a number of iterations. The words
elephant and the retain their large distance.
</figureCaption>
<equation confidence="0.533747444444444">
Example 1. For the following matrix J
� w1 w1 w2 w3
� � � w2 82/1002 77/1002 112/1002 �
w3 90/1002 18/1002 107/1002 � � �
99/1002 120/1002 297/1002
we get p1,s,r,1 = (0.31, 0.28, 0.41) by comput-
ing the product JTNp1,p,r,1. E.g., p1,s,r,1(w2) =
pww(w1w2)/pw(w1) · 1.0 = 77/(82 + 77 + 112) ≈
0.28.
</equation>
<bodyText confidence="0.974795">
By iteration m = 4, the series pi,s,r,m (Eq. (7))
and pi,p,r,m (Eq. (8)) have converged to:
</bodyText>
<equation confidence="0.77073175">
pi,s,r,m = pi,p,r,m = (0.2704, 0.2145, 0.5149)
for all three words wi. One can easily verify that
this is pw. E.g., pw(w1) = (82 + 90 + 99)/1002 =
(82 + 77 + 112)/1002 ≈ 0.27045.
</equation>
<bodyText confidence="0.998048083333333">
Example 2. We computed 15 iterations of
syn/para distributions for the corpus: The giraffe
ran. An elephantfell. The man ran. An auntfell. The
man slept. The aunt slept. Fig. 2 shows that the dis-
tance between the right syntagmatic distributions of
elephant and giraffe is large for m = 1. The reason
is that the two words have no right neighbors in com-
mon. The right neighbors of the two words are ran
and fell. Although ran and fell have no left neighbors
in common, their left neighbors have a right neigh-
bor in common: the word slept. This indirect simi-
larity information is exploited to deduce by iteration
</bodyText>
<figure confidence="0.995644266666666">
2 4 6 8 10 12 14
iteration m
JS divergence of right synt. distributions
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
g
t
gt
elephant−giraffe
elephant−the
g
g
g
g
g g
g g g g g g g g
</figure>
<page confidence="0.985337">
921
</page>
<bodyText confidence="0.999375666666667">
15 that the two words are very similar with respect to
their right syntactic context. In contrast, no such in-
ference, even a very indirect one, is possible for the
right contexts of elephant and the. Consequently, the
distance between the two distributions remains high
and unchanged with higher iterations.
In this case, the Markov chain is not ergodic and
the syntagmatic and paradigmatic series (Eq.s (5)–
(8)) do not converge to p,,,.
</bodyText>
<sectionHeader confidence="0.995125" genericHeader="method">
5 Experimental evaluation
</sectionHeader>
<bodyText confidence="0.996997454545455">
Recall from Section 3 that our evaluation task is to
discriminate sentences that exhibit local coherence
from those that do not; that sentences are repre-
sented as sequences of half-words; that syntactic co-
herence of a sentence is defined as all subsequences
of a given length n exhibiting local coherence; and
that a subsequence is locally coherent if its distance
from a sequence in memory is less than 0.
These definitions can be applied to the graph
model as follows. A left half-word is a left syntag-
matic (or paradigmatic) distribution and a right half-
word is a right syntagmatic (or paradigmatic) distri-
bution. We compute the distance of two half-words
either as the Jensen-Shannon (JS) divergence (Lin,
1991) or as (1 − cos(α)). JS divergence is more ap-
propriate for the comparison of probability distribu-
tions. But the cosine is more efficient when a sparse
vector is compared to a dense vector.5 We therefore
employ the cosine for the compute-intensive experi-
ments in Section 6.
The baseline representation is the categorical rep-
resentation proposed by Redington et al. (1998). A
difficulty in replicating their experiments is that they
use hierarchical agglomerative clustering (HAC),
which eventually agglomerates all words in a sin-
gle category. To circumvent the need for a stop-
ping criterion, we represent each word as the tem-
poral sequence of clusters it occurred in during ag-
glomeration and define the distance of two words as
the agglomeration step in which the two words are
joined in a cluster. E.g., given the agglomeration se-
quences {1}, {1, 2}, {1, 2, 4}, {1, 2, 3, 4} for w1 and
{4}, {4}, {1, 2, 4}, {1, 2, 3, 4} for w4, the distance
</bodyText>
<footnote confidence="0.778073666666667">
5This is so because, when computing the cosine, we can ig-
nore all dimensions where one of the two vectors has a zero
value.
</footnote>
<bodyText confidence="0.993799270833334">
between w1 and w4 is 3 since they are joined in step
3 when cluster {1, 2, 4} is created.
For both graded (graph-theoretic) and categorical
(cluster-based) representations, we need to set the
parameter 0 that is the boundary between locally co-
herent and locally incoherent sentences. This pa-
rameter gives rise to a precision-recall tradeoff. A
small 0 will impose strict requirements on which se-
quences in memory match, resulting in false nega-
tive decisions for local grammaticality. A large 0
will incorrectly judge many locally incoherent se-
quences to be grammatical.
We will pick the optimal 0 in both cases. For
categorical representations, this amounts to select-
ing the HAC dendrogram with optimal performance.
The experiment below evaluates whether grammati-
cal and ungrammatical sentences are well separated
by the proposed measure.6
Experiment on CHILDES. We used the well-
known CHILDES database (MacWhinney, 2000), a
corpus of conversations between young children and
their playmates, siblings, and caretakers. In order to
avoid mixing varieties of English (e.g., British En-
glish vs. American English), we selected the largest
homogeneous subcorpus of CHILDES, the Manch-
ester corpus. It contains roughly 350,000 sentences
and 1.5 million words. This is a conservative esti-
mate of the amount of child-directed speech a child
would receive annually (Redington et al., 1998). All
names in the corpus (i.e., all capitalized words) were
replaced with a special word “ n ”. A boundary
symbol “ b ” was introduced to separate sentences.
The representation of the corpus is then a concate-
nation of all its sentences. The vocabulary consists
of V = 8601 words.
Construction of the evaluation set. We tested
the ability of the two models to distinguish locally
coherent vs. incoherent sentences by selecting 100
unattested sentences from the corpus, which were
not used to train the model. We only selected unat-
tested sentences that were not a substring of a sen-
tence in the training corpus since, presumably, any
substring of a sentence in the training corpus is lo-
cally coherent. A further constraint was that the
6This evaluation of “separation” is not directly an evaluation
of classification performance, but more similar to an evaluation
of ranking using AUC or an evaluation of clustering using a
measure like purity.
</bodyText>
<page confidence="0.994139">
922
</page>
<bodyText confidence="0.999961625">
unattested sentence was not allowed to contain a
word that did not occur in the training corpus, the
rationale being that we want to address the prob-
lem of local coherence for known words only since
unknown words present special challenges. Finally,
we ensured that each unattested sentence contained
a word that occurred in only one sentence type in
the training corpus. In early experiments, we found
that local grammatical inference for frequent words
is easy as there is redundant evidence available that
characterizes legal syntactic environments for fre-
quent words. Since rare words are a key challenge in
syntactic acquisition, we only selected sentences as
unattested sentences that contained at least one rare
word (where a rare word is defined as a word that
occurs once in the training set).
100 ungrammatical sentences were generated by
randomly selecting and concatenating words from
the vocabulary. Ungrammatical sentences were
matched in length to unattested sentences, so that
both sets contained the same number of sentences
of a given length. As with unattested sentences, un-
grammatical sentences that were substrings of sen-
tences in the training corpus were eliminated. As
there are many more infrequent words than frequent
words in the vocabulary, the construction ensured
that, as with unattested sentences, infrequent words
were overrepresented in ungrammatical sentences.
To summarize, our setup consists of 348,463
training sentences, 100 unattested grammatical sen-
tences and 100 ungrammatical sentences.
The task of discriminating the 100 unattested
from the 100 ungrammatical sentences cannot be
solved perfectly as CHILDES contains ungrammat-
ical sentences, a few of which were randomly se-
lected as unattested sentences (e.g., yes pleas, which
is missing the final letter). Similarly, one or two
of the automatically generated ungrammatical sen-
tences were actually grammatical.
Since the test set does not consist of a random
sample of sentences, performance on the test set is
not a direct indicator of the percentage of sentences
that the model can correctly discriminate in a child’s
typical input. A large proportion of sentences in
child input are simple 1-word, 2-word, and 3-word
sentences that even simplistic models can evaluate
with high accuracy. However, the test set is appro-
priate for a comparative evaluation of graded and
</bodyText>
<figure confidence="0.620708">
2 4 6 8 10
number of half words
</figure>
<figureCaption confidence="0.998926">
Figure 3: Accuracy of discrimination between grammati-
cal and ungrammatical sentences for graded and categor-
ical representations.
</figureCaption>
<bodyText confidence="0.999871857142857">
categorical syntactic representations in language ac-
quisition, which is one of the goals of the paper. Dif-
ficult sentences (those with rare words and greater
length) are overrepresented in the test set as the dis-
crimination of short sentences containing only fre-
quent words can easily be done by simplistic mod-
els. Thus, a test set of “easy” sentences would not
distinguish good models from bad models.
Discrimination experiment. In order to train the
graph model, the entries of matrix J were estimated
using maximum likelihood based on the training
corpus. pi,s,l,1 and pi,s,r,1 were then computed for
all 8601 words. Replicating (Redington et al., 1998),
the most frequent 1000 words were clustered (using
single-link HAC, Manning and Sch¨utze (1999)). For
each remaining word w, the closest neighbor w′ in
the 1000 most frequent words was determined and
w was then assigned to the cluster of w′.
Fig. 3 shows the performance of graded and cat-
egorical representations for different subsequence
sizes n. To compute the accuracy for each n, the θ
with optimal discrimination performance was cho-
sen (for both graded and categorical).
For a subsequence of size n = 1, the performance
is 0.5 in both cases since the 200-sentence test set
does not contain unknown words. So for every half-
word, there is a sequence of one half-word in the
training corpus with distance 0. Thus, all sentences
</bodyText>
<figure confidence="0.99719265">
xc
x
c c c
x
x
x
c
x
c
x x
c
c
x
c
graded
categorical
x x
c c
0.5 0.6 0.7 0.8 0.9 1.0
accuracy of discrimination
</figure>
<page confidence="0.996844">
923
</page>
<bodyText confidence="0.997569513513513">
get the same local coherence scores, both for graded
and categorical representations.
This argument does not apply to n = 2 since we
earlier defined a sentence to be locally coherent if
all of its subsequences are coherent. While subse-
quences of 2 half-words that are part of the same
word have local coherence score 0, this is not true of
subsequences of 2 half-words that are part of differ-
ent words, e.g., the subsequence &lt;blackr,dogl &gt; in
black dog. If black dog does not occur in the train-
ing set, then its local coherence score is &gt; 0.
The main result of the experiment is that except
for n=1 (p = 1) and n=2 (p = 0.39) the differences
between categorical and graded representations are
significant (x2 test, p &lt; 0.05 for 3 &lt; n &lt; 10). This
is evidence that graded representations are more ac-
curate when determining local syntactic coherence
and grammaticality than categorical representations.
The experimental results demonstrate that, for
syntagmatic distributions of order 1, graded repre-
sentations discriminate locally coherent vs. incoher-
ent sentences better than categorical representations.
We attribute this to the ability of exemplar theory to
incorporate rich context information into discrimi-
nation decisions. This is of particular importance
for ambiguous words. Categorical representations of
ambiguous words are problematic because they are
either too similar or not similar enough to the two
alternatives. E.g., if a word with a verb/noun ambi-
guity is represented as one of the alternatives, say,
as a verb, then subsequences containing its noun use
will no longer be similar to other subsequences with
nouns. If a special conflation category noun/verb is
introduced, then we are faced with the same prob-
lem: subsequences containing the noun/verb cate-
gory are not similar to subsequences containing ei-
ther non-ambiguous verbs or non-ambiguous nouns.
</bodyText>
<sectionHeader confidence="0.997409" genericHeader="method">
6 Higher-order distributions
</sectionHeader>
<bodyText confidence="0.999306125">
The main motivation for higher-order distributions
is that syntagmatic vectors of order 1 do not per-
form well for some infrequent words. In the ele-
phant/giraffe example above, the distance between
the two words is close to maximum for order 1 repre-
sentations because each occurs only once, in entirely
different contexts. As we showed in Fig. 2, higher-
order representations address this problem because
</bodyText>
<figure confidence="0.579583">
2 4 6 8 10
number of half words
</figure>
<figureCaption confidence="0.9038968">
Figure 4: Accuracy of discrimination between grammat-
ical and ungrammatical sentences of the exemplar-based
method for different orders. Key: synt = syntagmatic,
para = paradigmatic; s is of order 1; p and t are of order
2; q is of order 3.
</figureCaption>
<bodyText confidence="0.998965653846154">
they exploit indirect evidence about the syntactic
properties of words.
To evaluate higher-order representations on
CHILDES, we used the same setup as before, but
computed several additional iterations. We also lim-
ited the experiments to a subset consisting of 60,000
words of the Manchester corpus. It contains only
V =1666 different words, which reduces the storage
requirements for the syn/para distributions (which is
2·V 2 for each order) and the cost of the matrix mul-
tiplications. We also used (1− cos(α)) instead of JS
divergence as distance measure.
The results of the experiment are shown in Fig. 4.
Higher-order representations are clearly superior for
short subsequences, especially for n = 2 and n = 3
(and up to 5 half-words when comparing synt-1 and
para-2). However, for long subsequences, there is no
consistent difference between the syntagmatic distri-
bution of order 1 (synt-1) and higher order distribu-
tions. Apparently, the generalized information avail-
able in higher orders is not helpful in local grammat-
ical inference if long contexts are considered.
We were surprised that the best-performing dis-
tribution for short sequences is para-2 (paradigmatic
distribution of order 2), not a higher order distri-
bution. E.g., para-3 performs worse than para-2.
</bodyText>
<figure confidence="0.998173078947368">
q
s
p
p
t
s
p
t
q
synt−1
para−2
synt−2
para−3
q q
s
t
p
i
q
p
s
t
s
q
p
t
q q
s
p
t t
ps sp
q
t
s
qt
p
0.70 0.75 0.80 0.85 0.90 0.95
accuracy of discrimination
</figure>
<page confidence="0.99278">
924
</page>
<bodyText confidence="0.999994">
We would expect the performance to decrease with
higher order eventually since the distributions con-
verge towards p,,,. The fact that this happens so early
in this experiment merits further investigation.
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.9999672">
Data-oriented parsing (Bod et al., 2003) shares
basic assumptions about linguistic inference with
exemplar-based theory, but it does not model or use
the similarity between input and stored exemplars.
Previous work on exemplar theory in syntax (Abbot-
Smith and Tomasello, 2006; Bybee, 2006; Hay and
Bresnan, 2006) has not been computational or for-
mal. Previous work on non-categorical representa-
tions of words has viewed these representations as
an intermediate step for arriving at categorical parts
of speech (Redington et al., 1998; Sch¨utze, 1995;
Clark, 2003). Consequently, all of these papers eval-
uate their results by comparing induced categories to
gold-standard parts of speech.
Redington et al. (1998) did not find a difference in
categorization accuracy between simple syntagmatic
representation and those using non-adjacent words.
The BEAGLE model (Jones and Mewhort, 2007),
and related work (Sahlgren et al., 2008), merges co-
occurrence information and word order information
into a single composite vector through a process of
vector convolution. Our model differs in that it ex-
plicitly captures the recursive relationship between
the orders in a unified framework.
Previous graph-theoretic work (Biemann, 2006)
uses order 1 representations. Several papers have
looked at higher-order representations, but have not
examined the equivalence of syn/para distributions
when formalized as Markov chains (Sch¨utze and
Pedersen, 1993; Lund and Burgess, 1996; Edmonds,
1997; Rapp, 2002; Biemann et al., 2004; Lemaire
and Denhi`ere, 2006). Toutanova et al. (2004) found
that their graph model of predicate argument struc-
ture deteriorated after a small number of iterations
of the random walk, similar to our findings.
</bodyText>
<sectionHeader confidence="0.996606" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999988590909091">
In this paper, we have presented a graph-theoretic
model of the acquisition of lexical syntactic rep-
resentations and a new exemplar-based evaluation
of lexical syntactic acquisition. When applied to
the CHILDES corpus, the evaluation shows that
the graded syntactic representations learned by the
model perform significantly better than previously
proposed categorical representations. An initial
evaluation of high-order representations showed lit-
tle improvement over low-order representations.
In future work, we intend to investigate the in-
fluence of noise and ambiguity on the quality of
the representations in order to characterize when
higher order representations improve generalization
and exemplar-theoretic inference. We also want
to address that the model as it currently stands is
trained under the false assumption that the train-
ing input is grammatical. Ungrammatical test input
which matches a learned ungrammatical sequence
will be deemed grammatical. Future work will ex-
amine how to best treat this challenge, e.g., by using
an estimation of density instead of the simplistic “1
nearest neighbor” distance used here.
The most important future work concerns class-
based language models. The cognitive-linguistic
tradition we have mainly addressed in this paper
has focused on the task of learning traditional parts
of speech and has usually not discussed the rele-
vance of language models to acquisition. If, as we
have argued, instead of learning traditional parts of
speech the focus should be on performance in par-
ticular language processing tasks (like grammatical-
ity judgments), then language models are the nat-
ural competing account that we must compare our
work to. Of particular relevance are class-based lan-
guage models (e.g., (Saul and Pereira, 1997; Brown
et al., 1992)). In ongoing work, we are attempting
to show that the exemplar-theoretic model performs
better on grammaticality judgments than class-based
language models.
Acknowledgements. This research was funded by
the German Research Council (DFG, Grant SFB
732). We thank K. Rothenh¨ausler, H. Schmid and
the reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.997233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.85070725">
Abbot-Smith, Kirsten and Michael Tomasello. 2006.
Exemplar-learning and schematization in a usage-
based account of syntactic acquisition. The Linguistic
Review, 23:275–290.
</reference>
<page confidence="0.983945">
925
</page>
<reference confidence="0.9998402">
Biemann, Chris, Stefan Bordag, and Uwe Quasthoff.
2004. Automatic acquisition of paradigmatic relations
using iterated co-occurrences. In LREC.
Biemann, Chris. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In ACL.
Bod, Rens, Remko Scha, and Khalil Sima′an. 2003.
Data-Oriented Parsing. CSLI Publications.
Brin, Sergey and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
WWW, pages 107–117.
Brown, Peter F., Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467–479.
Bybee, Joan L. 2006. From usage to grammar: The
mind’s response to repetition. Language, 82:711–733.
Christiansen, Morten, Luca Onnis, Padraic Monaghan,
and Nick Chater. 2004. Happy endings in language
acquisition. In AMLaP.
Church, Kenneth, Patrick Hanks, Donald Hindle,
William Gale, and Rosamund Moon. 1994. Lexical
substitutability. In Atkins, B.T.S. and A. Zampolli, ed-
itors, Computational Approaches to the Lexicon. OUP.
Clark, Alexander. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In EACL, pages 59–66.
de Saussure, Ferdinand. 1962. Cours de linguistique
g´en´erale. Payot, Paris. Originally published in 1916.
Edmonds, Philip. 1997. Choosing the word most typical
in context using a lexical co-occurrence network. In
ACL, pages 507–509.
Freudenthal, Daniel, Julian Pine, and Fernand Gobet.
2004. Resolving ambiguities in the extraction of syn-
tactic categories through chunking. In ICCM.
Gleitman, Lila and Elissa Newport. 1995. The invention
of language by children: Environmental and biologi-
cal influences on the acquisition of language. In Gleit-
man, Lila and Mark Liberman, editors, Language: An
invitation to cognitive science. MIT Press, 2nd edition.
Gleitman, Lila. 1990. The structural sources of verb
meanings. Language Acquisition, 1:3–55.
Goldinger, Stephen D. 1997. Words and voices—
perception and production in an episodic lexicon. In
(Johnson and Mullennix, 1997).
Hay, Jennifer and Joan Bresnan. 2006. Spoken syntax:
The phonetics of giving a hand in New Zealand En-
glish. The Linguistic Review, 23.
Johnson, Keith and John W. Mullennix, editors. 1997.
Talker Variability in Speech Processing. Academic
Press.
Johnson, Keith. 1997. Speech perception without
speaker normalization. In (Johnson and Mullennix,
1997).
Jones, Michael N. and Douglas J.K. Mewhort. 2007.
Representing word meaning and order information in
a composite holographic lexicon. Psychological Re-
view, 114:1–37.
Kemeny, John G. and J. Laurie Snell. 1976. Finite
Markov Chains. Springer, New York.
Lemaire, Benoit and Guy Denhi`ere. 2006. Effects of
high-order co-occurrences on word semantic similar-
ity. Behaviour, Brain &amp; Cognition, 18(1).
Lin, Jianhua. 1991. Divergence measures based on the
Shannon entropy. IEEE Trans. Inf. Theory, 37(1):145–
151.
Lund, Kevin and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instrumen-
tation, and Computers, 28:203–208.
MacWhinney, Brian. 2000. The CHILDES project:
Tools for analyzing talk. Lawrence Erlbaum.
Manning, Christopher D. and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Boston, MA.
Mintz, Toben H. 2002. Category induction from dis-
tributional cues in an artificial language. Memory &amp;
Cognition, 30:678–686.
Pierrehumbert, Janet. 2001. Exemplar dynamics: Word
frequency, lenition and contrast. In Bybee, Joan and
Paul Hopper, editors, Frequency and the Emergence of
Linguistic Structure, pages 137–157. Benjamins.
Rapp, Reinhard. 2002. The computation of word as-
sociations: comparing syntagmatic and paradigmatic
approaches. In Coling.
Redington, Martin, Nick Chater, and Steven Finch.
1998. Distributional information: A powerful cue
for acquiring syntactic categories. Cognitive Science,
22(4):425–469.
Sahlgren, Magnus, Anders Holst, and Jussi Karlgren.
2008. Permutations as a means to encode order in
word space. In CogSci.
Saul, Lawrence and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language processing. In EMNLP, pages 81–89.
Sch¨utze, Hinrich and Jan Pedersen. 1993. A vector
model for syntagmatic and paradigmatic relatedness.
In UW Centre for the New OED and Text Research.
Sch¨utze, Hinrich, Michael Walsh, Travis Wade, and
Bernd M¨obius. 2007. Towards a unified exemplar-
theoretic model of phonetic and syntactic phenomena.
In CogSci, Poster Session.
Sch¨utze, Hinrich. 1995. Distributional part-of-speech
tagging. In EACL, pages 141–148.
Toutanova, Kristina, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models for
inducing word dependency distributions. In ICML.
Wade, Travis, Grzegorz Dogil, Hinrich Sch¨utze, Michael
Walsh, and Bernd M¨obius. 2008. Syllable fre-
quency effects in a context-sensitive segment produc-
tion model. Submitted.
</reference>
<page confidence="0.998406">
926
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.809356">
<title confidence="0.998783">A graph-theoretic model of lexical syntactic acquisition</title>
<author confidence="0.830076">Hinrich Sch¨utze</author>
<author confidence="0.830076">Michael</author>
<affiliation confidence="0.9974385">Institute for Natural Language University of Stuttgart,</affiliation>
<abstract confidence="0.997756909090909">This paper presents a graph-theoretic model of the acquisition of lexical syntactic representations. The representations the model learns are non-categorical or graded. We propose a new evaluation methodology of syntactic acquisition in the framework of exemplar theory. When applied to the CHILDES corpus, the evaluation shows that the model’s graded syntactic representations perform better than previously proposed categorical representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kirsten Abbot-Smith</author>
<author>Michael Tomasello</author>
</authors>
<title>Exemplar-learning and schematization in a usagebased account of syntactic acquisition. The Linguistic Review,</title>
<date>2006</date>
<pages>23--275</pages>
<marker>Abbot-Smith, Tomasello, 2006</marker>
<rawString>Abbot-Smith, Kirsten and Michael Tomasello. 2006. Exemplar-learning and schematization in a usagebased account of syntactic acquisition. The Linguistic Review, 23:275–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Stefan Bordag</author>
<author>Uwe Quasthoff</author>
</authors>
<title>Automatic acquisition of paradigmatic relations using iterated co-occurrences.</title>
<date>2004</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="34665" citStr="Biemann et al., 2004" startWordPosition="5668" endWordPosition="5671">lated work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work In this paper, we have presented a graph-theoretic model of the acquisition of lexical syntactic representations and a new exemplar-based evaluation of lexical syntactic acquisition. When applied to the CHILDES corpus, the evaluation shows that the graded syntactic representations learned by the model perform significantly better than previously proposed cat</context>
</contexts>
<marker>Biemann, Bordag, Quasthoff, 2004</marker>
<rawString>Biemann, Chris, Stefan Bordag, and Uwe Quasthoff. 2004. Automatic acquisition of paradigmatic relations using iterated co-occurrences. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised part-of-speech tagging employing efficient graph clustering.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="34376" citStr="Biemann, 2006" startWordPosition="5629" endWordPosition="5630">eir results by comparing induced categories to gold-standard parts of speech. Redington et al. (1998) did not find a difference in categorization accuracy between simple syntagmatic representation and those using non-adjacent words. The BEAGLE model (Jones and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work In this paper, we have presented a graph-theoretic model of the acquisi</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Biemann, Chris. 2006. Unsupervised part-of-speech tagging employing efficient graph clustering. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
<author>Remko Scha</author>
<author>Khalil Sima′an</author>
</authors>
<title>Data-Oriented Parsing.</title>
<date>2003</date>
<publisher>CSLI Publications.</publisher>
<marker>Bod, Scha, Sima′an, 2003</marker>
<rawString>Bod, Rens, Remko Scha, and Khalil Sima′an. 2003. Data-Oriented Parsing. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In WWW,</booktitle>
<pages>107--117</pages>
<contexts>
<context position="17596" citStr="Brin and Page, 1998" startWordPosition="2848" endWordPosition="2851">after nouns and nouns only before inflected verb forms, then the right syntagmatic distributions of nouns will have non-zero activation only for verbs and the right paradigmatic distributions of nouns will have nonzero activation only for nouns, thus preventing convergence.4 The key difference between a simple PCFG and natural language is ambiguity and noise. Because of ambiguity and noise, JNJT N and JT NJN are likely to be ergodic – there is always a small nonzero probability that two words can occur next to each other. Ambiguity and noise have the same effect as teleportation for PageRank (Brin and Page, 1998) in the sense that we can jump from each word to each other word with non-zero probability. Assuming that the Markov chains are ergodic, all four converge to pw: pi,p,r,∞ = pi,p,l,∞ = pi,s,r,∞ = pi,s,l,∞ = pw, for 1 G i G V . Thus, in this formalization, given enough iterations, syntagmatic and paradigmatic distributions of words eventually all become identical with the prior distribution pw. This is surprising because linguistically and computationally syntagmatic and paradigmatic relations are fundamentally different. However, on closer inspection, we observe that limiting the number of iter</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Brin, Sergey and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. In WWW, pages 107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Comput. Linguist.,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Brown, Peter F., Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Comput. Linguist., 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan L Bybee</author>
</authors>
<title>From usage to grammar: The mind’s response to repetition.</title>
<date>2006</date>
<tech>Language, 82:711–733.</tech>
<contexts>
<context position="33440" citStr="Bybee, 2006" startWordPosition="5491" endWordPosition="5492"> s t s q p t q q s p t t ps sp q t s qt p 0.70 0.75 0.80 0.85 0.90 0.95 accuracy of discrimination 924 We would expect the performance to decrease with higher order eventually since the distributions converge towards p,,,. The fact that this happens so early in this experiment merits further investigation. 7 Related work Data-oriented parsing (Bod et al., 2003) shares basic assumptions about linguistic inference with exemplar-based theory, but it does not model or use the similarity between input and stored exemplars. Previous work on exemplar theory in syntax (AbbotSmith and Tomasello, 2006; Bybee, 2006; Hay and Bresnan, 2006) has not been computational or formal. Previous work on non-categorical representations of words has viewed these representations as an intermediate step for arriving at categorical parts of speech (Redington et al., 1998; Sch¨utze, 1995; Clark, 2003). Consequently, all of these papers evaluate their results by comparing induced categories to gold-standard parts of speech. Redington et al. (1998) did not find a difference in categorization accuracy between simple syntagmatic representation and those using non-adjacent words. The BEAGLE model (Jones and Mewhort, 2007), a</context>
</contexts>
<marker>Bybee, 2006</marker>
<rawString>Bybee, Joan L. 2006. From usage to grammar: The mind’s response to repetition. Language, 82:711–733.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morten Christiansen</author>
<author>Luca Onnis</author>
<author>Padraic Monaghan</author>
<author>Nick Chater</author>
</authors>
<title>Happy endings in language acquisition.</title>
<date>2004</date>
<booktitle>In AMLaP.</booktitle>
<contexts>
<context position="8415" citStr="Christiansen et al. (2004)" startWordPosition="1288" endWordPosition="1291"> stimulus and parts of memory. In phonetics, we use a time scale of 10s of milliseconds or even less. Conceivably, one could also use segments (e.g., consonants and vowels) as the smallest unit; however, this would presume a segmented signal. And segmentation is part of the perception task we want to explain in the first place. Separating left and right neighbors – which amounts to looking at left and right local contexts of each word separately – is the smallest scale we can operate at when doing syntactic matching. We 1A comprehensive account of acquisition must also include morphology. See Christiansen et al. (2004). 2Psycholinguistic evidence for the importance of neighbor information for learning categories includes (Mintz, 2002). 918 choose this small scale for the same reasons as we choose a small scale in phonetics: to ensure maximum flexibility when matching parts of the stimulus with exemplars in memory. Using words, bigrams or larger units would reduce the flexibility in matching and require a larger amount of experience (or training data) to learn a particular generalization. We refer to the representations of left and right contexts of a given word as half-words. In other words, we split a word</context>
</contexts>
<marker>Christiansen, Onnis, Monaghan, Chater, 2004</marker>
<rawString>Christiansen, Morten, Luca Onnis, Padraic Monaghan, and Nick Chater. 2004. Happy endings in language acquisition. In AMLaP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
<author>Donald Hindle</author>
<author>William Gale</author>
<author>Rosamund Moon</author>
</authors>
<title>Lexical substitutability.</title>
<date>1994</date>
<booktitle>Computational Approaches to the Lexicon. OUP.</booktitle>
<editor>In Atkins, B.T.S. and A. Zampolli, editors,</editor>
<contexts>
<context position="11915" citStr="Church et al., 1994" startWordPosition="1891" endWordPosition="1894">bles us to evaluate the model with respect to a task as opposed to its ability to reproduce a particular linguistic representation of syntactic categories.3 Obviously, the notion of local syntactic coherence only captures some aspects of syntax – e.g., it does not capture long-distance dependencies. However, it is a plausible component of syntactic competence and a plausible intermediate step in the acquisition of syntax. 4 Graph-theoretic model We briefly review the structuralist notions of syntagmatic and paradigmatic relationships that have been frequently used in prior work in NLP (e.g., (Church et al., 1994)). De Saussure defined a syntagmatic relationship between two words as their contiguous occurrence in a sentence and a paradigmatic relationship as mutual substitutability (de Saussure, 1962) (although he used the term rapport associatif instead of paradigmatic). E.g., brown and dog stand in a syntagmatic relationship with each other in the phrase brown dog; brown and black stand in a paradigmatic relationship with each other with respect to the position between the and dog in the phrase the X dog. De Saussure’s conceptualization of syntactic relationships captures the fact that both admissibl</context>
</contexts>
<marker>Church, Hanks, Hindle, Gale, Moon, 1994</marker>
<rawString>Church, Kenneth, Patrick Hanks, Donald Hindle, William Gale, and Rosamund Moon. 1994. Lexical substitutability. In Atkins, B.T.S. and A. Zampolli, editors, Computational Approaches to the Lexicon. OUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In EACL,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="33715" citStr="Clark, 2003" startWordPosition="5533" endWordPosition="5534">ent merits further investigation. 7 Related work Data-oriented parsing (Bod et al., 2003) shares basic assumptions about linguistic inference with exemplar-based theory, but it does not model or use the similarity between input and stored exemplars. Previous work on exemplar theory in syntax (AbbotSmith and Tomasello, 2006; Bybee, 2006; Hay and Bresnan, 2006) has not been computational or formal. Previous work on non-categorical representations of words has viewed these representations as an intermediate step for arriving at categorical parts of speech (Redington et al., 1998; Sch¨utze, 1995; Clark, 2003). Consequently, all of these papers evaluate their results by comparing induced categories to gold-standard parts of speech. Redington et al. (1998) did not find a difference in categorization accuracy between simple syntagmatic representation and those using non-adjacent words. The BEAGLE model (Jones and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a uni</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Clark, Alexander. 2003. Combining distributional and morphological information for part of speech induction. In EACL, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferdinand de Saussure</author>
</authors>
<title>Cours de linguistique g´en´erale. Payot,</title>
<date>1962</date>
<location>Paris.</location>
<note>Originally published in</note>
<marker>de Saussure, 1962</marker>
<rawString>de Saussure, Ferdinand. 1962. Cours de linguistique g´en´erale. Payot, Paris. Originally published in 1916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
</authors>
<title>Choosing the word most typical in context using a lexical co-occurrence network. In</title>
<date>1997</date>
<booktitle>ACL,</booktitle>
<pages>507--509</pages>
<contexts>
<context position="34631" citStr="Edmonds, 1997" startWordPosition="5664" endWordPosition="5665"> and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work In this paper, we have presented a graph-theoretic model of the acquisition of lexical syntactic representations and a new exemplar-based evaluation of lexical syntactic acquisition. When applied to the CHILDES corpus, the evaluation shows that the graded syntactic representations learned by the model perform significantly b</context>
</contexts>
<marker>Edmonds, 1997</marker>
<rawString>Edmonds, Philip. 1997. Choosing the word most typical in context using a lexical co-occurrence network. In ACL, pages 507–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Freudenthal</author>
<author>Julian Pine</author>
<author>Fernand Gobet</author>
</authors>
<title>Resolving ambiguities in the extraction of syntactic categories through chunking.</title>
<date>2004</date>
<booktitle>In ICCM.</booktitle>
<contexts>
<context position="12880" citStr="Freudenthal et al. (2004)" startWordPosition="2049" endWordPosition="2052">he phrase brown dog; brown and black stand in a paradigmatic relationship with each other with respect to the position between the and dog in the phrase the X dog. De Saussure’s conceptualization of syntactic relationships captures the fact that both admissible neighbors and admissible substitutes in language are an important part of the characterization of the syntactic properties of a word. We formalize the two relations as distributions over words, where we assume a vocabulary {w1, ... , wV } and V is the number of words in the vocabulary. We denote the left syntagmatic distribution of wi 3Freudenthal et al. (2004) have much the same motivation in introducing an evaluation measure of syntactic acquisition based on chunking. 919 by pi,s,l,m where i is the vocabulary index of wi, s stands for syntagmatic, l for left and m is the order of the distribution as discussed below. Intuitively, pi,s,l,m(wj) is the probability that word wj occurs to the left of wi. Similarly, for the left paradigmatic distribution of wi, pi,p,l,m(wj) is the probability that wj can be substituted for wi without changing local syntactic coherence as far as the context to the left is concerned. Note that we distinguish between left a</context>
</contexts>
<marker>Freudenthal, Pine, Gobet, 2004</marker>
<rawString>Freudenthal, Daniel, Julian Pine, and Fernand Gobet. 2004. Resolving ambiguities in the extraction of syntactic categories through chunking. In ICCM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lila Gleitman</author>
<author>Elissa Newport</author>
</authors>
<title>The invention of language by children: Environmental and biological influences on the acquisition of language.</title>
<date>1995</date>
<editor>In Gleitman, Lila and Mark Liberman, editors,</editor>
<publisher>MIT Press,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="3231" citStr="Gleitman and Newport, 1995" startWordPosition="477" endWordPosition="481"> third word whose syntactic properties are intermediate between the two words (Manning and Sch¨utze, 1999). Clearly exemplar theory is not the only framework in which lexical acquisition has been explored. Gleitman (1990) for example argues for syntactic bootstrapping to infer lexical semantics, work not at odds with our own (see discussion on the role of semantics below). Our argument for the importance of distributional evidence does not call into question the large body of work in child language acquisition that demonstrates that “part of the capacity to learn languages must be ’innate’ ” (Gleitman and Newport, 1995). Tabula rasa learning is not possible. Our goal is not to show that language acquisition proceeds with a minimum of inductive bias. Rather, we attempt to formalize one aspect of language acquisition, the use of distributional information. The paper is organized as follows. Section 2 motivates the exemplar-theoretic approach by reviewing its success in phonetics. Section 3 defines local syntactic coherence, which is the basis for a new evaluation methodology for the acquisition of lexical representations. Section 4 develops the graph-theoretic model. Section 5 compares graded and categorical r</context>
</contexts>
<marker>Gleitman, Newport, 1995</marker>
<rawString>Gleitman, Lila and Elissa Newport. 1995. The invention of language by children: Environmental and biological influences on the acquisition of language. In Gleitman, Lila and Mark Liberman, editors, Language: An invitation to cognitive science. MIT Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lila Gleitman</author>
</authors>
<title>The structural sources of verb meanings.</title>
<date>1990</date>
<journal>Language Acquisition,</journal>
<pages>1--3</pages>
<contexts>
<context position="2825" citStr="Gleitman (1990)" startWordPosition="413" endWordPosition="414">al syntactic representations in which a standard syntactic category like verb or noun is assigned to each word. Our third contribution is to show that, in the context of acquisition, graded representations are superior to standard categorical representations in supporting judgments of local syntactic coherence. A graded representation formalism is one that, for any two words, can represent a third word whose syntactic properties are intermediate between the two words (Manning and Sch¨utze, 1999). Clearly exemplar theory is not the only framework in which lexical acquisition has been explored. Gleitman (1990) for example argues for syntactic bootstrapping to infer lexical semantics, work not at odds with our own (see discussion on the role of semantics below). Our argument for the importance of distributional evidence does not call into question the large body of work in child language acquisition that demonstrates that “part of the capacity to learn languages must be ’innate’ ” (Gleitman and Newport, 1995). Tabula rasa learning is not possible. Our goal is not to show that language acquisition proceeds with a minimum of inductive bias. Rather, we attempt to formalize one aspect of language acquis</context>
</contexts>
<marker>Gleitman, 1990</marker>
<rawString>Gleitman, Lila. 1990. The structural sources of verb meanings. Language Acquisition, 1:3–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen D Goldinger</author>
</authors>
<title>Words and voices— perception and production in an episodic lexicon.</title>
<date>1997</date>
<booktitle>In (Johnson and Mullennix,</booktitle>
<contexts>
<context position="5386" citStr="Goldinger, 1997" startWordPosition="800" endWordPosition="801">challenges the notion that speech is perceived through a process of normalization whereby a speaker-specific representation is mapped or normalized into a speaker-neutral categorical abstraction. Johnson’s model successfully treats aspects of vowel perception, sex identification, and speaker variability. Crucially, no normalization of percepts into categorical representations takes place. The correct identification of phonemes and words in his model is a function of direct comparison to richly detailed exemplars stored in memory. Other examples of exemplar-theoretic phonetic accounts include (Goldinger, 1997), (Pierrehumbert, 2001), and our own work (Sch¨utze et al., 2007). Exemplar theory’s success in phonetics motivates us to investigate its use as a model for local syntactic phenomena. 3 Local syntactic coherence In the context sequence model for exemplartheoretic phonetics (Wade et al., 2008), we represent speech using amplitude envelopes derived from the acoustic signal and then compute similarity as the integral over the correlation of the two acoustic signals. For the syntactic level, we need a representation that has two key properties of the representation we use in phonetics in order to </context>
</contexts>
<marker>Goldinger, 1997</marker>
<rawString>Goldinger, Stephen D. 1997. Words and voices— perception and production in an episodic lexicon. In (Johnson and Mullennix, 1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Hay</author>
<author>Joan Bresnan</author>
</authors>
<title>Spoken syntax: The phonetics of giving a hand in New Zealand English. The Linguistic Review,</title>
<date>2006</date>
<volume>23</volume>
<contexts>
<context position="33464" citStr="Hay and Bresnan, 2006" startWordPosition="5493" endWordPosition="5496">q q s p t t ps sp q t s qt p 0.70 0.75 0.80 0.85 0.90 0.95 accuracy of discrimination 924 We would expect the performance to decrease with higher order eventually since the distributions converge towards p,,,. The fact that this happens so early in this experiment merits further investigation. 7 Related work Data-oriented parsing (Bod et al., 2003) shares basic assumptions about linguistic inference with exemplar-based theory, but it does not model or use the similarity between input and stored exemplars. Previous work on exemplar theory in syntax (AbbotSmith and Tomasello, 2006; Bybee, 2006; Hay and Bresnan, 2006) has not been computational or formal. Previous work on non-categorical representations of words has viewed these representations as an intermediate step for arriving at categorical parts of speech (Redington et al., 1998; Sch¨utze, 1995; Clark, 2003). Consequently, all of these papers evaluate their results by comparing induced categories to gold-standard parts of speech. Redington et al. (1998) did not find a difference in categorization accuracy between simple syntagmatic representation and those using non-adjacent words. The BEAGLE model (Jones and Mewhort, 2007), and related work (Sahlgre</context>
</contexts>
<marker>Hay, Bresnan, 2006</marker>
<rawString>Hay, Jennifer and Joan Bresnan. 2006. Spoken syntax: The phonetics of giving a hand in New Zealand English. The Linguistic Review, 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Johnson</author>
<author>John W Mullennix</author>
<author>editors</author>
</authors>
<date>1997</date>
<booktitle>Talker Variability in Speech Processing.</booktitle>
<publisher>Academic Press.</publisher>
<marker>Johnson, Mullennix, editors, 1997</marker>
<rawString>Johnson, Keith and John W. Mullennix, editors. 1997. Talker Variability in Speech Processing. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Johnson</author>
</authors>
<title>Speech perception without speaker normalization.</title>
<date>1997</date>
<booktitle>In (Johnson and Mullennix,</booktitle>
<contexts>
<context position="4738" citStr="Johnson (1997)" startWordPosition="709" endWordPosition="710">and 8 discuss related and future work, and present our conclusions. 2 Exemplar theory The general idea of research into exemplars in speech production and perception is that encountered items (segments, words, sentences etc.) are stored in great detail in memory along with rich linguistic and extra-linguistic context information. These exemplars are organized into clouds of memory traces with similar traces lying close to each other while dissimilar traces are more distant. A number of such models have had great success in accounting for production and perception phenomena in phonetics. E.g., Johnson (1997) offers an exemplar model which challenges the notion that speech is perceived through a process of normalization whereby a speaker-specific representation is mapped or normalized into a speaker-neutral categorical abstraction. Johnson’s model successfully treats aspects of vowel perception, sex identification, and speaker variability. Crucially, no normalization of percepts into categorical representations takes place. The correct identification of phonemes and words in his model is a function of direct comparison to richly detailed exemplars stored in memory. Other examples of exemplar-theor</context>
</contexts>
<marker>Johnson, 1997</marker>
<rawString>Johnson, Keith. 1997. Speech perception without speaker normalization. In (Johnson and Mullennix, 1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael N Jones</author>
<author>Douglas J K Mewhort</author>
</authors>
<title>Representing word meaning and order information in a composite holographic lexicon. Psychological Review,</title>
<date>2007</date>
<pages>114--1</pages>
<contexts>
<context position="34037" citStr="Jones and Mewhort, 2007" startWordPosition="5577" endWordPosition="5580">omasello, 2006; Bybee, 2006; Hay and Bresnan, 2006) has not been computational or formal. Previous work on non-categorical representations of words has viewed these representations as an intermediate step for arriving at categorical parts of speech (Redington et al., 1998; Sch¨utze, 1995; Clark, 2003). Consequently, all of these papers evaluate their results by comparing induced categories to gold-standard parts of speech. Redington et al. (1998) did not find a difference in categorization accuracy between simple syntagmatic representation and those using non-adjacent words. The BEAGLE model (Jones and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp</context>
</contexts>
<marker>Jones, Mewhort, 2007</marker>
<rawString>Jones, Michael N. and Douglas J.K. Mewhort. 2007. Representing word meaning and order information in a composite holographic lexicon. Psychological Review, 114:1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John G Kemeny</author>
<author>J Laurie Snell</author>
</authors>
<title>Finite Markov Chains.</title>
<date>1976</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="16721" citStr="Kemeny and Snell, 1976" startWordPosition="2704" endWordPosition="2707">Eq.s 1–4 define four Markov chains: pi,s,l,m = (JNJT N)pi,s,l,m−1 (5) pi,p,l,m = (JT NJN)pi,p,l,m−1 (6) pi,s,r,m = (JT NJN)pi,s,r,m−1 (7) pi,p,r,m = (JNJT N)pi,p,r,m−1 (8) It is easy to see that pw is a stationary distribution for Eq. 1–4. Writing x~ for pw, we have: pww(wiwj)pw(wj) = pw(wi) = xi pw(wj) pw(wj) = pw(wi) = xi woman girl boy man sang laughed cried ran V V E pww(wiwj) = E j=1 j=1 V (JN~x)i = E j=1 V (JT N~x)i = E j=1 pw(wj) pww(wjwi) 920 Hence, pw is a solution for Eq.s (5)–(8). The series converge if JNJT N and JT NJN are ergodic, i.e., if the chain is aperiodic and irreducible (Kemeny and Snell, 1976). Observe that for many simple probabilistic context-free grammars (PCFGs) the series in Eq. 1–4 will not converge. For simple PCFGs, the alternation between syntagmatic and paradigmatic distributions is periodic. E.g., if inflected verb forms only occur after nouns and nouns only before inflected verb forms, then the right syntagmatic distributions of nouns will have non-zero activation only for verbs and the right paradigmatic distributions of nouns will have nonzero activation only for nouns, thus preventing convergence.4 The key difference between a simple PCFG and natural language is ambi</context>
</contexts>
<marker>Kemeny, Snell, 1976</marker>
<rawString>Kemeny, John G. and J. Laurie Snell. 1976. Finite Markov Chains. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lemaire</author>
<author>Guy Denhi`ere</author>
</authors>
<title>Effects of high-order co-occurrences on word semantic similarity.</title>
<date>2006</date>
<journal>Behaviour, Brain &amp; Cognition,</journal>
<volume>18</volume>
<issue>1</issue>
<marker>Lemaire, Denhi`ere, 2006</marker>
<rawString>Lemaire, Benoit and Guy Denhi`ere. 2006. Effects of high-order co-occurrences on word semantic similarity. Behaviour, Brain &amp; Cognition, 18(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the Shannon entropy.</title>
<date>1991</date>
<journal>IEEE Trans. Inf. Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>151</pages>
<contexts>
<context position="21497" citStr="Lin, 1991" startWordPosition="3524" endWordPosition="3525">from those that do not; that sentences are represented as sequences of half-words; that syntactic coherence of a sentence is defined as all subsequences of a given length n exhibiting local coherence; and that a subsequence is locally coherent if its distance from a sequence in memory is less than 0. These definitions can be applied to the graph model as follows. A left half-word is a left syntagmatic (or paradigmatic) distribution and a right halfword is a right syntagmatic (or paradigmatic) distribution. We compute the distance of two half-words either as the Jensen-Shannon (JS) divergence (Lin, 1991) or as (1 − cos(α)). JS divergence is more appropriate for the comparison of probability distributions. But the cosine is more efficient when a sparse vector is compared to a dense vector.5 We therefore employ the cosine for the compute-intensive experiments in Section 6. The baseline representation is the categorical representation proposed by Redington et al. (1998). A difficulty in replicating their experiments is that they use hierarchical agglomerative clustering (HAC), which eventually agglomerates all words in a single category. To circumvent the need for a stopping criterion, we repres</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Lin, Jianhua. 1991. Divergence measures based on the Shannon entropy. IEEE Trans. Inf. Theory, 37(1):145– 151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instrumentation, and Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="34616" citStr="Lund and Burgess, 1996" startWordPosition="5660" endWordPosition="5663"> The BEAGLE model (Jones and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work In this paper, we have presented a graph-theoretic model of the acquisition of lexical syntactic representations and a new exemplar-based evaluation of lexical syntactic acquisition. When applied to the CHILDES corpus, the evaluation shows that the graded syntactic representations learned by the model perform </context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Lund, Kevin and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instrumentation, and Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum.</title>
<date>2000</date>
<contexts>
<context position="23482" citStr="MacWhinney, 2000" startWordPosition="3857" endWordPosition="3858">s rise to a precision-recall tradeoff. A small 0 will impose strict requirements on which sequences in memory match, resulting in false negative decisions for local grammaticality. A large 0 will incorrectly judge many locally incoherent sequences to be grammatical. We will pick the optimal 0 in both cases. For categorical representations, this amounts to selecting the HAC dendrogram with optimal performance. The experiment below evaluates whether grammatical and ungrammatical sentences are well separated by the proposed measure.6 Experiment on CHILDES. We used the wellknown CHILDES database (MacWhinney, 2000), a corpus of conversations between young children and their playmates, siblings, and caretakers. In order to avoid mixing varieties of English (e.g., British English vs. American English), we selected the largest homogeneous subcorpus of CHILDES, the Manchester corpus. It contains roughly 350,000 sentences and 1.5 million words. This is a conservative estimate of the amount of child-directed speech a child would receive annually (Redington et al., 1998). All names in the corpus (i.e., all capitalized words) were replaced with a special word “ n ”. A boundary symbol “ b ” was introduced to sep</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>MacWhinney, Brian. 2000. The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Boston, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, Christopher D. and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toben H Mintz</author>
</authors>
<title>Category induction from distributional cues in an artificial language.</title>
<date>2002</date>
<journal>Memory &amp; Cognition,</journal>
<pages>30--678</pages>
<contexts>
<context position="8533" citStr="Mintz, 2002" startWordPosition="1304" endWordPosition="1305">e segments (e.g., consonants and vowels) as the smallest unit; however, this would presume a segmented signal. And segmentation is part of the perception task we want to explain in the first place. Separating left and right neighbors – which amounts to looking at left and right local contexts of each word separately – is the smallest scale we can operate at when doing syntactic matching. We 1A comprehensive account of acquisition must also include morphology. See Christiansen et al. (2004). 2Psycholinguistic evidence for the importance of neighbor information for learning categories includes (Mintz, 2002). 918 choose this small scale for the same reasons as we choose a small scale in phonetics: to ensure maximum flexibility when matching parts of the stimulus with exemplars in memory. Using words, bigrams or larger units would reduce the flexibility in matching and require a larger amount of experience (or training data) to learn a particular generalization. We refer to the representations of left and right contexts of a given word as half-words. In other words, we split a word into two entities, a left halfword that characterizes its behavior to the left and a right half-word that characteriz</context>
</contexts>
<marker>Mintz, 2002</marker>
<rawString>Mintz, Toben H. 2002. Category induction from distributional cues in an artificial language. Memory &amp; Cognition, 30:678–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Pierrehumbert</author>
</authors>
<title>Exemplar dynamics: Word frequency, lenition and contrast.</title>
<date>2001</date>
<booktitle>Frequency and the Emergence of Linguistic Structure,</booktitle>
<pages>137--157</pages>
<editor>In Bybee, Joan and Paul Hopper, editors,</editor>
<publisher>Benjamins.</publisher>
<contexts>
<context position="5409" citStr="Pierrehumbert, 2001" startWordPosition="802" endWordPosition="803">on that speech is perceived through a process of normalization whereby a speaker-specific representation is mapped or normalized into a speaker-neutral categorical abstraction. Johnson’s model successfully treats aspects of vowel perception, sex identification, and speaker variability. Crucially, no normalization of percepts into categorical representations takes place. The correct identification of phonemes and words in his model is a function of direct comparison to richly detailed exemplars stored in memory. Other examples of exemplar-theoretic phonetic accounts include (Goldinger, 1997), (Pierrehumbert, 2001), and our own work (Sch¨utze et al., 2007). Exemplar theory’s success in phonetics motivates us to investigate its use as a model for local syntactic phenomena. 3 Local syntactic coherence In the context sequence model for exemplartheoretic phonetics (Wade et al., 2008), we represent speech using amplitude envelopes derived from the acoustic signal and then compute similarity as the integral over the correlation of the two acoustic signals. For the syntactic level, we need a representation that has two key properties of the representation we use in phonetics in order to support an exemplar-the</context>
</contexts>
<marker>Pierrehumbert, 2001</marker>
<rawString>Pierrehumbert, Janet. 2001. Exemplar dynamics: Word frequency, lenition and contrast. In Bybee, Joan and Paul Hopper, editors, Frequency and the Emergence of Linguistic Structure, pages 137–157. Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>The computation of word associations: comparing syntagmatic and paradigmatic approaches. In Coling.</title>
<date>2002</date>
<contexts>
<context position="34643" citStr="Rapp, 2002" startWordPosition="5666" endWordPosition="5667">007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work In this paper, we have presented a graph-theoretic model of the acquisition of lexical syntactic representations and a new exemplar-based evaluation of lexical syntactic acquisition. When applied to the CHILDES corpus, the evaluation shows that the graded syntactic representations learned by the model perform significantly better than p</context>
</contexts>
<marker>Rapp, 2002</marker>
<rawString>Rapp, Reinhard. 2002. The computation of word associations: comparing syntagmatic and paradigmatic approaches. In Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Redington</author>
<author>Nick Chater</author>
<author>Steven Finch</author>
</authors>
<title>Distributional information: A powerful cue for acquiring syntactic categories.</title>
<date>1998</date>
<journal>Cognitive Science,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="2154" citStr="Redington et al. (1998)" startWordPosition="308" endWordPosition="311">ntactic coherence. Our second contribution is a graph-theoretic model of the acquisition of lexical syntactic representations that is more rigorous than previous heuristic proposals. The graph-theoretic model can learn both categorical and non-categorical (or graded) representations. The model is also a unified framework for syntagmatic and paradigmatic relations (as will be discussed below), and for lowerorder syntactic relations (those that can be directly observed from the input) and higher-order syntactic relations (those that require some generalization from what is directly observable). Redington et al. (1998) give an influential account of the acquisition of lexical syntactic representations in which a standard syntactic category like verb or noun is assigned to each word. Our third contribution is to show that, in the context of acquisition, graded representations are superior to standard categorical representations in supporting judgments of local syntactic coherence. A graded representation formalism is one that, for any two words, can represent a third word whose syntactic properties are intermediate between the two words (Manning and Sch¨utze, 1999). Clearly exemplar theory is not the only fr</context>
<context position="21867" citStr="Redington et al. (1998)" startWordPosition="3583" endWordPosition="3586">follows. A left half-word is a left syntagmatic (or paradigmatic) distribution and a right halfword is a right syntagmatic (or paradigmatic) distribution. We compute the distance of two half-words either as the Jensen-Shannon (JS) divergence (Lin, 1991) or as (1 − cos(α)). JS divergence is more appropriate for the comparison of probability distributions. But the cosine is more efficient when a sparse vector is compared to a dense vector.5 We therefore employ the cosine for the compute-intensive experiments in Section 6. The baseline representation is the categorical representation proposed by Redington et al. (1998). A difficulty in replicating their experiments is that they use hierarchical agglomerative clustering (HAC), which eventually agglomerates all words in a single category. To circumvent the need for a stopping criterion, we represent each word as the temporal sequence of clusters it occurred in during agglomeration and define the distance of two words as the agglomeration step in which the two words are joined in a cluster. E.g., given the agglomeration sequences {1}, {1, 2}, {1, 2, 4}, {1, 2, 3, 4} for w1 and {4}, {4}, {1, 2, 4}, {1, 2, 3, 4} for w4, the distance 5This is so because, when com</context>
<context position="23940" citStr="Redington et al., 1998" startWordPosition="3925" endWordPosition="3928">grammatical and ungrammatical sentences are well separated by the proposed measure.6 Experiment on CHILDES. We used the wellknown CHILDES database (MacWhinney, 2000), a corpus of conversations between young children and their playmates, siblings, and caretakers. In order to avoid mixing varieties of English (e.g., British English vs. American English), we selected the largest homogeneous subcorpus of CHILDES, the Manchester corpus. It contains roughly 350,000 sentences and 1.5 million words. This is a conservative estimate of the amount of child-directed speech a child would receive annually (Redington et al., 1998). All names in the corpus (i.e., all capitalized words) were replaced with a special word “ n ”. A boundary symbol “ b ” was introduced to separate sentences. The representation of the corpus is then a concatenation of all its sentences. The vocabulary consists of V = 8601 words. Construction of the evaluation set. We tested the ability of the two models to distinguish locally coherent vs. incoherent sentences by selecting 100 unattested sentences from the corpus, which were not used to train the model. We only selected unattested sentences that were not a substring of a sentence in the traini</context>
<context position="28069" citStr="Redington et al., 1998" startWordPosition="4583" endWordPosition="4586">ions in language acquisition, which is one of the goals of the paper. Difficult sentences (those with rare words and greater length) are overrepresented in the test set as the discrimination of short sentences containing only frequent words can easily be done by simplistic models. Thus, a test set of “easy” sentences would not distinguish good models from bad models. Discrimination experiment. In order to train the graph model, the entries of matrix J were estimated using maximum likelihood based on the training corpus. pi,s,l,1 and pi,s,r,1 were then computed for all 8601 words. Replicating (Redington et al., 1998), the most frequent 1000 words were clustered (using single-link HAC, Manning and Sch¨utze (1999)). For each remaining word w, the closest neighbor w′ in the 1000 most frequent words was determined and w was then assigned to the cluster of w′. Fig. 3 shows the performance of graded and categorical representations for different subsequence sizes n. To compute the accuracy for each n, the θ with optimal discrimination performance was chosen (for both graded and categorical). For a subsequence of size n = 1, the performance is 0.5 in both cases since the 200-sentence test set does not contain unk</context>
<context position="33685" citStr="Redington et al., 1998" startWordPosition="5527" endWordPosition="5530">at this happens so early in this experiment merits further investigation. 7 Related work Data-oriented parsing (Bod et al., 2003) shares basic assumptions about linguistic inference with exemplar-based theory, but it does not model or use the similarity between input and stored exemplars. Previous work on exemplar theory in syntax (AbbotSmith and Tomasello, 2006; Bybee, 2006; Hay and Bresnan, 2006) has not been computational or formal. Previous work on non-categorical representations of words has viewed these representations as an intermediate step for arriving at categorical parts of speech (Redington et al., 1998; Sch¨utze, 1995; Clark, 2003). Consequently, all of these papers evaluate their results by comparing induced categories to gold-standard parts of speech. Redington et al. (1998) did not find a difference in categorization accuracy between simple syntagmatic representation and those using non-adjacent words. The BEAGLE model (Jones and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationsh</context>
</contexts>
<marker>Redington, Chater, Finch, 1998</marker>
<rawString>Redington, Martin, Nick Chater, and Steven Finch. 1998. Distributional information: A powerful cue for acquiring syntactic categories. Cognitive Science, 22(4):425–469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
<author>Anders Holst</author>
<author>Jussi Karlgren</author>
</authors>
<title>Permutations as a means to encode order in word space.</title>
<date>2008</date>
<booktitle>In CogSci.</booktitle>
<contexts>
<context position="34079" citStr="Sahlgren et al., 2008" startWordPosition="5584" endWordPosition="5587">, 2006) has not been computational or formal. Previous work on non-categorical representations of words has viewed these representations as an intermediate step for arriving at categorical parts of speech (Redington et al., 1998; Sch¨utze, 1995; Clark, 2003). Consequently, all of these papers evaluate their results by comparing induced categories to gold-standard parts of speech. Redington et al. (1998) did not find a difference in categorization accuracy between simple syntagmatic representation and those using non-adjacent words. The BEAGLE model (Jones and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and </context>
</contexts>
<marker>Sahlgren, Holst, Karlgren, 2008</marker>
<rawString>Sahlgren, Magnus, Anders Holst, and Jussi Karlgren. 2008. Permutations as a means to encode order in word space. In CogSci.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Saul</author>
<author>Fernando Pereira</author>
</authors>
<title>Aggregate and mixed-order markov models for statistical language processing.</title>
<date>1997</date>
<booktitle>In EMNLP,</booktitle>
<pages>81--89</pages>
<marker>Saul, Pereira, 1997</marker>
<rawString>Saul, Lawrence and Fernando Pereira. 1997. Aggregate and mixed-order markov models for statistical language processing. In EMNLP, pages 81–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
<author>Jan Pedersen</author>
</authors>
<title>A vector model for syntagmatic and paradigmatic relatedness.</title>
<date>1993</date>
<booktitle>In UW Centre for the New OED and Text Research.</booktitle>
<marker>Sch¨utze, Pedersen, 1993</marker>
<rawString>Sch¨utze, Hinrich and Jan Pedersen. 1993. A vector model for syntagmatic and paradigmatic relatedness. In UW Centre for the New OED and Text Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
<author>Michael Walsh</author>
<author>Travis Wade</author>
<author>Bernd M¨obius</author>
</authors>
<title>Towards a unified exemplartheoretic model of phonetic and syntactic phenomena. In CogSci, Poster Session.</title>
<date>2007</date>
<marker>Sch¨utze, Walsh, Wade, M¨obius, 2007</marker>
<rawString>Sch¨utze, Hinrich, Michael Walsh, Travis Wade, and Bernd M¨obius. 2007. Towards a unified exemplartheoretic model of phonetic and syntactic phenomena. In CogSci, Poster Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In EACL,</booktitle>
<pages>141--148</pages>
<marker>Sch¨utze, 1995</marker>
<rawString>Sch¨utze, Hinrich. 1995. Distributional part-of-speech tagging. In EACL, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning random walk models for inducing word dependency distributions.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="34720" citStr="Toutanova et al. (2004)" startWordPosition="5676" endWordPosition="5679">nce information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work In this paper, we have presented a graph-theoretic model of the acquisition of lexical syntactic representations and a new exemplar-based evaluation of lexical syntactic acquisition. When applied to the CHILDES corpus, the evaluation shows that the graded syntactic representations learned by the model perform significantly better than previously proposed categorical representations. An initial evaluation of high</context>
</contexts>
<marker>Toutanova, Manning, Ng, 2004</marker>
<rawString>Toutanova, Kristina, Christopher D. Manning, and Andrew Y. Ng. 2004. Learning random walk models for inducing word dependency distributions. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Travis Wade</author>
<author>Grzegorz Dogil</author>
<author>Hinrich Sch¨utze</author>
<author>Michael Walsh</author>
<author>Bernd M¨obius</author>
</authors>
<title>Syllable frequency effects in a context-sensitive segment production model.</title>
<date>2008</date>
<note>Submitted.</note>
<marker>Wade, Dogil, Sch¨utze, Walsh, M¨obius, 2008</marker>
<rawString>Wade, Travis, Grzegorz Dogil, Hinrich Sch¨utze, Michael Walsh, and Bernd M¨obius. 2008. Syllable frequency effects in a context-sensitive segment production model. Submitted.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>