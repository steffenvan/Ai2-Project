<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002808">
<title confidence="0.902006">
Conversational Implicatures
</title>
<author confidence="0.982802">
Robert van Rooy*
</author>
<affiliation confidence="0.996915">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<address confidence="0.784735">
Nieuwe Doelenstraat 15, 1012 CP Amsterdam
</address>
<email confidence="0.994743">
vanrooy@hum.uva.nl
</email>
<sectionHeader confidence="0.99652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999175">
According to standard pragmat-
ics, we should account for conver-
sational implicatures in terms of
Grice&apos;s (1975) maxims of conversa-
tion. Neo-Griceans like Atlas &amp;
Levinson (1981) and Horn (1984)
seek to reduce those maxims to the
so-called Q and I-principles. In
this paper I want to argue that (i)
there are major problems for reduc-
ing Gricean pragmatics to these two
principles, and (ii) that, in fact, we&apos;d
better account for implicatures in
terms of the principles of (a) opti-
mal relevance and (b) optimal cod-
ing. To formulate both, I will make
use of Shannon&apos;s (1948) mathemat-
ical theory of communication.
</bodyText>
<sectionHeader confidence="0.99873" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989226">
Natural language is efficient in the sense that
a single message can convey different seman-
tic contents in different contexts. And indeed,
recent trends in semantics (e.g. optimality
theoretic semantics) suggest that the actual
interpretation of an utterance is highly un-
derspecified by the conventional meanings of
the sentence that is used. This requires, how-
ever, that language users have robust ways
to resolve the underspecification and/or am-
biguity. In this paper I will discuss two ways
This research has been made possible by a fellow-
ship of the Royal Netherlands Academy of Arts and
Sciences.
of doing this. First, one where the particular
conversational situation is important; second,
one which depends on more general conven-
tions.
</bodyText>
<sectionHeader confidence="0.55827" genericHeader="method">
2 The Q and I principle
</sectionHeader>
<bodyText confidence="0.9997103125">
Neo-Gricean pragmatics seeks to reduce
Grice&apos;s maxims of conversation to the so-
called Q and I principles. Both are used
to account for many conversational implica-
tures. The Q-principle (implementing Grice&apos;s
first maxim of Quantity) advises the speaker
to say as much as he can to fulfill his com-
municative goals, while the I-principle (im-
plementing Grice&apos;s other maxims, except for
quality) advises the speaker to say no more
than he must to fulfill these goals. Both prin-
ciples help to strengthen what is communi-
cated by a sentence. The Q-principle induces
inferences from the use of one expression to
the assumption that the speaker did not in-
tend to communicate a contrasting, and in-
formationally stronger, one. This principle
is thus essentially metalinguistic in kind, and
accounts for both `scalar&apos; and `clausal&apos; impli-
catures. It allows us, for instance, to conclude
from &amp;quot;John ate some of the cookies&amp;quot; to &amp;quot;John
didn&apos;t eat all of the cookies&amp;quot; (scalar implica-
ture), and from &amp;quot;A or B&amp;quot; to &amp;quot;A or B, but not
both&amp;quot; (clausal + scalar implicature). The I-
principle allows us to infer from the use of an
expression to its most informative or stereo-
typical interpretation. It is used, for instance,
to enrich the interpretation of a conjunction
to a temporal sequential, or causal, relation,
and it allows us to interpret a conditional like
`John walks, if Mary walks&apos; as the bicondi-
tional `John walks if and only Mary walks&apos;.
</bodyText>
<sectionHeader confidence="0.972817" genericHeader="method">
3 Problems for the Q and I
principles
</sectionHeader>
<subsectionHeader confidence="0.998008">
3.1 Too general
</subsectionHeader>
<bodyText confidence="0.999991672413793">
Although the Q and I principles are intu-
itively appealing, they give rise to a num-
ber of conceptual and empirical problems.
Let&apos;s start with some cases where it is pre-
dicted that Q-implicatures arise, although in
fact they don&apos;t. First, at least when imple-
mented as Gazdar (1979) did, we can derive
from the existential &amp;quot;Someone is sick&amp;quot; as a
Q-implicature that (the speaker knows that)
a is not sick, for any individual a. Second,
on the assumption that scales are defined in
terms of entailment, it is predicted that we
can infer from `B, if A&apos; to the conclusion that
it is not the case that the stronger `B if and
only if A&apos; holds, although in a lot of situa-
tions this is exactly what we can conclude.
Third, on the same assumption, it is incor-
rectly predicted that we can infer `not regret
A&apos; from `know A&apos;. Horn, Levinson and oth-
ers have argued that these problems can be
prevented by (i) weakening the force of Q-
implicatures from know-not to not-know (for
the first problem), and by putting constraints
on what counts as contrastive expressions:
contrastive expressions must be lexical items
(second problem) and must have the same
presuppositions (for the third). Although it
can be argued that for the biconditional in-
terpretation this — somewhat ad hoc — solu-
tion solves the second problem, Gazdar (1979)
argued that the constraints doesn&apos;t solve the
third one. Moreover, the most serious prob-
lematic cases where Q-implicatures overgen-
erate cannot be explained away in this way:
The Horn/Gazdar/Levinson/Atlas analysis of
Q-implicatures as generalized conversational
implicatures (PCIs) triggered solely by lexi-
cal expressions cannot explain why from A&apos;s
answer &amp;quot;John has 2 children&amp;quot; to Q&apos;s question
&amp;quot;Who has 2 children?&amp;quot; the implicature &amp;quot;John
has only 2 children&amp;quot; does not even arise as a
default (cf. van Kuppevelt). This latter ex-
ample seems to suggest that these so-called
Q-implicatures are, after all, dependent on
the conversational situation, in particular on
the question being asked. Proponents of the
Q and I pragmatics (Horn, Levinson), fol-
lowed by Matsumoto (1995), argue that in
such particular conversational situations the
generalized conversational implicature is can-
celled, for reasons of relevance: The answer
is already informative enough for the purpose
of the conversation. I will argue, however,
that informativity is, in general, not the cru-
cial issue, and that it is much more natural to
assume that — for reasons of relevance in this
particular situation — the (potential) implica-
ture does not even arise.
</bodyText>
<subsectionHeader confidence="0.99319">
3.2 Not general enough
</subsectionHeader>
<bodyText confidence="0.999961859649123">
Not only does the standard analysis of Q-
implicatures overgeneralize, it also doesn&apos;t
seem to be general enough. First, as discussed
extensively by Hirschberg (1985), the stan-
dard analysis is of no help to account for cer-
tain examples that intuitively should be an-
alyzed as scalar implicatures. If Mary&apos;s po-
tential new boss asks her at her job-interview
whether she speaks French, and she answers
by saying &amp;quot;My sister does&amp;quot;, he can conclude
that Mary herself does not. The standard
analysis fails to account for this, because (a)
scalar implicatures are all analyzed in terms of
the Q-principle, (b) the Q-principle is stated
in terms of informativitg, but (c) the propo-
sition that Mary speaks French is not more
informative (i.e. entails) than the proposi-
tion that her sister does. This example sug-
gests (i) that scalar implicatures should not
exclusively be accounted for in terms of infor-
mativity, and (ii) that just like in the previ-
ous example, also here the relevant implica-
ture crucially depends on the conversational
situation (i.e. the beliefs and preferences of
the agents involved). Second, as discussed by
McCawley (1993), the implicatures generated
by the (and, or) scale cannot account for the
fact that a sentence of the form `A or B or
C gives rise to the inference that only one of
the three is true. A final example where the
standard analysis of Q-implicatures isn&apos;t gen-
eral enough was discussed by Groenendijk &amp;
Stokhof (1984). They observe that when A
answers Q&apos;s question &amp;quot;Who comes?&amp;quot; by say-
ing &amp;quot;Peter comes&amp;quot;, we typically interpret the
answer as being exhaustive. That is, we inter-
pret A&apos;s answer as &amp;quot;Only Peter comes&amp;quot;. They
claim that this kind of inference should intu-
itively be accounted for in terms of Grice&apos;s
maxim of Quantity (as a Q-implicature), but
note that the standard implementation does
not predict the exhaustivity of the answer.
Still, it seems that the exhaustive interpre-
tation of the answer should be derived by
Gricean pragmatics on the assumption that
answers are as informative as the question re-
quires.
I conclude that the scales relevant for the
implicatures depend on the conversational sit-
uation (i.e. question asked) and the beliefs
and preferences of the agents involved is in
correspondence with Hirschberg&apos;s claim that
scales are dependent on context. However, we
would like to say something more; we would
also like to say how the relevant scale depends
on the question asked and the relevant beliefs
and desires.
</bodyText>
<sectionHeader confidence="0.999077" genericHeader="method">
4 Relevance
</sectionHeader>
<bodyText confidence="0.999986647058824">
In this respect, important progress has been
made recently by Merin (1997). Following the
lead of Anscombre &amp; Ducrot (1983), Merin ar-
gues that scales should be defined not in terms
of informativity, but rather in terms of a no-
tion of relevance. The relevance of a propo-
sition is determined in terms of the argu-
mentative force the proposition would have in
that particular conversational situation. The
relevance of an assertion is then defined in
information/decision/game theoretical terms,
based on the assumption that the partici-
pants of the conversation have strictly oppos-
ing preferences, i.e. that the participants play
a zero-sum game.
Although Merin convincingly shows that
some scalar implicatures (in particular the
Hirschberg examples) can be accounted for
appropriately on the assumption that play-
ers argue for particular hypotheses, and that
their contribution should be interpreted in the
most relevant way (i.e. strongest argument),
it is intuitively clear that not all conversa-
tions can, and should, be modeled as zero-sum
games. It makes little sense, for instance, to
assume that the exhaustive interpretation of
&amp;quot;John has 2 children&amp;quot; as answer to the ques-
tion &amp;quot;How many children does John have?&amp;quot;
can be explained in terms of opposing pref-
erences between questioner and answerer, for
the latter typically cooperates with the for-
mer. What is called for, then, is a gener-
alization of Merin&apos;s notion of relevance that
also measures the relevance of propositions
in cooperative conversational situations. It
seems only natural, on the assumption that
speakers are relevance optimizers, that once
we can define such a measure, not only the
typical Q-implicatures can be accounted for in
terms of relevance, but also the I-implicatures
from conditional to biconditional, and Groe-
nendijk &amp; Stokhof&apos;s (1984) observation that
answers are normally interpreted in an ex-
haustive way. As we will see in the next sec-
tion, Groenendijk &amp; Stokhof (1984) show that
almost all typical Q-implicatures can be an-
alyzed alternatively in terms of their explicit
exhaustivity-operator, without giving rise to
the above discussed overgeneralizations, when
the clause that gives rise to the implicature is
used as an answer to a question.
</bodyText>
<sectionHeader confidence="0.986525" genericHeader="method">
5 Exhaustified answers
</sectionHeader>
<bodyText confidence="0.998469625">
Groenendijk &amp; Stokhof (1984) propose to ac-
count for the intuition that answer Peter
comes to question Who comes? should nor-
mally be read exhaustively by introducing an
explicit exhaustivity operator that is applied
to answers and the abstracts underlying the
questions to derive the exhaustive interpreta-
tion.
exh = ARAP[R(P)A,3P&apos;[R(P&apos;)AP =�
P&apos; A dx[P&apos;(x) —� P(x)111
This exhaustivity operator accounts for
many of the implicatures traditionally ac-
counted for in terms of Grice&apos;s maxim of
quantity. First, it obviously accounts for the
fact that when Who comes? is answered by
John we conclude that only John comes. Sec-
ond, when answer A man is given we can con-
clude that not all men come, an implicature
standardly triggered by the (all, some) scale.
In contrast to Gazdar&apos;s analysis of scalar im-
plicatures, however, our analysis does not give
rise to the wrong prediction that John is not
coming: the exhaustive reading of A man is
coming as answer to question Who comes? is
compatible with the fact that John is. Note
that this analysis, in distinction with the stan-
dard analysis of scalar implicatures, works
also well when more than one item gives rise
to an implicature. From the exhaustive inter-
pretation of the term Some of the bacon and
some of the eggs given as answer to the ques-
tion What did Mary ate? we can conclude
that Mary didn&apos;t eat all of the bacon, and
that she didn&apos;t eat all of the beans, just like
we should.
Notice that our exhaustification analysis
not only predicts intuitions standardly ac-
counted for in terms of the Q principle; also
some I-implicatures are accounted for. If the
question is Who quacks? the answer Every
duck quacks is predicted to mean that every
quacker is a duck. Horn (2000) calls this in-
ference conversion and explicitly proposes to
account for it in terms of the I-principle.
Similarly, if we allow for explicit quantifi-
cation over worlds, we can account for the in-
ference from (1b) to (1c), when the former is
given as answer to (1a):
</bodyText>
<listItem confidence="0.994427">
(1) a. Q: Did John walk?
b. A: If Mary talked.
c. John walked iff Mary talked.
</listItem>
<bodyText confidence="0.9995406">
We assume that the property underlying
a question like (1a) is Aw.Walk(j)(w), and
that answer (1b) should be represented by
AP.dw[Talk(m)(w) —� P(w)] which after ex-
haustification means that Mary talked iff
John walked.&apos;
This inference is accounted for by Groenendijk &amp;
Stokhof (1984) in terms of their generalized exhaustiv-
ity operator without using explicit quantification over
worlds. Such an analysis cannot account, however,
for the exclusive reading of disjunctive sentences with
more than two disjuncts, to be discussed below.
Our approach also predicts that (2a) should
be read as (2b) when the color of the flag is
at issue.
</bodyText>
<listItem confidence="0.738275">
(2) a. The flag is red.
b. The flag is all red.
</listItem>
<bodyText confidence="0.964738130434783">
This inference is normally (e.g. Atlas &amp;
Levinson, 1981) accounted for by assuming
that (2a) should be interpreted as informative
as possible. But then it should be explained
why in certain circumstances the inference is
absent. When 3 flags are mutually known by
us to be all white except for a small block of
some other distinguishing color (being either
red, yellow or green), and I ask you to identify
the flag you hold behind your back, your an-
swer (2a) satisfies me, and I do not imply that
(2b) is true. The standard analysis has to as-
sume that in these cases the triggered general-
ized implicature are cancelled, while we don&apos;t
even generate the implicature because we can
assume that the implicit question was some-
thing like What is the color of the small block?
Indeed, our topic-dependent analysis of
`scalar&apos; implicatures prevents us from trig-
gering implicatures to be cancelled later for
reasons of relevance (see also van Kuppevelt
(1997) and Carstyn (ms)). Consider the fol-
lowing example again:
</bodyText>
<listItem confidence="0.99618925">
(3) a. Q: Who has 2 children?
b. A: John has 2 children.
c. John doesn&apos;t have more than 2 chil-
dren.
</listItem>
<bodyText confidence="0.98465404">
Instead of saying that (3b) triggers the po-
tential implicature (3c) that is cancelled when
the former is given as answer to question (3a),
our analysis predicts that the implicature is
not even triggered, because (3b) completely
answers (3a).
A similar analysis can be given for the fact
that a disjunctive sentence sometimes gets an
exclusive reading and sometimes not. If we
allow for explicit quantification over worlds,
we can represent an answer like A or B or
C in terms of an existential quantifier as fol-
lows: AP.3w[(A(w)VB(w)VC(w))AP(w)]. If
we now assume that this sentence is given as
answer to the question `What proposition(s)
is/are true?&apos;, exhaustivity has the effect that
only worlds count that make just one of the
three propositions true, resulting in the ex-
clusive reading.
However, this analysis does not have the re-
sult that a disjunctive sentence should always
have the exclusive reading. In particular this
is rightly predicted not to be the case in (4),
where the complex sentence is given as an (ex-
haustive) polar answer:
</bodyText>
<listItem confidence="0.9986275">
(4) Q: Are the cookies or the chocolates in
the box?
</listItem>
<bodyText confidence="0.605888571428571">
A: Yes, the cookies or the chocolates are
in the box.
Something similar is the case with condi-
tional answers. Also after exhaustification
they don&apos;t get a bi-conditional interpretation
when they are used as complete answers to
polar questions:
</bodyText>
<listItem confidence="0.982468333333333">
(5) Q: Did John walk, if Mary talked?
A: Yes, John walked if Mary talked.
6 Relevance and Exhaustivity
</listItem>
<bodyText confidence="0.999941636363636">
In the previous section we have seen that
many so-called `quantity&apos; implicatures trig-
gered by sentences can be accounted for by
assuming that these sentences should be in-
terpreted as exhaustive answers to questions.
However, we would like to say something
more; we would also like to give an inde-
pendent motivation for why answers should
normally be interpreted exhaustively. Notice
that Groenendijk &amp; Stokhof&apos;s (1984) stipu-
lation that answers should always be inter-
preted exhaustivily would not only be ad hoc,
it would also give rise to counterexamples.
Most importantly, it would predict incorrectly
for so-called mention-some questions. Some-
times an assertion intuitively answers a ques-
tion completely without being read exhaus-
tively. To illustrate, when I ask you (6a) and
you answer by saying (6b), I am satisfied, al-
though I don&apos;t interpret your answer as claim-
ing that this is the only place where I can buy
an Italian newspaper.
</bodyText>
<listItem confidence="0.976000333333333">
(6) a. Where can I buy an Italian newspa-
per?
b. Around the corner.
</listItem>
<subsectionHeader confidence="0.941761">
6.1 Topic dependent relevance
</subsectionHeader>
<bodyText confidence="0.9998433">
In cooperative dialogues the relevance of com-
municative acts can be determined with re-
spect to decision problems (cf. van Rooy
(2001). A decision problem Using commu-
nication theory we can model these decision
problems by partitions of the logical space —
, i.e., the semantic questions of Groenendijk
&amp; Stokhof (1984). One proposition will then
be more relevant than another when it helps
more to resolve the question.
Intuitively, we would like to say that asser-
tions are relevant with respect to this decision
problem if the decision is easier to make af-
ter an assertion is learned. But to account
for this, we have to measure the difficulty of
the decision. A standard way to do this is in
terms of entropy.
Given a probability function P, we can de-
fine the entropy of decision problem Q as fol-
lows:
</bodyText>
<equation confidence="0.989618">
E(Q) = 1: P(q) x —lo92P(q)
qEQ
</equation>
<bodyText confidence="0.999293">
When our agent learns proposition A, we
can determine the entropy of decision prob-
lem Q conditional on learning A, EA(Q), as
follows:
</bodyText>
<equation confidence="0.997601">
EA(Q) = E P(q/A) x —lo92P(q/A)
qEQ
</equation>
<bodyText confidence="0.9777218">
In terms of this notion we can now define what
might be called the Relevance of proposition
A, with respect to partition Q, RQ(A), as
the reduction of entropy, or uncertainty, of
Q when A is learned:2
</bodyText>
<equation confidence="0.98665">
RQ(A) = E(Q) — EA(Q)
</equation>
<bodyText confidence="0.9985705">
Relevance will be used to determine the ac-
tual interpretation of a sentence underspeci-
fied by its conventional meaning. We will say
This notion was used by Lindley (1956) already to
measure the informational value of a particular result
of an experiment.
</bodyText>
<listItem confidence="0.7993295">
that interpretation A is better than interpre-
tation B, A &gt; B, iff RQ(A) &gt; RQ(B) with
respect to all probability functions for which
Q has maximal entropy.
</listItem>
<bodyText confidence="0.996288090909091">
It might be, of course, that for some proba-
bility distributions A is better, while for oth-
ers B is. Which one is then preferred? In
those cases, I propose, interpretation A is bet-
ter if the sentence `gives rise&apos; to a new ques-
tion, Q&apos;, which is orthogonal to Q, such that
after learning A, but not after learning B, ev-
ery complete answer to Q&apos; also completely an-
swers Q. This indirect notion of relevance will
be crucial to account for the implicatures of
disjunctive and conditional sentences.
</bodyText>
<subsectionHeader confidence="0.96326">
6.2 Why Exhaustify
</subsectionHeader>
<bodyText confidence="0.968050592592593">
Consider question (7):
(7) Whom of John and Bill are sick?
This question gives rise to a partition with
4 cells. Assuming that the probability that
John is sick equals the probability that Bill is
sick, but that the sickness of the one is inde-
pendent of the other, it is easy to see that the
entropy of the question is 2: the question im-
plicitly asks for answers to two independent
binary questions. Notice that after learning
that (At least) John is sick the entropy of the
question reduces to 1, which means that the
relevance of this answer is 2 - 1 = 1. Af-
ter learning of each of John and Bill whether
they are sick, however, the question/decision
problem is resolved: the entropy reduces to 0,
and the reduction of entropy, the relevance of
an answer like John and Bill are sick, is 2 -
0 = 2. Thus, for an answer to have maximal
relevance, it should say of each individual in
the domain of quantification whether that in-
dividual is sick or not. It should be obvious
that this means that complete, or exhaustive,
answers to questions are always at least as
relevant as partial answers.
Now consider answers (8a) and (8b) to
question (7)
</bodyText>
<listItem confidence="0.9855975">
(8) a. John is sick.
b. A man is sick.
</listItem>
<bodyText confidence="0.9990102">
What is the relevance of these answers,
i.e., in how far do these answers reduce the
entropy of the question? That depends on
how we interpret them. If we interpret them
non-exhaustively, the conditional entropy of
</bodyText>
<equation confidence="0.980053">
(7) given (8a) is (P(J ^ B=J) x —log2P(J ^
B=J))+(P(J^:B=J)x-log2P(J^:B=J)) =
((12 ��log22)+(1
1 2 ��log22)) = �Log2
1 2 = 1.
1
</equation>
<bodyText confidence="0.935264">
Similarly, given that John and Bill are the
only men, the conditional entropy of (7) given
</bodyText>
<equation confidence="0.9751795">
(8b) is (P(J ^ B=J _ B) x —log2P(J ^ B=J _
B))+(P(J^:B=J_B)x-log2(P(J^:B=J_
B)) + (P(:J ^ B=J _ B) x —log2(P(:J ^
B=J _ B)) = 3 x (13 x —log2 3) = �log2
1 3 &lt; 1.
1
</equation>
<bodyText confidence="0.999346435294118">
The relevance of these two answers accord-
ing to their non-exhaustive interpretation are
thus 1 and something less than 1, respec-
tively. What if we interpret the answers ex-
haustively? That is, what is the reduction
of entropy if we assume that the propositions
expressed by the answers are determined af-
ter we have applied the exhaustivity operator
to (8a) and (8b), respectively? After exhaus-
tification, answer (8a) really means John is
sick and Bill is not, and after this informa-
tion is received the entropy reduces from 2
to 0; its relevance is thus 2. Similarly, an-
swer (8b) really means that either only John
is sick or that only Bill is sick, and this new in-
formation reduces the entropy of the original
question from 2 to 1. The important fact to
note here is that in both cases the reduction
of entropy of the answer under its exhaustive
interpretation is higher than the reduction of
entropy under its non-exhaustive interpreta-
tion. And this is in general the case: most
answers have a higher relevance on their ex-
haustive reading than on their non-exhaustive
reading. On the assumption that speakers are
relevance maximizers this means that in case
answerers are expected to be cooperative we
should interpret these answers exhaustively.
For disjunctive and conditional sentences
we have to look at our indirect method. If the
question is whether A is the case, A?, and the
answer Yes, or B, it might be the case that
the entropy decreases more on the inclusive
reading than on the exclusive reading. Some-
thing similar happens with respect to the con-
ditional and biconditional interpretations of
answer If B. It is natural to assume, how-
ever, that both questions `give rise&apos; to another
question: B?. Only on the exclusive and bi-
conditional interpretation of the two answers
every answer to the second question will also
resolve the original question whether A is the
case. For this reason, the exclusive and bi-
conditional interpretations are preferred.
Above, we have criticized Groenendijk &amp;
Stokhof&apos;s (1984) assumption that in case an-
swers are not explicitly marked as being par-
tial answers, we should always read them ex-
haustively. One complaint was that this as-
sumption is just an ad hoc stipulation. Groe-
nendijk &amp; Stokhof agree, and explicitly regret
that they see no way to derive exhaustifica-
tion from the Gricean maxims of conversa-
tion, in particular not from Grice&apos;s maxim of
quantity. This complaint can now be met: I
have shown in this section that we can mo-
tivate the assumption that answers should
be read exhaustively by deriving it from the
much more general assumption that speakers
are relevance optimizers.
What about the other complaint I men-
tioned earlier? As noted above, an answer like
Around the corner intuitively resolves ques-
tion Where can I buy an Italian newspaper?
although it does not suggest that you can buy
an Italian newspaper around the corner only.
Fortunately, however, also the problematic
mention-some phenomena can be accounted
for when we assume that speakers are rele-
vance optimizers. In van Rooy (to appear) I
argue that mention-some questions are asked,
or mention-some answers are given, only in
very particular circumstances, and show that
in these circumstances the utility, or rele-
vance, of mention-some questions/answers co-
incide with their mention-all alternatives. For
reasons of economy, mention-some readings
are in these circumstances preferred. We
can conclude that although in normal circum-
stances the exhaustive reading of an answer is
more relevant than its non-exhaustive coun-
terpart, in special circumstances it is not. As
a result, we can explain that for reasons of op-
timizing relevance, exhaustification does not
always take place.
</bodyText>
<sectionHeader confidence="0.950185" genericHeader="conclusions">
7 Explaining markedness
</sectionHeader>
<subsectionHeader confidence="0.970847">
7.1 Horn&apos;s division of labor
</subsectionHeader>
<bodyText confidence="0.999974803921569">
Consider a typical case of communication
where two meanings m1 and m2 can be
expressed by two linguistic representations
r1 and r2. In principle this gives rise to
two possible codings: {(r1, m1), (r2, m2)} and
{(r1, m2), (r2, m1)}. In many communicative
situations, however, the underspecification
does not really exist, and is resolved due to
the general pragmatic principle that a lighter
form will be interpreted by a more salient,
or stereotypical, meaning: (i) It is a general
defeasible principle, for instance, in centering
theory that if a certain object/expression is
referred to be a pronoun, another more salient
object/expression should be referred to by a
pronoun too; (ii) Reinhard (1983) and Levin-
son (1987) seek to reduce Chomsky&apos;s B and C
principles of the binding theory to pragmat-
ics maxims. In particular, disjoint reference
of lexical NPs throughout the sentence is ex-
plained by pointing to the possibility of the
use of a lighter expression, viz. an anaphor
or pronoun; (iii) The preference for bridging
(Clark &amp; Haviland, 1977) and stereotypical
interpretations (Atlas &amp; Levinson, 1981); (iv)
and perhaps most obviously, Horn&apos;s (1984) di-
vision of pragmatic labor according to which
marked expression (morphologically complex
and less lexicalized) typically get a marked
meaning (cf. John made the car stop versus
John stopped the car and consider also the fact
that stressed pronouns can pick up less salient
objects). In neo-Gricean pragmatics proposed
by Atlas, Horn and Levinson, this principle is
explained through the interaction of the so-
called Q and I principles, and has recently
been incorporated in (bi-directional) optimal-
ity theory by Blutner (2000) and reformulated
in terms of game theory by Dekker &amp; van
Rooy (2000). However, as we have seen above,
explanations based on the Q and I princi-
ples are very shaky: these principles tend to
clash with one another, and it is not always
clear how to resolve this clash. In particular,
it&apos;s unclear under which circumstances which
principle should be used to explain the phe-
nomena. I will show that by thinking of lan-
guage as an efficient coding system the princi-
ple that lighter expressions get a more salient
meaning can be given a straightforward ex-
planation.
</bodyText>
<subsectionHeader confidence="0.992382">
7.2 Optimal coding of information
</subsectionHeader>
<bodyText confidence="0.999976375">
The question that started information the-
ory was: how can we send messages over a
channel as quickly as possible without distor-
tion? The answer is: by looking for the opti-
mal coding; to represent the data in a way as
comprehensive as possible. Suppose we have
a source (without memory) that sends mes-
sages from a set U = {u1, ..., un} in terms
of codes built up from codesymbols belong-
ing to the code-alphabet S = {s1, ..., sn}. A
source code, or coding system, C, is defined as
a function from U to S*, where`*&apos; is Kleene&apos;s
star. For example, C(red) = 00, C(white) =
11, C(blue) = 10, C(orange) = 01 is a source
code for U = {red, white, blue, orange} with
alphabet S = {0, 1}. Of course, the source
and alphabet allow for many different codes.
Intuitively, however, some codings are more
efficient than others. What is the best coding
system? The coding system with the short-
est expected length. The crucial insight of
Shannon (1948) was that this expected length
depends not only on the length of the mes-
sages after encoding, but also on the proba-
bility with which the messages are sent. Sup-
pose that function P assigns numbers to the
elements of U such that EuEU P(u) = 1, i.e.
suppose that P is a probability distribution
over U. Suppose, moreover, that l(C(u)) is
the length of the codeword associated with u.
In that case, the expected length of a source
code C for U and P is given by:
</bodyText>
<equation confidence="0.92873">
L(C) = E P(u) x l(C(u))
uEU
</equation>
<bodyText confidence="0.999679727272727">
To illustrate, let us extend our example by
assuming that the probability distribution of
U is P (red) = 12, P (white) = 14, P (blue) = 18,
P(orange) = 18. Then we can easily see
that the coding C&apos;(red) = 0, C&apos;(white) =
10, C&apos;(blue) = 110, C&apos;(orange) = 111 has
a shorter expected length than the coding
given above: 1.75 to 2. A crucial difference
between the two codings is that in distinc-
tion with C, C&apos; does not encode all elements
of U with the same length: the more prob-
able elements of U get an encoding with a
shorter length.3 This holds in general: in case
P(ui) &gt; P(uj) the optimal coding C will be
such that l(C(ui)) &lt; l(C(uj)) (cf. Cover &amp;
Thomas, 1991). Thus, the messages that are
more likely to be sent will be encoded with
a smaller length. This fact can now be used
to account for Horn&apos;s division of pragmatic
labor. Suppose that speakers have the follow-
ing set of contents/meanings that they might
want to communicate: M = {m1, ..., mn}.
On average, we might assume that the proba-
bilities with which they want to communicate
these contents/meanings are correlated with
the probabilities with which these contents
are true: if mi is more likely to be the case,
or more stereotypical, than mj, the probabil-
ity that speakers want to communicate mi is
higher than that of mj. In other communica-
tive situations the probability with which the
elements of M are communicated depends on
how salient the elements of M are.4 Speakers
cannot send the contents without represent-
ing them. Let us assume that speakers can
use the elements of R (the representations),
R = {r1, ..., rk}, to encode the elements of M.
The elements of R might be varied: some are
more complex than others. Codings are now
functions from M to R. Just as before we can
ask: what is the best coding? Following stan-
dards in data comprehension, the answer is:
the coding that minimizes average complex-
ity. Average complexity of coding system C
</bodyText>
<footnote confidence="0.9801025">
3There exists a close connection with entropy too:
for coding C&apos;, but not for C, the expected length,
L(C&apos;) is equal to the entropy of U, E(U). It turns
out that this is the optimal one can reach for w.r.t.
uniquely decodable codes. Notice that this means
that the optimal codelength for each ui is equal to
—log2P(ui), the surprise value of ui.
4In yet others, the probabilities depend even more
on the conversational situation and correlate with rel-
evance.
</footnote>
<equation confidence="0.724164">
is defined as follows:
Compl(C) _ E
m2M
P(m)xCompl(C(m))
</equation>
<bodyText confidence="0.981638875">
Atlas, J. and S. Levinson (1981), `It-Clefts, In-
formativeness and Logical Form&apos;, In: P. Cole
(ed.), Radical Pragmatics, New York, AP.
Now it is easy to show that the assump-
tion of optimal coding accounts for Horn&apos;s
observation that simple expressions get
a salient/stereotypical interpretation, while
complex expressions a marked one. Suppose
that the conventional meanings of represen-
tations ri and rj are such that they both
could express mi and mj. For instance, with
both words kill and cause to die we could de-
note situations of direct (stereotypical) and
indirect (marked) killing, and with both un-
stressed he and stressed HE we could refer to
both salient and non-salient male individuals
in the discourse. Still, the less complex kill
will typically be interpreted as direct stereo-
typical killing, an the other way around for
complex cause to die. And this follows from
the assumption that speakers use a language
that optimally encodes the relevant informa-
tion. In this case we have two relevantly dif-
ferent coding systems: C, which assigns mi to
ri and mj to rj, and C0, which assigns mi to
rj and mj to ri. The probabilities and com-
plexities are such that P(mi) &gt; P(mj) and
Compl(ri) &gt; Comp(rj). A standard proof
showing that Compl(C0) — Compl(C) &gt; 0
demonstrates then that C is a more optimal
coding than C0 (where I abbreviate P(mi) by
pi, Compl(ri) by cpli, and `x&apos; by `-&apos;):
</bodyText>
<equation confidence="0.9793275">
Compl(C0)— Compl(C) _
_ Epi - Compl(C0(i)) — Epi - Compl(C(i))
_ (pi - cpij) + (pj - cpli) — (pi - cpli) — (pj - cplj)
_ (pi — pj) - (cplj — cpli)
</equation>
<bodyText confidence="0.99931">
Because pi—pj &gt; 0, C0 can only be more opti-
mal than C in case cplj —cpli &lt; 0. But this is
by assumption not the case, and so C is pre-
ferred to C0. The same proof shows that when
pi—pj &gt; 0 the optimal code is such that cpli &lt;
cplj; only then Compl(C0) — Compl(C) &gt; 0.
Horn&apos;s division explained.
</bodyText>
<sectionHeader confidence="0.99891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999504376623377">
Anscombre J.C. and O. Ducrot (1983),
L&apos;Argumentation dans la langue, Brussels,
Mardaga.
Blutner, R. (2000), `Some aspects of Optimality
in Natural Language Interpretation&apos;, Journal of
Semantics.
Carston, R. (ms.), Informativeness, Relevance
and Scalar Implicature, University College Lon-
don.
Clarck H. H. &amp; J. Haviland (1977), `Comprehen-
sion and the given-new contract&apos;, In R. Freedle
(ed.), Discourse production and comprehension,
Hillsdale, NJ: Lawrence Erlbaum, pp. 1-40.
Cover, T.M. &amp; J.A. Thomas (1991), Elements of
Information Theory, Wiley: New York.
Dekker, P. &amp; R. van Rooy (2000), `Bidirectional
Optimality Theory: an application of Game
Theory&apos;, Journal of Semantics.
Gazdar, G. (1979), Pragmatics, London: Aca-
demic Press.
Groenendijk, J. and M. Stokhof (1984), Studies in
the Semantics of Questions and the Pragmatics
of Answers, Ph.D. thesis, University of Amster-
dam.
Grice, H. P. (1975), `Logic and Conversation&apos;, In:
P. Cole &amp; Morgan (eds.), Syntax and Semantics
3: Speech Acts, New York: Academic Press.
Hirschberg, J. (1985), A theory of scalar implica-
ture, Ph.D. thesis, UPenn.
Kuppevelt, J. van (1996), `Inferring from Topics:
Scalar Implicature as Topic-Dependent Infer-
ences&apos;, Linguistics and Philosophy, 19, pp. 555-
598.
Horn. L. (1972), The semantics of logical operators
in English, Ph.D. thesis, Yale University.
Horn, L. (1984), `Towards a new taxonomy of
pragmatic inference: Q-based and R-based im-
plicature&apos;. In: Schiffrin, D. (ed.), Meaning,
Form, and Use in Context:: Linguistic Appli-
cations, GURT84, 11-42, Washington; George-
town University Press.
Horn, L. (2000), `From if to iff: Conditional per-
fection as pragmatic strengthening&apos;, Journal of
Pragmatics, 32: 289-326.
Levinson, S.C. (1987), `Pragmatics and the gram-
mar of anaphora&apos;, Journal of Linguistics, 23:
379-434.
Levinson, S.C. (2000), Presumptive Meanings.
The Theory of Generalized Conversational Im-
plicatures, MIT Press: Cambridge, Mas-
sachusetts.
Lindley, D. V. (1956), `On a measure of informa-
tion provided by an experiment&apos;, Ann. Math.
Stat., 29, pp. 986-1005.
Matsumota, Y. (1995), `The conversational con-
dition on Horn scales&apos;, Linguistics and Philos-
ophy, 18: 21- 60.
McCawley, J. (1993), Everything that Linguists
always wanted to know about Logic, but were
afraid to ask, Chicago: Chicago University
Press.
Merin, A. (1997), `Information, relevance, and so-
cial decisionmaking&apos;, In: L. Moss, J. Ginzburg,
M. de Rijke (eds.), Logic, Language, and Com-
putation, Vol. 2, Stanford.
Reinhard, T. (1983), Anaphora and semantic in-
terpretation, London: Croom Helm.
Rooy, R. van (2001), `Relevance of communicative
acts&apos;, In Theoretical Aspects of Rationality and
Knowledge; Proceedings of TARK 2001, J. van
Benthem (ed.), San Francisco, Morgan Kauf-
mann Publishers, Inc., pp. 83-96.
Rooy, R. van (to appear), `Utility of mention-some
questions&apos;, Language and Computation.
Shannon, C. (1948), `The Mathematical Theory of
Communication&apos;, Bell System Technical Jour-
nal, 27.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.353942">
<title confidence="0.995941">Conversational Implicatures</title>
<author confidence="0.995588">van</author>
<affiliation confidence="0.9979805">Institute for Logic, Language and University of Amsterdam</affiliation>
<address confidence="0.803816">Nieuwe Doelenstraat 15, 1012 CP</address>
<email confidence="0.969206">vanrooy@hum.uva.nl</email>
<abstract confidence="0.959552052631579">According to standard pragmatics, we should account for conversational implicatures in terms of Grice&apos;s (1975) maxims of conversation. Neo-Griceans like Atlas &amp; Levinson (1981) and Horn (1984) seek to reduce those maxims to the In this paper I want to argue that (i) there are major problems for reducing Gricean pragmatics to these two principles, and (ii) that, in fact, we&apos;d better account for implicatures in of the principles of (a) optirelevance (b) cod- To formulate both, I will make use of Shannon&apos;s (1948) mathemattheory of</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J C Anscombre</author>
<author>O Ducrot</author>
</authors>
<title>L&apos;Argumentation dans la langue,</title>
<date>1983</date>
<location>Brussels, Mardaga.</location>
<contexts>
<context position="8219" citStr="Anscombre &amp; Ducrot (1983)" startWordPosition="1358" endWordPosition="1361">e assumption that answers are as informative as the question requires. I conclude that the scales relevant for the implicatures depend on the conversational situation (i.e. question asked) and the beliefs and preferences of the agents involved is in correspondence with Hirschberg&apos;s claim that scales are dependent on context. However, we would like to say something more; we would also like to say how the relevant scale depends on the question asked and the relevant beliefs and desires. 4 Relevance In this respect, important progress has been made recently by Merin (1997). Following the lead of Anscombre &amp; Ducrot (1983), Merin argues that scales should be defined not in terms of informativity, but rather in terms of a notion of relevance. The relevance of a proposition is determined in terms of the argumentative force the proposition would have in that particular conversational situation. The relevance of an assertion is then defined in information/decision/game theoretical terms, based on the assumption that the participants of the conversation have strictly opposing preferences, i.e. that the participants play a zero-sum game. Although Merin convincingly shows that some scalar implicatures (in particular t</context>
</contexts>
<marker>Anscombre, Ducrot, 1983</marker>
<rawString>Anscombre J.C. and O. Ducrot (1983), L&apos;Argumentation dans la langue, Brussels, Mardaga.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Blutner</author>
</authors>
<title>Some aspects of Optimality in Natural Language Interpretation&apos;,</title>
<date>2000</date>
<journal>Journal of Semantics.</journal>
<contexts>
<context position="26057" citStr="Blutner (2000)" startWordPosition="4386" endWordPosition="4387">nterpretations (Atlas &amp; Levinson, 1981); (iv) and perhaps most obviously, Horn&apos;s (1984) division of pragmatic labor according to which marked expression (morphologically complex and less lexicalized) typically get a marked meaning (cf. John made the car stop versus John stopped the car and consider also the fact that stressed pronouns can pick up less salient objects). In neo-Gricean pragmatics proposed by Atlas, Horn and Levinson, this principle is explained through the interaction of the socalled Q and I principles, and has recently been incorporated in (bi-directional) optimality theory by Blutner (2000) and reformulated in terms of game theory by Dekker &amp; van Rooy (2000). However, as we have seen above, explanations based on the Q and I principles are very shaky: these principles tend to clash with one another, and it is not always clear how to resolve this clash. In particular, it&apos;s unclear under which circumstances which principle should be used to explain the phenomena. I will show that by thinking of language as an efficient coding system the principle that lighter expressions get a more salient meaning can be given a straightforward explanation. 7.2 Optimal coding of information The que</context>
</contexts>
<marker>Blutner, 2000</marker>
<rawString>Blutner, R. (2000), `Some aspects of Optimality in Natural Language Interpretation&apos;, Journal of Semantics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Carston</author>
</authors>
<title>Informativeness, Relevance and Scalar Implicature,</title>
<location>University College London.</location>
<marker>Carston, </marker>
<rawString>Carston, R. (ms.), Informativeness, Relevance and Scalar Implicature, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clarck</author>
<author>J Haviland</author>
</authors>
<title>Comprehension and the given-new contract&apos;,</title>
<date>1977</date>
<booktitle>Discourse production and comprehension, Hillsdale, NJ: Lawrence Erlbaum,</booktitle>
<pages>1--40</pages>
<editor>In R. Freedle (ed.),</editor>
<marker>Clarck, Haviland, 1977</marker>
<rawString>Clarck H. H. &amp; J. Haviland (1977), `Comprehension and the given-new contract&apos;, In R. Freedle (ed.), Discourse production and comprehension, Hillsdale, NJ: Lawrence Erlbaum, pp. 1-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<date>1991</date>
<booktitle>Elements of Information Theory,</booktitle>
<location>Wiley: New York.</location>
<contexts>
<context position="28736" citStr="Cover &amp; Thomas, 1991" startWordPosition="4885" endWordPosition="4888"> assuming that the probability distribution of U is P (red) = 12, P (white) = 14, P (blue) = 18, P(orange) = 18. Then we can easily see that the coding C&apos;(red) = 0, C&apos;(white) = 10, C&apos;(blue) = 110, C&apos;(orange) = 111 has a shorter expected length than the coding given above: 1.75 to 2. A crucial difference between the two codings is that in distinction with C, C&apos; does not encode all elements of U with the same length: the more probable elements of U get an encoding with a shorter length.3 This holds in general: in case P(ui) &gt; P(uj) the optimal coding C will be such that l(C(ui)) &lt; l(C(uj)) (cf. Cover &amp; Thomas, 1991). Thus, the messages that are more likely to be sent will be encoded with a smaller length. This fact can now be used to account for Horn&apos;s division of pragmatic labor. Suppose that speakers have the following set of contents/meanings that they might want to communicate: M = {m1, ..., mn}. On average, we might assume that the probabilities with which they want to communicate these contents/meanings are correlated with the probabilities with which these contents are true: if mi is more likely to be the case, or more stereotypical, than mj, the probability that speakers want to communicate mi is</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T.M. &amp; J.A. Thomas (1991), Elements of Information Theory, Wiley: New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dekker</author>
<author>R van Rooy</author>
</authors>
<title>Bidirectional Optimality Theory: an application of Game Theory&apos;,</title>
<date>2000</date>
<journal>Journal of Semantics.</journal>
<marker>Dekker, van Rooy, 2000</marker>
<rawString>Dekker, P. &amp; R. van Rooy (2000), `Bidirectional Optimality Theory: an application of Game Theory&apos;, Journal of Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<date>1979</date>
<publisher>Academic Press.</publisher>
<location>Pragmatics, London:</location>
<contexts>
<context position="3331" citStr="Gazdar (1979)" startWordPosition="549" endWordPosition="550">eotypical interpretation. It is used, for instance, to enrich the interpretation of a conjunction to a temporal sequential, or causal, relation, and it allows us to interpret a conditional like `John walks, if Mary walks&apos; as the biconditional `John walks if and only Mary walks&apos;. 3 Problems for the Q and I principles 3.1 Too general Although the Q and I principles are intuitively appealing, they give rise to a number of conceptual and empirical problems. Let&apos;s start with some cases where it is predicted that Q-implicatures arise, although in fact they don&apos;t. First, at least when implemented as Gazdar (1979) did, we can derive from the existential &amp;quot;Someone is sick&amp;quot; as a Q-implicature that (the speaker knows that) a is not sick, for any individual a. Second, on the assumption that scales are defined in terms of entailment, it is predicted that we can infer from `B, if A&apos; to the conclusion that it is not the case that the stronger `B if and only if A&apos; holds, although in a lot of situations this is exactly what we can conclude. Third, on the same assumption, it is incorrectly predicted that we can infer `not regret A&apos; from `know A&apos;. Horn, Levinson and others have argued that these problems can be pr</context>
</contexts>
<marker>Gazdar, 1979</marker>
<rawString>Gazdar, G. (1979), Pragmatics, London: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Groenendijk</author>
<author>M Stokhof</author>
</authors>
<date>1984</date>
<booktitle>Studies in the Semantics of Questions and the Pragmatics of Answers, Ph.D. thesis,</booktitle>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="7061" citStr="Groenendijk &amp; Stokhof (1984)" startWordPosition="1167" endWordPosition="1170">ar implicatures should not exclusively be accounted for in terms of informativity, and (ii) that just like in the previous example, also here the relevant implicature crucially depends on the conversational situation (i.e. the beliefs and preferences of the agents involved). Second, as discussed by McCawley (1993), the implicatures generated by the (and, or) scale cannot account for the fact that a sentence of the form `A or B or C gives rise to the inference that only one of the three is true. A final example where the standard analysis of Q-implicatures isn&apos;t general enough was discussed by Groenendijk &amp; Stokhof (1984). They observe that when A answers Q&apos;s question &amp;quot;Who comes?&amp;quot; by saying &amp;quot;Peter comes&amp;quot;, we typically interpret the answer as being exhaustive. That is, we interpret A&apos;s answer as &amp;quot;Only Peter comes&amp;quot;. They claim that this kind of inference should intuitively be accounted for in terms of Grice&apos;s maxim of Quantity (as a Q-implicature), but note that the standard implementation does not predict the exhaustivity of the answer. Still, it seems that the exhaustive interpretation of the answer should be derived by Gricean pragmatics on the assumption that answers are as informative as the question requir</context>
<context position="10050" citStr="Groenendijk &amp; Stokhof (1984)" startWordPosition="1648" endWordPosition="1651">perates with the former. What is called for, then, is a generalization of Merin&apos;s notion of relevance that also measures the relevance of propositions in cooperative conversational situations. It seems only natural, on the assumption that speakers are relevance optimizers, that once we can define such a measure, not only the typical Q-implicatures can be accounted for in terms of relevance, but also the I-implicatures from conditional to biconditional, and Groenendijk &amp; Stokhof&apos;s (1984) observation that answers are normally interpreted in an exhaustive way. As we will see in the next section, Groenendijk &amp; Stokhof (1984) show that almost all typical Q-implicatures can be analyzed alternatively in terms of their explicit exhaustivity-operator, without giving rise to the above discussed overgeneralizations, when the clause that gives rise to the implicature is used as an answer to a question. 5 Exhaustified answers Groenendijk &amp; Stokhof (1984) propose to account for the intuition that answer Peter comes to question Who comes? should normally be read exhaustively by introducing an explicit exhaustivity operator that is applied to answers and the abstracts underlying the questions to derive the exhaustive interpr</context>
<context position="12698" citStr="Groenendijk &amp; Stokhof (1984)" startWordPosition="2099" endWordPosition="2102">0) calls this inference conversion and explicitly proposes to account for it in terms of the I-principle. Similarly, if we allow for explicit quantification over worlds, we can account for the inference from (1b) to (1c), when the former is given as answer to (1a): (1) a. Q: Did John walk? b. A: If Mary talked. c. John walked iff Mary talked. We assume that the property underlying a question like (1a) is Aw.Walk(j)(w), and that answer (1b) should be represented by AP.dw[Talk(m)(w) —� P(w)] which after exhaustification means that Mary talked iff John walked.&apos; This inference is accounted for by Groenendijk &amp; Stokhof (1984) in terms of their generalized exhaustivity operator without using explicit quantification over worlds. Such an analysis cannot account, however, for the exclusive reading of disjunctive sentences with more than two disjuncts, to be discussed below. Our approach also predicts that (2a) should be read as (2b) when the color of the flag is at issue. (2) a. The flag is red. b. The flag is all red. This inference is normally (e.g. Atlas &amp; Levinson, 1981) accounted for by assuming that (2a) should be interpreted as informative as possible. But then it should be explained why in certain circumstance</context>
<context position="17004" citStr="Groenendijk &amp; Stokhof (1984)" startWordPosition="2827" endWordPosition="2830"> exhaustively. To illustrate, when I ask you (6a) and you answer by saying (6b), I am satisfied, although I don&apos;t interpret your answer as claiming that this is the only place where I can buy an Italian newspaper. (6) a. Where can I buy an Italian newspaper? b. Around the corner. 6.1 Topic dependent relevance In cooperative dialogues the relevance of communicative acts can be determined with respect to decision problems (cf. van Rooy (2001). A decision problem Using communication theory we can model these decision problems by partitions of the logical space — , i.e., the semantic questions of Groenendijk &amp; Stokhof (1984). One proposition will then be more relevant than another when it helps more to resolve the question. Intuitively, we would like to say that assertions are relevant with respect to this decision problem if the decision is easier to make after an assertion is learned. But to account for this, we have to measure the difficulty of the decision. A standard way to do this is in terms of entropy. Given a probability function P, we can define the entropy of decision problem Q as follows: E(Q) = 1: P(q) x —lo92P(q) qEQ When our agent learns proposition A, we can determine the entropy of decision probl</context>
</contexts>
<marker>Groenendijk, Stokhof, 1984</marker>
<rawString>Groenendijk, J. and M. Stokhof (1984), Studies in the Semantics of Questions and the Pragmatics of Answers, Ph.D. thesis, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and Conversation&apos;,</title>
<date>1975</date>
<booktitle>Syntax and Semantics 3: Speech Acts,</booktitle>
<editor>In: P. Cole &amp; Morgan (eds.),</editor>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<marker>Grice, 1975</marker>
<rawString>Grice, H. P. (1975), `Logic and Conversation&apos;, In: P. Cole &amp; Morgan (eds.), Syntax and Semantics 3: Speech Acts, New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
</authors>
<title>A theory of scalar implicature,</title>
<date>1985</date>
<tech>Ph.D. thesis, UPenn.</tech>
<contexts>
<context position="5780" citStr="Hirschberg (1985)" startWordPosition="950" endWordPosition="951">icular conversational situations the generalized conversational implicature is cancelled, for reasons of relevance: The answer is already informative enough for the purpose of the conversation. I will argue, however, that informativity is, in general, not the crucial issue, and that it is much more natural to assume that — for reasons of relevance in this particular situation — the (potential) implicature does not even arise. 3.2 Not general enough Not only does the standard analysis of Qimplicatures overgeneralize, it also doesn&apos;t seem to be general enough. First, as discussed extensively by Hirschberg (1985), the standard analysis is of no help to account for certain examples that intuitively should be analyzed as scalar implicatures. If Mary&apos;s potential new boss asks her at her job-interview whether she speaks French, and she answers by saying &amp;quot;My sister does&amp;quot;, he can conclude that Mary herself does not. The standard analysis fails to account for this, because (a) scalar implicatures are all analyzed in terms of the Q-principle, (b) the Q-principle is stated in terms of informativitg, but (c) the proposition that Mary speaks French is not more informative (i.e. entails) than the proposition that</context>
</contexts>
<marker>Hirschberg, 1985</marker>
<rawString>Hirschberg, J. (1985), A theory of scalar implicature, Ph.D. thesis, UPenn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J van Kuppevelt</author>
</authors>
<title>Inferring from Topics: Scalar Implicature as Topic-Dependent Inferences&apos;,</title>
<date>1996</date>
<journal>Linguistics and Philosophy,</journal>
<volume>19</volume>
<pages>555--598</pages>
<marker>Kuppevelt, 1996</marker>
<rawString>Kuppevelt, J. van (1996), `Inferring from Topics: Scalar Implicature as Topic-Dependent Inferences&apos;, Linguistics and Philosophy, 19, pp. 555-598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L</author>
</authors>
<title>The semantics of logical operators</title>
<date>1972</date>
<booktitle>in English, Ph.D. thesis,</booktitle>
<institution>Yale University.</institution>
<marker>L, 1972</marker>
<rawString>Horn. L. (1972), The semantics of logical operators in English, Ph.D. thesis, Yale University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Horn</author>
</authors>
<title>Towards a new taxonomy of pragmatic inference: Q-based and R-based implicature&apos;.</title>
<date>1984</date>
<booktitle>Meaning, Form, and Use in Context:: Linguistic Applications, GURT84,</booktitle>
<pages>11--42</pages>
<editor>In: Schiffrin, D. (ed.),</editor>
<publisher>University Press.</publisher>
<location>Washington; Georgetown</location>
<marker>Horn, 1984</marker>
<rawString>Horn, L. (1984), `Towards a new taxonomy of pragmatic inference: Q-based and R-based implicature&apos;. In: Schiffrin, D. (ed.), Meaning, Form, and Use in Context:: Linguistic Applications, GURT84, 11-42, Washington; Georgetown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Horn</author>
</authors>
<title>From if to iff: Conditional perfection as pragmatic strengthening&apos;,</title>
<date>2000</date>
<journal>Journal of Pragmatics,</journal>
<volume>32</volume>
<pages>289--326</pages>
<contexts>
<context position="12072" citStr="Horn (2000)" startWordPosition="1993" endWordPosition="1994"> also well when more than one item gives rise to an implicature. From the exhaustive interpretation of the term Some of the bacon and some of the eggs given as answer to the question What did Mary ate? we can conclude that Mary didn&apos;t eat all of the bacon, and that she didn&apos;t eat all of the beans, just like we should. Notice that our exhaustification analysis not only predicts intuitions standardly accounted for in terms of the Q principle; also some I-implicatures are accounted for. If the question is Who quacks? the answer Every duck quacks is predicted to mean that every quacker is a duck. Horn (2000) calls this inference conversion and explicitly proposes to account for it in terms of the I-principle. Similarly, if we allow for explicit quantification over worlds, we can account for the inference from (1b) to (1c), when the former is given as answer to (1a): (1) a. Q: Did John walk? b. A: If Mary talked. c. John walked iff Mary talked. We assume that the property underlying a question like (1a) is Aw.Walk(j)(w), and that answer (1b) should be represented by AP.dw[Talk(m)(w) —� P(w)] which after exhaustification means that Mary talked iff John walked.&apos; This inference is accounted for by Gr</context>
</contexts>
<marker>Horn, 2000</marker>
<rawString>Horn, L. (2000), `From if to iff: Conditional perfection as pragmatic strengthening&apos;, Journal of Pragmatics, 32: 289-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Levinson</author>
</authors>
<title>Pragmatics and the grammar of anaphora&apos;,</title>
<date>1987</date>
<journal>Journal of Linguistics,</journal>
<volume>23</volume>
<pages>379--434</pages>
<contexts>
<context position="25095" citStr="Levinson (1987)" startWordPosition="4234" endWordPosition="4236">resentations r1 and r2. In principle this gives rise to two possible codings: {(r1, m1), (r2, m2)} and {(r1, m2), (r2, m1)}. In many communicative situations, however, the underspecification does not really exist, and is resolved due to the general pragmatic principle that a lighter form will be interpreted by a more salient, or stereotypical, meaning: (i) It is a general defeasible principle, for instance, in centering theory that if a certain object/expression is referred to be a pronoun, another more salient object/expression should be referred to by a pronoun too; (ii) Reinhard (1983) and Levinson (1987) seek to reduce Chomsky&apos;s B and C principles of the binding theory to pragmatics maxims. In particular, disjoint reference of lexical NPs throughout the sentence is explained by pointing to the possibility of the use of a lighter expression, viz. an anaphor or pronoun; (iii) The preference for bridging (Clark &amp; Haviland, 1977) and stereotypical interpretations (Atlas &amp; Levinson, 1981); (iv) and perhaps most obviously, Horn&apos;s (1984) division of pragmatic labor according to which marked expression (morphologically complex and less lexicalized) typically get a marked meaning (cf. John made the ca</context>
</contexts>
<marker>Levinson, 1987</marker>
<rawString>Levinson, S.C. (1987), `Pragmatics and the grammar of anaphora&apos;, Journal of Linguistics, 23: 379-434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Levinson</author>
</authors>
<title>Presumptive Meanings. The Theory of Generalized Conversational Implicatures,</title>
<date>2000</date>
<publisher>MIT Press:</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Levinson, 2000</marker>
<rawString>Levinson, S.C. (2000), Presumptive Meanings. The Theory of Generalized Conversational Implicatures, MIT Press: Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D V Lindley</author>
</authors>
<title>On a measure of information provided by an experiment&apos;,</title>
<date>1956</date>
<journal>Ann. Math. Stat.,</journal>
<volume>29</volume>
<pages>986--1005</pages>
<contexts>
<context position="18082" citStr="Lindley (1956)" startWordPosition="3027" endWordPosition="3028">problem Q as follows: E(Q) = 1: P(q) x —lo92P(q) qEQ When our agent learns proposition A, we can determine the entropy of decision problem Q conditional on learning A, EA(Q), as follows: EA(Q) = E P(q/A) x —lo92P(q/A) qEQ In terms of this notion we can now define what might be called the Relevance of proposition A, with respect to partition Q, RQ(A), as the reduction of entropy, or uncertainty, of Q when A is learned:2 RQ(A) = E(Q) — EA(Q) Relevance will be used to determine the actual interpretation of a sentence underspecified by its conventional meaning. We will say This notion was used by Lindley (1956) already to measure the informational value of a particular result of an experiment. that interpretation A is better than interpretation B, A &gt; B, iff RQ(A) &gt; RQ(B) with respect to all probability functions for which Q has maximal entropy. It might be, of course, that for some probability distributions A is better, while for others B is. Which one is then preferred? In those cases, I propose, interpretation A is better if the sentence `gives rise&apos; to a new question, Q&apos;, which is orthogonal to Q, such that after learning A, but not after learning B, every complete answer to Q&apos; also completely a</context>
</contexts>
<marker>Lindley, 1956</marker>
<rawString>Lindley, D. V. (1956), `On a measure of information provided by an experiment&apos;, Ann. Math. Stat., 29, pp. 986-1005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumota</author>
</authors>
<title>The conversational condition on Horn scales&apos;,</title>
<date>1995</date>
<journal>Linguistics and Philosophy,</journal>
<volume>18</volume>
<pages>21--60</pages>
<marker>Matsumota, 1995</marker>
<rawString>Matsumota, Y. (1995), `The conversational condition on Horn scales&apos;, Linguistics and Philosophy, 18: 21- 60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCawley</author>
</authors>
<title>Everything that Linguists always wanted to know about Logic, but were afraid to ask, Chicago:</title>
<date>1993</date>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="6748" citStr="McCawley (1993)" startWordPosition="1112" endWordPosition="1113">because (a) scalar implicatures are all analyzed in terms of the Q-principle, (b) the Q-principle is stated in terms of informativitg, but (c) the proposition that Mary speaks French is not more informative (i.e. entails) than the proposition that her sister does. This example suggests (i) that scalar implicatures should not exclusively be accounted for in terms of informativity, and (ii) that just like in the previous example, also here the relevant implicature crucially depends on the conversational situation (i.e. the beliefs and preferences of the agents involved). Second, as discussed by McCawley (1993), the implicatures generated by the (and, or) scale cannot account for the fact that a sentence of the form `A or B or C gives rise to the inference that only one of the three is true. A final example where the standard analysis of Q-implicatures isn&apos;t general enough was discussed by Groenendijk &amp; Stokhof (1984). They observe that when A answers Q&apos;s question &amp;quot;Who comes?&amp;quot; by saying &amp;quot;Peter comes&amp;quot;, we typically interpret the answer as being exhaustive. That is, we interpret A&apos;s answer as &amp;quot;Only Peter comes&amp;quot;. They claim that this kind of inference should intuitively be accounted for in terms of Gri</context>
</contexts>
<marker>McCawley, 1993</marker>
<rawString>McCawley, J. (1993), Everything that Linguists always wanted to know about Logic, but were afraid to ask, Chicago: Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Merin</author>
</authors>
<title>Information, relevance, and social decisionmaking&apos;, In:</title>
<date>1997</date>
<booktitle>Logic, Language, and Computation,</booktitle>
<volume>2</volume>
<editor>L. Moss, J. Ginzburg, M. de Rijke (eds.),</editor>
<location>Stanford.</location>
<contexts>
<context position="8170" citStr="Merin (1997)" startWordPosition="1352" endWordPosition="1353"> derived by Gricean pragmatics on the assumption that answers are as informative as the question requires. I conclude that the scales relevant for the implicatures depend on the conversational situation (i.e. question asked) and the beliefs and preferences of the agents involved is in correspondence with Hirschberg&apos;s claim that scales are dependent on context. However, we would like to say something more; we would also like to say how the relevant scale depends on the question asked and the relevant beliefs and desires. 4 Relevance In this respect, important progress has been made recently by Merin (1997). Following the lead of Anscombre &amp; Ducrot (1983), Merin argues that scales should be defined not in terms of informativity, but rather in terms of a notion of relevance. The relevance of a proposition is determined in terms of the argumentative force the proposition would have in that particular conversational situation. The relevance of an assertion is then defined in information/decision/game theoretical terms, based on the assumption that the participants of the conversation have strictly opposing preferences, i.e. that the participants play a zero-sum game. Although Merin convincingly sho</context>
</contexts>
<marker>Merin, 1997</marker>
<rawString>Merin, A. (1997), `Information, relevance, and social decisionmaking&apos;, In: L. Moss, J. Ginzburg, M. de Rijke (eds.), Logic, Language, and Computation, Vol. 2, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Reinhard</author>
</authors>
<title>Anaphora and semantic interpretation,</title>
<date>1983</date>
<location>London: Croom Helm.</location>
<contexts>
<context position="25075" citStr="Reinhard (1983)" startWordPosition="4231" endWordPosition="4232">y two linguistic representations r1 and r2. In principle this gives rise to two possible codings: {(r1, m1), (r2, m2)} and {(r1, m2), (r2, m1)}. In many communicative situations, however, the underspecification does not really exist, and is resolved due to the general pragmatic principle that a lighter form will be interpreted by a more salient, or stereotypical, meaning: (i) It is a general defeasible principle, for instance, in centering theory that if a certain object/expression is referred to be a pronoun, another more salient object/expression should be referred to by a pronoun too; (ii) Reinhard (1983) and Levinson (1987) seek to reduce Chomsky&apos;s B and C principles of the binding theory to pragmatics maxims. In particular, disjoint reference of lexical NPs throughout the sentence is explained by pointing to the possibility of the use of a lighter expression, viz. an anaphor or pronoun; (iii) The preference for bridging (Clark &amp; Haviland, 1977) and stereotypical interpretations (Atlas &amp; Levinson, 1981); (iv) and perhaps most obviously, Horn&apos;s (1984) division of pragmatic labor according to which marked expression (morphologically complex and less lexicalized) typically get a marked meaning (</context>
</contexts>
<marker>Reinhard, 1983</marker>
<rawString>Reinhard, T. (1983), Anaphora and semantic interpretation, London: Croom Helm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R van Rooy</author>
</authors>
<title>Relevance of communicative acts&apos;,</title>
<date>2001</date>
<booktitle>In Theoretical Aspects of Rationality and Knowledge; Proceedings of TARK 2001,</booktitle>
<pages>83--96</pages>
<editor>J. van Benthem (ed.),</editor>
<publisher>Morgan Kaufmann Publishers, Inc.,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="16820" citStr="Rooy (2001)" startWordPosition="2799" endWordPosition="2800">ost importantly, it would predict incorrectly for so-called mention-some questions. Sometimes an assertion intuitively answers a question completely without being read exhaustively. To illustrate, when I ask you (6a) and you answer by saying (6b), I am satisfied, although I don&apos;t interpret your answer as claiming that this is the only place where I can buy an Italian newspaper. (6) a. Where can I buy an Italian newspaper? b. Around the corner. 6.1 Topic dependent relevance In cooperative dialogues the relevance of communicative acts can be determined with respect to decision problems (cf. van Rooy (2001). A decision problem Using communication theory we can model these decision problems by partitions of the logical space — , i.e., the semantic questions of Groenendijk &amp; Stokhof (1984). One proposition will then be more relevant than another when it helps more to resolve the question. Intuitively, we would like to say that assertions are relevant with respect to this decision problem if the decision is easier to make after an assertion is learned. But to account for this, we have to measure the difficulty of the decision. A standard way to do this is in terms of entropy. Given a probability fu</context>
</contexts>
<marker>Rooy, 2001</marker>
<rawString>Rooy, R. van (2001), `Relevance of communicative acts&apos;, In Theoretical Aspects of Rationality and Knowledge; Proceedings of TARK 2001, J. van Benthem (ed.), San Francisco, Morgan Kaufmann Publishers, Inc., pp. 83-96.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Rooy</author>
</authors>
<title>van (to appear), `Utility of mention-some questions&apos;, Language and Computation.</title>
<marker>Rooy, </marker>
<rawString>Rooy, R. van (to appear), `Utility of mention-some questions&apos;, Language and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>The Mathematical Theory of Communication&apos;,</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<contexts>
<context position="27585" citStr="Shannon (1948)" startWordPosition="4663" endWordPosition="4664">{u1, ..., un} in terms of codes built up from codesymbols belonging to the code-alphabet S = {s1, ..., sn}. A source code, or coding system, C, is defined as a function from U to S*, where`*&apos; is Kleene&apos;s star. For example, C(red) = 00, C(white) = 11, C(blue) = 10, C(orange) = 01 is a source code for U = {red, white, blue, orange} with alphabet S = {0, 1}. Of course, the source and alphabet allow for many different codes. Intuitively, however, some codings are more efficient than others. What is the best coding system? The coding system with the shortest expected length. The crucial insight of Shannon (1948) was that this expected length depends not only on the length of the messages after encoding, but also on the probability with which the messages are sent. Suppose that function P assigns numbers to the elements of U such that EuEU P(u) = 1, i.e. suppose that P is a probability distribution over U. Suppose, moreover, that l(C(u)) is the length of the codeword associated with u. In that case, the expected length of a source code C for U and P is given by: L(C) = E P(u) x l(C(u)) uEU To illustrate, let us extend our example by assuming that the probability distribution of U is P (red) = 12, P (w</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Shannon, C. (1948), `The Mathematical Theory of Communication&apos;, Bell System Technical Journal, 27.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>