<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.855999">
Predicting a Scientific Community’s Response to an Article
</title>
<author confidence="0.998441">
Dani Yogatama Michael Heilman Brendan O’Connor Chris Dyer
</author>
<affiliation confidence="0.887389">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.999413">
{dyogatama,mheilman,brenocon,cdyer}@cs.cmu.edu
</email>
<author confidence="0.993663">
Bryan R. Routledge
</author>
<affiliation confidence="0.875599666666667">
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.996775">
routledge@cmu.edu
</email>
<sectionHeader confidence="0.996596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.909669">
We consider the problem of predicting mea-
surable responses to scientific articles based
primarily on their text content. Specif-
ically, we consider papers in two fields
(economics and computational linguistics)
and make predictions about downloads and
within-community citations. Our approach is
based on generalized linear models, allowing
interpretability; a novel extension that cap-
tures first-order temporal effects is also pre-
sented. We demonstrate that text features
significantly improve accuracy of predictions
over metadata features like authors, topical
categories, and publication venues.
</bodyText>
<sectionHeader confidence="0.998546" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994014">
Written communication is an essential component
of the complex social phenomenon of science. As
such, natural language processing is well-positioned
to provide tools for understanding the scientific pro-
cess, by analyzing the textual artifacts (papers, pro-
ceedings, etc.) that it produces. This paper is about
modeling collections of scientific documents to un-
derstand how their textual content relates to how a
scientific community responds to them. While past
work has often focused on citation structure (Borner
et al., 2003; Qazvinian and Radev, 2008), our em-
phasis is on the text content, following Ramage et
al. (2010) and Gerrish and Blei (2010).
Instead of task-independent exploratory data anal-
ysis (e.g., topic modeling) or multi-document sum-
</bodyText>
<author confidence="0.882348">
Noah A. Smith
</author>
<affiliation confidence="0.938408666666667">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.972497">
nasmith@cs.cmu.edu
</email>
<bodyText confidence="0.99989790625">
marization, we consider supervised models of the
collective response of a scientific community to a
published article. There are many measures of im-
pact of a scientific paper; ours come from direct
measurements of the number of downloads (from
an established website where prominent economists
post papers before formal publication) and citations
(within a fixed scientific community). We adopt a
discriminative approach based on generalized lin-
ear models that can make use of any text or meta-
data features, and show that simple lexical fea-
tures offer substantial power in modeling out-of-
sample response and in forecasting response for fu-
ture articles. Realistic forecasting evaluations re-
quire methodological care beyond the usual best
practices of train/test separation, and we elucidate
these issues.
In addition, we introduce a new regularization
technique that leverages the intuition that the rela-
tionship between observable features and response
should evolve smoothly over time. This regularizer
allows the learner to rely more strongly on more re-
cent evidence, while taking into account a long his-
tory of training data. Our time series-inspired regu-
larizer is computationally efficient in learning and is
a significant advance over earlier text-driven fore-
casting models that ignore the time variable alto-
gether (Kogan et al., 2009; Joshi et al., 2010).
We evaluate our approaches in two novel experi-
mental settings: predicting downloads of economics
articles and predicting citation of papers at ACL
conferences. Our approaches substantially outper-
</bodyText>
<page confidence="0.975509">
594
</page>
<note confidence="0.957647">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594–604,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.998432">
0 1500
# docs.
0 2500
# docs.
4 log(# downloads) 9 0 # citations 18
</figure>
<figureCaption confidence="0.9488954">
Figure 1: Left: the distribution of log download counts
for papers in the NBER dataset one year after post-
ing. Right: the distribution of within-dataset citations of
ACL papers within three years of publication (outliers ex-
cluded for readability).
</figureCaption>
<bodyText confidence="0.9989275">
form text-ignorant baselines on ground-truth predic-
tions. Our time series models permit flexibility in
features and offer a novel and perhaps more inter-
pretable view of the data than summary statistics.
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999803">
We make use of two collections of scientific litera-
ture, one from the economics domain, and the other
from computational linguistics and natural language
processing. Statistics are summarized in Table 1.
</bodyText>
<subsectionHeader confidence="0.765258">
2.1 NBER
</subsectionHeader>
<bodyText confidence="0.999972">
Our first dataset consists of research papers in eco-
nomics from the National Bureau of Economic
Research (NBER) from 1999 to 2009 (http://
www.nber.org). Approximately 1,000 research
economists are affiliated with the NBER. New
NBER working papers are posted to the website
weekly. The papers are not yet peer-reviewed, but
given the prominence of many economists affiliated
with the NBER, many of the papers are widely read.
Text from the abstracts of the papers and related
metadata are publicly available. Full text is available
to subscribers (universities typically have access).
The NBER provided us with download statistics
for these papers. For each paper, we computed
the total number of downloads in the first year af-
ter each paper’s posting.1 The download counts are
log-normally distributed, as shown in Figure 1, and
so our regression models (§3) minimize squared er-
rors in the log space. Our download logs begin in
</bodyText>
<footnote confidence="0.9911118">
1For the vast majority of papers, most of the downloads oc-
cur soon after the paper’s posting. We explored different mea-
sures with different download windows (two years, for exam-
ple) with broadly similar results. We leave a more detailed anal-
ysis of the time series patterns of downloads to future work.
</footnote>
<table confidence="0.873557833333333">
Dataset # Docs. Avg. # Response
Words
NBER 8,814 155 # downloads in first
year (mean 761)
ACL 4,026 3,966 at least 1 citation in
first 3 years? (54% no)
</table>
<tableCaption confidence="0.999738">
Table 1: Descriptive statistics about the datasets.
</tableCaption>
<bodyText confidence="0.999795333333333">
1999. We use the 8,814 papers from 1999–2009 pe-
riod (there are 16,334 papers in the full dataset dat-
ing back to 1985). We only use text from the ab-
stracts, since we were able to obtain full texts for
just a portion of the papers, and since the OCR of
the full texts we do have is very noisy.
</bodyText>
<subsectionHeader confidence="0.99066">
2.2 ACL
</subsectionHeader>
<bodyText confidence="0.999868444444444">
Our second dataset consists of research papers
from the Association for Computational Linguis-
tics (ACL) from 1980 to 2006 (Radev et al., 2009a;
Radev et al., 2009b). We have the full texts for pa-
pers (OCR output) as well as structured citation data.
There are 15,689 papers in the whole dataset. For
the citation prediction task, we include conference
papers from ACL, EACL, HLT, and NAACL.2 We
remove journal papers, since they are characteristi-
cally different from conference papers, as well as
workshop papers. We do include short papers, in-
teractive demo session papers, and student research
papers that are included in the companion volumes
for these conferences (such papers are cited less than
full papers, but many are still cited). The resulting
dataset contains 4,026 papers. The number of pa-
pers in each year varies because not all conferences
are annual.
We look at citations in the three-year window fol-
lowing publication, excluding self-citations and only
considering citations from papers within these con-
ferences. Figure 1 shows a histogram; note that
many papers (54%) are not cited at all, and the dis-
tribution of citations per paper is neither normal nor
log-normal. We organize the papers into two classes:
those with zero citations and those with non-zero ci-
tations in the three-year window.
</bodyText>
<footnote confidence="0.997871">
2EMNLP is a relatively recent conference, and, in this col-
lection, complete data for its papers postdate the end of the last
training period, so we chose to exclude it from our dataset.
</footnote>
<page confidence="0.998538">
595
</page>
<sectionHeader confidence="0.994695" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999922384615385">
Our forecasting approach is based on generalized
linear models for regression and classification. The
models are trained with an `2-penalty, often called
a “ridge” model (Hoerl and Kennard, 1970).3 For
the NBER data, where (log) number of downloads is
nearly a continuous measure, we use linear regres-
sion. For the ACL data, where response is the bi-
nary cited-or-not variable we use logistic regression,
often referred to as a “maximum entropy” model
(Berger et al., 1996) or a log-linear model. We
briefly review the class of models. Then, we de-
scribe a time series model appropriate for time series
data.
</bodyText>
<subsectionHeader confidence="0.992815">
3.1 Generalized Linear Models
</subsectionHeader>
<bodyText confidence="0.9997806">
Consider a model that predicts a response y given a
vector input x = (x1, ... , xd) E Rd. Our models
are linear functions of x and parameterized by the
vector β. Given a corpus of M document features,
X, and responses Y , we estimate:
</bodyText>
<equation confidence="0.993433">
β = argminβ R(β) + L(β, X, Y ) (1)
</equation>
<bodyText confidence="0.999636947368421">
where L is a model-dependent loss function and R
is a regularization penalty to encourage models with
small weight vectors. We describe models and loss
functions first and then turn to regularization.
For the NBER data, the (log) number of down-
loads is continuous, and so we use least-squares
linear regression model. The loss function is the
sum of the squared errors for the M documents in
our training data: L(β, X, Y ) = PM i=1(yi − ˆyi)2,
where the prediction rule for new documents is:
yˆ = Pd j=0 βjxj. Probabilistically, this equates to an
assumption that βTx is the mean of a normal (i.e.,
Gaussian) distribution from which random variable
y is drawn.
For the ACL data, we predict y from a discrete
set C (specifically, the binary set of zero citations or
more than zero citations), and we use logistic regres-
sion. This model assumes that for the ith training
input xi, the output yi is drawn according to:
</bodyText>
<equation confidence="0.9987905">
� ��P �
p(yi  |xi) = �expβT c xi c,cC exp βT c,xi
</equation>
<footnote confidence="0.953997333333333">
3Preliminary experiments found no consistent benefit from
Bl (“lasso”) models, though we note that Bl-regularization leads
to sparse, compact models that may be more interpretable.
</footnote>
<bodyText confidence="0.984067285714286">
where there is a feature vector βc for each class
c E C. Under this interpretation, parameter esti-
mation is maximum a posteriori inference for β,
and R(β) is a log-prior for the weights. The loss
function is the negative log likelihood for the M
documents: L(β, X, Y ) = − PMi=1 logp(yi  |xi).
The prediction rule for a new document is: yˆ =
</bodyText>
<equation confidence="0.846219">
Pd
argmaxccC j=0 βc,jxj. Generalized linear mod-
</equation>
<bodyText confidence="0.999664833333333">
els and penalized regression are well-studied with
an extensive literature (Mccullagh and Nelder, 1989;
Hastie et al., 2009). We leave other types of mod-
els, such as Poisson (Cameron and Trivedi, 1998)
or ordinal (McCullagh, 1980) regression models, to
future work.
</bodyText>
<subsectionHeader confidence="0.99911">
3.2 Ridge Regression
</subsectionHeader>
<bodyText confidence="0.998488333333333">
With large numbers of features, regularization is
crucial to avoid overfitting. In ridge regression (Ho-
erl and Kennard, 1970), a standard method to which
we compare the time series regularization discussed
in §3.3, the penalty R(β) is proportional to the `2-
norm of β:
</bodyText>
<equation confidence="0.998792">
R(β) = λ11β112 = λPj β2 j
</equation>
<bodyText confidence="0.9829978">
where λ is a regularization hyperparameter that is
tuned on development data or by cross-validation.4
This penalty pushes many βj close (but not com-
pletely) to zero. In practice, we multiply the penalty
by the number of examples M to facilitate tuning of
λ.
The ridge linear regression model can be inter-
preted probabilistically as each coefficient βj is
drawn i.i.d. from a normal distribution with mean
0 and variance 2λ−1.
</bodyText>
<subsectionHeader confidence="0.999642">
3.3 Time Series Regularization
</subsectionHeader>
<bodyText confidence="0.9967465">
A simple way to capture temporal variation is to con-
join traditional features with a time variable. Here,
we divide the dataset into T time steps (years). In the
new representation, the feature space expands from
Rd to RTxd. For a document published at year t, the
elements of x are non-zero only for those features
that correspond to year-t; that is xt,,j = 0 for all
t&apos; =� t.
</bodyText>
<footnote confidence="0.874199333333333">
4The linear regression has a bias Oo that is always active.
The logistic regression also has an unpenalized bias Oc,0 for
each class c. This weight is not regularized.
</footnote>
<page confidence="0.998219">
596
</page>
<bodyText confidence="0.985169566666667">
Estimating this model with the new features using
the `2-penalty would be effectively estimating sepa-
rate models for each year under the assumption that
each βt,j is independent; even for features that dif-
fered only temporally (e.g., βt,j and βt+1,j).
In this work, we apply time series regularization
to GLMs, enabling models that have coefficients that
change over time but prefer gradual changes across
time steps. Boyd and Vandenberghe (2004, §6.3) de-
scribe a general version of this sort of regularizer.
To our knowledge, such regularizers have not previ-
ously been applied to temporal modeling of text.
The time series regularization penalty becomes:
(βt,j − βt−1,j)2
It includes a standard `2-penalty on the coefficients,
and a penalty for differences between coefficients
for adjacent time steps to induce smooth changes.5
Similar to the previous model, in practice, we mul-
tiply the regularization constant λ by MT to facili-
tate tuning of λ for datasets with different numbers
of examples M and numbers of time steps T. The
new parameter, α, controls the smoothness of the es-
timated coefficients. Setting α to zero imposes no
penalty for time-variation in the coefficients and re-
sults in independent ridge regressions at each time
step. Also, when the number of examples is con-
stant across time steps, setting a large α parameter
(α —+ oc) results in a single ridge regression over all
years since it imposes βt,j = βt+1,j for all t E T.
The partial derivative is:
</bodyText>
<equation confidence="0.988416666666667">
∂R/∂βt,j = 2λβt,j
+ 11t &gt; 1}2λα(βt,j − βt−1,j)
+ 11t &lt; T}2λα(βt,j − βt+1,j)
</equation>
<bodyText confidence="0.9998818">
This time series regularization can be applied more
generally, not just to linear and logistic regression.
With either ridge regularization or this time se-
ries regularization scheme, Eq. 1 is an unconstrained
convex optimization problem for the linear models
</bodyText>
<footnote confidence="0.65767575">
5Our implementation of the time series regularizer does not
penalize the magnitude of the weight for the bias feature (as in
ridge regression). It does, however, penalize the difference in
the bias weight between time steps (as with other features).
</footnote>
<figureCaption confidence="0.991452333333333">
Figure 2: Time series regression as a graphical model;
the variables Xt and Yt are the sets of feature vectors
and response variables from documents dated t.
</figureCaption>
<bodyText confidence="0.997828333333333">
we describe here. There exist a number of optimiza-
tion procedures for it; we use the L-BFGS quasi-
Newton algorithm (Liu and Nocedal, 1989).
</bodyText>
<subsectionHeader confidence="0.875134">
Probabilistic Interpretation
</subsectionHeader>
<bodyText confidence="0.979268142857143">
We can interpret the time series regularization prob-
abilistically as follows. Let the coefficient for the
jth feature over time be βj = (β1,j,β2,j, ..., βT,j).
βj are draws from a multivariate normal distribu-
tion with a tridiagonal precision matrix E−1 = A E
RTxT:
The form of R(β) follows from noting:
−2log p(βj; α, λ) = β� Aβj + constant
The squared difference between adjacent time steps
comes from the off-diagonal entries in the preci-
sion matrix.6 Figure 2 shows a graphical represen-
tation of the time series regularization in our model.
Its Markov chain structure corresponds to the off-
diagonals.
There is a rich literature on time series analysis
(Box et al., 2008; Hamilton, 1994). The prior dis-
tribution over the sequence (β1,j, ... , βT,j) that our
regularizer posits is closely linked to a first-order au-
toregressive process, AR(1).
6Consistent with the previous section, we assume that pa-
rameters for different features, βj and βk, are independent.
</bodyText>
<equation confidence="0.752441592592593">
X1
fi1 fi2 fi3 fiT
Y1 Y2 Y3 YT
X2
X3
α,λ
XT
A = λ
1 + α −α 0 0 . . .
−α 1 + 2α −α 0 . . .
0 −α 1 + 2α −α ...
0 0 −α 1 + 2α ...
... ... ... ... ...
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎤
⎦⎥⎥⎥⎥⎥
T
t=1
R(β) = λ
β2t,j+λα
T
t=2
d
j=1
d
j=0
</equation>
<page confidence="0.982504">
597
</page>
<table confidence="0.998129">
NBER ACL
Response log(#downloads+1) 1{#citations &gt; 0}
GLM type normal / squared-loss logistic / log-loss
Metric 1 mean absolute error accuracy
Metric 2 Kendall’s τ Kendall’s τ
</table>
<tableCaption confidence="0.97807">
Table 2: Summary of the setup for the NBER download
and ACL citation prediction experiments.
</tableCaption>
<sectionHeader confidence="0.6548065" genericHeader="method">
4 Features
NBER metadata features
</sectionHeader>
<listItem confidence="0.989152461538461">
• Authors’ last names. We treat each name as a bi-
nary feature. If a paper has multiple authors, all
authors are used and they have equal weights re-
gardless of their ordering.
• NBER program(s).7 There are 19 major re-
search programs at the NBER (e.g., Monetary
Economics, Health Economics, etc.).
ACL metadata features
• Authors’ last names as binary features.
• Conference venues. We use first letter of the ACL
anthology paper ID, which correlates with its con-
ference venue (e.g., P for the ACL main confer-
ence, H for the HLT conference, etc.).8
</listItem>
<subsectionHeader confidence="0.444275">
Text features
</subsectionHeader>
<bodyText confidence="0.812886166666667">
• Binary indicator features for the presence of each
unigram, bigram, and trigram. For the NBER
data, we have separate features for titles and ab-
stracts. For the ACL data, we have separate fea-
tures for titles and full texts. We pruned text fea-
tures by document frequency (details in §5).
</bodyText>
<listItem confidence="0.981266666666667">
• Log transformed word counts. We include fea-
tures for the numbers of words in the title and the
abstract (NBER) or the full text (ACL).
</listItem>
<bodyText confidence="0.9249254">
7Almost all NBER papers are tagged with one or more pro-
grams (we assign untagged papers a “null” tag). The complete
list of NBER programs can be found at http://www.nber.
org/programs
8Papers in the ACL dataset have a tag which shows which
workshop, conference, or journal they appeared in. However,
sometimes a conference is jointly held with another confer-
ence, such that meta information in the dataset is different even
though the conference is the same. For this reason, we simply
use the first letter of the paper ID.
</bodyText>
<sectionHeader confidence="0.997747" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999936722222222">
For each of the datasets in §2, we test our models
for two tasks: forecasting about future papers (i.e.,
making predictions about papers that appeared af-
ter a training dataset) and modeling held-out papers
from the past (i.e., making predictions within the
same time period as the training dataset, on held-out
examples).
For the NBER dataset, the task is to predict the
number of downloads a paper will receive in its first
year after publication. For the ACL dataset, the task
is to predict whether a paper will be cited at all (by
another ACL paper in our dataset) within the first
three years after its publication. To our knowledge,
clean, reliable citation counts are not available for
the NBER dataset; nor are download statistics avail-
able for the ACL dataset. Table 2 summarizes the
variables of interest, model types, and evaluation
metrics for the tasks.
</bodyText>
<subsectionHeader confidence="0.946719">
5.1 Extrapolation
</subsectionHeader>
<bodyText confidence="0.999954653846154">
The lag between a paper’s publication and when its
outcome (download or citation count) can be ob-
served poses a unique methodological challenge.
Consider predicting the number of downloads over
g future time steps. If t is the time of forecasting,
we can observe the texts of all articles published be-
fore t. However, any article published in the interval
[t − g, t] is too recent for the outcome measurement
of y to be taken. We refer to the interval [t − g, t] as
the “forecast gap”. Since recent articles are some-
times the most relevant predictions at t, we do not
want to ignore them. Consider a paper at time step
t&apos;, t−g &lt; t&apos; &lt; t. To extrapolate its number of down-
loads, we consider the observed number in [t&apos;, t], and
then estimate the ratio r of downloads that occur in
the first t−t&apos; time steps, against the first g time steps,
using the fully observed portion of the training data.
We then scale the observed downloads during [t&apos;, t]
by r−1 to extrapolate. The same method is used to
extrapolate citation counts.
In preliminary experiments, we observed that ex-
trapolating responses for papers in the forecast gap
led to better performance in general. For example,
for the ridge regressions trained on all past years
with the full feature set, the error dropped from 262
to 259 when using extrapolation compared to with-
</bodyText>
<page confidence="0.991807">
598
</page>
<bodyText confidence="0.999876571428571">
out extrapolation. Also, the extrapolated download
counts were quite close to the true values (which we
have but do not use because of the forecast gap): for
example, the mean absolute error of the extrapolated
responses was 99 when extrapolated based on the
median of the fully observed portion of the training
data (measured monthly).
</bodyText>
<subsectionHeader confidence="0.991877">
5.2 Forecasting NBER Downloads
</subsectionHeader>
<bodyText confidence="0.999464972222223">
In our first set of experiments, we predict the number
of downloads of an NBER paper within one year of
its publication.
We compare four approaches for predicting
downloads. The first is a baseline that simply uses
the median of the log of the training and develop-
ment data as the prediction. The second and third
use GLMs with ridge regression-style regularization
(§3.2), trained on all past years (“all years”) and on
the single most recent past year (“one year”), respec-
tively. The last model (“time series”) is a GLM with
time series regularization (§3.3).
We divided papers by year. Figure 3 illustrates the
experimental setup. We held out a random 20% of
papers for each year from 1999–2007 as a test set for
the task of modeling the past. To define the feature
set and tune hyperparameters, we used the remain-
ing 80% of papers from 1999–2005 as our training
data and the remaining papers in 2006 as our devel-
opment data. After pruning,9 we have 37,251 to-
tal features, of which 2,549 are metadata features.
When tuning hyperparameters, we simulated the ex-
istence of a forecast gap by using extrapolated re-
sponses for papers in the last year of the training
data instead of their true responses. We considered
λ E 5{2,1,...,−5,−6}, and α E 5{3,2,...,−1,−2} and se-
lected those that led to the best performance on the
development set.
We then used the selected feature set and hyperpa-
rameters to test the forecasting and modeling capa-
bilities of each model. For forecasting, we predicted
numbers of downloads of papers in 2008 and 2009.
We used the baseline median, ridge regression, and
time series regularization models trained on papers
in 1999–2007 and 1999–2008, respectively. We
treated the last year of the training data (2007 and
</bodyText>
<footnote confidence="0.636409">
9For NBER, text features appearing in less than 0.1% or
more than 99.9% of the training documents were removed. For
ACL, the thresholds were 2% and 98%.
</footnote>
<table confidence="0.804362428571429">
NBER Experiments
&apos;99 ... &apos;04 &apos;05 &apos;06
&apos;99 ... &apos;06 &apos;07 &apos;08
training gap test
modeling test (unused)
&apos;99 ... &apos;07 &apos;08 &apos;09
training gap test
modeling test
ACL Experiments
&apos;80 ... &apos;98 &apos;99 &apos;00 &apos;01
training gap dev. (tuning, feature pruning)
modeling test (unused)
&apos;80 ... &apos;01 &apos;02 &apos;03 &apos;04
training gap test
modeling test (unused)
&apos;80 ... &apos;02 &apos;03 &apos;04
training
modeling test (unused)
&apos;80 ... &apos;03 &apos;04
training gap test
modeling test
</table>
<figureCaption confidence="0.727142">
Figure 3: An illustration of how the datasets were seg-
mented for the experiments. Portions of data for which
we report results are shaded. Time spans are not to scale.
</figureCaption>
<bodyText confidence="0.999392217391304">
2008, respectively) as a forecast gap, since we would
not have observed complete responses of papers in
these years when forecasting. For the “one year”
models, we trained ridge regressions only on the
most recent past year, using papers in 2007 and
2008, respectively, as training data.10 To test the
additive benefit of text features, we trained models
with just metadata features (NBER programs and
authors, denoted “Meta”) and with both metadata
10Papers from the most recent past year in a training set have
incomplete responses, so the models were trained on extrapo-
lated responses for that year. For the NBER development set
from 2005, a ridge regression on just 2004 papers (for which
extrapolation is needed) outperformed a regression on just 2003
(for which extrapolation is not needed), 278 to 367 mean abso-
lute error. For the ACL development set from 2001, a regression
on just 2000 (for which extrapolation is needed) led to slightly
lower performance (59% versus 61%) than a regression on just
1998 (for which extrapolation is not needed), probably due to
the relatively small number of conferences and papers in 2000.
For consistency with the other models and with the NBER ex-
periments, we evaluated regressions on the most recent (extrap-
olated) year in our ACL experiments.
</bodyText>
<figure confidence="0.99341388">
20%
80%
training
modeling test (unused)
gap
dev.
(tuning, feature
pruning)
80%
20%
80%
20%
80%
20%
80%
20%
80%
20%
80%
20%
gap
&apos;05
test
&apos;05
&apos;06
</figure>
<page confidence="0.994196">
599
</page>
<table confidence="0.999464444444444">
Features Model Modeling Forecasting
1999–07 2008 2009
– median 333 371 397
Meta one year 279 354 375
Meta all years 303 334 378
Meta time series 279 353 375
Full one year 271 346 351
Full all years 265 †300 339
Full time series ∗†245 ∗321 ∗332
</table>
<tableCaption confidence="0.908962">
Table 3: Mean absolute errors for the NBER download
predictions. “*” indicates statistical significance between
</tableCaption>
<bodyText confidence="0.977676961538461">
time series models using metadata features and the full
feature set. “†” indicates statistical significance between
the time series and ridge regression models using the full
feature set (Wilcoxon signed-rank test, p &lt; 0.01).
and text features (denoted “Full”).
To evaluate the modeling capabilities, we trained
the ridge regression and time series regularization
models on papers from 1999–2008 and predicted the
numbers of downloads of held-out papers in 1999–
2007. For comparison, we also trained ridge regres-
sion models on each individual year (“one year”)
and predicted the numbers of downloads of the held-
out papers in the corresponding year.
Table 3 shows mean absolute errors for each
method on both forecasting test splits, and mean ab-
solute errors averaged across papers over nine mod-
eling test splits. For interpretability, we report pre-
dictions in terms of download counts, though the
models were trained with log counts (§2.1). The re-
sults show that even a simple n-gram representation
of text contains a valuable, learnable signal that is
predictive of future downloads. While the time se-
ries model did not significantly outperform ridge re-
gression at predicting future downloads, it did result
in significantly better performance for modeling pa-
pers in the past.
</bodyText>
<subsectionHeader confidence="0.991489">
5.3 Forecasting ACL Citations
</subsectionHeader>
<bodyText confidence="0.998750666666667">
We now turn to the problem of predicting citation
levels. Recall that here we aim to predict whether
an ACL paper will be cited within our dataset within
three years. Our experimental setup (Figure 3) is
similar to the setup for the NBER dataset, except
that we use logistic regression to model the discrete
cited-or-not response variable. We also make the
simplifying assumption that all citations occur at the
end of each year. Therefore, the forecast gap is only
</bodyText>
<table confidence="0.999355444444444">
Feat. Model Modeling Forecasting
1980–03 2004 2005 2006
– majority 55 56 60 50
Meta one year 61 56 54 62
Meta all years 65 58 53 60
Meta time series 66 56 53 56
Full one year 69 70 64 67
Full all years 67 69 70 70
Full time series 70 ∗69 ∗70 ∗72
</table>
<tableCaption confidence="0.954608888888889">
Table 4: Classification accuracy (%) for predicting
whether ACL papers will be cited within three years. “*”
indicates statistical significance between time series mod-
els using metadata features and the full feature set (bi-
nomial sign test, p &lt; 0.01). With the full feature set,
differences between the time series and ridge (all years)
models are not statistically significant at the 0.01 level,
but for the modeling task p is estimated at 0.026, and for
the 2006 forecasting task, p is estimated at 0.050.
</tableCaption>
<bodyText confidence="0.9980515">
two years (we have observed complete citations in
the test year).
After feature pruning, there were 30,760 total fea-
tures, of which 1,694 are metadata features. We
considered A E 5{2,1,··· ,−8,−9} (“Full”) and A E
5{2,1,··· ,−11,−12} (“Meta”); and α E 5{6,5,··· ,0,−1}
(both “Full” and “Meta”), selecting the best values
using the development data.
Again, we compare four methods: a baseline of
always predicting the most frequent class in the
training data, “all years” and “one year” logistic re-
gression models, and a logistic regression with the
time series regularizer.
For the forecasting task, we used papers in 2004,
2005, and 2006 as test sets. As the training sets for
the “all years” and time series models, we used pa-
pers from 1980 up to the last year before each test
set, with the last two years extrapolated. As the
training sets for the “one year” models, we used pa-
pers from the year immediately before the test set,
with extrapolated responses.
To evaluate modeling capabilities, we predicted
citation levels of held-out papers in 1980–2003. We
used the “all years” and time series models trained
on 1980–2005. We trained “one year” models sepa-
rately for each year and predicted downloads for the
held-out papers in that year.
Table 4 shows classification accuracy for each
model on the test data for both the forecasting and
modeling tasks. It is again clear that adding text sig-
</bodyText>
<page confidence="0.992934">
600
</page>
<bodyText confidence="0.9999525">
nificantly improved the performance of the model.
Also, the time series regression model shows a
small, though not statistically significant, gain for
modeling whether past papers will be cited—as well
as similarly small gains on two of the three forecast-
ing test years.
</bodyText>
<subsectionHeader confidence="0.987223">
5.4 Ranking
</subsectionHeader>
<bodyText confidence="0.999959357142857">
We can also use the models for ranking to help de-
cide which papers are expected to have the greatest
impact. With rankings, we can use the same metric
both for download and citation predictions. For the
NBER data, we ranked test-set papers based on the
predicted numbers of downloads and computed the
correlation to the actual numbers of downloads. For
the ACL data, we ranked papers based on the prob-
ability of being cited (within the next three years)
and computed the correlation to the actual numbers
of citations.11
To measure ranking models’ ranking quality, we
used Kendall’s T, a nonparametric statistic that mea-
sures the similarity of two different orderings over
the same set of items. Here, the items are scien-
tific papers and the two metrics are the gold stan-
dard numbers of downloads (or citations) and model
predictions for the numbers of downloads, or cita-
tion probabilities. If q is the chance that a randomly
drawn pair of items will be ranked in the same way
by the two metrics, then T = 2(q − 0.5).
Table 5 shows Kendall’s T for each model for the
forecasting tasks (i.e., prediction of future citations
or downloads) in both datasets. As in the previous
experiments, we see small benefits for the time se-
ries regression model on most held-out data splits—
and larger benefits for including text features along
with metadata features.
</bodyText>
<sectionHeader confidence="0.963874" genericHeader="conclusions">
6 Analysis
</sectionHeader>
<bodyText confidence="0.9728436">
An advantage of the time series regularized regres-
sion model is its interpretability. Inspecting feature
coefficients in the model allows us to identify trends
and changes of interests over time within a scientific
community.
11Here, we use models of responses to individual papers for
ranking (i.e., in a pointwise ranking scheme). Time series reg-
ularization could also be applied to ranking models that model
pairwise preferences to optimize metrics like Kendall’s τ di-
rectly, as discussed by Joachims (2002).
</bodyText>
<table confidence="0.999606125">
Feat. Model NBER ACL
’08 ’09 ’04 ’05 ’06
Meta one year .29 .22 .17 .08 .16
Meta all years .31 .22 .15 .12 .21
Meta time series .29 .22 .14 .10 .17
Full one year .35 .31 .44 .39 .33
Full all years .43 .37 .42 .43 .40
Full time series .43 .38 .47 .44 .43
</table>
<tableCaption confidence="0.9988885">
Table 5: Kendall’s T rank correlation for future prediction
models on both datasets.
</tableCaption>
<bodyText confidence="0.999986">
First, we illustrate the difference between the time
series and the other models in Figure 4, for NBER
models’ weights for unemployment rate and infla-
tion rate appearing in a paper’s abstract. The year-
to-year weights of “one year” models fluctuate sub-
stantially, and the “all years” model is necessar-
ily constant, but the time series regularizer gives a
smooth trajectory.
</bodyText>
<subsectionHeader confidence="0.992534">
6.1 Trends
</subsectionHeader>
<bodyText confidence="0.9999309375">
Previous work has examined the flow of ideas
as trends in word and phrase frequencies, as in
the Google Books Ngram Viewer (Michel et al.,
2011).12 Topic models have been used extensively to
explore trends in low-dimensional spaces (Blei and
Lafferty, 2006; Wang et al., 2008; Wang and McCal-
lum, 2006; Ahmed and Xing, 2010). By contrast,
our approach allows us to examine trends in the im-
pact of text related to specific observation variables:
the coefficient trendline for a feature illustrates its
association with measurements of scholarly impact
(citation and download frequency).
Text frequencies can be quite different from the
discriminative weights our model assigns to fea-
tures. Figure 5 illustrates the Ot,j trends in the ACL
time series model for some selected terms that oc-
</bodyText>
<footnote confidence="0.645688">
12http://ngrams.googlelabs.com
</footnote>
<figure confidence="0.866367">
unemployment_rate inflation_rate
−0.004 0.004 time series
all years
one year
2000 2005 2010 2000 2005 2010
</figure>
<figureCaption confidence="0.934711">
Figure 4: Coefficients for two NBER bigram features.
</figureCaption>
<page confidence="0.890047">
601
</page>
<figure confidence="0.999165286624204">
−0.010 0.000 0.010 Time series coef.
1980 1990 2000
Term frequencies
−0.002 0.001 0.004
1980 1990 2000
machine_translation
parsing 0
semantics _
discourse
grammars
generation
●
●
●
● ●
● ● ● ●
● ●
● ●
● ● ●
● ●
● ●
● ● ● ●
● ● ●
● ●
● ● ●
● ●
●
● ● ● ● ●
● ●
● ●
● ● ● ●
● ●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ●
●
●
●
● ● ●
● ● ●
●
● ●
● ●
● ● ●
●
●
● ● ●
● ●
● ●
● ● ●
● ● ●
● ●
● ● ●
● ● ●
● ● ●
●
● ● ● ●●
● ● ● ● ● ●
●
●
●
0 10 20 30 40
Doc. frequency
(a) Doc. freq. vs. coef.
Citation proportion
(b) Citation prop. vs. coef.
● ●
●
●
●
●
● ● ● ● ●
● ● ●
● ● ●● ● ●
● ● ●
●
● ●
● ● ●
●
● ●
● ● ● ● ●
●
● ●
● ●
● ● ● ●
●
● ●
●
●
● ●● ●
● ●
●
● ● ● ● ●
●
●
●
●
●
●
●
●
●
0.0 0.4 0.8
●
● ●
● ●
●
●
● ● ●
● ● ●
● ● ●
● ●
●
●
● ● ● ●
● ●
●
●
●
●
●
●
● ● ●
●
●
●
●
●
●
●
● ●
● ●
● ● ●
−0.002 0.001 0.004
β
0.0000 0.0010
●
●●
●●
●
●
●
●
</figure>
<figureCaption confidence="0.9996725">
Figure 5: Feature trends: model coefficients vs. term fre-
quencies over time in the ACL corpus. Term freq. is the
fraction of tokens (or bigrams for m.t.) that year, that are
the term, averaged over a centered five-year window.
</figureCaption>
<bodyText confidence="0.999952363636364">
cur frequently in conference session titles. On the
right are term frequencies (with smoothing, since
year-to-year frequencies are bumpy). Most terms
decline over time. On the left, by contrast, are the
weights learned by our time series model. They
tell a very different story: for example, parsing has
shown a definite increase in interest, while interest
in grammars (e.g., formalisms) has declined some-
what. These trends have face validity, giving cre-
dence to our analysis; they also broadly agree with
Hall et al. (2008).
</bodyText>
<subsectionHeader confidence="0.996274">
6.2 Authors
</subsectionHeader>
<bodyText confidence="0.9999426">
The regression method also allows analysis of author
influence, since we fit a coefficient for each of the
authors in the ACL dataset. Figure 6(a) addresses
the following question: do prolific authors get cited
more often, even after accounting for the content of
their papers?13 The effect is present but relatively
small according to our model: the total number of
papers co-authored by an author has a weak corre-
lation to the author’s citation prediction coefficient
(T = 0.16).
Next, does the model provide more information
than the simple citation probability of an author?
Figure 6(b) compares coefficients to an author’s pa-
pers’ probability of being cited. Since we did not
prune author features, there are many authors with
</bodyText>
<footnote confidence="0.813736">
13More precisely: if a prolific author and a non-prolific au-
thor write a paper, does the prolific author’s paper have a higher
probability of being cited than the non-prolific author’s, all
other things being equal?
</footnote>
<figureCaption confidence="0.730883666666667">
Figure 6: Analysis of author citation coefficients. Every
point is one ACL author, and the vertical axis shows the
citation coefficient, compared to (a) the number of docu-
</figureCaption>
<bodyText confidence="0.999170791666667">
ments co-authored by the author; and (b) the proportion
of an author’s papers that are cited within three years.
The vertical bar is the macro-averaged citation propor-
tion across authors, 41%.
only a few papers, resulting in unsmoothed proba-
bilities of 0, 0.5, 1, etc. (these correspond to the ver-
tical “bands” in the plot). By contrast, the `2-penalty
of the model naturally assigned coefficients close to
zero for such authors if it is justified.
In general, the simple probability agrees with the
coefficient, but there are differences. The semantics
of the regression imply we are measuring the rela-
tive citation probability of an author, controlling for
text and venue effects. If an author has a high cita-
tion prediction coefficient but a low citation proba-
bility, that implies the author has better-cited work
than would be expected according to the n-grams in
his or her papers. We have omitted names of au-
thors from the figure for clarity and confidentiality,
but high outlier authors tend to be well-known re-
searchers in the ACL community. Obviously, since
the prediction model is not perfect, it is not possible
to completely verify this hypothesis, but we feel this
analysis is reasonably suggestive.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="references">
7 Related Work
</sectionHeader>
<bodyText confidence="0.607657714285714">
Previous work on modeling scientific literature
mostly focused on citation graphs (Borner et al.,
2003; Qazvinian and Radev, 2008). Some re-
searchers, e.g., Erosheva et al. (2004), have used
text content. Most of these are based on topic mod-
els: Gerrish and Blei (2010) measure scholarly im-
pact, Hall et al. (2008) study the “history of ideas”,
</bodyText>
<page confidence="0.976071">
602
</page>
<bodyText confidence="0.986345931818182">
and Ramage et al. (2010) rank universities based on References
scholarly output using topic models. A. Ahmed and E. P. Xing. 2010. Timeline: A dy-
Download rates and citation prediction were two namic hierarchical Dirichlet process model for recov-
of the main tasks in the KDD Cup 2003 (McGovern ering birth/death and evolution of topics in text stream.
et al., 2003; Brank and Leskovec, 2003). Bethard In Proc. of UAI.
and Jurafsky (2010) considered the problem slightly A. L. Berger, V. J. Della Pietra, and S. A. Della
differently and proposed an information retrieval ap- Pietra. 1996. A maximum entropy approach to nat-
proach to citation prediction. Our approach is novel ural language processing. Computational Linguistics,
in that we formulate the problem as a forecasting 22(1):39–71.
task and we seek to predict future impact of articles. S. Bethard and D. Jurafsky. 2010. Who should I cite?
Linear regression with text features has been used Learning literature search models from citation behav-
to predict financial risk (Kogan et al., 2009) and ior. In Proc. of CIKM.
movie revenues (Joshi et al., 2010). While the fore- D. Blei and J. Lafferty. 2006. Dynamic topic models. In
casts in those papers are similar to ours, those au- Proc. of ICML.
thors did not consider a forecast gap or allowing the K. Borner, C. Chen, and K. Boyack. 2003. Visualiz-
parameters of the model to vary over time. ing knowledge domains. In B. Cronin, editor, Annual
Our time series regularization is closely related Review of Information Science and Technology, vol-
to the fused lasso (Tibshirani et al., 2005). It pe- ume 37, pages 179–255. Information Today, Inc.
nalizes a loss function by the `1-norm of the co- G. Box, G. M. Jenkins, and G. Reinsel. 2008. Time Se-
efficients and their differences. The `1-penalty for ries Analysis: Forecasting and Control. Wiley Series
differences between coefficients encourages sparsity in Probability and Statistics.
in the differences. We use the E2-norm to induce S. Boyd and L. Vandenberghe. 2004. Convex Optimiza-
smooth changes across time steps. tion. Cambridge University Press.
8 Conclusions J. Brank and J. Leskovec. 2003. The download estima-
We presented a statistical approach to predicting a tion task on KDD Cup 2003. SIGKDD Explorations,
scientific community’s response to an article, based 5(2):160–162.
on its textual content. To improve the interpretability A. Cameron and P. Trivedi. 1998. Regression Analysis of
of the linear model, we developed a novel time series Count Data. Cambridge University Press.
regularizer that encourages gradual changes across E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed
time steps. Our experiments showed that text fea- membership models of scientific publications. In
tures significantly improve accuracy of predictions Proc. of PNAS.
over baseline models, and we found that the feature S. Gerrish and D. M. Blei. 2010. A language-based
weights learned with the time series regularizer re- approach to measuring scholarly impact. In Proc. of
flect important trends in the literature. ICML.
Acknowledgements D. Hall, D. Jurafsky, and C. D. Manning. 2008. Studying
We thank the National Bureau of Economic Re- the history of ideas using topic models. In Proc. of
search for providing the NBER dataset for this EMNLP.
research, Fallaw Sowell for helpful discussions, J. D. Hamilton. 1994. Time Series Analysis. Princeton
and three anonymous reviewers for comments on University Press.
an earlier draft of this paper. This research was T. Hastie, R. Tibshirani, and J. Friedman. 2009. The Ele-
supported by the Intelligence Advanced Research ments of Statistical Learning: Data Mining, Inference,
Projects Activity under grant number N10PC20222 and Prediction. Springer.
and TeraGrid resources provided by the Pittsburgh A. E. Hoerl and R. W. Kennard. 1970. Ridge regression:
Supercomputing Center under grant number TG- Biased estimation for nonorthogonal problems. Tech-
</bodyText>
<reference confidence="0.998561074074074">
DBS110003. nometrics, 12(1):55–67.
603 T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. of KDD.
M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010.
Movie reviews and revenues: An experiment in text
regression. In Proc. of HLT-NAACL.
S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A.
Smith. 2009. Predicting risk from financial reports
with regression. In Proc. of HLT-NAACL.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming B, 45(3):503–528.
P. Mccullagh and A. J. Nelder. 1989. Generalized Linear
Models. London: Chapman &amp; Hall.
P. McCullagh. 1980. Regression models for ordinal data.
Journal of the Royal Statistical Society B, 42(2):109–
142.
A. McGovern, L. Friedland, M. Hay, B. Gallagher,
A. Fast, J. Neville, and D. Jensen. 2003. Exploit-
ing relational structure to understand publication pat-
terns in high-energy physics. SIGKDD Explorations,
5(2):165–172.
J. Michel, Y. Shen, A. Aiden, A. Veres, M. Gray, The
Google Books Team, J. Pickett, D. Hoiberg, D. Clancy,
P. Norvig, J. Orwant, S. Pinker, M. Nowak, and
E. Aiden. 2011. Quantitative analysis of culture using
millions of digitized books. Science, 331(6014):176–
182.
V. Qazvinian and D. R. Radev. 2008. Scientific paper
summarization using citation summary networks. In
Proc. of COLING.
D. R. Radev, M. T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009a. A bibliometric and network analysis of
the field of computational linguistics. Journal of the
American Society for Information Science and Tech-
nology.
D. R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009b.
The ACL anthology network corpus. In Proc. of ACL
Workshop on Natural Language Processing and Infor-
mation Retrieval for Digital Libraries.
D. Ramage, C. D. Manning, and D. A. McFarland. 2010.
Which universities lead and lag? Toward university
rankings based on scholarly output. In Proc. of NIPS
Workshop on Computational Social Science and the
Wisdom of the Crowds.
R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and
K. Knight. 2005. Sparsity and smoothness via the
fused lasso. Journal of the Royal Statistical Society B,
67(1):91–108.
X. Wang and A. McCallum. 2006. Topics over time: A
non-Markov continuous-time model of topical trends.
In Proc. of KDD.
C. Wang, D. Blei, and D. Heckerman. 2008. Continuous
time dynamic topic models. In Proc. of UAI.
</reference>
<page confidence="0.998718">
604
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972473">
<title confidence="0.999896">Predicting a Scientific Community’s Response to an Article</title>
<author confidence="0.999982">Dani Yogatama Michael Heilman Brendan O’Connor Chris Dyer</author>
<affiliation confidence="0.999946">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.99974">Pittsburgh, PA 15213, USA</address>
<author confidence="0.999745">Bryan R Routledge</author>
<affiliation confidence="0.99189">Tepper School of Business Carnegie Mellon University</affiliation>
<address confidence="0.998065">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999865">routledge@cmu.edu</email>
<abstract confidence="0.9994046">consider the problem of predicting surable responses to scientific articles based on their text content. ically, we consider papers in two fields (economics and computational linguistics) and make predictions about downloads and within-community citations. Our approach is based on generalized linear models, allowing interpretability; a novel extension that captures first-order temporal effects is also presented. We demonstrate that text features significantly improve accuracy of predictions over metadata features like authors, topical categories, and publication venues.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>