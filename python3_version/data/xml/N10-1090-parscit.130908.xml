<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007982">
<title confidence="0.999168">
A Simple Approach for HPSG Supertagging Using Dependency Information
</title>
<author confidence="0.99935">
Yao-zhong Zhang t Takuya Matsuzaki t Jun’ichi Tsujiitt§
</author>
<affiliation confidence="0.993683333333333">
† Department of Computer Science, University of Tokyo
$ School of Computer Science, University of Manchester
§National Centre for Text Mining, UK
</affiliation>
<email confidence="0.995561">
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.997176" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.941434">
In a supertagging task, sequence labeling
models are commonly used. But their lim-
ited ability to model long-distance informa-
tion presents a bottleneck to make further im-
provements. In this paper, we modeled this
long-distance information in dependency for-
malism and integrated it into the process of
HPSG supertagging. The experiments showed
that the dependency information is very in-
formative for supertag disambiguation. We
also evaluated the improved supertagger in the
HPSG parser.
</bodyText>
<sectionHeader confidence="0.99931" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891959183674">
Supertagging is a widely used speed-up technique
for lexicalized grammar parsing. It was Þrst
proposed for lexicalized tree adjoining grammar
(LTAG) (Bangalore and Joshi, 1999), then extended
to combinatory categorial grammar (CCG) (Clark,
2002) and head-driven phrase structure grammar
(HPSG) (Ninomiya et al., 2006). For deep parsing,
supertagging is an important preprocessor: an ac-
curate supertagger greatly reduces search space of
a parser. Not limited to parsing, supertags can be
used for NP chunking (Shen and Joshi, 2003), se-
mantic role labeling (Chen and Rambow, 2003) and
machine translation (Birch et al., 2007; Hassan et
al., 2007) to explore rich syntactic information con-
tained in them.
Generally speaking, supertags are lexical tem-
plates extracted from a grammar. These templates
encode possible syntactic behavior of a word. Al-
though the number of supertags is far larger than the
45 POS tags deÞned in Penn Treebank, sequence la-
beling techniques are still effective for supertagging.
Previous research (Clark, 2002) showed that a POS
sequence is very informative for supertagging, and
some extent of local syntactic information can be
captured by the context of surrounding words and
POS tags. However, since the context window
length is limited for the computational cost reasons,
there are still long-range dependencies which are not
easily captured in sequential models (Zhang et al.,
2009). In practice, the multi-tagging technique pro-
posed by Clark (2002) assigned more than one su-
pertag to each word and let the ambiguous supertags
be selected by the parser. As for other NLP applica-
tions which use supertags, resolving more supertag
ambiguities in supertagging stage is preferred. With
this consideration, we focus on supertagging and
aim to make it as accurate as possible.
In this paper, we incorporated long-distance in-
formation into supertagging. First, we used depen-
dency parser formalism to model long-distance re-
lationships between the input words, which is hard
to model in sequence labeling models. Then, we
combined the dependency information with local
context in a simple point-wise model. The experi-
ments showed that dependency information is very
informative for supertagging and we got a compet-
itive 93.70% on supertagging accuracy (fed golden
POS). In addition, we also evaluated the improved
supertagger in the HPSG parser.
</bodyText>
<sectionHeader confidence="0.839495" genericHeader="method">
2 HPSG Supertagging and Dependency
</sectionHeader>
<subsectionHeader confidence="0.97071">
2.1 HPSG Supertags
</subsectionHeader>
<bodyText confidence="0.988547444444444">
HPSG (Pollard and Sag, 1994) is a lexicalist gram-
mar framework. In HPSG, a large number of
lexical entries is used to describe word-speciÞc
syntactic characteristics, while only a small num-
ber of schemas is used to explain general con-
struction rules. These lexical entries are called
“HPSG supertags”. For example, one possi-
ble supertag for the word “like” is written like
“[NP.nom&lt;V.bse&gt;NP.acc] lxm”, which indicates
</bodyText>
<page confidence="0.978342">
645
</page>
<subsubsectionHeader confidence="0.576238">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 645–648,
</subsubsectionHeader>
<subsectionHeader confidence="0.27638">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.9999675">
the head syntactic category of “like” is verb in base
form. It has a NP subject and a NP complement.
With such fine-grained grammatical type distinc-
tions, the number of supertags is much larger than
the number of tags used in other sequence labeling
tasks. The HPSG grammar used in our experiment
includes 2,308 supertags. This increases computa-
tional cost of sequence labeling models.
</bodyText>
<subsectionHeader confidence="0.998278">
2.2 Why Use Dependency in Supertagging
</subsectionHeader>
<bodyText confidence="0.999820076923077">
By analyzing the internal structure of the supertags,
we found that subject and complements are two im-
portant syntactic properties for each supertag. If
we could predict subject and complements of the
word well, supertagging would be an easier job to
do. However, current widely used sequence labeling
models have the limited ability to catch these long-
distance syntactic relations. In supertagging stage,
tree structures are still not constructed. Dependency
formalism is an alternative way to describe these two
syntactic properties. Based on this observation, we
think dependency information could assist supertag
prediction.
</bodyText>
<figureCaption confidence="0.78500275">
Figure 1: Model structure of incorporating dependency
information into the supertagging stage. Dotted arrows
describe the augmented long distance dependency infor-
mation provided for supertag prediction.
</figureCaption>
<sectionHeader confidence="0.998088" genericHeader="method">
3 Our Method
</sectionHeader>
<subsectionHeader confidence="0.99999">
3.1 Modeling Dependency for Supertags
</subsectionHeader>
<bodyText confidence="0.999661590909091">
First of all, we need to characterize the dependency
between words for supertagging. Since exact de-
pendency locations are not encoded in supertags, to
make use of state-of-the-art dependency parser, we
recover HPSG supertag dependencies with the aid
of HPSG treebanks. The dependencies are extracted
from each branch in the HPSG trees by regarding
the non-head daughter as the modifier of the head-
daughter. HPSG schemas are expressed in depen-
dency arcs.
To model the dependency, we follow mainstream
dependency parsing formalism. Two representa-
tive methods for dependency parsing are transition-
based model like MaltParser (Nivre, 2003) and
graph-based model like MSTParser1 (McDonald et
al., 2005). Previous research (Nivre and McDon-
ald, 2008) showed that MSTParser is more accurate
than MaltParser for long dependencies. Since our
motivation is to capture long-distance dependency
as a complement for local supertagging models, we
use the projective MSTParser formalism to model
dependencies.
</bodyText>
<table confidence="0.999637125">
MOD-IN {(pi ⇐ pj)&amp;sj|(j,i) ∈ E}
{(pi ⇐ wj)&amp;sj|(j,i) ∈ E}
{(wi ⇐ pj)&amp;sj|(j,i) ∈ E}
{(wi ⇐ wj)&amp;sj|(j,i) ∈ E}
MOD-OUT {(pi ⇒ pj)&amp;si|(i, j) ∈ E}
{(pi ⇒ wj)&amp;si|(i, j) ∈ E}
{(wi ⇒ pj)&amp;si|(i, j) ∈ E}
{(wi ⇒ wj)&amp;si|(i, j) ∈ E}
</table>
<tableCaption confidence="0.998916">
Table 1: Non-local feature templates used for super-
</tableCaption>
<bodyText confidence="0.8268695">
tagging. Here, p, w and s represent POS, word
and schema respectively. Direction (Left/Right) from
MODIN/MODOUT word to the current word is also con-
sidered in the feature templates.
</bodyText>
<subsectionHeader confidence="0.998892">
3.2 Integrating Dependency into Supertagging
</subsectionHeader>
<bodyText confidence="0.999976625">
There are several ways to combine long-distance
dependency into supertagging. Integrating depen-
dency information into training process would be
more intuitive. Here, we use feature-based integra-
tion. The base model is a point-wise averaged per-
ceptron (PW-AP) which has been shown very ef-
fective (Zhang et al., 2009). The improved model
structure is described in Figure 1. The long-distance
information is formalized as first-order dependency.
For the word being predicted, we extract its modi-
fiers (MODIN) and its head (MODOUT) (Table 1)
based on first-order dependency arcs. Then MODIN
and MODOUT relations are combined as features
with local context for supertag prediction. To com-
pare with previous work, the basic local context fea-
tures are the same as in Matsuzaki et al. (2007).
</bodyText>
<footnote confidence="0.991822">
1http://sourceforge.net/projects/mstparser/
</footnote>
<page confidence="0.984873">
646
</page>
<table confidence="0.996747705882353">
4 Experiments
We evaluated dependency-informed supertagger
(PW-DEP) both by supertag accuracy 2 and by a
HPSG parser. The experiments were conducted on
WSJ-HPSG treebank (Miyao, 2006). Sections 02-
21 were used to train the dependency parser, the
dependency-informed supertagger and the HPSG
parser. Section 23 was used as the testing set. The
evaluation metric for HPSG parser is the accuracy
of predicate-argument relations in the parser’s out-
put, as in previous work (Sagae et al., 2007).
Model Dep Acc%† Acc%
PW-AP / 91.14
PW-DEP 90.98 92.18
PW-AP (gold POS) / 92.48
PW-DEP (gold POS) 92.05 93.70
100 97.43
</table>
<tableCaption confidence="0.921813">
Table 2: Supertagging accuracy on section 23. (†)
Dependencies are given by MSTParser evaluated with
</tableCaption>
<figureCaption confidence="0.6759066">
labeled accuracy. PW-AP is the baseline point-wise
averaged perceptron model. PW-DEP is point-wise
dependency-informed model. The automatically tagged
POS tags were given by a maximum entropy tagger with
97.39% accuracy.
</figureCaption>
<subsectionHeader confidence="0.913509">
4.1 Results on Supertagging
</subsectionHeader>
<bodyText confidence="0.999880105263158">
We first evaluated the upper-bound of dependency-
informed supertagging model, given gold standard
first-order dependencies. As shown in Table 2,
with such long-distance information supertagging
accuracy can reach 97.43%. Comparing to point-
wise model (PW-AP) which only used local con-
text (92.48%), this absolute 4.95% gain indicated
that dependency information is really informative
for supertagging. When automatically predicted de-
pendency relations were given, there still were ab-
solute 1.04% (auto POS) and 1.22% (gold POS) im-
provements from baseline PW-AP model.
We also compared supertagging results with pre-
vious works (reported on section 22). Here we
mainly compared the dependency-informed point-
wise models with perceptron-based Bayes point ma-
chine (BPM) plus CFG-filter (Zhang et al., 2009).
To the best of our knowledge, these are the state-of-
the-art results on the same dataset with gold POS
</bodyText>
<footnote confidence="0.491281">
2ÒUNKÓ supertags are ignored in evaluation as previous.
</footnote>
<figureCaption confidence="0.9989895">
Figure 2: HPSG Parser F-score on section 23, given au-
tomatically tagged POS.
</figureCaption>
<bodyText confidence="0.999847">
tags. CFG-filtering can be considered as an al-
ternative way of incorporating long-distance con-
straints on supertagging results. Although our base-
line system was slightly behind (PW-AP: 92.16%
vs. BPM:92.53%), the final accuracies of grammati-
cally constrained models were very close (PW-DEP:
93.53% vs. BPM-CFG: 93.60%); They were not sta-
tistically significantly different (P-value is 0.26). As
the result of oracle PW-DEP indicated, supertagging
accuracy can be further improved with better depen-
dency modeling (e.g., with a semi-supervised de-
pendency parser), which makes it more extensible
and attractive than using CFG-filter after the super-
tagging process.
</bodyText>
<subsectionHeader confidence="0.995372">
4.2 HPSG parsing results
</subsectionHeader>
<bodyText confidence="0.9972445">
We also evaluated the dependency-informed su-
pertagger in a HPSG parser. Considering the effi-
ciency, we use the HPSG parser3 described by Ma-
tsuzaki et al. (2007).
In practice, several supertag candidates are re-
served for each word to avoid parsing failure. To
evaluate the quality of the two supertaggers, we re-
stricted the number of each word’s supertag candi-
dates fed to the HPSG parser. As shown in Figure 2,
for the case when only one supertag was predicted
for each word, F-score of the HPSG parser using
dependency-informed supertagger is 5.06% higher
than the parser using the baseline supertagger mod-
ule. As the candidate number increased, the gap nar-
rowed: when all candidates were given, the gains
gradually came down to 0.2%. This indicated that
</bodyText>
<footnote confidence="0.993662">
3Enju v2.3.1, http://www-tsujii.is.s.u-tokyo.ac.jp/enju.
</footnote>
<page confidence="0.996789">
647
</page>
<bodyText confidence="0.999964115384615">
improved supertagger can optimize the search space
of the deep parser, which may contribute to more ac-
curate and fast deep parsing. From another aspect,
supertagging can be viewed as an interface to com-
bine different types of parsers.
As for the overall parsing time, we didn’t opti-
mize for speed in current setting. The parsing time4
saved by using the improved supertagger (around
6.0 ms/sen, 21.5% time reduction) can not compen-
sate for the extra cost of MSTParser (around 73.8
ms/sen) now. But there is much room to improve the
final speed (e.g., optimizing the dependency parser
for speed or reusing acquired dependencies for ef-
fective pruning). In addition, small beam-size can be
“safely” used with improved supertagger for speed.
Using shallow dependencies in deep HPSG pars-
ing has been previously explored by Sagae et al.
(2007), who used dependency constraints in schema
application stage to guide HPSG tree construction
(F-score was improved from 87.2% to 87.9% with
a single shift-reduce dependency parser). Since the
baseline parser is different, we didn’t make a direct
comparison here. However, it would be interesting
to compare these two different ways of incorporat-
ing the dependency parser into HPSG parsing. We
left it as further work.
</bodyText>
<sectionHeader confidence="0.999652" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999964090909091">
In this paper, focusing on improving the accu-
racy of supertagging, we proposed a simple but
effective way to incorporate long-distance depen-
dency relations into supertagging. The experiments
mainly showed that these long-distance dependen-
cies, which are not easy to model in traditional se-
quence labeling models, are very informative for su-
pertag predictions. Although these were preliminary
results, the method shows its potential strength for
related applications. Not limited to HPSG, it can be
extended to other lexicalized grammar supertaggers.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98333675">
Thanks to the anonymous reviewers for valuable
comments. We also thank Goran Topic for his self-
less help. The first author was supported by The
University of Tokyo Fellowship (UT-Fellowship).
</bodyText>
<footnote confidence="0.898401">
4Tested on section 23 (2291 sentences) using an AMD
Opteron 2.4GHz server, given all supertag candidates.
</footnote>
<bodyText confidence="0.896006">
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
</bodyText>
<sectionHeader confidence="0.995333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999598979166667">
Srinivas Bangalore and Aravind K. Joshi. 1999. Super-
tagging: An approach to almost parsing. Computa-
tional Linguistics, 25:237–265.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003.
Stephen Clark. 2002. Supertagging for combinatory cat-
egorial grammar. In Proceedings of the 6th Interna-
tional Workshop on Tree Adjoining Grammars and Re-
lated Frameworks (TAG+ 6), pages 19–24.
Hany Hassan, Mary Hearne, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of ACL 2007, pages 288–295.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2007. Efficient hpsg parsing with supertagging and
cfg-filtering. In Proceedings of IJCAI-07.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL-05.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Develop-
ment and Feature Forest Model. Ph.D. Dissertation,
The University of Tokyo.
Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Matsu-
zaki, and Yusuke Miyao. 2006. Extremely lexicalized
models for accurate and fast hpsg parsing. In Proceed-
ings of EMNLP-2006, pages 155–163.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of IWPT-03, pages
149–160. Citeseer.
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase
Structure Grammar. University of Chicago / CSLI.
Kenji Sagae, Yusuke Miyao, and Jun’ichi Tsujii. 2007.
Hpsg parsing with shallow dependency constraints. In
Proceedings of ACL-07.
Libin Shen and Aravind K. Joshi. 2003. A snow based
supertagger with application to np chunking. In Pro-
ceedings of ACL 2003, pages 505–512.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun’ichi Tsu-
jii. 2009. Hpsg supertagging: A sequence labeling
view. In Proceedings of IWPT-09, Paris, France.
</reference>
<page confidence="0.997049">
648
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.846365">
<title confidence="0.999916">A Simple Approach for HPSG Supertagging Using Dependency Information</title>
<author confidence="0.999915">Zhang Matsuzaki</author>
<affiliation confidence="0.986722666666667">of Computer Science, University of of Computer Science, University of Centre for Text Mining,</affiliation>
<email confidence="0.937244">matuzaki,</email>
<abstract confidence="0.994398769230769">In a supertagging task, sequence labeling models are commonly used. But their limited ability to model long-distance information presents a bottleneck to make further improvements. In this paper, we modeled this long-distance information in dependency formalism and integrated it into the process of HPSG supertagging. The experiments showed that the dependency information is very informative for supertag disambiguation. We also evaluated the improved supertagger in the HPSG parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--237</pages>
<contexts>
<context position="1011" citStr="Bangalore and Joshi, 1999" startWordPosition="139" endWordPosition="142">e commonly used. But their limited ability to model long-distance information presents a bottleneck to make further improvements. In this paper, we modeled this long-distance information in dependency formalism and integrated it into the process of HPSG supertagging. The experiments showed that the dependency information is very informative for supertag disambiguation. We also evaluated the improved supertagger in the HPSG parser. 1 Introduction Supertagging is a widely used speed-up technique for lexicalized grammar parsing. It was Þrst proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999), then extended to combinatory categorial grammar (CCG) (Clark, 2002) and head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). For deep parsing, supertagging is an important preprocessor: an accurate supertagger greatly reduces search space of a parser. Not limited to parsing, supertags can be used for NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Birch et al., 2007; Hassan et al., 2007) to explore rich syntactic information contained in them. Generally speaking, supertags are lexical templates extracted from a gramm</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25:237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>CCG supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1457" citStr="Birch et al., 2007" startWordPosition="207" endWordPosition="210">rtagging is a widely used speed-up technique for lexicalized grammar parsing. It was Þrst proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999), then extended to combinatory categorial grammar (CCG) (Clark, 2002) and head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). For deep parsing, supertagging is an important preprocessor: an accurate supertagger greatly reduces search space of a parser. Not limited to parsing, supertags can be used for NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Birch et al., 2007; Hassan et al., 2007) to explore rich syntactic information contained in them. Generally speaking, supertags are lexical templates extracted from a grammar. These templates encode possible syntactic behavior of a word. Although the number of supertags is far larger than the 45 POS tags deÞned in Penn Treebank, sequence labeling techniques are still effective for supertagging. Previous research (Clark, 2002) showed that a POS sequence is very informative for supertagging, and some extent of local syntactic information can be captured by the context of surrounding words and POS tags. However, s</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007. CCG supertags in factored statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Owen Rambow</author>
</authors>
<title>Use of deep linguistic features for the recognition and labeling of semantic arguments.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003.</booktitle>
<contexts>
<context position="1413" citStr="Chen and Rambow, 2003" startWordPosition="200" endWordPosition="203">ertagger in the HPSG parser. 1 Introduction Supertagging is a widely used speed-up technique for lexicalized grammar parsing. It was Þrst proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999), then extended to combinatory categorial grammar (CCG) (Clark, 2002) and head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). For deep parsing, supertagging is an important preprocessor: an accurate supertagger greatly reduces search space of a parser. Not limited to parsing, supertags can be used for NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Birch et al., 2007; Hassan et al., 2007) to explore rich syntactic information contained in them. Generally speaking, supertags are lexical templates extracted from a grammar. These templates encode possible syntactic behavior of a word. Although the number of supertags is far larger than the 45 POS tags deÞned in Penn Treebank, sequence labeling techniques are still effective for supertagging. Previous research (Clark, 2002) showed that a POS sequence is very informative for supertagging, and some extent of local syntactic information can be captured by the context o</context>
</contexts>
<marker>Chen, Rambow, 2003</marker>
<rawString>John Chen and Owen Rambow. 2003. Use of deep linguistic features for the recognition and labeling of semantic arguments. In Proceedings of EMNLP-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Supertagging for combinatory categorial grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+ 6),</booktitle>
<pages>pages</pages>
<contexts>
<context position="1080" citStr="Clark, 2002" startWordPosition="150" endWordPosition="151">nts a bottleneck to make further improvements. In this paper, we modeled this long-distance information in dependency formalism and integrated it into the process of HPSG supertagging. The experiments showed that the dependency information is very informative for supertag disambiguation. We also evaluated the improved supertagger in the HPSG parser. 1 Introduction Supertagging is a widely used speed-up technique for lexicalized grammar parsing. It was Þrst proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999), then extended to combinatory categorial grammar (CCG) (Clark, 2002) and head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). For deep parsing, supertagging is an important preprocessor: an accurate supertagger greatly reduces search space of a parser. Not limited to parsing, supertags can be used for NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Birch et al., 2007; Hassan et al., 2007) to explore rich syntactic information contained in them. Generally speaking, supertags are lexical templates extracted from a grammar. These templates encode possible syntactic behavior of a word. Alt</context>
<context position="2313" citStr="Clark (2002)" startWordPosition="342" endWordPosition="343">upertags is far larger than the 45 POS tags deÞned in Penn Treebank, sequence labeling techniques are still effective for supertagging. Previous research (Clark, 2002) showed that a POS sequence is very informative for supertagging, and some extent of local syntactic information can be captured by the context of surrounding words and POS tags. However, since the context window length is limited for the computational cost reasons, there are still long-range dependencies which are not easily captured in sequential models (Zhang et al., 2009). In practice, the multi-tagging technique proposed by Clark (2002) assigned more than one supertag to each word and let the ambiguous supertags be selected by the parser. As for other NLP applications which use supertags, resolving more supertag ambiguities in supertagging stage is preferred. With this consideration, we focus on supertagging and aim to make it as accurate as possible. In this paper, we incorporated long-distance information into supertagging. First, we used dependency parser formalism to model long-distance relationships between the input words, which is hard to model in sequence labeling models. Then, we combined the dependency information </context>
</contexts>
<marker>Clark, 2002</marker>
<rawString>Stephen Clark. 2002. Supertagging for combinatory categorial grammar. In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+ 6), pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Supertagged phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>288--295</pages>
<contexts>
<context position="1479" citStr="Hassan et al., 2007" startWordPosition="211" endWordPosition="214"> used speed-up technique for lexicalized grammar parsing. It was Þrst proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999), then extended to combinatory categorial grammar (CCG) (Clark, 2002) and head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). For deep parsing, supertagging is an important preprocessor: an accurate supertagger greatly reduces search space of a parser. Not limited to parsing, supertags can be used for NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Birch et al., 2007; Hassan et al., 2007) to explore rich syntactic information contained in them. Generally speaking, supertags are lexical templates extracted from a grammar. These templates encode possible syntactic behavior of a word. Although the number of supertags is far larger than the 45 POS tags deÞned in Penn Treebank, sequence labeling techniques are still effective for supertagging. Previous research (Clark, 2002) showed that a POS sequence is very informative for supertagging, and some extent of local syntactic information can be captured by the context of surrounding words and POS tags. However, since the context windo</context>
</contexts>
<marker>Hassan, Hearne, Way, 2007</marker>
<rawString>Hany Hassan, Mary Hearne, and Andy Way. 2007. Supertagged phrase-based statistical machine translation. In Proceedings of ACL 2007, pages 288–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficient hpsg parsing with supertagging and cfg-filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI-07.</booktitle>
<contexts>
<context position="7457" citStr="Matsuzaki et al. (2007)" startWordPosition="1137" endWordPosition="1140">Here, we use feature-based integration. The base model is a point-wise averaged perceptron (PW-AP) which has been shown very effective (Zhang et al., 2009). The improved model structure is described in Figure 1. The long-distance information is formalized as first-order dependency. For the word being predicted, we extract its modifiers (MODIN) and its head (MODOUT) (Table 1) based on first-order dependency arcs. Then MODIN and MODOUT relations are combined as features with local context for supertag prediction. To compare with previous work, the basic local context features are the same as in Matsuzaki et al. (2007). 1http://sourceforge.net/projects/mstparser/ 646 4 Experiments We evaluated dependency-informed supertagger (PW-DEP) both by supertag accuracy 2 and by a HPSG parser. The experiments were conducted on WSJ-HPSG treebank (Miyao, 2006). Sections 02- 21 were used to train the dependency parser, the dependency-informed supertagger and the HPSG parser. Section 23 was used as the testing set. The evaluation metric for HPSG parser is the accuracy of predicate-argument relations in the parser’s output, as in previous work (Sagae et al., 2007). Model Dep Acc%† Acc% PW-AP / 91.14 PW-DEP 90.98 92.18 PW-A</context>
<context position="10350" citStr="Matsuzaki et al. (2007)" startWordPosition="1568" endWordPosition="1572">racies of grammatically constrained models were very close (PW-DEP: 93.53% vs. BPM-CFG: 93.60%); They were not statistically significantly different (P-value is 0.26). As the result of oracle PW-DEP indicated, supertagging accuracy can be further improved with better dependency modeling (e.g., with a semi-supervised dependency parser), which makes it more extensible and attractive than using CFG-filter after the supertagging process. 4.2 HPSG parsing results We also evaluated the dependency-informed supertagger in a HPSG parser. Considering the efficiency, we use the HPSG parser3 described by Matsuzaki et al. (2007). In practice, several supertag candidates are reserved for each word to avoid parsing failure. To evaluate the quality of the two supertaggers, we restricted the number of each word’s supertag candidates fed to the HPSG parser. As shown in Figure 2, for the case when only one supertag was predicted for each word, F-score of the HPSG parser using dependency-informed supertagger is 5.06% higher than the parser using the baseline supertagger module. As the candidate number increased, the gap narrowed: when all candidates were given, the gains gradually came down to 0.2%. This indicated that 3Enj</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Efficient hpsg parsing with supertagging and cfg-filtering. In Proceedings of IJCAI-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05.</booktitle>
<contexts>
<context position="5880" citStr="McDonald et al., 2005" startWordPosition="884" endWordPosition="887">rtagging. Since exact dependency locations are not encoded in supertags, to make use of state-of-the-art dependency parser, we recover HPSG supertag dependencies with the aid of HPSG treebanks. The dependencies are extracted from each branch in the HPSG trees by regarding the non-head daughter as the modifier of the headdaughter. HPSG schemas are expressed in dependency arcs. To model the dependency, we follow mainstream dependency parsing formalism. Two representative methods for dependency parsing are transitionbased model like MaltParser (Nivre, 2003) and graph-based model like MSTParser1 (McDonald et al., 2005). Previous research (Nivre and McDonald, 2008) showed that MSTParser is more accurate than MaltParser for long dependencies. Since our motivation is to capture long-distance dependency as a complement for local supertagging models, we use the projective MSTParser formalism to model dependencies. MOD-IN {(pi ⇐ pj)&amp;sj|(j,i) ∈ E} {(pi ⇐ wj)&amp;sj|(j,i) ∈ E} {(wi ⇐ pj)&amp;sj|(j,i) ∈ E} {(wi ⇐ wj)&amp;sj|(j,i) ∈ E} MOD-OUT {(pi ⇒ pj)&amp;si|(i, j) ∈ E} {(pi ⇒ wj)&amp;si|(i, j) ∈ E} {(wi ⇒ pj)&amp;si|(i, j) ∈ E} {(wi ⇒ wj)&amp;si|(i, j) ∈ E} Table 1: Non-local feature templates used for supertagging. Here, p, w and s represe</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
</authors>
<title>From Linguistic Theory to Syntactic Analysis: Corpus-Oriented Grammar Development and Feature Forest Model.</title>
<date>2006</date>
<tech>Ph.D. Dissertation,</tech>
<institution>The University of Tokyo.</institution>
<contexts>
<context position="7690" citStr="Miyao, 2006" startWordPosition="1167" endWordPosition="1168"> formalized as first-order dependency. For the word being predicted, we extract its modifiers (MODIN) and its head (MODOUT) (Table 1) based on first-order dependency arcs. Then MODIN and MODOUT relations are combined as features with local context for supertag prediction. To compare with previous work, the basic local context features are the same as in Matsuzaki et al. (2007). 1http://sourceforge.net/projects/mstparser/ 646 4 Experiments We evaluated dependency-informed supertagger (PW-DEP) both by supertag accuracy 2 and by a HPSG parser. The experiments were conducted on WSJ-HPSG treebank (Miyao, 2006). Sections 02- 21 were used to train the dependency parser, the dependency-informed supertagger and the HPSG parser. Section 23 was used as the testing set. The evaluation metric for HPSG parser is the accuracy of predicate-argument relations in the parser’s output, as in previous work (Sagae et al., 2007). Model Dep Acc%† Acc% PW-AP / 91.14 PW-DEP 90.98 92.18 PW-AP (gold POS) / 92.48 PW-DEP (gold POS) 92.05 93.70 100 97.43 Table 2: Supertagging accuracy on section 23. (†) Dependencies are given by MSTParser evaluated with labeled accuracy. PW-AP is the baseline point-wise averaged perceptron </context>
</contexts>
<marker>Miyao, 2006</marker>
<rawString>Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-Oriented Grammar Development and Feature Forest Model. Ph.D. Dissertation, The University of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Yoshimasa Tsuruoka</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
</authors>
<title>Extremely lexicalized models for accurate and fast hpsg parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP-2006,</booktitle>
<pages>155--163</pages>
<contexts>
<context position="1152" citStr="Ninomiya et al., 2006" startWordPosition="158" endWordPosition="161">we modeled this long-distance information in dependency formalism and integrated it into the process of HPSG supertagging. The experiments showed that the dependency information is very informative for supertag disambiguation. We also evaluated the improved supertagger in the HPSG parser. 1 Introduction Supertagging is a widely used speed-up technique for lexicalized grammar parsing. It was Þrst proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999), then extended to combinatory categorial grammar (CCG) (Clark, 2002) and head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). For deep parsing, supertagging is an important preprocessor: an accurate supertagger greatly reduces search space of a parser. Not limited to parsing, supertags can be used for NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Birch et al., 2007; Hassan et al., 2007) to explore rich syntactic information contained in them. Generally speaking, supertags are lexical templates extracted from a grammar. These templates encode possible syntactic behavior of a word. Although the number of supertags is far larger than the 45 POS tags deÞned </context>
</contexts>
<marker>Ninomiya, Tsuruoka, Matsuzaki, Miyao, 2006</marker>
<rawString>Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Matsuzaki, and Yusuke Miyao. 2006. Extremely lexicalized models for accurate and fast hpsg parsing. In Proceedings of EMNLP-2006, pages 155–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="5926" citStr="Nivre and McDonald, 2008" startWordPosition="890" endWordPosition="894">are not encoded in supertags, to make use of state-of-the-art dependency parser, we recover HPSG supertag dependencies with the aid of HPSG treebanks. The dependencies are extracted from each branch in the HPSG trees by regarding the non-head daughter as the modifier of the headdaughter. HPSG schemas are expressed in dependency arcs. To model the dependency, we follow mainstream dependency parsing formalism. Two representative methods for dependency parsing are transitionbased model like MaltParser (Nivre, 2003) and graph-based model like MSTParser1 (McDonald et al., 2005). Previous research (Nivre and McDonald, 2008) showed that MSTParser is more accurate than MaltParser for long dependencies. Since our motivation is to capture long-distance dependency as a complement for local supertagging models, we use the projective MSTParser formalism to model dependencies. MOD-IN {(pi ⇐ pj)&amp;sj|(j,i) ∈ E} {(pi ⇐ wj)&amp;sj|(j,i) ∈ E} {(wi ⇐ pj)&amp;sj|(j,i) ∈ E} {(wi ⇐ wj)&amp;sj|(j,i) ∈ E} MOD-OUT {(pi ⇒ pj)&amp;si|(i, j) ∈ E} {(pi ⇒ wj)&amp;si|(i, j) ∈ E} {(wi ⇒ pj)&amp;si|(i, j) ∈ E} {(wi ⇒ wj)&amp;si|(i, j) ∈ E} Table 1: Non-local feature templates used for supertagging. Here, p, w and s represent POS, word and schema respectively. Directio</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT-03,</booktitle>
<pages>149--160</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="5818" citStr="Nivre, 2003" startWordPosition="877" endWordPosition="878">o characterize the dependency between words for supertagging. Since exact dependency locations are not encoded in supertags, to make use of state-of-the-art dependency parser, we recover HPSG supertag dependencies with the aid of HPSG treebanks. The dependencies are extracted from each branch in the HPSG trees by regarding the non-head daughter as the modifier of the headdaughter. HPSG schemas are expressed in dependency arcs. To model the dependency, we follow mainstream dependency parsing formalism. Two representative methods for dependency parsing are transitionbased model like MaltParser (Nivre, 2003) and graph-based model like MSTParser1 (McDonald et al., 2005). Previous research (Nivre and McDonald, 2008) showed that MSTParser is more accurate than MaltParser for long dependencies. Since our motivation is to capture long-distance dependency as a complement for local supertagging models, we use the projective MSTParser formalism to model dependencies. MOD-IN {(pi ⇐ pj)&amp;sj|(j,i) ∈ E} {(pi ⇐ wj)&amp;sj|(j,i) ∈ E} {(wi ⇐ pj)&amp;sj|(j,i) ∈ E} {(wi ⇐ wj)&amp;sj|(j,i) ∈ E} MOD-OUT {(pi ⇒ pj)&amp;si|(i, j) ∈ E} {(pi ⇒ wj)&amp;si|(i, j) ∈ E} {(wi ⇒ pj)&amp;si|(i, j) ∈ E} {(wi ⇒ wj)&amp;si|(i, j) ∈ E} Table 1: Non-local fea</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>J. Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT-03, pages 149–160. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<date>1994</date>
<institution>Head-driven Phrase Structure Grammar. University of Chicago / CSLI.</institution>
<contexts>
<context position="3283" citStr="Pollard and Sag, 1994" startWordPosition="492" endWordPosition="495">ated long-distance information into supertagging. First, we used dependency parser formalism to model long-distance relationships between the input words, which is hard to model in sequence labeling models. Then, we combined the dependency information with local context in a simple point-wise model. The experiments showed that dependency information is very informative for supertagging and we got a competitive 93.70% on supertagging accuracy (fed golden POS). In addition, we also evaluated the improved supertagger in the HPSG parser. 2 HPSG Supertagging and Dependency 2.1 HPSG Supertags HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, a large number of lexical entries is used to describe word-speciÞc syntactic characteristics, while only a small number of schemas is used to explain general construction rules. These lexical entries are called “HPSG supertags”. For example, one possible supertag for the word “like” is written like “[NP.nom&lt;V.bse&gt;NP.acc] lxm”, which indicates 645 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 645–648, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics the head</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago / CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Hpsg parsing with shallow dependency constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07.</booktitle>
<contexts>
<context position="7997" citStr="Sagae et al., 2007" startWordPosition="1215" endWordPosition="1218">rk, the basic local context features are the same as in Matsuzaki et al. (2007). 1http://sourceforge.net/projects/mstparser/ 646 4 Experiments We evaluated dependency-informed supertagger (PW-DEP) both by supertag accuracy 2 and by a HPSG parser. The experiments were conducted on WSJ-HPSG treebank (Miyao, 2006). Sections 02- 21 were used to train the dependency parser, the dependency-informed supertagger and the HPSG parser. Section 23 was used as the testing set. The evaluation metric for HPSG parser is the accuracy of predicate-argument relations in the parser’s output, as in previous work (Sagae et al., 2007). Model Dep Acc%† Acc% PW-AP / 91.14 PW-DEP 90.98 92.18 PW-AP (gold POS) / 92.48 PW-DEP (gold POS) 92.05 93.70 100 97.43 Table 2: Supertagging accuracy on section 23. (†) Dependencies are given by MSTParser evaluated with labeled accuracy. PW-AP is the baseline point-wise averaged perceptron model. PW-DEP is point-wise dependency-informed model. The automatically tagged POS tags were given by a maximum entropy tagger with 97.39% accuracy. 4.1 Results on Supertagging We first evaluated the upper-bound of dependencyinformed supertagging model, given gold standard first-order dependencies. As sho</context>
<context position="11844" citStr="Sagae et al. (2007)" startWordPosition="1811" endWordPosition="1814">ypes of parsers. As for the overall parsing time, we didn’t optimize for speed in current setting. The parsing time4 saved by using the improved supertagger (around 6.0 ms/sen, 21.5% time reduction) can not compensate for the extra cost of MSTParser (around 73.8 ms/sen) now. But there is much room to improve the final speed (e.g., optimizing the dependency parser for speed or reusing acquired dependencies for effective pruning). In addition, small beam-size can be “safely” used with improved supertagger for speed. Using shallow dependencies in deep HPSG parsing has been previously explored by Sagae et al. (2007), who used dependency constraints in schema application stage to guide HPSG tree construction (F-score was improved from 87.2% to 87.9% with a single shift-reduce dependency parser). Since the baseline parser is different, we didn’t make a direct comparison here. However, it would be interesting to compare these two different ways of incorporating the dependency parser into HPSG parsing. We left it as further work. 5 Conclusions In this paper, focusing on improving the accuracy of supertagging, we proposed a simple but effective way to incorporate long-distance dependency relations into supert</context>
</contexts>
<marker>Sagae, Miyao, Tsujii, 2007</marker>
<rawString>Kenji Sagae, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Hpsg parsing with shallow dependency constraints. In Proceedings of ACL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>A snow based supertagger with application to np chunking.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>505--512</pages>
<contexts>
<context position="1365" citStr="Shen and Joshi, 2003" startWordPosition="192" endWordPosition="195">ambiguation. We also evaluated the improved supertagger in the HPSG parser. 1 Introduction Supertagging is a widely used speed-up technique for lexicalized grammar parsing. It was Þrst proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999), then extended to combinatory categorial grammar (CCG) (Clark, 2002) and head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). For deep parsing, supertagging is an important preprocessor: an accurate supertagger greatly reduces search space of a parser. Not limited to parsing, supertags can be used for NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Birch et al., 2007; Hassan et al., 2007) to explore rich syntactic information contained in them. Generally speaking, supertags are lexical templates extracted from a grammar. These templates encode possible syntactic behavior of a word. Although the number of supertags is far larger than the 45 POS tags deÞned in Penn Treebank, sequence labeling techniques are still effective for supertagging. Previous research (Clark, 2002) showed that a POS sequence is very informative for supertagging, and some extent of local syntac</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. A snow based supertagger with application to np chunking. In Proceedings of ACL 2003, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yao-zhong Zhang</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Hpsg supertagging: A sequence labeling view.</title>
<date>2009</date>
<booktitle>In Proceedings of IWPT-09,</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="2246" citStr="Zhang et al., 2009" startWordPosition="330" endWordPosition="333">tes encode possible syntactic behavior of a word. Although the number of supertags is far larger than the 45 POS tags deÞned in Penn Treebank, sequence labeling techniques are still effective for supertagging. Previous research (Clark, 2002) showed that a POS sequence is very informative for supertagging, and some extent of local syntactic information can be captured by the context of surrounding words and POS tags. However, since the context window length is limited for the computational cost reasons, there are still long-range dependencies which are not easily captured in sequential models (Zhang et al., 2009). In practice, the multi-tagging technique proposed by Clark (2002) assigned more than one supertag to each word and let the ambiguous supertags be selected by the parser. As for other NLP applications which use supertags, resolving more supertag ambiguities in supertagging stage is preferred. With this consideration, we focus on supertagging and aim to make it as accurate as possible. In this paper, we incorporated long-distance information into supertagging. First, we used dependency parser formalism to model long-distance relationships between the input words, which is hard to model in sequ</context>
<context position="6989" citStr="Zhang et al., 2009" startWordPosition="1062" endWordPosition="1065">{(wi ⇒ wj)&amp;si|(i, j) ∈ E} Table 1: Non-local feature templates used for supertagging. Here, p, w and s represent POS, word and schema respectively. Direction (Left/Right) from MODIN/MODOUT word to the current word is also considered in the feature templates. 3.2 Integrating Dependency into Supertagging There are several ways to combine long-distance dependency into supertagging. Integrating dependency information into training process would be more intuitive. Here, we use feature-based integration. The base model is a point-wise averaged perceptron (PW-AP) which has been shown very effective (Zhang et al., 2009). The improved model structure is described in Figure 1. The long-distance information is formalized as first-order dependency. For the word being predicted, we extract its modifiers (MODIN) and its head (MODOUT) (Table 1) based on first-order dependency arcs. Then MODIN and MODOUT relations are combined as features with local context for supertag prediction. To compare with previous work, the basic local context features are the same as in Matsuzaki et al. (2007). 1http://sourceforge.net/projects/mstparser/ 646 4 Experiments We evaluated dependency-informed supertagger (PW-DEP) both by supert</context>
<context position="9267" citStr="Zhang et al., 2009" startWordPosition="1402" endWordPosition="1405">supertagging accuracy can reach 97.43%. Comparing to pointwise model (PW-AP) which only used local context (92.48%), this absolute 4.95% gain indicated that dependency information is really informative for supertagging. When automatically predicted dependency relations were given, there still were absolute 1.04% (auto POS) and 1.22% (gold POS) improvements from baseline PW-AP model. We also compared supertagging results with previous works (reported on section 22). Here we mainly compared the dependency-informed pointwise models with perceptron-based Bayes point machine (BPM) plus CFG-filter (Zhang et al., 2009). To the best of our knowledge, these are the state-ofthe-art results on the same dataset with gold POS 2ÒUNKÓ supertags are ignored in evaluation as previous. Figure 2: HPSG Parser F-score on section 23, given automatically tagged POS. tags. CFG-filtering can be considered as an alternative way of incorporating long-distance constraints on supertagging results. Although our baseline system was slightly behind (PW-AP: 92.16% vs. BPM:92.53%), the final accuracies of grammatically constrained models were very close (PW-DEP: 93.53% vs. BPM-CFG: 93.60%); They were not statistically significantly d</context>
</contexts>
<marker>Zhang, Matsuzaki, Tsujii, 2009</marker>
<rawString>Yao-zhong Zhang, Takuya Matsuzaki, and Jun’ichi Tsujii. 2009. Hpsg supertagging: A sequence labeling view. In Proceedings of IWPT-09, Paris, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>