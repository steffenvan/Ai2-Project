<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.990627">
Generation in Machine Translation from Deep Syntactic Trees
</title>
<author confidence="0.945219">
Keith Hall
</author>
<affiliation confidence="0.892021">
Center for Language and Speech Processing
</affiliation>
<address confidence="0.694038">
Johns Hopkins University
Baltimore, MD 21218
</address>
<email confidence="0.983877">
keith hall@jhu.edu
</email>
<author confidence="0.869104">
Petr Nˇemec
</author>
<affiliation confidence="0.79502">
Institute of Formal and Applied Linguistics
Charles University
Prague, Czech Republic
</affiliation>
<email confidence="0.995988">
nemec@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999554263157895">
In this paper we explore a generative
model for recovering surface syntax and
strings from deep-syntactic tree structures.
Deep analysis has been proposed for a
number of language and speech process-
ing tasks, such as machine translation and
paraphrasing of speech transcripts. In an
effort to validate one such formalism of
deep syntax, the Praguian Tectogrammat-
ical Representation (TR), we present a
model of synthesis for English which gen-
erates surface-syntactic trees as well as
strings. We propose a generative model
for function word insertion (prepositions,
definite/indefinite articles, etc.) and sub-
phrase reordering. We show by way of
empirical results that this model is ef-
fective in constructing acceptable English
sentences given impoverished trees.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943590909091">
Syntactic models for language are being reintro-
duced into language and speech processing sys-
tems thanks to the success of sophisticated statisti-
cal models of parsing (Charniak and Johnson, 2005;
Collins, 2003). Representing deep syntactic rela-
tionships is an open area of research; examples of
such models are exhibited in a variety of grammat-
ical formalisms, such as Lexical Functional Gram-
mars (Bresnan and Kaplan, 1982), Head-driven
Phrase Structure Grammars (Pollard and Sag, 1994)
and the Tectogrammatical Representation (TR) of
the Functional Generative Description (Sgall et al.,
1986). In this paper we do not attempt to analyze the
differences of these formalisms; instead, we show
how one particular formalism is sufficient for au-
tomatic analysis and synthesis. Specifically, in this
paper we provide evidence that TR is sufficient for
synthesis in English.
Augmenting models of machine translation (MT)
with syntactic features is one of the main fronts of
the MT research community. The Hiero model has
been the most successful to date by incorporating
syntactic structure amounting to simple tree struc-
tures (Chiang, 2005). Synchronous parsing mod-
els have been explored with moderate success (Wu,
1997; Quirk et al., 2005). An extension to this work
is the exploration of deeper syntactic models, such
as TR. However, a better understanding of the syn-
thesis of surface structure from the deep syntax is
necessary.
This paper presents a generative model for surface
syntax and strings of English given tectogrammati-
cal trees. Sentence generation begins by inserting
auxiliary words associated with autosemantic nodes;
these include prepositions, subordinating conjunc-
tions, modal verbs, and articles. Following this, the
linear order of nodes is modeled by a similar gen-
erative process. These two models are combined in
order to synthesize a sentence.
The Amalgam system provides a similar model
for generation from a logical form (Corston-Oliver
et al., 2002). The primary difference between our
approach and that of the Amalgam system is that
we focus on an impoverished deep structure (akin to
</bodyText>
<page confidence="0.996191">
57
</page>
<note confidence="0.540932">
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 57–64,
Rochester, New York, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999827533333333">
logical form); we restrict the deep analysis to con-
tain only the features which transfer directly across
languages; specifically, those that transfer directly
in our Czech-English machine translation system.
Amalgam targets different issues. For example,
Amalgam’s generation of prepositions and subordi-
nating conjunctions is severely restricted as most of
these are considered part of the logical form.
The work of Langkilde-Geary (2002) on the Halo-
gen system is similar to the work we present here.
The differences that distinguish their work from
ours stem from the type of deep representation from
which strings are generated. Although their syntac-
tic and semantic representations appear similar to
the Tectogrammatical Representation, more explicit
information is preserved in their representation. For
example, the Halogen representation includes mark-
ings for determiners, voice, subject position, and
dative position which simplifies the generation pro-
cess. We believe their minimally specified results are
based on input which most closely resembles the in-
put from which we generate in our experiments.
Amalgam’s reordering model is similar to the one
presented here; their model reorders constituents in
a similar way that we reorder subtrees. Both the
model of Amalgam and that presented here differ
considerably from the n-gram models of Langkilde
and Knight (1998), the TAG models of Bangalore
and Rambow (2000), and the stochastic generation
from semantic representation approach of Soricut
and Marcu (2006). In our work, we order the local-
subtrees1 of an augmented deep-structure tree based
on the syntactic features of the nodes in the tree. By
factoring these decisions to be independent for each
local-subtree, the set of strings we consider is only
constrained by the projective strucutre of the input
tree and the local permutation limit described below.
In the following sections we first provide a brief
description of the Tectogrammatical Representation
as used in our work. Both manually annotated and
synthetic TR trees are utilized in our experiments;
we present a description of each type of tree as well
as the motivation for using it. We then describe the
generative statistical process used to model the syn-
thesis of analytical (surface-syntactic) trees based
</bodyText>
<footnote confidence="0.9919435">
1A local subtree consists of a parent node (governor) and it’s
immediate children.
</footnote>
<figureCaption confidence="0.9991845">
Figure 1: Example of a manually annotated, Synthetic TR
tree (see Section 2.2).
</figureCaption>
<subsectionHeader confidence="0.4719065">
Reference: Now the network has opened a news bureau in
the Hungarian capital
</subsectionHeader>
<bodyText confidence="0.994295142857143">
Each sentence has an artificial root node labeled #. Verbs con-
tain their tense and mood (labeled T M).
on the TR trees. Details of the model’s features
are presented in the following section. Finally we
present empirical results for experiments using both
the manually annotated and automatically generated
data.
</bodyText>
<sectionHeader confidence="0.987415" genericHeader="method">
2 Tectogrammatical (Deep) Syntax
</sectionHeader>
<bodyText confidence="0.979724555555555">
The Tectogrammatical Representation (TR) comes
out of the Praguian linguistic theory known as
the Functional Generative Description of language
(Sgall et al., 1986). TR attempts to capture deep
syntactic relationships based on the valency of pred-
icates (i.e., function-argument structure) and modifi-
cation of participants (i.e., nouns used as actors, pa-
tients, etc.). A key feature of TR is that dependency
relationships are represented only for autosemantic
words (content words), meaning that synsemantic
words (syntactic function words) are encoded as fea-
tures of the grammatical relationships rather than the
actual words. Abstracting away from specific syn-
tactic lexical items allows for the representation to
be less language-specific making the representation
attractive as a medium for machine translation and
summarization.
Figure 1 shows an example TR tree, the nodes of
</bodyText>
<figure confidence="0.999127928571429">
POS: &apos;NN&apos;
POS: &apos;RB&apos;
POS: &apos;NN&apos;
POS: &apos;NN&apos;
POS: &apos;NN&apos;
POS: &apos;JJ&apos;
FORM:
LEMM:
FUNC:
#2
#
SENT
opened
open
PRED
FORM:
LEMM:
FUNC:
POS: &apos;VBN&apos;
T_M: &apos;SIM&apos;_&apos;IND&apos;
FORM:
LEMM:
FUNC:
network
network
ACT
FORM:
LEMM:
FUNC:
Now
now
TWHEN
FORM:
LEMM:
FUNC:
bureau
bureau
PAT
FORM:
LEMM:
FUNC:
capital
capital
LOC
FORM:
LEMM:
FUNC:
news
news
RSTR
FORM:
LEMM:
FUNC:
Hungarian
hungarian
RSTR
</figure>
<page confidence="0.995204">
58
</page>
<bodyText confidence="0.999989428571429">
which represent the autosemantic words of the sen-
tence. Each node is labeled with a morphologically
reduced word-form called the lemma and a functor
that describes the deep syntactic relationship to its
governor (function-argument form). Additionally,
the nodes are labeled with grammatemes that cap-
ture morphological and semantic information asso-
ciated with the autosemantic words. For example,
English verb forms are represented by the infinitive
form as the lemma and the grammatemes encode
the tense, aspect, and mood of the verb. For a de-
tailed description of the TR annotation scheme see
B¨ohmov´a et al. (2002). In Figure 1 we show only
those features that are present in the TR structures
used throughout this paper.
Both the synsemantic nodes and the left-to-right
surface order2 in the TR trees is under-specified. In
the context of machine translation, we assume the
TR word order carries no information with the ex-
ception of a single situation: the order of coordi-
nated phrases is preserved in one of our models.
</bodyText>
<subsectionHeader confidence="0.996702">
2.1 Analytic Representation
</subsectionHeader>
<bodyText confidence="0.9999662">
While it is not part of the formal TR description, the
authors of the TR annotation scheme have found it
useful to define an intermediate representation be-
tween the sentence and the TR tree (B¨ohmov´a et
al., 2002). The analytical representation (AR) is a
surface-syntactic dependency tree that encodes syn-
tactic relationships between words (i.e., object, sub-
ject, attribute, etc.). Unlike the TR layer, the analyti-
cal layer contains all words of the sentence and their
relative ordering is identical to the surface order.
</bodyText>
<subsectionHeader confidence="0.998661">
2.2 Manually Annotated TR
</subsectionHeader>
<bodyText confidence="0.996267521739131">
In order to evaluate the efficacy of the generation
model, we construct a dataset from both manually
annotated data and automatically generated data.
The information contained in the originally manu-
ally annotated TR all but specifies the surface form.
We have modified the annotated data by removing
all features except those that could be directly trans-
fered across languages. Specifically, we preserve
the following features: lemma, functor, verbal gram-
2In a TR tree, a subtree is always between the nodes to the
left and right of its governor. More specifically, all TR trees
are projective. For this reason, the relative ordering of subtrees
imposes an absolute ordering for the tree.
matemes, and part-of-speech tags. The lemma is
the morphologically reduced form of the word; for
verbs this is the infinitive form and for nouns this is
the singular form. The functor is the deep-syntactic
function of the node; for example, the deep functor
indicates whether a node is a predicate, an actor, or a
patient. Modifiers can be labeled as locative, tempo-
ral, benefactive, etc. Additionally we include a ver-
bal grammateme which encodes tense and mood as
well as a Penn Treebank style part-of-speech tag.
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="method">
3 Generative Process
</sectionHeader>
<bodyText confidence="0.978927105263158">
In this section we describe the generative process
that inserts the synsemantic auxiliary words, re-
orders the trees, and produces a sentence. Our eval-
uation will be on English data, so we describe the
models and the model features in the context of En-
glish. While the model is language independent, the
specific features and the size of the necessary condi-
tioning contexts is a function of the language.
Given a TR tree T, we wish to predict the cor-
rect auxiliary nodes A and an ordering of the words
associated with IT U Al, defined by the function
f({T U Al). The functions f determine the surface
word order of the words associated with nodes of the
auxiliary-inserted TR tree: N = IT U Al. The node
features that we use from the nodes in the TR and
AR trees are: the word lemma, the part-of-speech
(POS) tag, and the functor.3 The objective of our
model is:
arg max
</bodyText>
<equation confidence="0.9479475">
A,f
P(f|A, T)P(A|T) (1)
P(f|T,arg maxP(A|T)) (2)
A
</equation>
<bodyText confidence="0.999846142857143">
In Equation 2 we approximate the full model with a
greedy procedure. First, we predict the most likely
A according to the model P(A|T). Given A, we
compute the best ordering of the nodes of the tree,
including those introduced in A.
There is an efficient dynamic-programming solu-
tion to the objective function in Equation 1; how-
</bodyText>
<footnote confidence="0.9492576">
3The type of functor used (deep syntactic or surface-
syntactic) depends on the tree to which we are applying the
model. One form of the reordering model operates on AR trees
and therefore uses surface syntactic functors. The other model
is based on TR trees and uses deep-syntactic functors.
</footnote>
<equation confidence="0.588485">
P(A,f|T)
= arg max
A,f
� arg max
f
</equation>
<page confidence="0.981083">
59
</page>
<bodyText confidence="0.988907">
ever, in this work we experiment with the greedy
approximation.
</bodyText>
<subsectionHeader confidence="0.905857">
3.1 Insertion Model
</subsectionHeader>
<bodyText confidence="0.998901214285714">
The specific English auxiliary nodes which are not
present in TR include articles, prepositions, subor-
dinating conjunctions, and modal verbs.4 For each
node in the TR tree, the generative process predicts
which synsemantic word, if any, should be inserted
as a dependent of the current node. We make the
assumption that these decisions are determined in-
dependently.
Let T = {w1, ... , wi, ... , wk} be the nodes of
the TR tree. For each node wi, we define the asso-
ciated node ai to be the auxiliary node that should
be inserted as a dependent of wi. Given a tree T,
we wish to find the set of auxiliary nodes A =
{a1,... , ak} that should be inserted5:
</bodyText>
<equation confidence="0.995859166666667">
P(A|T)
=ri1 P(ai|a1,...,ai−1,T) (3)
fl ≈P(ai|T) (4)
i
fl ≈P(ai|wi, wg(i)) (5)
i
</equation>
<bodyText confidence="0.999161461538462">
Equation 3 is simply a factorization of the origi-
nal model, Equation 4 shows the independence as-
sumption, and in Equation 5 we make an additional
conditional independence assumption that in order
to predict auxiliary ai, we need only know the asso-
ciated node wi and its governor wg(i).6
We further divide the model into three compo-
nents: one that models articles, such as the En-
glish articles the and a; one that models preposi-
tions and subordinating conjunctions; and one that
models modal verbs. The first two models are of the
form described by Equation 5. The modal verb in-
sertion model is a deterministic mapping based on
</bodyText>
<footnote confidence="0.9563902">
4The function of synsemantic nodes are encoded by func-
tors. For example, the prepositions to, at, in, by, and on may be
used to indicate time or location. An autosemantic modifier will
be labeled as temporal or locative, but the particular preposition
is not specified.
5Note that we include the auxiliary node labeled NOAUX to
be inserted, which in fact means a node is not inserted.
6In the case of nodes whose governor is a coordinating con-
junction, the governor information comes from the governor of
the coordination node.
</footnote>
<bodyText confidence="0.9999410625">
grammatemes expressing the verb modality of the
main verb. Additionally, each model is independent
of the other and therefore up to two insertions per
TR node are possible (an article and another syntac-
tic modifier). In a variant of our model, we perform
a small set of deterministic transformations in cases
where the classifier is relatively uncertain about the
predicted insertion node (i.e., the entropy of the con-
ditional distribution is high).
We note here that unlike the Amalgam system
(Corston-Oliver et al., 2002), we do not address fea-
tures which are determined (or almost completely
determined) by the underlying deep-structure. For
example, the task of inserting prepositions is non-
trivial given we only know a node’s functor (e.g.,
the node’s valency role).
</bodyText>
<subsectionHeader confidence="0.998439">
3.2 Analytical Representation Tree Generation
</subsectionHeader>
<bodyText confidence="0.999995444444445">
We have experimented with two paradigms for syn-
thesizing sentences from TR trees. The first tech-
nique involves first generating AR trees (surface
syntax). In this model, we predict the node inser-
tions, transform the functors from TR to AR func-
tions (deep valency relationship to surface-syntactic
relationships), and then reorder the nodes. In the
second framework, we reorder the nodes directly in
the TR trees with inserted auxiliary nodes.
</bodyText>
<subsectionHeader confidence="0.802951">
3.3 Surface-order Model
</subsectionHeader>
<bodyText confidence="0.999884263157895">
The node ordering model is used to determine a pro-
jection of the tree to a string. We assume the order-
ing of the nodes in the input TR trees is arbitrary,
the reordering model proposed here is based only on
the dependency structure and the node’s attributes
(words, POS tags, etc.). In a variant of the reorder-
ing model, we assume the deep order of coordinating
conjunctions to be the surface order.
Algorithm 1 presents the bottom-up node reorder-
ing algorithm. In the first part of the algorithm, we
determine the relative ordering of child nodes. We
maximize the likelihood of a particular order via the
precedence operator ≺. If node ci ≺ ci+1, then
the subtree of the word associated with ci imme-
diately precedes the subtree of the word associated
with ci+1 in the projected sentence.
In the second half of the algorithm (starting at
line 13), we predict the position of the governor
within the previously ordered child nodes. Recall
</bodyText>
<page confidence="0.9701">
60
</page>
<figure confidence="0.95406225">
Algorithm 1 Subtree Reordering Algorithm
procedure REORDER(T,A, O) &gt; Result in O
N +—bottomUp(T U A); O +— {}
for g E N do
bestScore +— 0; o9 +— {}
5: for C +—permutation of g’s children do
for i +— 1 ... |C |do
s +— s * P(ci � ci+1|ci, ci+1, g)
end for
if s &gt; bestScore then
10: bestScore +— s; o9 +— C
end if
end for
bestScore +— 0; m +— 0
for i +— 1... |bestOrder |do
15: s +— P(ci � g � ci+1|ci, ci+1, g)
if s &gt; bestScore then
s +— bestScore ; m + —i
end if
end for
20: Insert governor c9 after mth child in o9
O+—OUo9
end for
end procedure
</figure>
<bodyText confidence="0.998439142857143">
that this is a dependency structure; knowing the gov-
ernor does not tell us where it lies on the surface
with respect to its children. The model is similar
to the general reordering model, except we consider
an absolute ordering of three nodes (left child, gov-
ernor, right child). Finally, we can reconstruct the
total ordering from the subtree ordering defined in
</bodyText>
<equation confidence="0.860298">
O = {o1, ... , on}.
</equation>
<bodyText confidence="0.999992555555556">
The procedure described here is greedy; first we
choose the best child ordering and then we choose
the location of the governor. We do this to minimize
the computational complexity of the algorithm. The
current algorithm’s runtime complexity is O(n!), but
the complexity of the alternative algorithm for which
we consider triples of child nodes is O(n!(n − 1)!).
The actual complexity is determined by the maxi-
mum number of child nodes k = |C |and is O(n � k!).
</bodyText>
<subsectionHeader confidence="0.992384">
3.4 Morphological Generation
</subsectionHeader>
<bodyText confidence="0.999919285714286">
In order to produce true English sentences, we con-
vert the lemma and POS tag to a word form. We
use John Carroll’s morphg tool7 to generate English
word forms given lemma/POS tag pairs. This is
not perfect, but it performs an adequate job at re-
covering English inflected forms. In the complete-
system evaluation, we report scores based on gener-
</bodyText>
<footnote confidence="0.8975345">
7Available on the web at:
http://www.informatics.susx.ac.uk/research/nlp/carroll/morph.html.
</footnote>
<note confidence="0.323846">
ated morphological forms.
</note>
<subsectionHeader confidence="0.802833">
3.5 Insertion Features
</subsectionHeader>
<bodyText confidence="0.999902">
Features for the insertion model come from the cur-
rent node being examined and the node’s governor.
When the governor is a coordinating conjunction,
we use features from the governor of the conjunc-
tion node. The features used are the lemma, POS
tag, and functor for the current node, and the lemma,
POS tag, and functor of the governor.
</bodyText>
<equation confidence="0.9976715">
P(ai|wi, wg) (6)
i
�= P (ai|li, ti, fi, lg, tg, fg)
i
</equation>
<bodyText confidence="0.99998225">
The left-hand side of Equation 6 is repeated from
Equation 5 above. Equation 6 shows the expanded
model for auxiliary insertion where li is the lemma,
ti is the POS tag, and fi is the functor of node wi
</bodyText>
<subsectionHeader confidence="0.998738">
3.6 Reordering Features
</subsectionHeader>
<bodyText confidence="0.986821">
Our reordering model for English is based primar-
ily on non-lexical features. We use the POS tag
and functor from each node as features. The two
distributions in our reordering model (used in Algo-
rithm 1) are:
</bodyText>
<equation confidence="0.93726525">
P(ci ≺ ci+1|ci, ci+1, g) (7)
= (ci ≺ ci+1|fi, ti, fi+1, ti+1, fg, tg)
P(ci ≺ g ≺ ci+1|ci, ci+1, g) (8)
= P(ci ≺ g ≺ ci+1|fi, ti, fi+1, ti+1, tg, fg)
</equation>
<bodyText confidence="0.99631">
In both Equation 7 and Equation 8, only the func-
tor and POS tag of each node is used.
</bodyText>
<sectionHeader confidence="0.988207" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.999942545454545">
We have experimented with the above models on
both manually annotated TR trees and synthetic
trees (i.e., automatically generated trees). The data
comes from the PCEDT 1.0 corpus8, a version of the
Penn WSJ Treebank that has been been translated to
Czech and automatically transformed to TR in both
English and Czech. The English TR was automat-
ically generated from the Penn Treebank’s manu-
ally annotated surface syntax trees (English phrase-
structure trees). Additionally, a small set of 497 sen-
tences were manually annotated at the TR level: 248
</bodyText>
<footnote confidence="0.758502">
8LDC catalog number: LDC2004T25.
</footnote>
<page confidence="0.994188">
61
</page>
<table confidence="0.997178545454545">
Model Ins. Manual Data Rules Ins. Synthetic Data Rules
Model Articles Rules No Prep &amp; SC Articles Rules No Prep &amp; SC
Prep &amp; SC Articles Prep &amp; SC Articles
Baseline N/A N/A 77.93 76.78 N/A N/A 78.00 78.40
w/o g. functor 87.29 89.65 86.25 89.31 88.07 91.83 87.34 91.06
w/o g. lemma 86.77 89.48 85.68 89.02 87.53 90.95 86.55 91.16
w/o g. POS 87.29 89.45 86.10 89.14 87.68 91.86 86.89 92.07
w/o functor 86.10 85.02 84.86 84.56 86.01 85.60 84.79 85.65
w/o lemma 81.34 89.02 80.88 88.91 81.28 91.03 81.42 91.33
w/o POS 84.81 88.01 84.01 87.29 85.53 91.08 84.69 90.98
All Features 87.49 89.68 86.45 89.28 87.87 91.83 87.24 92.02
</table>
<tableCaption confidence="0.8612935">
Table 1: Classification accuracy for insertion models on development data from PCEDT 1.0. Article accuracy is computed over
the set of nouns. Preposition and subordinating conjunction accuracy (P &amp; SC) is computed over the set of nodes that appear on
the surface (excluding hidden nodes in the TR – these will not exist in automatically generated data). Models are shown for all
features minus the specified feature. Features with the prefix “g.” indicate governor features, otherwise the features are from the
node’s attributes. The Baseline model is one which never inserts any nodes (i.e., the model which inserts the most probable value –
NOAUX).
</tableCaption>
<bodyText confidence="0.999968368421053">
for development and 249 for evaluation; results are
presented for these two datasets.
All models were trained on the PCEDT 1.0 data
set, approximately 49,000 sentences, of which 4,200
were randomly selected as held-out training data, the
remainder was used for training. We estimate the
model distributions with a smoothed maximum like-
lihood estimator, using Jelinek-Mercer EM smooth-
ing (i.e., linearly interpolated backoff distributions).
Lower order distributions used for smoothing are es-
timated by deleting the rightmost conditioning vari-
able (as presented in the above models).
Similar experiments were performed at the 2002
Johns Hopkins summer workshop. The results re-
ported here are substantially better than those re-
ported in the workshop report (Hajic et al., 2002);
however, the details of the workshop experiments
are not clear enough to ensure the experimental con-
ditions are identical.
</bodyText>
<subsectionHeader confidence="0.997802">
4.1 Insertion Results
</subsectionHeader>
<bodyText confidence="0.999752666666667">
For each of the two insertion models (the article
model and the preposition and subordinating con-
junction model), there is a finite set of values for
the dependent variable ai. For example, the articles
are the complete set of English articles as collected
from the Penn Treebank training data (these have
manual POS tag annotations). We add a dummy
value to this set which indicates no article should
be inserted.9 The preposition and auxiliary model
</bodyText>
<footnote confidence="0.654536">
9In the classifier evaluation we consider the article a and an
to be equivalent.
</footnote>
<bodyText confidence="0.99954">
assumes the set of possible modifiers to be all those
seen in the training data that were removed when
modifying the manual TR trees.
The classification accuracy is the percentage of
nodes for which we predicted the correct auxiliary
from the set of candidate nodes for the auxiliary
type. Articles are only predicted and evaluated for
nouns (determined by the POS tag). Prepositions
and subordinating conjunctions are predicted and
evaluated for all nodes that appear on the surface.
We do not report results for the modal verb inser-
tion as it is primarily determined by the features of
the verb being modified (accuracy is approximately
100%). We have experimented with different fea-
tures sets and found that the model described in
Equation 6 performs best when all features are used.
In a variant of the insertion model, when the clas-
sifier prediction is of low certainty (probability less
than .5) we defer to a small set of deterministic rules.
For infinitives, we insert “to”; for origin nouns, we
insert “from”, for actors we insert “of”, and we at-
tach “by” to actors of passive verbs. In the article
insertion model, we do not insert anything if there
is another determiner (e.g., “none” or “any”) or per-
sonal pronoun; we insert “the” if the word appeared
within the previous four sentences or if there is a
suggestive adjective attached to the noun.10
Table 1 shows that the classifiers perform better
on automatically generated data (Synthetic Data),
but also perform well on the manually annotated
</bodyText>
<footnote confidence="0.7478295">
10Any adjective that is always followed by the definite article
in the training data.
</footnote>
<page confidence="0.99347">
62
</page>
<table confidence="0.998758">
Model Manual Data Synthetic Data
Coord. Rules No Rules Coord. Rules No Rules
All Interior All Interior All Interior All Interior
Baseline N/A N/A 68.43 21.67 N/A N/A 69.00 21.42
w/o g. functor 94.51 86.44 92.42 81.27 94.90 87.25 93.37 83.42
w/o g. tag 93.43 83.75 90.89 77.50 93.82 84.56 91.64 79.12
w/o c. functors 91.38 78.70 89.71 74.57 91.91 79.79 90.41 76.04
w/o c. tags 88.85 72.44 82.29 57.36 88.91 72.29 83.04 57.60
All Features 94.43 86.24 92.01 80.26 95.21 88.04 93.37 83.42
</table>
<tableCaption confidence="0.78468">
Table 2: Reordering accuracy for TR trees on development data from PCEDT 1.0. We include performance on the interior nodes
(excluding leaf nodes) for the Manual data to show a more detailed analysis of the performance. “g.” are the governor features and
“c.” are the child features. The baseline model sorts subtrees of each node randomly.
</tableCaption>
<bodyText confidence="0.999809333333333">
data. Prediction of articles is primarily dependent on
the lemma and the tag of the node. The lemma and
tag of the governing node and the node’s functor is
important to a lesser degree. In predicting the prepo-
sitions and subordinating conjunctions, the node’s
functor is the most critical factor.
</bodyText>
<table confidence="0.983990571428571">
% Errors Reference--+Hypothesis
41 the --+ NULL
19 a/an --+ NULL
16 NULL --+ the
11 a/an --+ the
11 the --+ a/an
2 NULL --+ a/an
</table>
<tableCaption confidence="0.986993">
Table 3: Article classifier errors on development data.
</tableCaption>
<table confidence="0.999893">
Det. Manual Det. Synthetic
P &amp; SC P &amp; SC
85.53 89.18 85.31 91.54
</table>
<tableCaption confidence="0.999793">
Table 4: Accuracy of best models on the evaluation data.
</tableCaption>
<bodyText confidence="0.998969230769231">
Table 3 presents a confusion set from the best ar-
ticle classifier on the development data. Our model
is relatively conservative, incurring 60% of the error
by choosing to insert nothing when it should have in-
serted an article. The model requires more informed
features as we are currently being overly conserva-
tive.
In Table 4 we report the overall accuracy on evalu-
ation data using the model that performed best on the
development data. The results are consistent with
the results for the development data; however, the
article model performs slightly worse on the evalua-
tion set.
</bodyText>
<subsectionHeader confidence="0.986305">
4.2 Reordering Results
</subsectionHeader>
<bodyText confidence="0.9999835">
Evaluation of the final sentence ordering was based
on predicting the correct words in the correct po-
sitions. We use the reordering metric described in
Hajiˇc et al. (2002) which computes the percentage
of nodes for which all children are correctly ordered
(i.e., no credit for partially correct orderings).
Table 2 shows the reordering accuracy for the
full model and variants where a particular feature
type is removed. These results are for ordering
the correct auxiliary-inserted TR trees (using deep-
syntactic functors and the correctly inserted auxil-
iaries). In the model variant that preserves the deep
order of coordinating conjunctions, we see a signif-
icant increase in performance. The child node tags
are critical for the reordering model, followed by the
child functors.
</bodyText>
<subsectionHeader confidence="0.942655">
4.3 Combined System Results
</subsectionHeader>
<table confidence="0.99778025">
Model Manual Synthetic
TR w/ Rules .4614 .4777
TR w/o Rules .4532 .4657
AR .2337 .2451
</table>
<tableCaption confidence="0.996622">
Table 5: BLEU scores for complete generation system for TR
trees (with and without rules applied) and the AR trees.
</tableCaption>
<bodyText confidence="0.999976538461539">
In order to evaluate the combined system, we used
the multiple-translation dataset in the PCEDT cor-
pus. This data contains four retranslations from
Czech to English of each of the original English sen-
tences in the development and evaluation datasets.
In Table 5 we report the BLEU scores on develop-
ment data for our TR generation model (including
the morphological generation module) and the AR
generation model. Results for the system that uses
AR trees as an intermediate stage are very poor; this
is likely due to the noise introduced when generating
AR trees. Additionally, the results for the TR model
with the additional rules are consistent with the pre-
</bodyText>
<page confidence="0.998155">
63
</page>
<bodyText confidence="0.999937428571429">
vious results; the rules provide only a marginal im-
provement. Finally, we have run the complete sys-
tem on the evaluation data and achieved a BLEU
score of .4633 on the manual data and .4750 on
the synthetic data. These can be interpreted as the
upper-bound for Czech-English translation systems
based on TR tree transduction.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999955142857143">
We have provided a model for sentence synthesis
from Tectogrammatical Representation trees. We
provide a number of models based on relatively sim-
ple, local features that can be extracted from impov-
erished TR trees. We believe that further improve-
ments will be made by allowing for more flexible
use of the features. The current model uses sim-
ple linear interpolation smoothing which limits the
types of model features used (forcing an explicit fac-
torization). The advantage of simple models of the
type presented in this paper is that they are robust
to errors in the TR trees – which are expected when
the TR trees are generated automatically (e.g., in a
machine translation system).
</bodyText>
<sectionHeader confidence="0.997762" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99991825">
This work was partially supported by U.S.
NSF grants IIS–9982329 and OISE–0530118; by
the project of the Czech Ministry of Educa-
tion #LC536; by the Information Society Project
No. 1ET201120505 of the Grant Agency of the
Academy of Sciences of the Czech Republic; and
Grant No. 352/2006 of the Grant Agency of Charles
University.
</bodyText>
<sectionHeader confidence="0.998194" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995067935483871">
Srinivas Bangalore and Owen Rambow. 2000. Exploiting a
probabilistic hierarchical model for generation. In Proceed-
ings of the 18th International Conference on Computational
Linguistics (COLING 2000), Saarbr¨ucken, Germany.
Alena B¨ohmov´a, Jan Hajic, Eva Hajicov´a, and Barbora Vidov´a
Hladk´a. 2002. The prague dependency treebank: Three-
level annotation scenario. In Anne Abeille, editor, In Tree-
banks: Building and Using Syntactically Annotated Cor-
pora. Dordrecht, Kluwer Academic Publishers, The Neter-
lands.
Joan Bresnan and Ronald M. Kaplan. 1982. Lexical-functional
grammar: A formal system for grammatical representation.
In The Mental Representation of Grammatical Relations.
MIT Press.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics, pages 263–270, Ann Arbor, MI.
Michael Collins. 2003. Head-driven statistical models for
natural language processing. Computational Linguistics,
29(4):589–637.
Simon Corston-Oliver, Michael Gamon, Eric Ringger, and
Robert Moore. 2002. An overview of Amalgam: A
machine-learned generation module. In Proceedings of
the International Natural Language Generation Conference,
pages 33–40, New York, USA.
Jan Hajic, Martin Cmejrek, Bonnie Dorr, Yuan Ding, Jason
Eisner, Dan Gildea, Terry Koo, Kristen Parton, Dragomir
Radev, and Owen Rambow. 2002. Natural language genera-
tion in the context of machine translation. Technical report,
Center for Language and Speech Processing, Johns Hopkins
University, Balitmore. Summer Workshop Final Report.
Irene Langkilde and Kevin Knight. 1998. The practical value of
n-grams in generation. In Proceedings of the International
Natural Language Generation Workshop.
Irene Langkilde-Geary. 2002. An empirical verification of cov-
erage and correctness for a general-purpose sentence gener-
ator. In Proceedings of the International Natural Language
Generation Conference.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL’05), pages
271–279, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Petr Sgall, Eva Hajicov´a, and Jarmila Panevov´a. 1986. The
Meaning of the Sentence in Its Semantic and Pragmatic As-
pects. Kluwer Academic, Boston.
Radu Soricut and Daniel Marcu. 2006. Stochastic language
generation using WIDL–expressions and its application in
machine translation and summarization. In Proceedings of
the 44th Annual Meeting of the Association for Computa-
tional Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377–404.
</reference>
<page confidence="0.99941">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.245269">
<title confidence="0.999636">Generation in Machine Translation from Deep Syntactic Trees</title>
<author confidence="0.937079">Keith</author>
<affiliation confidence="0.8092135">Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.971788">Baltimore, MD</address>
<email confidence="0.777759">keithhall@jhu.edu</email>
<author confidence="0.852078">Petr</author>
<affiliation confidence="0.7475925">Institute of Formal and Applied Charles</affiliation>
<address confidence="0.745064">Prague, Czech</address>
<email confidence="0.923873">nemec@ufal.mff.cuni.cz</email>
<abstract confidence="0.99967075">In this paper we explore a generative model for recovering surface syntax and strings from deep-syntactic tree structures. Deep analysis has been proposed for a number of language and speech processing tasks, such as machine translation and paraphrasing of speech transcripts. In an effort to validate one such formalism of deep syntax, the Praguian Tectogrammatical Representation (TR), we present a model of synthesis for English which generates surface-syntactic trees as well as strings. We propose a generative model for function word insertion (prepositions, definite/indefinite articles, etc.) and subphrase reordering. We show by way of empirical results that this model is effective in constructing acceptable English sentences given impoverished trees.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="4818" citStr="Bangalore and Rambow (2000)" startWordPosition="717" endWordPosition="720">tation. For example, the Halogen representation includes markings for determiners, voice, subject position, and dative position which simplifies the generation process. We believe their minimally specified results are based on input which most closely resembles the input from which we generate in our experiments. Amalgam’s reordering model is similar to the one presented here; their model reorders constituents in a similar way that we reorder subtrees. Both the model of Amalgam and that presented here differ considerably from the n-gram models of Langkilde and Knight (1998), the TAG models of Bangalore and Rambow (2000), and the stochastic generation from semantic representation approach of Soricut and Marcu (2006). In our work, we order the localsubtrees1 of an augmented deep-structure tree based on the syntactic features of the nodes in the tree. By factoring these decisions to be independent for each local-subtree, the set of strings we consider is only constrained by the projective strucutre of the input tree and the local permutation limit described below. In the following sections we first provide a brief description of the Tectogrammatical Representation as used in our work. Both manually annotated an</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena B¨ohmov´a</author>
<author>Jan Hajic</author>
</authors>
<title>Eva Hajicov´a, and Barbora Vidov´a Hladk´a.</title>
<date>2002</date>
<booktitle>In Anne Abeille, editor, In Treebanks: Building and Using Syntactically Annotated Corpora.</booktitle>
<publisher>Dordrecht, Kluwer Academic Publishers, The Neterlands.</publisher>
<marker>B¨ohmov´a, Hajic, 2002</marker>
<rawString>Alena B¨ohmov´a, Jan Hajic, Eva Hajicov´a, and Barbora Vidov´a Hladk´a. 2002. The prague dependency treebank: Threelevel annotation scenario. In Anne Abeille, editor, In Treebanks: Building and Using Syntactically Annotated Corpora. Dordrecht, Kluwer Academic Publishers, The Neterlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Ronald M Kaplan</author>
</authors>
<title>Lexical-functional grammar: A formal system for grammatical representation. In The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1507" citStr="Bresnan and Kaplan, 1982" startWordPosition="217" endWordPosition="220">ndefinite articles, etc.) and subphrase reordering. We show by way of empirical results that this model is effective in constructing acceptable English sentences given impoverished trees. 1 Introduction Syntactic models for language are being reintroduced into language and speech processing systems thanks to the success of sophisticated statistical models of parsing (Charniak and Johnson, 2005; Collins, 2003). Representing deep syntactic relationships is an open area of research; examples of such models are exhibited in a variety of grammatical formalisms, such as Lexical Functional Grammars (Bresnan and Kaplan, 1982), Head-driven Phrase Structure Grammars (Pollard and Sag, 1994) and the Tectogrammatical Representation (TR) of the Functional Generative Description (Sgall et al., 1986). In this paper we do not attempt to analyze the differences of these formalisms; instead, we show how one particular formalism is sufficient for automatic analysis and synthesis. Specifically, in this paper we provide evidence that TR is sufficient for synthesis in English. Augmenting models of machine translation (MT) with syntactic features is one of the main fronts of the MT research community. The Hiero model has been the</context>
</contexts>
<marker>Bresnan, Kaplan, 1982</marker>
<rawString>Joan Bresnan and Ronald M. Kaplan. 1982. Lexical-functional grammar: A formal system for grammatical representation. In The Mental Representation of Grammatical Relations. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1278" citStr="Charniak and Johnson, 2005" startWordPosition="181" endWordPosition="184">uian Tectogrammatical Representation (TR), we present a model of synthesis for English which generates surface-syntactic trees as well as strings. We propose a generative model for function word insertion (prepositions, definite/indefinite articles, etc.) and subphrase reordering. We show by way of empirical results that this model is effective in constructing acceptable English sentences given impoverished trees. 1 Introduction Syntactic models for language are being reintroduced into language and speech processing systems thanks to the success of sophisticated statistical models of parsing (Charniak and Johnson, 2005; Collins, 2003). Representing deep syntactic relationships is an open area of research; examples of such models are exhibited in a variety of grammatical formalisms, such as Lexical Functional Grammars (Bresnan and Kaplan, 1982), Head-driven Phrase Structure Grammars (Pollard and Sag, 1994) and the Tectogrammatical Representation (TR) of the Functional Generative Description (Sgall et al., 1986). In this paper we do not attempt to analyze the differences of these formalisms; instead, we show how one particular formalism is sufficient for automatic analysis and synthesis. Specifically, in this</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine nbest parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2219" citStr="Chiang, 2005" startWordPosition="327" endWordPosition="328">tion (TR) of the Functional Generative Description (Sgall et al., 1986). In this paper we do not attempt to analyze the differences of these formalisms; instead, we show how one particular formalism is sufficient for automatic analysis and synthesis. Specifically, in this paper we provide evidence that TR is sufficient for synthesis in English. Augmenting models of machine translation (MT) with syntactic features is one of the main fronts of the MT research community. The Hiero model has been the most successful to date by incorporating syntactic structure amounting to simple tree structures (Chiang, 2005). Synchronous parsing models have been explored with moderate success (Wu, 1997; Quirk et al., 2005). An extension to this work is the exploration of deeper syntactic models, such as TR. However, a better understanding of the synthesis of surface structure from the deep syntax is necessary. This paper presents a generative model for surface syntax and strings of English given tectogrammatical trees. Sentence generation begins by inserting auxiliary words associated with autosemantic nodes; these include prepositions, subordinating conjunctions, modal verbs, and articles. Following this, the li</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language processing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1294" citStr="Collins, 2003" startWordPosition="185" endWordPosition="186">entation (TR), we present a model of synthesis for English which generates surface-syntactic trees as well as strings. We propose a generative model for function word insertion (prepositions, definite/indefinite articles, etc.) and subphrase reordering. We show by way of empirical results that this model is effective in constructing acceptable English sentences given impoverished trees. 1 Introduction Syntactic models for language are being reintroduced into language and speech processing systems thanks to the success of sophisticated statistical models of parsing (Charniak and Johnson, 2005; Collins, 2003). Representing deep syntactic relationships is an open area of research; examples of such models are exhibited in a variety of grammatical formalisms, such as Lexical Functional Grammars (Bresnan and Kaplan, 1982), Head-driven Phrase Structure Grammars (Pollard and Sag, 1994) and the Tectogrammatical Representation (TR) of the Functional Generative Description (Sgall et al., 1986). In this paper we do not attempt to analyze the differences of these formalisms; instead, we show how one particular formalism is sufficient for automatic analysis and synthesis. Specifically, in this paper we provid</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language processing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
<author>Eric Ringger</author>
<author>Robert Moore</author>
</authors>
<title>An overview of Amalgam: A machine-learned generation module.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Natural Language Generation Conference,</booktitle>
<pages>33--40</pages>
<location>New York, USA.</location>
<contexts>
<context position="3056" citStr="Corston-Oliver et al., 2002" startWordPosition="454" endWordPosition="457">rstanding of the synthesis of surface structure from the deep syntax is necessary. This paper presents a generative model for surface syntax and strings of English given tectogrammatical trees. Sentence generation begins by inserting auxiliary words associated with autosemantic nodes; these include prepositions, subordinating conjunctions, modal verbs, and articles. Following this, the linear order of nodes is modeled by a similar generative process. These two models are combined in order to synthesize a sentence. The Amalgam system provides a similar model for generation from a logical form (Corston-Oliver et al., 2002). The primary difference between our approach and that of the Amalgam system is that we focus on an impoverished deep structure (akin to 57 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 57–64, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics logical form); we restrict the deep analysis to contain only the features which transfer directly across languages; specifically, those that transfer directly in our Czech-English machine translation system. Amalgam targets different issues. For example, Amalga</context>
<context position="14425" citStr="Corston-Oliver et al., 2002" startWordPosition="2310" endWordPosition="2313">s a coordinating conjunction, the governor information comes from the governor of the coordination node. grammatemes expressing the verb modality of the main verb. Additionally, each model is independent of the other and therefore up to two insertions per TR node are possible (an article and another syntactic modifier). In a variant of our model, we perform a small set of deterministic transformations in cases where the classifier is relatively uncertain about the predicted insertion node (i.e., the entropy of the conditional distribution is high). We note here that unlike the Amalgam system (Corston-Oliver et al., 2002), we do not address features which are determined (or almost completely determined) by the underlying deep-structure. For example, the task of inserting prepositions is nontrivial given we only know a node’s functor (e.g., the node’s valency role). 3.2 Analytical Representation Tree Generation We have experimented with two paradigms for synthesizing sentences from TR trees. The first technique involves first generating AR trees (surface syntax). In this model, we predict the node insertions, transform the functors from TR to AR functions (deep valency relationship to surface-syntactic relation</context>
</contexts>
<marker>Corston-Oliver, Gamon, Ringger, Moore, 2002</marker>
<rawString>Simon Corston-Oliver, Michael Gamon, Eric Ringger, and Robert Moore. 2002. An overview of Amalgam: A machine-learned generation module. In Proceedings of the International Natural Language Generation Conference, pages 33–40, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Martin Cmejrek</author>
<author>Bonnie Dorr</author>
<author>Yuan Ding</author>
<author>Jason Eisner</author>
<author>Dan Gildea</author>
<author>Terry Koo</author>
<author>Kristen Parton</author>
<author>Dragomir Radev</author>
<author>Owen Rambow</author>
</authors>
<title>Natural language generation in the context of machine translation.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University, Balitmore. Summer Workshop Final Report.</institution>
<contexts>
<context position="21749" citStr="Hajic et al., 2002" startWordPosition="3574" endWordPosition="3577">0 sentences, of which 4,200 were randomly selected as held-out training data, the remainder was used for training. We estimate the model distributions with a smoothed maximum likelihood estimator, using Jelinek-Mercer EM smoothing (i.e., linearly interpolated backoff distributions). Lower order distributions used for smoothing are estimated by deleting the rightmost conditioning variable (as presented in the above models). Similar experiments were performed at the 2002 Johns Hopkins summer workshop. The results reported here are substantially better than those reported in the workshop report (Hajic et al., 2002); however, the details of the workshop experiments are not clear enough to ensure the experimental conditions are identical. 4.1 Insertion Results For each of the two insertion models (the article model and the preposition and subordinating conjunction model), there is a finite set of values for the dependent variable ai. For example, the articles are the complete set of English articles as collected from the Penn Treebank training data (these have manual POS tag annotations). We add a dummy value to this set which indicates no article should be inserted.9 The preposition and auxiliary model 9</context>
</contexts>
<marker>Hajic, Cmejrek, Dorr, Ding, Eisner, Gildea, Koo, Parton, Radev, Rambow, 2002</marker>
<rawString>Jan Hajic, Martin Cmejrek, Bonnie Dorr, Yuan Ding, Jason Eisner, Dan Gildea, Terry Koo, Kristen Parton, Dragomir Radev, and Owen Rambow. 2002. Natural language generation in the context of machine translation. Technical report, Center for Language and Speech Processing, Johns Hopkins University, Balitmore. Summer Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>The practical value of n-grams in generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Natural Language Generation Workshop.</booktitle>
<contexts>
<context position="4771" citStr="Langkilde and Knight (1998)" startWordPosition="709" endWordPosition="712">icit information is preserved in their representation. For example, the Halogen representation includes markings for determiners, voice, subject position, and dative position which simplifies the generation process. We believe their minimally specified results are based on input which most closely resembles the input from which we generate in our experiments. Amalgam’s reordering model is similar to the one presented here; their model reorders constituents in a similar way that we reorder subtrees. Both the model of Amalgam and that presented here differ considerably from the n-gram models of Langkilde and Knight (1998), the TAG models of Bangalore and Rambow (2000), and the stochastic generation from semantic representation approach of Soricut and Marcu (2006). In our work, we order the localsubtrees1 of an augmented deep-structure tree based on the syntactic features of the nodes in the tree. By factoring these decisions to be independent for each local-subtree, the set of strings we consider is only constrained by the projective strucutre of the input tree and the local permutation limit described below. In the following sections we first provide a brief description of the Tectogrammatical Representation </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. The practical value of n-grams in generation. In Proceedings of the International Natural Language Generation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Natural Language Generation Conference.</booktitle>
<contexts>
<context position="3833" citStr="Langkilde-Geary (2002)" startWordPosition="569" endWordPosition="570">SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 57–64, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics logical form); we restrict the deep analysis to contain only the features which transfer directly across languages; specifically, those that transfer directly in our Czech-English machine translation system. Amalgam targets different issues. For example, Amalgam’s generation of prepositions and subordinating conjunctions is severely restricted as most of these are considered part of the logical form. The work of Langkilde-Geary (2002) on the Halogen system is similar to the work we present here. The differences that distinguish their work from ours stem from the type of deep representation from which strings are generated. Although their syntactic and semantic representations appear similar to the Tectogrammatical Representation, more explicit information is preserved in their representation. For example, the Halogen representation includes markings for determiners, voice, subject position, and dative position which simplifies the generation process. We believe their minimally specified results are based on input which mos</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>Irene Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Proceedings of the International Natural Language Generation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1570" citStr="Pollard and Sag, 1994" startWordPosition="225" endWordPosition="228"> of empirical results that this model is effective in constructing acceptable English sentences given impoverished trees. 1 Introduction Syntactic models for language are being reintroduced into language and speech processing systems thanks to the success of sophisticated statistical models of parsing (Charniak and Johnson, 2005; Collins, 2003). Representing deep syntactic relationships is an open area of research; examples of such models are exhibited in a variety of grammatical formalisms, such as Lexical Functional Grammars (Bresnan and Kaplan, 1982), Head-driven Phrase Structure Grammars (Pollard and Sag, 1994) and the Tectogrammatical Representation (TR) of the Functional Generative Description (Sgall et al., 1986). In this paper we do not attempt to analyze the differences of these formalisms; instead, we show how one particular formalism is sufficient for automatic analysis and synthesis. Specifically, in this paper we provide evidence that TR is sufficient for synthesis in English. Augmenting models of machine translation (MT) with syntactic features is one of the main fronts of the MT research community. The Hiero model has been the most successful to date by incorporating syntactic structure a</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>271--279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2319" citStr="Quirk et al., 2005" startWordPosition="341" endWordPosition="344">ot attempt to analyze the differences of these formalisms; instead, we show how one particular formalism is sufficient for automatic analysis and synthesis. Specifically, in this paper we provide evidence that TR is sufficient for synthesis in English. Augmenting models of machine translation (MT) with syntactic features is one of the main fronts of the MT research community. The Hiero model has been the most successful to date by incorporating syntactic structure amounting to simple tree structures (Chiang, 2005). Synchronous parsing models have been explored with moderate success (Wu, 1997; Quirk et al., 2005). An extension to this work is the exploration of deeper syntactic models, such as TR. However, a better understanding of the synthesis of surface structure from the deep syntax is necessary. This paper presents a generative model for surface syntax and strings of English given tectogrammatical trees. Sentence generation begins by inserting auxiliary words associated with autosemantic nodes; these include prepositions, subordinating conjunctions, modal verbs, and articles. Following this, the linear order of nodes is modeled by a similar generative process. These two models are combined in ord</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 271–279, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajicov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<title>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<publisher>Kluwer Academic,</publisher>
<location>Boston.</location>
<marker>Sgall, Hajicov´a, Panevov´a, 1986</marker>
<rawString>Petr Sgall, Eva Hajicov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. Kluwer Academic, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Stochastic language generation using WIDL–expressions and its application in machine translation and summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4915" citStr="Soricut and Marcu (2006)" startWordPosition="730" endWordPosition="733">sition, and dative position which simplifies the generation process. We believe their minimally specified results are based on input which most closely resembles the input from which we generate in our experiments. Amalgam’s reordering model is similar to the one presented here; their model reorders constituents in a similar way that we reorder subtrees. Both the model of Amalgam and that presented here differ considerably from the n-gram models of Langkilde and Knight (1998), the TAG models of Bangalore and Rambow (2000), and the stochastic generation from semantic representation approach of Soricut and Marcu (2006). In our work, we order the localsubtrees1 of an augmented deep-structure tree based on the syntactic features of the nodes in the tree. By factoring these decisions to be independent for each local-subtree, the set of strings we consider is only constrained by the projective strucutre of the input tree and the local permutation limit described below. In the following sections we first provide a brief description of the Tectogrammatical Representation as used in our work. Both manually annotated and synthetic TR trees are utilized in our experiments; we present a description of each type of tr</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Stochastic language generation using WIDL–expressions and its application in machine translation and summarization. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2298" citStr="Wu, 1997" startWordPosition="339" endWordPosition="340">er we do not attempt to analyze the differences of these formalisms; instead, we show how one particular formalism is sufficient for automatic analysis and synthesis. Specifically, in this paper we provide evidence that TR is sufficient for synthesis in English. Augmenting models of machine translation (MT) with syntactic features is one of the main fronts of the MT research community. The Hiero model has been the most successful to date by incorporating syntactic structure amounting to simple tree structures (Chiang, 2005). Synchronous parsing models have been explored with moderate success (Wu, 1997; Quirk et al., 2005). An extension to this work is the exploration of deeper syntactic models, such as TR. However, a better understanding of the synthesis of surface structure from the deep syntax is necessary. This paper presents a generative model for surface syntax and strings of English given tectogrammatical trees. Sentence generation begins by inserting auxiliary words associated with autosemantic nodes; these include prepositions, subordinating conjunctions, modal verbs, and articles. Following this, the linear order of nodes is modeled by a similar generative process. These two model</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>