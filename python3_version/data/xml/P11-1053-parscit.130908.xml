<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.989912">
Semi-supervised Relation Extraction with Large-scale Word Clustering
</title>
<author confidence="0.996759">
Ang Sun Ralph Grishman Satoshi Sekine
</author>
<affiliation confidence="0.997043">
Computer Science Department
</affiliation>
<address confidence="0.533085">
New York University
</address>
<email confidence="0.998479">
{asun,grishman,sekine}@cs.nyu.edu
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999099090909091">
We present a simple semi-supervised
relation extraction system with large-scale
word clustering. We focus on
systematically exploring the effectiveness
of different cluster-based features. We also
propose several statistical methods for
selecting clusters at an appropriate level of
granularity. When training on different
sizes of data, our semi-supervised approach
consistently outperformed a state-of-the-art
supervised baseline system.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949283018868">
Relation extraction is an important information
extraction task in natural language processing
(NLP), with many practical applications. The goal
of relation extraction is to detect and characterize
semantic relations between pairs of entities in text.
For example, a relation extraction system needs to
be able to extract an Employment relation between
the entities US soldier and US in the phrase US
soldier.
Current supervised approaches for tackling this
problem, in general, fall into two categories:
feature based and kernel based. Given an entity
pair and a sentence containing the pair, both
approaches usually start with multiple level
analyses of the sentence such as tokenization,
partial or full syntactic parsing, and dependency
parsing. Then the feature based method explicitly
extracts a variety of lexical, syntactic and semantic
features for statistical learning, either generative or
discriminative (Miller et al., 2000; Kambhatla,
2004; Boschee et al., 2005; Grishman et al., 2005;
Zhou et al., 2005; Jiang and Zhai, 2007). In
contrast, the kernel based method does not
explicitly extract features; it designs kernel
functions over the structured sentence
representations (sequence, dependency or parse
tree) to capture the similarities between different
relation instances (Zelenko et al., 2003; Bunescu
and Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhao and Grishman, 2005; Zhang et al., 2006;
Zhou et al., 2007; Qian et al., 2008). Both lines of
work depend on effective features, either explicitly
or implicitly.
The performance of a supervised relation
extraction system is usually degraded by the
sparsity of lexical features. For example, unless the
example US soldier has previously been seen in the
training data, it would be difficult for both the
feature based and the kernel based systems to
detect whether there is an Employment relation or
not. Because the syntactic feature of the phrase US
soldier is simply a noun-noun compound which is
quite general, the words in it are crucial for
extracting the relation.
This motivates our work to use word clusters as
additional features for relation extraction. The
assumption is that even if the word soldier may
never have been seen in the annotated Employment
relation instances, other words which share the
same cluster membership with soldier such as
president and ambassador may have been
observed in the Employment instances. The
absence of lexical features can be compensated by
</bodyText>
<page confidence="0.967992">
521
</page>
<note confidence="0.979557">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 521–529,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999676">
the cluster features. Moreover, word clusters may
implicitly correspond to different relation classes.
For example, the cluster of president may be
related to the Employment relation as in US
president while the cluster of businessman may be
related to the Affiliation relation as in US
businessman.
The main contributions of this paper are: we
explore the cluster-based features in a systematic
way and propose several statistical methods for
selecting effective clusters. We study the impact
of the size of training data on cluster features and
analyze the performance improvements through an
extensive experimental study.
The rest of this paper is organized as follows:
Section 2 presents related work and Section 3
provides the background of the relation extraction
task and the word clustering algorithm. Section 4
describes in detail a state-of-the-art supervised
baseline system. Section 5 describes the cluster-
based features and the cluster selection methods.
We present experimental results in Section 6 and
conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999972307692308">
The idea of using word clusters as features in
discriminative learning was pioneered by Miller et
al. (2004), who augmented name tagging training
data with hierarchical word clusters generated by
the Brown clustering algorithm (Brown et al., 1992)
from a large unlabeled corpus. They used different
thresholds to cut the word hierarchy to obtain
clusters of various granularities for feature
decoding. Ratinov and Roth (2009) and Turian et
al. (2010) also explored this approach for name
tagging. Though all of them used the same
hierarchical word clustering algorithm for the task
of name tagging and reported improvements, we
noticed that the clusters used by Miller et al. (2004)
were quite different from that of Ratinov and Roth
(2009) and Turian et al. (2010). To our knowledge,
there has not been work on selecting clusters in a
principled way. We move a step further to explore
several methods in choosing effective clusters. A
second difference between this work and the above
ones is that we utilize word clusters in the task of
relation extraction which is very different from
sequence labeling tasks such as name tagging and
chunking.
Though Boschee et al. (2005) and Chan and
Roth (2010) used word clusters in relation
extraction, they shared the same limitation as the
above approaches in choosing clusters. For
example, Boschee et al. (2005) chose clusters of
different granularities and Chan and Roth (2010)
simply used a single threshold for cutting the word
hierarchy. Moreover, Boschee et al. (2005) only
augmented the predicate (typically a verb or a
noun of the most importance in a relation in their
definition) with word clusters while Chan and Roth
(2010) performed this for any lexical feature
consisting of a single word. In this paper, we
systematically explore the effectiveness of adding
word clusters to different lexical features.
</bodyText>
<sectionHeader confidence="0.999683" genericHeader="method">
3 Background
</sectionHeader>
<subsectionHeader confidence="0.999678">
3.1 Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999926266666667">
One of the well defined relation extraction tasks is
the Automatic Content Extraction1 (ACE) program
sponsored by the U.S. government. ACE 2004
defined 7 major entity types: PER (Person), ORG
(Organization), FAC (Facility), GPE (Geo-Political
Entity: countries, cities, etc.), LOC (Location),
WEA (Weapon) and VEH (Vehicle). An entity has
three types of mention: NAM (proper name), NOM
(nominal) or PRO (pronoun). A relation was
defined over a pair of entity mentions within a
single sentence. The 7 major relation types with
examples are shown in Table 1. ACE 2004 also
defined 23 relation subtypes. Following most of
the previous work, this paper only focuses on
relation extraction of major types.
</bodyText>
<subsectionHeader confidence="0.849098">
Given a relation instance x = (s, m; , mj) , where
</subsectionHeader>
<bodyText confidence="0.994633538461538">
m; and are a pair of mentions and is the
sentence containing the pair, the goal is to learn a
function which maps the instance x to a type c,
where c is one of the 7 defined relation types or the
type Nil (no relation exists). There are two
commonly used learning paradigms for relation
extraction:
Flat: This strategy performs relation detection
and classification at the same time. One multi-class
classifier is trained to discriminate among the 7
relation types plus the Nil type.
Hierarchical: This one separates relation
detection from relation classification. One binary
</bodyText>
<footnote confidence="0.9990985">
1 Task definition: http://www.itl.nist.gov/iad/894.01/tests/ace/
ACE guidelines: http://projects.ldc.upenn.edu/ace/
</footnote>
<page confidence="0.99692">
522
</page>
<bodyText confidence="0.999677625">
classifier is trained first to distinguish between
relation instances and non-relation instances. This
can be done by grouping all the instances of the 7
relation types into a positive class and the instances
of Nil into a negative class. Then the thresholded
output of this binary classifier is used as training
data for learning a multi-class classifier for the 7
relation types (Bunescu and Mooney, 2005b).
</bodyText>
<table confidence="0.999045625">
Type Example
EMP-ORG US president
PHYS a military base in Germany
GPE-AFF U.S. businessman
PER-SOC a spokesman for the senator
DISC each of whom
ART US helicopters
OTHER-AFF Cuban-American people
</table>
<tableCaption confidence="0.9549085">
Table 1: ACE relation types and examples from the
annotation guideline 2 . The heads of the two entity
mentions are marked. Types are listed in decreasing
order of frequency of occurrence in the ACE corpus.
</tableCaption>
<subsectionHeader confidence="0.999539">
3.2 Brown Word Clustering
</subsectionHeader>
<bodyText confidence="0.999988304347826">
The Brown algorithm is a hierarchical clustering
algorithm which initially assigns each word to its
own cluster and then repeatedly merges the two
clusters which cause the least loss in average
mutual information between adjacent clusters
based on bigram statistics. By tracing the pairwise
merging steps, one can obtain a word hierarchy
which can be represented as a binary tree. A word
can be compactly represented as a bit string by
following the path from the root to itself in the tree,
assigning a 0 for each left branch, and a 1 for each
right branch. A cluster is just a branch of that tree.
A high branch may correspond to more general
concepts while the lower branches it includes
might correspond to more specific ones.
Brown et al. (1992) described an efficient
implementation based on a greedy algorithm which
initially assigned only the most frequent words into
distinct clusters. It is worth pointing out that in this
implementation each word occupies a leaf in the
hierarchy, but each leaf might contain more than
one word as can be seen from Table 2. The lengths
of the bit strings also vary among different words.
</bodyText>
<footnote confidence="0.9892845">
2 http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-
2.PDF
</footnote>
<table confidence="0.999461076923077">
Bit string Examples
111011011100 US ...
1110110111011 U.S. ...
1110110110000 American ...
1110110111110110 Cuban, Pakistani, Russian ...
11111110010111 Germany, Poland, Greece ...
110111110100 businessman, journalist, reporter
1101111101111 president, governor, premier...
1101111101100 senator, soldier, ambassador ...
11011101110 spokesman, spokeswoman, ...
11001100 people, persons, miners, Haitians
110110111011111 base, compound, camps, camp ...
110010111 helicopters, tanks, Marines ...
</table>
<tableCaption confidence="0.994335666666667">
Table 2: An example of words and their bit string
representations obtained in this paper. Words in bold are
head words that appeared in Table 1.
</tableCaption>
<sectionHeader confidence="0.991361" genericHeader="method">
4 Feature Based Relation Extraction
</sectionHeader>
<bodyText confidence="0.98585725">
Given a pair of entity mentions and the
sentence containing the pair, a feature based
system extracts a feature vector which contains
diverse lexical, syntactic and semantic features.
The goal is to learn a function which can estimate
the conditional probability p(c I , the probability
of a relation type given the feature vector . The
type with the highest probability will be output as
the class label for the mention pair.
We now describe a supervised baseline system
with a very large set of features and its learning
strategy.
</bodyText>
<subsectionHeader confidence="0.997462">
4.1 Baseline Feature Set
</subsectionHeader>
<bodyText confidence="0.998558444444444">
We first adopted the full feature set from Zhou et
al. (2005), a state-of-the-art feature based relation
extraction system. For space reasons, we only
show the lexical features as in Table 3 and refer the
reader to the paper for the rest of the features.
At the lexical level, a relation instance can be
seen as a sequence of tokens which form a five
tuple &lt;Before, M1, Between, M2, After&gt;. Tokens
of the five members and the interaction between
the heads of the two mentions can be extracted as
features as shown in Table 3.
In addition, we cherry-picked the following
features which were not included in Zhou et al.
(2005) but were shown to be quite effective for
relation extraction.
Bigram of the words between the two mentions:
This was extracted by both Zhao and Grishman
(2005) and Jiang and Zhai (2007), aiming to
</bodyText>
<page confidence="0.991823">
523
</page>
<bodyText confidence="0.996621875">
provide more order information of the tokens
between the two mentions.
Patterns: There are three types of patterns: 1)
the sequence of the tokens between the two
mentions as used in Boschee et al. (2005); 2) the
sequence of the heads of the constituents between
the two mentions as used by Grishman et al. (2005);
3) the shortest dependency path between the two
mentions in a dependency tree as adopted by
Bunescu and Mooney (2005a). These patterns can
provide more structured information of how the
two mentions are connected.
Title list: This is tailored for the EMP-ORG type
of relations as the head of one of the mentions is
usually a title. The features are decoded in a way
similar to that of Sun (2009).
</bodyText>
<table confidence="0.9994219">
Position Feature Description
Before BM1F first word before M1
BM1L second word before M1
M1 WM1 bag-of-words in M1
HM1 head3 word of M1
Between WBNULL when no word in between
WBFL the only word in between when
only one word in between
WBF first word in between when at
least two words in between
WBL last word in between when at
least two words in between
WBO other words in between except
first and last words when at
least three words in between
M2 WM2 bag-of-words in M2
HM2 head word of M2
M12 HM12 combination of HM1 and HM2
After AM2F first word after M2
AM2L second word after M2
</table>
<tableCaption confidence="0.998795">
Table 3: Lexical features for relation extraction.
</tableCaption>
<subsectionHeader confidence="0.983381">
4.2 Baseline Learning Strategy
</subsectionHeader>
<bodyText confidence="0.957571666666667">
We employ a simple learning framework that is
similar to the hierarchical learning strategy as
described in Section 3.1. Specifically, we first train
a binary classifier to distinguish between relation
instances and non-relation instances. Then rather
than using the thresholded output of this binary
classifier as training data, we use only the
annotated relation instances to train a multi-class
classifier for the 7 relation types. In the test phase,
3 The head word of a mention is normally set as the last word
of the mention as in Zhou et al. (2005).
given a test instance x , we first apply the binary
classifier to it for relation detection; if it is detected
as a relation instance we then apply the multi-class
relation classifier to classify it4.
</bodyText>
<sectionHeader confidence="0.97413" genericHeader="method">
5 Cluster Feature Selection
</sectionHeader>
<bodyText confidence="0.999919333333333">
The selection of cluster features aims to answer the
following two questions: which lexical features
should be augmented with word clusters to
improve generalization accuracy? How to select
clusters at an appropriate level of granularity? We
will describe our solutions in Section 5.1 and 5.2.
</bodyText>
<subsectionHeader confidence="0.987216">
5.1 Cluster Feature Decoding
</subsectionHeader>
<bodyText confidence="0.999553888888889">
While each one of the lexical features in Table 3
used by the baseline can potentially be augmented
with word clusters, we believe the effectiveness of
a lexical feature with augmentation of word
clusters should be tested either individually or
incrementally according to a rank of its importance
as shown in Table 4. We will show the
effectiveness of each cluster feature in the
experiment section.
</bodyText>
<table confidence="0.999642">
Impor- Lexical Description of Cluster Feature
tance Feature lexical feature
1 HM HM1, HM2 and HM1_WC,
HM12 HM2_WC,
HM12_WC
2 BagWM WM1 and WM2 BagWM_WC
3 HC a head5 of a chunk HC_WC
in context
4 BagWC word of context BagWC_WC
</table>
<tableCaption confidence="0.999461">
Table 4: Cluster features ordered by importance.
</tableCaption>
<bodyText confidence="0.978742866666667">
The importance is based on linguistic intuitions
and observations of the contributions of different
lexical features from various feature based systems.
Table 4 simplifies a relation instance as a three
tuple &lt;Context, M1, M2&gt; where the Context
includes the Before, Between and After from the
4 Both the binary and multi-class classifiers output normalized
probabilities in the range [0,1]. When the binary classifier’s
prediction probability is greater than 0.5, we take the
prediction with the highest probability of the multi-class
classifier as the final class label. When it is in the range
[0.3,0.5], we only consider as the final class label the
prediction of the multi-class classifier with a probability which
is greater than 0.9. All other cases are taken as non-relation
instances.
</bodyText>
<footnote confidence="0.253565">
5 The head of a chunk is defined as the last word in the chunk.
</footnote>
<page confidence="0.993728">
524
</page>
<bodyText confidence="0.999949565217391">
five tuple representation. As a relation in ACE is
usually short, the words of the two entity mentions
can provide more critical indications for relation
classification than the words from the context.
Within the two entity mentions, the head word of
each mention is usually more important than other
words of the mention; the conjunction of the two
heads can provide an additional clue. And in
general words other than the chunk head in the
context do not contribute to establishing a
relationship between the two entity mentions.
The cluster based semi-supervised system works
by adding an additional layer of lexical features
that incorporate word clusters as shown in column
4 of Table 4. Take the US soldier as an example, if
we decide to use a length of 10 as a threshold to
cut the Brown word hierarchy to generate word
clusters, we will extract a cluster feature
HM1_WC10=1101111101 in addition to the
lexical feature HM1=soldier given that the full bit
string of soldier is 1101111101100 in Table 2.
(Note that the cluster feature is a nominal feature,
not to be confused with an integer feature.)
</bodyText>
<sectionHeader confidence="0.779576" genericHeader="method">
 |m |
</sectionHeader>
<subsectionHeader confidence="0.998142">
5.2 Selection of Clusters
</subsectionHeader>
<bodyText confidence="0.968599333333334">
 |m |
Given the bit string representations of all the words
in a vocabulary, researchers usually use prefixes of
different lengths of the bit strings to produce word
clusters of various granularities. However, how to
choose the set of prefix lengths in a principled way?
This has not been answered by prior work.
Our main idea is to learn the best set of prefix
lengths, perhaps through the validation of their
effectiveness on a development set of data. To our
knowledge, previous research simply uses ad-hoc
prefix lengths and lacks this training procedure.
The training procedure can be extremely slow for
reasons to be explained below.
Formally, let be the set of available prefix
lengths ranging from 1 bit to the length of the
longest bit string in the Brown word hierarchy and
let be the set of prefix lengths we want to use in
decoding cluster features, then the problem of
selecting effective clusters transforms to finding a
-combination of the set which maximizes
system performance. The training procedure can be
extremely time consuming if we enumerate every
possible -combination of , given thatImI
can range from 1 to the size of and the size of
1 equals the length of the longest bit string which is
usually 20 when inducing 1,000 clusters using the
Brown algorithm.
One way to achieve better efficiency is to
consider only a subset of instead of the full set. In
addition, we limit ourselves to use sizes 3 and 4 for
m for matching prior work. This keeps the cluster
features to a manageable size considering that
every word in your vocabulary could contribute to
a lexical feature. For picking a subset of , we
propose below two statistical measures for
computing the importance of a certain prefix
length.
Information Gain (IG): IG measures the
quality or importance of a feature f by computing
the difference between the prior entropy of classes
IG f
</bodyText>
<equation confidence="0.8025917">
( )    p c p c
( )log ( ) 
C and the posterior entropy, given values V of the
c C

feature f (Hunt et al., 1966; Quinlan, 1986). For
 
( ( ) (  |)log (  |))
 
p v p c v p c v
</equation>
<bodyText confidence="0.995671">
our purpose, C is the set of relation types, f is a
cluster-based feature with a certain prefix length
such as HM1_WC* where * means the prefix
length and a value v is the prefix of the bit string
representation of HM1. More formally, the IG of f
is computed as follows:
</bodyText>
<equation confidence="0.887239666666667">
v V
 c C

</equation>
<bodyText confidence="0.998598909090909">
where the first and second terms refer to the prior
and posterior entropies respectively.
For each prefix length in the set , we can
compute its IG for a type of cluster feature and
then rank the prefix lengths based on their IGs for
that cluster feature. For simplicity, we rank the
prefix lengths for a group of cluster features (a
group is a row from column 4 in Table 4) by
collapsing the individual cluster features into a
single cluster feature. For example, we collapse the
3 types: HM1_WC, HM2_WC and HM12_WC into
a single type HM_WC for computing the IG.
Prefix Coverage (PC): If we use a short prefix
then the clusters produced correspond to the high
branches in the word hierarchy and would be very
general. The cluster features may not provide more
informative information than the words themselves.
Similarly, if we use a long prefix such as the length
of the longest bit string, then maybe only a few of
the lexical features can be covered by clusters. To
capture this intuition, we define the PC of a prefix
length i as below:
</bodyText>
<equation confidence="0.895725">
(1)
</equation>
<page confidence="0.734943">
525
</page>
<bodyText confidence="0.975153">
(2)
where stands for a lexical feature such as HM1
and f,. a cluster feature with prefix length i such as
HM1_WCi, is the number of
occurrences of that feature in training data.
Similar to IG, we compute PC for a group of
cluster features, not for each individual feature.
In our experiments, the top 10 ranked prefix
lengths based on IG and prefix lengths with PC
values in the range [0.4, 0.9] were used.
In addition to the above two statistical measures,
for comparison, we introduce another two simple
but extreme measures for the selection of clusters.
Use All Prefixes (UA): UA produces a cluster
feature at every available bit length with the hope
that the underlying supervised system can learn
proper weights of different cluster features during
training. For example, if the full bit representation
of “Apple” is “000”, UA would produce three
cluster features: prefix1=0, prefix2=00 and
prefix3=000. Because this method does not need
validation on the development set, it is the laziest
but the fastest method for selecting clusters.
Exhaustive Search (ES): ES works by trying
every possible combination of the set and picking
the one that works the best for the development set.
This is the most cautious and the slowest method
for selecting clusters.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99996169047619">
In this section, we first present details of our
unsupervised word clusters, the relation extraction
data set and its preprocessing. We then present a
series of experiments coupled with result analyses.
We used the English portion of the TDT5
corpora (LDC2006T18) as our unlabeled data for
inducing word clusters. It contains roughly 83
million words in 3.4 million sentences with a
vocabulary size of 450K. We left case intact in the
corpora. Following previous work, we used
Liang’s implementation of the Brown clustering
algorithm (Liang, 2005). We induced 1,000 word
clusters for words that appeared at least twice in
the corpora. The reduced vocabulary contains
255K unique words. The clusters are available at
http://www.cs.nyu.edu/~asun/data/TDT5_BrownW
C.tar.gz.
For relation extraction, we used the benchmark
ACE 2004 training data. Following most of the
previous research, we used in experiments the
nwire (newswire) and bnews (broadcast news)
genres of the data containing 348 documents and
4374 relation instances. We extracted an instance
for every pair of mentions in the same sentence
which were separated by no more than two other
mentions. The non-relation instances generated
were about 8 times more than the relation instances.
Preprocessing of the ACE documents: We used
the Stanford parser6 for syntactic and dependency
parsing. We used chunklink7 to derive chunking
information from the Stanford parsing. Because
some bnews documents are in lower case, we
recover the case for the head of a mention if its
type is NAM by making the first character into its
upper case. This is for better matching between the
words in ACE and the words in the unsupervised
word clusters.
We used the OpenNLP 8 maximum entropy
(maxent) package as our machine learning tool.
We choose to work with maxent because the
training is fast and it has a good support for multi-
class classification.
</bodyText>
<subsectionHeader confidence="0.999095">
6.1 Baseline Performance
</subsectionHeader>
<bodyText confidence="0.9998">
Following previous work, we did 5-fold cross-
validation on the 348 documents with hand-
annotated entity mentions. Our results are shown in
Table 5 which also lists the results of another three
state-of-the-art feature based systems. For this and
the following experiments, all the results were
computed at the relation mention level.
</bodyText>
<table confidence="0.99955">
System P(%) R(%) F(%)
Zhou et al. (2007)9 78.2 63.4 70.1
Zhao and Grishman (2005)10 69.2 71.5 70.4
Our Baseline 73.4 67.7 70.4
Jiang and Zhai (2007) 11 72.4 70.2 71.3
</table>
<tableCaption confidence="0.9695635">
Table 5: Performance comparison on the ACE 2004
data over the 7 relation types.
</tableCaption>
<footnote confidence="0.948955833333333">
6 http://nlp.stanford.edu/software/lex-parser.shtml
7 http://ilk.uvt.nl/team/sabine/chunklink/README.html
8 http://opennlp.sourceforge.net/
9 Zhou et al. (2005) tested their system on the ACE 2003 data;
Zhou et al. (2007) tested their system on the ACE 2004 data.
10 The paper gives a recall value of 70.5, which is not
consistent with the given values of P and F. An examination of
the correspondence in preparing this paper indicates that the
correct recall value is 71.5.
11 The result is from using the All features in Jiang and Zhai
(2007). It is not quite clear from the paper that whether they
used the 348 documents or the whole 2004 training data.
</footnote>
<page confidence="0.997097">
526
</page>
<bodyText confidence="0.999871375">
Note that although all the 4 systems did 5-fold
cross-validation on the ACE 2004 data, the
detailed data partition might be different. Also, we
were doing cross-validation at the document level
which we believe was more natural than the
instance level. Nonetheless, we believe our
baseline system has achieved very competitive
performance.
</bodyText>
<subsectionHeader confidence="0.873488">
6.2 The Effectiveness of Cluster Selection
Methods
</subsectionHeader>
<bodyText confidence="0.999570428571429">
We investigated the tradeoff between performance
and training time of each proposed method in
selecting clusters. In this experiment, we randomly
selected 70 documents from the 348 documents as
test data which roughly equaled the size of 1 fold
in the baseline in Section 6.1. For the baseline in
this section, all the rest of the documents were used
as training data. For the semi-supervised system,
70 percent of the rest of the documents were
randomly selected as training data and 30 percent
as development data. The set of prefix lengths that
worked the best for the development set was
chosen to select clusters. We only used the cluster
feature HM_WC in this experiment.
</bodyText>
<table confidence="0.999485333333333">
System F △ Training Time (in minute)
Baseline 70.70 1
UA 71.19 +0.49 1.5
PC3 71.65 +0.95 30
PC4 71.72 +1.02 46
IG3 71.65 +0.95 45
IG4 71.68 +0.98 78
ES3 71.66 +0.96 465
ES4 71.60 +0.90 1678
</table>
<tableCaption confidence="0.9077875">
Table 6: The tradeoff between performance and training
time of each method in selecting clusters. PC3 means
using 3 prefixes with the PC method. △ in this paper
means the difference between a system and the baseline.
</tableCaption>
<bodyText confidence="0.982499818181818">
Table 6 shows that all the 4 proposed methods
improved baseline performance, with UA as the
fastest and ES as the slowest. It was interesting that
ES did not always outperform the two statistical
methods which might be because of its overfitting
to the development set. In general, both PC and IG
had good balances between performance and
training time. There was no dramatic difference in
performance between using 3 and 4 prefix lengths.
For the rest of this paper, we will only use PC4
as our method in selecting clusters.
</bodyText>
<subsectionHeader confidence="0.998663">
6.3 The Effectiveness of Cluster Features
</subsectionHeader>
<bodyText confidence="0.99917425">
The baseline here is the same one used in Section
6.1. For the semi-supervised system, each test fold
was the same one used in the baseline and the other
4 folds were further split into a training set and a
development set in a ratio of 7:3 for selecting
clusters. We first added the cluster features
individually into the baseline and then added them
incrementally according to the order specified in
</bodyText>
<tableCaption confidence="0.803671">
Table 4.
</tableCaption>
<table confidence="0.500039333333333">
System F △
1 Baseline 70.4
2 1 + HM_WC 71.5 + 1.1
3 1 + BagWM_WC 71.0 + 0.6
4 1 + HC_WC 69.6 - 0.8
5 1 + BagWC_WC 46.1 - 24.3
6 2 + BagWM_WC 71.0 + 0.6
7 6 + HC_WC 70.6 + 0.2
8 7+ BagWC_WC 50.3 - 20.1
</table>
<tableCaption confidence="0.9858765">
Table 7: Performance 12 of the baseline and using
different cluster features with PC4 over the 7 types.
</tableCaption>
<bodyText confidence="0.9999522">
We found that adding clusters to the heads of the
two mentions was the most effective way of
introducing cluster features. Adding clusters to the
words of the mentions can also help, though not as
good as the heads. We were surprised that the
heads of chunks in context did not help. This might
be because ACE relations are usually short and the
limited number of long relations is not sufficient in
generalizing cluster features. Adding clusters to
every word in context hurt the performance a lot.
Because of the behavior of each individual feature,
it was not surprising that adding them
incrementally did not give more performance gain.
For the rest of this paper, we will only use
HM WC as cluster features.
</bodyText>
<subsectionHeader confidence="0.963723">
6.4 The Impact of Training Size
</subsectionHeader>
<bodyText confidence="0.9998768">
We studied the impact of training data size on
cluster features as shown in Table 8. The test data
was always the same as the 5-fold used in the
baseline in Section 6.1. no matter the size of the
training data. The training documents for the
</bodyText>
<footnote confidence="0.9672355">
12 All the improvements of F in Table 7, 8 and 9 were
significant at confidence levels &gt;= 95%.
</footnote>
<page confidence="0.979919">
527
</page>
<table confidence="0.999941875">
# docs F of Relation Classification F of Relation Detection
Baseline PC4 (△) Prefix10(△) Baseline PC4(△) Prefix10(△)
50 62.9 63.8(+ 0.9) 63.7(+0.8) 71.4 71.9(+ 0.5) 71.6(+0.2)
75 62.8 64.6(+ 1.8) 63.9(+1.1) 71.5 72.3(+ 0.8) 72.5(+1.0)
125 66.1 68.1(+ 2.0) 67.5(+1.4) 74.5 74.8(+ 0.3) 74.3(-0.2)
175 67.8 69.7(+ 1.9) 69.5(+1.7) 75.2 75.5(+ 0.3) 75.2(0.0)
225 68.9 70.1(+ 1.2) 69.6(+0.7) 75.6 75.9(+ 0.3) 75.3(-0.3)
≈280 70.4 71.5(+ 1.1) 70.7(+0.3) 76.4 76.9(+ 0.5) 76.3(-0.1)
</table>
<tableCaption confidence="0.998483">
Table 8: Performance over the 7 relation types with different sizes of training data. Prefix10 uses the single prefix
length 10 to generate word clusters as used by Chan and Roth (2010).
</tableCaption>
<table confidence="0.999962777777778">
Type P R F
Baseline PC4 (△) Baseline PC4 (△) Baseline PC4 (△)
EMP-ORG 75.4 77.2(+1.8) 79.8 81.5(+1.7) 77.6 79.3(+1.7)
PHYS 73.2 71.2(-2.0) 61.6 60.2(-1.4) 66.9 65.3(-1.7)
GPE-AFF 67.1 69.0(+1.9) 60.0 63.2(+3.2) 63.3 65.9(+2.6)
PER-SOC 88.2 83.9(-4.3) 58.4 61.0(+2.6) 70.3 70.7(+0.4)
DISC 79.4 80.6(+1.2) 42.9 46.0(+3.2) 55.7 58.6(+2.9)
ART 87.9 96.9(+9.0) 63.0 67.4(+4.4) 73.4 79.3(+5.9)
OTHER-AFF 70.6 80.0(+9.4) 41.4 41.4(0.0) 52.2 54.6(+2.4)
</table>
<tableCaption confidence="0.999728">
Table 9: Performance of each individual relation type based on 5-fold cross-validation.
</tableCaption>
<bodyText confidence="0.99995175">
current size setup were randomly selected and
added to the previous size setup (if applicable). For
example, we randomly selected another 25
documents and added them to the previous 50
documents to get 75 documents. We made sure
that every document participated in this experiment.
The training documents for each size setup were
split into a real training set and a development set
in a ratio of 7:3 for selecting clusters.
There are some clear trends in Table 8. Under
each training size, PC4 consistently outperformed
the baseline and the system Prefix10 for relation
classification. For PC4, the gain for classification
was more pronounced than detection. The mixed
detection results of Prefix10 indicated that only
using a single prefix may not be stable.
We did not observe the same trend in the
reduction of annotation need with cluster-based
features as in Koo et al. (2008) for dependency
parsing. PC4 with sizes 50, 125, 175 outperformed
the baseline with sizes 75, 175, 225 respectively.
But this was not the case when PC4 was tested
with sizes 75 and 225. This might due to the
complexity of the relation extraction task.
</bodyText>
<subsectionHeader confidence="0.979864">
6.5 Analysis
</subsectionHeader>
<bodyText confidence="0.999997444444444">
There were on average 69 cross-type errors in the
baseline in Section 6.1 which were reduced to 56
by using PC4. Table 9 showed that most of the
improvements involved EMP-ORG, GPE-AFF,
DISC, ART and OTHER-AFF. The performance
gain for PER-SOC was not as pronounced as the
other five types. The five types of relations are
ambiguous as they share the same entity type GPE
while the PER-SOC relation only holds between
PER and PER. This reflects that word clusters can
help to distinguish between ambiguous relation
types.
As mentioned earlier the gain of relation
detection was not as pronounced as classification
as shown in Table 8. The unbalanced distribution
of relation instances and non-relation instances
remains as an obstacle for pushing the performance
of relation extraction to the next level.
</bodyText>
<sectionHeader confidence="0.996359" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999960888888889">
We have described a semi-supervised relation
extraction system with large-scale word clustering.
We have systematically explored the effectiveness
of different cluster-based features. We have also
demonstrated that the two proposed statistical
methods are both effective and efficient in
selecting clusters at an appropriate level of
granularity through an extensive experimental
study.
</bodyText>
<page confidence="0.99095">
528
</page>
<bodyText confidence="0.999934285714286">
Based on the experimental results, we plan to
investigate additional ways to improve the
performance of relation detection. Moreover,
extending word clustering to phrase clustering (Lin
and Wu, 2009) and pattern clustering (Sun and
Grishman, 2010) is worth future investigation for
relation extraction.
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999813348837209">
Rie K. Ando and Tong Zhang. 2005 A Framework for
Learning Predictive Structures from Multiple Tasks
and Unlabeled Data. Journal of Machine Learning
Research, Vol 6:1817-1853.
Elizabeth Boschee, Ralph Weischedel, and Alex
Zamanian. 2005. Automatic information extraction.
In Proceedings of the International Conference on
Intelligence Analysis.
Peter F. Brown, Vincent J. Della Pietra, Peter V.
deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479.
Razvan C. Bunescu and Raymond J. Mooney. 2005a. A
shortest path dependency kenrel for relation
extraction. In Proceedings of HLT/EMNLP.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In
Proceedings of NIPS.
Yee Seng Chan and Dan Roth. 2010. Exploiting
background knowledge for relation extraction. In
Proc. of COLING.
Ralph Grishman, David Westbrook and Adam Meyers.
2005. NYU’s English ACE 2005 System Description.
ACE 2005 Evaluation Workshop.
Earl B. Hunt, Philip J. Stone and Janet Marin. 1966.
Experiments in Induction. New York: Academic
Press, 1966.
Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation
extraction. In Proceedings of HLT-NAACL-07.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for information extraction. In Proceedings of ACL-04.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In
Proceedings of ACL-08: HLT.
Percy Liang. 2005. Semi-Supervised Learning for
Natural Language. Master’s thesis, Massachusetts
Institute of Technology.
Dekang Lin and Xiaoyun Wu. 2009. Phrase Clustering
for Discriminative Learning. In Proc. of ACL-09.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proc. of NAACL.
Scott Miller, Jethran Guinness and Alex Zamanian.
2004. Name Tagging with Word Clusters and
Discriminative Training. In Proc. of HLT-NAACL.
Longhua Qian, Guodong Zhou, Qiaoming Zhu and
Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction . In Proc. of COLING.
John Ross Quinlan. 1986. Induction of decision trees.
Machine Learning, 1(1), 81-106.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL-09.
Ang Sun. 2009. A Two-stage Bootstrapping Algorithm
for Relation Extraction. In RANLP-09.
Ang Sun and Ralph Grishman. 2010. Semi-supervised
Semantic Pattern Discovery with Guidance from
Unsupervised Pattern Clusters. In Proc. of COLING.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
3:1083–1106.
Zhu Zhang. 2004. Weakly supervised relation
classification for information extraction. In Proc. of
CIKM’2004.
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou.
2006. A composite kernel to extract relations
between entities with both flat and structured features.
In Proceedings of COLING-ACL-06.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of ACL.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation
extraction. In Proceedings of ACL-05.
Guodong Zhou, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2007. Tree kernel-based relation
extraction with context-sensitive structured parse tree
information. In Proceedings of EMNLPCoNLL-07.
</reference>
<page confidence="0.998588">
529
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.967149">
<title confidence="0.999745">Semi-supervised Relation Extraction with Large-scale Word Clustering</title>
<author confidence="0.988818">Ang Sun Ralph Grishman Satoshi Sekine</author>
<affiliation confidence="0.9981415">Computer Science New York University</affiliation>
<email confidence="0.999064">asun@cs.nyu.edu</email>
<email confidence="0.999064">grishman@cs.nyu.edu</email>
<email confidence="0.999064">sekine@cs.nyu.edu</email>
<abstract confidence="0.99853225">We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie K Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>Vol</volume>
<pages>6--1817</pages>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie K. Ando and Tong Zhang. 2005 A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data. Journal of Machine Learning Research, Vol 6:1817-1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Boschee</author>
<author>Ralph Weischedel</author>
<author>Alex Zamanian</author>
</authors>
<title>Automatic information extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Intelligence Analysis.</booktitle>
<contexts>
<context position="1628" citStr="Boschee et al., 2005" startWordPosition="218" endWordPosition="221">e entities US soldier and US in the phrase US soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation</context>
<context position="5538" citStr="Boschee et al. (2005)" startWordPosition="831" endWordPosition="834">ring algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al. (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al. (2010). To our knowledge, there has not been work on selecting clusters in a principled way. We move a step further to explore several methods in choosing effective clusters. A second difference between this work and the above ones is that we utilize word clusters in the task of relation extraction which is very different from sequence labeling tasks such as name tagging and chunking. Though Boschee et al. (2005) and Chan and Roth (2010) used word clusters in relation extraction, they shared the same limitation as the above approaches in choosing clusters. For example, Boschee et al. (2005) chose clusters of different granularities and Chan and Roth (2010) simply used a single threshold for cutting the word hierarchy. Moreover, Boschee et al. (2005) only augmented the predicate (typically a verb or a noun of the most importance in a relation in their definition) with word clusters while Chan and Roth (2010) performed this for any lexical feature consisting of a single word. In this paper, we systemati</context>
<context position="12006" citStr="Boschee et al. (2005)" startWordPosition="1865" endWordPosition="1868">e members and the interaction between the heads of the two mentions can be extracted as features as shown in Table 3. In addition, we cherry-picked the following features which were not included in Zhou et al. (2005) but were shown to be quite effective for relation extraction. Bigram of the words between the two mentions: This was extracted by both Zhao and Grishman (2005) and Jiang and Zhai (2007), aiming to 523 provide more order information of the tokens between the two mentions. Patterns: There are three types of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. (2005); 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between the two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a). These patterns can provide more structured information of how the two mentions are connected. Title list: This is tailored for the EMP-ORG type of relations as the head of one of the mentions is usually a title. The features are decoded in a way similar to that of Sun (2009). Position Feature Description Before BM1F first word before M1 BM1L second word before M1 M1 </context>
</contexts>
<marker>Boschee, Weischedel, Zamanian, 2005</marker>
<rawString>Elizabeth Boschee, Ralph Weischedel, and Alex Zamanian. 2005. Automatic information extraction. In Proceedings of the International Conference on Intelligence Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="4610" citStr="Brown et al., 1992" startWordPosition="677" endWordPosition="680">presents related work and Section 3 provides the background of the relation extraction task and the word clustering algorithm. Section 4 describes in detail a state-of-the-art supervised baseline system. Section 5 describes the clusterbased features and the cluster selection methods. We present experimental results in Section 6 and conclude in Section 7. 2 Related Work The idea of using word clusters as features in discriminative learning was pioneered by Miller et al. (2004), who augmented name tagging training data with hierarchical word clusters generated by the Brown clustering algorithm (Brown et al., 1992) from a large unlabeled corpus. They used different thresholds to cut the word hierarchy to obtain clusters of various granularities for feature decoding. Ratinov and Roth (2009) and Turian et al. (2010) also explored this approach for name tagging. Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al. (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al. (2010). To our knowledge, there has not been work on selecting clusters in a principled </context>
<context position="9300" citStr="Brown et al. (1992)" startWordPosition="1437" endWordPosition="1440"> merges the two clusters which cause the least loss in average mutual information between adjacent clusters based on bigram statistics. By tracing the pairwise merging steps, one can obtain a word hierarchy which can be represented as a binary tree. A word can be compactly represented as a bit string by following the path from the root to itself in the tree, assigning a 0 for each left branch, and a 1 for each right branch. A cluster is just a branch of that tree. A high branch may correspond to more general concepts while the lower branches it includes might correspond to more specific ones. Brown et al. (1992) described an efficient implementation based on a greedy algorithm which initially assigned only the most frequent words into distinct clusters. It is worth pointing out that in this implementation each word occupies a leaf in the hierarchy, but each leaf might contain more than one word as can be seen from Table 2. The lengths of the bit strings also vary among different words. 2 http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3- 2.PDF Bit string Examples 111011011100 US ... 1110110111011 U.S. ... 1110110110000 American ... 1110110111110110 Cuban, Pakistani, Russian ... 11111110010111 Ger</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kenrel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="1992" citStr="Bunescu and Mooney, 2005" startWordPosition="271" endWordPosition="274">c parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simp</context>
<context position="8117" citStr="Bunescu and Mooney, 2005" startWordPosition="1237" endWordPosition="1240">ype. Hierarchical: This one separates relation detection from relation classification. One binary 1 Task definition: http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE guidelines: http://projects.ldc.upenn.edu/ace/ 522 classifier is trained first to distinguish between relation instances and non-relation instances. This can be done by grouping all the instances of the 7 relation types into a positive class and the instances of Nil into a negative class. Then the thresholded output of this binary classifier is used as training data for learning a multi-class classifier for the 7 relation types (Bunescu and Mooney, 2005b). Type Example EMP-ORG US president PHYS a military base in Germany GPE-AFF U.S. businessman PER-SOC a spokesman for the senator DISC each of whom ART US helicopters OTHER-AFF Cuban-American people Table 1: ACE relation types and examples from the annotation guideline 2 . The heads of the two entity mentions are marked. Types are listed in decreasing order of frequency of occurrence in the ACE corpus. 3.2 Brown Word Clustering The Brown algorithm is a hierarchical clustering algorithm which initially assigns each word to its own cluster and then repeatedly merges the two clusters which cause</context>
<context position="12233" citStr="Bunescu and Mooney (2005" startWordPosition="1905" endWordPosition="1908">e shown to be quite effective for relation extraction. Bigram of the words between the two mentions: This was extracted by both Zhao and Grishman (2005) and Jiang and Zhai (2007), aiming to 523 provide more order information of the tokens between the two mentions. Patterns: There are three types of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. (2005); 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between the two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a). These patterns can provide more structured information of how the two mentions are connected. Title list: This is tailored for the EMP-ORG type of relations as the head of one of the mentions is usually a title. The features are decoded in a way similar to that of Sun (2009). Position Feature Description Before BM1F first word before M1 BM1L second word before M1 M1 WM1 bag-of-words in M1 HM1 head3 word of M1 Between WBNULL when no word in between WBFL the only word in between when only one word in between WBF first word in between when at least two words in between WBL last word in betwee</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005a. A shortest path dependency kenrel for relation extraction. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1992" citStr="Bunescu and Mooney, 2005" startWordPosition="271" endWordPosition="274">c parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simp</context>
<context position="8117" citStr="Bunescu and Mooney, 2005" startWordPosition="1237" endWordPosition="1240">ype. Hierarchical: This one separates relation detection from relation classification. One binary 1 Task definition: http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE guidelines: http://projects.ldc.upenn.edu/ace/ 522 classifier is trained first to distinguish between relation instances and non-relation instances. This can be done by grouping all the instances of the 7 relation types into a positive class and the instances of Nil into a negative class. Then the thresholded output of this binary classifier is used as training data for learning a multi-class classifier for the 7 relation types (Bunescu and Mooney, 2005b). Type Example EMP-ORG US president PHYS a military base in Germany GPE-AFF U.S. businessman PER-SOC a spokesman for the senator DISC each of whom ART US helicopters OTHER-AFF Cuban-American people Table 1: ACE relation types and examples from the annotation guideline 2 . The heads of the two entity mentions are marked. Types are listed in decreasing order of frequency of occurrence in the ACE corpus. 3.2 Brown Word Clustering The Brown algorithm is a hierarchical clustering algorithm which initially assigns each word to its own cluster and then repeatedly merges the two clusters which cause</context>
<context position="12233" citStr="Bunescu and Mooney (2005" startWordPosition="1905" endWordPosition="1908">e shown to be quite effective for relation extraction. Bigram of the words between the two mentions: This was extracted by both Zhao and Grishman (2005) and Jiang and Zhai (2007), aiming to 523 provide more order information of the tokens between the two mentions. Patterns: There are three types of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. (2005); 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between the two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a). These patterns can provide more structured information of how the two mentions are connected. Title list: This is tailored for the EMP-ORG type of relations as the head of one of the mentions is usually a title. The features are decoded in a way similar to that of Sun (2009). Position Feature Description Before BM1F first word before M1 BM1L second word before M1 M1 WM1 bag-of-words in M1 HM1 head3 word of M1 Between WBNULL when no word in between WBFL the only word in between when only one word in between WBF first word in between when at least two words in between WBL last word in betwee</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005b. Subsequence kernels for relation extraction. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Exploiting background knowledge for relation extraction.</title>
<date>2010</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="5563" citStr="Chan and Roth (2010)" startWordPosition="836" endWordPosition="839">k of name tagging and reported improvements, we noticed that the clusters used by Miller et al. (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al. (2010). To our knowledge, there has not been work on selecting clusters in a principled way. We move a step further to explore several methods in choosing effective clusters. A second difference between this work and the above ones is that we utilize word clusters in the task of relation extraction which is very different from sequence labeling tasks such as name tagging and chunking. Though Boschee et al. (2005) and Chan and Roth (2010) used word clusters in relation extraction, they shared the same limitation as the above approaches in choosing clusters. For example, Boschee et al. (2005) chose clusters of different granularities and Chan and Roth (2010) simply used a single threshold for cutting the word hierarchy. Moreover, Boschee et al. (2005) only augmented the predicate (typically a verb or a noun of the most importance in a relation in their definition) with word clusters while Chan and Roth (2010) performed this for any lexical feature consisting of a single word. In this paper, we systematically explore the effecti</context>
<context position="29293" citStr="Chan and Roth (2010)" startWordPosition="4834" endWordPosition="4837"> Relation Detection Baseline PC4 (△) Prefix10(△) Baseline PC4(△) Prefix10(△) 50 62.9 63.8(+ 0.9) 63.7(+0.8) 71.4 71.9(+ 0.5) 71.6(+0.2) 75 62.8 64.6(+ 1.8) 63.9(+1.1) 71.5 72.3(+ 0.8) 72.5(+1.0) 125 66.1 68.1(+ 2.0) 67.5(+1.4) 74.5 74.8(+ 0.3) 74.3(-0.2) 175 67.8 69.7(+ 1.9) 69.5(+1.7) 75.2 75.5(+ 0.3) 75.2(0.0) 225 68.9 70.1(+ 1.2) 69.6(+0.7) 75.6 75.9(+ 0.3) 75.3(-0.3) ≈280 70.4 71.5(+ 1.1) 70.7(+0.3) 76.4 76.9(+ 0.5) 76.3(-0.1) Table 8: Performance over the 7 relation types with different sizes of training data. Prefix10 uses the single prefix length 10 to generate word clusters as used by Chan and Roth (2010). Type P R F Baseline PC4 (△) Baseline PC4 (△) Baseline PC4 (△) EMP-ORG 75.4 77.2(+1.8) 79.8 81.5(+1.7) 77.6 79.3(+1.7) PHYS 73.2 71.2(-2.0) 61.6 60.2(-1.4) 66.9 65.3(-1.7) GPE-AFF 67.1 69.0(+1.9) 60.0 63.2(+3.2) 63.3 65.9(+2.6) PER-SOC 88.2 83.9(-4.3) 58.4 61.0(+2.6) 70.3 70.7(+0.4) DISC 79.4 80.6(+1.2) 42.9 46.0(+3.2) 55.7 58.6(+2.9) ART 87.9 96.9(+9.0) 63.0 67.4(+4.4) 73.4 79.3(+5.9) OTHER-AFF 70.6 80.0(+9.4) 41.4 41.4(0.0) 52.2 54.6(+2.4) Table 9: Performance of each individual relation type based on 5-fold cross-validation. current size setup were randomly selected and added to the previo</context>
</contexts>
<marker>Chan, Roth, 2010</marker>
<rawString>Yee Seng Chan and Dan Roth. 2010. Exploiting background knowledge for relation extraction. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>David Westbrook</author>
<author>Adam Meyers</author>
</authors>
<date>2005</date>
<booktitle>NYU’s English ACE 2005 System Description. ACE 2005 Evaluation Workshop.</booktitle>
<contexts>
<context position="1651" citStr="Grishman et al., 2005" startWordPosition="222" endWordPosition="225">and US in the phrase US soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is u</context>
<context position="12115" citStr="Grishman et al. (2005)" startWordPosition="1885" endWordPosition="1888"> Table 3. In addition, we cherry-picked the following features which were not included in Zhou et al. (2005) but were shown to be quite effective for relation extraction. Bigram of the words between the two mentions: This was extracted by both Zhao and Grishman (2005) and Jiang and Zhai (2007), aiming to 523 provide more order information of the tokens between the two mentions. Patterns: There are three types of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. (2005); 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between the two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a). These patterns can provide more structured information of how the two mentions are connected. Title list: This is tailored for the EMP-ORG type of relations as the head of one of the mentions is usually a title. The features are decoded in a way similar to that of Sun (2009). Position Feature Description Before BM1F first word before M1 BM1L second word before M1 M1 WM1 bag-of-words in M1 HM1 head3 word of M1 Between WBNULL when no word in between WBFL the only word in betw</context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>Ralph Grishman, David Westbrook and Adam Meyers. 2005. NYU’s English ACE 2005 System Description. ACE 2005 Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Earl B Hunt</author>
<author>Philip J Stone</author>
<author>Janet Marin</author>
</authors>
<title>Experiments in Induction.</title>
<date>1966</date>
<publisher>Academic Press,</publisher>
<location>New York:</location>
<contexts>
<context position="18952" citStr="Hunt et al., 1966" startWordPosition="3058" endWordPosition="3061">l set. In addition, we limit ourselves to use sizes 3 and 4 for m for matching prior work. This keeps the cluster features to a manageable size considering that every word in your vocabulary could contribute to a lexical feature. For picking a subset of , we propose below two statistical measures for computing the importance of a certain prefix length. Information Gain (IG): IG measures the quality or importance of a feature f by computing the difference between the prior entropy of classes IG f ( )    p c p c ( )log ( )  C and the posterior entropy, given values V of the c C  feature f (Hunt et al., 1966; Quinlan, 1986). For   ( ( ) ( |)log ( |))   p v p c v p c v our purpose, C is the set of relation types, f is a cluster-based feature with a certain prefix length such as HM1_WC* where * means the prefix length and a value v is the prefix of the bit string representation of HM1. More formally, the IG of f is computed as follows: v V  c C  where the first and second terms refer to the prior and posterior entropies respectively. For each prefix length in the set , we can compute its IG for a type of cluster feature and then rank the prefix lengths based on their IGs for that cluster feat</context>
</contexts>
<marker>Hunt, Stone, Marin, 1966</marker>
<rawString>Earl B. Hunt, Philip J. Stone and Janet Marin. 1966. Experiments in Induction. New York: Academic Press, 1966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A systematic exploration of the feature space for relation extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL-07.</booktitle>
<contexts>
<context position="1693" citStr="Jiang and Zhai, 2007" startWordPosition="230" endWordPosition="233">upervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical</context>
<context position="11787" citStr="Jiang and Zhai (2007)" startWordPosition="1827" endWordPosition="1830">3 and refer the reader to the paper for the rest of the features. At the lexical level, a relation instance can be seen as a sequence of tokens which form a five tuple &lt;Before, M1, Between, M2, After&gt;. Tokens of the five members and the interaction between the heads of the two mentions can be extracted as features as shown in Table 3. In addition, we cherry-picked the following features which were not included in Zhou et al. (2005) but were shown to be quite effective for relation extraction. Bigram of the words between the two mentions: This was extracted by both Zhao and Grishman (2005) and Jiang and Zhai (2007), aiming to 523 provide more order information of the tokens between the two mentions. Patterns: There are three types of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. (2005); 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between the two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a). These patterns can provide more structured information of how the two mentions are connected. Title list: This is tailored for the EMP-ORG type of rel</context>
<context position="24027" citStr="Jiang and Zhai (2007)" startWordPosition="3922" endWordPosition="3925">We choose to work with maxent because the training is fast and it has a good support for multiclass classification. 6.1 Baseline Performance Following previous work, we did 5-fold crossvalidation on the 348 documents with handannotated entity mentions. Our results are shown in Table 5 which also lists the results of another three state-of-the-art feature based systems. For this and the following experiments, all the results were computed at the relation mention level. System P(%) R(%) F(%) Zhou et al. (2007)9 78.2 63.4 70.1 Zhao and Grishman (2005)10 69.2 71.5 70.4 Our Baseline 73.4 67.7 70.4 Jiang and Zhai (2007) 11 72.4 70.2 71.3 Table 5: Performance comparison on the ACE 2004 data over the 7 relation types. 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data; Zhou et al. (2007) tested their system on the ACE 2004 data. 10 The paper gives a recall value of 70.5, which is not consistent with the given values of P and F. An examination of the correspondence in preparing this paper indicates that the correct recall value is 71.5. 11 The result is from usin</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. A systematic exploration of the feature space for relation extraction. In Proceedings of HLT-NAACL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04.</booktitle>
<contexts>
<context position="1606" citStr="Kambhatla, 2004" startWordPosition="216" endWordPosition="217">lation between the entities US soldier and US in the phrase US soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction. In Proceedings of ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="30710" citStr="Koo et al. (2008)" startWordPosition="5053" endWordPosition="5056">this experiment. The training documents for each size setup were split into a real training set and a development set in a ratio of 7:3 for selecting clusters. There are some clear trends in Table 8. Under each training size, PC4 consistently outperformed the baseline and the system Prefix10 for relation classification. For PC4, the gain for classification was more pronounced than detection. The mixed detection results of Prefix10 indicated that only using a single prefix may not be stable. We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al. (2008) for dependency parsing. PC4 with sizes 50, 125, 175 outperformed the baseline with sizes 75, 175, 225 respectively. But this was not the case when PC4 was tested with sizes 75 and 225. This might due to the complexity of the relation extraction task. 6.5 Analysis There were on average 69 cross-type errors in the baseline in Section 6.1 which were reduced to 56 by using PC4. Table 9 showed that most of the improvements involved EMP-ORG, GPE-AFF, DISC, ART and OTHER-AFF. The performance gain for PER-SOC was not as pronounced as the other five types. The five types of relations are ambiguous as </context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-Supervised Learning for Natural Language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="22175" citStr="Liang, 2005" startWordPosition="3627" endWordPosition="3628">ous and the slowest method for selecting clusters. 6 Experiments In this section, we first present details of our unsupervised word clusters, the relation extraction data set and its preprocessing. We then present a series of experiments coupled with result analyses. We used the English portion of the TDT5 corpora (LDC2006T18) as our unlabeled data for inducing word clusters. It contains roughly 83 million words in 3.4 million sentences with a vocabulary size of 450K. We left case intact in the corpora. Following previous work, we used Liang’s implementation of the Brown clustering algorithm (Liang, 2005). We induced 1,000 word clusters for words that appeared at least twice in the corpora. The reduced vocabulary contains 255K unique words. The clusters are available at http://www.cs.nyu.edu/~asun/data/TDT5_BrownW C.tar.gz. For relation extraction, we used the benchmark ACE 2004 training data. Following most of the previous research, we used in experiments the nwire (newswire) and bnews (broadcast news) genres of the data containing 348 documents and 4374 relation instances. We extracted an instance for every pair of mentions in the same sentence which were separated by no more than two other </context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-Supervised Learning for Natural Language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrase Clustering for Discriminative Learning.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-09.</booktitle>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrase Clustering for Discriminative Learning. In Proc. of ACL-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1589" citStr="Miller et al., 2000" startWordPosition="212" endWordPosition="215">ract an Employment relation between the entities US soldier and US in the phrase US soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. T</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel. 2000. A novel use of statistical parsing to extract information from text. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name Tagging with Word Clusters and Discriminative Training.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="4471" citStr="Miller et al. (2004)" startWordPosition="657" endWordPosition="660">and analyze the performance improvements through an extensive experimental study. The rest of this paper is organized as follows: Section 2 presents related work and Section 3 provides the background of the relation extraction task and the word clustering algorithm. Section 4 describes in detail a state-of-the-art supervised baseline system. Section 5 describes the clusterbased features and the cluster selection methods. We present experimental results in Section 6 and conclude in Section 7. 2 Related Work The idea of using word clusters as features in discriminative learning was pioneered by Miller et al. (2004), who augmented name tagging training data with hierarchical word clusters generated by the Brown clustering algorithm (Brown et al., 1992) from a large unlabeled corpus. They used different thresholds to cut the word hierarchy to obtain clusters of various granularities for feature decoding. Ratinov and Roth (2009) and Turian et al. (2010) also explored this approach for name tagging. Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al. (2004) were quite different from</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness and Alex Zamanian. 2004. Name Tagging with Word Clusters and Discriminative Training. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
<author>Peide Qian</author>
</authors>
<title>Exploiting constituent dependencies for tree kernel-based semantic relation extraction .</title>
<date>2008</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="2104" citStr="Qian et al., 2008" startWordPosition="291" endWordPosition="294">nd semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are crucial for extracting the relation. This mo</context>
</contexts>
<marker>Qian, Zhou, Zhu, Qian, 2008</marker>
<rawString>Longhua Qian, Guodong Zhou, Qiaoming Zhu and Peide Qian. 2008. Exploiting constituent dependencies for tree kernel-based semantic relation extraction . In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Ross Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<volume>1</volume>
<issue>1</issue>
<pages>81--106</pages>
<contexts>
<context position="18968" citStr="Quinlan, 1986" startWordPosition="3062" endWordPosition="3063"> we limit ourselves to use sizes 3 and 4 for m for matching prior work. This keeps the cluster features to a manageable size considering that every word in your vocabulary could contribute to a lexical feature. For picking a subset of , we propose below two statistical measures for computing the importance of a certain prefix length. Information Gain (IG): IG measures the quality or importance of a feature f by computing the difference between the prior entropy of classes IG f ( )    p c p c ( )log ( )  C and the posterior entropy, given values V of the c C  feature f (Hunt et al., 1966; Quinlan, 1986). For   ( ( ) ( |)log ( |))   p v p c v p c v our purpose, C is the set of relation types, f is a cluster-based feature with a certain prefix length such as HM1_WC* where * means the prefix length and a value v is the prefix of the bit string representation of HM1. More formally, the IG of f is computed as follows: v V  c C  where the first and second terms refer to the prior and posterior entropies respectively. For each prefix length in the set , we can compute its IG for a type of cluster feature and then rank the prefix lengths based on their IGs for that cluster feature. For simplic</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>John Ross Quinlan. 1986. Induction of decision trees. Machine Learning, 1(1), 81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL-09.</booktitle>
<contexts>
<context position="4788" citStr="Ratinov and Roth (2009)" startWordPosition="704" endWordPosition="707">t supervised baseline system. Section 5 describes the clusterbased features and the cluster selection methods. We present experimental results in Section 6 and conclude in Section 7. 2 Related Work The idea of using word clusters as features in discriminative learning was pioneered by Miller et al. (2004), who augmented name tagging training data with hierarchical word clusters generated by the Brown clustering algorithm (Brown et al., 1992) from a large unlabeled corpus. They used different thresholds to cut the word hierarchy to obtain clusters of various granularities for feature decoding. Ratinov and Roth (2009) and Turian et al. (2010) also explored this approach for name tagging. Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al. (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al. (2010). To our knowledge, there has not been work on selecting clusters in a principled way. We move a step further to explore several methods in choosing effective clusters. A second difference between this work and the above ones is that we utilize word clusters i</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of CoNLL-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
</authors>
<title>A Two-stage Bootstrapping Algorithm for Relation Extraction.</title>
<date>2009</date>
<booktitle>In RANLP-09.</booktitle>
<contexts>
<context position="12512" citStr="Sun (2009)" startWordPosition="1957" endWordPosition="1958">pes of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. (2005); 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between the two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a). These patterns can provide more structured information of how the two mentions are connected. Title list: This is tailored for the EMP-ORG type of relations as the head of one of the mentions is usually a title. The features are decoded in a way similar to that of Sun (2009). Position Feature Description Before BM1F first word before M1 BM1L second word before M1 M1 WM1 bag-of-words in M1 HM1 head3 word of M1 Between WBNULL when no word in between WBFL the only word in between when only one word in between WBF first word in between when at least two words in between WBL last word in between when at least two words in between WBO other words in between except first and last words when at least three words in between M2 WM2 bag-of-words in M2 HM2 head word of M2 M12 HM12 combination of HM1 and HM2 After AM2F first word after M2 AM2L second word after M2 Table 3: Le</context>
</contexts>
<marker>Sun, 2009</marker>
<rawString>Ang Sun. 2009. A Two-stage Bootstrapping Algorithm for Relation Extraction. In RANLP-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
</authors>
<title>Semi-supervised Semantic Pattern Discovery with Guidance from Unsupervised Pattern Clusters.</title>
<date>2010</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>Sun, Grishman, 2010</marker>
<rawString>Ang Sun and Ralph Grishman. 2010. Semi-supervised Semantic Pattern Discovery with Guidance from Unsupervised Pattern Clusters. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4813" citStr="Turian et al. (2010)" startWordPosition="709" endWordPosition="712">. Section 5 describes the clusterbased features and the cluster selection methods. We present experimental results in Section 6 and conclude in Section 7. 2 Related Work The idea of using word clusters as features in discriminative learning was pioneered by Miller et al. (2004), who augmented name tagging training data with hierarchical word clusters generated by the Brown clustering algorithm (Brown et al., 1992) from a large unlabeled corpus. They used different thresholds to cut the word hierarchy to obtain clusters of various granularities for feature decoding. Ratinov and Roth (2009) and Turian et al. (2010) also explored this approach for name tagging. Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al. (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al. (2010). To our knowledge, there has not been work on selecting clusters in a principled way. We move a step further to explore several methods in choosing effective clusters. A second difference between this work and the above ones is that we utilize word clusters in the task of relation ex</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="1966" citStr="Zelenko et al., 2003" startWordPosition="267" endWordPosition="270">rtial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhu Zhang</author>
</authors>
<title>Weakly supervised relation classification for information extraction.</title>
<date>2004</date>
<booktitle>In Proc. of CIKM’2004.</booktitle>
<marker>Zhang, 2004</marker>
<rawString>Zhu Zhang. 2004. Weakly supervised relation classification for information extraction. In Proc. of CIKM’2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>GuoDong Zhou</author>
</authors>
<title>A composite kernel to extract relations between entities with both flat and structured features.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL-06.</booktitle>
<contexts>
<context position="2065" citStr="Zhang et al., 2006" startWordPosition="283" endWordPosition="286">racts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are cruci</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou. 2006. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of COLING-ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2045" citStr="Zhao and Grishman, 2005" startWordPosition="279" endWordPosition="282">sed method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the w</context>
<context position="11761" citStr="Zhao and Grishman (2005)" startWordPosition="1822" endWordPosition="1825">lexical features as in Table 3 and refer the reader to the paper for the rest of the features. At the lexical level, a relation instance can be seen as a sequence of tokens which form a five tuple &lt;Before, M1, Between, M2, After&gt;. Tokens of the five members and the interaction between the heads of the two mentions can be extracted as features as shown in Table 3. In addition, we cherry-picked the following features which were not included in Zhou et al. (2005) but were shown to be quite effective for relation extraction. Bigram of the words between the two mentions: This was extracted by both Zhao and Grishman (2005) and Jiang and Zhai (2007), aiming to 523 provide more order information of the tokens between the two mentions. Patterns: There are three types of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. (2005); 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between the two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a). These patterns can provide more structured information of how the two mentions are connected. Title list: This is tailored f</context>
<context position="23960" citStr="Zhao and Grishman (2005)" startWordPosition="3910" endWordPosition="3913">nNLP 8 maximum entropy (maxent) package as our machine learning tool. We choose to work with maxent because the training is fast and it has a good support for multiclass classification. 6.1 Baseline Performance Following previous work, we did 5-fold crossvalidation on the 348 documents with handannotated entity mentions. Our results are shown in Table 5 which also lists the results of another three state-of-the-art feature based systems. For this and the following experiments, all the results were computed at the relation mention level. System P(%) R(%) F(%) Zhou et al. (2007)9 78.2 63.4 70.1 Zhao and Grishman (2005)10 69.2 71.5 70.4 Our Baseline 73.4 67.7 70.4 Jiang and Zhai (2007) 11 72.4 70.2 71.3 Table 5: Performance comparison on the ACE 2004 data over the 7 relation types. 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data; Zhou et al. (2007) tested their system on the ACE 2004 data. 10 The paper gives a recall value of 70.5, which is not consistent with the given values of P and F. An examination of the correspondence in preparing this paper indicate</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05.</booktitle>
<contexts>
<context position="1670" citStr="Zhou et al., 2005" startWordPosition="226" endWordPosition="229"> soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by </context>
<context position="11038" citStr="Zhou et al. (2005)" startWordPosition="1696" endWordPosition="1699">iven a pair of entity mentions and the sentence containing the pair, a feature based system extracts a feature vector which contains diverse lexical, syntactic and semantic features. The goal is to learn a function which can estimate the conditional probability p(c I , the probability of a relation type given the feature vector . The type with the highest probability will be output as the class label for the mention pair. We now describe a supervised baseline system with a very large set of features and its learning strategy. 4.1 Baseline Feature Set We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature based relation extraction system. For space reasons, we only show the lexical features as in Table 3 and refer the reader to the paper for the rest of the features. At the lexical level, a relation instance can be seen as a sequence of tokens which form a five tuple &lt;Before, M1, Between, M2, After&gt;. Tokens of the five members and the interaction between the heads of the two mentions can be extracted as features as shown in Table 3. In addition, we cherry-picked the following features which were not included in Zhou et al. (2005) but were shown to be quite effective</context>
<context position="13738" citStr="Zhou et al. (2005)" startWordPosition="2170" endWordPosition="2173">atures for relation extraction. 4.2 Baseline Learning Strategy We employ a simple learning framework that is similar to the hierarchical learning strategy as described in Section 3.1. Specifically, we first train a binary classifier to distinguish between relation instances and non-relation instances. Then rather than using the thresholded output of this binary classifier as training data, we use only the annotated relation instances to train a multi-class classifier for the 7 relation types. In the test phase, 3 The head word of a mention is normally set as the last word of the mention as in Zhou et al. (2005). given a test instance x , we first apply the binary classifier to it for relation detection; if it is detected as a relation instance we then apply the multi-class relation classifier to classify it4. 5 Cluster Feature Selection The selection of cluster features aims to answer the following two questions: which lexical features should be augmented with word clusters to improve generalization accuracy? How to select clusters at an appropriate level of granularity? We will describe our solutions in Section 5.1 and 5.2. 5.1 Cluster Feature Decoding While each one of the lexical features in Tabl</context>
<context position="24286" citStr="Zhou et al. (2005)" startWordPosition="3951" endWordPosition="3954">ts are shown in Table 5 which also lists the results of another three state-of-the-art feature based systems. For this and the following experiments, all the results were computed at the relation mention level. System P(%) R(%) F(%) Zhou et al. (2007)9 78.2 63.4 70.1 Zhao and Grishman (2005)10 69.2 71.5 70.4 Our Baseline 73.4 67.7 70.4 Jiang and Zhai (2007) 11 72.4 70.2 71.3 Table 5: Performance comparison on the ACE 2004 data over the 7 relation types. 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data; Zhou et al. (2007) tested their system on the ACE 2004 data. 10 The paper gives a recall value of 70.5, which is not consistent with the given values of P and F. An examination of the correspondence in preparing this paper indicates that the correct recall value is 71.5. 11 The result is from using the All features in Jiang and Zhai (2007). It is not quite clear from the paper that whether they used the 348 documents or the whole 2004 training data. 526 Note that although all the 4 systems did 5-fold cross-validation on the ACE 2004 data, the detailed</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proceedings of ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Min Zhang</author>
<author>DongHong Ji</author>
<author>QiaoMing Zhu</author>
</authors>
<title>Tree kernel-based relation extraction with context-sensitive structured parse tree information.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL-07.</booktitle>
<contexts>
<context position="2084" citStr="Zhou et al., 2007" startWordPosition="287" endWordPosition="290">exical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are crucial for extracting t</context>
<context position="23919" citStr="Zhou et al. (2007)" startWordPosition="3903" endWordPosition="3906">ised word clusters. We used the OpenNLP 8 maximum entropy (maxent) package as our machine learning tool. We choose to work with maxent because the training is fast and it has a good support for multiclass classification. 6.1 Baseline Performance Following previous work, we did 5-fold crossvalidation on the 348 documents with handannotated entity mentions. Our results are shown in Table 5 which also lists the results of another three state-of-the-art feature based systems. For this and the following experiments, all the results were computed at the relation mention level. System P(%) R(%) F(%) Zhou et al. (2007)9 78.2 63.4 70.1 Zhao and Grishman (2005)10 69.2 71.5 70.4 Our Baseline 73.4 67.7 70.4 Jiang and Zhai (2007) 11 72.4 70.2 71.3 Table 5: Performance comparison on the ACE 2004 data over the 7 relation types. 6 http://nlp.stanford.edu/software/lex-parser.shtml 7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 8 http://opennlp.sourceforge.net/ 9 Zhou et al. (2005) tested their system on the ACE 2003 data; Zhou et al. (2007) tested their system on the ACE 2004 data. 10 The paper gives a recall value of 70.5, which is not consistent with the given values of P and F. An examination of the corres</context>
</contexts>
<marker>Zhou, Zhang, Ji, Zhu, 2007</marker>
<rawString>Guodong Zhou, Min Zhang, DongHong Ji, and QiaoMing Zhu. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proceedings of EMNLPCoNLL-07.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>