<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001859">
<title confidence="0.9978795">
A Comparative Study of Reinforcement Learning Techniques on
Dialogue Management
</title>
<author confidence="0.563803">
Alexandros Papangelis
</author>
<affiliation confidence="0.7053365">
NCSR ”Demokritos”,
Institute of Informatics
</affiliation>
<sectionHeader confidence="0.418978" genericHeader="abstract">
&amp; Telecommunications
</sectionHeader>
<keyword confidence="0.183229">
and
</keyword>
<affiliation confidence="0.9871135">
Univ. of Texas at Arlington,
Comp. Science and Engineering
</affiliation>
<email confidence="0.996614">
alexandros.papangelis@mavs.uta.edu
</email>
<sectionHeader confidence="0.997345" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.99868725">
Adaptive Dialogue Systems are rapidly be-
coming part of our everyday lives. As they
progress and adopt new technologies they
become more intelligent and able to adapt
better and faster to their environment. Re-
search in this field is currently focused on
how to achieve adaptation, and particularly
on applying Reinforcement Learning (RL)
techniques, so a comparative study of the
related methods, such as this, is necessary.
In this work we compare several standard
and state of the art online RL algorithms
that are used to train the dialogue manager
in a dynamic environment, aiming to aid re-
searchers / developers choose the appropri-
ate RL algorithm for their system. This is
the first work, to the best of our knowledge,
to evaluate online RL algorithms on the di-
alogue problem and in a dynamic environ-
ment.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999792365384615">
Dialogue Systems (DS) are systems that are able
to make natural conversation with their users.
There are many types of DS that serve various
aims, from hotel and flight booking to provid-
ing information or keeping company and forming
long term relationships with the users. Other in-
teresting types of DS are tutorial systems, whose
goal is to teach something new, persuasive sys-
tems whose goal is to affect the user’s attitude to-
wards something through casual conversation and
rehabilitation systems that aim at engaging pa-
tients to various activities that help their rehabili-
tation process. DS that incorporate adaptation to
their environment are called Adaptive Dialogue
Systems (ADS). Over the past few years ADS
have seen a lot of progress and have attracted the
research community’s and industry’s interest.
There is a number of available ADS, apply-
ing state of the art techniques for adaptation and
learning, such as the one presented by Young et
al., (2010), where the authors propose an ADS
that provides tourist information in a fictitious
town. Their system is trained using RL and some
clever state compression techniques to make it
scalable, it is robust to noise and able to recover
from errors (misunderstandings). Cuay´ahuitl et
al. (2010) propose a travel planning ADS, that is
able to learn dialogue policies using RL, building
on top of existing handcrafted policies. This en-
ables the designers of the system to provide prior
knowledge and the system can then learn the de-
tails. Konstantopoulos (2010) proposes an affec-
tive ADS which serves as a museum guide. It is
able to adapt to each user’s personality by assess-
ing his / her emotional state and current mood and
also adapt its output to the user’s expertise level.
The system itself has an emotional state that is af-
fected by the user and affects its output.
An example ADS architecture is depicted in
Figure 1, where we can see several components
trying to understand the user’s utterance and sev-
eral others trying to express the system’s re-
sponse. The system first attempts to convert spo-
ken input to text using the Automatic Speech
Recognition (ASR) component and then tries to
infer the meaning using the Natural Language Un-
derstanding (NLU) component. At the core lies
the Dialogue Manager (DM), a component re-
sponsible for understanding what the user’s utter-
ance means and deciding which action to take that
will lead to achieving his / her goals. The DM
may also take into account contextual information
</bodyText>
<page confidence="0.978556">
22
</page>
<note confidence="0.9932035">
Proceedings of the EACL 2012 Student Research Workshop, pages 22–31,
Avignon, France, 26 April 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999802">
Figure 1: Example architecture of an ADS.
</figureCaption>
<bodyText confidence="0.999746551724138">
or historical data before making a decision. After
the system has decided what to say, it uses the
Referring Expression Generation (REG) compo-
nent to create appropriate referring expressions,
the Natural Language Generation (NLG) compo-
nent to create the textual form of the output and
last, the Text To Speech (TTS) component to con-
vert the text to spoken output.
Trying to make ADS as human-like as possi-
ble researchers have focused on techniques that
achieve adaptation, i.e. adjust to the current user’s
personality, behaviour, mood, needs and to the
environment in general. Examples include adap-
tive or trainable NLG (Rieser and Lemon, 2009),
where the authors formulate their problem as a
statistical planning problem and use RL to find
a policy according to which the system will de-
cide how to present information. Another exam-
ple is adaptive REG (Janarthanam and Lemon,
2009), where the authors again use RL to choose
one of three strategies (jargon, tutorial, descrip-
tive) according to the user’s expertise level. An
example of adaptive TTS is the work of Boidin
et al. (2009), where the authors propose a model
that sorts paraphrases with respect to predictions
of which sounds more natural. Jurˇciˇcek et al.
(2010) propose a RL algorithm to optimize ADS
parameters in general. Last, many researchers
have used RL to achieve adaptive Dialogue Man-
agement (Pietquin and Hastie, 2011; Gaˇsi´c et al.,
2010; Cuay´ahuitl et al., 2010).
As the reader may have noticed, the current
trend in training these components is the appli-
cation of RL techniques. RL is a well established
field of artificial intelligence and provides us with
robust frameworks that are able to deal with un-
certainty and can scale to real world problems.
One sub category of RL is Online RL where the
system can be trained on the fly, as it interacts
with its environment. These techniques have re-
cently begun to be applied to Dialogue Manage-
ment and in this paper we perform an extensive
evaluation of several standard and state of the art
Online RL techniques on a generic dialogue prob-
lem. Our experiments were conducted with user
simulations, with or without noise and using a
model that is able to alter the user’s needs at any
given point. We were thus able to see how well
each algorithm adapted to minor (noise / uncer-
tainty) or major (change in user needs) changes in
the environment.
In general, RL algorithms fall in two cate-
gories, planning and learning algorithms. Plan-
ning or model-based algorithms use training ex-
amples from previous interactions with the envi-
ronment as well as a model of the environment
that simulates interactions. Learning or model-
free algorithms only use training examples from
previous interactions with the environment and
that is the main difference of these two categories,
according to Sutton and Barto, (1998). The goal
of an RL algorithm is to learn a good policy (or
strategy) that dictates how the system should in-
teract with the environment. An algorithm then
can follow a specific policy (i.e. interact with
the environment in a specific, maybe predefined,
way) while searching for a good policy. This way
of learning is called “off policy” learning. The op-
posite is “on policy” learning, when the algorithm
follows the policy that it is trying to learn. This
will become clear in section 2.2 where we pro-
vide the basics of RL. Last, these algorithms can
be categorized as policy iteration or value itera-
tion algorithms, according to the way they evalu-
ate and train a policy.
Table 1 shows the algorithms we evaluated
along with some of their characteristics. We se-
lected representative algorithms for each category
and used the Dyna architecture (Sutton and Barto,
1998) to implement model based algorithms.
SARSA(A) (Sutton and Barto, 1998), Q Learn-
ing (Watkins, 1989), Q(A) (Watkins, 1989; Peng
and Williams, 1996) and AC-QV (Wiering and
Van Hasselt, 2009) are well established RL al-
gorithms, proven to work and simple to imple-
ment. A serious disadvantage though is the fact
that they do not scale well (assuming we have
</bodyText>
<page confidence="0.996632">
23
</page>
<bodyText confidence="0.99996996">
enough memory), as also supported by our results
in section 5. Least Squares SARSA(A) (Chen and
Wei, 2008) is a variation of SARSA(A) that uses
the least squares method to find the optimal pol-
icy. Incremental Actor Critic (IAC) (Bhatnagar
et al., 2007) and Natural Actor Critic (NAC) (Pe-
ters et al., 2005) are actor - critic algorithms that
follow the expected rewards gradient and the nat-
ural or Fisher Information gradient respectively
(Szepesv´ari, 2010).
An important attribute of many learning algo-
rithms is function approximation which allows
them to scale to real world problems. Function
approximation attempts to approximate a target
function by selecting from a class of functions
that closely resembles the target. Care must be
taken however, when applying this method, be-
cause many RL algorithms are not guaranteed to
converge when using function approximation. On
the other hand, policy gradient algorithms (algo-
rithms that perform gradient ascend/descend on
a performance surface), such as NAC or Natural
Actor Belief Critic (Jurˇciˇcek et al., 2010) have
good guarantees for convergence, even if we use
function approximation (Bhatnagar et al., 2007).
</bodyText>
<table confidence="0.999526166666667">
Algorithm Model Policy Iteration
SARSA(A) No On Value
LS-SARSA(A) No On Policy
Q Learning No Off Value
Q(A) No Off Value
Actor Critic - QV No On Policy
IAC No On Policy
NAC No On Policy
DynaSARSA(A) Yes On Value
DynaQ Yes Off Value
DynaQ(A) Yes Off Value
DynaAC-QV Yes On Policy
</table>
<tableCaption confidence="0.9749505">
Table 1: Online RL algorithms used in our
evaluation.
</tableCaption>
<bodyText confidence="0.999858269230769">
While there is a significant amount of work in
evaluating RL algorithms, this is the first attempt,
to the best of our knowledge, to evaluate online
learning RL algorithms on the dialogue manage-
ment problem, in the presence of uncertainty and
changes in the environment.
Atkeson and Santamaria (1997) evaluate model
based and model free algorithms on the single
pendulum swingup problem but their algorithms
are not the ones we have selected and the prob-
lem on which they were evaluated differs from
ours in many ways. Ross et al. (2008) com-
pare many online planning algorithms for solving
Partially Observable Markov Decision Processes
(POMDP). It is a comprehensive study but not di-
rectly related to ours, as we model our problem
with Markov Decision Processes (MDP) and eval-
uate model-based and model-free algorithms on a
specific task.
In the next section we provide some back-
ground knowledge on MDPs and RL techniques,
in section 3 we present our proposed formulation
of the slot filling dialogue problem, in section 4
we describe our experimental setup and results, in
section 5 we discuss those results and in section 6
we conclude this study.
</bodyText>
<sectionHeader confidence="0.993209" genericHeader="method">
2 Background
</sectionHeader>
<bodyText confidence="0.999866">
In order to fully understand the concepts dis-
cussed in this work we will briefly introduce MDP
and RL and explain how these techniques can be
applied to the dialogue policy learning problem.
</bodyText>
<subsectionHeader confidence="0.986775">
2.1 Markov Decision Process
</subsectionHeader>
<bodyText confidence="0.999697166666666">
A MDP is defined as a triplet M = {X, A, P},
where X is a non empty set of states, A is a non
empty set of actions and P is a transition probabil-
ity kernel that assigns probability measures over
X x R for each state-action pair (x, a) E X x A.
We can also define the state transition probabil-
ity kernel Pt that for each triplet (x1, a, x2) E
X x A x X would give us the probability of
moving from state x1 to state x2 by taking action
a. Each transition from a state to another is as-
sociated with an immediate reward, the expected
value of which is called the reward function and
is defined as R(x, a) = E[r(x, a)], where r(x, a)
is the immediate reward the system receives after
taking action a (Szepesv´ari, 2010). An episodic
MDP is defined as an MDP with terminal states,
Xt+3 = x, Vs &gt; 1. We consider an episode over
when a terminal state is reached.
</bodyText>
<subsectionHeader confidence="0.999145">
2.2 Reinforcement Learning
</subsectionHeader>
<bodyText confidence="0.99949025">
Motivation to use RL in the dialogue problem
came from the fact that it can easily tackle some
of the challenges that arise when implementing
dialogue systems. One of those, for example, is
error recovery. Hand crafted error recovery does
not scale at all so we need an automated process
to learn error-recovery strategies. More than this
we can automatically learn near optimal dialogue
</bodyText>
<page confidence="0.992277">
24
</page>
<bodyText confidence="0.999911961538462">
policies and thus maximize user satisfaction. An-
other benefit of RL is that it can be trained using
either real or simulated users and continue to learn
and adapt with each interaction (in the case of on-
line learning). To use RL we need to model the
dialogue system using MDPs, POMDPs or Semi
Markov Desicion Processes (SMDP). POMDPs
take uncertainty into account and model each state
with a distribution that represents our belief that
the system is in a specific state. SMDPs add tem-
poral abstraction to the model and allow for time
consuming operations. We, however, do not deal
with either of those in an attempt to keep the prob-
lem simple and focus on the task of comparing the
algorithms.
More formally, RL tries to maximize an objec-
tive function by learning how to control the ac-
tions of a system. A system in this setting is typ-
ically formulated as an MDP. As we discussed in
section 2.1 for every MDP we can define a pol-
icy π, which is a mapping from states x ∈ X and
actions α ∈ A to a distribution π(x, α) that repre-
sents the probability of taking action α when the
system is in state x. This policy dictates the be-
haviour of the system. To estimate how good a
policy is we define the value function V :
</bodyText>
<equation confidence="0.997194">
γtRt+1|x0 = x], x ∈ X (1)
</equation>
<bodyText confidence="0.9994722">
which gives us the expected cumulative rewards
when beginning from state x and following policy
π, discounted by a factor γ ∈ [0, 1] that models
the importance of future rewards. We define the
return of a policy π as:
</bodyText>
<equation confidence="0.981529333333333">
00
Jπ = E γtRt(xt,π(xt)) (2)
t=0
</equation>
<bodyText confidence="0.558724">
A policy π is optimal if Jπ(x) = V π(x), ∀x ∈
X. We can also define the action-value function
Q:
</bodyText>
<equation confidence="0.989603">
γtRt+1|x0 = x, a0 = α] (3)
</equation>
<bodyText confidence="0.997306777777778">
where x ∈ X, α ∈ A, which gives us the ex-
pected cumulative discounted rewards when be-
ginning from state x and taking action α, again
following policy π. Note that Vmax = rax
1�γ ,
where R(x) ∈ [rmin, rmax].
The goal of RL therefore is to find the optimal
policy, which maximizes either of these functions
(Szepesv´ari, 2010).
</bodyText>
<sectionHeader confidence="0.887178" genericHeader="method">
3 Slot Filling Problem
</sectionHeader>
<bodyText confidence="0.999993">
We formulated the problem as a generic slot fill-
ing ADS, represented as an MDP. This model has
been proposed in (Papangelis et al., 2012), and we
extend it here to account for uncertainty. Formally
the problem is defined as: S =&lt; s0, ..., sN &gt;∈
M,M = M0 ×M1 ×...×MN,Mi = {1,...,Ti},
where S are the N slots to be filled, each slot si
can take values from Mi and Ti is the number of
available values slot si can be filled with. Dia-
logue state is also defined as a vector d ∈ M,
where each dimension corresponds to a slot and
its value corresponds to the slot’s value. We call
the set of all possible dialogue states D. System
actions A ∈ {1, ..., |S|} are defined as requests
for slots to be filled and ai requests slot si. At
each dialogue state di we define a set of available
actions iii ⊂ A. A user query q ⊂ S is defined
as the slots that need to be filled so that the sys-
tem will be able to accurately provide an answer.
We assume action aN always means Give Answer.
The reward function is defined as:
</bodyText>
<equation confidence="0.937072">
−1, if a =6 aN
−100, if a = aN, ∃qi|qi = ∅
0, if a = aN, ¬∃qi|qi = ∅
(4)
</equation>
<bodyText confidence="0.999786">
Thus, the optimal reward for each problem is −|q|
since |q |&lt; |S|.
Available actions for every state can be mod-
elled as a matrix A ∈ {0,1}|D|x|A|, where:
</bodyText>
<equation confidence="0.97700875">
�
1, if aj ∈ ai
�Aij =
0, if aj ∈6
</equation>
<bodyText confidence="0.98746">
When designing A one must keep in mind that
the optimal solution depends on �A’s structure
and must take care not to create an unsolvable
problem, i.e. a disconnected MDP. This can be
avoided by making sure that each action is avail-
able at some state and that each state has at least
one available action. We should now define the
necessary conditions for the slot filling problem
to be solvable and the optimal reward be as de-
fined before:
</bodyText>
<equation confidence="0.976602916666667">
∃�αij = 1, 1 ≤ i &lt; |D|,∀j (6)
00
V π(x) = E[ E
t=0
00
Qπ(x, α) = E[ E
t=0
R(d, a) = {
i
(5)
25
]˜αij = 1, 1 &lt; j &lt; |A|,Vi (7)
</equation>
<bodyText confidence="0.642767666666667">
Note that j &gt; 1 since d1 is our starting state. We
also allow Give Answer (which is aN) to be avail-
able from any state:
</bodyText>
<equation confidence="0.968948">
˜Ai,N = 1, 1 &lt; i &lt; |D |(8)
</equation>
<bodyText confidence="0.9998903">
We define available action density to be the ra-
tio of 1s over the number of elements of ˜A:
We can now incorporate uncertainty in our
model. Rather than allowing deterministic transi-
tions from a state to another we define a distribu-
tion Pt(dj|di, am) which models the probability
by which the system will go from state di to dj
when taking action am. Consequently, when the
system takes action am from state di, it transits to
state dk with probability:
</bodyText>
<equation confidence="0.88474575">
{ Pt(dj|di, am), k = j
1−Pt(d9|di,an)
|D|−1 , k =� j
(9)
</equation>
<bodyText confidence="0.999133777777778">
assuming that under no noise conditions action
am would move the system from state di to state
dj. The probability of not transiting to state dj
is uniformly distributed among all other states.
Pt(dj|di, am) is updated after each episode with
a small additive noise ν, mainly to model unde-
sirable or unforeseen effects of actions. Another
distribution, Pc(sj = 1) E [0, 1], models our con-
fidence level that slot sj is filled:
</bodyText>
<equation confidence="0.98572725">
�
1, Pc(sj = 1) &gt; 0.5
sj = (10)
0, Pc(sj = 1) &lt; 0.5
</equation>
<bodyText confidence="0.9980403">
In our evaluation Pc(sj) is a random number be-
tween [1 − E,1] where E models the level of un-
certainty. Last, we can slightly alter A˜ after each
episode to model changes or faults in the avail-
able actions for each state, but we did not in our
experiments.
The algorithms selected for this evaluation are
then called to solve this problem online and find
an optimal policy π? that will yield the highest
possible reward.
</bodyText>
<table confidence="0.999728333333333">
Algorithm α β γ λ
SARSA(λ) 0.95 - 0.55 0.4
LS-SARSA(λ) 0.95 - 0.55 0.4
Q Learning 0.8 - 0.8 -
Q(λ) 0.8 - 0.8 0.05
Actor Critic - QV 0.9 0.25 0.75 -
IAC 0.9 0.25 0.75 -
NAC 0.9 0.25 0.75 -
DynaSARSA(λ) 0.95 - 0.25 0.25
DynaQ 0.8 - 0.4 -
DynaQ(λ) 0.8 - 0.4 0.05
DynaAC-QV 0.9 0.05 0.75 -
</table>
<tableCaption confidence="0.994455">
Table 2: Optimized parameter values.
</tableCaption>
<sectionHeader confidence="0.999126" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9825445">
Our main goal was to evaluate how each algo-
rithm behaves in the following situations:
</bodyText>
<listItem confidence="0.9919885">
• The system needs to adapt to a noise free en-
vironment.
• The system needs to adapt to a noisy envi-
ronment.
• There is a change in the environment and the
system needs to adapt.
</listItem>
<bodyText confidence="0.99994676">
To ensure each algorithm performed to the best
of its capabilities we tuned each one’s parameters
in an exhaustive manner. Table 2 shows the pa-
rameter values selected for each algorithm. The
parameter E in E-greedy strategies was set to 0.01
and model-based algorithms trained their model
for 15 iterations after each interaction with the
environment. Learning rates α and β and explo-
ration parameter E decayed as the episodes pro-
gressed to allow better stability.
At each episode the algorithms need enough it-
erations to explore the state space. At the initial
stages of learning, though, it is possible that some
algorithms fall into loops and require a very large
number of iterations before reaching a terminal
state. It would not hurt then if we bound the num-
ber of iterations to a reasonable limit, provided it
allows enough “negative” rewards to be accumu-
lated when following a “bad” direction. In our
evaluation the algorithms were allowed 2|D |iter-
ations, ensuring enough steps for exploration but
not allowing “bad” directions to be followed for
too long.
To assess each algorithm’s performance and
convergence speed, we run each algorithm 100
</bodyText>
<equation confidence="0.9961315">
Density =|W,j)|
˜Aij = 1l|
|D |x |A|
Pt(dk|di, am) =
</equation>
<page confidence="0.977759">
26
</page>
<bodyText confidence="0.999871235294118">
times on a slot filling problem with 6 slots, 6 ac-
tions and 300 episodes. The average reward over
a high number of episodes indicates how stable
each algorithm is after convergence. User query q
was set to be {s1, ..., s51 and there was no noise
in the environment, meaning that the action of
querying a slot deterministically gets the system
into a state where that slot is filled. This can be
formulated as: Pt(dj|di, am) = 1, P,(sj) = 1Vj,
v = 0 and �Aij = 1, Vi, j.
To evaluate the algorithms’ performance in
the presence of uncertainty we run each for 100
times, on the same slot filling problem but with
Pt(dj|di, am) E [1 − E, 1], with varying E and
available action density values. At each run, each
algorithm was evaluated using the same transition
probabilities and available actions. To assess how
the algorithms respond to environmental changes
we conducted a similar but noise free experiment,
where after a certain number of episodes the query
q was changed. Remember that q models the re-
quired information for the system to be able to an-
swer with some degree of certainty, so changing q
corresponds to requiring different slots to be filled
by the user. For this experiment we randomly gen-
erated two queries of approximately 65% of the
number of slots. The algorithms then needed to
learn a policy for the first query and then adapt
to the second, when the change occurs. This
could, for example, model scenarios where hotel
booking becomes unavailable or some airports are
closed, in a travel planning ADS. Last, we evalu-
ated each algorithm’s scalability, by running each
for 100 times on various slot filling problems, be-
ginning with a problem with 4 slots and 4 actions
up to a problem with 8 slots and 8 actions. We
measured the return averaged over the 100 runs
each algorithm achieved.
Despite many notable efforts, a standardized
evaluation framework for ADS or DS is still con-
sidered an open question by the research commu-
nity. The work in (Pietquin and Hastie, 2011)
provides a very good survey of current techniques
that evaluate several aspects of Dialogue Systems.
When RL is applied, researchers typically use
the reward function as a metric of performance.
This will be our evaluation metric as well, since
it is common across all algorithms. As defined
in section 2.3, it penalizes attempts to answer the
user’s query with incomplete information as well
as lengthy dialogues.
</bodyText>
<table confidence="0.960419666666667">
Algorithm Average Reward
SARSA(A) -10.5967
LS-SARSA(A) -14.3439
Q Learning -14.8888
Q(A) -63.7588
Actor Critic - QV -15.9245
IAC -10.5000
NAC -5.8273
DynaSARSA(A) -11.9758
DynaQ -14.7270
DynaQ(A) -17.1964
DynaAC-QV -58.4576
</table>
<tableCaption confidence="0.999429">
Table 3: Average Total Reward without noise.
</tableCaption>
<bodyText confidence="0.999924375">
As mentioned earlier in the text we opted for
user simulations for our evaluation experiments
instead of real users. This method has a number of
advantages, for example the fact that we can very
quickly generate huge numbers of training exam-
ples. One might suggest that since the system is
targeted to real users it might not perform as well
when trained using simulations. However, as can
be seen from our results, there are online algo-
rithms, such as NAC or SARSA(A), that can adapt
well to environmental changes, so it is reasonable
to expect such a system to adapt to a real user even
if trained using simulations. We can now present
the results of our evaluation, as described above
and in the next section we will provide insight on
the algorithms’ behaviour on each experiment.
</bodyText>
<table confidence="0.99887975">
Alg. E1 E2 E3 E4
S(A) -7.998 -13.94 -23.68 -30.01
LSS -9.385 -12.34 -25.67 -32.33
Q -6.492 -15.71 -23.36 -30.56
Q(A) -22.44 -23.27 -27.04 -29.37
AC -8.648 -17.91 -32.14 -38.46
IAC -6.680 -18.58 -33.60 -35.39
NAC -3.090 -9.142 -19.46 -21.33
DS(A) -8.108 -15.61 -38.22 -41.90
DQ -6.390 -13.04 -23.64 -28.69
DQ(A) -16.04 -17.33 -39.20 -38.42
DAC -28.39 -32.25 -44.26 -45.01
</table>
<tableCaption confidence="0.999345">
Table 4: Average Total Reward with noise.
</tableCaption>
<subsectionHeader confidence="0.991754">
4.1 Average reward without noise
</subsectionHeader>
<bodyText confidence="0.99979">
Table 3 shows the average total reward each al-
gorithm achieved (i.e. the average of the sum of
rewards for each episode), over 100 runs, each
run consisting of 300 episodes. The problem had
6 slots, 6 actions, a query q = {s1, ..., s51 and
no noise. In this scenario the algorithms need to
learn to request each slot only once and give the
</bodyText>
<page confidence="0.995479">
27
</page>
<bodyText confidence="0.998668428571429">
answer when all slots are filled. The optimal re-
ward in this case was −5. Remember that during
the early stages of training the algorithms receive
suboptimal rewards until they converge to the op-
timal policy that yields J&amp;quot;* = −5. The sum of re-
wards an algorithm received for each episode then
can give us a rough idea of how quickly it con-
verged and how stable it is. Clearly NAC outper-
forms all other algorithms with an average reward
of −5.8273 showing it converges early and is sta-
ble from then on. Note that the differences in per-
formance are statistically significant except be-
tween LS-SARSA(A), DynaSARSA(A) and Dy-
naQ Learning.
</bodyText>
<subsectionHeader confidence="0.977726">
4.2 Average reward with noise
</subsectionHeader>
<bodyText confidence="0.999965821428571">
Table 4 shows results from four similar exper-
iments (E1, E2, E3 and E4), with 4 slots, 4
actions, q = {s1, s2, s3} and 100 episodes
but in the presence of noise. For E1 we set
Pt(dj|dZ, am) = 1 and Density to 1, for E2 we
set Pt(dj|dZ, am) = 0.8 and Density to 0.95, for
E3 we set Pt(dj|dZ, am) = 0.6 and Density to
0.9 and for E4 we set Pt(dj|dZ, am) = 0.4 and
Density to 0.8. After each episode we added a
small noise v E [−0.05,0.05] to Pt(·). Remem-
ber that each algorithm run for 2|� |iterations
(32 in this case) for each episode, so an aver-
age lower than −32 indicates slow convergence
or even that the algorithm oscillates. In E1, since
there are few slots and no uncertainty, most algo-
rithms, except for IAC, NAC and Q(A) converge
quickly and have statistically insignificant differ-
ences with each other. In E2 we have less pairs
with statistically insignificant differences, and in
E3 and E4 we only have the ones mentioned in
the previous section. As we can see, NAC han-
dles uncertainty better, by a considerable margin,
than the rest algorithms. Note here that Q(A) con-
verges late while Q Learning, Dyna Q Learning,
SARSA(A) AC-QV and Dyna SARSA(A) oscil-
late a lot in the presence of noise. The optimal
reward is −3, so it is evident that most algorithms
cannot handle uncertainty well.
</bodyText>
<subsectionHeader confidence="0.998938">
4.3 Response to change
</subsectionHeader>
<bodyText confidence="0.999961277777778">
In this experiment we let each algorithm run for
500 episodes in a problem with 6 slots and 6
actions. We generated two queries, q1 and q2,
consisting of 4 slots each, and begun the algo-
rithms with q1. After 300 episodes the query
was changed to q2 and the algorithms were al-
lowed another 200 episodes to converge. Table
5 shows the episode at which, on average, each
algorithm converged after the change (after the
300th episode). Note here that the learning rates
α and Q were reset at the point of change. Differ-
ences in performance, with respect to the average
reward collected during this experiment are statis-
tically significant, except between SARSA(A), Q
Learning and DynaQ(A). We can see that NAC
converges only after 3 episodes on average, with
IAC converging after 4. All other algorithms re-
quire many more episodes, from about 38 to 134.
</bodyText>
<table confidence="0.996329916666667">
Algorithm Episode
SARSA(A) 360.5
LS-SARSA(A) 337.6
Q Learning 362.8
Q(A) 342.5
Actor Critic - QV 348.7
IAC 304.1
NAC 302.9
DynaSARSA(A) 402.6
DynaQ 380.2
DynaQ(A) 384.6
DynaAC-QV 433.3
</table>
<tableCaption confidence="0.980323">
Table 5: Average number of episodes required
</tableCaption>
<bodyText confidence="0.701758">
for convergence after the change.
</bodyText>
<subsectionHeader confidence="0.999828">
4.4 Convergence Speed
</subsectionHeader>
<bodyText confidence="0.999990545454546">
To assess the algorithms’ convergence speed we
run each algorithm 100 times for problems of “di-
mension” 4 to 8 (i.e. 4 slots and 4 actions, 5 slots
and 5 actions and so on). We then marked the
episode at which each algorithm had converged
and averaged it over the 100 runs. Table 6 shows
the results. It is important to note here that LS-
SARSA, IAC and NAC use function approxima-
tion while the rest algorithms do not. We, how-
ever, assume that we have enough memory for
problems up to 8 slots and 8 actions and are only
interested in how many episodes it takes each
algorithm to converge, on average. The results
show how scalable the algorithms are with respect
to computational power.
We can see that after dimension 7 many algo-
rithms require much more episodes in order to
converge. LS-SARSA(A), IAC and NAC once
again seem to behave better than the others, re-
quiring only a few more episodes as the prob-
lem dimension increases. Note here however that
these algorithms take much more absolute time to
</bodyText>
<page confidence="0.996449">
28
</page>
<bodyText confidence="0.981117333333333">
converge compared to simpler algorithms (eg Q
Learning) who might require more episodes but
each episode is completed faster.
</bodyText>
<table confidence="0.999575166666667">
Algorithm 4 5 6 7 8
S(A) 5 23 29 42 101
LSS(A) 10 22 27 38 51
Q 11 29 47 212 816
Q(A) 5 12 29 55 96
AC 12 21 42 122 520
IAC 7 14 29 32 39
NAC 5 9 17 23 28
DS(A) 5 11 22 35 217
DQ 15 22 60 186 669
DQ(A) 9 13 55 72 128
DAC 13 32 57 208 738
</table>
<tableCaption confidence="0.9369545">
Table 6: Average number of episodes required
for convergence on various problem dimensions.
</tableCaption>
<sectionHeader confidence="0.99861" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999939759493671">
SARSA(A) performed almost equally to IAC
at the experiment with deterministic transitions
but did not react well to the change in q. As
we can see in Table 6, SARSA(A) generally con-
verges at around episode 29 for a problem with
6 slots and 6 actions, therefore the 61 episodes it
takes it to adapt to change are somewhat many.
This could be due to the fact that SARSA(A) uses
eligibility traces which means that past state - ac-
tion pairs still contribute to the updates, so even if
the learning rate α is reset immediately after the
change to allow faster convergence, it seems not
enough. It might be possible though to come up
with a strategy and deal with this type of situa-
tion, for example zero out all traces as well as re-
setting α. SARSA(A) performs above average in
the presence of noise in this particular problem.
LS-SARSA(A) practically is SARSA(A) with
function approximation. While this gives the ad-
vantage of requiring less memory, it converges a
little slower than SARSA(A) in the presence of
noise or in noise free environments and it needs
more episodes to converge as the size of the prob-
lem grows. It does, however, react better to
changes in the user’s goals, since it requires 38
episodes to converge after the change, compared
to 27 it normally needs as we can see in Table 6.
Q Learning exhibits similar behaviour with
the only difference that it converges a little later.
Again it takes many episodes to converge after the
change in the environment (compared to the 47
that it needs initially). This could be explained by
the fact that Q Learning only updates one row of
Q(x, a) at each iteration, thus needing more itera-
tions for Q(x, a) to reflect expected rewards in the
new environment. Like SARSA(A), Q Learning is
able to deal with uncertainty well enough on the
dialogue task in the given time, but does not scale
well.
Q(A) , quite opposite from SARSA(A) and Q
Learning, is the slowest to initially converge, but
handles changes in the environment much better.
In Q(A) the update of Q(x, a) is (very roughly)
based on the difference of Q(x, a&apos;) − Q(x, a*)
where a* is the best possible action the algo-
rithm can take, whereas in SARSA(A) the update
is (again roughly) based on Q(x, a&apos;) − Q(x, a).
Also, in Q(A) eligibility traces become zero if the
selected action is not the best possible. These two
reasons help obsolete information in Q(x, a) be
quickly updated. While it performs worse in the
presence of uncertainty, the average reward does
not drop as steeply as for the rest algorithms.
AC-QV converges better than average, com-
pared to the other algorithms, and seems to cope
well with changes in the environment. While
it needs 42 episodes, on average, to converge
for a problem of 6 slots and 6 actions, it only
needs around 49 episodes to converge again af-
ter a change. Unlike SARSA(A) and Q(A) it does
not have eligibility traces to delay the update of
Q(x, a) (or P(x, a) for Preferences in this case,
see (Wiering and Van Hasselt, 2009)) while it also
keeps track of V (x). The updates are then based
on the difference of P(x, a) and V (x) which,
from our results, seems to make this algorithm be-
have better in a dynamic environment. AC-QV
also cannot cope with uncertainty very well on
this problem.
IAC is an actor - critic algorithm that fol-
lows the gradient of cumulative discounted re-
wards VJ&amp;quot;. It always performs slightly worse
than NAC but in a consistent way, except in the
experiments with noise. It only requires approx-
imately 4 episodes to converge after a change
but cannot handle noise as well as other algo-
rithms. This can be in part explained by the
policy gradient theorem (Sutton et al., 2000) ac-
cording to which changes in the policy do not
</bodyText>
<page confidence="0.995084">
29
</page>
<bodyText confidence="0.999968674418605">
affect the distribution of state the system visits
(IAC and NAC perform gradient ascend in the
space of policies rather than in parameter space
(Szepesv´ari, 2010)). Policy gradient methods in
general seem to converge rapidly, as supported by
results of Sutton et al. (2000) or Konda and Tsit-
siklis (2001) for example.
NAC , as expected, performs better than any
other algorithm in all settings. It not only con-
verges in very few episodes but is also very robust
to noise and changes in the environment. Follow-
ing the natural gradient has proven to be much
more efficient than simply using the gradient of
the expected rewards. There are many positive
examples of NAC performance (or following the
natural gradient in general), such as (Bagnell and
Schneider, 2003; Peters et al., 2005) and this work
is one of them.
Dyna Algorithms except for Dyna
SARSA(A), seem to perform worse than av-
erage on the deterministic problem. In the
presence of changes, none of them seems to
perform very well. These algorithms use a
model of the environment to update Q(x, a) or
P(x, a), meaning that after each interaction with
the environment they perform several iterations
using simulated triplets (x, a, r). In the presence
of changes this results in obsolete information
being reused again and again until sufficient real
interactions with the environment occur and the
model is updated as well. This is possibly the
main reason why each Dyna algorithm requires
more episodes after the change than its corre-
sponding learning algorithm. Dyna Q Learning
only updates a single entry of Q(x, a) at each
simulated iteration, which could explain why
noise does not corrupt Q(x, a) too much and
why this algorithm performs well in the presence
of uncertainty. Noise in this case is added at a
single entry of Q(x, a), rather than to the whole
matrix, at each iteration. Dyna SARSA(A) and
Dyna Q(A) handle noise slightly better than Dyna
AC-QV.
</bodyText>
<sectionHeader confidence="0.991624" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999978472727273">
NAC proved to be the best algorithm in our eval-
uation. It is, however, much more complex to im-
plement and run and thus each episode takes more
(absolute) time to complete. One might suggest
then that a lighter algorithm such as SARSA(A)
will have the opportunity to run more iterations
in the same absolute time. One should definitely
take this into account when designing a real world
system, when timely responses are necessary and
resources are limited as, for example, in a mobile
system. Note that SARSA(A), Q-Learning, Q(A)
and AC-QV are significantly faster than the rest
algorithms.
On the other hand, all algorithms except for
NAC, IAC and LS-SARSA have the major draw-
back of the size of the table representing Q(x, a)
or P(x, a) that is needed to store state-action val-
ues. This is a disadvantage that practically pro-
hibits the use of these algorithms in high dimen-
sional or continuous problems. Function approxi-
mation might alleviate this problem, according to
Bertsekas (2007), if we reformulate the problem
and reduce control space while increasing state
space. In such a setting function approximation
performs well, while in general it cannot deal with
large control spaces. It becomes very expensive
as computation cost grows exponentially on the
size of the lookahead horizon. Also, according to
Sutton and Barto (1998) and Sutton et al. (2000),
better convergence guarantees exist for online al-
gorithms when combined with function approx-
imation or for policy gradient methods (such as
IAC or NAC) in general. Finally, one must take
great care when selecting features to approximate
Q(x, a) or V (x) as they are important to con-
vergence and speed of the algorithm (Allen and
Fritzsche, 2011; Bertsekas, 2007).
To summarize, NAC outperforms the other al-
gorithms in every experiment we conducted. It
does require a lot of computational power though
and might not be suitable if it is limited. On
the other hand, SARSA(A) or Q Learning per-
form well enough while requiring less computa-
tional power but a lot more memory space. The
researcher / developer then must make his / her
choice between them taking into account such
practical limitations.
As future work we plan to implement these al-
gorithms on the Olympus / RavenClaw (Bohus
and Rudnicky, 2009) platform, using the results
of this work as a guide. Our aim will be to cre-
ate a hybrid state of the art ADS that will com-
bine advantages of existing state of the art tech-
niques. Moreover we plan to install our system
on a robotic platform and conduct real user trials.
</bodyText>
<page confidence="0.997504">
30
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992769642857">
Allen, M., Fritzsche, P., 2011, Reinforcement Learn-
ing with Adaptive Kanerva Encoding for Xpilot
Game AI, Annual Congress on Evolutionary Com-
putation, pp 1521–1528.
Atkeson, C.G., Santamaria, J.C., 1997, A comparison
of direct and model-based reinforcement learning,
IEEE Robotics and Automation, pp 3557–3564.
Bagnell, J., Schneider, J., 2003, Covariant pol-
icy search, Proceedings of the Eighteenth Interna-
tional Joint Conference on Artificial Intelligence, pp
1019–1024.
Bertsekas D.P., 2007, Dynamic Programming and
Optimal Control, Athena Scientific, vol 2, 3rd edi-
tion.
Bhatnagar, S, Sutton, R.S., Ghavamzadeh, M., Lee,
M. 2007, Incremental Natural Actor-Critic Algo-
rithms, Neural Information Processing Systems, pp
105–112.
Bohus, D., Rudnicky, A.I., 2009, The RavenClaw di-
alog management framework: Architecture and sys-
tems, Computer Speech &amp; Language, vol 23:3, pp
332-361.
Boidin, C., Rieser, V., Van Der Plas, L., Lemon, O.,
and Chevelu, J. 2009, Predicting how it sounds:
Re-ranking dialogue prompts based on TTS qual-
ity for adaptive Spoken Dialogue Systems, Pro-
ceedings of the Interspeech Special Session Ma-
chine Learning for Adaptivity in Spoken Dialogue,
pp 2487–2490.
Chen, S-L., Wei, Y-M. 2008, Least-Squares
SARSA(Lambda) Algorithms for Reinforcement
Learning, Natural Computation, 2008. ICNC ’08,
vol.2, pp 632–636.
Cuay´ahuitl, H., Renals, S., Lemon, O., Shimodaira,
H. 2010, Evaluation of a hierarchical reinforce-
ment learning spoken dialogue system, Computer
Speech &amp; Language, Academic Press Ltd., vol 24:2,
pp 395–429.
Ga&amp;quot;si´c, M., Jur&amp;quot;c´ı&amp;quot;cek, F., Keizer, S., Mairesse, F.
and Thomson, B., Yu, K. and Young, S, 2010,
Gaussian processes for fast policy optimisation of
POMDP-based dialogue managers, Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pp 201–204.
Geist, M., Pietquin, O., 2010, Kalman temporal
differences, Journal of Artificial Intelligence Re-
search, vol 39:1, pp 483–532.
Janarthanam, S., Lemon, O. 2009, A Two-Tier User
Simulation Model for Reinforcement Learning of
Adaptive Referring Expression Generation Policies,
SIGDIAL Conference’09, pp 120–123.
Jur&amp;quot;c´ı&amp;quot;cek, F., Thomson, B., Keizer, S., Mairesse, F.,
Ga&amp;quot;si´c, M., Yu, K., Young, S 2010, Natural Belief-
Critic: A Reinforcement Algorithm for Parameter
Estimation in Statistical Spoken Dialogue Systems,
International Speech Communication Association,
vol 7, pp 1–26.
Konda, V.R., Tsitsiklis, J.N., 2001, Actor-Critic Al-
gorithms, SIAM Journal on Control and Optimiza-
tion, MIT Press, pp 1008–1014.
Konstantopoulos S., 2010, An Embodied Dialogue
System with Personality and Emotions, Proceedings
of the 2010 Workshop on Companionable Dialogue
Systems, ACL 2010, pp 3136.
Papangelis, A., Karkaletsis, V., Makedon, F., 2012,
Evaluation of Online Dialogue Policy Learning
Techniques, Proceedings of the 8th Conference on
Language Resources and Evaluation (LREC) 2012,
to appear.
Peng, J., Williams, R., 1996, Incremental multi-step
Q-Learning, Machine Learning pp 283–290.
Peters, J., Vijayakumar, S., Schaal, S. 2005, Natural
actor-critic , Machine Learning: ECML 2005, pp
280–291.
Pietquin, O., Hastie H. 2011, A survey on metrics for
the evaluation of user simulations, The Knowledge
Engineering Review, Cambridge University Press
(to appear).
Rieser, V., Lemon, O. 2009, Natural Language Gen-
eration as Planning Under Uncertainty for Spoken
Dialogue Systems, Proceedings of the 12th Confer-
ence of the European Chapter of the ACL (EACL
2009), pp 683–691.
Ross, S., Pineau, J., Paquet, S., Chaib-draa, B., 2008,
Online planning algorithms for POMDPs, Journal
of Artificial Intelligence Research, pp 663–704.
Sutton R.S., Barto, A.G., 1998, Reinforcement Learn-
ing: An Introduction, The MIT Press, Cambridge,
MA.
Sutton, R.S.,Mcallester, D., Singh, S., Mansour, Y.
2000, Policy gradient methods for reinforcement
learning with function approximation, In Advances
in Neural Information Processing Systems 12, pp
1057–1063.
Szepesv´ari, C., 2010, Algorithms for Reinforcement
Learning, Morgan &amp; Claypool Publishers, Synthe-
sis Lectures on Artificial Intelligence and Machine
Learning, vol 4:1, pp 1–103.
Watkins C.J.C.H., 1989, Learning from delayed re-
wards, PhD Thesis, University of Cambridge, Eng-
land.
Wiering, M. A, Van Hasselt, H. 2009, The QV
family compared to other reinforcement learning
algorithms, IEEE Symposium on Adaptive Dy-
namic Programming and Reinforcement Learning,
pp 101–108.
Young S., Ga&amp;quot;si´c, M., Keizer S., Mairesse, F., Schatz-
mann J., Thomson, B., Yu, K., 2010, The Hid-
den Information State model: A practical frame-
work for POMDP-based spoken dialogue manage-
ment, Computer Speech &amp; Language, vol 24:2, pp
150–174.
</reference>
<page confidence="0.999913">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.458851">
<title confidence="0.994081">A Comparative Study of Reinforcement Learning Techniques Dialogue Management</title>
<author confidence="0.677249">Alexandros</author>
<affiliation confidence="0.9392086">NCSR Institute of &amp; Univ. of Texas at Comp. Science and</affiliation>
<email confidence="0.995013">alexandros.papangelis@mavs.uta.edu</email>
<abstract confidence="0.988765857142857">Adaptive Dialogue Systems are rapidly becoming part of our everyday lives. As they progress and adopt new technologies they become more intelligent and able to adapt better and faster to their environment. Research in this field is currently focused on how to achieve adaptation, and particularly on applying Reinforcement Learning (RL) techniques, so a comparative study of the related methods, such as this, is necessary. In this work we compare several standard and state of the art online RL algorithms that are used to train the dialogue manager in a dynamic environment, aiming to aid researchers / developers choose the appropriate RL algorithm for their system. This is the first work, to the best of our knowledge, to evaluate online RL algorithms on the dialogue problem and in a dynamic environment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Allen</author>
<author>P Fritzsche</author>
</authors>
<title>Reinforcement Learning with Adaptive Kanerva Encoding for Xpilot Game AI, Annual Congress on Evolutionary Computation,</title>
<date>2011</date>
<pages>1521--1528</pages>
<contexts>
<context position="35365" citStr="Allen and Fritzsche, 2011" startWordPosition="6240" endWordPosition="6243"> a setting function approximation performs well, while in general it cannot deal with large control spaces. It becomes very expensive as computation cost grows exponentially on the size of the lookahead horizon. Also, according to Sutton and Barto (1998) and Sutton et al. (2000), better convergence guarantees exist for online algorithms when combined with function approximation or for policy gradient methods (such as IAC or NAC) in general. Finally, one must take great care when selecting features to approximate Q(x, a) or V (x) as they are important to convergence and speed of the algorithm (Allen and Fritzsche, 2011; Bertsekas, 2007). To summarize, NAC outperforms the other algorithms in every experiment we conducted. It does require a lot of computational power though and might not be suitable if it is limited. On the other hand, SARSA(A) or Q Learning perform well enough while requiring less computational power but a lot more memory space. The researcher / developer then must make his / her choice between them taking into account such practical limitations. As future work we plan to implement these algorithms on the Olympus / RavenClaw (Bohus and Rudnicky, 2009) platform, using the results of this work</context>
</contexts>
<marker>Allen, Fritzsche, 2011</marker>
<rawString>Allen, M., Fritzsche, P., 2011, Reinforcement Learning with Adaptive Kanerva Encoding for Xpilot Game AI, Annual Congress on Evolutionary Computation, pp 1521–1528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G Atkeson</author>
<author>J C Santamaria</author>
</authors>
<title>A comparison of direct and model-based reinforcement learning,</title>
<date>1997</date>
<journal>IEEE Robotics and Automation,</journal>
<pages>3557--3564</pages>
<contexts>
<context position="9629" citStr="Atkeson and Santamaria (1997)" startWordPosition="1582" endWordPosition="1585">rithm Model Policy Iteration SARSA(A) No On Value LS-SARSA(A) No On Policy Q Learning No Off Value Q(A) No Off Value Actor Critic - QV No On Policy IAC No On Policy NAC No On Policy DynaSARSA(A) Yes On Value DynaQ Yes Off Value DynaQ(A) Yes Off Value DynaAC-QV Yes On Policy Table 1: Online RL algorithms used in our evaluation. While there is a significant amount of work in evaluating RL algorithms, this is the first attempt, to the best of our knowledge, to evaluate online learning RL algorithms on the dialogue management problem, in the presence of uncertainty and changes in the environment. Atkeson and Santamaria (1997) evaluate model based and model free algorithms on the single pendulum swingup problem but their algorithms are not the ones we have selected and the problem on which they were evaluated differs from ours in many ways. Ross et al. (2008) compare many online planning algorithms for solving Partially Observable Markov Decision Processes (POMDP). It is a comprehensive study but not directly related to ours, as we model our problem with Markov Decision Processes (MDP) and evaluate model-based and model-free algorithms on a specific task. In the next section we provide some background knowledge on </context>
</contexts>
<marker>Atkeson, Santamaria, 1997</marker>
<rawString>Atkeson, C.G., Santamaria, J.C., 1997, A comparison of direct and model-based reinforcement learning, IEEE Robotics and Automation, pp 3557–3564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bagnell</author>
<author>J Schneider</author>
</authors>
<title>Covariant policy search,</title>
<date>2003</date>
<booktitle>Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1019--1024</pages>
<contexts>
<context position="32475" citStr="Bagnell and Schneider, 2003" startWordPosition="5755" endWordPosition="5758"> space (Szepesv´ari, 2010)). Policy gradient methods in general seem to converge rapidly, as supported by results of Sutton et al. (2000) or Konda and Tsitsiklis (2001) for example. NAC , as expected, performs better than any other algorithm in all settings. It not only converges in very few episodes but is also very robust to noise and changes in the environment. Following the natural gradient has proven to be much more efficient than simply using the gradient of the expected rewards. There are many positive examples of NAC performance (or following the natural gradient in general), such as (Bagnell and Schneider, 2003; Peters et al., 2005) and this work is one of them. Dyna Algorithms except for Dyna SARSA(A), seem to perform worse than average on the deterministic problem. In the presence of changes, none of them seems to perform very well. These algorithms use a model of the environment to update Q(x, a) or P(x, a), meaning that after each interaction with the environment they perform several iterations using simulated triplets (x, a, r). In the presence of changes this results in obsolete information being reused again and again until sufficient real interactions with the environment occur and the model</context>
</contexts>
<marker>Bagnell, Schneider, 2003</marker>
<rawString>Bagnell, J., Schneider, J., 2003, Covariant policy search, Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, pp 1019–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D P Bertsekas</author>
</authors>
<title>Dynamic Programming and Optimal Control, Athena Scientific, vol 2, 3rd edition.</title>
<date>2007</date>
<contexts>
<context position="34646" citStr="Bertsekas (2007)" startWordPosition="6125" endWordPosition="6126"> a real world system, when timely responses are necessary and resources are limited as, for example, in a mobile system. Note that SARSA(A), Q-Learning, Q(A) and AC-QV are significantly faster than the rest algorithms. On the other hand, all algorithms except for NAC, IAC and LS-SARSA have the major drawback of the size of the table representing Q(x, a) or P(x, a) that is needed to store state-action values. This is a disadvantage that practically prohibits the use of these algorithms in high dimensional or continuous problems. Function approximation might alleviate this problem, according to Bertsekas (2007), if we reformulate the problem and reduce control space while increasing state space. In such a setting function approximation performs well, while in general it cannot deal with large control spaces. It becomes very expensive as computation cost grows exponentially on the size of the lookahead horizon. Also, according to Sutton and Barto (1998) and Sutton et al. (2000), better convergence guarantees exist for online algorithms when combined with function approximation or for policy gradient methods (such as IAC or NAC) in general. Finally, one must take great care when selecting features to </context>
</contexts>
<marker>Bertsekas, 2007</marker>
<rawString>Bertsekas D.P., 2007, Dynamic Programming and Optimal Control, Athena Scientific, vol 2, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bhatnagar</author>
<author>R S Sutton</author>
<author>M Ghavamzadeh</author>
<author>M Lee</author>
</authors>
<title>Incremental Natural Actor-Critic Algorithms, Neural Information Processing Systems,</title>
<date>2007</date>
<pages>105--112</pages>
<contexts>
<context position="8081" citStr="Bhatnagar et al., 2007" startWordPosition="1333" endWordPosition="1336">nd Barto, 1998) to implement model based algorithms. SARSA(A) (Sutton and Barto, 1998), Q Learning (Watkins, 1989), Q(A) (Watkins, 1989; Peng and Williams, 1996) and AC-QV (Wiering and Van Hasselt, 2009) are well established RL algorithms, proven to work and simple to implement. A serious disadvantage though is the fact that they do not scale well (assuming we have 23 enough memory), as also supported by our results in section 5. Least Squares SARSA(A) (Chen and Wei, 2008) is a variation of SARSA(A) that uses the least squares method to find the optimal policy. Incremental Actor Critic (IAC) (Bhatnagar et al., 2007) and Natural Actor Critic (NAC) (Peters et al., 2005) are actor - critic algorithms that follow the expected rewards gradient and the natural or Fisher Information gradient respectively (Szepesv´ari, 2010). An important attribute of many learning algorithms is function approximation which allows them to scale to real world problems. Function approximation attempts to approximate a target function by selecting from a class of functions that closely resembles the target. Care must be taken however, when applying this method, because many RL algorithms are not guaranteed to converge when using fu</context>
</contexts>
<marker>Bhatnagar, Sutton, Ghavamzadeh, Lee, 2007</marker>
<rawString>Bhatnagar, S, Sutton, R.S., Ghavamzadeh, M., Lee, M. 2007, Incremental Natural Actor-Critic Algorithms, Neural Information Processing Systems, pp 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A I Rudnicky</author>
</authors>
<title>The RavenClaw dialog management framework: Architecture and systems,</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>23</volume>
<pages>332--361</pages>
<marker>Bohus, Rudnicky, 2009</marker>
<rawString>Bohus, D., Rudnicky, A.I., 2009, The RavenClaw dialog management framework: Architecture and systems, Computer Speech &amp; Language, vol 23:3, pp 332-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Boidin</author>
<author>V Rieser</author>
<author>L Van Der Plas</author>
<author>O Lemon</author>
<author>J Chevelu</author>
</authors>
<title>Predicting how it sounds: Re-ranking dialogue prompts based on TTS quality for adaptive Spoken Dialogue Systems,</title>
<date>2009</date>
<booktitle>Proceedings of the Interspeech Special Session Machine Learning for Adaptivity in Spoken Dialogue,</booktitle>
<pages>2487--2490</pages>
<marker>Boidin, Rieser, Van Der Plas, Lemon, Chevelu, 2009</marker>
<rawString>Boidin, C., Rieser, V., Van Der Plas, L., Lemon, O., and Chevelu, J. 2009, Predicting how it sounds: Re-ranking dialogue prompts based on TTS quality for adaptive Spoken Dialogue Systems, Proceedings of the Interspeech Special Session Machine Learning for Adaptivity in Spoken Dialogue, pp 2487–2490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-L Chen</author>
<author>Y-M Wei</author>
</authors>
<date>2008</date>
<journal>ICNC</journal>
<booktitle>Least-Squares SARSA(Lambda) Algorithms for Reinforcement Learning, Natural Computation,</booktitle>
<volume>08</volume>
<pages>632--636</pages>
<contexts>
<context position="7935" citStr="Chen and Wei, 2008" startWordPosition="1308" endWordPosition="1311">ted along with some of their characteristics. We selected representative algorithms for each category and used the Dyna architecture (Sutton and Barto, 1998) to implement model based algorithms. SARSA(A) (Sutton and Barto, 1998), Q Learning (Watkins, 1989), Q(A) (Watkins, 1989; Peng and Williams, 1996) and AC-QV (Wiering and Van Hasselt, 2009) are well established RL algorithms, proven to work and simple to implement. A serious disadvantage though is the fact that they do not scale well (assuming we have 23 enough memory), as also supported by our results in section 5. Least Squares SARSA(A) (Chen and Wei, 2008) is a variation of SARSA(A) that uses the least squares method to find the optimal policy. Incremental Actor Critic (IAC) (Bhatnagar et al., 2007) and Natural Actor Critic (NAC) (Peters et al., 2005) are actor - critic algorithms that follow the expected rewards gradient and the natural or Fisher Information gradient respectively (Szepesv´ari, 2010). An important attribute of many learning algorithms is function approximation which allows them to scale to real world problems. Function approximation attempts to approximate a target function by selecting from a class of functions that closely re</context>
</contexts>
<marker>Chen, Wei, 2008</marker>
<rawString>Chen, S-L., Wei, Y-M. 2008, Least-Squares SARSA(Lambda) Algorithms for Reinforcement Learning, Natural Computation, 2008. ICNC ’08, vol.2, pp 632–636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cuay´ahuitl</author>
<author>S Renals</author>
<author>O Lemon</author>
<author>H Shimodaira</author>
</authors>
<title>Evaluation of a hierarchical reinforcement learning spoken dialogue system,</title>
<date>2010</date>
<journal>Computer Speech &amp; Language, Academic Press Ltd.,</journal>
<volume>24</volume>
<pages>395--429</pages>
<marker>Cuay´ahuitl, Renals, Lemon, Shimodaira, 2010</marker>
<rawString>Cuay´ahuitl, H., Renals, S., Lemon, O., Shimodaira, H. 2010, Evaluation of a hierarchical reinforcement learning spoken dialogue system, Computer Speech &amp; Language, Academic Press Ltd., vol 24:2, pp 395–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gasi´c</author>
<author>F Jurc´ıcek</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>B Thomson</author>
<author>K Yu</author>
<author>S Young</author>
</authors>
<title>Gaussian processes for fast policy optimisation of POMDP-based dialogue managers,</title>
<date>2010</date>
<booktitle>Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>201--204</pages>
<marker>Gasi´c, Jurc´ıcek, Keizer, Mairesse, Thomson, Yu, Young, 2010</marker>
<rawString>Ga&amp;quot;si´c, M., Jur&amp;quot;c´ı&amp;quot;cek, F., Keizer, S., Mairesse, F. and Thomson, B., Yu, K. and Young, S, 2010, Gaussian processes for fast policy optimisation of POMDP-based dialogue managers, Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pp 201–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geist</author>
<author>O Pietquin</author>
</authors>
<title>Kalman temporal differences,</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>39</volume>
<pages>483--532</pages>
<marker>Geist, Pietquin, 2010</marker>
<rawString>Geist, M., Pietquin, O., 2010, Kalman temporal differences, Journal of Artificial Intelligence Research, vol 39:1, pp 483–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>A Two-Tier User Simulation Model for Reinforcement Learning of Adaptive Referring Expression Generation Policies, SIGDIAL Conference’09,</title>
<date>2009</date>
<pages>120--123</pages>
<contexts>
<context position="4660" citStr="Janarthanam and Lemon, 2009" startWordPosition="754" endWordPosition="757">textual form of the output and last, the Text To Speech (TTS) component to convert the text to spoken output. Trying to make ADS as human-like as possible researchers have focused on techniques that achieve adaptation, i.e. adjust to the current user’s personality, behaviour, mood, needs and to the environment in general. Examples include adaptive or trainable NLG (Rieser and Lemon, 2009), where the authors formulate their problem as a statistical planning problem and use RL to find a policy according to which the system will decide how to present information. Another example is adaptive REG (Janarthanam and Lemon, 2009), where the authors again use RL to choose one of three strategies (jargon, tutorial, descriptive) according to the user’s expertise level. An example of adaptive TTS is the work of Boidin et al. (2009), where the authors propose a model that sorts paraphrases with respect to predictions of which sounds more natural. Jurˇciˇcek et al. (2010) propose a RL algorithm to optimize ADS parameters in general. Last, many researchers have used RL to achieve adaptive Dialogue Management (Pietquin and Hastie, 2011; Gaˇsi´c et al., 2010; Cuay´ahuitl et al., 2010). As the reader may have noticed, the curre</context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>Janarthanam, S., Lemon, O. 2009, A Two-Tier User Simulation Model for Reinforcement Learning of Adaptive Referring Expression Generation Policies, SIGDIAL Conference’09, pp 120–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jurc´ıcek</author>
<author>B Thomson</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>M Gasi´c</author>
<author>K Yu</author>
<author>S Young</author>
</authors>
<title>Natural BeliefCritic: A Reinforcement Algorithm for Parameter Estimation in Statistical Spoken Dialogue Systems,</title>
<date>2010</date>
<journal>International Speech Communication Association,</journal>
<volume>7</volume>
<pages>1--26</pages>
<marker>Jurc´ıcek, Thomson, Keizer, Mairesse, Gasi´c, Yu, Young, 2010</marker>
<rawString>Jur&amp;quot;c´ı&amp;quot;cek, F., Thomson, B., Keizer, S., Mairesse, F., Ga&amp;quot;si´c, M., Yu, K., Young, S 2010, Natural BeliefCritic: A Reinforcement Algorithm for Parameter Estimation in Statistical Spoken Dialogue Systems, International Speech Communication Association, vol 7, pp 1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V R Konda</author>
<author>J N Tsitsiklis</author>
</authors>
<title>Actor-Critic Algorithms,</title>
<date>2001</date>
<journal>SIAM Journal on Control and Optimization,</journal>
<pages>1008--1014</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="32016" citStr="Konda and Tsitsiklis (2001)" startWordPosition="5676" endWordPosition="5680">AC but in a consistent way, except in the experiments with noise. It only requires approximately 4 episodes to converge after a change but cannot handle noise as well as other algorithms. This can be in part explained by the policy gradient theorem (Sutton et al., 2000) according to which changes in the policy do not 29 affect the distribution of state the system visits (IAC and NAC perform gradient ascend in the space of policies rather than in parameter space (Szepesv´ari, 2010)). Policy gradient methods in general seem to converge rapidly, as supported by results of Sutton et al. (2000) or Konda and Tsitsiklis (2001) for example. NAC , as expected, performs better than any other algorithm in all settings. It not only converges in very few episodes but is also very robust to noise and changes in the environment. Following the natural gradient has proven to be much more efficient than simply using the gradient of the expected rewards. There are many positive examples of NAC performance (or following the natural gradient in general), such as (Bagnell and Schneider, 2003; Peters et al., 2005) and this work is one of them. Dyna Algorithms except for Dyna SARSA(A), seem to perform worse than average on the dete</context>
</contexts>
<marker>Konda, Tsitsiklis, 2001</marker>
<rawString>Konda, V.R., Tsitsiklis, J.N., 2001, Actor-Critic Algorithms, SIAM Journal on Control and Optimization, MIT Press, pp 1008–1014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Konstantopoulos</author>
</authors>
<title>An Embodied Dialogue System with Personality and Emotions,</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL</booktitle>
<pages>3136</pages>
<contexts>
<context position="2618" citStr="Konstantopoulos (2010)" startWordPosition="416" endWordPosition="417">ion and learning, such as the one presented by Young et al., (2010), where the authors propose an ADS that provides tourist information in a fictitious town. Their system is trained using RL and some clever state compression techniques to make it scalable, it is robust to noise and able to recover from errors (misunderstandings). Cuay´ahuitl et al. (2010) propose a travel planning ADS, that is able to learn dialogue policies using RL, building on top of existing handcrafted policies. This enables the designers of the system to provide prior knowledge and the system can then learn the details. Konstantopoulos (2010) proposes an affective ADS which serves as a museum guide. It is able to adapt to each user’s personality by assessing his / her emotional state and current mood and also adapt its output to the user’s expertise level. The system itself has an emotional state that is affected by the user and affects its output. An example ADS architecture is depicted in Figure 1, where we can see several components trying to understand the user’s utterance and several others trying to express the system’s response. The system first attempts to convert spoken input to text using the Automatic Speech Recognition</context>
</contexts>
<marker>Konstantopoulos, 2010</marker>
<rawString>Konstantopoulos S., 2010, An Embodied Dialogue System with Personality and Emotions, Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pp 3136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Papangelis</author>
<author>V Karkaletsis</author>
<author>F Makedon</author>
</authors>
<date>2012</date>
<booktitle>Evaluation of Online Dialogue Policy Learning Techniques, Proceedings of the 8th Conference on Language Resources and Evaluation (LREC)</booktitle>
<note>to appear.</note>
<contexts>
<context position="14088" citStr="Papangelis et al., 2012" startWordPosition="2407" endWordPosition="2410">(xt)) (2) t=0 A policy π is optimal if Jπ(x) = V π(x), ∀x ∈ X. We can also define the action-value function Q: γtRt+1|x0 = x, a0 = α] (3) where x ∈ X, α ∈ A, which gives us the expected cumulative discounted rewards when beginning from state x and taking action α, again following policy π. Note that Vmax = rax 1�γ , where R(x) ∈ [rmin, rmax]. The goal of RL therefore is to find the optimal policy, which maximizes either of these functions (Szepesv´ari, 2010). 3 Slot Filling Problem We formulated the problem as a generic slot filling ADS, represented as an MDP. This model has been proposed in (Papangelis et al., 2012), and we extend it here to account for uncertainty. Formally the problem is defined as: S =&lt; s0, ..., sN &gt;∈ M,M = M0 ×M1 ×...×MN,Mi = {1,...,Ti}, where S are the N slots to be filled, each slot si can take values from Mi and Ti is the number of available values slot si can be filled with. Dialogue state is also defined as a vector d ∈ M, where each dimension corresponds to a slot and its value corresponds to the slot’s value. We call the set of all possible dialogue states D. System actions A ∈ {1, ..., |S|} are defined as requests for slots to be filled and ai requests slot si. At each dialog</context>
</contexts>
<marker>Papangelis, Karkaletsis, Makedon, 2012</marker>
<rawString>Papangelis, A., Karkaletsis, V., Makedon, F., 2012, Evaluation of Online Dialogue Policy Learning Techniques, Proceedings of the 8th Conference on Language Resources and Evaluation (LREC) 2012, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peng</author>
<author>R Williams</author>
</authors>
<title>Incremental multi-step Q-Learning,</title>
<date>1996</date>
<journal>Machine Learning</journal>
<pages>283--290</pages>
<contexts>
<context position="7619" citStr="Peng and Williams, 1996" startWordPosition="1252" endWordPosition="1255"> the algorithm follows the policy that it is trying to learn. This will become clear in section 2.2 where we provide the basics of RL. Last, these algorithms can be categorized as policy iteration or value iteration algorithms, according to the way they evaluate and train a policy. Table 1 shows the algorithms we evaluated along with some of their characteristics. We selected representative algorithms for each category and used the Dyna architecture (Sutton and Barto, 1998) to implement model based algorithms. SARSA(A) (Sutton and Barto, 1998), Q Learning (Watkins, 1989), Q(A) (Watkins, 1989; Peng and Williams, 1996) and AC-QV (Wiering and Van Hasselt, 2009) are well established RL algorithms, proven to work and simple to implement. A serious disadvantage though is the fact that they do not scale well (assuming we have 23 enough memory), as also supported by our results in section 5. Least Squares SARSA(A) (Chen and Wei, 2008) is a variation of SARSA(A) that uses the least squares method to find the optimal policy. Incremental Actor Critic (IAC) (Bhatnagar et al., 2007) and Natural Actor Critic (NAC) (Peters et al., 2005) are actor - critic algorithms that follow the expected rewards gradient and the natu</context>
</contexts>
<marker>Peng, Williams, 1996</marker>
<rawString>Peng, J., Williams, R., 1996, Incremental multi-step Q-Learning, Machine Learning pp 283–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peters</author>
<author>S Vijayakumar</author>
<author>S Schaal</author>
</authors>
<date>2005</date>
<booktitle>Natural actor-critic , Machine Learning: ECML</booktitle>
<pages>280--291</pages>
<contexts>
<context position="8134" citStr="Peters et al., 2005" startWordPosition="1342" endWordPosition="1346">SA(A) (Sutton and Barto, 1998), Q Learning (Watkins, 1989), Q(A) (Watkins, 1989; Peng and Williams, 1996) and AC-QV (Wiering and Van Hasselt, 2009) are well established RL algorithms, proven to work and simple to implement. A serious disadvantage though is the fact that they do not scale well (assuming we have 23 enough memory), as also supported by our results in section 5. Least Squares SARSA(A) (Chen and Wei, 2008) is a variation of SARSA(A) that uses the least squares method to find the optimal policy. Incremental Actor Critic (IAC) (Bhatnagar et al., 2007) and Natural Actor Critic (NAC) (Peters et al., 2005) are actor - critic algorithms that follow the expected rewards gradient and the natural or Fisher Information gradient respectively (Szepesv´ari, 2010). An important attribute of many learning algorithms is function approximation which allows them to scale to real world problems. Function approximation attempts to approximate a target function by selecting from a class of functions that closely resembles the target. Care must be taken however, when applying this method, because many RL algorithms are not guaranteed to converge when using function approximation. On the other hand, policy gradi</context>
<context position="32497" citStr="Peters et al., 2005" startWordPosition="5759" endWordPosition="5762">Policy gradient methods in general seem to converge rapidly, as supported by results of Sutton et al. (2000) or Konda and Tsitsiklis (2001) for example. NAC , as expected, performs better than any other algorithm in all settings. It not only converges in very few episodes but is also very robust to noise and changes in the environment. Following the natural gradient has proven to be much more efficient than simply using the gradient of the expected rewards. There are many positive examples of NAC performance (or following the natural gradient in general), such as (Bagnell and Schneider, 2003; Peters et al., 2005) and this work is one of them. Dyna Algorithms except for Dyna SARSA(A), seem to perform worse than average on the deterministic problem. In the presence of changes, none of them seems to perform very well. These algorithms use a model of the environment to update Q(x, a) or P(x, a), meaning that after each interaction with the environment they perform several iterations using simulated triplets (x, a, r). In the presence of changes this results in obsolete information being reused again and again until sufficient real interactions with the environment occur and the model is updated as well. T</context>
</contexts>
<marker>Peters, Vijayakumar, Schaal, 2005</marker>
<rawString>Peters, J., Vijayakumar, S., Schaal, S. 2005, Natural actor-critic , Machine Learning: ECML 2005, pp 280–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Pietquin</author>
<author>H Hastie</author>
</authors>
<title>A survey on metrics for the evaluation of user simulations, The Knowledge Engineering Review,</title>
<date>2011</date>
<publisher>University Press</publisher>
<location>Cambridge</location>
<note>(to appear).</note>
<contexts>
<context position="5168" citStr="Pietquin and Hastie, 2011" startWordPosition="837" endWordPosition="840">hich the system will decide how to present information. Another example is adaptive REG (Janarthanam and Lemon, 2009), where the authors again use RL to choose one of three strategies (jargon, tutorial, descriptive) according to the user’s expertise level. An example of adaptive TTS is the work of Boidin et al. (2009), where the authors propose a model that sorts paraphrases with respect to predictions of which sounds more natural. Jurˇciˇcek et al. (2010) propose a RL algorithm to optimize ADS parameters in general. Last, many researchers have used RL to achieve adaptive Dialogue Management (Pietquin and Hastie, 2011; Gaˇsi´c et al., 2010; Cuay´ahuitl et al., 2010). As the reader may have noticed, the current trend in training these components is the application of RL techniques. RL is a well established field of artificial intelligence and provides us with robust frameworks that are able to deal with uncertainty and can scale to real world problems. One sub category of RL is Online RL where the system can be trained on the fly, as it interacts with its environment. These techniques have recently begun to be applied to Dialogue Management and in this paper we perform an extensive evaluation of several sta</context>
<context position="21151" citStr="Pietquin and Hastie, 2011" startWordPosition="3738" endWordPosition="3741">hen the change occurs. This could, for example, model scenarios where hotel booking becomes unavailable or some airports are closed, in a travel planning ADS. Last, we evaluated each algorithm’s scalability, by running each for 100 times on various slot filling problems, beginning with a problem with 4 slots and 4 actions up to a problem with 8 slots and 8 actions. We measured the return averaged over the 100 runs each algorithm achieved. Despite many notable efforts, a standardized evaluation framework for ADS or DS is still considered an open question by the research community. The work in (Pietquin and Hastie, 2011) provides a very good survey of current techniques that evaluate several aspects of Dialogue Systems. When RL is applied, researchers typically use the reward function as a metric of performance. This will be our evaluation metric as well, since it is common across all algorithms. As defined in section 2.3, it penalizes attempts to answer the user’s query with incomplete information as well as lengthy dialogues. Algorithm Average Reward SARSA(A) -10.5967 LS-SARSA(A) -14.3439 Q Learning -14.8888 Q(A) -63.7588 Actor Critic - QV -15.9245 IAC -10.5000 NAC -5.8273 DynaSARSA(A) -11.9758 DynaQ -14.72</context>
</contexts>
<marker>Pietquin, Hastie, 2011</marker>
<rawString>Pietquin, O., Hastie H. 2011, A survey on metrics for the evaluation of user simulations, The Knowledge Engineering Review, Cambridge University Press (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems,</title>
<date>2009</date>
<booktitle>Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>683--691</pages>
<contexts>
<context position="4423" citStr="Rieser and Lemon, 2009" startWordPosition="714" endWordPosition="717">ore making a decision. After the system has decided what to say, it uses the Referring Expression Generation (REG) component to create appropriate referring expressions, the Natural Language Generation (NLG) component to create the textual form of the output and last, the Text To Speech (TTS) component to convert the text to spoken output. Trying to make ADS as human-like as possible researchers have focused on techniques that achieve adaptation, i.e. adjust to the current user’s personality, behaviour, mood, needs and to the environment in general. Examples include adaptive or trainable NLG (Rieser and Lemon, 2009), where the authors formulate their problem as a statistical planning problem and use RL to find a policy according to which the system will decide how to present information. Another example is adaptive REG (Janarthanam and Lemon, 2009), where the authors again use RL to choose one of three strategies (jargon, tutorial, descriptive) according to the user’s expertise level. An example of adaptive TTS is the work of Boidin et al. (2009), where the authors propose a model that sorts paraphrases with respect to predictions of which sounds more natural. Jurˇciˇcek et al. (2010) propose a RL algori</context>
</contexts>
<marker>Rieser, Lemon, 2009</marker>
<rawString>Rieser, V., Lemon, O. 2009, Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems, Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pp 683–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ross</author>
<author>J Pineau</author>
<author>S Paquet</author>
<author>B Chaib-draa</author>
</authors>
<title>Online planning algorithms for POMDPs,</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>663--704</pages>
<contexts>
<context position="9866" citStr="Ross et al. (2008)" startWordPosition="1624" endWordPosition="1627">ue DynaAC-QV Yes On Policy Table 1: Online RL algorithms used in our evaluation. While there is a significant amount of work in evaluating RL algorithms, this is the first attempt, to the best of our knowledge, to evaluate online learning RL algorithms on the dialogue management problem, in the presence of uncertainty and changes in the environment. Atkeson and Santamaria (1997) evaluate model based and model free algorithms on the single pendulum swingup problem but their algorithms are not the ones we have selected and the problem on which they were evaluated differs from ours in many ways. Ross et al. (2008) compare many online planning algorithms for solving Partially Observable Markov Decision Processes (POMDP). It is a comprehensive study but not directly related to ours, as we model our problem with Markov Decision Processes (MDP) and evaluate model-based and model-free algorithms on a specific task. In the next section we provide some background knowledge on MDPs and RL techniques, in section 3 we present our proposed formulation of the slot filling dialogue problem, in section 4 we describe our experimental setup and results, in section 5 we discuss those results and in section 6 we conclud</context>
</contexts>
<marker>Ross, Pineau, Paquet, Chaib-draa, 2008</marker>
<rawString>Ross, S., Pineau, J., Paquet, S., Chaib-draa, B., 2008, Online planning algorithms for POMDPs, Journal of Artificial Intelligence Research, pp 663–704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Sutton</author>
<author>A G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction,</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6606" citStr="Sutton and Barto, (1998)" startWordPosition="1082" endWordPosition="1085"> any given point. We were thus able to see how well each algorithm adapted to minor (noise / uncertainty) or major (change in user needs) changes in the environment. In general, RL algorithms fall in two categories, planning and learning algorithms. Planning or model-based algorithms use training examples from previous interactions with the environment as well as a model of the environment that simulates interactions. Learning or modelfree algorithms only use training examples from previous interactions with the environment and that is the main difference of these two categories, according to Sutton and Barto, (1998). The goal of an RL algorithm is to learn a good policy (or strategy) that dictates how the system should interact with the environment. An algorithm then can follow a specific policy (i.e. interact with the environment in a specific, maybe predefined, way) while searching for a good policy. This way of learning is called “off policy” learning. The opposite is “on policy” learning, when the algorithm follows the policy that it is trying to learn. This will become clear in section 2.2 where we provide the basics of RL. Last, these algorithms can be categorized as policy iteration or value itera</context>
<context position="34994" citStr="Sutton and Barto (1998)" startWordPosition="6177" endWordPosition="6180">enting Q(x, a) or P(x, a) that is needed to store state-action values. This is a disadvantage that practically prohibits the use of these algorithms in high dimensional or continuous problems. Function approximation might alleviate this problem, according to Bertsekas (2007), if we reformulate the problem and reduce control space while increasing state space. In such a setting function approximation performs well, while in general it cannot deal with large control spaces. It becomes very expensive as computation cost grows exponentially on the size of the lookahead horizon. Also, according to Sutton and Barto (1998) and Sutton et al. (2000), better convergence guarantees exist for online algorithms when combined with function approximation or for policy gradient methods (such as IAC or NAC) in general. Finally, one must take great care when selecting features to approximate Q(x, a) or V (x) as they are important to convergence and speed of the algorithm (Allen and Fritzsche, 2011; Bertsekas, 2007). To summarize, NAC outperforms the other algorithms in every experiment we conducted. It does require a lot of computational power though and might not be suitable if it is limited. On the other hand, SARSA(A) </context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Sutton R.S., Barto, A.G., 1998, Reinforcement Learning: An Introduction, The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Sutton</author>
<author>D Mcallester</author>
<author>S Singh</author>
<author>Y Mansour</author>
</authors>
<title>Policy gradient methods for reinforcement learning with function approximation,</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 12,</booktitle>
<pages>1057--1063</pages>
<contexts>
<context position="31659" citStr="Sutton et al., 2000" startWordPosition="5616" endWordPosition="5619">then based on the difference of P(x, a) and V (x) which, from our results, seems to make this algorithm behave better in a dynamic environment. AC-QV also cannot cope with uncertainty very well on this problem. IAC is an actor - critic algorithm that follows the gradient of cumulative discounted rewards VJ&amp;quot;. It always performs slightly worse than NAC but in a consistent way, except in the experiments with noise. It only requires approximately 4 episodes to converge after a change but cannot handle noise as well as other algorithms. This can be in part explained by the policy gradient theorem (Sutton et al., 2000) according to which changes in the policy do not 29 affect the distribution of state the system visits (IAC and NAC perform gradient ascend in the space of policies rather than in parameter space (Szepesv´ari, 2010)). Policy gradient methods in general seem to converge rapidly, as supported by results of Sutton et al. (2000) or Konda and Tsitsiklis (2001) for example. NAC , as expected, performs better than any other algorithm in all settings. It not only converges in very few episodes but is also very robust to noise and changes in the environment. Following the natural gradient has proven to</context>
<context position="35019" citStr="Sutton et al. (2000)" startWordPosition="6182" endWordPosition="6185">at is needed to store state-action values. This is a disadvantage that practically prohibits the use of these algorithms in high dimensional or continuous problems. Function approximation might alleviate this problem, according to Bertsekas (2007), if we reformulate the problem and reduce control space while increasing state space. In such a setting function approximation performs well, while in general it cannot deal with large control spaces. It becomes very expensive as computation cost grows exponentially on the size of the lookahead horizon. Also, according to Sutton and Barto (1998) and Sutton et al. (2000), better convergence guarantees exist for online algorithms when combined with function approximation or for policy gradient methods (such as IAC or NAC) in general. Finally, one must take great care when selecting features to approximate Q(x, a) or V (x) as they are important to convergence and speed of the algorithm (Allen and Fritzsche, 2011; Bertsekas, 2007). To summarize, NAC outperforms the other algorithms in every experiment we conducted. It does require a lot of computational power though and might not be suitable if it is limited. On the other hand, SARSA(A) or Q Learning perform wel</context>
</contexts>
<marker>Sutton, Mcallester, Singh, Mansour, 2000</marker>
<rawString>Sutton, R.S.,Mcallester, D., Singh, S., Mansour, Y. 2000, Policy gradient methods for reinforcement learning with function approximation, In Advances in Neural Information Processing Systems 12, pp 1057–1063.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Szepesv´ari</author>
</authors>
<title>Algorithms for Reinforcement Learning, Morgan</title>
<date>2010</date>
<booktitle>Claypool Publishers, Synthesis Lectures on Artificial Intelligence and Machine Learning,</booktitle>
<volume>4</volume>
<pages>1--103</pages>
<marker>Szepesv´ari, 2010</marker>
<rawString>Szepesv´ari, C., 2010, Algorithms for Reinforcement Learning, Morgan &amp; Claypool Publishers, Synthesis Lectures on Artificial Intelligence and Machine Learning, vol 4:1, pp 1–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J C H Watkins</author>
</authors>
<title>Learning from delayed rewards, PhD Thesis,</title>
<date>1989</date>
<institution>University of Cambridge,</institution>
<location>England.</location>
<contexts>
<context position="7572" citStr="Watkins, 1989" startWordPosition="1247" endWordPosition="1248">pposite is “on policy” learning, when the algorithm follows the policy that it is trying to learn. This will become clear in section 2.2 where we provide the basics of RL. Last, these algorithms can be categorized as policy iteration or value iteration algorithms, according to the way they evaluate and train a policy. Table 1 shows the algorithms we evaluated along with some of their characteristics. We selected representative algorithms for each category and used the Dyna architecture (Sutton and Barto, 1998) to implement model based algorithms. SARSA(A) (Sutton and Barto, 1998), Q Learning (Watkins, 1989), Q(A) (Watkins, 1989; Peng and Williams, 1996) and AC-QV (Wiering and Van Hasselt, 2009) are well established RL algorithms, proven to work and simple to implement. A serious disadvantage though is the fact that they do not scale well (assuming we have 23 enough memory), as also supported by our results in section 5. Least Squares SARSA(A) (Chen and Wei, 2008) is a variation of SARSA(A) that uses the least squares method to find the optimal policy. Incremental Actor Critic (IAC) (Bhatnagar et al., 2007) and Natural Actor Critic (NAC) (Peters et al., 2005) are actor - critic algorithms that fo</context>
</contexts>
<marker>Watkins, 1989</marker>
<rawString>Watkins C.J.C.H., 1989, Learning from delayed rewards, PhD Thesis, University of Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Wiering</author>
<author>H Van Hasselt</author>
</authors>
<title>The QV family compared to other reinforcement learning algorithms,</title>
<date>2009</date>
<booktitle>IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,</booktitle>
<pages>101--108</pages>
<marker>Wiering, Van Hasselt, 2009</marker>
<rawString>Wiering, M. A, Van Hasselt, H. 2009, The QV family compared to other reinforcement learning algorithms, IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, pp 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>M Gasi´c</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management,</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<pages>150--174</pages>
<marker>Young, Gasi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>Young S., Ga&amp;quot;si´c, M., Keizer S., Mairesse, F., Schatzmann J., Thomson, B., Yu, K., 2010, The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management, Computer Speech &amp; Language, vol 24:2, pp 150–174.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>