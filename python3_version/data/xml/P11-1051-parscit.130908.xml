<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001557">
<title confidence="0.993575">
Coherent Citation-Based Summarization of Scientific Papers
</title>
<author confidence="0.808542">
Amjad Abu-Jbara
</author>
<affiliation confidence="0.884617">
EECS Department
University of Michigan
</affiliation>
<address confidence="0.840744">
Ann Arbor, MI, USA
</address>
<email confidence="0.999029">
amjbara@umich.edu
</email>
<author confidence="0.93565">
Dragomir Radev
</author>
<affiliation confidence="0.970747666666667">
EECS Department and
School of Information
University of Michigan
</affiliation>
<address confidence="0.85575">
Ann Arbor, MI, USA
</address>
<email confidence="0.999413">
radev@umich.edu
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919888888889">
In citation-based summarization, text written
by several researchers is leveraged to identify
the important aspects of a target paper. Previ-
ous work on this problem focused almost ex-
clusively on its extraction aspect (i.e. selecting
a representative set of citation sentences that
highlight the contribution of the target paper).
Meanwhile, the fluency of the produced sum-
maries has been mostly ignored. For exam-
ple, diversity, readability, cohesion, and order-
ing of the sentences included in the summary
have not been thoroughly considered. This re-
sulted in noisy and confusing summaries. In
this work, we present an approach for produc-
ing readable and cohesive citation-based sum-
maries. Our experiments show that the pro-
posed approach outperforms several baselines
in terms of both extraction quality and fluency.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931557692308">
Scientific research is a cumulative activity. The
work of downstream researchers depends on access
to upstream discoveries. The footnotes, end notes,
or reference lists within research articles make this
accumulation possible. When a reference appears in
a scientific paper, it is often accompanied by a span
of text describing the work being cited.
We name the sentence that contains an explicit
reference to another paper citation sentence. Cita-
tion sentences usually highlight the most important
aspects of the cited paper such as the research prob-
lem it addresses, the method it proposes, the good
results it reports, and even its drawbacks and limita-
tions.
By aggregating all the citation sentences that cite
a paper, we have a rich source of information about
it. This information is valuable because human ex-
perts have put their efforts to read the paper and sum-
marize its important contributions.
One way to make use of these sentences is creat-
ing a summary of the target paper. This summary
is different from the abstract or a summary gener-
ated from the paper itself. While the abstract rep-
resents the author’s point of view, the citation sum-
mary is the summation of multiple scholars’ view-
points. The task of summarizing a scientific paper
using its set of citation sentences is called citation-
based summarization.
There has been previous work done on citation-
based summarization (Nanba et al., 2000; Elkiss et
al., 2008; Qazvinian and Radev, 2008; Mei and Zhai,
2008; Mohammad et al., 2009). Previous work fo-
cused on the extraction aspect; i.e. analyzing the
collection of citation sentences and selecting a rep-
resentative subset that covers the main aspects of the
paper. The cohesion and the readability of the pro-
duced summaries have been mostly ignored. This
resulted in noisy and confusing summaries.
In this work, we focus on the coherence and read-
ability aspects of the problem. Our approach pro-
duces citation-based summaries in three stages: pre-
processing, extraction, and postprocessing. Our ex-
periments show that our approach produces better
summaries than several baseline summarization sys-
tems.
The rest of this paper is organized as follows. Af-
ter we examine previous work in Section 2, we out-
line the motivation of our approach in Section 3.
Section 4 describes the three stages of our summa-
rization system. The evaluation and the results are
presented in Section 5. Section 6 concludes the pa-
per.
</bodyText>
<page confidence="0.928232">
500
</page>
<note confidence="0.9843805">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 500–509,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997407" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999931454545455">
The idea of analyzing and utilizing citation informa-
tion is far from new. The motivation for using in-
formation latent in citations has been explored tens
of years back (Garfield et al., 1984; Hodges, 1972).
Since then, there has been a large body of research
done on citations.
Nanba and Okumura (2000) analyzed citation
sentences and automatically categorized citations
into three groups using 160 pre-defined phrase-
based rules. They also used citation categoriza-
tion to support a system for writing surveys (Nanba
and Okumura, 1999). Newman (2001) analyzed
the structure of the citation networks. Teufel et
al. (2006) addressed the problem of classifying ci-
tations based on their function.
Siddharthan and Teufel (2007) proposed a method
for determining the scientific attribution of an arti-
cle by analyzing citation sentences. Teufel (2007)
described a rhetorical classification task, in which
sentences are labeled as one of Own, Other, Back-
ground, Textual, Aim, Basis, or Contrast according
to their role in the authors argument. In parts of our
approach, we were inspired by this work.
Elkiss et al. (2008) performed a study on citation
summaries and their importance. They concluded
that citation summaries are more focused and con-
tain more information than abstracts. Mohammad
et al. (2009) suggested using citation information to
generate surveys of scientific paradigms.
Qazvinian and Radev (2008) proposed a method
for summarizing scientific articles by building a sim-
ilarity network of the citation sentences that cite
the target paper, and then applying network analy-
sis techniques to find a set of sentences that covers
as much of the summarized paper facts as possible.
We use this method as one of the baselines when we
evaluate our approach. Qazvinian et al. (2010) pro-
posed a citation-based summarization method that
first extracts a number of important keyphrases from
the set of citation sentences, and then finds the best
subset of sentences that covers as many keyphrases
as possible. Qazvinian and Radev (2010) addressed
the problem of identifying the non-explicit citing
sentences to aid citation-based summarization.
</bodyText>
<sectionHeader confidence="0.99066" genericHeader="method">
3 Motivation
</sectionHeader>
<bodyText confidence="0.999694833333333">
The coherence and readability of citation-based
summaries are impeded by several factors. First,
many citation sentences cite multiple papers besides
the target. For example, the following is a citation
sentence that appeared in the NLP literature and
talked about Resnik’s (1999) work.
</bodyText>
<listItem confidence="0.367875">
(1) Grefenstette and Nioche (2000) and Jones
and Ghani (2000) use the web to generate cor-
pora for languages where electronic resources are
</listItem>
<bodyText confidence="0.996522318181818">
scarce, while Resnik (1999) describes a method
for mining the web for bilingual texts.
The first fragment of this sentence describes dif-
ferent work other than Resnik’s. The contribution
of Resnik is mentioned in the underlined fragment.
Including the irrelevant fragments in the summary
causes several problems. First, the aim of the sum-
marization task is to summarize the contribution of
the target paper using minimal text. These frag-
ments take space in the summary while being irrel-
evant and less important. Second, including these
fragments in the summary breaks the context and,
hence, degrades the readability and confuses the
reader. Third, the existence of irrelevant fragments
in a sentence makes the ranking algorithm assign a
low weight to it although the relevant fragment may
cover an aspect of the paper that no other sentence
covers.
A second factor has to do with the ordering of the
sentences included in the summary. For example,
the following are two other citation sentences for
Resnik (1999).
</bodyText>
<listItem confidence="0.981675333333333">
(2) Mining the Web for bilingual text (Resnik, 1999) is
not likely to provide sufficient quantities of high quality
data.
(3) Resnik (1999) addressed the issue of language
identification for finding Web pages in the languages of
interest.
</listItem>
<bodyText confidence="0.9973196">
If these two sentences are to be included in the
summary, the reasonable ordering would be to put
the second sentence first.
Thirdly, in some instances of citation sentences,
the reference is not a syntactic constituent in the sen-
</bodyText>
<page confidence="0.995512">
501
</page>
<bodyText confidence="0.999671666666667">
tence. It is added just to indicate the existence of
citation. For example, in sentence (2) above, the ref-
erence could be safely removed from the sentence
without hurting its grammaticality.
In other instances (e.g. sentence (3) above), the
reference is a syntactic constituent of the sentence
and removing it makes the sentence ungrammatical.
However, in certain cases, the reference could be re-
placed with a suitable pronoun (i.e. he, she or they).
This helps avoid the redundancy that results from re-
peating the author name(s) in every sentence.
Finally, a significant number of citation sentences
are not suitable for summarization (Teufel et al.,
2006) and should be filtered out. The following
sentences are two examples.
</bodyText>
<listItem confidence="0.990277">
(4) The two algorithms we employed in our depen-
dency parsing model are the Eisner parsing (Eisner,
1996) and Chu-Lius algorithm (Chu and Liu, 1965).
(5) This type of model has been used by, among others,
Eisner (1996).
</listItem>
<bodyText confidence="0.994088888888889">
Sentence (4) appeared in a paper by Nguyen et al
(2007). It does not describe any aspect of Eisner’s
work, rather it informs the reader that Nguyen et al.
used Eisner’s algorithm in their model. There is no
value in adding this sentence to the summary of Eis-
ner’s paper. Teufel (2007) reported that a significant
number of citation sentences (67% of the sentences
in her dataset) were of this type.
Likewise, the comprehension of sentence (5) de-
pends on knowing its context (i.e. its surrounding
sentences). This sentence alone does not provide
any valuable information about Eisner’s paper and
should not be added to the summary unless its con-
text is extracted and included in the summary as
well.
In our approach, we address these issues to
achieve the goal of improving the coherence and the
readability of citation-based summaries.
</bodyText>
<sectionHeader confidence="0.9965" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999988916666667">
In this section we describe a system that takes a sci-
entific paper and a set of citation sentences that cite
it as input, and outputs a citation summary of the
paper. Our system produces the summaries in three
stages. In the first stage, the citation sentences are
preprocessed to rule out the unsuitable sentences and
the irrelevant fragments of sentences. In the sec-
ond stage, a number of citation sentences that cover
the various aspects of the paper are selected. In the
last stage, the selected sentences are post-processed
to enhance the readability of the summary. We de-
scribe the stages in the following three subsections.
</bodyText>
<subsectionHeader confidence="0.997213">
4.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999853333333333">
The aim of this stage is to determine which pieces of
text (sentences or fragments of sentences) should be
considered for selection in the next stage and which
ones should be excluded. This stage involves three
tasks: reference tagging, reference scope identifica-
tion, and sentence filtering.
</bodyText>
<subsubsectionHeader confidence="0.636815">
4.1.1 Reference Tagging
</subsubsectionHeader>
<bodyText confidence="0.9890574375">
A citation sentence contains one or more references.
At least one of these references corresponds to the
target paper. When writing scientific articles, au-
thors usually use standard patterns to include point-
ers to their references within the text. We use pattern
matching to tag such references. The reference to
the target is given a different tag than the references
to other papers.
The following example shows a citation sentence
with all the references tagged and the target refer-
ence given a different tag.
In &lt;TREF&gt;Resnik (1999)&lt;/TREF&gt;, &lt;REF&gt;Nie,
Simard, and Foster (2001)&lt;/REF&gt;, &lt;REF&gt;Ma and
Liberman (1999)&lt;/REF&gt;, and &lt;REF&gt;Resnik and
Smith (2002)&lt;/REF&gt;, the Web is harvested in search of
pages that are available in two languages.
</bodyText>
<subsectionHeader confidence="0.947549">
4.1.2 Identifying the Reference Scope
</subsectionHeader>
<bodyText confidence="0.999973363636364">
In the previous section, we showed the importance
of identifying the scope of the target reference; i.e.
the fragment of the citation sentence that corre-
sponds to the target paper. We define the scope of
a reference as the shortest fragment of the citation
sentence that contains the reference and could form
a grammatical sentence if the rest of the sentence
was removed.
To find such a fragment, we use a simple yet ade-
quate heuristic. We start by parsing the sentence us-
ing the link grammar parser (Sleator and Temperley,
</bodyText>
<page confidence="0.985028">
502
</page>
<bodyText confidence="0.9995302">
1991). Since the parser is not trained on citation sen-
tences, we replace the references with placeholders
before passing the sentence to the parser. Figure 1
shows a portion of the parse tree for Sentence (1)
(from Section 1).
</bodyText>
<figureCaption confidence="0.9930075">
Figure 1: An example showing the scope of a target ref-
erence
</figureCaption>
<bodyText confidence="0.973658090909091">
We extract the scope of the reference from the
parse tree as follows. We find the smallest subtree
rooted at an S node (sentence clause node) and con-
tains the target reference node. We extract the text
that corresponds to this subtree if it is grammati-
cal. Otherwise, we find the second smallest subtree
rooted at an S node and so on. For example, the
parse tree shown in Figure 1 suggests that the scope
of the reference is:
Resnik (1999) describes a method for mining the web for
bilingual texts.
</bodyText>
<subsectionHeader confidence="0.919656">
4.1.3 Sentence Filtering
</subsectionHeader>
<bodyText confidence="0.999990214285714">
The task in this step is to detect and filter out unsuit-
able sentences; i.e., sentences that depend on their
context (e.g. Sentence (5) above) or describe the
own work of their authors, not the contribution of
the target paper (e.g Sentence (4) above). Formally,
we classify the citation sentences into two classes:
suitable and unsuitable sentences. We use a ma-
chine learning technique for this purpose. We ex-
tract a number of features from each sentence and
train a classification model using these features. The
trained model is then used to classify the sentences.
We use Support Vector Machines (SVM) with linear
kernel as our classifier. The features that we use in
this step and their descriptions are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.990314">
4.2 Extraction
</subsectionHeader>
<bodyText confidence="0.999516074074074">
In the first stage, the sentences and sentence frag-
ments that are not useful for our summarization task
are ruled out. The input to this stage is a set of cita-
tion sentences that are believed to be suitable for the
summary. From these sentences, we need to select
a representative subset. The sentences are selected
based on these three main properties:
First, they should cover diverse aspects of the pa-
per. Second, the sentences that cover the same as-
pect should not contain redundant information. For
example, if two sentences talk about the drawbacks
of the target paper, one sentence can mention the
computation inefficiency, while the other criticize
the assumptions the paper makes. Third, the sen-
tences should cover as many important facts about
the target paper as possible using minimal text.
In this stage, the summary sentences are selected
in three steps. In the first step, the sentences are clas-
sified into five functional categories: Background,
Problem Statement, Method, Results, and Limita-
tions. In the second step, we cluster the sen-
tences within each category into clusters of simi-
lar sentences. In the third step, we compute the
LexRank (Erkan and Radev, 2004) values for the
sentences within each cluster. The summary sen-
tences are selected based on the classification, the
clustering, and the LexRank values.
</bodyText>
<subsectionHeader confidence="0.651502">
4.2.1 Functional Category Classification
</subsectionHeader>
<bodyText confidence="0.9999055">
We classify the citation sentences into the five cat-
egories mentioned above using a machine learning
technique. A classification model is trained on a
number of features (Table 2) extracted from a la-
beled set of citation sentences. We use SVM with
linear kernel as our classifier.
</bodyText>
<subsubsectionHeader confidence="0.513409">
4.2.2 Sentence Clustering
</subsubsectionHeader>
<bodyText confidence="0.9995166">
In the previous step we determined the category
of each citation sentence. It is very likely that
sentences from the same category contain similar or
overlapping information. For example, Sentences
(6), (7), and (8) below appear in the set of citation
</bodyText>
<page confidence="0.987353">
503
</page>
<table confidence="0.9977699">
Feature Description
Similarity to the target paper The value of the cosine similarity (using TF-IDF vectors) between the citation sentence and the target paper.
Headlines The section in which the citation sentence appeared in the citing paper. We recognize 10 section types such
as Introduction, Related Work, Approach, etc.
Relative position The relative position of the sentence in the section and the paragraph in which it appears
First person pronouns This feature takes a value of 1 if the sentence contains a first person pronoun (I, we, our, us, etc.), and 0
otherwise.
Tense of the first verb A sentence that contains a past tense verb near its beginning is more likely to be describing previous work.
Determiners Demonstrative Determiners (this, that, these, those, and which) and Alternative Determiners (another, other).
The value of this feature is the relative position of the first determiner (if one exists) in the sentence.
</table>
<tableCaption confidence="0.998829">
Table 1: The features used for sentence filtering
</tableCaption>
<table confidence="0.998862333333333">
Feature Description
Similarity to the sections of the target paper The sections of the target paper are categorized into five categories: 1) Introduction, Moti-
vation, Problem Statement. 2) Background, Prior Work, Previous Work, and Related Work.
3) Experiments, Results, and Evaluation. 4) Discussion, Conclusion, and Future work. 5)
All other headlines. The value of this feature is the cosine similarity (using TF-IDF vectors)
between the sentence and the text of the sections of each of the five section categories.
Headlines This is the same feature that we used for sentence filtering in Section 4.1.3.
Number of references in the sentence Sentences that contain multiple references are more likely to be Background sentences.
Verbs We use all the verbs that their lemmatized form appears in at least three sentences that belong
to the same category in the training set. Auxiliary verbs are excluded. In our annotated dataset,
for example, the verb propose appeared in 67 sentences from the Methodology category, while
the verbs outperform and achieve appeared in 33 Result sentences.
</table>
<tableCaption confidence="0.999564">
Table 2: The features used for sentence classification
</tableCaption>
<bodyText confidence="0.999007833333333">
sentences that cite Goldwater and Griffiths’ (2007).
These sentences belong to the same category (i.e
Method). Both Sentences (6) and (7) convey the
same information about Goldwater and Griffiths
(2007) contribution. Sentence (8), however, de-
scribes a different aspect of the paper methodology.
</bodyText>
<listItem confidence="0.997295666666667">
(6) Goldwater and Griffiths (2007) proposed an
information-theoretic measure known as the Variation of
Information (VI)
(7) Goldwater and Griffiths (2007) propose using the
Variation of Information (VI) metric
(8) A fully-Bayesian approach to unsupervised POS
tagging has been developed by Goldwater and Griffiths
(2007) as a viable alternative to the traditional maximum
likelihood-based HMM approach.
</listItem>
<bodyText confidence="0.999773714285714">
Clustering divides the sentences of each cate-
gory into groups of similar sentences. Following
Qazvinian and Radev (2008), we build a cosine sim-
ilarity graph out of the sentences of each category.
This is an undirected graph in which nodes are sen-
tences and edges represent similarity relations. Each
edge is weighted by the value of the cosine similarity
(using TF-IDF vectors) between the two sentences
the edge connects. Once we have the similarity net-
work constructed, we partition it into clusters using
a community finding technique. We use the Clauset
algorithm (Clauset et al., 2004), a hierarchical ag-
glomerative community finding algorithm that runs
in linear time.
</bodyText>
<subsectionHeader confidence="0.962615">
4.2.3 Ranking
</subsectionHeader>
<bodyText confidence="0.999886333333333">
Although the sentences that belong to the same clus-
ter are similar, they are not necessarily equally im-
portant. We rank the sentences within each clus-
ter by computing their LexRank (Erkan and Radev,
2004). Sentences with higher rank are more impor-
tant.
</bodyText>
<subsectionHeader confidence="0.540384">
4.2.4 Sentence Selection
</subsectionHeader>
<bodyText confidence="0.9999012">
At this point we have determined (Figure 2), for each
sentence, its category, its cluster, and its relative im-
portance. Sentences are added to the summary in
order based on their category, the size of their clus-
ters, then their LexRank values. The categories are
</bodyText>
<page confidence="0.99565">
504
</page>
<figureCaption confidence="0.999523">
Figure 2: Example illustrating sentence selection
</figureCaption>
<bodyText confidence="0.996002214285714">
ordered as Background, Problem, Method, Results,
then Limitations. Clusters within each category are
ordered by the number of sentences in them whereas
the sentences of each cluster are ordered by their
LexRank values.
In the example shown in Figure 2, we have three
categories. Each category contains several clusters.
Each cluster contains several sentences with differ-
ent LexRank values (illustrated by the sizes of the
dots in the figure.) If the desired length of the sum-
mary is 3 sentences, the selected sentences will be
in order S1, S12, then S18. If the desired length is 5,
the selected sentences will be S1, S5, S12, S15, then
S18.
</bodyText>
<subsectionHeader confidence="0.999091">
4.3 Postprocessing
</subsectionHeader>
<bodyText confidence="0.999915433333333">
In this stage, we refine the sentences that we ex-
tracted in the previous stage. Each citation sentence
will have the target reference (the author’s names
and the publication year) mentioned at least once.
The reference could be either syntactically and se-
mantically part of the sentence (e.g. Sentence (3)
above) or not (e.g. Sentence (2)). The aim of this
refinement step is to avoid repeating the author’s
names and the publication year in every sentence.
We keep the author’s names and the publication year
only in the first sentence of the summary. In the
following sentences, we either replace the reference
with a suitable personal pronoun or remove it. The
reference is replaced with a pronoun if it is part of
the sentence and this replacement does not make the
sentence ungrammatical. The reference is removed
if it is not part of the sentence. If the sentence con-
tains references for other papers, they are removed if
this doesn’t hurt the grammaticality of the sentence.
To determine whether a reference is part of the
sentence or not, we again use a machine learning
approach. We train a model on a set of labeled sen-
tences. The features used in this step are listed in
Table 3. The trained model is then used to classify
the references that appear in a sentence into three
classes: keep, remove, replace. If a reference is to
be replaced, and the paper has one author, we use
”he/she” (we do not know if the author is male or
female). If the paper has two or more authors, we
use ”they”.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999705">
We provide three levels of evaluation. First, we eval-
uate each of the components in our system sepa-
rately. Then we evaluate the summaries that our
system generate in terms of extraction quality. Fi-
nally, we evaluate the coherence and readability of
the summaries.
</bodyText>
<subsectionHeader confidence="0.972574">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999983375">
We use the ACL Anthology Network (AAN) (Radev
et al., 2009) in our evaluation. AAN is a collection
of more than 16000 papers from the Computational
Linguistics journal, and the proceedings of the ACL
conferences and workshops. AAN provides all cita-
tion information from within the network including
the citation network, the citation sentences, and the
citation context for each paper.
We used 55 papers from AAN as our data. The
papers have a variable number of citation sentences,
ranging from 15 to 348. The total number of cita-
tion sentences in the dataset is 4,335. We split the
data randomly into two different sets; one for evalu-
ating the components of the system, and the other for
evaluating the extraction quality and the readability
of the generated summaries. The first set (dataset1,
henceforth) contained 2,284 sentences coming from
25 papers. We asked humans with good background
in NLP (the area of the annotated papers) to provide
two annotations for each sentence in this set: 1) label
the sentence as Background, Problem, Method, Re-
sult, Limitation, or Unsuitable, 2) for each reference
in the sentence, determine whether it could be re-
placed with a pronoun, removed, or should be kept.
</bodyText>
<page confidence="0.968092">
505
</page>
<bodyText confidence="0.915125222222222">
Feature Description
Part-of-speech (POS) tag We consider the POS tags of the reference, the word before, and the word after. Before passing the
sentence to the POS tagger, all the references in the sentence are replaced by placeholders.
Style of the reference It is common practice in writing scientific papers to put the whole citation between parenthesis
when the authors are not a constitutive part of the enclosing sentence, and to enclose just the year
between parenthesis when the author’s name is a syntactic constituent in the sentence.
Relative position of the reference This feature takes one of three values: first, last, and inside.
Grammaticality Grammaticality of the sentence if the reference is removed/replaced. Again, we use the Link
Grammar parser (Sleator and Temperley, 1991) to check the grammaticality
</bodyText>
<tableCaption confidence="0.992779">
Table 3: The features used for author name replacement
</tableCaption>
<bodyText confidence="0.9992474">
Each sentence was given to 3 different annotators.
We used the majority vote labels.
We use Kappa coefficient (Krippendorff, 2003) to
measure the inter-annotator agreement. Kappa coef-
ficient is defined as:
</bodyText>
<equation confidence="0.999375666666667">
P(A) − P(E)
Kappa = (1)
1 − P(E)
</equation>
<bodyText confidence="0.999968631578947">
where P(A) is the relative observed agreement
among raters and P(E) is the hypothetical proba-
bility of chance agreement.
The agreement among the three annotators on dis-
tinguishing the unsuitable sentences from the other
five categories is 0.85. On Landis and Kochs(1977)
scale, this value indicates an almost perfect agree-
ment. The agreement on classifying the sentences
into the five functional categories is 0.68. On the
same scale this value indicates substantial agree-
ment.
The second set (dataset2, henceforth) contained
30 papers (2051 sentences). We asked humans with
a good background in NLP (the papers topic) to gen-
erate a readable, coherent summary for each paper in
the set using its citation sentences as the source text.
We asked them to fix the length of the summaries
to 5 sentences. Each paper was assigned to two hu-
mans to summarize.
</bodyText>
<subsectionHeader confidence="0.999205">
5.2 Component Evaluation
</subsectionHeader>
<bodyText confidence="0.988689714285714">
Reference Tagging and Reference Scope Iden-
tification Evaluation: We ran our reference tag-
ging and scope identification components on the
2,284 sentences in dataset1. Then, we went through
the tagged sentences and the extracted scopes, and
counted the number of correctly/incorrectly tagged
(extracted)/missed references (scopes). Our tagging
</bodyText>
<table confidence="0.99894925">
- Bkgrnd Prob Method Results Limit.
Precision 64.62% 60.01% 88.66% 76.05% 33.53%
Recall 72.47% 59.30% 75.03% 82.29% 59.36%
F1 68.32% 59.65% 81.27% 79.04% 42.85%
</table>
<tableCaption confidence="0.9529625">
Table 4: Precision and recall results achieved by our cita-
tion sentence classifier
</tableCaption>
<bodyText confidence="0.98124075">
component achieved 98.2% precision and 94.4% re-
call. The reference to the target paper was tagged
correctly in all the sentences.
Our scope identification component extracted the
scope of target references with good precision
(86.4%) but low recall (35.2%). In fact, extracting
a useful scope for a reference requires more than
just finding a grammatical substring. In future work,
we plan to employ text regeneration techniques to
improve the recall by generating grammatical sen-
tences from ungrammatical fragments.
Sentence Filtering Evaluation: We used Sup-
port Vector Machines (SVM) with linear kernel as
our classifier. We performed 10-fold cross validation
on the labeled sentences (unsuitable vs all other cat-
egories) in dataset1. Our classifier achieved 80.3%
accuracy.
Sentence Classification Evaluation: We used
SVM in this step as well. We also performed 10-
fold cross validation on the labeled sentences (the
five functional categories). This classifier achieved
70.1% accuracy. The precision and recall for each
category are given in Table 4
Author Name Replacement Evaluation: The
classifier used in this task is also SVM. We per-
formed 10-fold cross validation on the labeled sen-
tences of dataset1. Our classifier achieved 77.41%
accuracy.
</bodyText>
<page confidence="0.962417">
506
</page>
<bodyText confidence="0.986071875">
Produced using our system
There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical tech-
niques, e.g. constraint-based techniques and transformation-based techniques. A thorough removal of ambiguity requires a syntactic
process. A rule-based tagger described in Voutilainen (1995) was equipped with a set of guessing rules that had been hand-crafted using
knowledge of English morphology and intuitions. The precision of rule-based taggers may exceed that of the probabilistic ones. The
construction of a linguistic rule-based tagger, however, has been considered a difficult and time-consuming task.
Produced using Qazvinian and Radev (2008) system
Another approach is the rule-based or constraint-based approach, recently most prominently exemplified by the Constraint Grammar work
(Karlsson et al. , 1995; Voutilainen, 1995b; Voutilainen et al. , 1992; Voutilainen and Tapanainen, 1993), where a large number of
hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context.
Some systems even perform the POS tagging as part of a syntactic analysis process (Voutilainen, 1995). A rule-based tagger described
in (Voutilainen, 1995) is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology
and intuition. Older versions of EngCG (using about 1,150 constraints) are reported ( butilainen et al. 1992; Voutilainen and HeikkiUi
1994; Tapanainen and Voutilainen 1994; Voutilainen 1995) to assign a correct analysis to about 99.7% of all words while each word in
the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remait unresolved. We evaluate the resulting
disambiguated text by a number of metrics defined as follows (Voutilainen, 1995a).
</bodyText>
<tableCaption confidence="0.990349">
Table 5: Sample Output
</tableCaption>
<subsectionHeader confidence="0.99804">
5.3 Extraction Evaluation
</subsectionHeader>
<bodyText confidence="0.999512363636364">
To evaluate the extraction quality, we use dataset2
(that has never been used for training or tuning any
of the system components). We use our system to
generate summaries for each of the 30 papers in
dataset2. We also generate summaries for the pa-
pers using a number of baseline systems (described
in Section 5.3.1). All the generated summaries were
5 sentences long. We use the Recall-Oriented Un-
derstudy for Gisting Evaluation (ROUGE) based on
the longest common substrings (ROUGE-L) as our
evaluation metric.
</bodyText>
<subsectionHeader confidence="0.930513">
5.3.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999976916666667">
We evaluate the extraction quality of our system
(FL) against 7 different baselines. In the first base-
line, the sentences are selected randomly from the
set of citation sentences and added to the sum-
mary. The second baseline is the MEAD summa-
rizer (Radev et al., 2004) with all its settings set
to default. The third baseline is LexRank (Erkan
and Radev, 2004) run on the entire set of citation
sentences of the target paper. The forth baseline is
Qazvinian and Radev (2008) citation-based summa-
rizer (QR08) in which the citation sentences are first
clustered then the sentences within each cluster are
ranked using LexRank. The remaining baselines are
variations of our system produced by removing one
component from the pipeline at a time. In one vari-
ation (FL-1), we remove the sentence filtering com-
ponent. In another variation (FL-2), we remove the
sentence classification component; so, all the sen-
tences are assumed to come from one category in the
subsequent components. In a third variation (FL-3),
the clustering component is removed. To make the
comparison of the extraction quality to those base-
lines fair, we remove the author name replacement
component from our system and all its variations.
</bodyText>
<sectionHeader confidence="0.949969" genericHeader="evaluation">
5.3.2 Results
</sectionHeader>
<bodyText confidence="0.999914111111111">
Table 6 shows the average ROUGE-L scores (with
95% confidence interval) for the summaries of the
30 papers in dataset2 generated using our system
and the different baselines. The two human sum-
maries were used as models for comparison. The
Human score reported in the table is the result of
comparing the two human summaries to each others.
Statistical significance was tested using a 2-tailed
paired t-test. The results are statistically significant
at the 0.05 level.
The results show that our approach outperforms
all the baseline techniques. It achieves higher
ROUGE-L score for most of the papers in our test-
ing set. Comparing the score of FL-1 to the score
of FL shows that sentence filtering has a significant
impact on the results. It also shows that the classifi-
cation and clustering components both improve the
extraction quality.
</bodyText>
<subsectionHeader confidence="0.980787">
5.4 Coherence and Readability Evaluation
</subsectionHeader>
<bodyText confidence="0.999989">
We asked human judges (not including the authors)
to rate the coherence and readability of a number
of summaries for each of dataset2 papers. For
each paper we evaluated 3 summaries. The sum-
</bodyText>
<page confidence="0.987242">
507
</page>
<table confidence="0.9992765">
- Human Random MEAD LexRank QR08
ROUGE-L 0.733 0.398 0.410 0.408 0.435
- FL-1 FL-2 FL-3 FL -
ROUGE-L 0.475 0.511 0.525 0.539 -
</table>
<tableCaption confidence="0.866756">
Table 6: Extraction Evaluation
</tableCaption>
<table confidence="0.9999585">
Average Coherence Rating Number of summaries
Human FL QV08
1&lt; coherence &lt;2 0 9 17
2&lt; coherence &lt;3 3 11 12
3&lt; coherence &lt;4 16 9 1
4&lt; coherence &lt;5 11 1 0
</table>
<tableCaption confidence="0.998317">
Table 7: Coherence Evaluation
</tableCaption>
<bodyText confidence="0.999950035714286">
mary that our system produced, the human sum-
mary, and a summary produced by Qazvinian and
Radev (2008) summarizer (the best baseline - after
our system and its variations - in terms of extrac-
tion quality as shown in the previous subsection.)
The summaries were randomized and given to the
judges without telling them how each summary was
produced. The judges were not given access to the
source text. They were asked to use a five point-
scale to rate how coherent and readable the sum-
maries are, where 1 means that the summary is to-
tally incoherent and needs significant modifications
to improve its readability, and 5 means that the sum-
mary is coherent and no modifications are needed to
improve its readability. We gave each summary to 5
different judges and took the average of their ratings
for each summary. We used Weighted Kappa with
linear weights (Cohen, 1968) to measure the inter-
rater agreement. The Weighted Kappa measure be-
tween the five groups of ratings was 0.72.
Table 7 shows the number of summaries in each
rating range. The results show that our approach sig-
nificantly improves the coherence of citation-based
summarization. Table 5 shows two sample sum-
maries (each 5 sentences long) for the Voutilainen
(1995) paper. One summary was produced using our
system and the other was produced using Qazvinian
and Radev (2008) system.
</bodyText>
<sectionHeader confidence="0.999665" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999657833333333">
In this paper, we presented a new approach for
citation-based summarization of scientific papers
that produces readable summaries. Our approach in-
volves three stages. The first stage preprocesses the
set of citation sentences to filter out the irrelevant
sentences or fragments of sentences. In the second
stage, a representative set of sentences are extracted
and added to the summary in a reasonable order. In
the last stage, the summary sentences are refined to
improve their readability. The results of our exper-
iments confirmed that our system outperforms sev-
eral baseline systems.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999739583333334">
This work is in part supported by the National
Science Foundation grant “iOPENER: A Flexible
Framework to Support Rapid Learning in Unfamil-
iar Research Domains”, jointly awarded to Univer-
sity of Michigan and University of Maryland as
IIS 0705832, and in part by the NIH Grant U54
DA021519 to the National Center for Integrative
Biomedical Informatics.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this paper are those of
the authors and do not necessarily reflect the views
of the supporters.
</bodyText>
<sectionHeader confidence="0.998601" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997276090909091">
Aaron Clauset, M. E. J. Newman, and Cristopher Moore.
2004. Finding community structure in very large net-
works. Phys. Rev. E, 70(6):066111, Dec.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or partial
credit. Psychological Bulletin, 70(4):213 – 220.
Aaron Elkiss, Siwei Shen, Anthony Fader, G¨unes¸ Erkan,
David States, and Dragomir Radev. 2008. Blind men
and elephants: What do citation summaries tell us
about a research article? J. Am. Soc. Inf. Sci. Tech-
nol., 59(1):51–62.
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. J. Artif. Int. Res., 22(1):457–479.
E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The
Use of Citation Data in Writing the History of Science.
Institute for Scientific Information Inc., Philadelphia,
Pennsylvania, USA.
T. L. Hodges. 1972. Citation indexing-its theory
and application in science, technology, and humani-
ties. Ph.D. thesis, University of California at Berke-
ley.Ph.D. thesis, University of California at Berkeley.
</reference>
<page confidence="0.975498">
508
</page>
<reference confidence="0.99887541025641">
Klaus H. Krippendorff. 2003. Content Analysis: An In-
troduction to Its Methodology. Sage Publications, Inc,
2nd edition, December.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159–174, March.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings ofACL-08: HLT, pages 816–824, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584–592, Boulder, Colorado, June. Association
for Computational Linguistics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ’99: Proceedings of the Six-
teenth International Joint Conference on Artificial In-
telligence, pages 926–931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
M. E. J. Newman. 2001. The structure of scientific
collaboration networks. Proceedings of the National
Academy of Sciences of the United States of America,
98(2):404–409, January.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689–696, Manchester, UK, August.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555–564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895–903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C¸elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ’09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54–61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attributing
scientific work to citations. In In Proceedings of
NAACL/HLT-07.
Daniel D. K. Sleator and Davy Temperley. 1991. Parsing
english with a link grammar. In In Third International
Workshop on Parsing Technologies.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
Simone Teufel. 2007. Argumentative zoning for im-
proved citation indexing. computing attitude and affect
in text. In Theory and Applications, pages 159170.
</reference>
<page confidence="0.998463">
509
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.338733">
<title confidence="0.992378">Coherent Citation-Based Summarization of Scientific Papers</title>
<author confidence="0.703767">Amjad</author>
<affiliation confidence="0.954801">EECS University of</affiliation>
<author confidence="0.546077">Ann Arbor</author>
<author confidence="0.546077">MI</author>
<email confidence="0.999074">amjbara@umich.edu</email>
<author confidence="0.953825">Dragomir Radev</author>
<affiliation confidence="0.998531">EECS Department and School of Information University of Michigan</affiliation>
<address confidence="0.99841">Ann Arbor, MI, USA</address>
<email confidence="0.999863">radev@umich.edu</email>
<abstract confidence="0.99788752631579">In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and fluency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>M E J Newman</author>
<author>Cristopher Moore</author>
</authors>
<title>Finding community structure in very large networks.</title>
<date>2004</date>
<journal>Phys. Rev. E,</journal>
<volume>70</volume>
<issue>6</issue>
<contexts>
<context position="18785" citStr="Clauset et al., 2004" startWordPosition="3011" endWordPosition="3014">likelihood-based HMM approach. Clustering divides the sentences of each category into groups of similar sentences. Following Qazvinian and Radev (2008), we build a cosine similarity graph out of the sentences of each category. This is an undirected graph in which nodes are sentences and edges represent similarity relations. Each edge is weighted by the value of the cosine similarity (using TF-IDF vectors) between the two sentences the edge connects. Once we have the similarity network constructed, we partition it into clusters using a community finding technique. We use the Clauset algorithm (Clauset et al., 2004), a hierarchical agglomerative community finding algorithm that runs in linear time. 4.2.3 Ranking Although the sentences that belong to the same cluster are similar, they are not necessarily equally important. We rank the sentences within each cluster by computing their LexRank (Erkan and Radev, 2004). Sentences with higher rank are more important. 4.2.4 Sentence Selection At this point we have determined (Figure 2), for each sentence, its category, its cluster, and its relative importance. Sentences are added to the summary in order based on their category, the size of their clusters, then t</context>
</contexts>
<marker>Clauset, Newman, Moore, 2004</marker>
<rawString>Aaron Clauset, M. E. J. Newman, and Cristopher Moore. 2004. Finding community structure in very large networks. Phys. Rev. E, 70(6):066111, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit.</title>
<date>1968</date>
<journal>Psychological Bulletin,</journal>
<volume>70</volume>
<issue>4</issue>
<pages>220</pages>
<contexts>
<context position="32934" citStr="Cohen, 1968" startWordPosition="5306" endWordPosition="5307">were randomized and given to the judges without telling them how each summary was produced. The judges were not given access to the source text. They were asked to use a five pointscale to rate how coherent and readable the summaries are, where 1 means that the summary is totally incoherent and needs significant modifications to improve its readability, and 5 means that the summary is coherent and no modifications are needed to improve its readability. We gave each summary to 5 different judges and took the average of their ratings for each summary. We used Weighted Kappa with linear weights (Cohen, 1968) to measure the interrater agreement. The Weighted Kappa measure between the five groups of ratings was 0.72. Table 7 shows the number of summaries in each rating range. The results show that our approach significantly improves the coherence of citation-based summarization. Table 5 shows two sample summaries (each 5 sentences long) for the Voutilainen (1995) paper. One summary was produced using our system and the other was produced using Qazvinian and Radev (2008) system. 6 Conclusions In this paper, we presented a new approach for citation-based summarization of scientific papers that produc</context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>Jacob Cohen. 1968. Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit. Psychological Bulletin, 70(4):213 – 220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Elkiss</author>
<author>Siwei Shen</author>
<author>Anthony Fader</author>
<author>G¨unes¸ Erkan</author>
<author>David States</author>
<author>Dragomir Radev</author>
</authors>
<title>Blind men and elephants: What do citation summaries tell us about a research article?</title>
<date>2008</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="2536" citStr="Elkiss et al., 2008" startWordPosition="394" endWordPosition="397">n experts have put their efforts to read the paper and summarize its important contributions. One way to make use of these sentences is creating a summary of the target paper. This summary is different from the abstract or a summary generated from the paper itself. While the abstract represents the author’s point of view, the citation summary is the summation of multiple scholars’ viewpoints. The task of summarizing a scientific paper using its set of citation sentences is called citationbased summarization. There has been previous work done on citationbased summarization (Nanba et al., 2000; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Previous work focused on the extraction aspect; i.e. analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the paper. The cohesion and the readability of the produced summaries have been mostly ignored. This resulted in noisy and confusing summaries. In this work, we focus on the coherence and readability aspects of the problem. Our approach produces citation-based summaries in three stages: preprocessing, extraction, and postprocessing. Our experiments show t</context>
<context position="4849" citStr="Elkiss et al. (2008)" startWordPosition="761" endWordPosition="764"> writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on citation summaries and their importance. They concluded that citation summaries are more focused and contain more information than abstracts. Mohammad et al. (2009) suggested using citation information to generate surveys of scientific paradigms. Qazvinian and Radev (2008) proposed a method for summarizing scientific articles by building a similarity network of the citation sentences that cite the target paper, and then applying network analysis techniques to find a set of sentences that covers as much of the summarized paper facts as possible. We use this method as one o</context>
</contexts>
<marker>Elkiss, Shen, Fader, Erkan, States, Radev, 2008</marker>
<rawString>Aaron Elkiss, Siwei Shen, Anthony Fader, G¨unes¸ Erkan, David States, and Dragomir Radev. 2008. Blind men and elephants: What do citation summaries tell us about a research article? J. Am. Soc. Inf. Sci. Technol., 59(1):51–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="14607" citStr="Erkan and Radev, 2004" startWordPosition="2361" endWordPosition="2364">e target paper, one sentence can mention the computation inefficiency, while the other criticize the assumptions the paper makes. Third, the sentences should cover as many important facts about the target paper as possible using minimal text. In this stage, the summary sentences are selected in three steps. In the first step, the sentences are classified into five functional categories: Background, Problem Statement, Method, Results, and Limitations. In the second step, we cluster the sentences within each category into clusters of similar sentences. In the third step, we compute the LexRank (Erkan and Radev, 2004) values for the sentences within each cluster. The summary sentences are selected based on the classification, the clustering, and the LexRank values. 4.2.1 Functional Category Classification We classify the citation sentences into the five categories mentioned above using a machine learning technique. A classification model is trained on a number of features (Table 2) extracted from a labeled set of citation sentences. We use SVM with linear kernel as our classifier. 4.2.2 Sentence Clustering In the previous step we determined the category of each citation sentence. It is very likely that sen</context>
<context position="19088" citStr="Erkan and Radev, 2004" startWordPosition="3060" endWordPosition="3063">t similarity relations. Each edge is weighted by the value of the cosine similarity (using TF-IDF vectors) between the two sentences the edge connects. Once we have the similarity network constructed, we partition it into clusters using a community finding technique. We use the Clauset algorithm (Clauset et al., 2004), a hierarchical agglomerative community finding algorithm that runs in linear time. 4.2.3 Ranking Although the sentences that belong to the same cluster are similar, they are not necessarily equally important. We rank the sentences within each cluster by computing their LexRank (Erkan and Radev, 2004). Sentences with higher rank are more important. 4.2.4 Sentence Selection At this point we have determined (Figure 2), for each sentence, its category, its cluster, and its relative importance. Sentences are added to the summary in order based on their category, the size of their clusters, then their LexRank values. The categories are 504 Figure 2: Example illustrating sentence selection ordered as Background, Problem, Method, Results, then Limitations. Clusters within each category are ordered by the number of sentences in them whereas the sentences of each cluster are ordered by their LexRan</context>
<context position="29790" citStr="Erkan and Radev, 2004" startWordPosition="4773" endWordPosition="4776"> of baseline systems (described in Section 5.3.1). All the generated summaries were 5 sentences long. We use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) based on the longest common substrings (ROUGE-L) as our evaluation metric. 5.3.1 Baselines We evaluate the extraction quality of our system (FL) against 7 different baselines. In the first baseline, the sentences are selected randomly from the set of citation sentences and added to the summary. The second baseline is the MEAD summarizer (Radev et al., 2004) with all its settings set to default. The third baseline is LexRank (Erkan and Radev, 2004) run on the entire set of citation sentences of the target paper. The forth baseline is Qazvinian and Radev (2008) citation-based summarizer (QR08) in which the citation sentences are first clustered then the sentences within each cluster are ranked using LexRank. The remaining baselines are variations of our system produced by removing one component from the pipeline at a time. In one variation (FL-1), we remove the sentence filtering component. In another variation (FL-2), we remove the sentence classification component; so, all the sentences are assumed to come from one category in the subs</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gunes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res., 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Garfield</author>
<author>Irving H Sher</author>
<author>R J Torpie</author>
</authors>
<date>1984</date>
<booktitle>The Use of Citation Data in Writing the History of Science. Institute for Scientific Information Inc.,</booktitle>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="3927" citStr="Garfield et al., 1984" startWordPosition="618" endWordPosition="621">Section 2, we outline the motivation of our approach in Section 3. Section 4 describes the three stages of our summarization system. The evaluation and the results are presented in Section 5. Section 6 concludes the paper. 500 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 500–509, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work The idea of analyzing and utilizing citation information is far from new. The motivation for using information latent in citations has been explored tens of years back (Garfield et al., 1984; Hodges, 1972). Since then, there has been a large body of research done on citations. Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an a</context>
</contexts>
<marker>Garfield, Sher, Torpie, 1984</marker>
<rawString>E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The Use of Citation Data in Writing the History of Science. Institute for Scientific Information Inc., Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Hodges</author>
</authors>
<title>Citation indexing-its theory and application in science, technology, and humanities.</title>
<date>1972</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley.Ph.D. thesis, University of California at Berkeley.</institution>
<contexts>
<context position="3942" citStr="Hodges, 1972" startWordPosition="622" endWordPosition="623">he motivation of our approach in Section 3. Section 4 describes the three stages of our summarization system. The evaluation and the results are presented in Section 5. Section 6 concludes the paper. 500 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 500–509, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work The idea of analyzing and utilizing citation information is far from new. The motivation for using information latent in citations has been explored tens of years back (Garfield et al., 1984; Hodges, 1972). Since then, there has been a large body of research done on citations. Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analy</context>
</contexts>
<marker>Hodges, 1972</marker>
<rawString>T. L. Hodges. 1972. Citation indexing-its theory and application in science, technology, and humanities. Ph.D. thesis, University of California at Berkeley.Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus H Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology.</title>
<date>2003</date>
<publisher>Sage Publications, Inc,</publisher>
<note>2nd edition,</note>
<contexts>
<context position="24148" citStr="Krippendorff, 2003" startWordPosition="3908" endWordPosition="3909">utive part of the enclosing sentence, and to enclose just the year between parenthesis when the author’s name is a syntactic constituent in the sentence. Relative position of the reference This feature takes one of three values: first, last, and inside. Grammaticality Grammaticality of the sentence if the reference is removed/replaced. Again, we use the Link Grammar parser (Sleator and Temperley, 1991) to check the grammaticality Table 3: The features used for author name replacement Each sentence was given to 3 different annotators. We used the majority vote labels. We use Kappa coefficient (Krippendorff, 2003) to measure the inter-annotator agreement. Kappa coefficient is defined as: P(A) − P(E) Kappa = (1) 1 − P(E) where P(A) is the relative observed agreement among raters and P(E) is the hypothetical probability of chance agreement. The agreement among the three annotators on distinguishing the unsuitable sentences from the other five categories is 0.85. On Landis and Kochs(1977) scale, this value indicates an almost perfect agreement. The agreement on classifying the sentences into the five functional categories is 0.68. On the same scale this value indicates substantial agreement. The second se</context>
</contexts>
<marker>Krippendorff, 2003</marker>
<rawString>Klaus H. Krippendorff. 2003. Content Analysis: An Introduction to Its Methodology. Sage Publications, Inc, 2nd edition, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1):159–174, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Generating impact-based summaries for scientific literature.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>816--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2583" citStr="Mei and Zhai, 2008" startWordPosition="402" endWordPosition="405">er and summarize its important contributions. One way to make use of these sentences is creating a summary of the target paper. This summary is different from the abstract or a summary generated from the paper itself. While the abstract represents the author’s point of view, the citation summary is the summation of multiple scholars’ viewpoints. The task of summarizing a scientific paper using its set of citation sentences is called citationbased summarization. There has been previous work done on citationbased summarization (Nanba et al., 2000; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Previous work focused on the extraction aspect; i.e. analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the paper. The cohesion and the readability of the produced summaries have been mostly ignored. This resulted in noisy and confusing summaries. In this work, we focus on the coherence and readability aspects of the problem. Our approach produces citation-based summaries in three stages: preprocessing, extraction, and postprocessing. Our experiments show that our approach produces better summaries than</context>
</contexts>
<marker>Mei, Zhai, 2008</marker>
<rawString>Qiaozhu Mei and ChengXiang Zhai. 2008. Generating impact-based summaries for scientific literature. In Proceedings ofACL-08: HLT, pages 816–824, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Melissa Egan</author>
<author>Ahmed Hassan</author>
<author>Pradeep Muthukrishan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
<author>David Zajic</author>
</authors>
<title>Using citations to generate surveys of scientific paradigms.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>584--592</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="2607" citStr="Mohammad et al., 2009" startWordPosition="406" endWordPosition="409"> important contributions. One way to make use of these sentences is creating a summary of the target paper. This summary is different from the abstract or a summary generated from the paper itself. While the abstract represents the author’s point of view, the citation summary is the summation of multiple scholars’ viewpoints. The task of summarizing a scientific paper using its set of citation sentences is called citationbased summarization. There has been previous work done on citationbased summarization (Nanba et al., 2000; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Previous work focused on the extraction aspect; i.e. analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the paper. The cohesion and the readability of the produced summaries have been mostly ignored. This resulted in noisy and confusing summaries. In this work, we focus on the coherence and readability aspects of the problem. Our approach produces citation-based summaries in three stages: preprocessing, extraction, and postprocessing. Our experiments show that our approach produces better summaries than several baseline summar</context>
<context position="5035" citStr="Mohammad et al. (2009)" startWordPosition="789" endWordPosition="792">their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on citation summaries and their importance. They concluded that citation summaries are more focused and contain more information than abstracts. Mohammad et al. (2009) suggested using citation information to generate surveys of scientific paradigms. Qazvinian and Radev (2008) proposed a method for summarizing scientific articles by building a similarity network of the citation sentences that cite the target paper, and then applying network analysis techniques to find a set of sentences that covers as much of the summarized paper facts as possible. We use this method as one of the baselines when we evaluate our approach. Qazvinian et al. (2010) proposed a citation-based summarization method that first extracts a number of important keyphrases from the set of</context>
</contexts>
<marker>Mohammad, Dorr, Egan, Hassan, Muthukrishan, Qazvinian, Radev, Zajic, 2009</marker>
<rawString>Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, and David Zajic. 2009. Using citations to generate surveys of scientific paradigms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 584–592, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In IJCAI ’99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>926--931</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4271" citStr="Nanba and Okumura, 1999" startWordPosition="670" endWordPosition="673">Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work The idea of analyzing and utilizing citation information is far from new. The motivation for using information latent in citations has been explored tens of years back (Garfield et al., 1984; Hodges, 1972). Since then, there has been a large body of research done on citations. Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on </context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>Hidetsugu Nanba and Manabu Okumura. 1999. Towards multi-paper summarization using reference information. In IJCAI ’99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 926–931, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
</authors>
<title>Noriko Kando, Manabu Okumura, and Of Information Science.</title>
<date>2000</date>
<marker>Nanba, 2000</marker>
<rawString>Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and Of Information Science. 2000. Classification of research papers using citation links and citation types: Towards automatic review article generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E J Newman</author>
</authors>
<title>The structure of scientific collaboration networks.</title>
<date>2001</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<volume>98</volume>
<issue>2</issue>
<contexts>
<context position="4286" citStr="Newman (2001)" startWordPosition="674" endWordPosition="675">c�2011 Association for Computational Linguistics 2 Related Work The idea of analyzing and utilizing citation information is far from new. The motivation for using information latent in citations has been explored tens of years back (Garfield et al., 1984; Hodges, 1972). Since then, there has been a large body of research done on citations. Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on citation summar</context>
</contexts>
<marker>Newman, 2001</marker>
<rawString>M. E. J. Newman. 2001. The structure of scientific collaboration networks. Proceedings of the National Academy of Sciences of the United States of America, 98(2):404–409, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>689--696</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2563" citStr="Qazvinian and Radev, 2008" startWordPosition="398" endWordPosition="401">eir efforts to read the paper and summarize its important contributions. One way to make use of these sentences is creating a summary of the target paper. This summary is different from the abstract or a summary generated from the paper itself. While the abstract represents the author’s point of view, the citation summary is the summation of multiple scholars’ viewpoints. The task of summarizing a scientific paper using its set of citation sentences is called citationbased summarization. There has been previous work done on citationbased summarization (Nanba et al., 2000; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Previous work focused on the extraction aspect; i.e. analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the paper. The cohesion and the readability of the produced summaries have been mostly ignored. This resulted in noisy and confusing summaries. In this work, we focus on the coherence and readability aspects of the problem. Our approach produces citation-based summaries in three stages: preprocessing, extraction, and postprocessing. Our experiments show that our approach produces b</context>
<context position="5144" citStr="Qazvinian and Radev (2008)" startWordPosition="803" endWordPosition="806">n of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on citation summaries and their importance. They concluded that citation summaries are more focused and contain more information than abstracts. Mohammad et al. (2009) suggested using citation information to generate surveys of scientific paradigms. Qazvinian and Radev (2008) proposed a method for summarizing scientific articles by building a similarity network of the citation sentences that cite the target paper, and then applying network analysis techniques to find a set of sentences that covers as much of the summarized paper facts as possible. We use this method as one of the baselines when we evaluate our approach. Qazvinian et al. (2010) proposed a citation-based summarization method that first extracts a number of important keyphrases from the set of citation sentences, and then finds the best subset of sentences that covers as many keyphrases as possible. </context>
<context position="18315" citStr="Qazvinian and Radev (2008)" startWordPosition="2933" endWordPosition="2936">ths (2007) contribution. Sentence (8), however, describes a different aspect of the paper methodology. (6) Goldwater and Griffiths (2007) proposed an information-theoretic measure known as the Variation of Information (VI) (7) Goldwater and Griffiths (2007) propose using the Variation of Information (VI) metric (8) A fully-Bayesian approach to unsupervised POS tagging has been developed by Goldwater and Griffiths (2007) as a viable alternative to the traditional maximum likelihood-based HMM approach. Clustering divides the sentences of each category into groups of similar sentences. Following Qazvinian and Radev (2008), we build a cosine similarity graph out of the sentences of each category. This is an undirected graph in which nodes are sentences and edges represent similarity relations. Each edge is weighted by the value of the cosine similarity (using TF-IDF vectors) between the two sentences the edge connects. Once we have the similarity network constructed, we partition it into clusters using a community finding technique. We use the Clauset algorithm (Clauset et al., 2004), a hierarchical agglomerative community finding algorithm that runs in linear time. 4.2.3 Ranking Although the sentences that bel</context>
<context position="27691" citStr="Qazvinian and Radev (2008)" startWordPosition="4444" endWordPosition="4447">ological disambiguation using various techniques such as statistical techniques, e.g. constraint-based techniques and transformation-based techniques. A thorough removal of ambiguity requires a syntactic process. A rule-based tagger described in Voutilainen (1995) was equipped with a set of guessing rules that had been hand-crafted using knowledge of English morphology and intuitions. The precision of rule-based taggers may exceed that of the probabilistic ones. The construction of a linguistic rule-based tagger, however, has been considered a difficult and time-consuming task. Produced using Qazvinian and Radev (2008) system Another approach is the rule-based or constraint-based approach, recently most prominently exemplified by the Constraint Grammar work (Karlsson et al. , 1995; Voutilainen, 1995b; Voutilainen et al. , 1992; Voutilainen and Tapanainen, 1993), where a large number of hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context. Some systems even perform the POS tagging as part of a syntactic analysis process (Voutilainen, 1995). A rule-based tagger described in (Voutilainen, 1995) is equipped with a set of guessing r</context>
<context position="29904" citStr="Qazvinian and Radev (2008)" startWordPosition="4793" endWordPosition="4796">he Recall-Oriented Understudy for Gisting Evaluation (ROUGE) based on the longest common substrings (ROUGE-L) as our evaluation metric. 5.3.1 Baselines We evaluate the extraction quality of our system (FL) against 7 different baselines. In the first baseline, the sentences are selected randomly from the set of citation sentences and added to the summary. The second baseline is the MEAD summarizer (Radev et al., 2004) with all its settings set to default. The third baseline is LexRank (Erkan and Radev, 2004) run on the entire set of citation sentences of the target paper. The forth baseline is Qazvinian and Radev (2008) citation-based summarizer (QR08) in which the citation sentences are first clustered then the sentences within each cluster are ranked using LexRank. The remaining baselines are variations of our system produced by removing one component from the pipeline at a time. In one variation (FL-1), we remove the sentence filtering component. In another variation (FL-2), we remove the sentence classification component; so, all the sentences are assumed to come from one category in the subsequent components. In a third variation (FL-3), the clustering component is removed. To make the comparison of the</context>
<context position="32168" citStr="Qazvinian and Radev (2008)" startWordPosition="5171" endWordPosition="5174">uman judges (not including the authors) to rate the coherence and readability of a number of summaries for each of dataset2 papers. For each paper we evaluated 3 summaries. The sum507 - Human Random MEAD LexRank QR08 ROUGE-L 0.733 0.398 0.410 0.408 0.435 - FL-1 FL-2 FL-3 FL - ROUGE-L 0.475 0.511 0.525 0.539 - Table 6: Extraction Evaluation Average Coherence Rating Number of summaries Human FL QV08 1&lt; coherence &lt;2 0 9 17 2&lt; coherence &lt;3 3 11 12 3&lt; coherence &lt;4 16 9 1 4&lt; coherence &lt;5 11 1 0 Table 7: Coherence Evaluation mary that our system produced, the human summary, and a summary produced by Qazvinian and Radev (2008) summarizer (the best baseline - after our system and its variations - in terms of extraction quality as shown in the previous subsection.) The summaries were randomized and given to the judges without telling them how each summary was produced. The judges were not given access to the source text. They were asked to use a five pointscale to rate how coherent and readable the summaries are, where 1 means that the summary is totally incoherent and needs significant modifications to improve its readability, and 5 means that the summary is coherent and no modifications are needed to improve its re</context>
<context position="33403" citStr="Qazvinian and Radev (2008)" startWordPosition="5381" endWordPosition="5384">. We gave each summary to 5 different judges and took the average of their ratings for each summary. We used Weighted Kappa with linear weights (Cohen, 1968) to measure the interrater agreement. The Weighted Kappa measure between the five groups of ratings was 0.72. Table 7 shows the number of summaries in each rating range. The results show that our approach significantly improves the coherence of citation-based summarization. Table 5 shows two sample summaries (each 5 sentences long) for the Voutilainen (1995) paper. One summary was produced using our system and the other was produced using Qazvinian and Radev (2008) system. 6 Conclusions In this paper, we presented a new approach for citation-based summarization of scientific papers that produces readable summaries. Our approach involves three stages. The first stage preprocesses the set of citation sentences to filter out the irrelevant sentences or fragments of sentences. In the second stage, a representative set of sentences are extracted and added to the summary in a reasonable order. In the last stage, the summary sentences are refined to improve their readability. The results of our experiments confirmed that our system outperforms several baseline</context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689–696, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Identifying non-explicit citing sentences for citation-based summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>555--564</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5770" citStr="Qazvinian and Radev (2010)" startWordPosition="906" endWordPosition="909"> proposed a method for summarizing scientific articles by building a similarity network of the citation sentences that cite the target paper, and then applying network analysis techniques to find a set of sentences that covers as much of the summarized paper facts as possible. We use this method as one of the baselines when we evaluate our approach. Qazvinian et al. (2010) proposed a citation-based summarization method that first extracts a number of important keyphrases from the set of citation sentences, and then finds the best subset of sentences that covers as many keyphrases as possible. Qazvinian and Radev (2010) addressed the problem of identifying the non-explicit citing sentences to aid citation-based summarization. 3 Motivation The coherence and readability of citation-based summaries are impeded by several factors. First, many citation sentences cite multiple papers besides the target. For example, the following is a citation sentence that appeared in the NLP literature and talked about Resnik’s (1999) work. (1) Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for m</context>
</contexts>
<marker>Qazvinian, Radev, 2010</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2010. Identifying non-explicit citing sentences for citation-based summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
<author>Arzucan Ozgur</author>
</authors>
<title>Citation summarization through keyphrase extraction.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>895--903</pages>
<location>Beijing, China,</location>
<contexts>
<context position="5519" citStr="Qazvinian et al. (2010)" startWordPosition="867" endWordPosition="870">eir importance. They concluded that citation summaries are more focused and contain more information than abstracts. Mohammad et al. (2009) suggested using citation information to generate surveys of scientific paradigms. Qazvinian and Radev (2008) proposed a method for summarizing scientific articles by building a similarity network of the citation sentences that cite the target paper, and then applying network analysis techniques to find a set of sentences that covers as much of the summarized paper facts as possible. We use this method as one of the baselines when we evaluate our approach. Qazvinian et al. (2010) proposed a citation-based summarization method that first extracts a number of important keyphrases from the set of citation sentences, and then finds the best subset of sentences that covers as many keyphrases as possible. Qazvinian and Radev (2010) addressed the problem of identifying the non-explicit citing sentences to aid citation-based summarization. 3 Motivation The coherence and readability of citation-based summaries are impeded by several factors. First, many citation sentences cite multiple papers besides the target. For example, the following is a citation sentence that appeared i</context>
</contexts>
<marker>Qazvinian, Radev, Ozgur, 2010</marker>
<rawString>Vahed Qazvinian, Dragomir R. Radev, and Arzucan Ozgur. 2010. Citation summarization through keyphrase extraction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895–903, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Timothy Allison</author>
<author>Sasha BlairGoldensohn</author>
<author>John Blitzer</author>
<author>Arda C¸elebi</author>
<author>Stanko Dimitrov</author>
<author>Elliott Drabek</author>
<author>Ali Hakim</author>
<author>Wai Lam</author>
</authors>
<title>Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion,</title>
<date>2004</date>
<booktitle>In LREC 2004,</booktitle>
<location>Simone Teufel, Michael Topper, Adam</location>
<marker>Radev, Allison, BlairGoldensohn, Blitzer, C¸elebi, Dimitrov, Drabek, Hakim, Lam, 2004</marker>
<rawString>Dragomir Radev, Timothy Allison, Sasha BlairGoldensohn, John Blitzer, Arda C¸elebi, Stanko Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion, Simone Teufel, Michael Topper, Adam Winkel, and Zhu Zhang. 2004. MEAD - a platform for multidocument multilingual text summarization. In LREC 2004, Lisbon, Portugal, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
</authors>
<title>The acl anthology network corpus.</title>
<date>2009</date>
<booktitle>In NLPIR4DL ’09: Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,</booktitle>
<pages>54--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="21987" citStr="Radev et al., 2009" startWordPosition="3558" endWordPosition="3561">assify the references that appear in a sentence into three classes: keep, remove, replace. If a reference is to be replaced, and the paper has one author, we use ”he/she” (we do not know if the author is male or female). If the paper has two or more authors, we use ”they”. 5 Evaluation We provide three levels of evaluation. First, we evaluate each of the components in our system separately. Then we evaluate the summaries that our system generate in terms of extraction quality. Finally, we evaluate the coherence and readability of the summaries. 5.1 Data We use the ACL Anthology Network (AAN) (Radev et al., 2009) in our evaluation. AAN is a collection of more than 16000 papers from the Computational Linguistics journal, and the proceedings of the ACL conferences and workshops. AAN provides all citation information from within the network including the citation network, the citation sentences, and the citation context for each paper. We used 55 papers from AAN as our data. The papers have a variable number of citation sentences, ranging from 15 to 348. The total number of citation sentences in the dataset is 4,335. We split the data randomly into two different sets; one for evaluating the components of</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, 2009</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. 2009. The acl anthology network corpus. In NLPIR4DL ’09: Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 54–61, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Simone Teufel</author>
</authors>
<title>Whose idea was this, and why does it matter? attributing scientific work to citations. In</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL/HLT-07.</booktitle>
<contexts>
<context position="4458" citStr="Siddharthan and Teufel (2007)" startWordPosition="698" endWordPosition="701"> using information latent in citations has been explored tens of years back (Garfield et al., 1984; Hodges, 1972). Since then, there has been a large body of research done on citations. Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on citation summaries and their importance. They concluded that citation summaries are more focused and contain more information than abstracts. Mohammad et al. (2009) suggested using citati</context>
</contexts>
<marker>Siddharthan, Teufel, 2007</marker>
<rawString>Advaith Siddharthan and Simone Teufel. 2007. Whose idea was this, and why does it matter? attributing scientific work to citations. In In Proceedings of NAACL/HLT-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D K Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing english with a link grammar. In</title>
<date>1991</date>
<booktitle>In Third International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="23934" citStr="Sleator and Temperley, 1991" startWordPosition="3873" endWordPosition="3876">er, all the references in the sentence are replaced by placeholders. Style of the reference It is common practice in writing scientific papers to put the whole citation between parenthesis when the authors are not a constitutive part of the enclosing sentence, and to enclose just the year between parenthesis when the author’s name is a syntactic constituent in the sentence. Relative position of the reference This feature takes one of three values: first, last, and inside. Grammaticality Grammaticality of the sentence if the reference is removed/replaced. Again, we use the Link Grammar parser (Sleator and Temperley, 1991) to check the grammaticality Table 3: The features used for author name replacement Each sentence was given to 3 different annotators. We used the majority vote labels. We use Kappa coefficient (Krippendorff, 2003) to measure the inter-annotator agreement. Kappa coefficient is defined as: P(A) − P(E) Kappa = (1) 1 − P(E) where P(A) is the relative observed agreement among raters and P(E) is the hypothetical probability of chance agreement. The agreement among the three annotators on distinguishing the unsuitable sentences from the other five categories is 0.85. On Landis and Kochs(1977) scale,</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Daniel D. K. Sleator and Davy Temperley. 1991. Parsing english with a link grammar. In In Third International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Advaith Siddharthan</author>
<author>Dan Tidhar</author>
</authors>
<title>Automatic classification of citation function. In</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP-06.</booktitle>
<contexts>
<context position="4356" citStr="Teufel et al. (2006)" startWordPosition="683" endWordPosition="686"> The idea of analyzing and utilizing citation information is far from new. The motivation for using information latent in citations has been explored tens of years back (Garfield et al., 1984; Hodges, 1972). Since then, there has been a large body of research done on citations. Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on citation summaries and their importance. They concluded that citation summaries are m</context>
<context position="8462" citStr="Teufel et al., 2006" startWordPosition="1333" endWordPosition="1336">he existence of citation. For example, in sentence (2) above, the reference could be safely removed from the sentence without hurting its grammaticality. In other instances (e.g. sentence (3) above), the reference is a syntactic constituent of the sentence and removing it makes the sentence ungrammatical. However, in certain cases, the reference could be replaced with a suitable pronoun (i.e. he, she or they). This helps avoid the redundancy that results from repeating the author name(s) in every sentence. Finally, a significant number of citation sentences are not suitable for summarization (Teufel et al., 2006) and should be filtered out. The following sentences are two examples. (4) The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965). (5) This type of model has been used by, among others, Eisner (1996). Sentence (4) appeared in a paper by Nguyen et al (2007). It does not describe any aspect of Eisner’s work, rather it informs the reader that Nguyen et al. used Eisner’s algorithm in their model. There is no value in adding this sentence to the summary of Eisner’s paper. Teufel (2007) reported that a significa</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006. Automatic classification of citation function. In In Proc. of EMNLP-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>Argumentative zoning for improved citation indexing. computing attitude and affect in text.</title>
<date>2007</date>
<booktitle>In Theory and Applications,</booktitle>
<pages>159170</pages>
<contexts>
<context position="4458" citStr="Teufel (2007)" startWordPosition="700" endWordPosition="701">on latent in citations has been explored tens of years back (Garfield et al., 1984; Hodges, 1972). Since then, there has been a large body of research done on citations. Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on citation summaries and their importance. They concluded that citation summaries are more focused and contain more information than abstracts. Mohammad et al. (2009) suggested using citati</context>
<context position="9036" citStr="Teufel (2007)" startWordPosition="1436" endWordPosition="1437">for summarization (Teufel et al., 2006) and should be filtered out. The following sentences are two examples. (4) The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965). (5) This type of model has been used by, among others, Eisner (1996). Sentence (4) appeared in a paper by Nguyen et al (2007). It does not describe any aspect of Eisner’s work, rather it informs the reader that Nguyen et al. used Eisner’s algorithm in their model. There is no value in adding this sentence to the summary of Eisner’s paper. Teufel (2007) reported that a significant number of citation sentences (67% of the sentences in her dataset) were of this type. Likewise, the comprehension of sentence (5) depends on knowing its context (i.e. its surrounding sentences). This sentence alone does not provide any valuable information about Eisner’s paper and should not be added to the summary unless its context is extracted and included in the summary as well. In our approach, we address these issues to achieve the goal of improving the coherence and the readability of citation-based summaries. 4 Approach In this section we describe a system </context>
</contexts>
<marker>Teufel, 2007</marker>
<rawString>Simone Teufel. 2007. Argumentative zoning for improved citation indexing. computing attitude and affect in text. In Theory and Applications, pages 159170.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>