<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012236">
<title confidence="0.99716">
Normalizing tweets with edit scripts and recurrent neural embeddings
</title>
<author confidence="0.984519">
Grzegorz Chrupała
</author>
<affiliation confidence="0.9933575">
Tilburg Center for Cognition and Communication
Tilburg University
</affiliation>
<email confidence="0.982204">
g.chrupala@uvt.nl
</email>
<sectionHeader confidence="0.99719" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950894736842">
Tweets often contain a large proportion of
abbreviations, alternative spellings, novel
words and other non-canonical language.
These features are problematic for stan-
dard language analysis tools and it can
be desirable to convert them to canoni-
cal form. We propose a novel text nor-
malization model based on learning edit
operations from labeled data while incor-
porating features induced from unlabeled
data via character-level neural text embed-
dings. The text embeddings are generated
using an Simple Recurrent Network. We
find that enriching the feature set with text
embeddings substantially lowers word er-
ror rates on an English tweet normaliza-
tion dataset. Our model improves on state-
of-the-art with little training data and with-
out any lexical resources.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913659574468">
A stream of posts from Twitter contains text writ-
ten in a large variety of languages and writing sys-
tems, in registers ranging from formal to inter-
net slang. Substantial effort has been expended
in recent years to adapt standard NLP process-
ing pipelines to be able to deal with such con-
tent. One approach has been text normaliza-
tion, i.e. transforming tweet text into a more
canonical form which standard NLP tools ex-
pect. A multitude of resources and approaches
have been used to deal with normalization: hand-
crafted and (semi-)automatically induced dictio-
naries, language models, finite state transduc-
ers, machine translation models and combinations
thereof. Methods such as those of Han and Bald-
win (2011), Liu et al. (2011), Gouws et al. (2011)
or Han et al. (2012) are unsupervised but they
typically use many adjustable parameters which
need to be tuned on some annotated data. In this
work we suggest a simple, supervised character-
level string transduction model which easily incor-
porates features automatically learned from large
amounts of unlabeled data and needs only a lim-
ited amount of labeled training data and no lexical
resources.
Our model learns sequences of edit operations
from labeled data using a Conditional Random
Field (Lafferty et al., 2001). Unlabeled data
is incorporated following recent work on using
character-level text embeddings for text segmen-
tation (Chrupała, 2013), and word and sentence
boundary detection (Evang et al., 2013). We
train a recurrent neural network language model
(Mikolov et al., 2010; Mikolov, 2012b) on a large
collection of tweets. When run on new strings, the
activations of the units in the hidden layer at each
position in the string are recorded and used as fea-
tures for training the string transduction model.
The principal contributions of our work are: (i)
we show that a discriminative sequence labeling
model is apt for text normalization and performs
at state-of-the-art levels with small amounts of la-
beled training data; (ii) we show that character-
level neural text embeddings can be used to effec-
tively incorporate information from unlabeled data
into the model and can substantially boost text nor-
malization performance.
</bodyText>
<sectionHeader confidence="0.998706" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.9996387">
Many approaches to text normalization adopt the
noisy channel setting, where the model normaliz-
ing source string s into target canonical form t is
factored into two parts: tˆ = arg maxi P(t)P(sIt).
The error term P(slt) models how canonical
strings are transformed into variants such as e.g.
misspellings, emphatic lengthenings or abbrevia-
tions. The language model P(t) encodes which
target strings are probable.
We think this decomposition is less appropriate
</bodyText>
<page confidence="0.962568">
680
</page>
<bodyText confidence="0.483806">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 680–686,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<figure confidence="0.615865111111111">
c w a t
DEL INS(see) NIL INS(h) NIL
Output see w ha t
tˆ = arg max
t
P(ses(s, t)|s)
Input
is:
Edit
</figure>
<tableCaption confidence="0.986914">
Table 1: Example edit script.
</tableCaption>
<bodyText confidence="0.999954764705882">
in the context of text normalization than in appli-
cations from which it was borrowed such as Ma-
chine Translations. This is because it is not obvi-
ous what kind of data can be used to estimate the
language model: there is plentiful text from the
source domain, but little of it is in normalized tar-
get form. There is also much edited text such as
news text, but it comes from a very different do-
main. One of the main advantages of the noisy
channel decomposition is that is makes it easy to
exploit large amounts of unlabeled data in the form
of a language model. This advantage does not hold
for text normalization.
We thus propose an alternative approach where
normalization is modeled directly, and which en-
ables easy incorporation of unlabeled data from
the source domain.
</bodyText>
<subsectionHeader confidence="0.999574">
2.1 Learning to transduce strings
</subsectionHeader>
<bodyText confidence="0.999824714285714">
Our string transduction model works by learning
the sequence of edits which transform the input
string into the output string. Given a pair of strings
such a sequence of edits (known as the shortest
edit script) can be found using the DIFF algorithm
(Miller and Myers, 1985; Myers, 1986). Our ver-
sion of DIFF uses the following types of edits:
</bodyText>
<listItem confidence="0.9847305">
• NIL – no edits,
• DEL – delete character at this position,
• INS(·) – insert specified string before charac-
ter at this position.1
</listItem>
<bodyText confidence="0.948617">
Table 1 shows a shortest edit script for the pair
of strings (c wat, see what).
We use a sequence labeling model to learn to
label input strings with edit scripts. The train-
ing data for the model is generated by comput-
ing shortest edit scripts for pairs of original and
normalized strings. As a sequence labeler we use
Conditional Random Fields (Lafferty et al., 2001).
Once trained the model is used to label new strings
and the predicted edit script is applied to the in-
put string producing the normalized output string.
Given source string s the predicted target string tˆ
1The input string is extended with an empty symbol to
account for the cases where an insertion is needed at the end
of the string.
where e = ses(s, t) is the shortest edit script map-
ping s to t. P(e|s) is modeled with a linear-chain
Conditional Random Field.
</bodyText>
<subsectionHeader confidence="0.999717">
2.2 Character-level text embeddings
</subsectionHeader>
<bodyText confidence="0.997544512195122">
Simple Recurrent Networks (SRNs) were intro-
duced by Elman (1990) as models of temporal, or
sequential, structure in data, including linguistic
data (Elman, 1991). More recently SRNs were
used as language models for speech recognition
and shown to outperform classical n-gram lan-
guage models (Mikolov et al., 2010; Mikolov,
2012b). Another version of recurrent neural nets
has been used to generate plausible text with a
character-level language model (Sutskever et al.,
2011). We use SRNs to induce character-level text
representations from unlabeled Twitter data to use
as features in the string transduction model.
The units in the hidden layer at time t receive
connections from input units at time t and also
from the hidden units at the previous time step
t − 1. The hidden layer predicts the state of the
output units at the next time step t + 1. The input
vector w(t) represents the input element at current
time step, here the current character. The output
vector y(t) represents the predicted probabilities
for the next character. The activation sj of a hid-
den unit j is a function of the current input and the
state of the hidden layer at the previous time step:
t − 1:
�sj(t − 1)Wjl
where a is the sigmoid function and Uji is the
weight between input component i and hidden unit
j, while Wjl is the weight between hidden unit l
at time t − 1 and hidden unit j at time t. The
representation of recent history is stored in a lim-
ited number of recurrently connected hidden units.
This forces the network to make the representation
compressed and abstract rather than just memo-
rize literal history. Chrupała (2013) and Evang
et al. (2013) show that these text embeddings can
be useful as features in textual segmentation tasks.
We use them to bring in information from unla-
beled data into our string transduction model and
then train a character-level SRN language model
on unlabeled tweets. We run the trained model on
</bodyText>
<equation confidence="0.997052">
I L
sj(t) = a wi(t)Uji +
i=1 l=1
</equation>
<page confidence="0.993402">
681
</page>
<figureCaption confidence="0.998987">
Figure 1: Tweets randomly generated with an SRN
</figureCaption>
<bodyText confidence="0.999871333333333">
new tweets and record the activation of the hid-
den layer at each position as the model predicts the
next character. These activation vectors form our
text embeddings: they are discretized and used as
input features to the supervised sequence labeler
as described in Section 3.4.
</bodyText>
<sectionHeader confidence="0.996916" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999938666666667">
We limit the size of the string alphabet by always
working with UTF-8 encoded strings, and using
bytes rather than characters as basic units.
</bodyText>
<subsectionHeader confidence="0.999258">
3.1 Unlabeled tweets
</subsectionHeader>
<bodyText confidence="0.998829371428571">
In order to train our SRN language model we col-
lected a set of tweets using the Twitter sampling
API. We use the raw sample directly without fil-
tering it in any way, relying on the SRN to learn
the structure of the data. The sample consists of
414 million bytes of UTF-8 encoded in a variety
of languages and scripts text. We trained a 400-
hidden-unit SRN, to predict the next byte in the
sequence using backpropagation through time. In-
put bytes were encoded using one-hot representa-
tion. We modified the RNNLM toolkit (Mikolov,
2012a) to record the activations of the hidden layer
and ran it with the default learning rate schedule.
Given that training SRNs on large amounts of text
takes a considerable amount of time we did not
vary the size of the hidden layer. We did try to
filter tweets by language and create specific em-
beddings for English but this had negligible effect
on tweet normalization performance.
The trained SRN language model can be used
to generate random text by sampling the next byte
from its predictive distribution and extending the
string with the result. Figure 1 shows example
strings generated in this way: the network seems
to prefer to output pseudo-tweets written consis-
tently in a single script with words and pseudo-
words mostly from a single language. The gener-
ated byte sequences are valid UTF-8 strings.
In Table 2 in the first column we show the suf-
fix of a string for which the SRN is predicting the
last byte. The rest of each row shows the nearest
neighbors of this string in embedding space, i.e.
should h should d will s will m should a
@justth @neenu @raven @lanae @despic
maybe u maybe y cause i wen i when i
</bodyText>
<tableCaption confidence="0.972466">
Table 2: Nearest neighbors in embedding space.
</tableCaption>
<bodyText confidence="0.997244333333333">
strings for which the SRN is activated in a similar
way when predicting its last byte as measured by
cosine similarity.
</bodyText>
<subsectionHeader confidence="0.999282">
3.2 Normalization datasets
</subsectionHeader>
<bodyText confidence="0.997654857142857">
A difficulty in comparing approaches to tweet nor-
malization is the sparsity of publicly available
datasets. Many authors evaluate on private tweet
collections and/or on the text message corpus of
Choudhury et al. (2007).
For English, Han and Baldwin (2011) created
a small tweet dataset annotated with normalized
variants at the word level. It is hard to inter-
pret the results from Han and Baldwin (2011),
as the evaluation is carried out by assuming that
the words to be normalized are known in ad-
vance: Han et al. (2012) remedy this shortcoming
by evaluating a number of systems without pre-
specifying ill-formed tokens. Another limitation
is that only word-level normalization is covered in
the annotation; e.g. splitting or merging of words
is not allowed. The dataset is also rather small:
549 tweets, which contain 2139 annotated out-
of-vocabulary (OOV) words. Nevertheless, we
use it here for training and evaluating our model.
This dataset does not specify a development/test
split. In order to maximize the size of the training
data while avoiding tuning on test data we use a
split cross-validation setup: we generate 10 cross-
validation folds, and use 5 of them during devel-
opment to evaluate variants of our model. The best
performing configuration is then evaluated on the
remaining 5 cross-validation folds.
</bodyText>
<subsectionHeader confidence="0.999572">
3.3 Model versions
</subsectionHeader>
<bodyText confidence="0.999815545454545">
The simplest way to normalize tweets with a string
transduction model is to treat whole tweets as in-
put sequences. Many other tweet normalization
methods work in a word-wise fashion: they first
identify OOV words and then replace them with
normalized forms. Consequently, publicly avail-
able normalization datasets are annotated at word
level. We can emulate this setup by training the se-
quence labeler on words, instead of whole tweets.
This approach sacrifices some generality, since
transformations involving multiple words cannot
</bodyText>
<page confidence="0.992893">
682
</page>
<bodyText confidence="0.935034">
be learned. However, word-wise models are more
comparable with previous work. We investigated
the following models:
</bodyText>
<listItem confidence="0.999710166666667">
• OOV-ONLY is trained on individual words and
in-vocabulary (IV) words are discarded for
training, and left unchanged for prediction.2
• ALL-WORDS is trained on all words and al-
lowed to change IV words.
• DOCUMENT is trained on whole tweets.
</listItem>
<bodyText confidence="0.998542357142857">
Model OOV-ONLY exploits the setting when the
task is constrained to only normalize words absent
from a reference dictionary, while DOCUMENT is
the one most generally applicable but does not
benefit from any constraints. To keep model size
within manageable limits we reduced the label set
for models ALL-WORDS and DOCUMENT by re-
placing labels which occur less than twice in the
training data with NIL. For OOV-ONLY we were
able to use the full label set. As our sequence la-
beling model we use the Wapiti implementation
of Conditional Random Fields (Lavergne et al.,
2010) with the L-BFGS optimizer and elastic net
regularization with default settings.
</bodyText>
<subsectionHeader confidence="0.742435">
3.4 Features
</subsectionHeader>
<bodyText confidence="0.999992214285714">
We run experiments with two feature sets: N-
GRAM and N-GRAM+SRN. N-GRAM are char-
acter n-grams of size 1–3 in a window of
(−2,+2) around the current position. For the N-
GRAM+SRN feature set we augment N-GRAM with
features derived from the activations of the hidden
units as the SRN is trying to predict the current
character. In order to use the activations in the
CRF model we discretize them as follows. For
each of the K = 10 most active units out of
total J = 400 hidden units, we create features
(f(1) ... f(K)) defined as f(k) = 1 if sj(k) &gt;
0.5 and f(k) = 0 otherwise, where sj(k) returns
the activation of the kth most active unit.
</bodyText>
<subsectionHeader confidence="0.930208">
3.5 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999917625">
As our evaluation metric we use word error rate
(WER) which is defined as the Levenshtein edit
distance between the predicted word sequence tˆ
and the target word sequence t, normalized by the
total number of words in the target string. A more
generally applicable metric would be character er-
ror rate, but we report WERs to make our results
easily comparable with previous work. Since the
</bodyText>
<footnote confidence="0.812263333333333">
2We used the IV/OOV annotations in the Han et al. (2012)
dataset, which are automatically derived from the aspell dic-
tionary.
</footnote>
<table confidence="0.98667725">
Model Features WER (%)
NO-OP 11.7
DOCUMENT NGRAM 6.8
DOCUMENT NGRAM+SRN 5.7
ALL WORDS NGRAM 7.2
ALL WORDS NGRAM+SRN 5.0
OOV-ONLY NGRAM 5.1
OOV-ONLY NGRAM+SRN 4.5
</table>
<tableCaption confidence="0.999567">
Table 3: WERs on development data.
</tableCaption>
<figure confidence="0.7771494">
9 cont continued 5 gon gonna
4 bro brother 4 congrats congratulations
3 yall you 3 pic picture
2 wuz what’s 2 mins minutes
2 juss just 2 fb facebook
</figure>
<tableCaption confidence="0.984479">
Table 4: Improvements from SRN features.
</tableCaption>
<bodyText confidence="0.9994425">
English dataset is pre-tokenized and only covers
word-to-word transformations, this choice has lit-
tle importance here and character error rates show
a similar pattern to word error rates.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999914666666667">
Table 3 shows the results of our development ex-
periments. NO-OP is a baseline which leaves text
unchanged. As expected the most constrained
model OOV-ONLY outperforms the more generic
models on this dataset. For all model variations,
adding SRN features substantially improves per-
formance: the relative error reductions range from
12% for OOV-ONLY to 30% for ALL-WORDS. Ta-
ble 4 shows the non-unique normalizations made
by the OOV-ONLY model with SRN features which
were missed without them. SRN features seem
to be especially useful for learning long-range,
multi-character edits, e.g. fb for facebook.
Table 5 shows the non-unique normalizations
which were missed by the best model: they are
a mixture of relatively standard variations which
happen to be infrequent in our data, like tonite or
gf, and a few idiosyncratic respellings like uu or
bhee. Our supervised approach makes it easy to
address the first type of failure by simply annotat-
ing additional training examples.
Table 6 presents evaluation results of several ap-
proaches reported in Han et al. (2012) as well as
the model which did best in our development ex-
periments. HB-dict is the Internet slang dictio-
nary from Han and Baldwin (2011). GHM-dict
is the automatically constructed dictionary from
</bodyText>
<page confidence="0.998228">
683
</page>
<bodyText confidence="0.978462777777778">
4 1 one 2 withh with
2 uu you 2 tonite tonight
2 thx thanks 2 thiis this
2 smh somehow 2 outta out
2nin 2mam
2 hmwrk homework 2 gf girlfriend
2 fxckin fucking 2 dha the
2 de the 2 d the
2 bhee be 2 bb baby
</bodyText>
<tableCaption confidence="0.998513">
Table 5: Missed transformations.
</tableCaption>
<table confidence="0.999776">
Method WER (%)
NO-OP 11.2
HB-dict 6.6
GHM-dict 7.6
S-dict 9.7
Dict-combo 4.9
Dict-combo+HB-norm 7.9
OOV-ONLY NGRAM+SRN (test) 4.8
</table>
<tableCaption confidence="0.99972">
Table 6: WERs compared to previous work.
</tableCaption>
<bodyText confidence="0.998829692307692">
Gouws et al. (2011); S-dict is the automatically
constructed dictionary from (Han et al., 2012);
Dict-combo are all the dictionaries combined and
Dict-combo+HB-norm are all dictionaries com-
bined with approach of Han and Baldwin (2011).
The WER reported for OOV-ONLY NGRAM+SRN
is on the test folds only. The score on the full
dataset is a bit better: 4.66%. As can be seen our
approach it the best performing approach overall
and in particular it does much better than all of the
single dictionary-based methods. Only the combi-
nation of all the dictionaries comes close in per-
formance.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999983395348838">
In the field of tweet normalization the approach
of Liu et al. (2011, 2012) shows some similarities
to ours: they gather a collection of OOV words
together with their canonical forms from the web
and train a character-level CRF sequence labeler
on the edit sequences computed from these pairs.
They use this as the error model in a noisy-channel
setup combined with a unigram language model.
In addition to character n-gram features they use
phoneme and syllable features, while we rely on
the SRN embeddings to provide generalized rep-
resentations of input strings.
Kaufmann and Kalita (2010) trained a phrase-
based statistical translation model on a parallel
text message corpus and applied it to tweet nor-
malization. In comparison to our first-order linear-
chain CRF, an MT model with reordering is more
flexible but for this reason needs more training
data. It also suffers from language model mis-
match mentioned in Section 2: optimal results
were obtained by using a low weight for the lan-
guage model trained on a balanced text corpus.
Many other approaches to tweet normalization
are more unsupervised in nature (e.g. Han and
Baldwin, 2011; Gouws et al., 2011; Xue et al.,
2011; Han et al., 2012). They still require an-
notated development data for tuning parameters
and a variety of heuristics. Our approach works
well with similar-sized training data, and unlike
unsupervised approaches can easily benefit from
more if it becomes available. Further afield,
our work has connections to research on mor-
phological analysis: for example Chrupała et al.
(2008) use edit scripts to learn lemmatization rules
while Dreyer et al. (2008) propose a discrimina-
tive model for string transductions and apply it
to morphological tasks. While Chrupała (2013)
and Evang et al. (2013) use character-level SRN
text embeddings for learning segmentation, and
recurrent nets themselves have been used for se-
quence transduction (Graves, 2012), to our knowl-
edge neural text embeddings have not been previ-
ously applied to string transduction.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999859">
Learning sequences of edit operations from exam-
ples while incorporating unlabeled data via neu-
ral text embeddings constitutes a compelling ap-
proach to tweet normalization. Our results are es-
pecially interesting considering that we trained on
only a small annotated data set and did not use
any other manually created resources such as dic-
tionaries. We want to push performance further
by expanding the training data and incorporating
existing lexical resources. It will also be impor-
tant to check how our method generalizes to other
language and datasets (e.g. de Clercq et al., 2013;
Alegria et al., 2013).
The general form of our model can be used
in settings where normalization is not limited to
word-to-word transformations. We are planning
to find or create data with such characteristics and
evaluate our approach under these conditions.
</bodyText>
<page confidence="0.998403">
684
</page>
<sectionHeader confidence="0.996293" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999806803738318">
I˜naki Alegria, Nora Aranberri, V´ıctor Fresno, Pablo
Gamallo, Lluis Padr´o, I˜naki San Vicente, Jordi
Turmo, and Arkaitz Zubiaga. 2013. Introducci´on a la
tarea compartida Tweet-Norm 2013: Normalizaci´on
l´exica de tuits en espa˜nol. In Workshop on Tweet
Normalization at SEPLN (Tweet-Norm), pages 36–
45.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the struc-
ture of texting language. International Journal of
Document Analysis and Recognition (IJDAR), 10(3-
4):157–174.
Grzegorz Chrupała. 2013. Text segmentation with
character-level text embeddings. In ICML Workshop
on Deep Learning for Audio, Speech and Language
Processing.
Grzegorz Chrupała, Georgiana Dinu, and Josef
Van Genabith. 2008. Learning morphology with
Morfette. In Proceedings of the 6th edition of the
Language Resources and Evaluation Conference.
Orph´ee de Clercq, Bart Desmet, Sarah Schulz, Els
Lefever, and V´eronique Hoste. 2013. Normaliza-
tion of Dutch user-generated content. In 9th Inter-
national Conference on Recent Advances in Natural
Language Processing (RANLP-2013), pages 179–
188. INCOMA Ltd.
Markus Dreyer, Jason R Smith, and Jason Eisner.
2008. Latent-variable modeling of string transduc-
tions with finite-state methods. In Proceedings of
the conference on empirical methods in natural lan-
guage processing, pages 1080–1089. Association
for Computational Linguistics.
Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179–211.
Jeffrey L Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical struc-
ture. Machine learning, 7(2-3):195–225.
Kilian Evang, Valerio Basile, Grzegorz Chrupała, and
Johan Bos. 2013. Elephant: Sequence labeling
for word and sentence segmentation. In Empirical
Methods in Natural Language Processing.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, pages 82–90. Association
for Computational Linguistics.
Alex Graves. 2012. Sequence transduction with recur-
rent neural networks. arXiv:1211.3711.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 368–378.
Association for Computational Linguistics.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 421–432. Association for
Computational Linguistics.
Max Kaufmann and Jugal Kalita. 2010. Syntactic
normalization of Twitter messages. In Interna-
tional conference on natural language processing,
Kharagpur, India.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289. Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA.
Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 504–513. As-
sociation for Computational Linguistics.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 1035–1044. As-
sociation for Computational Linguistics.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution?: normaliz-
ing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 71–76. Association for Computa-
tional Linguistics.
Tom´a&amp;quot;s Mikolov. 2012a. Recurrent neural network lan-
guage models. http://rnnlm.org.
Tom´a&amp;quot;s Mikolov. 2012b. Statistical language models
based on neural networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Tom´a&amp;quot;s Mikolov, Martin Karafi´at, Luk´a&amp;quot;s Burget, Jan
&amp;quot;Cernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech, pages 1045–1048.
Webb Miller and Eugene W Myers. 1985. A file com-
parison program. Software: Practice and Experi-
ence, 15(11):1025–1040.
Eugene W Myers. 1986. An O(ND) difference algo-
rithm and its variations. Algorithmica, 1(1-4):251–
266.
</reference>
<page confidence="0.984979">
685
</page>
<reference confidence="0.998928875">
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017–1024.
Zhenzhen Xue, Dawei Yin, and Brian D Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI-
11 Workshop on Analyzing Microtext, pages 74–79.
</reference>
<page confidence="0.998735">
686
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.505209">
<title confidence="0.996407">Normalizing tweets with edit scripts and recurrent neural embeddings</title>
<author confidence="0.830683">Grzegorz</author>
<affiliation confidence="0.90985">Tilburg Center for Cognition and</affiliation>
<address confidence="0.59597">Tilburg</address>
<email confidence="0.99473">g.chrupala@uvt.nl</email>
<abstract confidence="0.99904955">Tweets often contain a large proportion of abbreviations, alternative spellings, novel words and other non-canonical language. These features are problematic for standard language analysis tools and it can be desirable to convert them to canonical form. We propose a novel text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embeddings. The text embeddings are generated using an Simple Recurrent Network. We find that enriching the feature set with text embeddings substantially lowers word error rates on an English tweet normalization dataset. Our model improves on stateof-the-art with little training data and without any lexical resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I˜naki Alegria</author>
<author>Nora Aranberri</author>
</authors>
<title>V´ıctor Fresno, Pablo Gamallo, Lluis Padr´o, I˜naki San Vicente, Jordi Turmo, and Arkaitz Zubiaga.</title>
<date>2013</date>
<booktitle>In Workshop on Tweet Normalization at SEPLN (Tweet-Norm),</booktitle>
<pages>36--45</pages>
<marker>Alegria, Aranberri, 2013</marker>
<rawString>I˜naki Alegria, Nora Aranberri, V´ıctor Fresno, Pablo Gamallo, Lluis Padr´o, I˜naki San Vicente, Jordi Turmo, and Arkaitz Zubiaga. 2013. Introducci´on a la tarea compartida Tweet-Norm 2013: Normalizaci´on l´exica de tuits en espa˜nol. In Workshop on Tweet Normalization at SEPLN (Tweet-Norm), pages 36– 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>International Journal of Document Analysis and Recognition (IJDAR),</journal>
<pages>10--3</pages>
<contexts>
<context position="10662" citStr="Choudhury et al. (2007)" startWordPosition="1794" endWordPosition="1797">byte. The rest of each row shows the nearest neighbors of this string in embedding space, i.e. should h should d will s will m should a @justth @neenu @raven @lanae @despic maybe u maybe y cause i wen i when i Table 2: Nearest neighbors in embedding space. strings for which the SRN is activated in a similar way when predicting its last byte as measured by cosine similarity. 3.2 Normalization datasets A difficulty in comparing approaches to tweet normalization is the sparsity of publicly available datasets. Many authors evaluate on private tweet collections and/or on the text message corpus of Choudhury et al. (2007). For English, Han and Baldwin (2011) created a small tweet dataset annotated with normalized variants at the word level. It is hard to interpret the results from Han and Baldwin (2011), as the evaluation is carried out by assuming that the words to be normalized are known in advance: Han et al. (2012) remedy this shortcoming by evaluating a number of systems without prespecifying ill-formed tokens. Another limitation is that only word-level normalization is covered in the annotation; e.g. splitting or merging of words is not allowed. The dataset is also rather small: 549 tweets, which contain</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. International Journal of Document Analysis and Recognition (IJDAR), 10(3-4):157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
</authors>
<title>Text segmentation with character-level text embeddings.</title>
<date>2013</date>
<booktitle>In ICML Workshop on Deep Learning for Audio, Speech and Language Processing.</booktitle>
<contexts>
<context position="2356" citStr="Chrupała, 2013" startWordPosition="364" endWordPosition="365"> but they typically use many adjustable parameters which need to be tuned on some annotated data. In this work we suggest a simple, supervised characterlevel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated following recent work on using character-level text embeddings for text segmentation (Chrupała, 2013), and word and sentence boundary detection (Evang et al., 2013). We train a recurrent neural network language model (Mikolov et al., 2010; Mikolov, 2012b) on a large collection of tweets. When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model. The principal contributions of our work are: (i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we sho</context>
<context position="7739" citStr="Chrupała (2013)" startWordPosition="1287" endWordPosition="1288">abilities for the next character. The activation sj of a hidden unit j is a function of the current input and the state of the hidden layer at the previous time step: t − 1: �sj(t − 1)Wjl where a is the sigmoid function and Uji is the weight between input component i and hidden unit j, while Wjl is the weight between hidden unit l at time t − 1 and hidden unit j at time t. The representation of recent history is stored in a limited number of recurrently connected hidden units. This forces the network to make the representation compressed and abstract rather than just memorize literal history. Chrupała (2013) and Evang et al. (2013) show that these text embeddings can be useful as features in textual segmentation tasks. We use them to bring in information from unlabeled data into our string transduction model and then train a character-level SRN language model on unlabeled tweets. We run the trained model on I L sj(t) = a wi(t)Uji + i=1 l=1 681 Figure 1: Tweets randomly generated with an SRN new tweets and record the activation of the hidden layer at each position as the model predicts the next character. These activation vectors form our text embeddings: they are discretized and used as input fea</context>
<context position="19125" citStr="Chrupała (2013)" startWordPosition="3204" endWordPosition="3205">dwin, 2011; Gouws et al., 2011; Xue et al., 2011; Han et al., 2012). They still require annotated development data for tuning parameters and a variety of heuristics. Our approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available. Further afield, our work has connections to research on morphological analysis: for example Chrupała et al. (2008) use edit scripts to learn lemmatization rules while Dreyer et al. (2008) propose a discriminative model for string transductions and apply it to morphological tasks. While Chrupała (2013) and Evang et al. (2013) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction (Graves, 2012), to our knowledge neural text embeddings have not been previously applied to string transduction. 6 Conclusion Learning sequences of edit operations from examples while incorporating unlabeled data via neural text embeddings constitutes a compelling approach to tweet normalization. Our results are especially interesting considering that we trained on only a small annotated data set and did not use any other manually cr</context>
</contexts>
<marker>Chrupała, 2013</marker>
<rawString>Grzegorz Chrupała. 2013. Text segmentation with character-level text embeddings. In ICML Workshop on Deep Learning for Audio, Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
<author>Georgiana Dinu</author>
<author>Josef Van Genabith</author>
</authors>
<title>Learning morphology with Morfette.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th edition of the Language Resources and Evaluation Conference.</booktitle>
<marker>Chrupała, Dinu, Van Genabith, 2008</marker>
<rawString>Grzegorz Chrupała, Georgiana Dinu, and Josef Van Genabith. 2008. Learning morphology with Morfette. In Proceedings of the 6th edition of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Orph´ee de Clercq</author>
<author>Bart Desmet</author>
<author>Sarah Schulz</author>
<author>Els Lefever</author>
<author>V´eronique Hoste</author>
</authors>
<title>Normalization of Dutch user-generated content.</title>
<date>2013</date>
<booktitle>In 9th International Conference on Recent Advances in Natural Language Processing (RANLP-2013),</booktitle>
<pages>179--188</pages>
<publisher>INCOMA Ltd.</publisher>
<marker>de Clercq, Desmet, Schulz, Lefever, Hoste, 2013</marker>
<rawString>Orph´ee de Clercq, Bart Desmet, Sarah Schulz, Els Lefever, and V´eronique Hoste. 2013. Normalization of Dutch user-generated content. In 9th International Conference on Recent Advances in Natural Language Processing (RANLP-2013), pages 179– 188. INCOMA Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason R Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Latent-variable modeling of string transductions with finite-state methods.</title>
<date>2008</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<pages>1080--1089</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19010" citStr="Dreyer et al. (2008)" startWordPosition="3185" endWordPosition="3188">n a balanced text corpus. Many other approaches to tweet normalization are more unsupervised in nature (e.g. Han and Baldwin, 2011; Gouws et al., 2011; Xue et al., 2011; Han et al., 2012). They still require annotated development data for tuning parameters and a variety of heuristics. Our approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available. Further afield, our work has connections to research on morphological analysis: for example Chrupała et al. (2008) use edit scripts to learn lemmatization rules while Dreyer et al. (2008) propose a discriminative model for string transductions and apply it to morphological tasks. While Chrupała (2013) and Evang et al. (2013) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction (Graves, 2012), to our knowledge neural text embeddings have not been previously applied to string transduction. 6 Conclusion Learning sequences of edit operations from examples while incorporating unlabeled data via neural text embeddings constitutes a compelling approach to tweet normalization. Our results are especial</context>
</contexts>
<marker>Dreyer, Smith, Eisner, 2008</marker>
<rawString>Markus Dreyer, Jason R Smith, and Jason Eisner. 2008. Latent-variable modeling of string transductions with finite-state methods. In Proceedings of the conference on empirical methods in natural language processing, pages 1080–1089. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="6182" citStr="Elman (1990)" startWordPosition="1016" endWordPosition="1017">itional Random Fields (Lafferty et al., 2001). Once trained the model is used to label new strings and the predicted edit script is applied to the input string producing the normalized output string. Given source string s the predicted target string tˆ 1The input string is extended with an empty symbol to account for the cases where an insertion is needed at the end of the string. where e = ses(s, t) is the shortest edit script mapping s to t. P(e|s) is modeled with a linear-chain Conditional Random Field. 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as models of temporal, or sequential, structure in data, including linguistic data (Elman, 1991). More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models (Mikolov et al., 2010; Mikolov, 2012b). Another version of recurrent neural nets has been used to generate plausible text with a character-level language model (Sutskever et al., 2011). We use SRNs to induce character-level text representations from unlabeled Twitter data to use as features in the string transduction model. The units in the hidden layer at time t receive</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Distributed representations, simple recurrent networks, and grammatical structure.</title>
<date>1991</date>
<booktitle>Machine learning,</booktitle>
<pages>7--2</pages>
<contexts>
<context position="6279" citStr="Elman, 1991" startWordPosition="1030" endWordPosition="1031">s and the predicted edit script is applied to the input string producing the normalized output string. Given source string s the predicted target string tˆ 1The input string is extended with an empty symbol to account for the cases where an insertion is needed at the end of the string. where e = ses(s, t) is the shortest edit script mapping s to t. P(e|s) is modeled with a linear-chain Conditional Random Field. 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as models of temporal, or sequential, structure in data, including linguistic data (Elman, 1991). More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models (Mikolov et al., 2010; Mikolov, 2012b). Another version of recurrent neural nets has been used to generate plausible text with a character-level language model (Sutskever et al., 2011). We use SRNs to induce character-level text representations from unlabeled Twitter data to use as features in the string transduction model. The units in the hidden layer at time t receive connections from input units at time t and also from the hidden units at the previous time step </context>
</contexts>
<marker>Elman, 1991</marker>
<rawString>Jeffrey L Elman. 1991. Distributed representations, simple recurrent networks, and grammatical structure. Machine learning, 7(2-3):195–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Evang</author>
<author>Valerio Basile</author>
<author>Grzegorz Chrupała</author>
<author>Johan Bos</author>
</authors>
<title>Elephant: Sequence labeling for word and sentence segmentation.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2419" citStr="Evang et al., 2013" startWordPosition="372" endWordPosition="375">ed to be tuned on some annotated data. In this work we suggest a simple, supervised characterlevel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated following recent work on using character-level text embeddings for text segmentation (Chrupała, 2013), and word and sentence boundary detection (Evang et al., 2013). We train a recurrent neural network language model (Mikolov et al., 2010; Mikolov, 2012b) on a large collection of tweets. When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model. The principal contributions of our work are: (i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that characterlevel neural text embeddings can be used to eff</context>
<context position="7763" citStr="Evang et al. (2013)" startWordPosition="1290" endWordPosition="1293">xt character. The activation sj of a hidden unit j is a function of the current input and the state of the hidden layer at the previous time step: t − 1: �sj(t − 1)Wjl where a is the sigmoid function and Uji is the weight between input component i and hidden unit j, while Wjl is the weight between hidden unit l at time t − 1 and hidden unit j at time t. The representation of recent history is stored in a limited number of recurrently connected hidden units. This forces the network to make the representation compressed and abstract rather than just memorize literal history. Chrupała (2013) and Evang et al. (2013) show that these text embeddings can be useful as features in textual segmentation tasks. We use them to bring in information from unlabeled data into our string transduction model and then train a character-level SRN language model on unlabeled tweets. We run the trained model on I L sj(t) = a wi(t)Uji + i=1 l=1 681 Figure 1: Tweets randomly generated with an SRN new tweets and record the activation of the hidden layer at each position as the model predicts the next character. These activation vectors form our text embeddings: they are discretized and used as input features to the supervised </context>
<context position="19149" citStr="Evang et al. (2013)" startWordPosition="3207" endWordPosition="3210"> al., 2011; Xue et al., 2011; Han et al., 2012). They still require annotated development data for tuning parameters and a variety of heuristics. Our approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available. Further afield, our work has connections to research on morphological analysis: for example Chrupała et al. (2008) use edit scripts to learn lemmatization rules while Dreyer et al. (2008) propose a discriminative model for string transductions and apply it to morphological tasks. While Chrupała (2013) and Evang et al. (2013) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction (Graves, 2012), to our knowledge neural text embeddings have not been previously applied to string transduction. 6 Conclusion Learning sequences of edit operations from examples while incorporating unlabeled data via neural text embeddings constitutes a compelling approach to tweet normalization. Our results are especially interesting considering that we trained on only a small annotated data set and did not use any other manually created resources such as </context>
</contexts>
<marker>Evang, Basile, Chrupała, Bos, 2013</marker>
<rawString>Kilian Evang, Valerio Basile, Grzegorz Chrupała, and Johan Bos. 2013. Elephant: Sequence labeling for word and sentence segmentation. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Dirk Hovy</author>
<author>Donald Metzler</author>
</authors>
<title>Unsupervised mining of lexical variants from noisy text.</title>
<date>2011</date>
<booktitle>In Proceedings of the First workshop on Unsupervised Learning in NLP,</booktitle>
<pages>82--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1703" citStr="Gouws et al. (2011)" startWordPosition="261" endWordPosition="264">om formal to internet slang. Substantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content. One approach has been text normalization, i.e. transforming tweet text into a more canonical form which standard NLP tools expect. A multitude of resources and approaches have been used to deal with normalization: handcrafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof. Methods such as those of Han and Baldwin (2011), Liu et al. (2011), Gouws et al. (2011) or Han et al. (2012) are unsupervised but they typically use many adjustable parameters which need to be tuned on some annotated data. In this work we suggest a simple, supervised characterlevel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated following recent work on using character-level t</context>
<context position="16797" citStr="Gouws et al. (2011)" startWordPosition="2822" endWordPosition="2825"> the model which did best in our development experiments. HB-dict is the Internet slang dictionary from Han and Baldwin (2011). GHM-dict is the automatically constructed dictionary from 683 4 1 one 2 withh with 2 uu you 2 tonite tonight 2 thx thanks 2 thiis this 2 smh somehow 2 outta out 2nin 2mam 2 hmwrk homework 2 gf girlfriend 2 fxckin fucking 2 dha the 2 de the 2 d the 2 bhee be 2 bb baby Table 5: Missed transformations. Method WER (%) NO-OP 11.2 HB-dict 6.6 GHM-dict 7.6 S-dict 9.7 Dict-combo 4.9 Dict-combo+HB-norm 7.9 OOV-ONLY NGRAM+SRN (test) 4.8 Table 6: WERs compared to previous work. Gouws et al. (2011); S-dict is the automatically constructed dictionary from (Han et al., 2012); Dict-combo are all the dictionaries combined and Dict-combo+HB-norm are all dictionaries combined with approach of Han and Baldwin (2011). The WER reported for OOV-ONLY NGRAM+SRN is on the test folds only. The score on the full dataset is a bit better: 4.66%. As can be seen our approach it the best performing approach overall and in particular it does much better than all of the single dictionary-based methods. Only the combination of all the dictionaries comes close in performance. 5 Related work In the field of twe</context>
<context position="18540" citStr="Gouws et al., 2011" startWordPosition="3111" endWordPosition="3114">ons of input strings. Kaufmann and Kalita (2010) trained a phrasebased statistical translation model on a parallel text message corpus and applied it to tweet normalization. In comparison to our first-order linearchain CRF, an MT model with reordering is more flexible but for this reason needs more training data. It also suffers from language model mismatch mentioned in Section 2: optimal results were obtained by using a low weight for the language model trained on a balanced text corpus. Many other approaches to tweet normalization are more unsupervised in nature (e.g. Han and Baldwin, 2011; Gouws et al., 2011; Xue et al., 2011; Han et al., 2012). They still require annotated development data for tuning parameters and a variety of heuristics. Our approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available. Further afield, our work has connections to research on morphological analysis: for example Chrupała et al. (2008) use edit scripts to learn lemmatization rules while Dreyer et al. (2008) propose a discriminative model for string transductions and apply it to morphological tasks. While Chrupała (2013) and Evang et a</context>
</contexts>
<marker>Gouws, Hovy, Metzler, 2011</marker>
<rawString>Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011. Unsupervised mining of lexical variants from noisy text. In Proceedings of the First workshop on Unsupervised Learning in NLP, pages 82–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Sequence transduction with recurrent neural networks.</title>
<date>2012</date>
<tech>arXiv:1211.3711.</tech>
<contexts>
<context position="19302" citStr="Graves, 2012" startWordPosition="3230" endWordPosition="3231">works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available. Further afield, our work has connections to research on morphological analysis: for example Chrupała et al. (2008) use edit scripts to learn lemmatization rules while Dreyer et al. (2008) propose a discriminative model for string transductions and apply it to morphological tasks. While Chrupała (2013) and Evang et al. (2013) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction (Graves, 2012), to our knowledge neural text embeddings have not been previously applied to string transduction. 6 Conclusion Learning sequences of edit operations from examples while incorporating unlabeled data via neural text embeddings constitutes a compelling approach to tweet normalization. Our results are especially interesting considering that we trained on only a small annotated data set and did not use any other manually created resources such as dictionaries. We want to push performance further by expanding the training data and incorporating existing lexical resources. It will also be important </context>
</contexts>
<marker>Graves, 2012</marker>
<rawString>Alex Graves. 2012. Sequence transduction with recurrent neural networks. arXiv:1211.3711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a# twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>368--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1663" citStr="Han and Baldwin (2011)" startWordPosition="252" endWordPosition="256">nd writing systems, in registers ranging from formal to internet slang. Substantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content. One approach has been text normalization, i.e. transforming tweet text into a more canonical form which standard NLP tools expect. A multitude of resources and approaches have been used to deal with normalization: handcrafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof. Methods such as those of Han and Baldwin (2011), Liu et al. (2011), Gouws et al. (2011) or Han et al. (2012) are unsupervised but they typically use many adjustable parameters which need to be tuned on some annotated data. In this work we suggest a simple, supervised characterlevel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated followin</context>
<context position="10699" citStr="Han and Baldwin (2011)" startWordPosition="1800" endWordPosition="1803">earest neighbors of this string in embedding space, i.e. should h should d will s will m should a @justth @neenu @raven @lanae @despic maybe u maybe y cause i wen i when i Table 2: Nearest neighbors in embedding space. strings for which the SRN is activated in a similar way when predicting its last byte as measured by cosine similarity. 3.2 Normalization datasets A difficulty in comparing approaches to tweet normalization is the sparsity of publicly available datasets. Many authors evaluate on private tweet collections and/or on the text message corpus of Choudhury et al. (2007). For English, Han and Baldwin (2011) created a small tweet dataset annotated with normalized variants at the word level. It is hard to interpret the results from Han and Baldwin (2011), as the evaluation is carried out by assuming that the words to be normalized are known in advance: Han et al. (2012) remedy this shortcoming by evaluating a number of systems without prespecifying ill-formed tokens. Another limitation is that only word-level normalization is covered in the annotation; e.g. splitting or merging of words is not allowed. The dataset is also rather small: 549 tweets, which contain 2139 annotated outof-vocabulary (OOV</context>
<context position="16304" citStr="Han and Baldwin (2011)" startWordPosition="2730" endWordPosition="2733"> e.g. fb for facebook. Table 5 shows the non-unique normalizations which were missed by the best model: they are a mixture of relatively standard variations which happen to be infrequent in our data, like tonite or gf, and a few idiosyncratic respellings like uu or bhee. Our supervised approach makes it easy to address the first type of failure by simply annotating additional training examples. Table 6 presents evaluation results of several approaches reported in Han et al. (2012) as well as the model which did best in our development experiments. HB-dict is the Internet slang dictionary from Han and Baldwin (2011). GHM-dict is the automatically constructed dictionary from 683 4 1 one 2 withh with 2 uu you 2 tonite tonight 2 thx thanks 2 thiis this 2 smh somehow 2 outta out 2nin 2mam 2 hmwrk homework 2 gf girlfriend 2 fxckin fucking 2 dha the 2 de the 2 d the 2 bhee be 2 bb baby Table 5: Missed transformations. Method WER (%) NO-OP 11.2 HB-dict 6.6 GHM-dict 7.6 S-dict 9.7 Dict-combo 4.9 Dict-combo+HB-norm 7.9 OOV-ONLY NGRAM+SRN (test) 4.8 Table 6: WERs compared to previous work. Gouws et al. (2011); S-dict is the automatically constructed dictionary from (Han et al., 2012); Dict-combo are all the dictio</context>
<context position="18520" citStr="Han and Baldwin, 2011" startWordPosition="3107" endWordPosition="3110">eneralized representations of input strings. Kaufmann and Kalita (2010) trained a phrasebased statistical translation model on a parallel text message corpus and applied it to tweet normalization. In comparison to our first-order linearchain CRF, an MT model with reordering is more flexible but for this reason needs more training data. It also suffers from language model mismatch mentioned in Section 2: optimal results were obtained by using a low weight for the language model trained on a balanced text corpus. Many other approaches to tweet normalization are more unsupervised in nature (e.g. Han and Baldwin, 2011; Gouws et al., 2011; Xue et al., 2011; Han et al., 2012). They still require annotated development data for tuning parameters and a variety of heuristics. Our approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available. Further afield, our work has connections to research on morphological analysis: for example Chrupała et al. (2008) use edit scripts to learn lemmatization rules while Dreyer et al. (2008) propose a discriminative model for string transductions and apply it to morphological tasks. While Chrupała (</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a# twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 368–378. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>421--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1724" citStr="Han et al. (2012)" startWordPosition="266" endWordPosition="269">lang. Substantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content. One approach has been text normalization, i.e. transforming tweet text into a more canonical form which standard NLP tools expect. A multitude of resources and approaches have been used to deal with normalization: handcrafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof. Methods such as those of Han and Baldwin (2011), Liu et al. (2011), Gouws et al. (2011) or Han et al. (2012) are unsupervised but they typically use many adjustable parameters which need to be tuned on some annotated data. In this work we suggest a simple, supervised characterlevel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated following recent work on using character-level text embeddings for te</context>
<context position="10965" citStr="Han et al. (2012)" startWordPosition="1849" endWordPosition="1852">way when predicting its last byte as measured by cosine similarity. 3.2 Normalization datasets A difficulty in comparing approaches to tweet normalization is the sparsity of publicly available datasets. Many authors evaluate on private tweet collections and/or on the text message corpus of Choudhury et al. (2007). For English, Han and Baldwin (2011) created a small tweet dataset annotated with normalized variants at the word level. It is hard to interpret the results from Han and Baldwin (2011), as the evaluation is carried out by assuming that the words to be normalized are known in advance: Han et al. (2012) remedy this shortcoming by evaluating a number of systems without prespecifying ill-formed tokens. Another limitation is that only word-level normalization is covered in the annotation; e.g. splitting or merging of words is not allowed. The dataset is also rather small: 549 tweets, which contain 2139 annotated outof-vocabulary (OOV) words. Nevertheless, we use it here for training and evaluating our model. This dataset does not specify a development/test split. In order to maximize the size of the training data while avoiding tuning on test data we use a split cross-validation setup: we gener</context>
<context position="14447" citStr="Han et al. (2012)" startWordPosition="2427" endWordPosition="2430">features (f(1) ... f(K)) defined as f(k) = 1 if sj(k) &gt; 0.5 and f(k) = 0 otherwise, where sj(k) returns the activation of the kth most active unit. 3.5 Evaluation metrics As our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequence tˆ and the target word sequence t, normalized by the total number of words in the target string. A more generally applicable metric would be character error rate, but we report WERs to make our results easily comparable with previous work. Since the 2We used the IV/OOV annotations in the Han et al. (2012) dataset, which are automatically derived from the aspell dictionary. Model Features WER (%) NO-OP 11.7 DOCUMENT NGRAM 6.8 DOCUMENT NGRAM+SRN 5.7 ALL WORDS NGRAM 7.2 ALL WORDS NGRAM+SRN 5.0 OOV-ONLY NGRAM 5.1 OOV-ONLY NGRAM+SRN 4.5 Table 3: WERs on development data. 9 cont continued 5 gon gonna 4 bro brother 4 congrats congratulations 3 yall you 3 pic picture 2 wuz what’s 2 mins minutes 2 juss just 2 fb facebook Table 4: Improvements from SRN features. English dataset is pre-tokenized and only covers word-to-word transformations, this choice has little importance here and character error rates</context>
<context position="16167" citStr="Han et al. (2012)" startWordPosition="2705" endWordPosition="2708">N features which were missed without them. SRN features seem to be especially useful for learning long-range, multi-character edits, e.g. fb for facebook. Table 5 shows the non-unique normalizations which were missed by the best model: they are a mixture of relatively standard variations which happen to be infrequent in our data, like tonite or gf, and a few idiosyncratic respellings like uu or bhee. Our supervised approach makes it easy to address the first type of failure by simply annotating additional training examples. Table 6 presents evaluation results of several approaches reported in Han et al. (2012) as well as the model which did best in our development experiments. HB-dict is the Internet slang dictionary from Han and Baldwin (2011). GHM-dict is the automatically constructed dictionary from 683 4 1 one 2 withh with 2 uu you 2 tonite tonight 2 thx thanks 2 thiis this 2 smh somehow 2 outta out 2nin 2mam 2 hmwrk homework 2 gf girlfriend 2 fxckin fucking 2 dha the 2 de the 2 d the 2 bhee be 2 bb baby Table 5: Missed transformations. Method WER (%) NO-OP 11.2 HB-dict 6.6 GHM-dict 7.6 S-dict 9.7 Dict-combo 4.9 Dict-combo+HB-norm 7.9 OOV-ONLY NGRAM+SRN (test) 4.8 Table 6: WERs compared to prev</context>
<context position="18577" citStr="Han et al., 2012" startWordPosition="3119" endWordPosition="3122">ita (2010) trained a phrasebased statistical translation model on a parallel text message corpus and applied it to tweet normalization. In comparison to our first-order linearchain CRF, an MT model with reordering is more flexible but for this reason needs more training data. It also suffers from language model mismatch mentioned in Section 2: optimal results were obtained by using a low weight for the language model trained on a balanced text corpus. Many other approaches to tweet normalization are more unsupervised in nature (e.g. Han and Baldwin, 2011; Gouws et al., 2011; Xue et al., 2011; Han et al., 2012). They still require annotated development data for tuning parameters and a variety of heuristics. Our approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available. Further afield, our work has connections to research on morphological analysis: for example Chrupała et al. (2008) use edit scripts to learn lemmatization rules while Dreyer et al. (2008) propose a discriminative model for string transductions and apply it to morphological tasks. While Chrupała (2013) and Evang et al. (2013) use character-level SRN tex</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Kaufmann</author>
<author>Jugal Kalita</author>
</authors>
<title>Syntactic normalization of Twitter messages.</title>
<date>2010</date>
<booktitle>In International conference on natural language processing,</booktitle>
<location>Kharagpur, India.</location>
<contexts>
<context position="17970" citStr="Kaufmann and Kalita (2010)" startWordPosition="3015" endWordPosition="3018">se in performance. 5 Related work In the field of tweet normalization the approach of Liu et al. (2011, 2012) shows some similarities to ours: they gather a collection of OOV words together with their canonical forms from the web and train a character-level CRF sequence labeler on the edit sequences computed from these pairs. They use this as the error model in a noisy-channel setup combined with a unigram language model. In addition to character n-gram features they use phoneme and syllable features, while we rely on the SRN embeddings to provide generalized representations of input strings. Kaufmann and Kalita (2010) trained a phrasebased statistical translation model on a parallel text message corpus and applied it to tweet normalization. In comparison to our first-order linearchain CRF, an MT model with reordering is more flexible but for this reason needs more training data. It also suffers from language model mismatch mentioned in Section 2: optimal results were obtained by using a low weight for the language model trained on a balanced text corpus. Many other approaches to tweet normalization are more unsupervised in nature (e.g. Han and Baldwin, 2011; Gouws et al., 2011; Xue et al., 2011; Han et al.</context>
</contexts>
<marker>Kaufmann, Kalita, 2010</marker>
<rawString>Max Kaufmann and Jugal Kalita. 2010. Syntactic normalization of Twitter messages. In International conference on natural language processing, Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="2222" citStr="Lafferty et al., 2001" startWordPosition="344" endWordPosition="347">ations thereof. Methods such as those of Han and Baldwin (2011), Liu et al. (2011), Gouws et al. (2011) or Han et al. (2012) are unsupervised but they typically use many adjustable parameters which need to be tuned on some annotated data. In this work we suggest a simple, supervised characterlevel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated following recent work on using character-level text embeddings for text segmentation (Chrupała, 2013), and word and sentence boundary detection (Evang et al., 2013). We train a recurrent neural network language model (Mikolov et al., 2010; Mikolov, 2012b) on a large collection of tweets. When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model. The principal contributions of our work are: (i) we show that a discriminative sequence labelin</context>
<context position="5615" citStr="Lafferty et al., 2001" startWordPosition="915" endWordPosition="918">nd using the DIFF algorithm (Miller and Myers, 1985; Myers, 1986). Our version of DIFF uses the following types of edits: • NIL – no edits, • DEL – delete character at this position, • INS(·) – insert specified string before character at this position.1 Table 1 shows a shortest edit script for the pair of strings (c wat, see what). We use a sequence labeling model to learn to label input strings with edit scripts. The training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings. As a sequence labeler we use Conditional Random Fields (Lafferty et al., 2001). Once trained the model is used to label new strings and the predicted edit script is applied to the input string producing the normalized output string. Given source string s the predicted target string tˆ 1The input string is extended with an empty symbol to account for the cases where an insertion is needed at the end of the string. where e = ses(s, t) is the shortest edit script mapping s to t. P(e|s) is modeled with a linear-chain Conditional Random Field. 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as models of temporal, or sequen</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>504--513</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. 2010. Practical very large scale CRFs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Xiao Jiang</author>
</authors>
<title>A broadcoverage normalization system for social media language.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>1035--1044</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Liu, Weng, Jiang, 2012</marker>
<rawString>Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broadcoverage normalization system for social media language. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 1035–1044. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Bingqing Wang</author>
<author>Yang Liu</author>
</authors>
<title>Insertion, deletion, or substitution?: normalizing text messages without pre-categorization nor supervision.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papersVolume 2,</booktitle>
<pages>71--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1682" citStr="Liu et al. (2011)" startWordPosition="257" endWordPosition="260">egisters ranging from formal to internet slang. Substantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content. One approach has been text normalization, i.e. transforming tweet text into a more canonical form which standard NLP tools expect. A multitude of resources and approaches have been used to deal with normalization: handcrafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof. Methods such as those of Han and Baldwin (2011), Liu et al. (2011), Gouws et al. (2011) or Han et al. (2012) are unsupervised but they typically use many adjustable parameters which need to be tuned on some annotated data. In this work we suggest a simple, supervised characterlevel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated following recent work on us</context>
<context position="17446" citStr="Liu et al. (2011" startWordPosition="2931" endWordPosition="2934">structed dictionary from (Han et al., 2012); Dict-combo are all the dictionaries combined and Dict-combo+HB-norm are all dictionaries combined with approach of Han and Baldwin (2011). The WER reported for OOV-ONLY NGRAM+SRN is on the test folds only. The score on the full dataset is a bit better: 4.66%. As can be seen our approach it the best performing approach overall and in particular it does much better than all of the single dictionary-based methods. Only the combination of all the dictionaries comes close in performance. 5 Related work In the field of tweet normalization the approach of Liu et al. (2011, 2012) shows some similarities to ours: they gather a collection of OOV words together with their canonical forms from the web and train a character-level CRF sequence labeler on the edit sequences computed from these pairs. They use this as the error model in a noisy-channel setup combined with a unigram language model. In addition to character n-gram features they use phoneme and syllable features, while we rely on the SRN embeddings to provide generalized representations of input strings. Kaufmann and Kalita (2010) trained a phrasebased statistical translation model on a parallel text mess</context>
</contexts>
<marker>Liu, Weng, Wang, Liu, 2011</marker>
<rawString>Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011. Insertion, deletion, or substitution?: normalizing text messages without pre-categorization nor supervision. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papersVolume 2, pages 71–76. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Mikolov</author>
</authors>
<title>Recurrent neural network language models.</title>
<date>2012</date>
<note>http://rnnlm.org.</note>
<contexts>
<context position="2508" citStr="Mikolov, 2012" startWordPosition="388" endWordPosition="389">vel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated following recent work on using character-level text embeddings for text segmentation (Chrupała, 2013), and word and sentence boundary detection (Evang et al., 2013). We train a recurrent neural network language model (Mikolov et al., 2010; Mikolov, 2012b) on a large collection of tweets. When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model. The principal contributions of our work are: (i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that characterlevel neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially</context>
<context position="6445" citStr="Mikolov, 2012" startWordPosition="1056" endWordPosition="1057">t string is extended with an empty symbol to account for the cases where an insertion is needed at the end of the string. where e = ses(s, t) is the shortest edit script mapping s to t. P(e|s) is modeled with a linear-chain Conditional Random Field. 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as models of temporal, or sequential, structure in data, including linguistic data (Elman, 1991). More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models (Mikolov et al., 2010; Mikolov, 2012b). Another version of recurrent neural nets has been used to generate plausible text with a character-level language model (Sutskever et al., 2011). We use SRNs to induce character-level text representations from unlabeled Twitter data to use as features in the string transduction model. The units in the hidden layer at time t receive connections from input units at time t and also from the hidden units at the previous time step t − 1. The hidden layer predicts the state of the output units at the next time step t + 1. The input vector w(t) represents the input element at current time step, h</context>
<context position="9127" citStr="Mikolov, 2012" startWordPosition="1529" endWordPosition="1530">s, and using bytes rather than characters as basic units. 3.1 Unlabeled tweets In order to train our SRN language model we collected a set of tweets using the Twitter sampling API. We use the raw sample directly without filtering it in any way, relying on the SRN to learn the structure of the data. The sample consists of 414 million bytes of UTF-8 encoded in a variety of languages and scripts text. We trained a 400- hidden-unit SRN, to predict the next byte in the sequence using backpropagation through time. Input bytes were encoded using one-hot representation. We modified the RNNLM toolkit (Mikolov, 2012a) to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer. We did try to filter tweets by language and create specific embeddings for English but this had negligible effect on tweet normalization performance. The trained SRN language model can be used to generate random text by sampling the next byte from its predictive distribution and extending the string with the result. Figure 1 shows example strings generated in this </context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom´a&amp;quot;s Mikolov. 2012a. Recurrent neural network language models. http://rnnlm.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Mikolov</author>
</authors>
<title>Statistical language models based on neural networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="2508" citStr="Mikolov, 2012" startWordPosition="388" endWordPosition="389">vel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated following recent work on using character-level text embeddings for text segmentation (Chrupała, 2013), and word and sentence boundary detection (Evang et al., 2013). We train a recurrent neural network language model (Mikolov et al., 2010; Mikolov, 2012b) on a large collection of tweets. When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model. The principal contributions of our work are: (i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that characterlevel neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially</context>
<context position="6445" citStr="Mikolov, 2012" startWordPosition="1056" endWordPosition="1057">t string is extended with an empty symbol to account for the cases where an insertion is needed at the end of the string. where e = ses(s, t) is the shortest edit script mapping s to t. P(e|s) is modeled with a linear-chain Conditional Random Field. 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as models of temporal, or sequential, structure in data, including linguistic data (Elman, 1991). More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models (Mikolov et al., 2010; Mikolov, 2012b). Another version of recurrent neural nets has been used to generate plausible text with a character-level language model (Sutskever et al., 2011). We use SRNs to induce character-level text representations from unlabeled Twitter data to use as features in the string transduction model. The units in the hidden layer at time t receive connections from input units at time t and also from the hidden units at the previous time step t − 1. The hidden layer predicts the state of the output units at the next time step t + 1. The input vector w(t) represents the input element at current time step, h</context>
<context position="9127" citStr="Mikolov, 2012" startWordPosition="1529" endWordPosition="1530">s, and using bytes rather than characters as basic units. 3.1 Unlabeled tweets In order to train our SRN language model we collected a set of tweets using the Twitter sampling API. We use the raw sample directly without filtering it in any way, relying on the SRN to learn the structure of the data. The sample consists of 414 million bytes of UTF-8 encoded in a variety of languages and scripts text. We trained a 400- hidden-unit SRN, to predict the next byte in the sequence using backpropagation through time. Input bytes were encoded using one-hot representation. We modified the RNNLM toolkit (Mikolov, 2012a) to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer. We did try to filter tweets by language and create specific embeddings for English but this had negligible effect on tweet normalization performance. The trained SRN language model can be used to generate random text by sampling the next byte from its predictive distribution and extending the string with the result. Figure 1 shows example strings generated in this </context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom´a&amp;quot;s Mikolov. 2012b. Statistical language models based on neural networks. Ph.D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Mikolov</author>
<author>Martin Karafi´at</author>
<author>Luk´as Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Interspeech,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tom´a&amp;quot;s Mikolov, Martin Karafi´at, Luk´a&amp;quot;s Burget, Jan &amp;quot;Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Webb Miller</author>
<author>Eugene W Myers</author>
</authors>
<title>A file comparison program.</title>
<date>1985</date>
<journal>Software: Practice and Experience,</journal>
<volume>15</volume>
<issue>11</issue>
<contexts>
<context position="5044" citStr="Miller and Myers, 1985" startWordPosition="811" endWordPosition="814"> is that is makes it easy to exploit large amounts of unlabeled data in the form of a language model. This advantage does not hold for text normalization. We thus propose an alternative approach where normalization is modeled directly, and which enables easy incorporation of unlabeled data from the source domain. 2.1 Learning to transduce strings Our string transduction model works by learning the sequence of edits which transform the input string into the output string. Given a pair of strings such a sequence of edits (known as the shortest edit script) can be found using the DIFF algorithm (Miller and Myers, 1985; Myers, 1986). Our version of DIFF uses the following types of edits: • NIL – no edits, • DEL – delete character at this position, • INS(·) – insert specified string before character at this position.1 Table 1 shows a shortest edit script for the pair of strings (c wat, see what). We use a sequence labeling model to learn to label input strings with edit scripts. The training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings. As a sequence labeler we use Conditional Random Fields (Lafferty et al., 2001). Once trained the model is u</context>
</contexts>
<marker>Miller, Myers, 1985</marker>
<rawString>Webb Miller and Eugene W Myers. 1985. A file comparison program. Software: Practice and Experience, 15(11):1025–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene W Myers</author>
</authors>
<title>An O(ND) difference algorithm and its variations.</title>
<date>1986</date>
<journal>Algorithmica,</journal>
<pages>1--1</pages>
<contexts>
<context position="5058" citStr="Myers, 1986" startWordPosition="815" endWordPosition="816">y to exploit large amounts of unlabeled data in the form of a language model. This advantage does not hold for text normalization. We thus propose an alternative approach where normalization is modeled directly, and which enables easy incorporation of unlabeled data from the source domain. 2.1 Learning to transduce strings Our string transduction model works by learning the sequence of edits which transform the input string into the output string. Given a pair of strings such a sequence of edits (known as the shortest edit script) can be found using the DIFF algorithm (Miller and Myers, 1985; Myers, 1986). Our version of DIFF uses the following types of edits: • NIL – no edits, • DEL – delete character at this position, • INS(·) – insert specified string before character at this position.1 Table 1 shows a shortest edit script for the pair of strings (c wat, see what). We use a sequence labeling model to learn to label input strings with edit scripts. The training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings. As a sequence labeler we use Conditional Random Fields (Lafferty et al., 2001). Once trained the model is used to label n</context>
</contexts>
<marker>Myers, 1986</marker>
<rawString>Eugene W Myers. 1986. An O(ND) difference algorithm and its variations. Algorithmica, 1(1-4):251– 266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>1017--1024</pages>
<contexts>
<context position="6593" citStr="Sutskever et al., 2011" startWordPosition="1076" endWordPosition="1079"> t) is the shortest edit script mapping s to t. P(e|s) is modeled with a linear-chain Conditional Random Field. 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as models of temporal, or sequential, structure in data, including linguistic data (Elman, 1991). More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models (Mikolov et al., 2010; Mikolov, 2012b). Another version of recurrent neural nets has been used to generate plausible text with a character-level language model (Sutskever et al., 2011). We use SRNs to induce character-level text representations from unlabeled Twitter data to use as features in the string transduction model. The units in the hidden layer at time t receive connections from input units at time t and also from the hidden units at the previous time step t − 1. The hidden layer predicts the state of the output units at the next time step t + 1. The input vector w(t) represents the input element at current time step, here the current character. The output vector y(t) represents the predicted probabilities for the next character. The activation sj of a hidden unit </context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenzhen Xue</author>
<author>Dawei Yin</author>
<author>Brian D Davison</author>
</authors>
<title>Normalizing microtext.</title>
<date>2011</date>
<booktitle>In Proceedings of the AAAI11 Workshop on Analyzing Microtext,</booktitle>
<pages>74--79</pages>
<contexts>
<context position="18558" citStr="Xue et al., 2011" startWordPosition="3115" endWordPosition="3118">. Kaufmann and Kalita (2010) trained a phrasebased statistical translation model on a parallel text message corpus and applied it to tweet normalization. In comparison to our first-order linearchain CRF, an MT model with reordering is more flexible but for this reason needs more training data. It also suffers from language model mismatch mentioned in Section 2: optimal results were obtained by using a low weight for the language model trained on a balanced text corpus. Many other approaches to tweet normalization are more unsupervised in nature (e.g. Han and Baldwin, 2011; Gouws et al., 2011; Xue et al., 2011; Han et al., 2012). They still require annotated development data for tuning parameters and a variety of heuristics. Our approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available. Further afield, our work has connections to research on morphological analysis: for example Chrupała et al. (2008) use edit scripts to learn lemmatization rules while Dreyer et al. (2008) propose a discriminative model for string transductions and apply it to morphological tasks. While Chrupała (2013) and Evang et al. (2013) use char</context>
</contexts>
<marker>Xue, Yin, Davison, 2011</marker>
<rawString>Zhenzhen Xue, Dawei Yin, and Brian D Davison. 2011. Normalizing microtext. In Proceedings of the AAAI11 Workshop on Analyzing Microtext, pages 74–79.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>