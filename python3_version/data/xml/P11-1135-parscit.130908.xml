<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.996944">
Using Large Monolingual and Bilingual Corpora to
Improve Coordination Disambiguation
</title>
<author confidence="0.99951">
Shane Bergsma, David Yarowsky, Kenneth Church
</author>
<affiliation confidence="0.9303065">
Deptartment of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<email confidence="0.994358">
sbergsma@jhu.edu, yarowsky@cs.jhu.edu, kenneth.church@jhu.edu
</email>
<sectionHeader confidence="0.997322" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.98774028">
Resolving coordination ambiguity is a clas-
sic hard problem. This paper looks at co-
ordination disambiguation in complex noun
phrases (NPs). Parsers trained on the Penn
Treebank are reporting impressive numbers
these days, but they don’t do very well on this
problem (79%). We explore systems trained
using three types of corpora: (1) annotated
(e.g. the Penn Treebank), (2) bitexts (e.g. Eu-
roparl), and (3) unannotated monolingual (e.g.
Google N-grams). Size matters: (1) is a mil-
lion words, (2) is potentially billions of words
and (3) is potentially trillions of words. The
unannotated monolingual data is helpful when
the ambiguity can be resolved through associ-
ations among the lexical items. The bilingual
data is helpful when the ambiguity can be re-
solved by the order of words in the translation.
We train separate classifiers with monolingual
and bilingual features and iteratively improve
them via co-training. The co-trained classifier
achieves close to 96% accuracy on Treebank
data and makes 20% fewer errors than a su-
pervised system trained with Treebank anno-
tations.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998721581395349">
Determining which words are being linked by a co-
ordinating conjunction is a classic hard problem.
Consider the pair:
+ellipsis rocket\w1 and mortar\w2 attacks\h
−ellipsis asbestos\w1 and polyvinyl\w2 chloride\h
+ellipsis is about both rocket attacks and mortar at-
tacks, unlike −ellipsis which is not about asbestos
chloride. We use h to refer to the head of the phrase,
and w1 and w2 to refer to the other two lexical items.
Natural Language Processing applications need to
recognize NP ellipsis in order to make sense of new
sentences. For example, if an Internet search en-
gine is given the phrase rocket attacks as a query, it
should rank documents containing rocket and mor-
tar attacks highly, even though rocket and attacks
are not contiguous in the document. Furthermore,
NPs with ellipsis often require a distinct type of re-
ordering when translated into a foreign language.
Since coordination is both complex and produc-
tive, parsers and machine translation (MT) systems
cannot simply memorize the analysis of coordinate
phrases from training text. We propose an approach
to recognizing ellipsis that could benefit both MT
and other NLP technology that relies on shallow or
deep syntactic analysis.
While the general case of coordination is quite
complicated, we focus on the special case of com-
plex NPs. Errors in NP coordination typically ac-
count for the majority of parser coordination errors
(Hogan, 2007). The information needed to resolve
coordinate NP ambiguity cannot be derived from
hand-annotated data, and we follow previous work
in looking for new information sources to apply
to this problem (Resnik, 1999; Nakov and Hearst,
2005; Rus et al., 2007; Pitler et al., 2010).
We first resolve coordinate NP ambiguity in a
word-aligned parallel corpus. In bitexts, both mono-
lingual and bilingual information can indicate NP
structure. We create separate classifiers using mono-
lingual and bilingual feature views. We train the
two classifiers using co-training, iteratively improv-
ing the accuracy of one classifier by learning from
the predictions of the other. Starting from only two
</bodyText>
<page confidence="0.933483">
1346
</page>
<note confidence="0.9791925">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346–1355,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999813833333333">
initial labeled examples, we are able to train a highly
accurate classifier using only monolingual features.
The monolingual classifier can then be used both
within and beyond the aligned bitext. In particular,
it achieves close to 96% accuracy on both bitext data
and on out-of-domain examples in the Treebank.
</bodyText>
<sectionHeader confidence="0.890227" genericHeader="introduction">
2 Problem Definition and Related Tasks
</sectionHeader>
<bodyText confidence="0.99767083018868">
Our system operates over a part-of-speech tagged in-
put corpus. We attempt to resolve the ambiguity in
all tag sequences matching the expression:
[DT|PRP$] (N.*|J.*) and [DT|PRP$] (N.*|J.*) N.*
e.g. [the] rocket\w1 and [the] mortar\w2 attacks\h
Each example ends with a noun, h. Preceding h
are a pair of possibly-conjoined words, w1 and w2,
either nouns (rocket and mortar), adjectives, or a
mix of the two. We allow determiners or possessive
pronouns before w1 and/or w2. This pattern is very
common. Depending on the domain, we find it in
roughly one of every 10 to 20 sentences. We merge
identical matches in our corpus into a single exam-
ple for labeling. Roughly 38% of w1,w2 pairs are
both adjectives, 26% are nouns, and 36% are mixed.
The task is to determine whether w1 and w2 are
conjoined or not. When they are not conjoined, there
are two cases: 1) w1 is actually conjoined with w2 h
as a whole (e.g. asbestos and polyvinyl chloride),
or 2) The conjunction links something higher up in
the parse tree, as in, “farmers are getting older\w1
and younger\w2 people\h are reluctant to take up
farming.” Here, and links two separate clauses.
Our task is both narrower and broader than pre-
vious work. It is broader than previous approaches
that have focused only on conjoined nouns (Resnik,
1999; Nakov and Hearst, 2005). Although pairs
of adjectives are usually conjoined (and mixed tags
are usually not), this is not always true, as in
older/younger above. For comparison, we also state
accuracy on the noun-only examples (§ 8).
Our task is more narrow than the task tackled
by full-sentence parsers, but most parsers do not
bracket NP-internal structure at all, since such struc-
ture is absent from the primary training corpus for
statistical parsers, the Penn Treebank (Marcus et al.,
1993). We confirm that standard broad-coverage
parsers perform poorly on our task (§ 7).
Vadas and Curran (2007a) manually annotated NP
structure in the Penn Treebank, and a few custom NP
parsers have recently been developed using this data
(Vadas and Curran, 2007b; Pitler et al., 2010). Our
task is more narrow than the task handled by these
parsers since we do not handle other, less-frequent
and sometimes more complex constructions (e.g.
robot arms and legs). However, such constructions
are clearly amenable to our algorithm. In addition,
these parsers have only evaluated coordination res-
olution within base NPs, simplifying the task and
rendering the aforementioned older/younger prob-
lem moot. Finally, these custom parsers have only
used simple count features; for example, they have
not used the paraphrases we describe below.
</bodyText>
<sectionHeader confidence="0.996927" genericHeader="method">
3 Supervised Coordination Resolution
</sectionHeader>
<bodyText confidence="0.994406111111111">
We adopt a discriminative approach to resolving co-
ordinate NP ambiguity. For each unique coordinate
NP in our corpus, we encode relevant information
in a feature vector, x. A classifier scores these vec-
tors with a set of learned weights, w. We assume N
labeled examples {(y1, x1), ..., (yN, xN)} are avail-
able to train the classifier. We use ‘y = 1’ as the
class label for NPs with ellipsis and ‘y = 0’ for
NPs without. Since our particular task requires a bi-
nary decision, any standard learning algorithm can
be used to learn the feature weights on the train-
ing data. We use (regularized) logistic regression
(a.k.a. maximum entropy) since it has been shown
to perform well on a range of NLP tasks, and also
because its probabilistic interpretation is useful for
co-training (§ 4). In binary logistic regression, the
probability of a positive class takes the form of the
logistic function:
</bodyText>
<equation confidence="0.74987825">
exp( w� · x)
Pr(y = 1) = 1 + exp( w� · x)
Ellipsis is predicted if Pr(y = 1) &gt; 0.5 (equiva-
lently, w� · x &gt; 0), otherwise we predict no ellipsis.
</equation>
<bodyText confidence="0.999507">
Supervised classifiers easily incorporate a range
of interdependent information into a learned deci-
sion function. The cost for this flexibility is typically
the need for labeled training data. The more features
we use, the more labeled data we need, since for
linear classifiers, the number of examples needed to
reach optimum performance is at most linear in the
</bodyText>
<page confidence="0.985567">
1347
</page>
<table confidence="0.612793">
Phrase Evidence Pattern
dairy and meat English: ... production of dairy and meat... h of w1 and w2
production English: ... dairy production and meat production... w1 h and w2 h
(ellipsis) English: ... meat and dairy production... w2 and w1 h
Spanish: ... producci´on l´actea y c´arnica... h w1 ... w2
→ production dairy and meat w1- ... w2h
Finnish: ... maidon- ja lihantuotantoon... h ... w1 ... w2
→ dairy- and meatproduction
French: ... production de produits laitiers et de viande...
→ production ofproducts dairy and of meat
asbestos and English: ... polyvinyl chloride and asbestos... w2 h and w1
polyvinyl English: ... asbestos , and polyvinyl chloride... w1 , and w2 h
chloride English: ... asbestos and chloride... w1 and h
(no ellipsis) Portuguese: ... o amianto e o cloreto de polivinilo... w1 ... h ... w2
→ the asbestos and the chloride ofpolyvinyl w1 ... w2h
Italian: ... l’ asbesto e il polivinilcloruro...
→ the asbestos and the polyvinylchloride
</table>
<tableCaption confidence="0.997906">
Table 1: Monolingual and bilingual evidence for ellipsis or lack-of-ellipsis in coordination of [w1 and w2 h] phrases.
</tableCaption>
<bodyText confidence="0.999337666666667">
number of features (Vapnik, 1998). In § 4, we pro-
pose a way to circumvent the need for labeled data.
We now describe the particular monolingual and
bilingual information we use for this problem. We
refer to Table 1 for canonical examples of the two
classes and also to provide intuition for the features.
</bodyText>
<subsectionHeader confidence="0.999012">
3.1 Monolingual Features
</subsectionHeader>
<bodyText confidence="0.99995525">
Count features These real-valued features encode
the frequency, in a large auxiliary corpus, of rel-
evant word sequences. Co-occurrence frequencies
have long been used to resolve linguistic ambigui-
ties (Dagan and Itai, 1990; Hindle and Rooth, 1993;
Lauer, 1995). With the massive volumes of raw
text now available, we can look for very specific
and indicative word sequences. Consider the phrase
dairy and meat production (Table 1). A high count
in raw text for the paraphrase “production of dairy
and meat” implies ellipsis in the original example.
In the third column of Table 1, we suggest a pat-
tern that generalizes the particular piece of evidence.
It is these patterns and other English paraphrases
that we encode in our count features (Table 2). We
also use (but do not list) count features for the four
paraphrases proposed in Nakov and Hearst (2005,
§ 3.2.3). Such specific paraphrases are more com-
mon than one might think. In our experiments, at
least 20% of examples have non-zero counts for a
5-gram pattern, while over 70% of examples have
counts for a 4-gram pattern.
Our features also include counts for subsequences
of the full phrase. High counts for “dairy produc-
tion” alone or just “dairy and meat” also indicate el-
lipsis. On the other hand, like Pitler et al. (2010), we
have a feature for the count of “dairy and produc-
tion.” Frequent conjoining of w1 and h is evidence
that there is no ellipsis, that w1 and h are compatible
and heads of two separate and conjoined NPs.
Many of our patterns are novel in that they include
commas or determiners. The presence of these of-
ten indicate that there are two separate NPs. E.g.
seeing asbestos , and polyvinyl chloride or the as-
bestos and the polyvinyl chloride suggests no ellip-
sis. We also propose patterns that include left-and-
right context around the NP. These aim to capture
salient information about the NP’s distribution as an
entire unit. Finally, patterns involving prepositions
look for explicit paraphrasing of the nominal rela-
tions; the presence of “h PREP w1 and w2” in a cor-
pus would suggest ellipsis in the original NP.
In total, we have 48 separate count features, re-
quiring counts for 315 distinct N-grams for each ex-
ample. We use log-counts as the feature value, and
use a separate binary feature to indicate if a partic-
ular count is zero. We efficiently acquire the counts
using custom tools for managing web-scale N-gram
</bodyText>
<page confidence="0.965883">
1348
</page>
<table confidence="0.996017157894737">
Real-valued count features. C(p) --+ count of p
C(w1) C(w2) C(h)
C(w1 CC w2) C(w1 h) C(w2 h)
C(w2 CC w1) C(w1 CC h) C(h CC w1)
C(DT w1 CC w2) C(w1 , CC w2)
C(DT w2 CC w1) C(w2 , CC w1)
C(DT w1 CC h) C(w1 CC w2 ,)
C(DT h CC w1) C(w2 CC w1 ,)
C(DT w1 and DT w2) C(w1 CC DT w2)
C(DT w2 and DT w1) C(w2 CC DT w1)
C(DT h and DT w1) C(w1 CC DT h)
C(DT h and DT w2) C(h CC DT w1)
C((L-CTXTi) w1 and w2 h) C(w1 CC w2 h)
C(w1 and w2 h (R-CTXTi)) C(h PREP w1)
C(h PREP w1 CC w2) C(h PREP w2)
Count feature filler sets
DT = {the, a, an, its, his} CC = {and, or, ‘,’}
PREP = {of, for, in, at, on, from, with, about}
Binary features and feature templates --+ {0, 11
</table>
<equation confidence="0.8318495">
wrd1=(wrd(w1)) tag1=(tag(w1))
wrd2=(wrd(w2)) tag2=(tag(w2))
wrdh=(wrd(h)) tagh=(tag(h))
wrd12=(wrd(w1),wrd(w2)) wrd(w1)=wrd(w2)
tag12=(tag(w1),tag(w2)) tag(w1)=tag(w2)
tag12h=(tag(w1),tag(w1),tag(h))
</equation>
<bodyText confidence="0.92213736">
Table 2: Monolingual features. For counts using the
filler sets CC, DT and PREP, counts are summed across
all filler combinations. In contrast, feature templates are
denoted with (·), where the feature label depends on the
(bracketed argument). E.g., we have separate count fea-
ture for each item in the L/R context sets, where
{L-CTXT} = {with, and, as, including, on, is, are, &amp;},
{R-CTXT} = {and, have, of, on, said, to, were, &amp;}
data (§ 5). Previous approaches have used search
engine page counts as substitutes for co-occurrence
information (Nakov and Hearst, 2005; Rus et al.,
2007). These approaches clearly cannot scale to use
the wide range of information used in our system.
Binary features Table 2 gives the binary features
and feature templates. These are templates in the
sense that every unique word or tag fills the tem-
plate and corresponds to a unique feature. We can
thus learn if particular words or tags are associated
with ellipsis. We also include binary features to flag
the presence of any optional determiners before w1
or w2. We also have binary features for the context
words that precede and follow the tag sequence in
the source corpus. These context features are analo-
gous to the L/R-CTXT features that were counted in
the auxiliary corpus. Our classifier learns, for exam-
</bodyText>
<table confidence="0.999807086956522">
Monolingual: t. Bilingual: xb
C(w1):14.4 C(detl=h * w1 * w2),Dutch:1
C(w2):15.4 C(detl=h * * w1 * * w2),Fr.:1
C(h):17.2 C(detl=h w1 h * w2),Greek:1
C(w1 CC w2):9.0 C(detl=h w1 * w2),Spanish:1
C(w1 h):9.8 C(detl=w1- * w2h),Swedish:1
C(w2 h):10.2 C(simp=h w1 w2),Dutch:1
C(w2 CC w1):10.5 C(simp=h w1 w2),French:1
C(w1 CC h):3.5 C(simp=h w1 h w2),Greek:1
C(h CC w1):6.8 C(simp=h w1 w2),Spanish:1
C(DT w2 CC w1:7.8 C(simp=w1 w2h),Swedish:1
C(w1 and w2 h and):2.4 C(span=5),Dutch:1
C(h PREP w1 CC w2):2.6 C(span=7),French:1
wrd1=dairy:1 C(span=5),Greek:1
wrd2=meat:1 C(span=4),Spanish:1
wrdh=production:1 C(span=3),Swedish:1
tag1=NN:1 C(ord=h w1 w2),Dutch:1
tag2=NN:1 C(ord=h w1 w2),French:1
tagh=NN:1 C(ord=h w1 h w2),Greek:1
wrd12=dairy,meat:1 C(ord=h w1 w2),Spanish:1
tag12=NN,NN:1 C(ord=w1 w2 h),Swedish:1
tag(w1)=tag(w2):1 C(ord=h w1 w2):4
tag12h=NN,NN,NN:1 C(ord=w1 w2 h):1
</table>
<tableCaption confidence="0.8104022">
Table 3: Example of actual instantiated feature vectors
for dairy and meat production (in label:value format).
Monolingual feature vector, x,,,, on the left (both count
and binary features, see Table 2), Bilingual feature vec-
tor, xb, on the right (see Table 4).
</tableCaption>
<bodyText confidence="0.999929153846154">
ple, that instances preceded by the words its and in
are likely to have ellipsis: these words tend to pre-
cede single NPs as opposed to conjoined NP pairs.
Example Table 3 provides part of the actual in-
stantiated monolingual feature vector for dairy and
meat production. Note the count features have log-
arithmic values, while only the non-zero binary fea-
tures are included.
A later stage of processing extracts a list of feature
labels from the training data. This list is then used
to map feature labels to integers, yielding the stan-
dard (sparse) format used by most machine learning
software (e.g., 1:14.4 2:15.4 3:17.2 ... 7149:1 24208:1).
</bodyText>
<subsectionHeader confidence="0.999599">
3.2 Bilingual Features
</subsectionHeader>
<bodyText confidence="0.999969285714286">
The above features represent the best of the infor-
mation available to a coordinate NP classifier when
operating on an arbitrary text. In some domains,
however, we have additional information to inform
our decisions. We consider the case where we seek
to predict coordinate structure in parallel text: i.e.,
English text with a corresponding translation in one
</bodyText>
<page confidence="0.981805">
1349
</page>
<bodyText confidence="0.999948791666667">
or more target languages. A variety of mature NLP
tools exists in this domain, allowing us to robustly
align the parallel text first at the sentence and then
at the word level. Given a word-aligned parallel cor-
pus, we can see how the different types of coordinate
NPs are translated in the target languages.
In Romance languages, examples with ellipsis,
such as dairy and meat production (Table 1), tend to
correspond to translations with the head in the first
position, e.g. “producci´on l´actea y c´arnica” in Span-
ish (examples taken from Europarl (Koehn, 2005)).
When there is no ellipsis, the head-first syntax leads
to the “w1 and h w2” ordering, e.g. amianto e o
cloreto de polivinilo in Portuguese. Another clue
for ellipsis is the presence of a dangling hyphen, as
in the Finnish maidon- ja lihantuotantoon. We find
such hyphens especially common in Germanic lan-
guages like Dutch. In addition to language-specific
clues, a translation may resolve an ambiguity by
paraphrasing the example in the same way it may
be paraphrased in English. E.g., we see hard and
soft drugs translated into Spanish as drogas blandas
y drogas duras with the head, drogas, repeated (akin
to soft drugs and hard drugs in English).
One could imagine manually defining the rela-
tionship between English NP coordination and the
patterns in each language, but this would need to be
repeated for each language pair, and would likely
miss many useful patterns. In contrast, by represent-
ing the translation patterns as features in a classifier,
we can instead automatically learn the coordination-
translation correspondences, in any language pair.
For each occurrence of a coordinate NP in a word-
aligned bitext, we inspect the alignments and de-
termine the mapping of w1, w2 and h. Recall that
each of our examples represents all the occurrences
of a unique coordinate NP in a corpus. We there-
fore aggregate translation information over all the
occurrences. Since the alignments in automatically-
aligned parallel text are noisy, the more occurrences
we have, the more translations we have, and the
more likely we are to make a correct decision. For
some common instances in Europarl, like Agricul-
ture and Rural Development, we have thousands of
translations in several languages.
Table 4 provides the bilingual feature templates.
The notation indicates that, for a given coordi-
nate NP, we count the frequency of each transla-
</bodyText>
<equation confidence="0.9464578">
C(detl(w1,w2,h)),(LANG)
C(simp(w1,w2,h)),(LANG)
C(span(w1,w2,h)),(LANG)
C(ord(w1,w2,h)),(LANG)
C(ord(w1,w2,h))
</equation>
<tableCaption confidence="0.942606333333333">
Table 4: Real-valued bilingual feature templates. The
shorthand is detl=“detailed pattern,” simp=“simple pat-
tern,” span=“span of pattern,” ord=“order of words.” The
notation C(p),(LANG) means the number of times we see
the pattern (or span) (p) as the aligned translation of the
coordinate NP in the target language (LANG).
</tableCaption>
<bodyText confidence="0.999786086956522">
tion pattern in each target language, and generate
real-valued features for these counts. The feature
counts are indexed to the particular pattern and lan-
guage. We also have one language-independent fea-
ture, C(ord(w1,w2,h)), which gives the frequency of
each ordering across all languages. The span is the
number of tokens collectively spanned by the trans-
lations of w1, w2 and h. The “detailed pattern” rep-
resents the translation using wildcards for all other
foreign words, but maintains punctuation. Letting
‘*’ stand for the wildcard, the detailed patterns for
the translations of dairy and meat production in Ta-
ble 1 would be [h w1 * w2] (Spanish), [w1- * w2h]
(Finnish) and [h * * w1 * * w2] (French). Four
or more consecutive wildcards are converted to ‘...’.
For the “simple pattern,” we remove the wildcards
and punctuation. Note that our aligner allows the
English word to map to multiple target words. The
simple pattern differs from the ordering in that it de-
notes how many tokens each of w1, w2 and h span.
Example Table 3 also provides part of the actual
instantiated bilingual feature vector for dairy and
meat production.
</bodyText>
<sectionHeader confidence="0.998538" genericHeader="method">
4 Bilingual Co-training
</sectionHeader>
<bodyText confidence="0.9998509">
We exploit the orthogonality of the monolingual
and bilingual features using semi-supervised learn-
ing. These features are orthogonal in the sense that
they look at different sources of information for each
example. If we had enough training data, a good
classifier could be trained using either monolingual
or bilingual features on their own. With classifiers
trained on even a little labeled data, it’s feasible that
for a particular example, the monolingual classifier
might be confident when the bilingual classifier is
</bodyText>
<page confidence="0.958114">
1350
</page>
<bodyText confidence="0.3728875">
Algorithm 1 The bilingual co-training algorithm: subscript m corresponds to monolingual, b to bilingual
Given: • a set L of labeled training examples in the bitext, {(�xi, yi)}
</bodyText>
<listItem confidence="0.952927">
• a set U of unlabeled examples in the bitext, {xj}
• hyperparams: k (num. iterations), um and ub (size smaller unlabeled pools), nm and nb
</listItem>
<figure confidence="0.607456083333333">
(num. new labeled examples each iteration), C: regularization param. for classifier training
Create Lm ← L
Create Lb ← L
Create a pool Um by choosing um examples randomly from U.
Create a pool Ub by choosing ub examples randomly from U.
for i = 0 to k do
Use Lm to train a classifier hm using only xm, the monolingual features of x�
Use Lb to train a classifier hb using only xb, the bilingual features of x�
Use hm to label Um, move the nm most-confident examples to Lb
Use hb to label Ub, move the nb most-confident examples to Lm
Replenish Um and Ub randomly from U with nm and nb new examples
end for
</figure>
<bodyText confidence="0.998079078947368">
uncertain, and vice versa. This suggests using a
co-training approach (Yarowsky, 1995; Blum and
Mitchell, 1998). We train separate classifiers on the
labeled data. We use the predictions of one classi-
fier to label new examples for training the orthogo-
nal classifier. We iterate this training and labeling.
We outline how this procedure can be applied to
bitext data in Algorithm 1 (above). We follow prior
work in drawing predictions from smaller pools, Um
and Ub, rather than from U itself, to ensure the la-
beled examples “are more representative of the un-
derlying distribution” (Blum and Mitchell, 1998).
We use a logistic regression classifier for hm and
hb. Like Blum and Mitchell (1998), we also create
a combined classifier by making predictions accord-
ing to argmaxy=1,0 Pr(y|xm)Pr(y|xb).
The hyperparameters of the algorithm are 1) k,
the number of iterations, 2) um and ub, the size of
the smaller unlabeled pools, 3) nm and nb, the num-
ber of new labeled examples to include at each itera-
tion, and 4) the regularization parameter of the logis-
tic regression classifier. All such parameters can be
tuned on a development set. Like Blum and Mitchell
(1998), we ensure that we maintain roughly the true
class balance in the labeled examples added at each
iteration; we also estimate this balance using devel-
opment data.
There are some differences between our approach
and the co-training algorithm presented in Blum and
Mitchell (1998, Table 1). One of our key goals is to
produce an accurate classifier that uses only mono-
lingual features, since only this classifier can be ap-
plied to arbitrary monolingual text. We thus break
the symmetry in the original algorithm and allow hb
to label more examples for hm than vice versa, so
that hm will improve faster. This is desirable be-
cause we don’t have unlimited unlabeled examples
to draw from, only those found in our parallel text.
</bodyText>
<sectionHeader confidence="0.998456" genericHeader="method">
5 Data
</sectionHeader>
<bodyText confidence="0.999465833333333">
Web-scale text data is used for monolingual feature
counts, parallel text is used for classifier co-training,
and labeled data is used for training and evaluation.
Web-scale N-gram Data We extract our counts
from Google V2: a new N-gram corpus (with
N-grams of length one-to-five) created from the
same one-trillion-word snapshot of the web as the
Google 5-gram Corpus (Brants and Franz, 2006),
but with enhanced filtering and processing of the
source text (Lin et al., 2010, Section 5). We get
counts using the suffix array tools described in (Lin
et al., 2010). We add one to all counts for smooth-
ing.
Parallel Data We use the Danish, German, Greek,
Spanish, Finnish, French, Italian, Dutch, Por-
tuguese, and Swedish portions of Europarl (Koehn,
2005). We also use the Czech, German, Span-
ish and French news commentary data from WMT
</bodyText>
<page confidence="0.982431">
1351
</page>
<bodyText confidence="0.999458384615384">
2010.1 Word-aligned English-Foreign bitexts are
created using the Berkeley aligner.2 We run 5 itera-
tions of joint IBM Model 1 training, followed by 3-
to-5 iterations of joint HMM training, and align with
the competitive-thresholding heuristic. The English
portions of all bitexts are part-of-speech tagged with
CRFTagger (Phan, 2006). 94K unique coordinate
NPs and their translations are then extracted.
Labeled Data For experiments within the paral-
lel text, we manually labeled 1320 of the 94K co-
ordinate NP examples. We use 605 examples to set
development parameters, 607 examples as held-out
test data, and 2, 10 or 100 examples for training.
For experiments on the WSJ portion of the Penn
Treebank, we merge the original Treebank annota-
tions with the NP annotations provided by Vadas and
Curran (2007a). We collect all coordinate NP se-
quences matching our pattern and collapse them into
a single example. We label these instances by deter-
mining whether the annotations have w1 and w2 con-
joined. In only one case did the same coordinate NP
have different labels in different occurrences; this
was clearly an error and resolved accordingly. We
collected 1777 coordinate NPs in total, and divided
them into 777 examples for training, 500 for devel-
opment and 500 as a final held-out test set.
</bodyText>
<sectionHeader confidence="0.993486" genericHeader="method">
6 Evaluation and Settings
</sectionHeader>
<bodyText confidence="0.999855764705882">
We evaluate using accuracy: the percentage of ex-
amples classified correctly in held-out test data.
We compare our systems to a baseline referred to
as the Tag-Triple classifier. This classifier has a
single feature: the tag(w1), tag(w2), tag(h) triple.
Tag-Triple is therefore essentially a discriminative,
unlexicalized parser for our coordinate NPs.
All classifiers use L2-regularized logistic regres-
sion training via LIBLINEAR (Fan et al., 2008). For
co-training, we fix regularization at C = 0.1. For all
other classifiers, we optimize the C parameter on the
development data. At each iteration, i, classifier hm
annotates 50 new examples for training hb, from a
pool of 750 examples, while hb annotates 50 * i new
examples for hm, from a pool of 750 * i examples.
This ensures hm gets the majority of automatically-
labeled examples.
</bodyText>
<footnote confidence="0.92971">
1www.statmt.org/wmt10/translation-task.html
2nlp.cs.berkeley.edu/pages/wordaligner.html
</footnote>
<figure confidence="0.9514735">
0 10 20 30 40 50 60
Co-training iteration
</figure>
<figureCaption confidence="0.9971485">
Figure 1: Accuracy on Bitext development data over the
course of co-training (from 10 initial seed examples).
</figureCaption>
<bodyText confidence="0.999991142857143">
We also set k, the number of co-training itera-
tions. The monolingual, bilingual, and combined
classifiers reach their optimum levels of perfor-
mance after different numbers of iterations (Fig-
ure 1). We therefore set k separately for each, stop-
ping around 16 iterations for the combined, 51 for
the monolingual, and 57 for the bilingual classifier.
</bodyText>
<sectionHeader confidence="0.9513" genericHeader="method">
7 Bitext Experiments
</sectionHeader>
<bodyText confidence="0.99911880952381">
We evaluate our systems on our held-out bitext data.
The majority class is ellipsis, in 55.8% of exam-
ples. For comparison, we ran two publicly-available
broad-coverage parsers and analyzed whether they
correctly predicted ellipsis. The parsers were the
C&amp;C parser (Curran et al., 2007) and Minipar (Lin,
1998). They achieved 78.6% and 77.6%.3
Table 5 shows that co-training results in much
more accurate classifiers than supervised training
alone, regardless of the features or amount of ini-
tial training data. The Tag-Triple system is the
weakest system in all cases. This shows that better
monolingual features are very important, but semi-
supervised training can also make a big difference.
3We provided the parsers full sentences containing the NPs. We
directly extracted the labels from the C&amp;C bracketing, while
for Minipar we checked whether w1 was the head of w2. Of
course, the parsers performed very poorly on ellipsis involving
two nouns (partly because NP structure is absent from their
training corpora (see § 2 and also Vadas and Curran (2008)),
but neither exceeded 88% on adjective or mixed pairs either.
</bodyText>
<figure confidence="0.970867818181818">
Bilingual View
Monolingual View
Combined
Accuracy (%) 100
98
96
94
92
90
88
86
</figure>
<page confidence="0.974099">
1352
</page>
<table confidence="0.994149666666667">
System # of Examples
2 10 100
Tag-Triple classifier 67.4 79.1 82.9
Monolingual classifier 69.9 90.8 91.6
Co-trained Mono. classifier 96.4 95.9 96.0
Relative error reduction via co-training 88% 62% 52%
Bilingual classifier 76.8 85.5 92.1
Co-trained Bili. classifier 93.2 93.2 93.9
Relative error reduction via co-training 71% 53% 23%
Mono.+Bili. classifier 69.9 91.4 94.9
Co-trained Combo classifier 96.7 96.7 96.7
Relative error reduction via co-training 89% 62% 35%
</table>
<tableCaption confidence="0.972664666666667">
Table 5: Co-training improves accuracy (%) over stan-
dard supervised learning on Bitext test data for different
feature types and number of training examples.
Table 6: Net benefits of bilingual features and co-training
on Bitext data, 100-training-example setting. A = rela-
tive error reduction over Monolingual alone.
</tableCaption>
<bodyText confidence="0.996644571428572">
Table 6 shows the net benefit of our main contri-
butions. Bilingual features clearly help on this task,
but not as much as co-training. With bilingual fea-
tures and co-training together, we achieve 96.7% ac-
curacy. This combined system could be used to very
accurately resolve coordinate ambiguity in parallel
data prior to training an MT system.
</bodyText>
<sectionHeader confidence="0.984624" genericHeader="method">
8 WSJ Experiments
</sectionHeader>
<bodyText confidence="0.99982075">
While we can now accurately resolve coordinate NP
ambiguity in parallel text, it would be even better
if this accuracy carried over to new domains, where
bilingual features are not available. We test the ro-
bustness of our co-trained monolingual classifier by
evaluating it on our labeled WSJ data.
The Penn Treebank and the annotations added by
Vadas and Curran (2007a) comprise a very special
corpus; such data is clearly not available in every
domain. We can take advantage of the plentiful la-
beled examples to also test how our co-trained sys-
tem compares to supervised systems trained with in-
</bodyText>
<table confidence="0.999758571428571">
System Training WSJ Acc.
Set # Nouns All
Nakov &amp; Hearst - - 79.2 84.8
Tag-Triple WSJ 777 76.1 82.4
Pitler et al. WSJ 777 92.3 92.8
MonoWSJ WSJ 777 92.3 94.4
Co-trained Bitext 2 93.8 95.6
</table>
<tableCaption confidence="0.999479">
Table 7: Coordinate resolution accuracy (%) on WSJ.
</tableCaption>
<bodyText confidence="0.999784954545455">
domain labeled examples, and also other systems,
like Nakov and Hearst (2005), which although un-
supervised, are tuned on WSJ data.
We reimplemented Nakov and Hearst (2005)4 and
Pitler et al. (2010)5 and trained the latter on WSJ an-
notations. We compare these systems to Tag-Triple
and also to a supervised system trained on the WSJ
using only our monolingual features (MonoWSJ).
The (out-of-domain) bitext co-trained system is the
best system on the WSJ data, both on just the ex-
amples where w1 and w2 are nouns (Nouns), and on
all examples (All) (Table 7).6 It is statistically sig-
nificantly better than the prior state-of-the-art Pitler
et al. system (McNemar’s test, p&lt;0.05) and also
exceeds the WSJ-trained system using monolingual
features (p&lt;0.2). This domain robustness is less sur-
prising given its key features are derived from web-
scale N-gram data; such features are known to gen-
eralize well across domains (Bergsma et al., 2010).
We tried co-training without the N-gram features,
and performance was worse on the WSJ (85%) than
supervised training on WSJ data alone (87%).
</bodyText>
<sectionHeader confidence="0.999654" genericHeader="related work">
9 Related Work
</sectionHeader>
<bodyText confidence="0.9989796">
Bilingual data has been used to resolve a range of
ambiguities, from PP-attachment (Schwartz et al.,
2003; Fossum and Knight, 2008), to distinguishing
grammatical roles (Schwarck et al., 2010), to full
dependency parsing (Huang et al., 2009). Related
</bodyText>
<footnote confidence="0.952051777777778">
4Nakov and Hearst (2005) use an unsupervised algorithm that
predicts ellipsis on the basis of a majority vote over a number
of pattern counts and established heuristics.
5Pitler et al. (2010) uses a supervised classifier to predict brack-
etings; their count and binary features are a strict subset of the
features used in our Monolingual classifier.
6For co-training, we tuned k on the WSJ dev set but left other
parameters the same. We start from 2 training instances; results
were the same or slightly better with 10 or 100 instances.
</footnote>
<figure confidence="0.9969636">
System
Accuracy
A
Monolingual alone
+ Bilingual
+ Co-training
+ Bilingual &amp; Co-training
91.6
94.9
96.0
96.7
-
39%
54%
61%
</figure>
<page confidence="0.974185">
1353
</page>
<bodyText confidence="0.999990705882353">
work has also focused on projecting syntactic an-
notations from one language to another (Yarowsky
and Ngai, 2001; Hwa et al., 2005), and jointly pars-
ing the two sides of a bitext by leveraging the align-
ments during training and testing (Smith and Smith,
2004; Burkett and Klein, 2008) or just during train-
ing (Snyder et al., 2009). None of this work has fo-
cused on coordination, nor has it combined bitexts
with web-scale monolingual information.
Most prior work has focused on leveraging the
alignments between a single pair of languages. Da-
gan et al. (1991) first articulated the need for “a mul-
tilingual corpora based system, which exploits the
differences between languages to automatically ac-
quire knowledge about word senses.” Kuhn (2004)
used alignments across several Europarl bitexts to
devise rules for identifying parse distituents. Ban-
nard and Callison-Burch (2005) used multiple bi-
texts as part of a system for extracting paraphrases.
Our co-training algorithm is well suited to using
multiple bitexts because it automatically learns the
value of alignment information in each language. In
addition, our approach copes with noisy alignments
both by aggregating information across languages
(and repeated occurrences within a language), and
by only selecting the most confident examples at
each iteration. Burkett et al. (2010) also pro-
posed exploiting monolingual-view and bilingual-
view predictors. In their work, the bilingual view
encodes the per-instance agreement between mono-
lingual predictors in two languages, while our bilin-
gual view encodes the alignment and target text to-
gether, across multiple instances and languages.
The other side of the coin is the use of syntax to
perform better translation (Wu, 1997). This is a rich
field of research with its own annual workshop (Syn-
tax and Structure in Translation).
Our monolingual model is most similar to pre-
vious work using counts from web-scale text, both
for resolving coordination ambiguity (Nakov and
Hearst, 2005; Rus et al., 2007; Pitler et al., 2010),
and for syntax and semantics in general (Lapata
and Keller, 2005; Bergsma et al., 2010). We do
not currently use semantic similarity (either tax-
onomic (Resnik, 1999) or distributional (Hogan,
2007)) which has previously been found useful for
coordination. Our model can easily include such in-
formation as additional features. Adding new fea-
tures without adding new training data is often prob-
lematic, but is promising in our framework, since the
bitexts provide so much indirect supervision.
</bodyText>
<sectionHeader confidence="0.992882" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999960909090909">
Resolving coordination ambiguity is hard. Parsers
are reporting impressive numbers these days, but
coordination remains an area with room for im-
provement. We focused on a specific subcase, com-
plex NPs, and introduced a new evaluation set. We
achieved a huge performance improvement from
79% for state-of-the-art parsers to 96%.7
Size matters. Most parsers are trained on a mere
million words of the Penn Treebank. In this work,
we show how to take advantage of billions of words
of bitexts and trillions of words of unlabeled mono-
lingual text. Larger corpora make it possible to
use associations among lexical items (compare dairy
production vs. asbestos chloride) and precise para-
phrases (production of dairy and meat). Bitexts are
helpful when the ambiguity can be resolved by some
feature in another language (such as word order).
The Treebank is convenient for supervised train-
ing because it has annotations. We show that even
without such annotations, high-quality supervised
models can be trained using co-training and features
derived from huge volumes of unlabeled data.
</bodyText>
<sectionHeader confidence="0.999128" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999022823529412">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. ACL,
pages 597–604.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In Proc. ACL, pages 865–874.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proc.
COLT, pages 92–100.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram Corpus Version 1.1. LDC2006T13.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Proc.
EMNLP, pages 877–886.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proc. CoNLL, pages 46–53.
</reference>
<footnote confidence="0.938962">
7Evaluation scripts and data are available online:
www.clsp.jhu.edu/∼sbergsma/coordNP.ACL11.zip
</footnote>
<page confidence="0.983974">
1354
</page>
<reference confidence="0.999921552380952">
James Curran, Stephen Clark, and Johan Bos. 2007. Lin-
guistically motivated large-scale NLP with C&amp;C and
Boxer. In Proc. ACL Demo and Poster Sessions, pages
33–36.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In Proc. COLING, pages 330–332.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proc.
ACL, pages 130–137.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871–
1874.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual Chinese-English word alignments to resolve PP-
attachment ambiguity in English. In Proc. AMTA Stu-
dent Workshop, pages 48–53.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103–120.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proc. ACL,
pages 680–687.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. EMNLP, pages 1222–1231.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311–325.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. MT Summit X.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proc. ACL, pages 470–477.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM Trans.
Speech and Language Processing, 2(1):1–31.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: Some empirical results. In Proc. ACL, pages
47–54.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
N-grams. In Proc. LREC.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proc. LREC Workshop on the Evalu-
ation of Parsing Systems.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Preslav Nakov and Marti Hearst. 2005. Using the web as
an implicit training set: application to structural ambi-
guity resolution. In Proc. HLT-EMNLP, pages 17–24.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS
Tagger. crftagger.sourceforge.net.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth
Church. 2010. Using web-scale N-grams to improve
base NP parsing performance. In In Proc. COLING,
pages 886–894.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95–130.
Vasile Rus, Sireesha Ravi, Mihai C. Lintean, and
Philip M. McCarthy. 2007. Unsupervised method for
parsing coordinated base noun phrases. In Proc. CI-
CLing, pages 229–240.
Florian Schwarck, Alexander Fraser, and Hinrich
Sch¨utze. 2010. Bitext-based resolution of German
subject-object ambiguities. In Proc. HLT-NAACL,
pages 737–740.
Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003.
Disambiguation of English PP attachment using mul-
tilingual aligned data. In Proc. MT Summit IX, pages
330–337.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proc. EMNLP, pages 49–56.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proc. ACL-IJCNLP, pages 1041–1050.
David Vadas and James R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proc. ACL,
pages 240–247.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In PA-
CLING, pages 104–112.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proc. ACL, pages 104–
112.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley &amp; Sons.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In Proc. NAACL, pages
1–8.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. ACL,
pages 189–196.
</reference>
<page confidence="0.99281">
1355
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902028">
<title confidence="0.9993755">Using Large Monolingual and Bilingual Corpora Improve Coordination Disambiguation</title>
<author confidence="0.999983">Shane Bergsma</author>
<author confidence="0.999983">David Yarowsky</author>
<author confidence="0.999983">Kenneth Church</author>
<affiliation confidence="0.9987395">Deptartment of Computer Science and Human Language Technology Center of Johns Hopkins University</affiliation>
<email confidence="0.999136">sbergsma@jhu.edu,yarowsky@cs.jhu.edu,kenneth.church@jhu.edu</email>
<abstract confidence="0.996320307692308">Resolving coordination ambiguity is a classic hard problem. This paper looks at coordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don’t do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classifiers with monolingual and bilingual features and iteratively improve them via co-training. The co-trained classifier achieves close to 96% accuracy on Treebank data and makes 20% fewer errors than a supervised system trained with Treebank annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="33272" citStr="Bannard and Callison-Burch (2005)" startWordPosition="5438" endWordPosition="5442">nd Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also proposed exploiting monolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreem</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proc. ACL, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Emily Pitler</author>
<author>Dekang Lin</author>
</authors>
<title>Creating robust supervised classifiers via web-scale ngram data. In</title>
<date>2010</date>
<booktitle>Proc. ACL,</booktitle>
<pages>865--874</pages>
<contexts>
<context position="31322" citStr="Bergsma et al., 2010" startWordPosition="5122" endWordPosition="5125">ned on the WSJ using only our monolingual features (MonoWSJ). The (out-of-domain) bitext co-trained system is the best system on the WSJ data, both on just the examples where w1 and w2 are nouns (Nouns), and on all examples (All) (Table 7).6 It is statistically significantly better than the prior state-of-the-art Pitler et al. system (McNemar’s test, p&lt;0.05) and also exceeds the WSJ-trained system using monolingual features (p&lt;0.2). This domain robustness is less surprising given its key features are derived from webscale N-gram data; such features are known to generalize well across domains (Bergsma et al., 2010). We tried co-training without the N-gram features, and performance was worse on the WSJ (85%) than supervised training on WSJ data alone (87%). 9 Related Work Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5Pitler et al. (201</context>
<context position="34513" citStr="Bergsma et al., 2010" startWordPosition="5634" endWordPosition="5637">ual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information as additional features. Adding new features without adding new training data is often problematic, but is promising in our framework, since the bitexts provide so much indirect supervision. 10 Conclusion Resolving coordination ambiguity is hard. Parsers are reporting impressive numbers these days, but coordination remains an area with room for improvement. We focused on a specific s</context>
</contexts>
<marker>Bergsma, Pitler, Lin, 2010</marker>
<rawString>Shane Bergsma, Emily Pitler, and Dekang Lin. 2010. Creating robust supervised classifiers via web-scale ngram data. In Proc. ACL, pages 865–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proc. COLT,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="21815" citStr="Blum and Mitchell, 1998" startWordPosition="3574" endWordPosition="3577"> Create Lm ← L Create Lb ← L Create a pool Um by choosing um examples randomly from U. Create a pool Ub by choosing ub examples randomly from U. for i = 0 to k do Use Lm to train a classifier hm using only xm, the monolingual features of x� Use Lb to train a classifier hb using only xb, the bilingual features of x� Use hm to label Um, move the nm most-confident examples to Lb Use hb to label Ub, move the nb most-confident examples to Lm Replenish Um and Ub randomly from U with nm and nb new examples end for uncertain, and vice versa. This suggests using a co-training approach (Yarowsky, 1995; Blum and Mitchell, 1998). We train separate classifiers on the labeled data. We use the predictions of one classifier to label new examples for training the orthogonal classifier. We iterate this training and labeling. We outline how this procedure can be applied to bitext data in Algorithm 1 (above). We follow prior work in drawing predictions from smaller pools, Um and Ub, rather than from U itself, to ensure the labeled examples “are more representative of the underlying distribution” (Blum and Mitchell, 1998). We use a logistic regression classifier for hm and hb. Like Blum and Mitchell (1998), we also create a c</context>
<context position="23142" citStr="Blum and Mitchell (1998" startWordPosition="3796" endWordPosition="3799">of the algorithm are 1) k, the number of iterations, 2) um and ub, the size of the smaller unlabeled pools, 3) nm and nb, the number of new labeled examples to include at each iteration, and 4) the regularization parameter of the logistic regression classifier. All such parameters can be tuned on a development set. Like Blum and Mitchell (1998), we ensure that we maintain roughly the true class balance in the labeled examples added at each iteration; we also estimate this balance using development data. There are some differences between our approach and the co-training algorithm presented in Blum and Mitchell (1998, Table 1). One of our key goals is to produce an accurate classifier that uses only monolingual features, since only this classifier can be applied to arbitrary monolingual text. We thus break the symmetry in the original algorithm and allow hb to label more examples for hm than vice versa, so that hm will improve faster. This is desirable because we don’t have unlimited unlabeled examples to draw from, only those found in our parallel text. 5 Data Web-scale text data is used for monolingual feature counts, parallel text is used for classifier co-training, and labeled data is used for trainin</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proc. COLT, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>The Google Web 1T 5-gram Corpus Version</booktitle>
<volume>1</volume>
<pages>2006--13</pages>
<contexts>
<context position="23989" citStr="Brants and Franz, 2006" startWordPosition="3939" endWordPosition="3942">lgorithm and allow hb to label more examples for hm than vice versa, so that hm will improve faster. This is desirable because we don’t have unlimited unlabeled examples to draw from, only those found in our parallel text. 5 Data Web-scale text data is used for monolingual feature counts, parallel text is used for classifier co-training, and labeled data is used for training and evaluation. Web-scale N-gram Data We extract our counts from Google V2: a new N-gram corpus (with N-grams of length one-to-five) created from the same one-trillion-word snapshot of the web as the Google 5-gram Corpus (Brants and Franz, 2006), but with enhanced filtering and processing of the source text (Lin et al., 2010, Section 5). We get counts using the suffix array tools described in (Lin et al., 2010). We add one to all counts for smoothing. Parallel Data We use the Danish, German, Greek, Spanish, Finnish, French, Italian, Dutch, Portuguese, and Swedish portions of Europarl (Koehn, 2005). We also use the Czech, German, Spanish and French news commentary data from WMT 1351 2010.1 Word-aligned English-Foreign bitexts are created using the Berkeley aligner.2 We run 5 iterations of joint IBM Model 1 training, followed by 3- to-</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. The Google Web 1T 5-gram Corpus Version 1.1. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>877--886</pages>
<contexts>
<context position="32679" citStr="Burkett and Klein, 2008" startWordPosition="5347" endWordPosition="5350">ur Monolingual classifier. 6For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. System Accuracy A Monolingual alone + Bilingual + Co-training + Bilingual &amp; Co-training 91.6 94.9 96.0 96.7 - 39% 54% 61% 1353 work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used m</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proc. EMNLP, pages 877–886.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Slav Petrov</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Learning better monolingual models with unannotated bilingual text.</title>
<date>2010</date>
<booktitle>In Proc. CoNLL,</booktitle>
<pages>46--53</pages>
<contexts>
<context position="33734" citStr="Burkett et al. (2010)" startWordPosition="5508" endWordPosition="5511"> word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also proposed exploiting monolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolv</context>
</contexts>
<marker>Burkett, Petrov, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010. Learning better monolingual models with unannotated bilingual text. In Proc. CoNLL, pages 46–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<date>2007</date>
<booktitle>Linguistically motivated large-scale NLP with C&amp;C and Boxer. In Proc. ACL Demo and Poster Sessions,</booktitle>
<pages>33--36</pages>
<contexts>
<context position="27485" citStr="Curran et al., 2007" startWordPosition="4496" endWordPosition="4499">raining iterations. The monolingual, bilingual, and combined classifiers reach their optimum levels of performance after different numbers of iterations (Figure 1). We therefore set k separately for each, stopping around 16 iterations for the combined, 51 for the monolingual, and 57 for the bilingual classifier. 7 Bitext Experiments We evaluate our systems on our held-out bitext data. The majority class is ellipsis, in 55.8% of examples. For comparison, we ran two publicly-available broad-coverage parsers and analyzed whether they correctly predicted ellipsis. The parsers were the C&amp;C parser (Curran et al., 2007) and Minipar (Lin, 1998). They achieved 78.6% and 77.6%.3 Table 5 shows that co-training results in much more accurate classifiers than supervised training alone, regardless of the features or amount of initial training data. The Tag-Triple system is the weakest system in all cases. This shows that better monolingual features are very important, but semisupervised training can also make a big difference. 3We provided the parsers full sentences containing the NPs. We directly extracted the labels from the C&amp;C bracketing, while for Minipar we checked whether w1 was the head of w2. Of course, the</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale NLP with C&amp;C and Boxer. In Proc. ACL Demo and Poster Sessions, pages 33–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alan Itai</author>
</authors>
<title>Automatic processing of large corpora for the resolution of anaphora references.</title>
<date>1990</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>330--332</pages>
<contexts>
<context position="9751" citStr="Dagan and Itai, 1990" startWordPosition="1573" endWordPosition="1576">ipsis or lack-of-ellipsis in coordination of [w1 and w2 h] phrases. number of features (Vapnik, 1998). In § 4, we propose a way to circumvent the need for labeled data. We now describe the particular monolingual and bilingual information we use for this problem. We refer to Table 1 for canonical examples of the two classes and also to provide intuition for the features. 3.1 Monolingual Features Count features These real-valued features encode the frequency, in a large auxiliary corpus, of relevant word sequences. Co-occurrence frequencies have long been used to resolve linguistic ambiguities (Dagan and Itai, 1990; Hindle and Rooth, 1993; Lauer, 1995). With the massive volumes of raw text now available, we can look for very specific and indicative word sequences. Consider the phrase dairy and meat production (Table 1). A high count in raw text for the paraphrase “production of dairy and meat” implies ellipsis in the original example. In the third column of Table 1, we suggest a pattern that generalizes the particular piece of evidence. It is these patterns and other English paraphrases that we encode in our count features (Table 2). We also use (but do not list) count features for the four paraphrases </context>
</contexts>
<marker>Dagan, Itai, 1990</marker>
<rawString>Ido Dagan and Alan Itai. 1990. Automatic processing of large corpora for the resolution of anaphora references. In Proc. COLING, pages 330–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
<author>Ulrike Schwall</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>130--137</pages>
<contexts>
<context position="32954" citStr="Dagan et al. (1991)" startWordPosition="5393" endWordPosition="5397">lingual &amp; Co-training 91.6 94.9 96.0 96.7 - 39% 54% 61% 1353 work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both </context>
</contexts>
<marker>Dagan, Itai, Schwall, 1991</marker>
<rawString>Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two languages are more informative than one. In Proc. ACL, pages 130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>JMLR,</journal>
<volume>9</volume>
<pages>1874</pages>
<contexts>
<context position="26203" citStr="Fan et al., 2008" startWordPosition="4296" endWordPosition="4299">ected 1777 coordinate NPs in total, and divided them into 777 examples for training, 500 for development and 500 as a final held-out test set. 6 Evaluation and Settings We evaluate using accuracy: the percentage of examples classified correctly in held-out test data. We compare our systems to a baseline referred to as the Tag-Triple classifier. This classifier has a single feature: the tag(w1), tag(w2), tag(h) triple. Tag-Triple is therefore essentially a discriminative, unlexicalized parser for our coordinate NPs. All classifiers use L2-regularized logistic regression training via LIBLINEAR (Fan et al., 2008). For co-training, we fix regularization at C = 0.1. For all other classifiers, we optimize the C parameter on the development data. At each iteration, i, classifier hm annotates 50 new examples for training hb, from a pool of 750 examples, while hb annotates 50 * i new examples for hm, from a pool of 750 * i examples. This ensures hm gets the majority of automaticallylabeled examples. 1www.statmt.org/wmt10/translation-task.html 2nlp.cs.berkeley.edu/pages/wordaligner.html 0 10 20 30 40 50 60 Co-training iteration Figure 1: Accuracy on Bitext development data over the course of co-training (fro</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. JMLR, 9:1871– 1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
</authors>
<title>Using bilingual Chinese-English word alignments to resolve PPattachment ambiguity in English.</title>
<date>2008</date>
<booktitle>In Proc. AMTA Student Workshop,</booktitle>
<pages>48--53</pages>
<contexts>
<context position="31613" citStr="Fossum and Knight, 2008" startWordPosition="5169" endWordPosition="5172">the prior state-of-the-art Pitler et al. system (McNemar’s test, p&lt;0.05) and also exceeds the WSJ-trained system using monolingual features (p&lt;0.2). This domain robustness is less surprising given its key features are derived from webscale N-gram data; such features are known to generalize well across domains (Bergsma et al., 2010). We tried co-training without the N-gram features, and performance was worse on the WSJ (85%) than supervised training on WSJ data alone (87%). 9 Related Work Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results wer</context>
</contexts>
<marker>Fossum, Knight, 2008</marker>
<rawString>Victoria Fossum and Kevin Knight. 2008. Using bilingual Chinese-English word alignments to resolve PPattachment ambiguity in English. In Proc. AMTA Student Workshop, pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="9775" citStr="Hindle and Rooth, 1993" startWordPosition="1577" endWordPosition="1580">sis in coordination of [w1 and w2 h] phrases. number of features (Vapnik, 1998). In § 4, we propose a way to circumvent the need for labeled data. We now describe the particular monolingual and bilingual information we use for this problem. We refer to Table 1 for canonical examples of the two classes and also to provide intuition for the features. 3.1 Monolingual Features Count features These real-valued features encode the frequency, in a large auxiliary corpus, of relevant word sequences. Co-occurrence frequencies have long been used to resolve linguistic ambiguities (Dagan and Itai, 1990; Hindle and Rooth, 1993; Lauer, 1995). With the massive volumes of raw text now available, we can look for very specific and indicative word sequences. Consider the phrase dairy and meat production (Table 1). A high count in raw text for the paraphrase “production of dairy and meat” implies ellipsis in the original example. In the third column of Table 1, we suggest a pattern that generalizes the particular piece of evidence. It is these patterns and other English paraphrases that we encode in our count features (Table 2). We also use (but do not list) count features for the four paraphrases proposed in Nakov and He</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deirdre Hogan</author>
</authors>
<title>Coordinate noun phrase disambiguation in a generative parsing model.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>680--687</pages>
<contexts>
<context position="2818" citStr="Hogan, 2007" startWordPosition="433" endWordPosition="434">equire a distinct type of reordering when translated into a foreign language. Since coordination is both complex and productive, parsers and machine translation (MT) systems cannot simply memorize the analysis of coordinate phrases from training text. We propose an approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one</context>
<context position="34623" citStr="Hogan, 2007" startWordPosition="5652" endWordPosition="5653">le instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information as additional features. Adding new features without adding new training data is often problematic, but is promising in our framework, since the bitexts provide so much indirect supervision. 10 Conclusion Resolving coordination ambiguity is hard. Parsers are reporting impressive numbers these days, but coordination remains an area with room for improvement. We focused on a specific subcase, complex NPs, and introduced a new evaluation set. We achieved a huge performance improvement from 79% </context>
</contexts>
<marker>Hogan, 2007</marker>
<rawString>Deirdre Hogan. 2007. Coordinate noun phrase disambiguation in a generative parsing model. In Proc. ACL, pages 680–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1222--1231</pages>
<contexts>
<context position="31723" citStr="Huang et al., 2009" startWordPosition="5185" endWordPosition="5188">ing monolingual features (p&lt;0.2). This domain robustness is less surprising given its key features are derived from webscale N-gram data; such features are known to generalize well across domains (Bergsma et al., 2010). We tried co-training without the N-gram features, and performance was worse on the WSJ (85%) than supervised training on WSJ data alone (87%). 9 Related Work Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. System Accuracy A Monolingual alone + Bilingual + Co-t</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proc. EMNLP, pages 1222–1231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="32526" citStr="Hwa et al., 2005" startWordPosition="5320" endWordPosition="5323">r et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. System Accuracy A Monolingual alone + Bilingual + Co-training + Bilingual &amp; Co-training 91.6 94.9 96.0 96.7 - 39% 54% 61% 1353 work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, 11(3):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. MT Summit X.</booktitle>
<contexts>
<context position="16822" citStr="Koehn, 2005" startWordPosition="2758" endWordPosition="2759">ext with a corresponding translation in one 1349 or more target languages. A variety of mature NLP tools exists in this domain, allowing us to robustly align the parallel text first at the sentence and then at the word level. Given a word-aligned parallel corpus, we can see how the different types of coordinate NPs are translated in the target languages. In Romance languages, examples with ellipsis, such as dairy and meat production (Table 1), tend to correspond to translations with the head in the first position, e.g. “producci´on l´actea y c´arnica” in Spanish (examples taken from Europarl (Koehn, 2005)). When there is no ellipsis, the head-first syntax leads to the “w1 and h w2” ordering, e.g. amianto e o cloreto de polivinilo in Portuguese. Another clue for ellipsis is the presence of a dangling hyphen, as in the Finnish maidon- ja lihantuotantoon. We find such hyphens especially common in Germanic languages like Dutch. In addition to language-specific clues, a translation may resolve an ambiguity by paraphrasing the example in the same way it may be paraphrased in English. E.g., we see hard and soft drugs translated into Spanish as drogas blandas y drogas duras with the head, drogas, repe</context>
<context position="24348" citStr="Koehn, 2005" startWordPosition="4002" endWordPosition="4003">raining and evaluation. Web-scale N-gram Data We extract our counts from Google V2: a new N-gram corpus (with N-grams of length one-to-five) created from the same one-trillion-word snapshot of the web as the Google 5-gram Corpus (Brants and Franz, 2006), but with enhanced filtering and processing of the source text (Lin et al., 2010, Section 5). We get counts using the suffix array tools described in (Lin et al., 2010). We add one to all counts for smoothing. Parallel Data We use the Danish, German, Greek, Spanish, Finnish, French, Italian, Dutch, Portuguese, and Swedish portions of Europarl (Koehn, 2005). We also use the Czech, German, Spanish and French news commentary data from WMT 1351 2010.1 Word-aligned English-Foreign bitexts are created using the Berkeley aligner.2 We run 5 iterations of joint IBM Model 1 training, followed by 3- to-5 iterations of joint HMM training, and align with the competitive-thresholding heuristic. The English portions of all bitexts are part-of-speech tagged with CRFTagger (Phan, 2006). 94K unique coordinate NPs and their translations are then extracted. Labeled Data For experiments within the parallel text, we manually labeled 1320 of the 94K coordinate NP exa</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. MT Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Kuhn</author>
</authors>
<title>Experiments in parallel-text based grammar induction.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>470--477</pages>
<contexts>
<context position="33139" citStr="Kuhn (2004)" startWordPosition="5423" endWordPosition="5424">and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also</context>
</contexts>
<marker>Kuhn, 2004</marker>
<rawString>Jonas Kuhn. 2004. Experiments in parallel-text based grammar induction. In Proc. ACL, pages 470–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Trans. Speech and Language Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="34490" citStr="Lapata and Keller, 2005" startWordPosition="5630" endWordPosition="5633">greement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information as additional features. Adding new features without adding new training data is often problematic, but is promising in our framework, since the bitexts provide so much indirect supervision. 10 Conclusion Resolving coordination ambiguity is hard. Parsers are reporting impressive numbers these days, but coordination remains an area with room for improvement. We </context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Mirella Lapata and Frank Keller. 2005. Web-based models for natural language processing. ACM Trans. Speech and Language Processing, 2(1):1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: Some empirical results.</title>
<date>1995</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="9789" citStr="Lauer, 1995" startWordPosition="1581" endWordPosition="1582">w1 and w2 h] phrases. number of features (Vapnik, 1998). In § 4, we propose a way to circumvent the need for labeled data. We now describe the particular monolingual and bilingual information we use for this problem. We refer to Table 1 for canonical examples of the two classes and also to provide intuition for the features. 3.1 Monolingual Features Count features These real-valued features encode the frequency, in a large auxiliary corpus, of relevant word sequences. Co-occurrence frequencies have long been used to resolve linguistic ambiguities (Dagan and Itai, 1990; Hindle and Rooth, 1993; Lauer, 1995). With the massive volumes of raw text now available, we can look for very specific and indicative word sequences. Consider the phrase dairy and meat production (Table 1). A high count in raw text for the paraphrase “production of dairy and meat” implies ellipsis in the original example. In the third column of Table 1, we suggest a pattern that generalizes the particular piece of evidence. It is these patterns and other English paraphrases that we encode in our count features (Table 2). We also use (but do not list) count features for the four paraphrases proposed in Nakov and Hearst (2005, § </context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Corpus statistics meet the noun compound: Some empirical results. In Proc. ACL, pages 47–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New tools for web-scale N-grams. In</title>
<date>2010</date>
<booktitle>Proc. LREC.</booktitle>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant</institution>
<contexts>
<context position="24070" citStr="Lin et al., 2010" startWordPosition="3953" endWordPosition="3956">rove faster. This is desirable because we don’t have unlimited unlabeled examples to draw from, only those found in our parallel text. 5 Data Web-scale text data is used for monolingual feature counts, parallel text is used for classifier co-training, and labeled data is used for training and evaluation. Web-scale N-gram Data We extract our counts from Google V2: a new N-gram corpus (with N-grams of length one-to-five) created from the same one-trillion-word snapshot of the web as the Google 5-gram Corpus (Brants and Franz, 2006), but with enhanced filtering and processing of the source text (Lin et al., 2010, Section 5). We get counts using the suffix array tools described in (Lin et al., 2010). We add one to all counts for smoothing. Parallel Data We use the Danish, German, Greek, Spanish, Finnish, French, Italian, Dutch, Portuguese, and Swedish portions of Europarl (Koehn, 2005). We also use the Czech, German, Spanish and French news commentary data from WMT 1351 2010.1 Word-aligned English-Foreign bitexts are created using the Berkeley aligner.2 We run 5 iterations of joint IBM Model 1 training, followed by 3- to-5 iterations of joint HMM training, and align with the competitive-thresholding h</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New tools for web-scale N-grams. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proc. LREC Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="27509" citStr="Lin, 1998" startWordPosition="4502" endWordPosition="4503">l, bilingual, and combined classifiers reach their optimum levels of performance after different numbers of iterations (Figure 1). We therefore set k separately for each, stopping around 16 iterations for the combined, 51 for the monolingual, and 57 for the bilingual classifier. 7 Bitext Experiments We evaluate our systems on our held-out bitext data. The majority class is ellipsis, in 55.8% of examples. For comparison, we ran two publicly-available broad-coverage parsers and analyzed whether they correctly predicted ellipsis. The parsers were the C&amp;C parser (Curran et al., 2007) and Minipar (Lin, 1998). They achieved 78.6% and 77.6%.3 Table 5 shows that co-training results in much more accurate classifiers than supervised training alone, regardless of the features or amount of initial training data. The Tag-Triple system is the weakest system in all cases. This shows that better monolingual features are very important, but semisupervised training can also make a big difference. 3We provided the parsers full sentences containing the NPs. We directly extracted the labels from the C&amp;C bracketing, while for Minipar we checked whether w1 was the head of w2. Of course, the parsers performed very </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In Proc. LREC Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5843" citStr="Marcus et al., 1993" startWordPosition="920" endWordPosition="923">broader than previous work. It is broader than previous approaches that have focused only on conjoined nouns (Resnik, 1999; Nakov and Hearst, 2005). Although pairs of adjectives are usually conjoined (and mixed tags are usually not), this is not always true, as in older/younger above. For comparison, we also state accuracy on the noun-only examples (§ 8). Our task is more narrow than the task tackled by full-sentence parsers, but most parsers do not bracket NP-internal structure at all, since such structure is absent from the primary training corpus for statistical parsers, the Penn Treebank (Marcus et al., 1993). We confirm that standard broad-coverage parsers perform poorly on our task (§ 7). Vadas and Curran (2007a) manually annotated NP structure in the Penn Treebank, and a few custom NP parsers have recently been developed using this data (Vadas and Curran, 2007b; Pitler et al., 2010). Our task is more narrow than the task handled by these parsers since we do not handle other, less-frequent and sometimes more complex constructions (e.g. robot arms and legs). However, such constructions are clearly amenable to our algorithm. In addition, these parsers have only evaluated coordination resolution wi</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using the web as an implicit training set: application to structural ambiguity resolution.</title>
<date>2005</date>
<booktitle>In Proc. HLT-EMNLP,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="3051" citStr="Nakov and Hearst, 2005" startWordPosition="467" endWordPosition="470"> phrases from training text. We propose an approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one classifier by learning from the predictions of the other. Starting from only two 1346 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346–1355, Portland, Oregon, June 19-24, 2011. c�20</context>
<context position="5370" citStr="Nakov and Hearst, 2005" startWordPosition="843" endWordPosition="846">tives, 26% are nouns, and 36% are mixed. The task is to determine whether w1 and w2 are conjoined or not. When they are not conjoined, there are two cases: 1) w1 is actually conjoined with w2 h as a whole (e.g. asbestos and polyvinyl chloride), or 2) The conjunction links something higher up in the parse tree, as in, “farmers are getting older\w1 and younger\w2 people\h are reluctant to take up farming.” Here, and links two separate clauses. Our task is both narrower and broader than previous work. It is broader than previous approaches that have focused only on conjoined nouns (Resnik, 1999; Nakov and Hearst, 2005). Although pairs of adjectives are usually conjoined (and mixed tags are usually not), this is not always true, as in older/younger above. For comparison, we also state accuracy on the noun-only examples (§ 8). Our task is more narrow than the task tackled by full-sentence parsers, but most parsers do not bracket NP-internal structure at all, since such structure is absent from the primary training corpus for statistical parsers, the Penn Treebank (Marcus et al., 1993). We confirm that standard broad-coverage parsers perform poorly on our task (§ 7). Vadas and Curran (2007a) manually annotated</context>
<context position="10385" citStr="Nakov and Hearst (2005" startWordPosition="1682" endWordPosition="1685"> Rooth, 1993; Lauer, 1995). With the massive volumes of raw text now available, we can look for very specific and indicative word sequences. Consider the phrase dairy and meat production (Table 1). A high count in raw text for the paraphrase “production of dairy and meat” implies ellipsis in the original example. In the third column of Table 1, we suggest a pattern that generalizes the particular piece of evidence. It is these patterns and other English paraphrases that we encode in our count features (Table 2). We also use (but do not list) count features for the four paraphrases proposed in Nakov and Hearst (2005, § 3.2.3). Such specific paraphrases are more common than one might think. In our experiments, at least 20% of examples have non-zero counts for a 5-gram pattern, while over 70% of examples have counts for a 4-gram pattern. Our features also include counts for subsequences of the full phrase. High counts for “dairy production” alone or just “dairy and meat” also indicate ellipsis. On the other hand, like Pitler et al. (2010), we have a feature for the count of “dairy and production.” Frequent conjoining of w1 and h is evidence that there is no ellipsis, that w1 and h are compatible and heads </context>
<context position="13359" citStr="Nakov and Hearst, 2005" startWordPosition="2212" endWordPosition="2215">(w2)) tag(w1)=tag(w2) tag12h=(tag(w1),tag(w1),tag(h)) Table 2: Monolingual features. For counts using the filler sets CC, DT and PREP, counts are summed across all filler combinations. In contrast, feature templates are denoted with (·), where the feature label depends on the (bracketed argument). E.g., we have separate count feature for each item in the L/R context sets, where {L-CTXT} = {with, and, as, including, on, is, are, &amp;}, {R-CTXT} = {and, have, of, on, said, to, were, &amp;} data (§ 5). Previous approaches have used search engine page counts as substitutes for co-occurrence information (Nakov and Hearst, 2005; Rus et al., 2007). These approaches clearly cannot scale to use the wide range of information used in our system. Binary features Table 2 gives the binary features and feature templates. These are templates in the sense that every unique word or tag fills the template and corresponds to a unique feature. We can thus learn if particular words or tags are associated with ellipsis. We also include binary features to flag the presence of any optional determiners before w1 or w2. We also have binary features for the context words that precede and follow the tag sequence in the source corpus. Thes</context>
<context position="30461" citStr="Nakov and Hearst (2005)" startWordPosition="4980" endWordPosition="4983">WSJ data. The Penn Treebank and the annotations added by Vadas and Curran (2007a) comprise a very special corpus; such data is clearly not available in every domain. We can take advantage of the plentiful labeled examples to also test how our co-trained system compares to supervised systems trained with inSystem Training WSJ Acc. Set # Nouns All Nakov &amp; Hearst - - 79.2 84.8 Tag-Triple WSJ 777 76.1 82.4 Pitler et al. WSJ 777 92.3 92.8 MonoWSJ WSJ 777 92.3 94.4 Co-trained Bitext 2 93.8 95.6 Table 7: Coordinate resolution accuracy (%) on WSJ. domain labeled examples, and also other systems, like Nakov and Hearst (2005), which although unsupervised, are tuned on WSJ data. We reimplemented Nakov and Hearst (2005)4 and Pitler et al. (2010)5 and trained the latter on WSJ annotations. We compare these systems to Tag-Triple and also to a supervised system trained on the WSJ using only our monolingual features (MonoWSJ). The (out-of-domain) bitext co-trained system is the best system on the WSJ data, both on just the examples where w1 and w2 are nouns (Nouns), and on all examples (All) (Table 7).6 It is statistically significantly better than the prior state-of-the-art Pitler et al. system (McNemar’s test, p&lt;0.05)</context>
<context position="31757" citStr="Nakov and Hearst (2005)" startWordPosition="5190" endWordPosition="5193">2). This domain robustness is less surprising given its key features are derived from webscale N-gram data; such features are known to generalize well across domains (Bergsma et al., 2010). We tried co-training without the N-gram features, and performance was worse on the WSJ (85%) than supervised training on WSJ data alone (87%). 9 Related Work Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. System Accuracy A Monolingual alone + Bilingual + Co-training + Bilingual &amp; Co-training </context>
<context position="34384" citStr="Nakov and Hearst, 2005" startWordPosition="5611" endWordPosition="5614">nolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information as additional features. Adding new features without adding new training data is often problematic, but is promising in our framework, since the bitexts provide so much indirect supervision. 10 Conclusion Resolving coordination ambiguity is hard. Parsers ar</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Using the web as an implicit training set: application to structural ambiguity resolution. In Proc. HLT-EMNLP, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
</authors>
<date>2006</date>
<journal>CRFTagger: CRF English POS Tagger. crftagger.sourceforge.net.</journal>
<contexts>
<context position="24769" citStr="Phan, 2006" startWordPosition="4067" endWordPosition="4068">. We add one to all counts for smoothing. Parallel Data We use the Danish, German, Greek, Spanish, Finnish, French, Italian, Dutch, Portuguese, and Swedish portions of Europarl (Koehn, 2005). We also use the Czech, German, Spanish and French news commentary data from WMT 1351 2010.1 Word-aligned English-Foreign bitexts are created using the Berkeley aligner.2 We run 5 iterations of joint IBM Model 1 training, followed by 3- to-5 iterations of joint HMM training, and align with the competitive-thresholding heuristic. The English portions of all bitexts are part-of-speech tagged with CRFTagger (Phan, 2006). 94K unique coordinate NPs and their translations are then extracted. Labeled Data For experiments within the parallel text, we manually labeled 1320 of the 94K coordinate NP examples. We use 605 examples to set development parameters, 607 examples as held-out test data, and 2, 10 or 100 examples for training. For experiments on the WSJ portion of the Penn Treebank, we merge the original Treebank annotations with the NP annotations provided by Vadas and Curran (2007a). We collect all coordinate NP sequences matching our pattern and collapse them into a single example. We label these instances</context>
</contexts>
<marker>Phan, 2006</marker>
<rawString>Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS Tagger. crftagger.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
</authors>
<title>Using web-scale N-grams to improve base NP parsing performance. In</title>
<date>2010</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>886--894</pages>
<contexts>
<context position="3091" citStr="Pitler et al., 2010" startWordPosition="475" endWordPosition="478"> approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one classifier by learning from the predictions of the other. Starting from only two 1346 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346–1355, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguis</context>
<context position="6125" citStr="Pitler et al., 2010" startWordPosition="966" endWordPosition="969">ve. For comparison, we also state accuracy on the noun-only examples (§ 8). Our task is more narrow than the task tackled by full-sentence parsers, but most parsers do not bracket NP-internal structure at all, since such structure is absent from the primary training corpus for statistical parsers, the Penn Treebank (Marcus et al., 1993). We confirm that standard broad-coverage parsers perform poorly on our task (§ 7). Vadas and Curran (2007a) manually annotated NP structure in the Penn Treebank, and a few custom NP parsers have recently been developed using this data (Vadas and Curran, 2007b; Pitler et al., 2010). Our task is more narrow than the task handled by these parsers since we do not handle other, less-frequent and sometimes more complex constructions (e.g. robot arms and legs). However, such constructions are clearly amenable to our algorithm. In addition, these parsers have only evaluated coordination resolution within base NPs, simplifying the task and rendering the aforementioned older/younger problem moot. Finally, these custom parsers have only used simple count features; for example, they have not used the paraphrases we describe below. 3 Supervised Coordination Resolution We adopt a di</context>
<context position="10814" citStr="Pitler et al. (2010)" startWordPosition="1757" endWordPosition="1760">patterns and other English paraphrases that we encode in our count features (Table 2). We also use (but do not list) count features for the four paraphrases proposed in Nakov and Hearst (2005, § 3.2.3). Such specific paraphrases are more common than one might think. In our experiments, at least 20% of examples have non-zero counts for a 5-gram pattern, while over 70% of examples have counts for a 4-gram pattern. Our features also include counts for subsequences of the full phrase. High counts for “dairy production” alone or just “dairy and meat” also indicate ellipsis. On the other hand, like Pitler et al. (2010), we have a feature for the count of “dairy and production.” Frequent conjoining of w1 and h is evidence that there is no ellipsis, that w1 and h are compatible and heads of two separate and conjoined NPs. Many of our patterns are novel in that they include commas or determiners. The presence of these often indicate that there are two separate NPs. E.g. seeing asbestos , and polyvinyl chloride or the asbestos and the polyvinyl chloride suggests no ellipsis. We also propose patterns that include left-andright context around the NP. These aim to capture salient information about the NP’s distrib</context>
<context position="30581" citStr="Pitler et al. (2010)" startWordPosition="5000" endWordPosition="5003"> is clearly not available in every domain. We can take advantage of the plentiful labeled examples to also test how our co-trained system compares to supervised systems trained with inSystem Training WSJ Acc. Set # Nouns All Nakov &amp; Hearst - - 79.2 84.8 Tag-Triple WSJ 777 76.1 82.4 Pitler et al. WSJ 777 92.3 92.8 MonoWSJ WSJ 777 92.3 94.4 Co-trained Bitext 2 93.8 95.6 Table 7: Coordinate resolution accuracy (%) on WSJ. domain labeled examples, and also other systems, like Nakov and Hearst (2005), which although unsupervised, are tuned on WSJ data. We reimplemented Nakov and Hearst (2005)4 and Pitler et al. (2010)5 and trained the latter on WSJ annotations. We compare these systems to Tag-Triple and also to a supervised system trained on the WSJ using only our monolingual features (MonoWSJ). The (out-of-domain) bitext co-trained system is the best system on the WSJ data, both on just the examples where w1 and w2 are nouns (Nouns), and on all examples (All) (Table 7).6 It is statistically significantly better than the prior state-of-the-art Pitler et al. system (McNemar’s test, p&lt;0.05) and also exceeds the WSJ-trained system using monolingual features (p&lt;0.2). This domain robustness is less surprising g</context>
<context position="31924" citStr="Pitler et al. (2010)" startWordPosition="5217" endWordPosition="5220">gsma et al., 2010). We tried co-training without the N-gram features, and performance was worse on the WSJ (85%) than supervised training on WSJ data alone (87%). 9 Related Work Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. System Accuracy A Monolingual alone + Bilingual + Co-training + Bilingual &amp; Co-training 91.6 94.9 96.0 96.7 - 39% 54% 61% 1353 work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 200</context>
<context position="34424" citStr="Pitler et al., 2010" startWordPosition="5619" endWordPosition="5622">s. In their work, the bilingual view encodes the per-instance agreement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information as additional features. Adding new features without adding new training data is often problematic, but is promising in our framework, since the bitexts provide so much indirect supervision. 10 Conclusion Resolving coordination ambiguity is hard. Parsers are reporting impressive numbers these day</context>
</contexts>
<marker>Pitler, Bergsma, Lin, Church, 2010</marker>
<rawString>Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth Church. 2010. Using web-scale N-grams to improve base NP parsing performance. In In Proc. COLING, pages 886–894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--95</pages>
<contexts>
<context position="3027" citStr="Resnik, 1999" startWordPosition="465" endWordPosition="466"> of coordinate phrases from training text. We propose an approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one classifier by learning from the predictions of the other. Starting from only two 1346 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346–1355, Portland, Oregon</context>
<context position="5345" citStr="Resnik, 1999" startWordPosition="841" endWordPosition="842">are both adjectives, 26% are nouns, and 36% are mixed. The task is to determine whether w1 and w2 are conjoined or not. When they are not conjoined, there are two cases: 1) w1 is actually conjoined with w2 h as a whole (e.g. asbestos and polyvinyl chloride), or 2) The conjunction links something higher up in the parse tree, as in, “farmers are getting older\w1 and younger\w2 people\h are reluctant to take up farming.” Here, and links two separate clauses. Our task is both narrower and broader than previous work. It is broader than previous approaches that have focused only on conjoined nouns (Resnik, 1999; Nakov and Hearst, 2005). Although pairs of adjectives are usually conjoined (and mixed tags are usually not), this is not always true, as in older/younger above. For comparison, we also state accuracy on the noun-only examples (§ 8). Our task is more narrow than the task tackled by full-sentence parsers, but most parsers do not bracket NP-internal structure at all, since such structure is absent from the primary training corpus for statistical parsers, the Penn Treebank (Marcus et al., 1993). We confirm that standard broad-coverage parsers perform poorly on our task (§ 7). Vadas and Curran (</context>
<context position="34591" citStr="Resnik, 1999" startWordPosition="5648" endWordPosition="5649">rget text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information as additional features. Adding new features without adding new training data is often problematic, but is promising in our framework, since the bitexts provide so much indirect supervision. 10 Conclusion Resolving coordination ambiguity is hard. Parsers are reporting impressive numbers these days, but coordination remains an area with room for improvement. We focused on a specific subcase, complex NPs, and introduced a new evaluation set. We achieved a huge p</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal of Artificial Intelligence Research, 11:95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Sireesha Ravi</author>
<author>Mihai C Lintean</author>
<author>Philip M McCarthy</author>
</authors>
<title>Unsupervised method for parsing coordinated base noun phrases.</title>
<date>2007</date>
<booktitle>In Proc. CICLing,</booktitle>
<pages>229--240</pages>
<contexts>
<context position="3069" citStr="Rus et al., 2007" startWordPosition="471" endWordPosition="474">ext. We propose an approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one classifier by learning from the predictions of the other. Starting from only two 1346 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346–1355, Portland, Oregon, June 19-24, 2011. c�2011 Association for</context>
<context position="13378" citStr="Rus et al., 2007" startWordPosition="2216" endWordPosition="2219">g12h=(tag(w1),tag(w1),tag(h)) Table 2: Monolingual features. For counts using the filler sets CC, DT and PREP, counts are summed across all filler combinations. In contrast, feature templates are denoted with (·), where the feature label depends on the (bracketed argument). E.g., we have separate count feature for each item in the L/R context sets, where {L-CTXT} = {with, and, as, including, on, is, are, &amp;}, {R-CTXT} = {and, have, of, on, said, to, were, &amp;} data (§ 5). Previous approaches have used search engine page counts as substitutes for co-occurrence information (Nakov and Hearst, 2005; Rus et al., 2007). These approaches clearly cannot scale to use the wide range of information used in our system. Binary features Table 2 gives the binary features and feature templates. These are templates in the sense that every unique word or tag fills the template and corresponds to a unique feature. We can thus learn if particular words or tags are associated with ellipsis. We also include binary features to flag the presence of any optional determiners before w1 or w2. We also have binary features for the context words that precede and follow the tag sequence in the source corpus. These context features </context>
<context position="34402" citStr="Rus et al., 2007" startWordPosition="5615" endWordPosition="5618">gualview predictors. In their work, the bilingual view encodes the per-instance agreement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information as additional features. Adding new features without adding new training data is often problematic, but is promising in our framework, since the bitexts provide so much indirect supervision. 10 Conclusion Resolving coordination ambiguity is hard. Parsers are reporting impres</context>
</contexts>
<marker>Rus, Ravi, Lintean, McCarthy, 2007</marker>
<rawString>Vasile Rus, Sireesha Ravi, Mihai C. Lintean, and Philip M. McCarthy. 2007. Unsupervised method for parsing coordinated base noun phrases. In Proc. CICLing, pages 229–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Schwarck</author>
<author>Alexander Fraser</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Bitext-based resolution of German subject-object ambiguities.</title>
<date>2010</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>737--740</pages>
<marker>Schwarck, Fraser, Sch¨utze, 2010</marker>
<rawString>Florian Schwarck, Alexander Fraser, and Hinrich Sch¨utze. 2010. Bitext-based resolution of German subject-object ambiguities. In Proc. HLT-NAACL, pages 737–740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Schwartz</author>
<author>Takako Aikawa</author>
<author>Chris Quirk</author>
</authors>
<title>Disambiguation of English PP attachment using multilingual aligned data.</title>
<date>2003</date>
<booktitle>In Proc. MT Summit IX,</booktitle>
<pages>330--337</pages>
<contexts>
<context position="31587" citStr="Schwartz et al., 2003" startWordPosition="5165" endWordPosition="5168">nificantly better than the prior state-of-the-art Pitler et al. system (McNemar’s test, p&lt;0.05) and also exceeds the WSJ-trained system using monolingual features (p&lt;0.2). This domain robustness is less surprising given its key features are derived from webscale N-gram data; such features are known to generalize well across domains (Bergsma et al., 2010). We tried co-training without the N-gram features, and performance was worse on the WSJ (85%) than supervised training on WSJ data alone (87%). 9 Related Work Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 train</context>
</contexts>
<marker>Schwartz, Aikawa, Quirk, 2003</marker>
<rawString>Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003. Disambiguation of English PP attachment using multilingual aligned data. In Proc. MT Summit IX, pages 330–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: Using English to parse Korean. In</title>
<date>2004</date>
<booktitle>Proc. EMNLP,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="32653" citStr="Smith and Smith, 2004" startWordPosition="5343" endWordPosition="5346"> the features used in our Monolingual classifier. 6For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. System Accuracy A Monolingual alone + Bilingual + Co-training + Bilingual &amp; Co-training 91.6 94.9 96.0 96.7 - 39% 54% 61% 1353 work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Ca</context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: Using English to parse Korean. In Proc. EMNLP, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. ACL-IJCNLP,</booktitle>
<pages>1041--1050</pages>
<contexts>
<context position="32725" citStr="Snyder et al., 2009" startWordPosition="5356" endWordPosition="5359">ned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. System Accuracy A Monolingual alone + Bilingual + Co-training + Bilingual &amp; Co-training 91.6 94.9 96.0 96.7 - 39% 54% 61% 1353 work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extrac</context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In Proc. ACL-IJCNLP, pages 1041–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn Treebank. In</title>
<date>2007</date>
<booktitle>Proc. ACL,</booktitle>
<pages>240--247</pages>
<contexts>
<context position="5949" citStr="Vadas and Curran (2007" startWordPosition="937" endWordPosition="940">ouns (Resnik, 1999; Nakov and Hearst, 2005). Although pairs of adjectives are usually conjoined (and mixed tags are usually not), this is not always true, as in older/younger above. For comparison, we also state accuracy on the noun-only examples (§ 8). Our task is more narrow than the task tackled by full-sentence parsers, but most parsers do not bracket NP-internal structure at all, since such structure is absent from the primary training corpus for statistical parsers, the Penn Treebank (Marcus et al., 1993). We confirm that standard broad-coverage parsers perform poorly on our task (§ 7). Vadas and Curran (2007a) manually annotated NP structure in the Penn Treebank, and a few custom NP parsers have recently been developed using this data (Vadas and Curran, 2007b; Pitler et al., 2010). Our task is more narrow than the task handled by these parsers since we do not handle other, less-frequent and sometimes more complex constructions (e.g. robot arms and legs). However, such constructions are clearly amenable to our algorithm. In addition, these parsers have only evaluated coordination resolution within base NPs, simplifying the task and rendering the aforementioned older/younger problem moot. Finally, </context>
<context position="25240" citStr="Vadas and Curran (2007" startWordPosition="4144" endWordPosition="4147">ining, and align with the competitive-thresholding heuristic. The English portions of all bitexts are part-of-speech tagged with CRFTagger (Phan, 2006). 94K unique coordinate NPs and their translations are then extracted. Labeled Data For experiments within the parallel text, we manually labeled 1320 of the 94K coordinate NP examples. We use 605 examples to set development parameters, 607 examples as held-out test data, and 2, 10 or 100 examples for training. For experiments on the WSJ portion of the Penn Treebank, we merge the original Treebank annotations with the NP annotations provided by Vadas and Curran (2007a). We collect all coordinate NP sequences matching our pattern and collapse them into a single example. We label these instances by determining whether the annotations have w1 and w2 conjoined. In only one case did the same coordinate NP have different labels in different occurrences; this was clearly an error and resolved accordingly. We collected 1777 coordinate NPs in total, and divided them into 777 examples for training, 500 for development and 500 as a final held-out test set. 6 Evaluation and Settings We evaluate using accuracy: the percentage of examples classified correctly in held-o</context>
<context position="29917" citStr="Vadas and Curran (2007" startWordPosition="4884" endWordPosition="4887">not as much as co-training. With bilingual features and co-training together, we achieve 96.7% accuracy. This combined system could be used to very accurately resolve coordinate ambiguity in parallel data prior to training an MT system. 8 WSJ Experiments While we can now accurately resolve coordinate NP ambiguity in parallel text, it would be even better if this accuracy carried over to new domains, where bilingual features are not available. We test the robustness of our co-trained monolingual classifier by evaluating it on our labeled WSJ data. The Penn Treebank and the annotations added by Vadas and Curran (2007a) comprise a very special corpus; such data is clearly not available in every domain. We can take advantage of the plentiful labeled examples to also test how our co-trained system compares to supervised systems trained with inSystem Training WSJ Acc. Set # Nouns All Nakov &amp; Hearst - - 79.2 84.8 Tag-Triple WSJ 777 76.1 82.4 Pitler et al. WSJ 777 92.3 92.8 MonoWSJ WSJ 777 92.3 94.4 Co-trained Bitext 2 93.8 95.6 Table 7: Coordinate resolution accuracy (%) on WSJ. domain labeled examples, and also other systems, like Nakov and Hearst (2005), which although unsupervised, are tuned on WSJ data. We</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007a. Adding noun phrase structure to the Penn Treebank. In Proc. ACL, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Large-scale supervised models for noun phrase bracketing.</title>
<date>2007</date>
<booktitle>In PACLING,</booktitle>
<pages>104--112</pages>
<contexts>
<context position="5949" citStr="Vadas and Curran (2007" startWordPosition="937" endWordPosition="940">ouns (Resnik, 1999; Nakov and Hearst, 2005). Although pairs of adjectives are usually conjoined (and mixed tags are usually not), this is not always true, as in older/younger above. For comparison, we also state accuracy on the noun-only examples (§ 8). Our task is more narrow than the task tackled by full-sentence parsers, but most parsers do not bracket NP-internal structure at all, since such structure is absent from the primary training corpus for statistical parsers, the Penn Treebank (Marcus et al., 1993). We confirm that standard broad-coverage parsers perform poorly on our task (§ 7). Vadas and Curran (2007a) manually annotated NP structure in the Penn Treebank, and a few custom NP parsers have recently been developed using this data (Vadas and Curran, 2007b; Pitler et al., 2010). Our task is more narrow than the task handled by these parsers since we do not handle other, less-frequent and sometimes more complex constructions (e.g. robot arms and legs). However, such constructions are clearly amenable to our algorithm. In addition, these parsers have only evaluated coordination resolution within base NPs, simplifying the task and rendering the aforementioned older/younger problem moot. Finally, </context>
<context position="25240" citStr="Vadas and Curran (2007" startWordPosition="4144" endWordPosition="4147">ining, and align with the competitive-thresholding heuristic. The English portions of all bitexts are part-of-speech tagged with CRFTagger (Phan, 2006). 94K unique coordinate NPs and their translations are then extracted. Labeled Data For experiments within the parallel text, we manually labeled 1320 of the 94K coordinate NP examples. We use 605 examples to set development parameters, 607 examples as held-out test data, and 2, 10 or 100 examples for training. For experiments on the WSJ portion of the Penn Treebank, we merge the original Treebank annotations with the NP annotations provided by Vadas and Curran (2007a). We collect all coordinate NP sequences matching our pattern and collapse them into a single example. We label these instances by determining whether the annotations have w1 and w2 conjoined. In only one case did the same coordinate NP have different labels in different occurrences; this was clearly an error and resolved accordingly. We collected 1777 coordinate NPs in total, and divided them into 777 examples for training, 500 for development and 500 as a final held-out test set. 6 Evaluation and Settings We evaluate using accuracy: the percentage of examples classified correctly in held-o</context>
<context position="29917" citStr="Vadas and Curran (2007" startWordPosition="4884" endWordPosition="4887">not as much as co-training. With bilingual features and co-training together, we achieve 96.7% accuracy. This combined system could be used to very accurately resolve coordinate ambiguity in parallel data prior to training an MT system. 8 WSJ Experiments While we can now accurately resolve coordinate NP ambiguity in parallel text, it would be even better if this accuracy carried over to new domains, where bilingual features are not available. We test the robustness of our co-trained monolingual classifier by evaluating it on our labeled WSJ data. The Penn Treebank and the annotations added by Vadas and Curran (2007a) comprise a very special corpus; such data is clearly not available in every domain. We can take advantage of the plentiful labeled examples to also test how our co-trained system compares to supervised systems trained with inSystem Training WSJ Acc. Set # Nouns All Nakov &amp; Hearst - - 79.2 84.8 Tag-Triple WSJ 777 76.1 82.4 Pitler et al. WSJ 777 92.3 92.8 MonoWSJ WSJ 777 92.3 94.4 Co-trained Bitext 2 93.8 95.6 Table 7: Coordinate resolution accuracy (%) on WSJ. domain labeled examples, and also other systems, like Nakov and Hearst (2005), which although unsupervised, are tuned on WSJ data. We</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007b. Large-scale supervised models for noun phrase bracketing. In PACLING, pages 104–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Parsing noun phrase structure with CCG.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>104--112</pages>
<contexts>
<context position="28256" citStr="Vadas and Curran (2008)" startWordPosition="4621" endWordPosition="4624">raining alone, regardless of the features or amount of initial training data. The Tag-Triple system is the weakest system in all cases. This shows that better monolingual features are very important, but semisupervised training can also make a big difference. 3We provided the parsers full sentences containing the NPs. We directly extracted the labels from the C&amp;C bracketing, while for Minipar we checked whether w1 was the head of w2. Of course, the parsers performed very poorly on ellipsis involving two nouns (partly because NP structure is absent from their training corpora (see § 2 and also Vadas and Curran (2008)), but neither exceeded 88% on adjective or mixed pairs either. Bilingual View Monolingual View Combined Accuracy (%) 100 98 96 94 92 90 88 86 1352 System # of Examples 2 10 100 Tag-Triple classifier 67.4 79.1 82.9 Monolingual classifier 69.9 90.8 91.6 Co-trained Mono. classifier 96.4 95.9 96.0 Relative error reduction via co-training 88% 62% 52% Bilingual classifier 76.8 85.5 92.1 Co-trained Bili. classifier 93.2 93.2 93.9 Relative error reduction via co-training 71% 53% 23% Mono.+Bili. classifier 69.9 91.4 94.9 Co-trained Combo classifier 96.7 96.7 96.7 Relative error reduction via co-traini</context>
</contexts>
<marker>Vadas, Curran, 2008</marker>
<rawString>David Vadas and James R. Curran. 2008. Parsing noun phrase structure with CCG. In Proc. ACL, pages 104– 112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="9232" citStr="Vapnik, 1998" startWordPosition="1490" endWordPosition="1491">fproducts dairy and of meat asbestos and English: ... polyvinyl chloride and asbestos... w2 h and w1 polyvinyl English: ... asbestos , and polyvinyl chloride... w1 , and w2 h chloride English: ... asbestos and chloride... w1 and h (no ellipsis) Portuguese: ... o amianto e o cloreto de polivinilo... w1 ... h ... w2 → the asbestos and the chloride ofpolyvinyl w1 ... w2h Italian: ... l’ asbesto e il polivinilcloruro... → the asbestos and the polyvinylchloride Table 1: Monolingual and bilingual evidence for ellipsis or lack-of-ellipsis in coordination of [w1 and w2 h] phrases. number of features (Vapnik, 1998). In § 4, we propose a way to circumvent the need for labeled data. We now describe the particular monolingual and bilingual information we use for this problem. We refer to Table 1 for canonical examples of the two classes and also to provide intuition for the features. 3.1 Monolingual Features Count features These real-valued features encode the frequency, in a large auxiliary corpus, of relevant word sequences. Co-occurrence frequencies have long been used to resolve linguistic ambiguities (Dagan and Itai, 1990; Hindle and Rooth, 1993; Lauer, 1995). With the massive volumes of raw text now </context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="34127" citStr="Wu, 1997" startWordPosition="5571" endWordPosition="5572">ch copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also proposed exploiting monolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In</title>
<date>2001</date>
<booktitle>Proc. NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="32507" citStr="Yarowsky and Ngai, 2001" startWordPosition="5316" endWordPosition="5319">lished heuristics. 5Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. System Accuracy A Monolingual alone + Bilingual + Co-training + Bilingual &amp; Co-training 91.6 94.9 96.0 96.7 - 39% 54% 61% 1353 work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Proc. NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="21789" citStr="Yarowsky, 1995" startWordPosition="3572" endWordPosition="3573">ssifier training Create Lm ← L Create Lb ← L Create a pool Um by choosing um examples randomly from U. Create a pool Ub by choosing ub examples randomly from U. for i = 0 to k do Use Lm to train a classifier hm using only xm, the monolingual features of x� Use Lb to train a classifier hb using only xb, the bilingual features of x� Use hm to label Um, move the nm most-confident examples to Lb Use hb to label Ub, move the nb most-confident examples to Lm Replenish Um and Ub randomly from U with nm and nb new examples end for uncertain, and vice versa. This suggests using a co-training approach (Yarowsky, 1995; Blum and Mitchell, 1998). We train separate classifiers on the labeled data. We use the predictions of one classifier to label new examples for training the orthogonal classifier. We iterate this training and labeling. We outline how this procedure can be applied to bitext data in Algorithm 1 (above). We follow prior work in drawing predictions from smaller pools, Um and Ub, rather than from U itself, to ensure the labeled examples “are more representative of the underlying distribution” (Blum and Mitchell, 1998). We use a logistic regression classifier for hm and hb. Like Blum and Mitchell </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. ACL, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>