<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.084255">
<title confidence="0.9726085">
Automated Team Discourse Annotation and Performance Prediction
Using LSA
</title>
<author confidence="0.988171">
Melanie J. Martin
</author>
<affiliation confidence="0.8883835">
Department of Computer Science
New Mexico State University
</affiliation>
<address confidence="0.937062">
P.O. Box 30001, MSC CS
Las Cruces, New Mexico 88003-8001
</address>
<email confidence="0.999348">
mmartin@cs.nmsu.edu
</email>
<author confidence="0.905179">
Peter W. Foltz
</author>
<affiliation confidence="0.8698985">
Department of Psychology
New Mexico State University
</affiliation>
<address confidence="0.925691">
P.O. Box 30001, MSC 3452
Las Cruces, New Mexico 88003-8001
</address>
<email confidence="0.999145">
pfoltz@crl.nmsu.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998645">
We describe two approaches to analyzing and
tagging team discourse using Latent Semantic
Analysis (LSA) to predict team performance.
The first approach automatically categorizes
the contents of each statement made by each
of the three team members using an estab-
lished set of tags. Performance predicting the
tags automatically was 15% below human
agreement. These tagged statements are then
used to predict team performance. The second
approach measures the semantic content of the
dialogue of the team as a whole and accu-
rately predicts the team&apos;s performance on a
simulated military mission.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971022727273">
The growing complexity of tasks frequently surpasses
the cognitive capabilities of individuals and thus, often
necessitates a team approach. Teams play an increas-
ingly critical role in complex military operations in
which technological and information demands require a
multi-operator environment. The ability to automatically
predict team performance would be of great value for
team training systems.
Verbal communication data from teams provides a
rich indication of cognitive processing at both the indi-
vidual and the team level and can be tied back to both
the team&apos;s and each individual team member&apos;s abilities
and knowledge. The current manual analysis of team
communication shows promising results, see for exam-
ple, Bowers et al. (1998). Nevertheless, the analysis is
quite costly. Hand coding for content is time consum-
ing and can be highly subjective. Thus, what is required
are techniques for automatically analyzing team com-
munications in order to categorize and predict perform-
ance.
In the research described in this paper we apply La-
tent Semantic Analysis (LSA), to measure free-form
verbal interactions among team members. Because it
can measure and compare the semantic information in
these verbal interactions, it can be used to characterize
the quality and quantity of information expressed. This
can be used to determine the semantic content of any
utterance made by a team member as well as to measure
the semantic similarity of an entire team&apos;s communica-
tion to another team. In this paper we describe research
on developing automated techniques for analyzing the
communication and predicting team performance using
a corpus of communication of teams performing simu-
lated military missions. We focus on two applications of
this approach. The first application is to automatically
predict the categories of discourse for each utterance
made by team members during a mission. These tagged
statements can then be used to predict overall team per-
formance. The second application is to automatically
predict the effectiveness of a team based on an analysis
of the entire discourse of the team during a mission. We
then conclude with a discussion of how these techniques
can be applied for automatic communications analysis
and integrated into training.
</bodyText>
<sectionHeader confidence="0.988848" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999982027777778">
Our corpus (UAV-Corpus) consists of 67 transcripts
collected from 11 teams, who each completed 7 mis-
sions that simulate flight of an Uninhabited Air Vehicle
(UAV) in the CERTT (Cognitive Engineering Research
on Team Tasks) Lab&apos;s synthetic team task environment
(CERTT UAV-STE). CERTT&apos;s UAV-STE is a three-
team member task in which each team member is pro-
vided with distinct, though overlapping, training; has
unique, yet interdependent roles; and is presented with
different and overlapping information during the mis-
sion. The overall goal is to fly the UAV to designated
target areas and to take acceptable photos at these areas.
The 67 team-at-mission transcripts in the UAV-
Corpus contain approximately 2700 minutes of spoken
dialogue, in 20545 separate utterances or turns. There
are approximately 232,000 words or 660 KB of text. All
communication was manually transcribed.
We were provided with the results of manual anno-
tation of the corpus by three annotators using the Bow-
ers Tag Set (Bowers et al. 1998), which includes tags
for: acknowledgement, action, factual, planning, re-
sponse, uncertainty, and non-task related utterances.
The three annotators had each tagged 26 or 27 team-at-
missions so that 12 team-at-missions were tagged by
two annotators. Inter-coder reliability had been com-
puted using the C-value measure (Schvaneveldt, 1990).
The overall C-value for transcripts with two taggers was
0.70. We computed Cohen&apos;s Kappa to be 0.62 (see Sec-
tion 4 and Table 1).
In addition to the moderate level inter-coder agree-
ment, tagging was done at the turn level, where a turn
could range from a single word to several utterances by
a single speaker, and the number of tags that taggers
assigned to a given turn might not agree. We hope to
address these limitations in the data set with a more
thorough annotation study in the near future.
</bodyText>
<sectionHeader confidence="0.942164" genericHeader="method">
3 Latent Semantic Analysis
</sectionHeader>
<bodyText confidence="0.999986857142857">
LSA is a fully automatic corpus-based statistical method
for extracting and inferring relations of expected con-
textual usage of words in discourse (Landauer et al.,
1998).
LSA has been used for a wide range of applications
and for simulating knowledge representation, discourse
and psycholinguistic phenomena. These approaches
have included: information retrieval (Deerwester et al.,
1990), and automated text analysis (Foltz, 1996). In
addition, LSA has been applied to a number of NLP
tasks, such as text segmentation (Choi et al., 2001).
More recently Serafin et al. (2003) used LSA for dia-
logue act classification, finding that LSA can effectively
be used for such classification and that adding features
to LSA showed promise.
To train LSA we added 2257 documents to the cor-
pus UAV transcripts. These documents consisted of
training documents and pre- and post-training inter-
views related to UAVs, resulting in a total of 22802
documents in the final corpus. For the UAV-Corpus we
used a 300 dimensional semantic space.
</bodyText>
<sectionHeader confidence="0.993045" genericHeader="method">
4 Automatic Discourse Tagging
</sectionHeader>
<bodyText confidence="0.999945137931034">
Our goal was to use semantic content of team dialogues
to better understand and predict team performance. The
approach we focus on here is to study the dialogue on
the turn level. Working within the limitations of the
manual annotations, we developed an algorithm to tag
transcripts automatically, resulting in some decrease in
performance, but a significant savings in time and re-
sources.
We established a lower bounds tagging performance
of 0.27 by computing the tag frequency in the 12 tran-
scripts tagged by two taggers. If all utterances were
tagged with the most frequent tag, the percentage of
turns tagged correctly would be 27%.
Automatic Annotation with LSA. In order to test our
algorithm to automatically annotate the data, we com-
puted a &amp;quot;corrected tag&amp;quot; for all 2916 turns in the 12 team-
at-mission transcripts tagged by two taggers. This was
necessary due to the only moderate agreement between
the taggers. We used the union of the sets of tags as-
signed by the taggers as the &amp;quot;corrected tag&amp;quot;.
The union, rather than the intersection, was used
since taggers sometimes missed relevant tags within a
turn. The union of tags assigned by multiple taggers
better captures all likely tag types within the turn. A
disadvantage to using &amp;quot;corrected tags&amp;quot; is the loss of
sequential tag information within individual turns.
However the focus of this study was on identifying the
existence of relevant discourse, not on its order within
the turn.
Then, for each of the 12 team-at-mission transcripts,
we automatically assigned &amp;quot;most probable&amp;quot; tags to each
turn, based on the corrected tags of the &amp;quot;most similar&amp;quot;
turns in the other 11 team-at-missions. For a given turn,
T, the algorithm proceeds as follows:
Find the turns in the other 11 team-at-mission tran-
scripts, whose vectors in the semantic space have the
largest cosines, when compared with T&apos;s vector in the
semantic space. We choose either the ones with the top
n (usually top 10) cosines, or the ones whose cosines are
above a certain threshold (usually 0.6). The corrected
tags for these &amp;quot;most similar&amp;quot; turns are retrieved. The
sum of the cosines for each tag that appears is computed
and normalized to give a probability that the tag is the
corrected tag. Finally, we determine the predicted tag by
applying a cutoff (0.3 and 0.4 seem to produce the best
results): all of the tags above the cutoff are chosen as
the predicted tag. If no tag has a probability above the
cutoff, them the single tag with the maximum probabil-
ity is chosen as the predicted tag.
We also computed the average cosine similarity of T
to its 10 closest tags as a measure of certainty of catego-
rization. For example, if T is not similar to any previ-
ously categorized turns, then it would have a low
certainty. This permits the flagging of turns that the
algorithm is not likely to tag as reliability.
In order to improve our results, we considered ways
to incorporate simple discourse elements into our pre-
dictions. We added two discourse features to our algo-
rithm: for any turn with a question mark, &amp;quot;?&amp;quot;, we
increased to probability that uncertainty, &amp;quot;U&amp;quot;, would be
one of the tags in its predicted tag; and for any turn fol-
lowing a turn with a question mark, &amp;quot;T&amp;quot;, we increased to
probability that response, &amp;quot;R&amp;quot;, would be one of the tags
in its predicted tag.
We refer to our original algorithm as &amp;quot;LSA&amp;quot; and our
algorithm with the two discourse features added as
&amp;quot;LSA+&amp;quot;. Using LSA+ with our two methods now per-
forms only 11% and 15% below human-human agree-
ment (see Table 1).
We realize that training our system on tags where
humans had only moderate agreement is not ideal. Our
failure analyses indicated that the distinctions our algo-
rithm has difficulty making are the same distinctions
that the humans found difficult to make, so we believe
that improved agreement among human annotators
would result in similar improvements for our algorithm.
The results suggest that we can automatically anno-
tate team transcripts with tags. While the approach is
not quite as accurate as human taggers, LSA is able to
tag an hour of transcripts in under a minute. As a com-
parison, it can take half an hour or longer for a trained
tagger to do the same task.
Measuring Agreement. The C-value measures the pro-
portion of inter-coder agreement, but does not take into
account agreement by chance. In order to adjust for
chance agreement we computed Cohen&apos;s Kappa (Cohen
1960), as shown in Table 1.
</bodyText>
<table confidence="0.870031">
CODERS-AGREEMENT C-VALUE KAPPA
Human-Human 0.70 0.62
LSA-Human 0.59 0.48
LSA+-Human 0.63 0.53
</table>
<tableCaption confidence="0.998402">
Table 1. Kappa and C-Values.
</tableCaption>
<sectionHeader confidence="0.966814" genericHeader="method">
5 Predicting Overall Team Performance
</sectionHeader>
<bodyText confidence="0.993926269230769">
Throughout the CERTT Lab UAV-STE missions a per-
formance measure was calculated to determine each
team&apos;s effectiveness at completing the mission. The
performance score was a composite of objective meas-
ures including: amount of fuel/film used, number/type
of photographic errors, time spent in warning and alarm
states, and un-visited waypoints. This composite score
ranged from 0 to 1000. The score is highly predictive of
how well a team succeeded in accomplishing their mis-
sion. We used two approaches to predict these overall
team performance scores: correlating the tag frequencies
with the scores and by correlating entire mission tran-
scripts with one another.
Team Performance Based on Tags. We computed
correlations between the team performance score and
tag frequencies in each team-at-mission transcript.
The tags for all 20545 utterances were first gener-
ated using the LSA+ method. The tag frequencies for
each team-at-mission transcript were then computed by
counting the number of times each individual tag ap-
peared in the transcript and dividing by the total number
of individual tags occurring in the transcript.
Our preliminary results indicate that frequency of
certain types of utterances correlate with team perform-
ance. The correlations for tags predicted by computer
are shown in Table 2.
</bodyText>
<table confidence="0.99748">
TAG PEARSON SIG.
CORRELATION 2-TAILED
Acknowledgement 0.335 0.006
Fact 0.320 0.008
Response -0.321 0.008
Uncertainty -0.460 0.000
</table>
<tableCaption confidence="0.999894">
Table 2. Tag to Performance Correlations.
</tableCaption>
<bodyText confidence="0.974353722222222">
Table 2 shows that the automated tagging provides
useful results that can be interpreted in terms of team
processes. Teams that tend to state more facts and ac-
knowledge other team members more tend to perform
better. Those that express more uncertainty and need to
make more responses to each other tend to perform
worse. These results are consistent with those found in
Bowers et al. (1998), but were generated automatically
rather than by the hand-coding done by Bowers.
Team Performance Based on Whole Transcripts.
Another approach to measuring content in team dis-
course is to analyze the transcript as a whole. Using a
method similar to that used to score essays with LSA
(Landauer et al. 1998), we used the transcripts to predict
the team performance score. We generate the predicted
team performance scores was as follows: Given a sub-
set of transcripts, S, with known performance scores,
and a transcript, t, with unknown performance score, we
can estimate the performance score for t by computing
its similarity to each transcript in S. The similarity be-
tween any two transcripts is measured by the cosine
between the transcript vectors in the UAV-Corpus se-
mantic space. To compute the estimated score for t, we
take the average of the performance scores of the 10
closest transcripts in S, weighted by cosines. A holdout
procedure was used in which the score for a team&apos;s tran-
script was predicted based on the transcripts and scores
of all other teams (i.e. a team&apos;s score was only predicted
by the similarity to other teams). Our results indicated
that the LSA estimated performance scores correlated
strongly with the actual team performance scores (r =
0.76, p &lt; 0.01), as shown in Figure 1. Thus, the results
indicate that we can accurately predict the overall per-
formance of the team (i.e. how well they fly and com-
plete their mission) just based on an analysis of their
transcript from the mission.
</bodyText>
<figureCaption confidence="0.9780465">
Figure 1. Correlation: Predicted and Actual Team
Performance.
</figureCaption>
<sectionHeader confidence="0.997174" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999760565217391">
Overall, the results of the study show that LSA can be
used for tagging content as well as predicting team per-
formance based on team dialogues. Given the limita-
tions of the manual annotations, the results from the
tagging portion of the study are still comparable to other
efforts of automatic discourse tagging using different
methods and different corpora (Stolcke et al., 2000),
which found performance within 15% of the perform-
ance of human taggers. We plan to conduct a more rig-
orous manual annotation study. We expect that
improved human inter-coder reliability would eliminate
the need for &amp;quot;corrected tags&amp;quot; and allow for sequential
analysis of tags within turns. It is also anticipated that
incorporating additional methods that account for syntax
and discourse turns should further improve the overall
performance, see also Serafin et al. (2003).
Even with the limitations of the discourse tagging,
our LSA-based approach demonstrates it can be applied
as a method for doing automated measurement of team
performance. Using automatic methods we were able to
duplicate some of the results of Bowers, and colleagues,
(1998) who analyzed the sequence of content categories
occurring in communication in a flight simulator task.
They found that high team effectiveness was associated
with consistent responding to uncertainty, planning, and
fact statements with acknowledgments and responses.
The LSA-predicted team performance scores corre-
lated strongly with the actual team performance meas-
ures. This demonstrates that analyses of discourse can
automatically measure how well a team is performing
on a mission. This has implications both for automati-
cally determining what discourse characterizes good and
poor teams as well as developing systems for monitor-
ing team performance in near real-time. We are cur-
rently exploring two promising avenues to predict
performance in real time: integration of speech recogni-
tion technology, and inter-turn tag sequences.
Research into team discourse is a new but growing
area. However, up to recently, the large amounts of
transcript data have limited researchers from performing
analyses of team discourse. The results of this study
show that applying NLP techniques to team discourse
can provide accurate predictions of performance. These
automated tools can help inform theories of team per-
formance and also aid in the development of more ef-
fective automated team training systems.
</bodyText>
<sectionHeader confidence="0.996369" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9984098">
This research was completed in collaboration with the
CERTT laboratory including, Nancy Cooke, Steven
Shope, Preston Kiekel, Jamie Gorman and Susan Smith.
This work was supported by Office of Naval Research
and Army Research Laboratory grants.
</bodyText>
<sectionHeader confidence="0.999363" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998104">
C. A. Bowers, F. Jentsch, E. Salas, and C.C. Braun.
1998. Analyzing communication sequences for team
training needs assessment. Human Factors, 40, 672-
679.
F. Y. Y. Choi, P. Wiemer-Hastings, and J. D. Moore.
2001. Latent Semantic Analysis for Text Segmenta-
tion. In Proceedings of the 2001 Conference on Em-
pirical Methods in Natural Language Processing,
Lillian Lee and Donna Harman (Eds.), 109117.
J. Cohen. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Measure-
ment, 20, 34-46.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing By Latent
Semantic Analysis. Journal of the American Society
for Information Science, 41, 391-407.
P. W. Foltz. 1996. Latent Semantic Analysis for text-
based research. Behavior Research Methods, Instru-
ments and Computers. 28(2), 197-202.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. In-
troduction to Latent Semantic Analysis. Discourse
Processes, 25, 259-284.
R. W. Schvaneveldt. 1990. Pathfinder associative net-
works: Studies in knowledge organization. Nor-
wood, NJ: Ablex.
R. Serafin, B. Di Eugenio, and M. Glass. 2003. Latent
Semantic Analysis for dialogue act classification.
HLT-NAACL03, 2003 Human Language Technology
Conference, Edmonton, Canada, May (Short Paper)
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-
Dykema, and M. Meteer. 2000. Dialogue Act Mod-
eling for Automatic Tagging and Recognition of
Conversational Speech, Computational Linguistics
26(3), 339-373.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.036535">
<title confidence="0.9983725">Automated Team Discourse Annotation and Performance Prediction Using LSA</title>
<author confidence="0.999899">J Melanie</author>
<affiliation confidence="0.999256">Department of Computer</affiliation>
<address confidence="0.714839">New Mexico State P.O. Box 30001, MSC</address>
<author confidence="0.381937">Las Cruces</author>
<author confidence="0.381937">New Mexico</author>
<email confidence="0.999366">mmartin@cs.nmsu.edu</email>
<author confidence="0.983706">W Peter</author>
<affiliation confidence="0.6595515">Department of New Mexico State</affiliation>
<address confidence="0.69775">P.O. Box 30001, MSC</address>
<author confidence="0.466474">Las Cruces</author>
<author confidence="0.466474">New Mexico</author>
<email confidence="0.998549">pfoltz@crl.nmsu.edu</email>
<abstract confidence="0.999113466666667">We describe two approaches to analyzing and tagging team discourse using Latent Semantic Analysis (LSA) to predict team performance. The first approach automatically categorizes the contents of each statement made by each of the three team members using an established set of tags. Performance predicting the tags automatically was 15% below human agreement. These tagged statements are then used to predict team performance. The second approach measures the semantic content of the dialogue of the team as a whole and accurately predicts the team&apos;s performance on a simulated military mission.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C A Bowers</author>
<author>F Jentsch</author>
<author>E Salas</author>
<author>C C Braun</author>
</authors>
<title>Analyzing communication sequences for team training needs assessment.</title>
<date>1998</date>
<journal>Human Factors,</journal>
<volume>40</volume>
<pages>672--679</pages>
<contexts>
<context position="1737" citStr="Bowers et al. (1998)" startWordPosition="255" endWordPosition="258">a team approach. Teams play an increasingly critical role in complex military operations in which technological and information demands require a multi-operator environment. The ability to automatically predict team performance would be of great value for team training systems. Verbal communication data from teams provides a rich indication of cognitive processing at both the individual and the team level and can be tied back to both the team&apos;s and each individual team member&apos;s abilities and knowledge. The current manual analysis of team communication shows promising results, see for example, Bowers et al. (1998). Nevertheless, the analysis is quite costly. Hand coding for content is time consuming and can be highly subjective. Thus, what is required are techniques for automatically analyzing team communications in order to categorize and predict performance. In the research described in this paper we apply Latent Semantic Analysis (LSA), to measure free-form verbal interactions among team members. Because it can measure and compare the semantic information in these verbal interactions, it can be used to characterize the quality and quantity of information expressed. This can be used to determine the </context>
<context position="4280" citStr="Bowers et al. 1998" startWordPosition="659" endWordPosition="662">, training; has unique, yet interdependent roles; and is presented with different and overlapping information during the mission. The overall goal is to fly the UAV to designated target areas and to take acceptable photos at these areas. The 67 team-at-mission transcripts in the UAVCorpus contain approximately 2700 minutes of spoken dialogue, in 20545 separate utterances or turns. There are approximately 232,000 words or 660 KB of text. All communication was manually transcribed. We were provided with the results of manual annotation of the corpus by three annotators using the Bowers Tag Set (Bowers et al. 1998), which includes tags for: acknowledgement, action, factual, planning, response, uncertainty, and non-task related utterances. The three annotators had each tagged 26 or 27 team-atmissions so that 12 team-at-missions were tagged by two annotators. Inter-coder reliability had been computed using the C-value measure (Schvaneveldt, 1990). The overall C-value for transcripts with two taggers was 0.70. We computed Cohen&apos;s Kappa to be 0.62 (see Section 4 and Table 1). In addition to the moderate level inter-coder agreement, tagging was done at the turn level, where a turn could range from a single w</context>
<context position="12650" citStr="Bowers et al. (1998)" startWordPosition="2040" endWordPosition="2043">ons for tags predicted by computer are shown in Table 2. TAG PEARSON SIG. CORRELATION 2-TAILED Acknowledgement 0.335 0.006 Fact 0.320 0.008 Response -0.321 0.008 Uncertainty -0.460 0.000 Table 2. Tag to Performance Correlations. Table 2 shows that the automated tagging provides useful results that can be interpreted in terms of team processes. Teams that tend to state more facts and acknowledge other team members more tend to perform better. Those that express more uncertainty and need to make more responses to each other tend to perform worse. These results are consistent with those found in Bowers et al. (1998), but were generated automatically rather than by the hand-coding done by Bowers. Team Performance Based on Whole Transcripts. Another approach to measuring content in team discourse is to analyze the transcript as a whole. Using a method similar to that used to score essays with LSA (Landauer et al. 1998), we used the transcripts to predict the team performance score. We generate the predicted team performance scores was as follows: Given a subset of transcripts, S, with known performance scores, and a transcript, t, with unknown performance score, we can estimate the performance score for t </context>
</contexts>
<marker>Bowers, Jentsch, Salas, Braun, 1998</marker>
<rawString>C. A. Bowers, F. Jentsch, E. Salas, and C.C. Braun. 1998. Analyzing communication sequences for team training needs assessment. Human Factors, 40, 672-679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Y Y Choi</author>
<author>P Wiemer-Hastings</author>
<author>J D Moore</author>
</authors>
<title>Latent Semantic Analysis for Text Segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, Lillian Lee and Donna Harman (Eds.),</booktitle>
<pages>109--117</pages>
<contexts>
<context position="5682" citStr="Choi et al., 2001" startWordPosition="882" endWordPosition="885"> more thorough annotation study in the near future. 3 Latent Semantic Analysis LSA is a fully automatic corpus-based statistical method for extracting and inferring relations of expected contextual usage of words in discourse (Landauer et al., 1998). LSA has been used for a wide range of applications and for simulating knowledge representation, discourse and psycholinguistic phenomena. These approaches have included: information retrieval (Deerwester et al., 1990), and automated text analysis (Foltz, 1996). In addition, LSA has been applied to a number of NLP tasks, such as text segmentation (Choi et al., 2001). More recently Serafin et al. (2003) used LSA for dialogue act classification, finding that LSA can effectively be used for such classification and that adding features to LSA showed promise. To train LSA we added 2257 documents to the corpus UAV transcripts. These documents consisted of training documents and pre- and post-training interviews related to UAVs, resulting in a total of 22802 documents in the final corpus. For the UAV-Corpus we used a 300 dimensional semantic space. 4 Automatic Discourse Tagging Our goal was to use semantic content of team dialogues to better understand and pred</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>F. Y. Y. Choi, P. Wiemer-Hastings, and J. D. Moore. 2001. Latent Semantic Analysis for Text Segmentation. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, Lillian Lee and Donna Harman (Eds.), 109117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>20</volume>
<pages>34--46</pages>
<contexts>
<context position="10595" citStr="Cohen 1960" startWordPosition="1724" endWordPosition="1725">improved agreement among human annotators would result in similar improvements for our algorithm. The results suggest that we can automatically annotate team transcripts with tags. While the approach is not quite as accurate as human taggers, LSA is able to tag an hour of transcripts in under a minute. As a comparison, it can take half an hour or longer for a trained tagger to do the same task. Measuring Agreement. The C-value measures the proportion of inter-coder agreement, but does not take into account agreement by chance. In order to adjust for chance agreement we computed Cohen&apos;s Kappa (Cohen 1960), as shown in Table 1. CODERS-AGREEMENT C-VALUE KAPPA Human-Human 0.70 0.62 LSA-Human 0.59 0.48 LSA+-Human 0.63 0.53 Table 1. Kappa and C-Values. 5 Predicting Overall Team Performance Throughout the CERTT Lab UAV-STE missions a performance measure was calculated to determine each team&apos;s effectiveness at completing the mission. The performance score was a composite of objective measures including: amount of fuel/film used, number/type of photographic errors, time spent in warning and alarm states, and un-visited waypoints. This composite score ranged from 0 to 1000. The score is highly predicti</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, 34-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing By Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<pages>391--407</pages>
<contexts>
<context position="5532" citStr="Deerwester et al., 1990" startWordPosition="856" endWordPosition="859"> a single speaker, and the number of tags that taggers assigned to a given turn might not agree. We hope to address these limitations in the data set with a more thorough annotation study in the near future. 3 Latent Semantic Analysis LSA is a fully automatic corpus-based statistical method for extracting and inferring relations of expected contextual usage of words in discourse (Landauer et al., 1998). LSA has been used for a wide range of applications and for simulating knowledge representation, discourse and psycholinguistic phenomena. These approaches have included: information retrieval (Deerwester et al., 1990), and automated text analysis (Foltz, 1996). In addition, LSA has been applied to a number of NLP tasks, such as text segmentation (Choi et al., 2001). More recently Serafin et al. (2003) used LSA for dialogue act classification, finding that LSA can effectively be used for such classification and that adding features to LSA showed promise. To train LSA we added 2257 documents to the corpus UAV transcripts. These documents consisted of training documents and pre- and post-training interviews related to UAVs, resulting in a total of 22802 documents in the final corpus. For the UAV-Corpus we use</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing By Latent Semantic Analysis. Journal of the American Society for Information Science, 41, 391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
</authors>
<title>Latent Semantic Analysis for textbased research.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments and Computers.</journal>
<volume>28</volume>
<issue>2</issue>
<pages>197--202</pages>
<contexts>
<context position="5575" citStr="Foltz, 1996" startWordPosition="864" endWordPosition="865">assigned to a given turn might not agree. We hope to address these limitations in the data set with a more thorough annotation study in the near future. 3 Latent Semantic Analysis LSA is a fully automatic corpus-based statistical method for extracting and inferring relations of expected contextual usage of words in discourse (Landauer et al., 1998). LSA has been used for a wide range of applications and for simulating knowledge representation, discourse and psycholinguistic phenomena. These approaches have included: information retrieval (Deerwester et al., 1990), and automated text analysis (Foltz, 1996). In addition, LSA has been applied to a number of NLP tasks, such as text segmentation (Choi et al., 2001). More recently Serafin et al. (2003) used LSA for dialogue act classification, finding that LSA can effectively be used for such classification and that adding features to LSA showed promise. To train LSA we added 2257 documents to the corpus UAV transcripts. These documents consisted of training documents and pre- and post-training interviews related to UAVs, resulting in a total of 22802 documents in the final corpus. For the UAV-Corpus we used a 300 dimensional semantic space. 4 Autom</context>
</contexts>
<marker>Foltz, 1996</marker>
<rawString>P. W. Foltz. 1996. Latent Semantic Analysis for textbased research. Behavior Research Methods, Instruments and Computers. 28(2), 197-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<contexts>
<context position="5313" citStr="Landauer et al., 1998" startWordPosition="827" endWordPosition="830">d Cohen&apos;s Kappa to be 0.62 (see Section 4 and Table 1). In addition to the moderate level inter-coder agreement, tagging was done at the turn level, where a turn could range from a single word to several utterances by a single speaker, and the number of tags that taggers assigned to a given turn might not agree. We hope to address these limitations in the data set with a more thorough annotation study in the near future. 3 Latent Semantic Analysis LSA is a fully automatic corpus-based statistical method for extracting and inferring relations of expected contextual usage of words in discourse (Landauer et al., 1998). LSA has been used for a wide range of applications and for simulating knowledge representation, discourse and psycholinguistic phenomena. These approaches have included: information retrieval (Deerwester et al., 1990), and automated text analysis (Foltz, 1996). In addition, LSA has been applied to a number of NLP tasks, such as text segmentation (Choi et al., 2001). More recently Serafin et al. (2003) used LSA for dialogue act classification, finding that LSA can effectively be used for such classification and that adding features to LSA showed promise. To train LSA we added 2257 documents t</context>
<context position="12957" citStr="Landauer et al. 1998" startWordPosition="2091" endWordPosition="2094">interpreted in terms of team processes. Teams that tend to state more facts and acknowledge other team members more tend to perform better. Those that express more uncertainty and need to make more responses to each other tend to perform worse. These results are consistent with those found in Bowers et al. (1998), but were generated automatically rather than by the hand-coding done by Bowers. Team Performance Based on Whole Transcripts. Another approach to measuring content in team discourse is to analyze the transcript as a whole. Using a method similar to that used to score essays with LSA (Landauer et al. 1998), we used the transcripts to predict the team performance score. We generate the predicted team performance scores was as follows: Given a subset of transcripts, S, with known performance scores, and a transcript, t, with unknown performance score, we can estimate the performance score for t by computing its similarity to each transcript in S. The similarity between any two transcripts is measured by the cosine between the transcript vectors in the UAV-Corpus semantic space. To compute the estimated score for t, we take the average of the performance scores of the 10 closest transcripts in S, </context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T. K. Landauer, P. W. Foltz, and D. Laham. 1998. Introduction to Latent Semantic Analysis. Discourse Processes, 25, 259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Schvaneveldt</author>
</authors>
<title>Pathfinder associative networks: Studies in knowledge organization.</title>
<date>1990</date>
<publisher>Ablex.</publisher>
<location>Norwood, NJ:</location>
<contexts>
<context position="4616" citStr="Schvaneveldt, 1990" startWordPosition="708" endWordPosition="709">ialogue, in 20545 separate utterances or turns. There are approximately 232,000 words or 660 KB of text. All communication was manually transcribed. We were provided with the results of manual annotation of the corpus by three annotators using the Bowers Tag Set (Bowers et al. 1998), which includes tags for: acknowledgement, action, factual, planning, response, uncertainty, and non-task related utterances. The three annotators had each tagged 26 or 27 team-atmissions so that 12 team-at-missions were tagged by two annotators. Inter-coder reliability had been computed using the C-value measure (Schvaneveldt, 1990). The overall C-value for transcripts with two taggers was 0.70. We computed Cohen&apos;s Kappa to be 0.62 (see Section 4 and Table 1). In addition to the moderate level inter-coder agreement, tagging was done at the turn level, where a turn could range from a single word to several utterances by a single speaker, and the number of tags that taggers assigned to a given turn might not agree. We hope to address these limitations in the data set with a more thorough annotation study in the near future. 3 Latent Semantic Analysis LSA is a fully automatic corpus-based statistical method for extracting a</context>
</contexts>
<marker>Schvaneveldt, 1990</marker>
<rawString>R. W. Schvaneveldt. 1990. Pathfinder associative networks: Studies in knowledge organization. Norwood, NJ: Ablex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Serafin</author>
<author>B Di Eugenio</author>
<author>M Glass</author>
</authors>
<title>Latent Semantic Analysis for dialogue act classification. HLT-NAACL03,</title>
<date>2003</date>
<booktitle>Human Language Technology Conference,</booktitle>
<tech>(Short Paper)</tech>
<location>Edmonton, Canada,</location>
<marker>Serafin, Di Eugenio, Glass, 2003</marker>
<rawString>R. Serafin, B. Di Eugenio, and M. Glass. 2003. Latent Semantic Analysis for dialogue act classification. HLT-NAACL03, 2003 Human Language Technology Conference, Edmonton, Canada, May (Short Paper)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>R Martin</author>
<author>C Van EssDykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech,</title>
<date>2000</date>
<journal>Computational Linguistics</journal>
<volume>26</volume>
<issue>3</issue>
<pages>339--373</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van EssDykema, Meteer, 2000</marker>
<rawString>A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van EssDykema, and M. Meteer. 2000. Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech, Computational Linguistics 26(3), 339-373.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>