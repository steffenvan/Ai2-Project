<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000282">
<title confidence="0.997113">
Linguistic Features for Automatic Evaluation of Heterogenous MT Systems
</title>
<author confidence="0.980456">
Jes´us Gim´enez and Lluis M`arquez
</author>
<affiliation confidence="0.949622">
TALP Research Center, LSI Department
</affiliation>
<address confidence="0.740383">
Universitat Polit`ecnica de Catalunya
Jordi Girona Salgado 1–3, E-08034, Barcelona
</address>
<email confidence="0.999694">
{jgimenez,lluism}@lsi.upc.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996425">
Evaluation results recently reported by
Callison-Burch et al. (2006) and Koehn and
Monz (2006), revealed that, in certain cases,
the BLEU metric may not be a reliable MT
quality indicator. This happens, for in-
stance, when the systems under evaluation
are based on different paradigms, and there-
fore, do not share the same lexicon. The
reason is that, while MT quality aspects are
diverse, BLEU limits its scope to the lex-
ical dimension. In this work, we suggest
using metrics which take into account lin-
guistic features at more abstract levels. We
provide experimental results showing that
metrics based on deeper linguistic informa-
tion (syntactic/shallow-semantic) are able to
produce more reliable system rankings than
metrics based on lexical matching alone,
specially when the systems under evaluation
are of a different nature.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999734069767442">
Most metrics used in the context of Automatic Ma-
chine Translation (MT) Evaluation are based on
the assumption that ‘acceptable’ translations tend to
share the lexicon (i.e., word forms) in a predefined
set of manual reference translations. This assump-
tion works well in many cases. However, several
results in recent MT evaluation campaigns have cast
some doubts on its general validity. For instance,
Callison-Burch et al. (2006) and Koehn and Monz
(2006) reported and analyzed several cases of strong
disagreement between system rankings provided by
human assessors and those produced by the BLEU
metric (Papineni et al., 2001). In particular, they
noted that when the systems under evaluation are
of a different nature (e.g., rule-based vs. statistical,
human-aided vs. fully automatical, etc.) BLEU may
not be a reliable MT quality indicator. The reason is
that BLEU favours MT systems which share the ex-
pected reference lexicon (e.g., statistical systems),
and penalizes those which use a different one.
Indeed, the underlying cause is much simpler. In
general, lexical similarity is nor a sufficient neither
a necessary condition so that two sentences convey
the same meaning. On the contrary, natural lan-
guages are expressive and ambiguous at different
levels. Consequently, the similarity between two
sentences may involve different dimensions. In this
work, we hypothesize that, in order to ‘fairly’ evalu-
ate MT systems based on different paradigms, simi-
larities at more abstract linguistic levels must be an-
alyzed. For that purpose, we have compiled a rich
set of metrics operating at the lexical, syntactic and
shallow-semantic levels (see Section 2). We present
a comparative study on the behavior of several met-
ric representatives from each linguistic level in the
context of some of the cases reported by Koehn and
Monz (2006) and Callison-Burch et al. (2006) (see
Section 3). We show that metrics based on deeper
linguistic information (syntactic/shallow-semantic)
are able to produce more reliable system rankings
than those produced by metrics which limit their
scope to the lexical dimension, specially when the
systems under evaluation are of a different nature.
</bodyText>
<page confidence="0.972356">
256
</page>
<note confidence="0.9608745">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 256–264,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.97082" genericHeader="method">
2 A Heterogeneous Metric Set
</sectionHeader>
<bodyText confidence="0.9999776">
For our experiments, we have compiled a represen-
tative set of metrics1 at different linguistic levels.
We have resorted to several existing metrics, and
we have also developed new ones. Below, we group
them according to the level at which they operate.
</bodyText>
<subsectionHeader confidence="0.990546">
2.1 Lexical Similarity
</subsectionHeader>
<bodyText confidence="0.999171818181818">
Most of the current metrics operate at the lexical
level. We have selected 7 representatives from dif-
ferent families which have been shown to obtain
high levels of correlation with human assessments:
BLEU We use the default accumulated score up to
the level of 4-grams (Papineni et al., 2001).
NIST We use the default accumulated score up to
the level of 5-grams (Doddington, 2002).
GTM We set to 1 the value of the e parame-
ter (Melamed et al., 2003).
METEOR We run all modules: ‘exact’, ‘porter-
stem’, ‘wn stem’ and ‘wn synonymy’, in that
order (Banerjee and Lavie, 2005).
ROUGE We used the ROUGE-S* variant (skip bi-
grams with no max-gap-length). Stemming is
enabled (Lin and Och, 2004a).
mWER We use 1 − mWER (Nießen et al., 2000).
mPER We use 1 − mPER (Tillmann et al., 1997).
Let us note that ROUGE and METEOR may con-
sider stemming (i.e., morphological variations). Ad-
ditionally, METEOR may perform a lookup for syn-
onyms in WordNet (Fellbaum, 1998).
</bodyText>
<subsectionHeader confidence="0.998052">
2.2 Beyond Lexical Similarity
</subsectionHeader>
<bodyText confidence="0.999224">
Modeling linguistic features at levels further than
the lexical level requires the usage of more complex
linguistic structures. We have defined what we call
‘linguistic elements’ (LEs).
</bodyText>
<subsectionHeader confidence="0.855588">
2.2.1 Linguistic Elements
</subsectionHeader>
<bodyText confidence="0.9997954">
LEs are linguistic units, structures, or relation-
ships, such that a sentence may be partially seen as a
‘bag’ of LEs. Possible kinds of LEs are: word forms,
parts-of-speech, dependency relationships, syntactic
phrases, named entities, semantic roles, etc. Each
</bodyText>
<footnote confidence="0.997990333333333">
1All metrics used in this work are publicly available inside
the IQMT Framework (Gim6nez and Amig6, 2006). http://
www.lsi.upc.edu/˜nlp/IQMT
</footnote>
<bodyText confidence="0.98332">
LE may consist, in its turn, of one or more LEs,
which we call ‘items’ inside the LE. For instance, a
‘phrase’ LE may consist of ‘phrase’ items, ‘part-of-
speech’ (PoS) items, ‘word form’ items, etc. Items
may be also combinations of LEs. For instance, a
‘phrase’ LE may be seen as a sequence of ‘word-
form:PoS’ items.
</bodyText>
<subsectionHeader confidence="0.606046">
2.2.2 Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999681846153846">
We are interested in comparing linguistic struc-
tures, and linguistic units. LEs allow for compar-
isons at different granularity levels, and from dif-
ferent viewpoints. For instance, we might compare
the semantic structure of two sentences (i.e., which
actions, semantic arguments and adjuncts exist) or
we might compare lexical units according to the se-
mantic role they play inside the sentence. For that
purpose, we use two very simple kinds of similarity
measures over LEs: ‘Overlapping’ and ‘Matching’.
We provide a general definition:
Overlapping between items inside LEs, according
to their type. Formally:
</bodyText>
<equation confidence="0.99872725">
X count�hyp(i, t)
iEitemst(hyp)
E countref (i, t)
iEitemst(ref)
</equation>
<bodyText confidence="0.999724666666667">
where t is the LE type2, itemst(s) refers to the
set of items occurring inside LEs of type t in
sentence s, countref(i, t) denotes the number
of times item i appears in the reference trans-
lation inside a LE of type t, and count�hyp(i, t)
denotes the number of times i appears in the
candidate translation inside a LE of type t, lim-
ited by the number of times i appears in the ref-
erence translation inside a LE of type t. Thus,
‘Overlapping’ provides a rough measure of the
proportion of items inside elements of a cer-
tain type which have been ‘successfully’ trans-
lated. We also introduce a coarser metric, ‘Over-
lapping(*)’, which considers the uniformly aver-
aged ‘overlapping’ over all types:
</bodyText>
<equation confidence="0.9869755">
1 Overlapping(?) = |S |X Overlapping(t)
tET
</equation>
<bodyText confidence="0.715827">
where T is the set of types.
2LE types vary according to the specific LE class. For in-
stance, in the case of Named Entities types may be ‘PER’ (i.e.,
person), ‘LOC’ (i.e., location), ‘ORG’ (i.e., organization), etc.
</bodyText>
<equation confidence="0.983997">
Overlapping(t) =
</equation>
<page confidence="0.98142">
257
</page>
<bodyText confidence="0.999322818181818">
Matching between items inside LEs, according to
their type. Its definition is analogous to the
‘Overlapping’ definition, but in this case the
relative order of the items is important. All
items inside the same element are considered as
a single unit (i.e., a sequence in left-to-right or-
der). In other words, we are computing the pro-
portion of ‘fully’ translated elements, accord-
ing to their type. We also introduce a coarser
metric, ‘Matching(*)’, which considers the uni-
formly averaged ‘Matching’ over all types.
</bodyText>
<listItem confidence="0.952184727272727">
notes:
• ‘Overlapping’ and ‘Matching’ operate on the
assumption of a single reference translation.
The extension to the multi-reference setting is
computed by assigning the maximum value at-
tained over all human references individually.
• ‘Overlapping’ and ‘Matching’ are general met-
rics. We may apply them to specific scenarios
by defining the class of linguistic elements and
items to be used. Below, we instantiate these
measures over several particular cases.
</listItem>
<subsectionHeader confidence="0.995231">
2.3 Shallow Syntactic Similarity
</subsectionHeader>
<bodyText confidence="0.997915694444445">
Metrics based on shallow parsing (SP’) analyze
similarities at the level of PoS-tagging, lemmati-
zation, and base phrase chunking. Outputs and
references are automatically annotated using state-
of-the-art tools. PoS-tagging and lemmatization
are provided by the SVMTool package (Gim´enez and
M`arquez, 2004), and base phrase chunking is pro-
vided by the Phreco software (Carreras et al., 2005).
Tag sets for English are derived from the Penn Tree-
bank (Marcus et al., 1993).
We instantiate ‘Overlapping’ over parts-of-speech
and chunk types. The goal is to capture the propor-
tion of lexical items correctly translated, according
to their shallow syntactic realization:
SP-Op-t Lexical overlapping according to the part-
of-speech ‘t’. For instance, ‘SP-O,-NN’ roughly
reflects the proportion of correctly translated
singular nouns. We also introduce a coarser
metric, ‘SP-Op-*’ which computes average
overlapping over all parts-of-speech.
SP-Oc-t Lexical overlapping according to the
chunk type ‘t’. For instance, ‘SP-Oc-NP’ roughly
reflects the successfully translated proportion
of noun phrases. We also introduce a coarser
metric, ‘SP-Oc-*’ which considers the average
overlapping over all chunk types.
At a more abstract level, we use the NIST
metric (Doddington, 2002) to compute accumu-
lated/individual scores over sequences of:
Lemmas – SP-NIST(i)l-n
Parts-of-speech – SP-NIST(i)p-n
Base phrase chunks – SP-NIST(i)c-n
For instance, ‘SP-NISTl-5’ corresponds to the accu-
mulated NIST score for lemma n-grams up to length
5, whereas ‘SP-NISTi,-5’ corresponds to the individ-
ual NIST score for PoS 5-grams.
</bodyText>
<subsectionHeader confidence="0.996211">
2.4 Syntactic Similarity
</subsectionHeader>
<bodyText confidence="0.9996465">
We have incorporated, with minor modifications,
some of the syntactic metrics described by Liu and
Gildea (2005) and Amig´o et al. (2006) based on de-
pendency and constituency parsing.
</bodyText>
<subsectionHeader confidence="0.81786">
2.4.1 On Dependency Parsing (DP)
</subsectionHeader>
<bodyText confidence="0.998133521739131">
DP’ metrics capture similarities between depen-
dency trees associated to automatic and reference
translations. Dependency trees are provided by the
MINIPAR dependency parser (Lin, 1998). Similari-
ties are captured from different viewpoints:
DP-HWC(i)-l This metric corresponds to the HWC
metric presented by Liu and Gildea (2005). All
head-word chains are retrieved. The fraction of
matching head-word chains of a given length,
‘l’, is computed. We have slightly modified
this metric in order to distinguish three differ-
ent variants according to the type of items head-
word chains may consist of:
Lexical forms – DP-HWC(i)w-l
Grammatical categories – DP-HWC(i)c-l
Grammatical relationships – DP-HWC(i)r-l
Average accumulated scores up to a given chain
length may be used as well. For instance,
‘DP-HWCiw-4’ retrieves the proportion of match-
ing length-4 word-chains, whereas ‘DP-HWCw-
4’ retrieves average accumulated proportion of
matching word-chains up to length-4. Anal-
ogously, ‘DP-HWCc-4’, and ‘DP-HWCr-4’ com-
</bodyText>
<page confidence="0.98297">
258
</page>
<bodyText confidence="0.99449156">
pute average accumulated proportion of cate-
gory/relationship chains up to length-4.
DP-Ol|Oc|Or These metrics correspond exactly to
the LEVEL, GRAM and TREE metrics intro-
duced by Amig6 et al. (2006).
DP-Ol-l Overlapping between words hanging
at level ‘l’, or deeper.
DP-Oc-t Overlapping between words directly
hanging from terminal nodes (i.e. gram-
matical categories) of type ‘t’.
DP-Or-t Overlapping between words ruled
by non-terminal nodes (i.e. grammatical
relationships) of type ‘t’.
Node types are determined by grammatical cat-
egories and relationships defined by MINIPAR.
For instance, ‘DP-Or-s’ reflects lexical overlap-
ping between subtrees of type ‘s’ (subject). ‘DP-
Oc-A’ reflects lexical overlapping between ter-
minal nodes of type ‘A’ (Adjective/Adverbs).
‘DP-Ol-4’ reflects lexical overlapping between
nodes hanging at level 4 or deeper. Addition-
ally, we consider three coarser metrics (‘DP-Ol-
*’, ‘DP-Oc-*’ and ‘DP-Or-*’) which correspond
to the uniformly averaged values over all lev-
els, categories, and relationships, respectively.
</bodyText>
<subsectionHeader confidence="0.630044">
2.4.2 On Constituency Parsing (CP)
</subsectionHeader>
<bodyText confidence="0.9999536875">
‘CP’ metrics capture similarities between con-
stituency parse trees associated to automatic and
reference translations. Constituency trees are pro-
vided by the Charniak-Johnson’s Max-Ent reranking
parser (Charniak and Johnson, 2005).
CP-STM(i)-l This metric corresponds to the STM
metric presented by Liu and Gildea (2005).
All syntactic subpaths in the candidate and the
reference trees are retrieved. The fraction of
matching subpaths of a given length, ‘l’, is
computed. For instance, ‘CP-STMi-5’ retrieves
the proportion of length-5 matching subpaths.
Average accumulated scores may be computed
as well. For instance, ‘CP-STM-9’ retrieves av-
erage accumulated proportion of matching sub-
paths up to length-9.
</bodyText>
<subsectionHeader confidence="0.987512">
2.5 Shallow-Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.999941">
We have designed two new families of metrics, ‘NE’
and ‘SR’, which are intended to capture similari-
ties over Named Entities (NEs) and Semantic Roles
(SRs), respectively.
</bodyText>
<subsectionHeader confidence="0.744381">
2.5.1 On Named Entities (NE)
</subsectionHeader>
<bodyText confidence="0.999800857142857">
‘NE’ metrics analyze similarities between auto-
matic and reference translations by comparing the
NEs which occur in them. Sentences are automati-
cally annotated using the BIOS package (Surdeanu
et al., 2005). BIOS requires at the input shallow
parsed text, which is obtained as described in Sec-
tion 2.3. See the list of NE types in Table 1.
</bodyText>
<table confidence="0.99797325">
Type Description
ORG Organization
PER Person
LOC Location
MISC Miscellaneous
O Not-a-NE
DATE Temporal expressions
NUM Numerical expressions
ANGLE QUANTITY Quantities
DISTANCE QUANTITY
SIZE QUANTITY
SPEED QUANTITY
TEMPERATURE QUANTITY
WEIGHT QUANTITY
METHOD Other
MONEY
LANGUAGE
PERCENT
PROJECT
SYSTEM
</table>
<tableCaption confidence="0.998421">
Table 1: Named Entity types.
</tableCaption>
<bodyText confidence="0.9960246">
We define two types of metrics:
NE-Oe-t Lexical overlapping between NEs accord-
ing to their type t. For instance, ‘NE-Oe-PER’ re-
flects lexical overlapping between NEs of type
‘PER’ (i.e., person), which provides a rough es-
timate of the successfully translated proportion
of person names. The ‘NE-Oe-*’ metric consid-
ers the average lexical overlapping over all NE
types. This metric includes the NE type ‘O’
(i.e., Not-a-NE). We introduce another variant,
‘NE-Oe-**’, which considers only actual NEs.
NE-Me-t Lexical matching between NEs accord-
ing to their type t. For instance, ‘NE-Me-LOC’
reflects the proportion of fully translated NEs
of type ‘LOC’ (i.e., location). The ‘NE-Me-*’
</bodyText>
<page confidence="0.990526">
259
</page>
<bodyText confidence="0.992456833333333">
metric considers the average lexical matching
over all NE types, this time excluding type ‘O’.
Other authors have measured MT quality over
NEs in the recent literature. In particular, the ‘NE-
Me-*’ metric is similar to the ‘NEE’ metric defined
by Reeder et al. (2001).
</bodyText>
<subsectionHeader confidence="0.871589">
2.5.2 On Semantic Roles (SR)
</subsectionHeader>
<bodyText confidence="0.998761875">
SR’ metrics analyze similarities between auto-
matic and reference translations by comparing the
SRs (i.e., arguments and adjuncts) which occur in
them. Sentences are automatically annotated using
the SwiRL package (M`arquez et al., 2005). This
package requires at the input shallow parsed text en-
riched with NEs, which is obtained as described in
Section 2.5.1. See the list of SR types in Table 2.
</bodyText>
<table confidence="0.999755333333333">
Type Description
A0 arguments associated with a verb predicate,
A1 defined in the PropBank Frames scheme.
A2
A3
A4
A5
AA Causative agent
AM-ADV Adverbial (general-purpose) adjunct
AM-CAU Causal adjunct
AM-DIR Directional adjunct
AM-DIS Discourse marker
AM-EXT Extent adjunct
AM-LOC Locative adjunct
AM-MNR Manner adjunct
AM-MOD Modal adjunct
AM-NEG Negation marker
AM-PNC Purpose and reason adjunct
AM-PRD Predication adjunct
AM-REC Reciprocal adjunct
AM-TMP Temporal adjunct
</table>
<tableCaption confidence="0.994925">
Table 2: Semantic Roles.
</tableCaption>
<bodyText confidence="0.995682695652174">
We define three types of metrics:
SR-Or-t Lexical overlapping between SRs accord-
ing to their type t. For instance, ‘SR-O,.-A0’ re-
flects lexical overlapping between ‘A0’ argu-
ments. ‘SR-Or-*’ considers the average lexical
overlapping over all SR types.
SR-Mr-t Lexical matching between SRs accord-
ing to their type t. For instance, the met-
ric ‘SR-M,.-AM-MOD’ reflects the proportion of
fully translated modal adjuncts. The ‘SR-Mr-*’
metric considers the average lexical matching
over all SR types.
SR-Or This metric reflects ‘role overlapping’, i.e..
overlapping between semantic roles indepen-
dently from their lexical realization.
Note that in the same sentence several verbs, with
their respective SRs, may co-occur. However, the
metrics described above do not distinguish between
SRs associated to different verbs. In order to account
for such a distinction we introduce a more restric-
tive version of these metrics (‘SR-M,.,,-t’, ‘SR-O,.,,-t’,
‘SR-Mrv-*’, ‘SR-Orv-*’, and ‘SR-Orv’), which require
SRs to be associated to the same verb.
</bodyText>
<sectionHeader confidence="0.999888" genericHeader="method">
3 Experimental Work
</sectionHeader>
<bodyText confidence="0.9986824375">
In this section, we study the behavior of some
of the metrics described in Section 2, according
to the linguistic level at which they operate. We
have selected a set of coarse-grained metric vari-
ants (i.e., accumulated/average scores over linguis-
tic units and structures of different kinds)3. We ana-
lyze some of the cases reported by Koehn and Monz
(2006) and Callison-Burch et al. (2006). We distin-
guish different evaluation contexts. In Section 3.1,
we study the case of a single reference translation
being available. In principle, this scenario should
diminish the reliability of metrics based on lexical
matching alone, and favour metrics based on deeper
linguistic features. In Section 3.2, we study the case
of several reference translations available. This sce-
nario should alleviate the deficiencies caused by the
shallowness of metrics based on lexical matching.
We also analyze separately the case of homoge-
neous’ systems (i.e., all systems being of the same
nature), and the case of heterogenous’ systems (i.e.,
there exist systems based on different paradigms).
As to the metric meta-evaluation criterion, the two
most prominent criteria are:
Human Acceptability Metrics are evaluated on the
basis of correlation with human evaluators.
Human Likeness Metrics are evaluated in terms of
descriptive power, i.e., their ability to distin-
guish between human and automatic transla-
tions (Lin and Och, 2004b; Amig´o et al., 2005).
In our case, metrics are evaluated on the basis of
‘Human Acceptability’. Specifically, we use Pear-
son correlation coefficients between metric scores
</bodyText>
<footnote confidence="0.9029045">
3When computing ‘lexical’ overlapping/matching, we use
lemmas instead of word forms.
</footnote>
<page confidence="0.996141">
260
</page>
<bodyText confidence="0.999893333333333">
and the average sum of adequacy and fluency as-
sessments at the document level. The reason is
that meta-evaluation based on ‘Human Likeness’ re-
quires the availability of heterogenous test beds (i.e.,
representative sets of automatic outputs and human
references), which, unfortunately, is not the case of
all the tasks under study. First, because most transla-
tion systems are statistical. Second, because in most
cases only one reference translation is available.
</bodyText>
<subsectionHeader confidence="0.998161">
3.1 Single-reference Scenario
</subsectionHeader>
<bodyText confidence="0.965496444444445">
We use some of the test beds corresponding to
the NAACL 2006 Workshop on Statistical Machine
Translation” (WMT 2006) (Koehn and Monz, 2006).
Since linguistic features described in Section 2 are
so far implemented only for the case of English be-
ing the target language, among the 12 translation
tasks available, we studied only the 6 tasks corre-
sponding to the Foreign-to-English direction. A sin-
gle reference translation is available. System out-
puts consist of 2000 and 1064 sentences for the ‘in-
domain’ and ‘out-of-domain’ test beds, respectively.
In each case, human assessments on adequacy and
fluency are available for a subset of systems and sen-
tences. Table 3 shows the number of sentences as-
sessed in each case. Each sentence was evaluated
by two different human judges. System scores have
been obtained by averaging over all sentence scores.
in out sys
French-to-English 2,247 1,274 11/14
German-to-English 2,401 1,535 10/12
Spanish-to-English 1,944 1,070 11/15
Table 3: WMT 2006. ‘in’ and ‘out’ columns
show the number of sentences assessed for the ‘in-
domain’ and ‘out-of-domain’ subtasks. The ‘sys’
column shows the number of systems counting on
human assessments with respect to the total number
of systems which presented to each task.
</bodyText>
<subsectionHeader confidence="0.917723">
Evaluation of Heterogeneous Systems
</subsectionHeader>
<bodyText confidence="0.9995125">
In four of the six translation tasks under study, all
the systems are statistical except Systran’, which is
rule-based. This is the case of the German/French-
to-English in-domain/out-of-domain tasks. Table 4
shows correlation with human assessments for some
metric representatives at different linguistic levels.
</bodyText>
<table confidence="0.999826833333333">
Level Metric fr2en de2en
in out in out
1-PER 0.73 0.64 0.57 0.46
1-WER 0.73 0.73 0.32 0.38
BLEU 0.71 0.87 0.60 0.67
Lexical NIST 0.74 0.82 0.56 0.63
GTM 0.84 0.86 0.12 0.70
METEOR 0.92 0.95 0.76 0.81
ROUGE 0.85 0.89 0.65 0.79
SP-Op-* 0.81 0.88 0.64 0.71
SP-Oc-* 0.81 0.89 0.65 0.75
Shallow SP-NISTl-5 0.75 0.81 0.56 0.64
Syntactic SP-NISTp-5 0.75 0.91 0.77 0.77
SP-NISTc-5 0.73 0.88 0.71 0.54
DP-HWCw-4 0.76 0.88 0.64 0.74
DP-HWCc-4 0.93 0.97 0.88 0.72
DP-HWCr-4 0.92 0.96 0.91 0.76
Syntactic DP-Ol-* 0.87 0.94 0.84 0.84
DP-Oc-* 0.91 0.95 0.88 0.87
DP-Or-* 0.87 0.97 0.91 0.88
CP-STM-9 0.93 0.95 0.93 0.87
NE-Me-* 0.80 0.79 0.93 0.63
NE-Oe-* 0.79 0.76 0.91 0.59
NE-Oe-** 0.81 0.87 0.63 0.70
SR-Mr-* 0.83 0.95 0.92 0.84
Shallow SR-Or-* 0.89 0.95 0.88 0.90
Semantic SR-Or 0.95 0.85 0.80 0.75
SR-Mrv-* 0.77 0.92 0.72 0.85
SR-Orv-* 0.81 0.93 0.76 0.94
SR-Orv 0.84 0.93 0.81 0.92
</table>
<tableCaption confidence="0.99053">
Table 4: WMT 2006. Evaluation of Heterogeneous
</tableCaption>
<bodyText confidence="0.957231272727273">
Systems. French-to-English (fr2en) / German-to-
English (de2en), in-domain and out-of-domain.
Although the four cases are different, we have
identified several regularities. For instance, BLEU
and, in general, all metrics based on lexical match-
ing alone, except METEOR, obtain significantly
lower levels of correlation than metrics based on
deeper linguistic similarities. The problem with lex-
ical metrics is that they are unable to capture the ac-
tual quality of the ‘Systran’ system. Interestingly,
METEOR obtains a higher correlation, which, in
the case of French-to-English, rivals the top-scoring
metrics based on deeper linguistic features. The rea-
son, however, does not seem to be related to its ad-
ditional linguistic operations (i.e., stemming or syn-
onymy lookup), but rather to the METEOR matching
strategy itself (unigram precision/recall).
Metrics at the shallow syntactic level are in the
same range of lexical metrics. At the properly
syntactic level, metrics obtain in most cases high
correlation coefficients. However, the ‘DP-HWCw-4’
metric, which, although from the viewpoint of de-
</bodyText>
<page confidence="0.993395">
261
</page>
<bodyText confidence="0.999983176470588">
pendency relationships, still considers only lexical
matching, obtains a lower level of correlation. This
reinforces the idea that metrics based on rewarding
long n-grams matchings may not be a reliable qual-
ity indicator in these cases.
At the level of shallow semantics, while ‘NE’
metrics are not equally useful in all cases, ‘SR’ met-
rics prove very effective. For instance, correlation
attained by ‘SR-Or-*’ reveals that it is important to
translate lexical items according to the semantic role
they play inside the sentence. Moreover, correlation
attained by the ‘SR-Mr-*’ metric is a clear indication
that in order to achieve a high quality, it is impor-
tant to ‘fully’ translate ‘whole’ semantic structures
(i.e., arguments/adjuncts). The existence of all the
semantic structures (‘SR-Or’), specially associated to
the same verb (‘SR-Orv’), is also important.
</bodyText>
<subsectionHeader confidence="0.928857">
Evaluation of Homogeneous Systems
</subsectionHeader>
<bodyText confidence="0.999995">
In the two remaining tasks, Spanish-to-English
in-domain/out-of-domain, all the systems are sta-
tistical. Table 5 shows correlation with human as-
sessments for some metric representatives. In this
case, BLEU proves very effective, both in-domain
and out-of-domain. Indeed, all metrics based on lex-
ical matching obtain high levels of correlation with
human assessments. However, still metrics based on
deeper linguistic analysis attain in most cases higher
correlation coefficients, although not as significantly
higher as in the case of heterogeneous systems.
</bodyText>
<subsectionHeader confidence="0.999376">
3.2 Multiple-reference Scenario
</subsectionHeader>
<bodyText confidence="0.997847222222222">
We study the case reported by Callison-Burch et
al. (2006) in the context of the Arabic-to-English
exercise of the “2005 NIST MT Evaluation Cam-
paign4 (Le and Przybocki, 2005). In this case all
systems are statistical but ‘LinearB’, a human-aided
MT system (Callison-Burch, 2005). Five reference
translations are available. System outputs consist of
1056 sentences. We obtained permission5 to use 7
system outputs. For six of these systems we counted
</bodyText>
<footnote confidence="0.95161925">
4http://www.nist.gov/speech/tests/
summaries/2005/mt05.htm
5Due to data confidentiality, we contacted each participant
individually and asked for permission to use their data. A num-
</footnote>
<affiliation confidence="0.8608954">
ber of groups and companies responded positively: Univer-
sity of Southern California Information Sciences Institute (ISI),
University of Maryland (UMD), Johns Hopkins University &amp;
University of Cambridge (JHU-CU), IBM, University of Edin-
burgh, MITRE and LinearB.
</affiliation>
<table confidence="0.993728066666666">
es2en
Level Metric in out
Lexical 1-PER 0.82 0.78
1-WER 0.88 0.83
BLEU 0.89 0.87
NIST 0.88 0.84
GTM 0.86 0.80
METEOR 0.84 0.81
ROUGE 0.89 0.83
SP-Op-* 0.88 0.80
SP-Oc-* 0.89 0.84
Shallow SP-NISTl-5 0.88 0.85
Syntactic SP-NISTp-5 0.85 0.86
SP-NISTc-5 0.84 0.83
DP-HWCw-4 0.94 0.83
DP-HWCc-4 0.91 0.87
DP-HWCr-4 0.91 0.88
Syntactic DP-Ol-* 0.91 0.84
DP-Oc-* 0.88 0.83
DP-Or-* 0.88 0.84
CP-STM-9 0.89 0.86
NE-Me-* 0.75 0.76
NE-Oe-* 0.71 0.71
NE-Oe-** 0.88 0.80
SR-Mr-* 0.86 0.82
Shallow SR-Or-* 0.92 0.92
Semantic SR-Or 0.91 0.92
SR-Mrv-* 0.89 0.88
SR-Orv-* 0.91 0.92
SR-Orv 0.91 0.91
</table>
<tableCaption confidence="0.769416333333333">
Table 5: WMT 2006. Evaluation of Homogeneous
Systems. Spanish-to-English (es2en), in-domain
and out-of-domain.
</tableCaption>
<bodyText confidence="0.99976125">
on a subjective manual evaluation based on ade-
quacy and fluency for a subset of 266 sentences (i.e.,
1596 sentences were assessed). Each sentence was
evaluated by two different human judges. System
scores have been obtained by averaging over all sen-
tence scores.
Table 6 shows the level of correlation with hu-
man assessments for some metric representatives
(see ‘ALL’ column). In this case, lexical metrics
obtain extremely low levels of correlation. Again,
the problem is that lexical metrics are unable to cap-
ture the actual quality of ‘LinearB’. At the shallow
syntactic level, only metrics which do not consider
any lexical information (‘SP-NISTp-5’ and ‘SP-NISTc-
5’) attain a significantly higher quality. At the prop-
erly syntactic level, all metrics attain a higher corre-
lation. At the shallow semantic level, again, while
‘NE’ metrics are not specially useful, ‘SR’ metrics
prove very effective.
On the other hand, if we remove ‘LinearB’ (see
</bodyText>
<page confidence="0.990521">
262
</page>
<table confidence="0.992839533333334">
ar2en
Level Metric ALL SMT
Lexical 1-PER -0.35 0.75
1-WER -0.50 0.69
BLEU 0.06 0.83
NIST 0.04 0.81
GTM 0.03 0.92
ROUGE -0.17 0.81
METEOR 0.05 0.86
SP-Op-* 0.05 0.84
SP-Oc-* 0.12 0.89
Shallow SP-NISTl-5 0.04 0.82
Syntactic SP-NISTp-5 0.42 0.89
SP-NISTc-5 0.44 0.68
DP-HWCw-4 0.52 0.86
DP-HWCc-4 0.80 0.75
DP-HWCr-4 0.88 0.86
Syntactic DP-Ol-* 0.51 0.94
DP-Oc-* 0.53 0.91
DP-Or-* 0.72 0.93
CP-STM-9 0.74 0.95
NE-Me-* 0.33 0.78
NE-Oe-* 0.24 0.82
NE-Oe-** 0.04 0.81
SR-Mr-* 0.72 0.96
Shallow SR-Or-* 0.61 0.87
Semantic SR-Or 0.66 0.75
SR-Mrv-* 0.68 0.97
SR-Orv-* 0.47 0.84
SR-Orv 0.46 0.81
</table>
<tableCaption confidence="0.939729">
Table 6: NIST 2005. Arabic-to-English (ar2en) ex-
</tableCaption>
<bodyText confidence="0.98246275">
ercise. ‘ALL’ refers to the evaluation of all systems.
‘SMT’ refers to the evaluation of statistical systems
alone (i.e., removing ‘LinearB’).
‘SMT’ column), lexical metrics attain a much higher
correlation, in the same range of metrics based on
deeper linguistic information. However, still met-
rics based on syntactic parsing, and semantic roles,
exhibit a slightly higher quality.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.991997173913044">
We have presented a comparative study on the
behavior of a wide set of metrics for automatic
MT evaluation at different linguistic levels (lexical,
shallow-syntactic, syntactic, and shallow-semantic)
under different scenarios. We have shown, through
empirical evidence, that linguistic features at more
abstract levels may provide more reliable system
rankings, specially when the systems under evalu-
ation do not share the same lexicon.
We strongly believe that future MT evaluation
campaigns should benefit from these results, by in-
cluding metrics at different linguistic levels. For in-
stance, the following set could be used:
{ ‘DP-HWCr-4’, ‘DP-Oc-*’, ‘DP-Ol-*’, ‘DP-Or-*’, ‘CP-
STM-9’, ‘SR-Or-*’, ‘SR-Orv’ }
All these metrics are among the top-scoring in all
the translation tasks studied. However, none of these
metrics provides, in isolation, a ‘global’ measure of
quality. Indeed, all these metrics focus on ‘partial’
aspects of quality. We believe that, in order to per-
form ‘global’ evaluations, different quality dimen-
sions should be integrated into a single measure of
quality. With that purpose, we are currently explor-
ing several metric combination strategies. Prelim-
inary results, based on the QUEEN measure inside
the QARLA Framework (Amig´o et al., 2005), indi-
cate that metrics at different linguistic levels may be
robustly combined.
Experimental results also show that metrics re-
quiring linguistic analysis seem very robust against
parsing errors committed by automatic linguistic
processors, at least at the document level. That
is very interesting, taking into account that, while
reference translations are supposedly well formed,
that is not always the case of automatic translations.
However, it remains pending to test the behaviour at
the sentence level, which could be very useful for er-
ror analysis. Moreover, relying on automatic proces-
sors implies two other important limitations. First,
these tools are not available for all languages. Sec-
ond, usually they are too slow to allow for massive
evaluations, as required, for instance, in the case of
system development. In the future, we plan to incor-
porate more accurate, and possibly faster, linguistic
processors, also for languages other than English, as
they become publicly available.
</bodyText>
<sectionHeader confidence="0.996493" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999841272727273">
This research has been funded by the Span-
ish Ministry of Education and Science, projects
OpenMT (TIN2006-15307-C03-02) and TRAN-
GRAM (TIN2004-07925-C03-02). We are recog-
nized as a Quality Research Group (2005 SGR-
00130) by DURSI, the Research Department of the
Catalan Government. Authors are thankful to the
WMT organizers for providing such valuable test
beds. Authors are also thankful to Audrey Le (from
NIST), and to the 2005 NIST MT Evaluation Cam-
paign participants who agreed to share their system
</bodyText>
<page confidence="0.992718">
263
</page>
<bodyText confidence="0.992512">
outputs and human assessments for the purpose of
this research.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. Technical
report, NIST, August.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999810202247191">
Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Sumarization. In Proceed-
ings of the 43th Annual Meeting of the Association for
Computational Linguistics.
Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Lluis
M`arquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of COLING-ACL06.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in Ma-
chine Translation Research. In Proceedings of EACL.
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Xavier Carreras, Lluis M´arquez, and Jorge Castro. 2005.
Filtering-ranking perceptron learning for partial pars-
ing. Machine Learning, 59:1–31.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings ofACL.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the 2nd IHLT.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
Jes´us Gim´enez and Enrique Amig´o. 2006. IQMT: A
Framework for Automatic Machine Translation Eval-
uation. In Proceedings of the 5th LREC.
Jes´us Gim´enez and Lluis M`arquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of 4th LREC.
Philipp Koehn and Christof Monz. 2006. Manual and
Automatic Evaluation of Machine Translation between
European Languages. In Proceedings of the Workshop
on Statistical Machine Translation, pages 102–121.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings ofACL.
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of COLING.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation ofParsing Systems.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings ofACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313–330.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings ofHLT/NAACL.
Lluis M`arquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. Robust Combination Strat-
egy for Semantic Role Labeling. In Proceedings of
HLT/EMNLP.
S. Nießen, F.J. Och, G. Leusch, and H. Ney. 2000. Eval-
uation Tool for Machine Translation: Fast Evaluation
for MT Research. In Proceedings of the 2nd LREC.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalu-
ation of machine translation, rc22176, ibm. Technical
report, IBM T.J. Watson Research Center.
Florence Reeder, Keith Miller, Jennifer Doyon, and John
White. 2001. The Naming of Things and the Con-
fusion of Tongues: an MT Metric. In Proceedings
of the Workshop on MT Evaluation ”Who did what to
whom?” at MT Summit VIII, pages 55–59.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proceedings of the 9th Inter-
national Conference on Speech Communication and
Technology (Interspeech).
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based Search for Statistical
Translation. In Proceedings of European Conference
on Speech Communication and Technology.
</reference>
<page confidence="0.998182">
264
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.316629">
<title confidence="0.999798">Linguistic Features for Automatic Evaluation of Heterogenous MT Systems</title>
<author confidence="0.976897">Gim´enez</author>
<affiliation confidence="0.975904">TALP Research Center, LSI Universitat Polit`ecnica de</affiliation>
<note confidence="0.343863">Jordi Girona Salgado 1–3, E-08034,</note>
<abstract confidence="0.999363619047619">Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006), revealed that, in certain cases, may not be a reliable MT quality indicator. This happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon. The reason is that, while MT quality aspects are its scope to the lexical dimension. In this work, we suggest using metrics which take into account linguistic features at more abstract levels. We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Anselmo Pe˜nas</author>
<author>Felisa Verdejo</author>
</authors>
<title>QARLA: a Framework for the Evaluation of Automatic Sumarization.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Amig´o, Gonzalo, Pe˜nas, Verdejo, 2005</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas, and Felisa Verdejo. 2005. QARLA: a Framework for the Evaluation of Automatic Sumarization. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Jes´us Gim´enez</author>
<author>Julio Gonzalo</author>
<author>Lluis M`arquez</author>
</authors>
<title>MT Evaluation: Human-Like vs. Human Acceptable.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL06.</booktitle>
<marker>Amig´o, Gim´enez, Gonzalo, M`arquez, 2006</marker>
<rawString>Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Lluis M`arquez. 2006. MT Evaluation: Human-Like vs. Human Acceptable. In Proceedings of COLING-ACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="4330" citStr="Banerjee and Lavie, 2005" startWordPosition="669" endWordPosition="672">e level at which they operate. 2.1 Lexical Similarity Most of the current metrics operate at the lexical level. We have selected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). mWER We use 1 − mWER (Nießen et al., 2000). mPER We use 1 − mPER (Tillmann et al., 1997). Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998). 2.2 Beyond Lexical Similarity Modeling linguistic features at levels further than the lexical level requires the usage of more complex linguistic structures. We have defined what we call ‘linguistic elements’ (LEs). 2.</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the Role of BLEU in Machine Translation Research.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="1544" citStr="Callison-Burch et al. (2006)" startWordPosition="227" endWordPosition="230">allow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature. 1 Introduction Most metrics used in the context of Automatic Machine Translation (MT) Evaluation are based on the assumption that ‘acceptable’ translations tend to share the lexicon (i.e., word forms) in a predefined set of manual reference translations. This assumption works well in many cases. However, several results in recent MT evaluation campaigns have cast some doubts on its general validity. For instance, Callison-Burch et al. (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001). In particular, they noted that when the systems under evaluation are of a different nature (e.g., rule-based vs. statistical, human-aided vs. fully automatical, etc.) BLEU may not be a reliable MT quality indicator. The reason is that BLEU favours MT systems which share the expected reference lexicon (e.g., statistical systems), and penalizes those which use a different one. Indeed, the underlyin</context>
<context position="2987" citStr="Callison-Burch et al. (2006)" startWordPosition="454" endWordPosition="457">s at different levels. Consequently, the similarity between two sentences may involve different dimensions. In this work, we hypothesize that, in order to ‘fairly’ evaluate MT systems based on different paradigms, similarities at more abstract linguistic levels must be analyzed. For that purpose, we have compiled a rich set of metrics operating at the lexical, syntactic and shallow-semantic levels (see Section 2). We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006) (see Section 3). We show that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than those produced by metrics which limit their scope to the lexical dimension, specially when the systems under evaluation are of a different nature. 256 Proceedings of the Second Workshop on Statistical Machine Translation, pages 256–264, Prague, June 2007. c�2007 Association for Computational Linguistics 2 A Heterogeneous Metric Set For our experiments, we have compiled a representative set of metrics1 at different linguistic levels. W</context>
<context position="17228" citStr="Callison-Burch et al. (2006)" startWordPosition="2636" endWordPosition="2639">n order to account for such a distinction we introduce a more restrictive version of these metrics (‘SR-M,.,,-t’, ‘SR-O,.,,-t’, ‘SR-Mrv-*’, ‘SR-Orv-*’, and ‘SR-Orv’), which require SRs to be associated to the same verb. 3 Experimental Work In this section, we study the behavior of some of the metrics described in Section 2, according to the linguistic level at which they operate. We have selected a set of coarse-grained metric variants (i.e., accumulated/average scores over linguistic units and structures of different kinds)3. We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006). We distinguish different evaluation contexts. In Section 3.1, we study the case of a single reference translation being available. In principle, this scenario should diminish the reliability of metrics based on lexical matching alone, and favour metrics based on deeper linguistic features. In Section 3.2, we study the case of several reference translations available. This scenario should alleviate the deficiencies caused by the shallowness of metrics based on lexical matching. We also analyze separately the case of homogeneous’ systems (i.e., all systems being of the same nature), and the ca</context>
<context position="24172" citStr="Callison-Burch et al. (2006)" startWordPosition="3703" endWordPosition="3706"> tasks, Spanish-to-English in-domain/out-of-domain, all the systems are statistical. Table 5 shows correlation with human assessments for some metric representatives. In this case, BLEU proves very effective, both in-domain and out-of-domain. Indeed, all metrics based on lexical matching obtain high levels of correlation with human assessments. However, still metrics based on deeper linguistic analysis attain in most cases higher correlation coefficients, although not as significantly higher as in the case of heterogeneous systems. 3.2 Multiple-reference Scenario We study the case reported by Callison-Burch et al. (2006) in the context of the Arabic-to-English exercise of the “2005 NIST MT Evaluation Campaign4 (Le and Przybocki, 2005). In this case all systems are statistical but ‘LinearB’, a human-aided MT system (Callison-Burch, 2005). Five reference translations are available. System outputs consist of 1056 sentences. We obtained permission5 to use 7 system outputs. For six of these systems we counted 4http://www.nist.gov/speech/tests/ summaries/2005/mt05.htm 5Due to data confidentiality, we contacted each participant individually and asked for permission to use their data. A number of groups and companie</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the Role of BLEU in Machine Translation Research. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Linear B system description for the 2005 NIST MT evaluation exercise.</title>
<date>2005</date>
<booktitle>In Proceedings of the NIST 2005 Machine Translation Evaluation Workshop.</booktitle>
<contexts>
<context position="24393" citStr="Callison-Burch, 2005" startWordPosition="3739" endWordPosition="3740">out-of-domain. Indeed, all metrics based on lexical matching obtain high levels of correlation with human assessments. However, still metrics based on deeper linguistic analysis attain in most cases higher correlation coefficients, although not as significantly higher as in the case of heterogeneous systems. 3.2 Multiple-reference Scenario We study the case reported by Callison-Burch et al. (2006) in the context of the Arabic-to-English exercise of the “2005 NIST MT Evaluation Campaign4 (Le and Przybocki, 2005). In this case all systems are statistical but ‘LinearB’, a human-aided MT system (Callison-Burch, 2005). Five reference translations are available. System outputs consist of 1056 sentences. We obtained permission5 to use 7 system outputs. For six of these systems we counted 4http://www.nist.gov/speech/tests/ summaries/2005/mt05.htm 5Due to data confidentiality, we contacted each participant individually and asked for permission to use their data. A number of groups and companies responded positively: University of Southern California Information Sciences Institute (ISI), University of Maryland (UMD), Johns Hopkins University &amp; University of Cambridge (JHU-CU), IBM, University of Edinburgh, MITR</context>
</contexts>
<marker>Callison-Burch, 2005</marker>
<rawString>Chris Callison-Burch. 2005. Linear B system description for the 2005 NIST MT evaluation exercise. In Proceedings of the NIST 2005 Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M´arquez</author>
<author>Jorge Castro</author>
</authors>
<title>Filtering-ranking perceptron learning for partial parsing.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>59--1</pages>
<marker>Carreras, M´arquez, Castro, 2005</marker>
<rawString>Xavier Carreras, Lluis M´arquez, and Jorge Castro. 2005. Filtering-ranking perceptron learning for partial parsing. Machine Learning, 59:1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="12522" citStr="Charniak and Johnson, 2005" startWordPosition="1919" endWordPosition="1922">flects lexical overlapping between terminal nodes of type ‘A’ (Adjective/Adverbs). ‘DP-Ol-4’ reflects lexical overlapping between nodes hanging at level 4 or deeper. Additionally, we consider three coarser metrics (‘DP-Ol*’, ‘DP-Oc-*’ and ‘DP-Or-*’) which correspond to the uniformly averaged values over all levels, categories, and relationships, respectively. 2.4.2 On Constituency Parsing (CP) ‘CP’ metrics capture similarities between constituency parse trees associated to automatic and reference translations. Constituency trees are provided by the Charniak-Johnson’s Max-Ent reranking parser (Charniak and Johnson, 2005). CP-STM(i)-l This metric corresponds to the STM metric presented by Liu and Gildea (2005). All syntactic subpaths in the candidate and the reference trees are retrieved. The fraction of matching subpaths of a given length, ‘l’, is computed. For instance, ‘CP-STMi-5’ retrieves the proportion of length-5 matching subpaths. Average accumulated scores may be computed as well. For instance, ‘CP-STM-9’ retrieves average accumulated proportion of matching subpaths up to length-9. 2.5 Shallow-Semantic Similarity We have designed two new families of metrics, ‘NE’ and ‘SR’, which are intended to captur</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd IHLT.</booktitle>
<contexts>
<context position="4140" citStr="Doddington, 2002" startWordPosition="635" endWordPosition="636">representative set of metrics1 at different linguistic levels. We have resorted to several existing metrics, and we have also developed new ones. Below, we group them according to the level at which they operate. 2.1 Lexical Similarity Most of the current metrics operate at the lexical level. We have selected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). mWER We use 1 − mWER (Nießen et al., 2000). mPER We use 1 − mPER (Tillmann et al., 1997). Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998). 2.2 Beyond Lexical Similarit</context>
<context position="9624" citStr="Doddington, 2002" startWordPosition="1506" endWordPosition="1507">c realization: SP-Op-t Lexical overlapping according to the partof-speech ‘t’. For instance, ‘SP-O,-NN’ roughly reflects the proportion of correctly translated singular nouns. We also introduce a coarser metric, ‘SP-Op-*’ which computes average overlapping over all parts-of-speech. SP-Oc-t Lexical overlapping according to the chunk type ‘t’. For instance, ‘SP-Oc-NP’ roughly reflects the successfully translated proportion of noun phrases. We also introduce a coarser metric, ‘SP-Oc-*’ which considers the average overlapping over all chunk types. At a more abstract level, we use the NIST metric (Doddington, 2002) to compute accumulated/individual scores over sequences of: Lemmas – SP-NIST(i)l-n Parts-of-speech – SP-NIST(i)p-n Base phrase chunks – SP-NIST(i)c-n For instance, ‘SP-NISTl-5’ corresponds to the accumulated NIST score for lemma n-grams up to length 5, whereas ‘SP-NISTi,-5’ corresponds to the individual NIST score for PoS 5-grams. 2.4 Syntactic Similarity We have incorporated, with minor modifications, some of the syntactic metrics described by Liu and Gildea (2005) and Amig´o et al. (2006) based on dependency and constituency parsing. 2.4.1 On Dependency Parsing (DP) DP’ metrics capture simi</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the 2nd IHLT.</rawString>
</citation>
<citation valid="true">
<title>WordNet. An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet. An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Enrique Amig´o</author>
</authors>
<title>IQMT: A Framework for Automatic Machine Translation Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th LREC.</booktitle>
<marker>Gim´enez, Amig´o, 2006</marker>
<rawString>Jes´us Gim´enez and Enrique Amig´o. 2006. IQMT: A Framework for Automatic Machine Translation Evaluation. In Proceedings of the 5th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of 4th LREC.</booktitle>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2004. SVMTool: A general POS tagger generator based on Support Vector Machines. In Proceedings of 4th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and Automatic Evaluation of Machine Translation between European Languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="1570" citStr="Koehn and Monz (2006)" startWordPosition="232" endWordPosition="235">ce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature. 1 Introduction Most metrics used in the context of Automatic Machine Translation (MT) Evaluation are based on the assumption that ‘acceptable’ translations tend to share the lexicon (i.e., word forms) in a predefined set of manual reference translations. This assumption works well in many cases. However, several results in recent MT evaluation campaigns have cast some doubts on its general validity. For instance, Callison-Burch et al. (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001). In particular, they noted that when the systems under evaluation are of a different nature (e.g., rule-based vs. statistical, human-aided vs. fully automatical, etc.) BLEU may not be a reliable MT quality indicator. The reason is that BLEU favours MT systems which share the expected reference lexicon (e.g., statistical systems), and penalizes those which use a different one. Indeed, the underlying cause is much simpler. I</context>
<context position="2954" citStr="Koehn and Monz (2006)" startWordPosition="449" endWordPosition="452">re expressive and ambiguous at different levels. Consequently, the similarity between two sentences may involve different dimensions. In this work, we hypothesize that, in order to ‘fairly’ evaluate MT systems based on different paradigms, similarities at more abstract linguistic levels must be analyzed. For that purpose, we have compiled a rich set of metrics operating at the lexical, syntactic and shallow-semantic levels (see Section 2). We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006) (see Section 3). We show that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than those produced by metrics which limit their scope to the lexical dimension, specially when the systems under evaluation are of a different nature. 256 Proceedings of the Second Workshop on Statistical Machine Translation, pages 256–264, Prague, June 2007. c�2007 Association for Computational Linguistics 2 A Heterogeneous Metric Set For our experiments, we have compiled a representative set of metrics1 </context>
<context position="17195" citStr="Koehn and Monz (2006)" startWordPosition="2631" endWordPosition="2634">ated to different verbs. In order to account for such a distinction we introduce a more restrictive version of these metrics (‘SR-M,.,,-t’, ‘SR-O,.,,-t’, ‘SR-Mrv-*’, ‘SR-Orv-*’, and ‘SR-Orv’), which require SRs to be associated to the same verb. 3 Experimental Work In this section, we study the behavior of some of the metrics described in Section 2, according to the linguistic level at which they operate. We have selected a set of coarse-grained metric variants (i.e., accumulated/average scores over linguistic units and structures of different kinds)3. We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006). We distinguish different evaluation contexts. In Section 3.1, we study the case of a single reference translation being available. In principle, this scenario should diminish the reliability of metrics based on lexical matching alone, and favour metrics based on deeper linguistic features. In Section 3.2, we study the case of several reference translations available. This scenario should alleviate the deficiencies caused by the shallowness of metrics based on lexical matching. We also analyze separately the case of homogeneous’ systems (i.e., all systems bein</context>
<context position="19147" citStr="Koehn and Monz, 2006" startWordPosition="2926" endWordPosition="2929">age sum of adequacy and fluency assessments at the document level. The reason is that meta-evaluation based on ‘Human Likeness’ requires the availability of heterogenous test beds (i.e., representative sets of automatic outputs and human references), which, unfortunately, is not the case of all the tasks under study. First, because most translation systems are statistical. Second, because in most cases only one reference translation is available. 3.1 Single-reference Scenario We use some of the test beds corresponding to the NAACL 2006 Workshop on Statistical Machine Translation” (WMT 2006) (Koehn and Monz, 2006). Since linguistic features described in Section 2 are so far implemented only for the case of English being the target language, among the 12 translation tasks available, we studied only the 6 tasks corresponding to the Foreign-to-English direction. A single reference translation is available. System outputs consist of 2000 and 1064 sentences for the ‘indomain’ and ‘out-of-domain’ test beds, respectively. In each case, human assessments on adequacy and fluency are available for a subset of systems and sentences. Table 3 shows the number of sentences assessed in each case. Each sentence was ev</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and Automatic Evaluation of Machine Translation between European Languages. In Proceedings of the Workshop on Statistical Machine Translation, pages 102–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4444" citStr="Lin and Och, 2004" startWordPosition="688" endWordPosition="691">lected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). mWER We use 1 − mWER (Nießen et al., 2000). mPER We use 1 − mPER (Tillmann et al., 1997). Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998). 2.2 Beyond Lexical Similarity Modeling linguistic features at levels further than the lexical level requires the usage of more complex linguistic structures. We have defined what we call ‘linguistic elements’ (LEs). 2.2.1 Linguistic Elements LEs are linguistic units, structures, or relationships, such that a sentence may be partia</context>
<context position="18250" citStr="Lin and Och, 2004" startWordPosition="2791" endWordPosition="2794">e deficiencies caused by the shallowness of metrics based on lexical matching. We also analyze separately the case of homogeneous’ systems (i.e., all systems being of the same nature), and the case of heterogenous’ systems (i.e., there exist systems based on different paradigms). As to the metric meta-evaluation criterion, the two most prominent criteria are: Human Acceptability Metrics are evaluated on the basis of correlation with human evaluators. Human Likeness Metrics are evaluated in terms of descriptive power, i.e., their ability to distinguish between human and automatic translations (Lin and Och, 2004b; Amig´o et al., 2005). In our case, metrics are evaluated on the basis of ‘Human Acceptability’. Specifically, we use Pearson correlation coefficients between metric scores 3When computing ‘lexical’ overlapping/matching, we use lemmas instead of word forms. 260 and the average sum of adequacy and fluency assessments at the document level. The reason is that meta-evaluation based on ‘Human Likeness’ requires the availability of heterogenous test beds (i.e., representative sets of automatic outputs and human references), which, unfortunately, is not the case of all the tasks under study. First</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004a. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="4444" citStr="Lin and Och, 2004" startWordPosition="688" endWordPosition="691">lected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). mWER We use 1 − mWER (Nießen et al., 2000). mPER We use 1 − mPER (Tillmann et al., 1997). Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998). 2.2 Beyond Lexical Similarity Modeling linguistic features at levels further than the lexical level requires the usage of more complex linguistic structures. We have defined what we call ‘linguistic elements’ (LEs). 2.2.1 Linguistic Elements LEs are linguistic units, structures, or relationships, such that a sentence may be partia</context>
<context position="18250" citStr="Lin and Och, 2004" startWordPosition="2791" endWordPosition="2794">e deficiencies caused by the shallowness of metrics based on lexical matching. We also analyze separately the case of homogeneous’ systems (i.e., all systems being of the same nature), and the case of heterogenous’ systems (i.e., there exist systems based on different paradigms). As to the metric meta-evaluation criterion, the two most prominent criteria are: Human Acceptability Metrics are evaluated on the basis of correlation with human evaluators. Human Likeness Metrics are evaluated in terms of descriptive power, i.e., their ability to distinguish between human and automatic translations (Lin and Och, 2004b; Amig´o et al., 2005). In our case, metrics are evaluated on the basis of ‘Human Acceptability’. Specifically, we use Pearson correlation coefficients between metric scores 3When computing ‘lexical’ overlapping/matching, we use lemmas instead of word forms. 260 and the average sum of adequacy and fluency assessments at the document level. The reason is that meta-evaluation based on ‘Human Likeness’ requires the availability of heterogenous test beds (i.e., representative sets of automatic outputs and human references), which, unfortunately, is not the case of all the tasks under study. First</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on the Evaluation ofParsing Systems.</booktitle>
<contexts>
<context position="10384" citStr="Lin, 1998" startWordPosition="1616" endWordPosition="1617"> For instance, ‘SP-NISTl-5’ corresponds to the accumulated NIST score for lemma n-grams up to length 5, whereas ‘SP-NISTi,-5’ corresponds to the individual NIST score for PoS 5-grams. 2.4 Syntactic Similarity We have incorporated, with minor modifications, some of the syntactic metrics described by Liu and Gildea (2005) and Amig´o et al. (2006) based on dependency and constituency parsing. 2.4.1 On Dependency Parsing (DP) DP’ metrics capture similarities between dependency trees associated to automatic and reference translations. Dependency trees are provided by the MINIPAR dependency parser (Lin, 1998). Similarities are captured from different viewpoints: DP-HWC(i)-l This metric corresponds to the HWC metric presented by Liu and Gildea (2005). All head-word chains are retrieved. The fraction of matching head-word chains of a given length, ‘l’, is computed. We have slightly modified this metric in order to distinguish three different variants according to the type of items headword chains may consist of: Lexical forms – DP-HWC(i)w-l Grammatical categories – DP-HWC(i)c-l Grammatical relationships – DP-HWC(i)r-l Average accumulated scores up to a given chain length may be used as well. For ins</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based Evaluation of MINIPAR. In Proceedings of the Workshop on the Evaluation ofParsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="10095" citStr="Liu and Gildea (2005)" startWordPosition="1572" endWordPosition="1575">er metric, ‘SP-Oc-*’ which considers the average overlapping over all chunk types. At a more abstract level, we use the NIST metric (Doddington, 2002) to compute accumulated/individual scores over sequences of: Lemmas – SP-NIST(i)l-n Parts-of-speech – SP-NIST(i)p-n Base phrase chunks – SP-NIST(i)c-n For instance, ‘SP-NISTl-5’ corresponds to the accumulated NIST score for lemma n-grams up to length 5, whereas ‘SP-NISTi,-5’ corresponds to the individual NIST score for PoS 5-grams. 2.4 Syntactic Similarity We have incorporated, with minor modifications, some of the syntactic metrics described by Liu and Gildea (2005) and Amig´o et al. (2006) based on dependency and constituency parsing. 2.4.1 On Dependency Parsing (DP) DP’ metrics capture similarities between dependency trees associated to automatic and reference translations. Dependency trees are provided by the MINIPAR dependency parser (Lin, 1998). Similarities are captured from different viewpoints: DP-HWC(i)-l This metric corresponds to the HWC metric presented by Liu and Gildea (2005). All head-word chains are retrieved. The fraction of matching head-word chains of a given length, ‘l’, is computed. We have slightly modified this metric in order to d</context>
<context position="12612" citStr="Liu and Gildea (2005)" startWordPosition="1933" endWordPosition="1936">lects lexical overlapping between nodes hanging at level 4 or deeper. Additionally, we consider three coarser metrics (‘DP-Ol*’, ‘DP-Oc-*’ and ‘DP-Or-*’) which correspond to the uniformly averaged values over all levels, categories, and relationships, respectively. 2.4.2 On Constituency Parsing (CP) ‘CP’ metrics capture similarities between constituency parse trees associated to automatic and reference translations. Constituency trees are provided by the Charniak-Johnson’s Max-Ent reranking parser (Charniak and Johnson, 2005). CP-STM(i)-l This metric corresponds to the STM metric presented by Liu and Gildea (2005). All syntactic subpaths in the candidate and the reference trees are retrieved. The fraction of matching subpaths of a given length, ‘l’, is computed. For instance, ‘CP-STMi-5’ retrieves the proportion of length-5 matching subpaths. Average accumulated scores may be computed as well. For instance, ‘CP-STM-9’ retrieves average accumulated proportion of matching subpaths up to length-9. 2.5 Shallow-Semantic Similarity We have designed two new families of metrics, ‘NE’ and ‘SR’, which are intended to capture similarities over Named Entities (NEs) and Semantic Roles (SRs), respectively. 2.5.1 On </context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings ofACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="8826" citStr="Marcus et al., 1993" startWordPosition="1393" endWordPosition="1396">lass of linguistic elements and items to be used. Below, we instantiate these measures over several particular cases. 2.3 Shallow Syntactic Similarity Metrics based on shallow parsing (SP’) analyze similarities at the level of PoS-tagging, lemmatization, and base phrase chunking. Outputs and references are automatically annotated using stateof-the-art tools. PoS-tagging and lemmatization are provided by the SVMTool package (Gim´enez and M`arquez, 2004), and base phrase chunking is provided by the Phreco software (Carreras et al., 2005). Tag sets for English are derived from the Penn Treebank (Marcus et al., 1993). We instantiate ‘Overlapping’ over parts-of-speech and chunk types. The goal is to capture the proportion of lexical items correctly translated, according to their shallow syntactic realization: SP-Op-t Lexical overlapping according to the partof-speech ‘t’. For instance, ‘SP-O,-NN’ roughly reflects the proportion of correctly translated singular nouns. We also introduce a coarser metric, ‘SP-Op-*’ which computes average overlapping over all parts-of-speech. SP-Oc-t Lexical overlapping according to the chunk type ‘t’. For instance, ‘SP-Oc-NP’ roughly reflects the successfully translated propo</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="4209" citStr="Melamed et al., 2003" startWordPosition="649" endWordPosition="652"> have resorted to several existing metrics, and we have also developed new ones. Below, we group them according to the level at which they operate. 2.1 Lexical Similarity Most of the current metrics operate at the lexical level. We have selected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). mWER We use 1 − mWER (Nießen et al., 2000). mPER We use 1 − mPER (Tillmann et al., 1997). Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998). 2.2 Beyond Lexical Similarity Modeling linguistic features at levels further than the lexical lev</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis M`arquez</author>
<author>Mihai Surdeanu</author>
<author>Pere Comas</author>
<author>Jordi Turmo</author>
</authors>
<title>Robust Combination Strategy for Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<marker>M`arquez, Surdeanu, Comas, Turmo, 2005</marker>
<rawString>Lluis M`arquez, Mihai Surdeanu, Pere Comas, and Jordi Turmo. 2005. Robust Combination Strategy for Semantic Role Labeling. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nießen</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>Evaluation Tool for Machine Translation: Fast Evaluation for MT Research.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd LREC.</booktitle>
<contexts>
<context position="4490" citStr="Nießen et al., 2000" startWordPosition="698" endWordPosition="701">ilies which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). mWER We use 1 − mWER (Nießen et al., 2000). mPER We use 1 − mPER (Tillmann et al., 1997). Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998). 2.2 Beyond Lexical Similarity Modeling linguistic features at levels further than the lexical level requires the usage of more complex linguistic structures. We have defined what we call ‘linguistic elements’ (LEs). 2.2.1 Linguistic Elements LEs are linguistic units, structures, or relationships, such that a sentence may be partially seen as a ‘bag’ of LEs. Possible kinds of </context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>S. Nießen, F.J. Och, G. Leusch, and H. Ney. 2000. Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. In Proceedings of the 2nd LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation, rc22176, ibm.</title>
<date>2001</date>
<tech>Technical report, IBM</tech>
<institution>T.J. Watson Research Center.</institution>
<contexts>
<context position="1743" citStr="Papineni et al., 2001" startWordPosition="258" endWordPosition="261">trics used in the context of Automatic Machine Translation (MT) Evaluation are based on the assumption that ‘acceptable’ translations tend to share the lexicon (i.e., word forms) in a predefined set of manual reference translations. This assumption works well in many cases. However, several results in recent MT evaluation campaigns have cast some doubts on its general validity. For instance, Callison-Burch et al. (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001). In particular, they noted that when the systems under evaluation are of a different nature (e.g., rule-based vs. statistical, human-aided vs. fully automatical, etc.) BLEU may not be a reliable MT quality indicator. The reason is that BLEU favours MT systems which share the expected reference lexicon (e.g., statistical systems), and penalizes those which use a different one. Indeed, the underlying cause is much simpler. In general, lexical similarity is nor a sufficient neither a necessary condition so that two sentences convey the same meaning. On the contrary, natural languages are express</context>
<context position="4051" citStr="Papineni et al., 2001" startWordPosition="618" endWordPosition="621">omputational Linguistics 2 A Heterogeneous Metric Set For our experiments, we have compiled a representative set of metrics1 at different linguistic levels. We have resorted to several existing metrics, and we have also developed new ones. Below, we group them according to the level at which they operate. 2.1 Lexical Similarity Most of the current metrics operate at the lexical level. We have selected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). mWER We use 1 − mWER (Nießen et al., 2000). mPER We use 1 − mPER (Tillmann et al., 1997). Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR ma</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation, rc22176, ibm. Technical report, IBM T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florence Reeder</author>
<author>Keith Miller</author>
<author>Jennifer Doyon</author>
<author>John White</author>
</authors>
<title>The Naming of Things and the Confusion of Tongues: an MT Metric.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on MT Evaluation ”Who did what to whom?” at MT Summit VIII,</booktitle>
<pages>55--59</pages>
<contexts>
<context position="14854" citStr="Reeder et al. (2001)" startWordPosition="2276" endWordPosition="2279">ical overlapping over all NE types. This metric includes the NE type ‘O’ (i.e., Not-a-NE). We introduce another variant, ‘NE-Oe-**’, which considers only actual NEs. NE-Me-t Lexical matching between NEs according to their type t. For instance, ‘NE-Me-LOC’ reflects the proportion of fully translated NEs of type ‘LOC’ (i.e., location). The ‘NE-Me-*’ 259 metric considers the average lexical matching over all NE types, this time excluding type ‘O’. Other authors have measured MT quality over NEs in the recent literature. In particular, the ‘NEMe-*’ metric is similar to the ‘NEE’ metric defined by Reeder et al. (2001). 2.5.2 On Semantic Roles (SR) SR’ metrics analyze similarities between automatic and reference translations by comparing the SRs (i.e., arguments and adjuncts) which occur in them. Sentences are automatically annotated using the SwiRL package (M`arquez et al., 2005). This package requires at the input shallow parsed text enriched with NEs, which is obtained as described in Section 2.5.1. See the list of SR types in Table 2. Type Description A0 arguments associated with a verb predicate, A1 defined in the PropBank Frames scheme. A2 A3 A4 A5 AA Causative agent AM-ADV Adverbial (general-purpose)</context>
</contexts>
<marker>Reeder, Miller, Doyon, White, 2001</marker>
<rawString>Florence Reeder, Keith Miller, Jennifer Doyon, and John White. 2001. The Naming of Things and the Confusion of Tongues: an MT Metric. In Proceedings of the Workshop on MT Evaluation ”Who did what to whom?” at MT Summit VIII, pages 55–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
<author>Eli Comelles</author>
</authors>
<title>Named Entity Recognition from Spontaneous OpenDomain Speech.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Conference on Speech Communication and Technology (Interspeech).</booktitle>
<contexts>
<context position="13437" citStr="Surdeanu et al., 2005" startWordPosition="2056" endWordPosition="2059">of length-5 matching subpaths. Average accumulated scores may be computed as well. For instance, ‘CP-STM-9’ retrieves average accumulated proportion of matching subpaths up to length-9. 2.5 Shallow-Semantic Similarity We have designed two new families of metrics, ‘NE’ and ‘SR’, which are intended to capture similarities over Named Entities (NEs) and Semantic Roles (SRs), respectively. 2.5.1 On Named Entities (NE) ‘NE’ metrics analyze similarities between automatic and reference translations by comparing the NEs which occur in them. Sentences are automatically annotated using the BIOS package (Surdeanu et al., 2005). BIOS requires at the input shallow parsed text, which is obtained as described in Section 2.3. See the list of NE types in Table 1. Type Description ORG Organization PER Person LOC Location MISC Miscellaneous O Not-a-NE DATE Temporal expressions NUM Numerical expressions ANGLE QUANTITY Quantities DISTANCE QUANTITY SIZE QUANTITY SPEED QUANTITY TEMPERATURE QUANTITY WEIGHT QUANTITY METHOD Other MONEY LANGUAGE PERCENT PROJECT SYSTEM Table 1: Named Entity types. We define two types of metrics: NE-Oe-t Lexical overlapping between NEs according to their type t. For instance, ‘NE-Oe-PER’ reflects le</context>
</contexts>
<marker>Surdeanu, Turmo, Comelles, 2005</marker>
<rawString>Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005. Named Entity Recognition from Spontaneous OpenDomain Speech. In Proceedings of the 9th International Conference on Speech Communication and Technology (Interspeech).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP based Search for Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="4536" citStr="Tillmann et al., 1997" startWordPosition="708" endWordPosition="711">evels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). mWER We use 1 − mWER (Nießen et al., 2000). mPER We use 1 − mPER (Tillmann et al., 1997). Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998). 2.2 Beyond Lexical Similarity Modeling linguistic features at levels further than the lexical level requires the usage of more complex linguistic structures. We have defined what we call ‘linguistic elements’ (LEs). 2.2.1 Linguistic Elements LEs are linguistic units, structures, or relationships, such that a sentence may be partially seen as a ‘bag’ of LEs. Possible kinds of LEs are: word forms, parts-of-speech, dependen</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based Search for Statistical Translation. In Proceedings of European Conference on Speech Communication and Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>