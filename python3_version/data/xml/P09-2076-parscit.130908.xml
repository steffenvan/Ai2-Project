<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000338">
<title confidence="0.807308">
Validating the web-based evaluation of NLG systems
</title>
<author confidence="0.336002">
Alexander Koller
Saarland U.
</author>
<email confidence="0.615351">
koller@mmci.uni-saarland.de
</email>
<author confidence="0.853168">
Kristina Striegnitz
</author>
<affiliation confidence="0.797498">
Union College
</affiliation>
<email confidence="0.970045">
striegnk@union.edu
</email>
<author confidence="0.4276315">
Donna Byron
Northeastern U.
</author>
<email confidence="0.900995">
dbyron@ccs.neu.edu
</email>
<note confidence="0.3386085">
Justine Cassell
Northwestern U.
</note>
<email confidence="0.93227">
justine@northwestern.edu
</email>
<author confidence="0.617612">
Robert Dale Sara Dalzel-Job Jon Oberlander Johanna Moore
</author>
<affiliation confidence="0.447644">
Macquarie U. U. of Edinburgh U. of Edinburgh U. of Edinburgh
</affiliation>
<email confidence="0.986342">
Robert.Dale@mq.edu.au S.Dalzel-Job@sms.ed.ac.uk {J.Oberlander|J.Moore}@ed.ac.uk
</email>
<sectionHeader confidence="0.993456" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976636363636">
The GIVE Challenge is a recent shared
task in which NLG systems are evaluated
over the Internet. In this paper, we validate
this novel NLG evaluation methodology by
comparing the Internet-based results with
results we collected in a lab experiment.
We find that the results delivered by both
methods are consistent, but the Internet-
based approach offers the statistical power
necessary for more fine-grained evaluations
and is cheaper to carry out.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970603773585">
Recently, there has been an increased interest in
evaluating and comparing natural language gener-
ation (NLG) systems on shared tasks (Belz, 2009;
Dale and White, 2007; Gatt et al., 2008). However,
this is a notoriously hard problem (Scott and Moore,
2007): Task-based evaluations with human experi-
mental subjects are time-consuming and expensive,
and corpus-based evaluations of NLG systems are
problematic because a mismatch between human-
generated output and system-generated output does
not necessarily mean that the system’s output is
inferior (Belz and Gatt, 2008). This lack of evalua-
tion methods which are both effective and efficient
is a serious obstacle to progress in NLG research.
The GIVE Challenge (Byron et al., 2009) is a
recent shared task which takes a third approach to
NLG evaluation: By connecting NLG systems to
experimental subjects over the Internet, it achieves
a true task-based evaluation at a much lower cost.
Indeed, the first GIVE Challenge acquired data
from over 1100 experimental subjects online. How-
ever, it still remains to be shown that the results
that can be obtained in this way are in fact com-
parable to more established task-based evaluation
efforts, which are based on a carefully selected sub-
ject pool and carried out in a controlled laboratory
environment. By accepting connections from arbi-
trary subjects over the Internet, the evaluator gives
up control over the subjects’ behavior, level of lan-
guage proficiency, cooperativeness, etc.; there is
also an issue of whether demographic factors such
as gender might skew the results.
In this paper, we provide the missing link by
repeating the GIVE evaluation in a laboratory en-
vironment and comparing the results. It turns out
that where the two experiments both find a signif-
icant difference between two NLG systems with
respect to a given evaluation measure, they always
agree. However, the Internet-based experiment
finds considerably more such differences, perhaps
because of the higher number of experimental sub-
jects (n = 374 vs. n = 91), and offers other oppor-
tunities for more fine-grained analysis as well. We
take this as an empirical validation of the Internet-
based evaluation of GIVE, and propose that it can
be applied to NLG more generally. Our findings
are in line with studies from psychology that indi-
cate that the results of web-based experiments are
typically consistent with the results of traditional
experiments (Gosling et al., 2004). Nevertheless,
we do find and discuss some effects of the uncon-
trolled subject pool that should be addressed in
future Internet-based NLG challenges.
</bodyText>
<sectionHeader confidence="0.961122" genericHeader="method">
2 The GIVE Challenge
</sectionHeader>
<bodyText confidence="0.999680333333333">
In the GIVE scenario (Byron et al., 2009), users
try to solve a treasure hunt in a virtual 3D world
that they have not seen before. The computer has
complete information about the virtual world. The
challenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
From the perspective of the users, GIVE con-
sists in playing a 3D game which they start from
a website. The game displays a virtual world and
allows the user to move around in the world and
manipulate objects; it also displays the generated
</bodyText>
<page confidence="0.9875">
301
</page>
<note confidence="0.9257865">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 301–304,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999939611111111">
instructions. The first room in each game is a tuto-
rial room in which users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Players
can either finish a game successfully, lose it by
triggering an alarm, or cancel the game at any time.
When a user starts the game, they are randomly
connected to one of the three worlds and one of the
NLG systems. The GIVE-1 Challenge evaluated
five NLG systems, which we abbreviate as A, M,
T, U, and W below. A running GIVE NLG system
has access to the current state of the world and to
an automatically computed plan that tells it what
actions the user should perform to solve the task. It
is notified whenever the user performs some action,
and can generate an instruction and send it to the
client for display at any time.
</bodyText>
<sectionHeader confidence="0.991371" genericHeader="method">
3 The experiments
</sectionHeader>
<bodyText confidence="0.993498194029851">
The web experiment. For the GIVE-1 challenge,
1143 valid games were collected over the Internet
over the course of three months. These were dis-
tributed over three evaluation worlds (World 1: 374,
World 2: 369, World 3: 400). A game was consid-
ered valid if the game client didn’t crash, the game
wasn’t marked as a test run by the developers, and
the player completed the tutorial.
Of these games, 80% were played by males and
10% by females (the remaining 10% of the partic-
ipants did not specify their gender). The players
were widely distributed over countries: 37% con-
nected from IP addresses in the US, 33% from
Germany, and 17% from China; the rest connected
from 45 further countries. About 34% of the par-
ticipants self-reported as native English speakers,
and 62% specified a language proficiency level of
at least “expert” (3 on a 5-point scale).
The lab experiment. We repeated the GIVE-1
evaluation in a traditional laboratory setting with
91 participants recruited from a college campus.
In the lab, each participant played the GIVE game
once with each of the five NLG systems. To avoid
learning effects, we only used the first game run
from each subject in the comparison with the web
experiment; as a consequence, subjects were dis-
tributed evenly over the NLG systems. To accom-
modate for the much lower number of participants,
the laboratory experiment only used a single game
world – World 1, which was known from the online
version to be the easiest world.
Among this group of subjects, 93% self-rated
their English proficiency as “expert” or better; 81%
were native speakers. In contrast to the online ex-
periment, 31% of participants were male and 65%
were female (4% did not specify their gender).
Results: Objective measures. The GIVE soft-
ware automatically recorded data for five objec-
tive measures: the percentage of successfully com-
pleted games and, for the successfully completed
games, the number of instructions generated by
the NLG system, of actions performed by the user
(such as pushing buttons), of steps taken by the
user (i.e., actions plus movements), and the task
completion time (in seconds).
Fig. 1 shows the results for the objective mea-
sures collected in both experiments. To make the
results comparable, the table for the Internet ex-
periment only includes data for World 1. The task
success rate is only evaluated on games that were
completed successfully or lost, not cancelled, as
laboratory subjects were asked not to cancel. This
brings the number of Internet subjects to 322 for
the success rate, and to 227 (only successful games)
for the other measures.
Task success is the percentage of successfully
completed games; the other measures are reported
as means. The chart assigns systems to groups A
through C or D for each evaluation measure. Sys-
tems in group A are better than systems in group
B, and so on; if two systems have no letter in com-
mon, the difference between them is significant
with p &lt; 0.05. Significance was tested using a k2-
test for task success and ANOVAs for instructions,
steps, actions, and seconds. These were followed
by post hoc tests (pairwise k2 and Tukey) to com-
pare the NLG systems pairwise.
</bodyText>
<sectionHeader confidence="0.840157" genericHeader="method">
Results: Subjective measures. Users were
</sectionHeader>
<bodyText confidence="0.999388307692308">
asked to fill in a questionnaire collecting subjec-
tive ratings of various aspects of the instructions.
For example, users were asked to rate the overall
quality of the direction giving system (on a 7-point
scale), the choice of words and the referring ex-
pressions (on 5-point scales), and they were asked
whether they thought the instructions came at the
right time. Overall, there were twelve subjective
measures (see (Byron et al., 2009)), of which we
only present four typical ones for space reasons.
For each question, the user could choose not to
answer. On the Internet, subjects made consider-
able use of this option: for instance, 32% of users
</bodyText>
<page confidence="0.976907">
302
</page>
<figure confidence="0.99988308">
Subjective Measures
Objective Measures
overall choice
of words
referring timing
expressions
task
success
9.4 A
4.7 A
4.7 A
4.7 A
81% A
70% ABC
73% AB
51% C
4.4 B 4.4 AB
4.0 B 4.0 B
W
24% C
159.7 D 256.0 C
9.6 AB
234.1 C 3.8 AB
3.8 B
4.2 AB
50% BC
9.9 A
9.6 A
9.8 A
4.7 A
4.7 A
92% A B
95% A B
64% A B
100% A
4.9 A
5.7 A
5.4 A
5.7 A
4.8 A
4.3 A
4.3 A
W
17% B
134.5 D 213.5 C
10.0 A
252.5 B
5.0 A
4.5 A B 4.0 A
100% B
</figure>
<figureCaption confidence="0.997715">
Figure 1: Objective and selected subjective measures on the web (top) and in the lab (bottom).
</figureCaption>
<figure confidence="0.9922225">
instructions steps actions seconds
91% A
76% B
85% AB
93% AB
83.4 B
68.1 A
97.8 C
99.8 C
99.8 A
145.1 B
142.1 B
142.6 B
10.0 AB
9.7 AB
10.3 B
123.9 A
195.4 BC
174.4 B
194.0 BC
3.8 AB
3.8 B
4.0 B
4.3 AB
4.0 B
100% A
95% A
93% A
100% A
78.2 AB
66.3 A
107.2 CD
88.8 BC
93.4 A
141.8 B
134.6 B
128.8 B
10.5 A
143.9 A
211.8 B
205.6 B
195.1 AB
3.8 B
4.5 A B 4.4 A
A
M
T
U
A
M
T
U
</figure>
<bodyText confidence="0.997506555555555">
didn’t fill in the “overall evaluation” field of the
questionnaire. In the laboratory experiment, the
subjects were asked to fill in the complete question-
naire and the response rate is close to 100%.
The results for the four selected subjective mea-
sures are summarized in Fig. 1 in the same way as
the objective measures. Also as above, the table
is based only on successfully completed games in
World 1. We will justify this latter choice below.
</bodyText>
<sectionHeader confidence="0.998708" genericHeader="evaluation">
4 Discussion
</sectionHeader>
<bodyText confidence="0.998327814814815">
The primary question that interests us in a compar-
ative evaluation is which NLG systems performed
significantly better or worse on any given evalua-
tion measure. In the experiments above, we find
that of the 170 possible significant differences (=
17 measures × 10 pairs of NLG systems), the labo-
ratory experiment only found six that the Internet-
based experiment didn’t find. Conversely, there
are 26 significant differences that only the Internet-
based experiment found. But even more impor-
tantly, all pairwise rankings are consistent across
the two evaluations: Where both systems found a
significant difference between two systems, they al-
ways ranked them in the same order. We conclude
that the Internet experiment provides significance
judgments that are comparable to, and in fact more
precise than, the laboratory experiment.
Nevertheless, there are important differences be-
tween the laboratory and Internet-based results. For
instance, the success rates in the laboratory tend
to be higher, but so are the completion times. We
believe that these differences can be attributed to
the demographic characteristics of the participants.
To substantiate this claim, we looked in some detail
at differences in gender, language proficiency, and
questionnaire response rates.
First, the gender distribution differed greatly be-
</bodyText>
<table confidence="0.9933372">
games Web mean
reported
success 227 = 61% 93% 4.9
lost 92 = 24% 48% 3.4
cancelled 55 = 15% 16% 3.3
# games Lab mean
reported
success 73 = 80% 100% 5.4
lost 18 = 20% 94% 3.3
cancelled 0 – –
</table>
<figureCaption confidence="0.9894">
Figure 2: Skewed results for “overall evaluation”.
</figureCaption>
<bodyText confidence="0.999975445945946">
tween the Internet experiment (10% female) and
the laboratory experiment (65% female). This is
relevant because gender had a significant effect
on task completion time (women took longer) and
on six subjective measures including “overall eval-
uation” in the laboratory. We speculate that the
difference in task completion time may be related
to well-known gender differences in processing
navigation instructions (Moffat et al., 1998).
Second, the two experiments collected data from
subjects of different language proficiencies. While
93% of the participants in the laboratory experi-
ment self-rated their English proficiency as “expert”
or better, only 62% of the Internet participants did.
This partially explains the lower task success rates
on the Internet, as Internet subjects with English
proficiencies of 3–5 performed significantly better
on “task success” than the group with proficiencies
1–2. If we only look at the results of high-English-
proficiency subjects on the Internet, the success
rates for all NLG systems except W rise to at least
86%, and are thus close to the laboratory results.
Finally, the Internet data are skewed by the ten-
dency of unsuccessful participants to not fill in the
questionnaire. Fig. 2 summarizes some data about
the “overall evaluation” question. Users who didn’t
complete the task successfully tended to judge the
systems much lower than successful users, but at
the same time tended not to answer the question
at all. This skew causes the mean subjective judg-
ments across all Internet subjects to be artificially
high. To avoid differences between the laboratory
and the Internet experiment due to this skew, Fig. 1
includes only judgments from successful games.
In summary, we find that while the two experi-
ments made consistent significance judgments, and
the Internet-based evaluation methodology thus
produces meaningful results, the absolute values
they find for the individual evaluation measures
differ due to the demographic characteristics of the
participants in the two studies. This could be taken
as a possible deficit of the Internet-based evalua-
tion. However, we believe that the opposite is true.
In many ways, an online user is in a much more
natural communicative situation than a laboratory
subject who is being discouraged from cancelling
a frustrating task. In addition, every experiment –
whether in the laboratory or on the Internet – suf-
fers from some skew in the subject population due
to sampling bias; for instance, one could argue that
an evaluation that is based almost exclusively on na-
tive speakers in universities leads to overly benign
judgments about the quality of NLG systems.
One advantage of the Internet-based approach
to data collection over the laboratory-based one is
that, due to the sheer number of subjects, we can de-
tect such skews and deal with them appropriately.
For instance, we might decide that we are only
interested in the results from proficient English
speakers and ignore the rest of the data; but we
retain the option to run the analysis over all partici-
pants, and to analyze how much each system relies
on the user’s language proficiency. The amount
of data also means that we can obtain much more
fine-grained comparisons between NLG systems.
For instance, the second and third evaluation world
specifically exercised an NLG system’s abilities to
generate referring expressions and navigation in-
structions, respectively, and there were significant
differences in the performance of some systems
across different worlds. Such data, which is highly
valuable for pinpointing specific weaknesses of a
system, would have been prohibitively costly and
time-consuming to collect with laboratory subjects.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999182">
In this paper, we have argued that carrying out task-
based evaluations of NLG systems over the Internet
is a valid alternative to more traditional laboratory-
based evaluations. Specifically, we have shown
that an Internet-based evaluation of systems in the
GIVE Challenge finds consistent significant differ-
ences as a lab-based evaluation. While the Internet-
based evaluation suffers from certain skews caused
by the lack of control over the subject pool, it does
find more differences than the lab-based evaluation
because much more data is available. The increased
amount of data also makes it possible to compare
the quality of NLG systems across different evalua-
tion worlds and users’ language proficiency levels.
We believe that this type of evaluation effort
can be applied to other NLG and dialogue tasks
beyond GIVE. Nevertheless, our results also show
that an Internet-based evaluation risks certain kinds
of skew in the data. It is an interesting question for
the future how this skew can be reduced.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999790303030303">
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings ofACL-08:HLT, Short Papers, pages
197–200, Columbus, Ohio.
A. Belz. 2009. That’s nice ... what can you do with it?
Computational Linguistics, 35(1):111–118.
D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2009. Report on the
First NLG Challenge on Generating Instructions in
Virtual Environments (GIVE). In Proceedings of the
12th European Workshop on Natural Language Gen-
eration (Special session on Generation Challenges).
R. Dale and M. White, editors. 2007. Proceedings
of the NSF/SIGGEN Workshop for Shared Tasks and
Comparative Evaluation in NLG, Arlington, VA.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA
challenge 2008: Overview and evaluation results.
In Proceedings of the 5th International Natural
Language Generation Conference (INLG’08), pages
198–206.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93–
104.
S. Moffat, E. Hampson, and M. Hatzipantelis. 1998.
Navigation in a “virtual” maze: Sex differences and
correlation with psychometric measures of spatial
ability in humans. Evolution and Human Behavior,
19(2):73–87.
D. Scott and J. Moore. 2007. An NLG evaluation com-
petition? Eight reasons to be cautious. In (Dale and
White, 2007).
</reference>
<page confidence="0.999327">
304
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.214704">
<title confidence="0.999934">Validating the web-based evaluation of NLG systems</title>
<author confidence="0.999825">Alexander Koller</author>
<affiliation confidence="0.93619">Saarland U.</affiliation>
<email confidence="0.955443">koller@mmci.uni-saarland.de</email>
<author confidence="0.987076">Kristina Striegnitz</author>
<affiliation confidence="0.999883">Union College</affiliation>
<email confidence="0.999415">striegnk@union.edu</email>
<author confidence="0.900522">Donna Byron Northeastern U</author>
<email confidence="0.99687">dbyron@ccs.neu.edu</email>
<author confidence="0.892012">Justine Cassell</author>
<affiliation confidence="0.566393">Northwestern U.</affiliation>
<email confidence="0.999492">justine@northwestern.edu</email>
<author confidence="0.998802">Robert Dale Sara Dalzel-Job Jon Oberlander Johanna Moore</author>
<affiliation confidence="0.663922">Macquarie U. U. of Edinburgh U. of Edinburgh U. of Edinburgh</affiliation>
<email confidence="0.996505">S.Dalzel-Job@sms.ed.ac.uk</email>
<abstract confidence="0.996131916666667">The GIVE Challenge is a recent shared task in which NLG systems are evaluated over the Internet. In this paper, we validate this novel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment. We find that the results delivered by both methods are consistent, but the Internetbased approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carry out.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>A Gatt</author>
</authors>
<title>Intrinsic vs. extrinsic evaluation measures for referring expression generation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08:HLT, Short Papers,</booktitle>
<pages>197--200</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1504" citStr="Belz and Gatt, 2008" startWordPosition="203" endWordPosition="206">uations and is cheaper to carry out. 1 Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach to NLG evaluation: By connecting NLG systems to experimental subjects over the Internet, it achieves a true task-based evaluation at a much lower cost. Indeed, the first GIVE Challenge acquired data from over 1100 experimental subjects online. However, it still remains to be shown that the results that can be obtained in this way are in fact comparable to more established task-bas</context>
</contexts>
<marker>Belz, Gatt, 2008</marker>
<rawString>A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic evaluation measures for referring expression generation. In Proceedings ofACL-08:HLT, Short Papers, pages 197–200, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>That’s nice ... what can you do with it?</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="1080" citStr="Belz, 2009" startWordPosition="143" endWordPosition="144">t The GIVE Challenge is a recent shared task in which NLG systems are evaluated over the Internet. In this paper, we validate this novel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment. We find that the results delivered by both methods are consistent, but the Internetbased approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carry out. 1 Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent </context>
</contexts>
<marker>Belz, 2009</marker>
<rawString>A. Belz. 2009. That’s nice ... what can you do with it? Computational Linguistics, 35(1):111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Byron</author>
<author>A Koller</author>
<author>K Striegnitz</author>
<author>J Cassell</author>
<author>R Dale</author>
<author>J Moore</author>
<author>J Oberlander</author>
</authors>
<title>Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE).</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation (Special session on Generation Challenges).</booktitle>
<contexts>
<context position="1667" citStr="Byron et al., 2009" startWordPosition="231" endWordPosition="234">tems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach to NLG evaluation: By connecting NLG systems to experimental subjects over the Internet, it achieves a true task-based evaluation at a much lower cost. Indeed, the first GIVE Challenge acquired data from over 1100 experimental subjects online. However, it still remains to be shown that the results that can be obtained in this way are in fact comparable to more established task-based evaluation efforts, which are based on a carefully selected subject pool and carried out in a controlled laboratory environment. By accepting connections from a</context>
<context position="3597" citStr="Byron et al., 2009" startWordPosition="545" endWordPosition="548">ffers other opportunities for more fine-grained analysis as well. We take this as an empirical validation of the Internetbased evaluation of GIVE, and propose that it can be applied to NLG more generally. Our findings are in line with studies from psychology that indicate that the results of web-based experiments are typically consistent with the results of traditional experiments (Gosling et al., 2004). Nevertheless, we do find and discuss some effects of the uncontrolled subject pool that should be addressed in future Internet-based NLG challenges. 2 The GIVE Challenge In the GIVE scenario (Byron et al., 2009), users try to solve a treasure hunt in a virtual 3D world that they have not seen before. The computer has complete information about the virtual world. The challenge for the NLG system is to generate, in real time, natural-language instructions that will guide the users to the successful completion of their task. From the perspective of the users, GIVE consists in playing a 3D game which they start from a website. The game displays a virtual world and allows the user to move around in the world and manipulate objects; it also displays the generated 301 Proceedings of the ACL-IJCNLP 2009 Conf</context>
<context position="8833" citStr="Byron et al., 2009" startWordPosition="1441" endWordPosition="1444">tructions, steps, actions, and seconds. These were followed by post hoc tests (pairwise k2 and Tukey) to compare the NLG systems pairwise. Results: Subjective measures. Users were asked to fill in a questionnaire collecting subjective ratings of various aspects of the instructions. For example, users were asked to rate the overall quality of the direction giving system (on a 7-point scale), the choice of words and the referring expressions (on 5-point scales), and they were asked whether they thought the instructions came at the right time. Overall, there were twelve subjective measures (see (Byron et al., 2009)), of which we only present four typical ones for space reasons. For each question, the user could choose not to answer. On the Internet, subjects made considerable use of this option: for instance, 32% of users 302 Subjective Measures Objective Measures overall choice of words referring timing expressions task success 9.4 A 4.7 A 4.7 A 4.7 A 81% A 70% ABC 73% AB 51% C 4.4 B 4.4 AB 4.0 B 4.0 B W 24% C 159.7 D 256.0 C 9.6 AB 234.1 C 3.8 AB 3.8 B 4.2 AB 50% BC 9.9 A 9.6 A 9.8 A 4.7 A 4.7 A 92% A B 95% A B 64% A B 100% A 4.9 A 5.7 A 5.4 A 5.7 A 4.8 A 4.3 A 4.3 A W 17% B 134.5 D 213.5 C 10.0 A 252</context>
</contexts>
<marker>Byron, Koller, Striegnitz, Cassell, Dale, Moore, Oberlander, 2009</marker>
<rawString>D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale, J. Moore, and J. Oberlander. 2009. Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE). In Proceedings of the 12th European Workshop on Natural Language Generation (Special session on Generation Challenges).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>M White</author>
<author>editors</author>
</authors>
<date>2007</date>
<booktitle>Proceedings of the NSF/SIGGEN Workshop for Shared Tasks and Comparative Evaluation in NLG,</booktitle>
<location>Arlington, VA.</location>
<marker>Dale, White, editors, 2007</marker>
<rawString>R. Dale and M. White, editors. 2007. Proceedings of the NSF/SIGGEN Workshop for Shared Tasks and Comparative Evaluation in NLG, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
<author>A Belz</author>
<author>E Kow</author>
</authors>
<title>The TUNA challenge 2008: Overview and evaluation results.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Natural Language Generation Conference (INLG’08),</booktitle>
<pages>198--206</pages>
<contexts>
<context position="1122" citStr="Gatt et al., 2008" startWordPosition="149" endWordPosition="152">hared task in which NLG systems are evaluated over the Internet. In this paper, we validate this novel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment. We find that the results delivered by both methods are consistent, but the Internetbased approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carry out. 1 Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach t</context>
</contexts>
<marker>Gatt, Belz, Kow, 2008</marker>
<rawString>A. Gatt, A. Belz, and E. Kow. 2008. The TUNA challenge 2008: Overview and evaluation results. In Proceedings of the 5th International Natural Language Generation Conference (INLG’08), pages 198–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D Gosling</author>
<author>S Vazire</author>
<author>S Srivastava</author>
<author>O P John</author>
</authors>
<title>Should we trust Web-based studies? A comparative analysis of six preconceptions about Internet questionnaires.</title>
<date>2004</date>
<journal>American Psychologist,</journal>
<volume>59</volume>
<pages>104</pages>
<contexts>
<context position="3384" citStr="Gosling et al., 2004" startWordPosition="510" endWordPosition="513">given evaluation measure, they always agree. However, the Internet-based experiment finds considerably more such differences, perhaps because of the higher number of experimental subjects (n = 374 vs. n = 91), and offers other opportunities for more fine-grained analysis as well. We take this as an empirical validation of the Internetbased evaluation of GIVE, and propose that it can be applied to NLG more generally. Our findings are in line with studies from psychology that indicate that the results of web-based experiments are typically consistent with the results of traditional experiments (Gosling et al., 2004). Nevertheless, we do find and discuss some effects of the uncontrolled subject pool that should be addressed in future Internet-based NLG challenges. 2 The GIVE Challenge In the GIVE scenario (Byron et al., 2009), users try to solve a treasure hunt in a virtual 3D world that they have not seen before. The computer has complete information about the virtual world. The challenge for the NLG system is to generate, in real time, natural-language instructions that will guide the users to the successful completion of their task. From the perspective of the users, GIVE consists in playing a 3D game </context>
</contexts>
<marker>Gosling, Vazire, Srivastava, John, 2004</marker>
<rawString>S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John. 2004. Should we trust Web-based studies? A comparative analysis of six preconceptions about Internet questionnaires. American Psychologist, 59:93– 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Moffat</author>
<author>E Hampson</author>
<author>M Hatzipantelis</author>
</authors>
<title>Navigation in a “virtual” maze: Sex differences and correlation with psychometric measures of spatial ability in humans. Evolution and Human Behavior,</title>
<date>1998</date>
<contexts>
<context position="12389" citStr="Moffat et al., 1998" startWordPosition="2101" endWordPosition="2104">t 92 = 24% 48% 3.4 cancelled 55 = 15% 16% 3.3 # games Lab mean reported success 73 = 80% 100% 5.4 lost 18 = 20% 94% 3.3 cancelled 0 – – Figure 2: Skewed results for “overall evaluation”. tween the Internet experiment (10% female) and the laboratory experiment (65% female). This is relevant because gender had a significant effect on task completion time (women took longer) and on six subjective measures including “overall evaluation” in the laboratory. We speculate that the difference in task completion time may be related to well-known gender differences in processing navigation instructions (Moffat et al., 1998). Second, the two experiments collected data from subjects of different language proficiencies. While 93% of the participants in the laboratory experiment self-rated their English proficiency as “expert” or better, only 62% of the Internet participants did. This partially explains the lower task success rates on the Internet, as Internet subjects with English proficiencies of 3–5 performed significantly better on “task success” than the group with proficiencies 1–2. If we only look at the results of high-Englishproficiency subjects on the Internet, the success rates for all NLG systems except </context>
</contexts>
<marker>Moffat, Hampson, Hatzipantelis, 1998</marker>
<rawString>S. Moffat, E. Hampson, and M. Hatzipantelis. 1998. Navigation in a “virtual” maze: Sex differences and correlation with psychometric measures of spatial ability in humans. Evolution and Human Behavior, 19(2):73–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Scott</author>
<author>J Moore</author>
</authors>
<title>An NLG evaluation competition? Eight reasons to be cautious.</title>
<date>2007</date>
<booktitle>In (Dale and White,</booktitle>
<contexts>
<context position="1191" citStr="Scott and Moore, 2007" startWordPosition="160" endWordPosition="163">In this paper, we validate this novel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment. We find that the results delivered by both methods are consistent, but the Internetbased approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carry out. 1 Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach to NLG evaluation: By connecting NLG systems to experimental subjects </context>
</contexts>
<marker>Scott, Moore, 2007</marker>
<rawString>D. Scott and J. Moore. 2007. An NLG evaluation competition? Eight reasons to be cautious. In (Dale and White, 2007).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>