<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030737">
<title confidence="0.94835">
Generalizing Hierarchical Phrase-based Translation
using Rules with Adjacent Nonterminals
</title>
<author confidence="0.994708">
Hendra Setiawan and Philip Resnik
</author>
<affiliation confidence="0.9977925">
UMIACS Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742, USA
</affiliation>
<email confidence="0.971416">
Kendra, resnik @umd.edu
</email>
<sectionHeader confidence="0.99504" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998122357142857">
Hierarchical phrase-based translation (Hiero,
(Chiang, 2005)) provides an attractive frame-
work within which both short- and long-
distance reorderings can be addressed consis-
tently and efficiently. However, Hiero is gen-
erally implemented with a constraint prevent-
ing the creation of rules with adjacent nonter-
minals, because such rules introduce compu-
tational and modeling challenges. We intro-
duce methods to address these challenges, and
demonstrate that rules with adjacent nontermi-
nals can improve Hiero&apos;s generalization power
and lead to significant performance gains in
Chinese-English translation.
</bodyText>
<sectionHeader confidence="0.997882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999857823529412">
Hierarchical phrase-based translation (Hiero, (Chi-
ang, 2005)) has proven to be a very useful com-
promise between syntactically informed and purely
corpus-driven translation. By automatically learn-
ing synchronous grammar rules from parallel text,
Hiero captures short- and long-distance reorderings
consistently and efficiently. However, implementa-
tions of Hiero generally forbid adjacent nonterminal
symbols on the source side of hierarchical rules, a
practice we will refer to as the non-adjacent nonter-
minals constraint. The main argument against such
rules is that they cause the system to produce multi-
ple derivations that all lead to the same translation –
a form of redundancy known as spurious ambiguity.
Spurious ambiguity can lead to drastic reductions in
decoding efficiency, and the obvious solutions, such
as reducing beam width, erode translation quality.
In Section 2, we argue that the non-adjacent non-
terminals constraints severely limits Hiero&apos;s gener-
alization power, limiting its coverage of important
reordering phenomena. In Section 3, we discuss
the challenges that arise in relaxing this constraint.
In Section 4 we introduce new methods to address
those challenges, and Section 5 validates the ap-
proach empirically.
Improving Hiero via variations on rule prun-
ing and filtering is well explored, e.g., (Chiang,
2005; Chiang et al., 2008; Zollmann and Venugopal,
2006), to name just a few. These proposals dif-
fer from each other mainly in the specific linguis-
tic knowledge being used, and on which side the
constraints are applied. In contrast, we complement
previous work by showing that adding rules to Hiero
can provide benefits if done judiciously.
</bodyText>
<sectionHeader confidence="0.713604" genericHeader="method">
2 Judicious Use of Adjacent Nonterminals
</sectionHeader>
<bodyText confidence="0.998908090909091">
Our motivations largely follow Menezes and Quirk&apos;s
(2007) discussion of reorderings and generalization.
As a specific example, we will use a Chinese to En-
glish verb phrase (VP) translation (Fig. 1), which
represents one of the most prominent phrase con-
structions in Chinese. Here the construction of the
Chinese VP involves joining a prepositional phrase
(PP) and a smaller verbal phrase (VP-A), with the
preposition at the beginning as a PP marker. In the
translation, the VP-A precedes the PP, a shift from
pre-verbal PP in Chinese to post-verbal in English.
</bodyText>
<equation confidence="0.9972057">
VP
PP �� ������
� ��
�
P NP VP-A
r± ,.pFV9 �3F�1%
PPP ���������
PPP
�� ��
�� ��
</equation>
<figureCaption confidence="0.854051">
rank 10th at Eastern division
Figure 1: A Chinese-English verb phrase translation
</figureCaption>
<page confidence="0.984218">
349
</page>
<subsubsectionHeader confidence="0.566441">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 349–352,
</subsubsectionHeader>
<subsectionHeader confidence="0.254482">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.9155395">
Hiero can correctly translate the example if it
learns any of the following rules from training data:
</bodyText>
<equation confidence="0.999729">
X,( rT X1 -,9 F , rank 10th at X1) (1)
X,( , AME X1, X1 at Eastern div.) (2)
X,(X1 ,r,AFV9 X2, X2 X1 Eastern div.) (3)
</equation>
<bodyText confidence="0.999885142857143">
However, in practice, data sparsity makes the chance
of learning these rules rather slim. For instance,
learning Rule 1 depends on training data containing
instances of the shift with identical wording for the
VP-A, which belongs to an open word class.
If Hiero fails to learn any of the above rules, it
will apply the “glue rules” S , (S X1, S X1) and
S , (X, X). But these glue rules clearly can-
not model the VP-A&apos;s movement. In failing to learn
Rules 1-3, Hiero has no choice but to translate VP-A
in a monotone order.
On the other hand, consider the following rules
with adjacent nonterminals on the source side (or XX
rules, for brevity):
</bodyText>
<equation confidence="0.942869666666667">
X,(r± X1X2, X2 at X1)
X,(X1X2 �i� F , rank 10th X1X2)
X,(X1X2, X2X1)
</equation>
<bodyText confidence="0.999978466666667">
Note that although XX rules 4-6 can potentially in-
crease the chance of modeling the pre-verbal to post-
verbal shift, not all of them are beneficial to learn.
For instance, Rule 5 models the word order shift but
introduces spurious ambiguity, since the nontermi-
nals are translated in monotone order. Rule 6, which
resembles the inverted rule of the Inversion Trans-
duction Grammar (Wu, 1997), is highly ambigu-
ous because its application has no lexical grounding.
Rule 4 avoids both problems, and is also easier to
learn, since it is lexically anchored by a preposition,
;i!�E(at), which we can expect to appear frequently in
training. These observations will motivate us to fo-
cus on rules that model non-monotone reordering of
phrases surrounding a lexical item on the target side.
</bodyText>
<sectionHeader confidence="0.985922" genericHeader="method">
3 Addressing XX Rule Challenges
</sectionHeader>
<bodyText confidence="0.999991195121951">
The first challenge created by introducing XX rules
is computational: relaxing the constraint signifi-
cantly increases the grammar size. Motivated by
our earlier discussion, we address this by permitting
only rules that model non-monotone reordering, i.e.
those rules whose nonterminals are projected into
the target language in a different word order, leaving
monotone mappings to be handled by the glue rules
as previously. This choice helps keep the search
space more manageable, and also avoids spurious
ambiguity. In addition, we disallow rules in which
nonterminals are adjacent on both the source and tar-
get sides, by imposing the non adjacent nonterminal
constraint on the target side whenever the constraint
is relaxed on the source side. This forces any non-
monotone reorderings to always be grounded in lex-
ical evidence. We refer to the permitted subset of
XX rules as XX-nonmono rules.
The second challenge involves modeling: intro-
ducing XX rules places them in competition with
the existing glue rules. In particular, these two kinds
of rules try to model the same phenomena, namely
the translations of phrases that appear next to each
other. However, they differ in terms of the features
associated with the rules. XX rules will be asso-
ciated with the same features as any other hierar-
chical rules, since they are all learned via an iden-
tical training method. In contrast, glue rules are
introduced into the grammar in an ad hoc manner,
and the only feature associated with them is a “glue
penalty”. These distinct feature sets makes direct
comparison of scores unreliable. As a result the de-
coder may simply prefer to always select glue rules
because they are associated with fewer features re-
sulting in adjacent phrases always being translated
in a monotone order. To address this issue, we in-
troduce a new model, which we call the target-side
function words orientation-based model, or simply
PoTit, which evaluates the application of the two
kinds of rules on the same context, i.e. for our ex-
ample, it is the function word ;i!�E(at).
</bodyText>
<sectionHeader confidence="0.9812855" genericHeader="method">
4 Target-side Function Words
Orientation-based Model
</sectionHeader>
<bodyText confidence="0.999868375">
The PoTit model is motivated by the function words
reordering hypothesis (Setiawan et al., 2007), which
suggests that function words encode essential infor-
mation about the (re)ordering of their neighboring
phrases. In contrast to Setiawan et al. (2007), who
looked at neighboring contexts for function words
on the source side, we focus here on modeling the
influence of function words on neighboring phrases
</bodyText>
<page confidence="0.984341">
350
</page>
<bodyText confidence="0.999960044444444">
on the target side. We argue that this focus better fits
our purpose, since the phrases that we want to model
are the function words&apos; neighbors on the target side,
as illustrated in Fig. 1.
To develop this idea, we first define an orit func-
tion that takes a source function word as a refer-
ence point, along with its neighboring phrase on the
target side. The orit function outputs one of the
following orientation values (Nagata et al., 2006):
Monotone-Adjacent (MA); Reverse-Adjacent (RA);
Monotone-Gap (MG); and Reverse-Gap (RG). The
Monotone/Reverse distinction indicates whether the
source order follows the target order. The Ad-
jacent/Gap distinction indicates whether the two
phrases are adjacent or separated by an intervening
phrase on the source side. For example, in Fig. 1,
the value of orit for right neighbor Eastern division
with respect to function word ;i!�E (at) is MA, since its
corresponding source phrase 3.009 is adjacent
to r± (at) and their order is preserved on the English
side. The value for left neighbor rank 10th with re-
spect to r± (at) is RG, since is separated
from ;i!�E (at) and their order is reversed on the En-
glish side.
More formally, we define Porit(orit(Y, X)|Y ),
where orit(Y, X) E {MA,RA,MG,RG} is the ori-
entation of a target phrase X with a source function
word Y as the reference point.&apos;
We estimate the orientation model us-
ing maximum likelihood, which involves
counting and normalizing events of interest:
(Y, o = orit(Y, X)). Specifically, we estimate
Porit(o|Y ) = C(Y, o)/C(Y, ·). Collecting training
counts C(Y,o) involves several steps. First, we
run GIZA++ on the training bitext and apply the
“grow-diag-final” heuristic over the training data
to produce a bi-directional word alignment. Then,
we enumerate all occurrences of Y and determine
orit(Y, X). To ensure uniqueness, we enforce
that neighbor X be the longest possible phrase
that satisfies the consistency constraint (Och and
Ney, 2004). Determining orit(Y, X) can then be
done in a straightforward manner by looking at the
monotonicity (monotone or reverse) and adjacency
(adjacent or gap) between Y&apos;s and X.
</bodyText>
<footnote confidence="0.538321333333333">
&apos;Tn fact, separate models are developed for left and right
neighbors, although for clarity we suppress this distinction
throughout.
</footnote>
<table confidence="0.995387285714286">
MT06 MT08
baseline 30.58 23.59
+itg 29.82 23.21
+XX 30.10 22.86
+XX-nonmono 30.96 24.07
+orit 30.19 23.69
+XX-nonmono+orit 31.49 24.73
</table>
<tableCaption confidence="0.975831666666667">
Table 1: Experimental results where better than baseline
results are italicized, and statistically significant better
(p &lt; 0.01) are in bold.
</tableCaption>
<sectionHeader confidence="0.992916" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999837382352941">
We evaluated the generalization of Hiero to include
XX rules on a Chinese-to-English translation task.
We treat the N = 128 most frequent words in
the corpus as function words, an approximation that
has worked well in the past and minimized depen-
dence on language-specific resources (Setiawan et
al., 2007). We report BLEU r4n4 and assess signifi-
cance using the standard bootstrapping approach.
We trained on the NIST MT06 Eval corpus ex-
cluding the UN data (approximately 900K sentence
pairs), segmenting Chinese using the Harbin seg-
menter (Zhao et al., 2001). Our 5-gram language
model with modified Kneser-Ney smoothing was
trained on the English side of our training data plus
portions of the Gigaword v2 English corpus. We
optimized the feature weights using minimum er-
ror rate training, using the NIST MT03 test set as
the development set. We report the results on the
NIST 2006 evaluation test (MT06) and the NIST
2008 evaluation test (MT08).
Table 1 reports experiments in an incremental
fashion, starting from the baseline model (the orig-
inal Hiero), then adding different sets of rules, and
finally adding the orientation-based model. In our
first experiments, we investigated the introduction
of three different sets of XX rules. First (+itg),
we simply add the ITG&apos;s inverted rule (Rule 6) to
the baseline system in an ad-hoc manner, similar to
the glue rules. This hurts performance consistently
across MT06 and MT08 sets, which we suspect is
a result of ITG rule applications often aggravating
search error. Second (+XX), we permitted general
XX rules. This results in a grammar size increase of
25-26%, filtering out rules irrelevant for the test set,
</bodyText>
<page confidence="0.996922">
351
</page>
<bodyText confidence="0.999983125">
and leads to a significant performance drop, again
perhaps attributable to search error. When we in-
spected the rules, we observe that the majority of
these rules involve spurious word insertions. Third
(+XX-nonmono), we introduced only XX-nonmono
rules; this produced only a 5% additional rules, and
yielded a marginal but consistent gain.
In a second experiment (+orit), we introduced
the target-side function words orientation-based
model. Note that this experiment is orthogonal to the
first set, since we introduce no additional rules. Re-
sults are mixed, worse for MT06 but better (with sig-
nificance) for MT08. Here, we suspect the model&apos;s
potential has not been fully realized, since Hiero
only considers monotone reordering in unseen cases.
Finally, we combine both the XX-nonmono rules
and the Pit model (+XX-nonmono+orit). The
combination produces a significant, consistent gain
across all test sets. This result suggests that the ori-
entation model contributes more strongly in unseen
cases when Hiero also considers non-monotone re-
ordering. We interpret this result as a validation
of our hypothesis that carefully relaxing the non-
adjacent constraint improves translation.
</bodyText>
<sectionHeader confidence="0.997535" genericHeader="discussions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999994785714286">
To our knowledge, the work reported here is the
first to relax the non-adjacent nonterminals con-
straint in hierarchical phrase-based models. The re-
sults confirm that judiciously adding rules to a Hiero
grammar, adjusting the modeling accordingly, can
achieve significant gains.
Although we found that XX-nonmono rules per-
formed better than general XX rules, we believe the
latter may nonetheless prove useful. Manually in-
specting our system&apos;s output, we find that the output
is often shorter than the references, and the missing
words often correspond to function words that are
modeled by those rules. Using XX rules to model
legitimate word insertions is a topic for future work.
</bodyText>
<sectionHeader confidence="0.999068" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999880142857143">
The authors gratefully acknowledge partial support
from the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-2-001. Any opinions, findings, conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reflect the
views of the sponsors.
</bodyText>
<sectionHeader confidence="0.992928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863541666667">
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224–233, Honolulu, Hawaii,
October.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL&apos;05), pages 263–270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Arul Menezes and Chris Quirk. 2007. Using dependency
order templates to improve generality in translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 1–8, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global phrase
reordering model for statistical machine translation.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 713–720, Sydney, Australia, July. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 712–719, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404, Sep.
Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun
Yang, and Fang Liu. 2001. Increasing accuracy of
chinese segmentation with strategy of multi-step pro-
cessing. Journal of Chinese Information Processing
(Chinese Version), 1:13–18.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138–141, New York City, June. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.998261">
352
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.699863">
<title confidence="0.9912155">Phrase-based with Adjacent Nonterminals</title>
<author confidence="0.952632">Hendra Setiawan</author>
<author confidence="0.952632">Philip</author>
<affiliation confidence="0.961404">Laboratory for Computational and Information</affiliation>
<address confidence="0.97735">of Maryland, Park, MD 20742,</address>
<email confidence="0.833743">Kendra,resnik@umd.edu</email>
<abstract confidence="0.9939454">Hierarchical phrase-based translation (Hiero, 2005)) provides an attractive framewithin which both shortand can be addressed consisand efficiently. However, Hiero is erally implemented with a constraint preventcreation of rules with adjacent nonterminals, because such rules introduce compuand We intromethods to address these and demonstrate that rules with adjacent nontermican improve Hiero&apos;s power lead to performance in translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>224--233</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2258" citStr="Chiang et al., 2008" startWordPosition="318" endWordPosition="321"> to drastic reductions in decoding efficiency, and the obvious solutions, such as reducing beam width, erode translation quality. In Section 2, we argue that the non-adjacent nonterminals constraints severely limits Hiero&apos;s generalization power, limiting its coverage of important reordering phenomena. In Section 3, we discuss the challenges that arise in relaxing this constraint. In Section 4 we introduce new methods to address those challenges, and Section 5 validates the approach empirically. Improving Hiero via variations on rule pruning and filtering is well explored, e.g., (Chiang, 2005; Chiang et al., 2008; Zollmann and Venugopal, 2006), to name just a few. These proposals differ from each other mainly in the specific linguistic knowledge being used, and on which side the constraints are applied. In contrast, we complement previous work by showing that adding rules to Hiero can provide benefits if done judiciously. 2 Judicious Use of Adjacent Nonterminals Our motivations largely follow Menezes and Quirk&apos;s (2007) discussion of reorderings and generalization. As a specific example, we will use a Chinese to English verb phrase (VP) translation (Fig. 1), which represents one of the most prominent p</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224–233, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05),</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="961" citStr="Chiang, 2005" startWordPosition="124" endWordPosition="126">ovides an attractive framework within which both short- and longdistance reorderings can be addressed consistently and efficiently. However, Hiero is generally implemented with a constraint preventing the creation of rules with adjacent nonterminals, because such rules introduce computational and modeling challenges. We introduce methods to address these challenges, and demonstrate that rules with adjacent nonterminals can improve Hiero&apos;s generalization power and lead to significant performance gains in Chinese-English translation. 1 Introduction Hierarchical phrase-based translation (Hiero, (Chiang, 2005)) has proven to be a very useful compromise between syntactically informed and purely corpus-driven translation. By automatically learning synchronous grammar rules from parallel text, Hiero captures short- and long-distance reorderings consistently and efficiently. However, implementations of Hiero generally forbid adjacent nonterminal symbols on the source side of hierarchical rules, a practice we will refer to as the non-adjacent nonterminals constraint. The main argument against such rules is that they cause the system to produce multiple derivations that all lead to the same translation –</context>
<context position="2237" citStr="Chiang, 2005" startWordPosition="316" endWordPosition="317">guity can lead to drastic reductions in decoding efficiency, and the obvious solutions, such as reducing beam width, erode translation quality. In Section 2, we argue that the non-adjacent nonterminals constraints severely limits Hiero&apos;s generalization power, limiting its coverage of important reordering phenomena. In Section 3, we discuss the challenges that arise in relaxing this constraint. In Section 4 we introduce new methods to address those challenges, and Section 5 validates the approach empirically. Improving Hiero via variations on rule pruning and filtering is well explored, e.g., (Chiang, 2005; Chiang et al., 2008; Zollmann and Venugopal, 2006), to name just a few. These proposals differ from each other mainly in the specific linguistic knowledge being used, and on which side the constraints are applied. In contrast, we complement previous work by showing that adding rules to Hiero can provide benefits if done judiciously. 2 Judicious Use of Adjacent Nonterminals Our motivations largely follow Menezes and Quirk&apos;s (2007) discussion of reorderings and generalization. As a specific example, we will use a Chinese to English verb phrase (VP) translation (Fig. 1), which represents one of</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05), pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arul Menezes</author>
<author>Chris Quirk</author>
</authors>
<title>Using dependency order templates to improve generality in translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Menezes, Quirk, 2007</marker>
<rawString>Arul Menezes and Chris Quirk. 2007. Using dependency order templates to improve generality in translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 1–8, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
<author>Kuniko Saito</author>
<author>Kazuhide Yamamoto</author>
<author>Kazuteru Ohashi</author>
</authors>
<title>A clustered global phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>713--720</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="8244" citStr="Nagata et al., 2006" startWordPosition="1324" endWordPosition="1327">wan et al. (2007), who looked at neighboring contexts for function words on the source side, we focus here on modeling the influence of function words on neighboring phrases 350 on the target side. We argue that this focus better fits our purpose, since the phrases that we want to model are the function words&apos; neighbors on the target side, as illustrated in Fig. 1. To develop this idea, we first define an orit function that takes a source function word as a reference point, along with its neighboring phrase on the target side. The orit function outputs one of the following orientation values (Nagata et al., 2006): Monotone-Adjacent (MA); Reverse-Adjacent (RA); Monotone-Gap (MG); and Reverse-Gap (RG). The Monotone/Reverse distinction indicates whether the source order follows the target order. The Adjacent/Gap distinction indicates whether the two phrases are adjacent or separated by an intervening phrase on the source side. For example, in Fig. 1, the value of orit for right neighbor Eastern division with respect to function word ;i!�E (at) is MA, since its corresponding source phrase 3.009 is adjacent to r± (at) and their order is preserved on the English side. The value for left neighbor rank 10th w</context>
</contexts>
<marker>Nagata, Saito, Yamamoto, Ohashi, 2006</marker>
<rawString>Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto, and Kazuteru Ohashi. 2006. A clustered global phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 713–720, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="9749" citStr="Och and Ney, 2004" startWordPosition="1565" endWordPosition="1568">&apos; We estimate the orientation model using maximum likelihood, which involves counting and normalizing events of interest: (Y, o = orit(Y, X)). Specifically, we estimate Porit(o|Y ) = C(Y, o)/C(Y, ·). Collecting training counts C(Y,o) involves several steps. First, we run GIZA++ on the training bitext and apply the “grow-diag-final” heuristic over the training data to produce a bi-directional word alignment. Then, we enumerate all occurrences of Y and determine orit(Y, X). To ensure uniqueness, we enforce that neighbor X be the longest possible phrase that satisfies the consistency constraint (Och and Ney, 2004). Determining orit(Y, X) can then be done in a straightforward manner by looking at the monotonicity (monotone or reverse) and adjacency (adjacent or gap) between Y&apos;s and X. &apos;Tn fact, separate models are developed for left and right neighbors, although for clarity we suppress this distinction throughout. MT06 MT08 baseline 30.58 23.59 +itg 29.82 23.21 +XX 30.10 22.86 +XX-nonmono 30.96 24.07 +orit 30.19 23.69 +XX-nonmono+orit 31.49 24.73 Table 1: Experimental results where better than baseline results are italicized, and statistically significant better (p &lt; 0.01) are in bold. 5 Experiments We </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Min-Yen Kan</author>
<author>Haizhou Li</author>
</authors>
<title>Ordering phrases with function words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>712--719</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7485" citStr="Setiawan et al., 2007" startWordPosition="1195" endWordPosition="1198">liable. As a result the decoder may simply prefer to always select glue rules because they are associated with fewer features resulting in adjacent phrases always being translated in a monotone order. To address this issue, we introduce a new model, which we call the target-side function words orientation-based model, or simply PoTit, which evaluates the application of the two kinds of rules on the same context, i.e. for our example, it is the function word ;i!�E(at). 4 Target-side Function Words Orientation-based Model The PoTit model is motivated by the function words reordering hypothesis (Setiawan et al., 2007), which suggests that function words encode essential information about the (re)ordering of their neighboring phrases. In contrast to Setiawan et al. (2007), who looked at neighboring contexts for function words on the source side, we focus here on modeling the influence of function words on neighboring phrases 350 on the target side. We argue that this focus better fits our purpose, since the phrases that we want to model are the function words&apos; neighbors on the target side, as illustrated in Fig. 1. To develop this idea, we first define an orit function that takes a source function word as a</context>
<context position="10652" citStr="Setiawan et al., 2007" startWordPosition="1705" endWordPosition="1708"> distinction throughout. MT06 MT08 baseline 30.58 23.59 +itg 29.82 23.21 +XX 30.10 22.86 +XX-nonmono 30.96 24.07 +orit 30.19 23.69 +XX-nonmono+orit 31.49 24.73 Table 1: Experimental results where better than baseline results are italicized, and statistically significant better (p &lt; 0.01) are in bold. 5 Experiments We evaluated the generalization of Hiero to include XX rules on a Chinese-to-English translation task. We treat the N = 128 most frequent words in the corpus as function words, an approximation that has worked well in the past and minimized dependence on language-specific resources (Setiawan et al., 2007). We report BLEU r4n4 and assess significance using the standard bootstrapping approach. We trained on the NIST MT06 Eval corpus excluding the UN data (approximately 900K sentence pairs), segmenting Chinese using the Harbin segmenter (Zhao et al., 2001). Our 5-gram language model with modified Kneser-Ney smoothing was trained on the English side of our training data plus portions of the Gigaword v2 English corpus. We optimized the feature weights using minimum error rate training, using the NIST MT03 test set as the development set. We report the results on the NIST 2006 evaluation test (MT06)</context>
</contexts>
<marker>Setiawan, Kan, Li, 2007</marker>
<rawString>Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007. Ordering phrases with function words. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 712–719, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4862" citStr="Wu, 1997" startWordPosition="770" endWordPosition="771">ate VP-A in a monotone order. On the other hand, consider the following rules with adjacent nonterminals on the source side (or XX rules, for brevity): X,(r± X1X2, X2 at X1) X,(X1X2 �i� F , rank 10th X1X2) X,(X1X2, X2X1) Note that although XX rules 4-6 can potentially increase the chance of modeling the pre-verbal to postverbal shift, not all of them are beneficial to learn. For instance, Rule 5 models the word order shift but introduces spurious ambiguity, since the nonterminals are translated in monotone order. Rule 6, which resembles the inverted rule of the Inversion Transduction Grammar (Wu, 1997), is highly ambiguous because its application has no lexical grounding. Rule 4 avoids both problems, and is also easier to learn, since it is lexically anchored by a preposition, ;i!�E(at), which we can expect to appear frequently in training. These observations will motivate us to focus on rules that model non-monotone reordering of phrases surrounding a lexical item on the target side. 3 Addressing XX Rule Challenges The first challenge created by introducing XX rules is computational: relaxing the constraint significantly increases the grammar size. Motivated by our earlier discussion, we a</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiejun Zhao</author>
<author>Yajuan Lv</author>
<author>Jianmin Yao</author>
<author>Hao Yu</author>
<author>Muyun Yang</author>
<author>Fang Liu</author>
</authors>
<title>Increasing accuracy of chinese segmentation with strategy of multi-step processing.</title>
<date>2001</date>
<journal>Journal of Chinese Information Processing (Chinese Version),</journal>
<pages>1--13</pages>
<contexts>
<context position="10905" citStr="Zhao et al., 2001" startWordPosition="1746" endWordPosition="1749">y significant better (p &lt; 0.01) are in bold. 5 Experiments We evaluated the generalization of Hiero to include XX rules on a Chinese-to-English translation task. We treat the N = 128 most frequent words in the corpus as function words, an approximation that has worked well in the past and minimized dependence on language-specific resources (Setiawan et al., 2007). We report BLEU r4n4 and assess significance using the standard bootstrapping approach. We trained on the NIST MT06 Eval corpus excluding the UN data (approximately 900K sentence pairs), segmenting Chinese using the Harbin segmenter (Zhao et al., 2001). Our 5-gram language model with modified Kneser-Ney smoothing was trained on the English side of our training data plus portions of the Gigaword v2 English corpus. We optimized the feature weights using minimum error rate training, using the NIST MT03 test set as the development set. We report the results on the NIST 2006 evaluation test (MT06) and the NIST 2008 evaluation test (MT08). Table 1 reports experiments in an incremental fashion, starting from the baseline model (the original Hiero), then adding different sets of rules, and finally adding the orientation-based model. In our first ex</context>
</contexts>
<marker>Zhao, Lv, Yao, Yu, Yang, Liu, 2001</marker>
<rawString>Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun Yang, and Fang Liu. 2001. Increasing accuracy of chinese segmentation with strategy of multi-step processing. Journal of Chinese Information Processing (Chinese Version), 1:13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="2289" citStr="Zollmann and Venugopal, 2006" startWordPosition="322" endWordPosition="325">s in decoding efficiency, and the obvious solutions, such as reducing beam width, erode translation quality. In Section 2, we argue that the non-adjacent nonterminals constraints severely limits Hiero&apos;s generalization power, limiting its coverage of important reordering phenomena. In Section 3, we discuss the challenges that arise in relaxing this constraint. In Section 4 we introduce new methods to address those challenges, and Section 5 validates the approach empirically. Improving Hiero via variations on rule pruning and filtering is well explored, e.g., (Chiang, 2005; Chiang et al., 2008; Zollmann and Venugopal, 2006), to name just a few. These proposals differ from each other mainly in the specific linguistic knowledge being used, and on which side the constraints are applied. In contrast, we complement previous work by showing that adding rules to Hiero can provide benefits if done judiciously. 2 Judicious Use of Adjacent Nonterminals Our motivations largely follow Menezes and Quirk&apos;s (2007) discussion of reorderings and generalization. As a specific example, we will use a Chinese to English verb phrase (VP) translation (Fig. 1), which represents one of the most prominent phrase constructions in Chinese.</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation, pages 138–141, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>