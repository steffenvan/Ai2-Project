<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028739">
<title confidence="0.994516">
Meaningful Conversation with a Mobile Robot
</title>
<author confidence="0.999299">
Johan Bos, Ewan Klein, Tetsushi Oka
</author>
<affiliation confidence="0.817259">
ICCS, School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
</affiliation>
<address confidence="0.786992">
Scotland, United Kingdom
</address>
<email confidence="0.995702">
Obos,ewan,okatl@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977">
We describe an implementation integrat-
ing a spoken dialogue system with a mo-
bile robot, which the user can direct to
specific locations, ask for information
about its status, and supply information
about its environment. The robot uses
an internal map for navigation, and com-
municates its current orientation and ac-
cessible locations to the dialogue system
using a topological map as interface.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999834666666667">
Most research on spoken dialogue has focused
on humans talking to virtual agents, often only
reached at the end of a telephone line. Interesting
challenges and opportunities arise when the inter-
locutor is a physically embodied mobile agent—
for example, a robot. When we enter into dia-
logue with a robot, we can talk about the phys-
ical environment that we share with the robot,
and we get a palpable indicator of dialogue suc-
cess when an utterance such as go to the corri-
dor produces the desired effect. In short, spoken
dialogue research with mobile robots opens up a
new vista for human-computer interaction design
which goes beyond the current preoccupation with
visual interfaces. In this paper, we give a short
overview of the kind of dialogue that can be held
with Godot, our robot, together with the architec-
ture and technologies that we have implemented.
</bodyText>
<sectionHeader confidence="0.98868" genericHeader="method">
2 The Spoken Dialogue System
</sectionHeader>
<bodyText confidence="0.999742428571429">
The spoken dialogue system allows the user to
move Godot by giving it commands, to ask about
its current location, or to provide it with new
information. This section explains how speech
processing, language modelling, language under-
standing and dialogue management have been im-
plemented.
</bodyText>
<subsectionHeader confidence="0.98375">
2.1 Speech Recognition
</subsectionHeader>
<bodyText confidence="0.9999681875">
Nuance&apos;s speaker-independent speech recognition
system (www. nuance . corn) allows language mod-
els to be specified using Speech Grammar rules.
Rather than writing these directly, we compile
them from a unification grammar for English.
Moreover, instead of adopting the Speech Gram-
mar slot-filling paradigm for semantic interpreta-
tion, our grammar builds a sophisticated, com-
positional semantics involving A terms which are
passed as the value of a single slot for the recog-
nised sentence. As a result the output of the speech
recognition stage is a semantic representation, and
no further parsing is required before handing it
over to the dialogue manager. In order to reduce
perplexity, different grammars are loaded at differ-
ent states in the dialogue.
</bodyText>
<subsectionHeader confidence="0.99834">
2.2 Natural Language Understanding
</subsectionHeader>
<bodyText confidence="0.999839117647059">
Discourse Representation Theory (DRT) (Kamp
and Reyle, 1993) is used for meaning represen-
tation in the system. The current implementation
covers a wide variety of linguistic phenomena,
including context-sensitive phenomena involving
anaphora, presupposition, quantification, and plu-
ral descriptions. One crucial benefit of DRT is that
it supports inference, using a translation from Dis-
course Representation Structures (DRSs) to for-
mulas of first-order logic. Inference helps to de-
tect inconsistencies in the dialogue and assists in
disambiguation.
Inference invokes standard theorem proving
techniques in a context sensitive way. The DRS
representing the dialogue is combined with a DRS
containing information about the current situation
(i.e., the position of the robot and currently ac-
</bodyText>
<page confidence="0.997624">
71
</page>
<bodyText confidence="0.999721545454545">
cessible locations), and translated into first-order
logic. This is combined with further background
knowledge (frame axioms and axioms about tem-
poral states, plus ontological information), and the
resulting formula 0 is sent to both a model gener-
ator and a theorem prover. If the theorem prover
finds a counter-proof, we treat 6 as inconsistent
information; conversely, if the model builder finds
a model for q, we use the model to deduce what
actions need to be performed by the robot, or to
answer questions posed by the user.
</bodyText>
<subsectionHeader confidence="0.990149">
2.3 Dialogue Management
</subsectionHeader>
<bodyText confidence="0.999893970588235">
We adopt the approach to dialogue move engines
developed within TRINDI (Traum et al., 1999),
in which an agent&apos;s information state is updated
on the basis of observed dialogue moves, lead-
ing to the selection of a new dialogue move to
be performed by the agent. Our notion of in-
formation state consists of grammar (the gram-
mar currently loaded by the speech recogniser),
contact (whether or not there is communica-
tive contact with someone), input (the results of
speech recognition), nextmoves (the next dia-
logue moves to be realised by the robot), last—
moves (the latest dialogue moves produced by
the user), and interpretation (consisting of
the DRS of the ongoing dialogue and a first-order
model generated for it).
An update rule links preconditions to effects.
The dialogue manager repeatedly computes the ef-
fects of those update rules whose preconditions
are satisfied by the current information state. Pre-
conditions are expressed in terms of current val-
ues in the information state, while the effects will
change these values. The 26 update rules in our
current system deal with establishing contact with
the user, initiating clarification dialogues (when
the recognition confidence score is below a certain
threshold), answering questions, acknowledging
requests and confirming or denying statements.
System output is generated from templates and
synthesised by the Festival TTS system (www.
cstr.ed.ac.uk/projects/festival4 Utter-
ances are coded in SABLE format (an XML stan-
dard for speech synthesis markup) in order to as-
sign appropriate prosodic contours.
</bodyText>
<sectionHeader confidence="0.990931" genericHeader="method">
3 The Mobile Robot System
</sectionHeader>
<bodyText confidence="0.999965">
In this section we describe the hardware of the
robot itself, the internal map representation it uses
for navigation and communication with the dia-
logue manager, and the navigation component.
</bodyText>
<subsectionHeader confidence="0.981104">
3.1 Godot. the Robot
</subsectionHeader>
<bodyText confidence="0.962957130434782">
Godot is an RWI Magellan Pro mobile robot plat-
form with an on-board PC running Linux (Fig. 1).
It is cylindrical, about 50 cm high and 41 cm in
diameter. Godot&apos;s sensor equipment consists of
16 sonars, infrared sen-
sors, and bumpers, an
odometry component,
and a colour video cam-
era with a pan-tilt unit.
The on-board computer
is connected to the local
network via a wireless Fig.1: Godot.
LAN interface. Godot&apos;s navigation system relies
on sonars, infrared sensors and odometry, and not
the bumpers or the
camera. However,
the camera is used as
live feedback to the
user (Fig. 2), who is
able to look &amp;quot;through
the eyes&amp;quot; of Godot
while engaging in a
dialogue.
</bodyText>
<subsectionHeader confidence="0.994006">
3.2 The Internal Map
</subsectionHeader>
<bodyText confidence="0.997969076923077">
Godot moves about in the basement of our depart-
ment and uses an internal map for navigation. It
has two levels of representation: a geometrical and
a topological layer.
The geometrical layer uses an occupancy grid
to represent occupied and free space in the envi-
ronment. The topological layer is automatically
constructed from the occupancy grid by subdivid-
ing the free space into distinct topological regions
corresponding to rooms or parts of the corridor
(Fig. 3). This is possible by creating a Generalised
Voronoi Diagram (Latombe, 1991; Thrun, 1998;
Theobalt, 2000).
</bodyText>
<sectionHeader confidence="0.161439" genericHeader="method">
Fig.2: Image Viewer
</sectionHeader>
<page confidence="0.877871">
72
</page>
<figureCaption confidence="0.881152">
Fig.3: Representation of the Environment
</figureCaption>
<bodyText confidence="0.983685916666666">
The numbers in the geometrical map shown
in Fig. 3 are identifiers of topological regions
which can be seen as nodes of an undirected
graph. There is a further layer of representation
interfacing the map of the navigation system
with a vocabulary of semantic symbols used
by the dialogue system. This layer extends
the topological map by associating semantic
labels to regions. These descriptions can be
arbitrarily complex. For instance, the DRS
Ap.(([x , [office (x), of (x , y), tim(y)]); p(x))
is used as a label denoting Tim&apos;s office.
</bodyText>
<subsectionHeader confidence="0.967005">
3.3 Navigation Module
</subsectionHeader>
<bodyText confidence="0.99998005882353">
The navigation module loops by reading sensory
input and writing motor commands at regular in-
tervals. The sensory input comprises the readings
of the sonars, infrared sensors and odometry. The
motor commands govern translational and rota-
tional velocity of the robot, and pan/tilt/zoom of
the camera unit. They are triggered by readings of
the sensors or by commands communicated from
the dialogue manager.
The behaviour of the navigation module is
triggered by the last command from the dia-
logue manager. There are primitive commands
such as go(Distance, Speed), tum(Angle, Speed)
and look(Pan,Tilt) as well as complex commands
like follow _wall(Distance) or move _to _region(N ).
Since the navigation module accepts commands
at any time step, it is possible to interrupt on-
going behaviour. It is also possible to change
the parameters of currently executing commands.
Thus set_rotation(Angle) can be used to change the
robot&apos;s belief about its current orientation.
Moving to a particular grid cell or region re-
quires accessing information about the environ-
ment that is stored in the internal map. The naviga-
tion module keeps track of the current position and
orientation using odometry. It can detect walls us-
ing the readings of the infrared sensors and correct
translational and rotational errors of the odorn etry
by comparing them with the geometrical map.
The topological map is used to compute the
shortest path from one region to another. This
path is executed by setting subgoals for naviga-
tion, because there can be walls or obstacles be-
tween two regions. The position of the centre of a
region lying along the path is also obtained from
the topological map. Given the grid cell of the cen-
tre of the next region, the navigation module plans
a path from the current location to the centre of
the region on the geometrical map, by means of a
distance transform path planner (Theobalt, 2000).
After planning a path, one of the cells in the path
is selected as the next subgoal. When the robot en-
ters a new region, a new path is planned on both of
the layers.
Motor commands are computed at every time
step based on the current position and orientation,
the location of subgoals and the sensor readings.
The robot can avoid obstacles using the sonars and
infrared sensors when it is moving to another re-
gion, since there is some latitude in how precisely
it traces a path with respect to the grid cells.
</bodyText>
<sectionHeader confidence="0.923557" genericHeader="method">
4 Putting Everything Together
</sectionHeader>
<bodyText confidence="0.9997244">
The dialogue system is implemented on top of
the Open Agent Architecture (OAA). Godot, how-
ever, uses CORBA for inter-process communica-
tion. Fig. 4 shows how the two systems are imple-
mented and combined into one working system.
</bodyText>
<figure confidence="0.96014">
4-0 OAA connection I USER
Spoken Dialogue System
command
state
on-board
Map Server
Topological Map •
Geometrical Map •
CORBA
41—lo Connection
&apos;Mr Object
CORBA sensors J motors
Robot Hardware j
</figure>
<figureCaption confidence="0.990729">
Fig. 4: System Architecture
</figureCaption>
<figure confidence="0.996095307692308">
Image Viewer F
Semantic Ma
map info.
off-board
OAA Facilitator
Navigation System
HNavigation
Module
Navigation •
H Navigation
OAA wrapper
ir:01:111
images
</figure>
<page confidence="0.987117">
73
</page>
<subsectionHeader confidence="0.984708">
4.1 The Dialogue System
</subsectionHeader>
<bodyText confidence="0.999739444444445">
The dialogue system is implemented on top of
OAA 2.1.0, (www s com/ Maa/). OAA
agents can run on different machines and even on
different platforms, and they communicate with
each other by posing solvables via a facilitator.
The system contains OAA-agents for speech
recognition (Nuance 8.0), speech synthesis (Fes-
tival), resolution (resolving ambiguities), infer-
ence (building models or finding counter-proofs),
and dialogue management. There are two further
agents for model building (MACE, www-unix.
mcs.anl.gov/AR/mace/) and theorem proving
(SPASS, spa s s mpi - sb . mpg . de/).
The system is coordinated by the dialogue man-
ager, triggering OAA-solvables for speech recog-
nition, speech synthesis and inference. It can also
request information as the the robot&apos;s position and
its current environment.
</bodyText>
<subsectionHeader confidence="0.977521">
4.2 The Navigation System
</subsectionHeader>
<bodyText confidence="0.999936611111111">
The navigation system consists of three compo-
nents which run concurrently on the on-board PC
of Godot (Fig. 4). They are Linux processes which
communicate with each other via CORBA objects.
The map server stores Godot&apos;s internal representa-
tion of the environment as CORBA objects. Infor-
mation stored in these objects can be retrieved by
the navigation and dialogue modules via CORBA
or OAA connections.
The navigation module has a CORBA object to
store information that it shares with the dialogue
system. The OAA-wrapper has access to this ob-
ject and OAA agents in the dialogue system can
communicate with the navigation module via the
wrapper. The dialogue system can monitor the
current state of the navigation system, e.g. the cur-
rent location of the robot in the map, and can send
commands to it.
</bodyText>
<subsectionHeader confidence="0.999039">
4.3 Running the System
</subsectionHeader>
<bodyText confidence="0.999988666666666">
The system is distributed across a Linux laptop
(the dialogue system) and the robot&apos;s on-board PC
(running the navigation system). Users can start a
new dialogue simply by addressing the robot, and
the robot reacts in real time. Although we have
not yet reached the stage of carrying out usability
studies, we have held informal tests where visitors
to our department who are unfamiliar with Godot
have controlled it and its camera with success.
</bodyText>
<sectionHeader confidence="0.987916" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979136363636">
Although it is tempting to work with simulated
&apos;embodied&apos; agents, we believe that the real test of
adequacy involves confronting the familiar prob-
lems of noisy sensor data and real hardware. Us-
ing the OAA architecture, we have developed an
effective interface between natural language se-
mantics and the robot control layer, thus enabling
users to refer to locations in a natural way, rather
than resorting to expressions like go to grid cell
45-66 or you are in region 12. The framework is
to a large extent domain-independent: a change of
environment would only require a change of the
internal map and possibly a new lexicon.
Future work will address the interpretation
of vague expressions (the end of the corridor),
metonymic expressions (go to the door, where an
artifact is interpreted as a location), together with
commands which require Godot to reason and talk
about its current activities (continue going to the
kitchen). We are further planning to extend the
system with a face-recognition component to en-
rich the possibilities of natural interaction.
</bodyText>
<sectionHeader confidence="0.986162" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997848666666667">
Part of this work was supported by the EU Project
Magicster (IST 1999-29078). We thank Nuance
for permission to use their software and tools.
</bodyText>
<sectionHeader confidence="0.998592" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999748571428571">
H. Kamp and U. Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
J. C. Latombe. 1991. Robot Motion Planning. Kluwer
Academic Publishers.
C. Theobalt. 2000. Navigation on a mobile robot.
Master&apos;s thesis, University of Edinburgh.
S. Thrun. 1998. Learning maps for indoor mobile
robots. Artificial Intelligence, 99(1): 21-71.
D. Traum, J. Bos, R. Cooper, S. Larsson, I. Lewin, C.
Matheson, and M. Poesio. 1999. A model of dia-
logue moves and information state revision. Trindi
Report D2.1.
</reference>
<page confidence="0.999135">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.523698">
<title confidence="0.999486">Meaningful Conversation with a Mobile Robot</title>
<author confidence="0.997863">Johan Bos</author>
<author confidence="0.997863">Ewan Klein</author>
<author confidence="0.997863">Tetsushi Oka</author>
<affiliation confidence="0.999535">ICCS, School of Informatics, University of Edinburgh</affiliation>
<address confidence="0.747659">2 Buccleuch Place, Edinburgh EH8 9LW Scotland, United Kingdom</address>
<email confidence="0.977112">Obos,ewan,okatl@inf.ed.ac.uk</email>
<abstract confidence="0.999799090909091">We describe an implementation integrating a spoken dialogue system with a mobile robot, which the user can direct to specific locations, ask for information about its status, and supply information about its environment. The robot uses an internal map for navigation, and communicates its current orientation and accessible locations to the dialogue system using a topological map as interface.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>U Reyle</author>
</authors>
<title>From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="2697" citStr="Kamp and Reyle, 1993" startWordPosition="421" endWordPosition="424">lish. Moreover, instead of adopting the Speech Grammar slot-filling paradigm for semantic interpretation, our grammar builds a sophisticated, compositional semantics involving A terms which are passed as the value of a single slot for the recognised sentence. As a result the output of the speech recognition stage is a semantic representation, and no further parsing is required before handing it over to the dialogue manager. In order to reduce perplexity, different grammars are loaded at different states in the dialogue. 2.2 Natural Language Understanding Discourse Representation Theory (DRT) (Kamp and Reyle, 1993) is used for meaning representation in the system. The current implementation covers a wide variety of linguistic phenomena, including context-sensitive phenomena involving anaphora, presupposition, quantification, and plural descriptions. One crucial benefit of DRT is that it supports inference, using a translation from Discourse Representation Structures (DRSs) to formulas of first-order logic. Inference helps to detect inconsistencies in the dialogue and assists in disambiguation. Inference invokes standard theorem proving techniques in a context sensitive way. The DRS representing the dial</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>H. Kamp and U. Reyle. 1993. From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Latombe</author>
</authors>
<title>Robot Motion Planning.</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="7008" citStr="Latombe, 1991" startWordPosition="1109" endWordPosition="1110">h the eyes&amp;quot; of Godot while engaging in a dialogue. 3.2 The Internal Map Godot moves about in the basement of our department and uses an internal map for navigation. It has two levels of representation: a geometrical and a topological layer. The geometrical layer uses an occupancy grid to represent occupied and free space in the environment. The topological layer is automatically constructed from the occupancy grid by subdividing the free space into distinct topological regions corresponding to rooms or parts of the corridor (Fig. 3). This is possible by creating a Generalised Voronoi Diagram (Latombe, 1991; Thrun, 1998; Theobalt, 2000). Fig.2: Image Viewer 72 Fig.3: Representation of the Environment The numbers in the geometrical map shown in Fig. 3 are identifiers of topological regions which can be seen as nodes of an undirected graph. There is a further layer of representation interfacing the map of the navigation system with a vocabulary of semantic symbols used by the dialogue system. This layer extends the topological map by associating semantic labels to regions. These descriptions can be arbitrarily complex. For instance, the DRS Ap.(([x , [office (x), of (x , y), tim(y)]); p(x)) is use</context>
</contexts>
<marker>Latombe, 1991</marker>
<rawString>J. C. Latombe. 1991. Robot Motion Planning. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Theobalt</author>
</authors>
<title>Navigation on a mobile robot. Master&apos;s thesis,</title>
<date>2000</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="7038" citStr="Theobalt, 2000" startWordPosition="1113" endWordPosition="1114">ngaging in a dialogue. 3.2 The Internal Map Godot moves about in the basement of our department and uses an internal map for navigation. It has two levels of representation: a geometrical and a topological layer. The geometrical layer uses an occupancy grid to represent occupied and free space in the environment. The topological layer is automatically constructed from the occupancy grid by subdividing the free space into distinct topological regions corresponding to rooms or parts of the corridor (Fig. 3). This is possible by creating a Generalised Voronoi Diagram (Latombe, 1991; Thrun, 1998; Theobalt, 2000). Fig.2: Image Viewer 72 Fig.3: Representation of the Environment The numbers in the geometrical map shown in Fig. 3 are identifiers of topological regions which can be seen as nodes of an undirected graph. There is a further layer of representation interfacing the map of the navigation system with a vocabulary of semantic symbols used by the dialogue system. This layer extends the topological map by associating semantic labels to regions. These descriptions can be arbitrarily complex. For instance, the DRS Ap.(([x , [office (x), of (x , y), tim(y)]); p(x)) is used as a label denoting Tim&apos;s of</context>
<context position="9563" citStr="Theobalt, 2000" startWordPosition="1527" endWordPosition="1528">otational errors of the odorn etry by comparing them with the geometrical map. The topological map is used to compute the shortest path from one region to another. This path is executed by setting subgoals for navigation, because there can be walls or obstacles between two regions. The position of the centre of a region lying along the path is also obtained from the topological map. Given the grid cell of the centre of the next region, the navigation module plans a path from the current location to the centre of the region on the geometrical map, by means of a distance transform path planner (Theobalt, 2000). After planning a path, one of the cells in the path is selected as the next subgoal. When the robot enters a new region, a new path is planned on both of the layers. Motor commands are computed at every time step based on the current position and orientation, the location of subgoals and the sensor readings. The robot can avoid obstacles using the sonars and infrared sensors when it is moving to another region, since there is some latitude in how precisely it traces a path with respect to the grid cells. 4 Putting Everything Together The dialogue system is implemented on top of the Open Agen</context>
</contexts>
<marker>Theobalt, 2000</marker>
<rawString>C. Theobalt. 2000. Navigation on a mobile robot. Master&apos;s thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Thrun</author>
</authors>
<title>Learning maps for indoor mobile robots.</title>
<date>1998</date>
<journal>Artificial Intelligence,</journal>
<volume>99</volume>
<issue>1</issue>
<pages>21--71</pages>
<contexts>
<context position="7021" citStr="Thrun, 1998" startWordPosition="1111" endWordPosition="1112">Godot while engaging in a dialogue. 3.2 The Internal Map Godot moves about in the basement of our department and uses an internal map for navigation. It has two levels of representation: a geometrical and a topological layer. The geometrical layer uses an occupancy grid to represent occupied and free space in the environment. The topological layer is automatically constructed from the occupancy grid by subdividing the free space into distinct topological regions corresponding to rooms or parts of the corridor (Fig. 3). This is possible by creating a Generalised Voronoi Diagram (Latombe, 1991; Thrun, 1998; Theobalt, 2000). Fig.2: Image Viewer 72 Fig.3: Representation of the Environment The numbers in the geometrical map shown in Fig. 3 are identifiers of topological regions which can be seen as nodes of an undirected graph. There is a further layer of representation interfacing the map of the navigation system with a vocabulary of semantic symbols used by the dialogue system. This layer extends the topological map by associating semantic labels to regions. These descriptions can be arbitrarily complex. For instance, the DRS Ap.(([x , [office (x), of (x , y), tim(y)]); p(x)) is used as a label </context>
</contexts>
<marker>Thrun, 1998</marker>
<rawString>S. Thrun. 1998. Learning maps for indoor mobile robots. Artificial Intelligence, 99(1): 21-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
<author>J Bos</author>
<author>R Cooper</author>
<author>S Larsson</author>
<author>I Lewin</author>
<author>C Matheson</author>
<author>M Poesio</author>
</authors>
<title>A model of dialogue moves and information state revision.</title>
<date>1999</date>
<tech>Trindi Report D2.1.</tech>
<contexts>
<context position="4073" citStr="Traum et al., 1999" startWordPosition="632" endWordPosition="635">ranslated into first-order logic. This is combined with further background knowledge (frame axioms and axioms about temporal states, plus ontological information), and the resulting formula 0 is sent to both a model generator and a theorem prover. If the theorem prover finds a counter-proof, we treat 6 as inconsistent information; conversely, if the model builder finds a model for q, we use the model to deduce what actions need to be performed by the robot, or to answer questions posed by the user. 2.3 Dialogue Management We adopt the approach to dialogue move engines developed within TRINDI (Traum et al., 1999), in which an agent&apos;s information state is updated on the basis of observed dialogue moves, leading to the selection of a new dialogue move to be performed by the agent. Our notion of information state consists of grammar (the grammar currently loaded by the speech recogniser), contact (whether or not there is communicative contact with someone), input (the results of speech recognition), nextmoves (the next dialogue moves to be realised by the robot), last— moves (the latest dialogue moves produced by the user), and interpretation (consisting of the DRS of the ongoing dialogue and a first-ord</context>
</contexts>
<marker>Traum, Bos, Cooper, Larsson, Lewin, Matheson, Poesio, 1999</marker>
<rawString>D. Traum, J. Bos, R. Cooper, S. Larsson, I. Lewin, C. Matheson, and M. Poesio. 1999. A model of dialogue moves and information state revision. Trindi Report D2.1.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>