<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030420">
<title confidence="0.983944">
Improved Statistical Machine Translation Using Paraphrases
</title>
<author confidence="0.996621">
Chris Callison-Burch Philipp Koehn Miles Osborne
</author>
<affiliation confidence="0.998428">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.820902">
2 Buccleuch Place
Edinburgh, EH8 9LW
</address>
<email confidence="0.998872">
callison-burch@ed.ac.uk
</email>
<sectionHeader confidence="0.993891" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866368421053">
Parallel corpora are crucial for training
SMT systems. However, for many lan-
guage pairs they are available only in
very limited quantities. For these lan-
guage pairs a huge portion of phrases en-
countered at run-time will be unknown.
We show how techniques from paraphras-
ing can be used to deal with these oth-
erwise unknown source language phrases.
Our results show that augmenting a state-
of-the-art SMT system with paraphrases
leads to significantly improved coverage
and translation quality. For a training
corpus with 10,000 sentence pairs we in-
crease the coverage of unique test set un-
igrams from 48% to 90%, with more than
half of the newly covered items accurately
translated, as opposed to none in current
approaches.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997575">
As with many other statistical natural language pro-
cessing tasks, statistical machine translation (Brown
et al., 1993) produces high quality results when am-
ple training data is available. This is problematic for
so called “low density” language pairs which do not
have very large parallel corpora. For example, when
words occur infrequently in a parallel corpus param-
eter estimates for word-level alignments can be in-
accurate, which can in turn lead to inaccurate phrase
translations. Limited amounts of training data can
further lead to a problem of low coverage in that
many phrases encountered at run-time are not ob-
</bodyText>
<page confidence="0.985584">
17
</page>
<bodyText confidence="0.996208083333333">
served in the training data and therefore their trans-
lations will not be learned.
Here we address the problem of unknown phrases.
Specifically we show that upon encountering an un-
known source phrase, we can substitute a paraphrase
for it and then proceed using the translation of that
paraphrase. We derive these paraphrases from re-
sources that are external to the parallel corpus that
the translation model is trained from, and we are
able to exploit (potentially more abundant) parallel
corpora from other language pairs to do so.
In this paper we:
</bodyText>
<listItem confidence="0.990431266666667">
• Define a method for incorporating paraphrases
of unseen source phrases into the statistical ma-
chine translation process.
• Show that by translating paraphrases we
achieve a marked improvement in coverage and
translation quality, especially in the case of un-
known words which to date have been left un-
translated.
• Argue that while we observe an improvement
in Bleu score, this metric is particularly poorly
suited to measuring the sort of improvements
that we achieve.
• Present an alternative methodology for targeted
manual evaluation that may be useful in other
research projects.
</listItem>
<sectionHeader confidence="0.916695" genericHeader="introduction">
2 The Problem of Coverage in SMT
</sectionHeader>
<bodyText confidence="0.9529655">
Statistical machine translation made considerable
advances in translation quality with the introduc-
tion of phrase-based translation (Marcu and Wong,
2002; Koehn et al., 2003; Och and Ney, 2004). By
</bodyText>
<note confidence="0.9924755">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 17–24,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<figure confidence="0.933885333333333">
Test Set Items with Translations (%)
10000 100000 1e+06 1e+07
Training Corpus Size (num words)
</figure>
<figureCaption confidence="0.999344">
Figure 1: Percent of unique unigrams, bigrams, tri-
</figureCaption>
<bodyText confidence="0.96622809375">
grams, and 4-grams from the Europarl Spanish test
sentences for which translations were learned in in-
creasingly large training corpora
increasing the size of the basic unit of translation,
phrase-based machine translation does away with
many of the problems associated with the original
word-based formulation of statistical machine trans-
lation (Brown et al., 1993). For instance, with multi-
word units less re-ordering needs to occur since lo-
cal dependencies are frequently captured. For exam-
ple, common adjective-noun alternations are mem-
orized. However, since this linguistic information
is not explicitly and generatively encoded in the
model, unseen adjective noun pairs may still be han-
dled incorrectly.
Thus, having observed phrases in the past dramat-
ically increases the chances that they will be trans-
lated correctly in the future. However, for any given
test set, a huge amount of training data has to be ob-
served before translations are learned for a reason-
able percentage of the test phrases. Figure 1 shows
the extent of this problem. For a training corpus
containing 10,000 words translations will have been
learned for only 10% of the unigrams (types, not
tokens). For a training corpus containing 100,000
words this increases to 30%. It is not until nearly
10,000,000 words worth of training data have been
analyzed that translation for more than 90% of the
vocabulary items have been learned. This problem
is obviously compounded for higher-order n-grams
(longer phrases), and for morphologically richer lan-
guages.
</bodyText>
<table confidence="0.948883285714286">
encargarnos to ensure, take care, ensure that
garantizar guarantee, ensure, guaranteed, as-
velar sure, provided
procurar ensure, ensuring, safeguard, making
asegurarnos sure
ensure that, try to, ensure, endeavour
to
ensure, secure, make certain
usado used
utilizado used, use, spent, utilized
empleado used, spent, employee
uso use,used,usage
utiliza used, uses, used, being used
utilizar to use, use, used
</table>
<tableCaption confidence="0.994642">
Table 1: Example of automatically generated para-
</tableCaption>
<bodyText confidence="0.932669666666667">
phrases for the Spanish words encargarnos and us-
ado along with their English translations which were
automatically learned from the Europarl corpus
</bodyText>
<subsectionHeader confidence="0.998297">
2.1 Handling unknown words
</subsectionHeader>
<bodyText confidence="0.9999163">
Currently most statistical machine translation sys-
tems are simply unable to handle unknown words.
There are two strategies that are generally employed
when an unknown source word is encountered. Ei-
ther the source word is simply omitted when pro-
ducing the translation, or alternatively it is passed
through untranslated, which is a reasonable strategy
if the unknown word happens to be a name (assum-
ing that no transliteration need be done). Neither of
these strategies is satisfying.
</bodyText>
<subsectionHeader confidence="0.999971">
2.2 Using paraphrases in SMT
</subsectionHeader>
<bodyText confidence="0.993567">
When a system is trained using 10,000 sentence
pairs (roughly 200,000 words) there will be a num-
ber of words and phrases in a test sentence which it
has not learned the translation of. For example, the
Spanish sentence
Es positivo llegar a un acuerdo sobre los
procedimientos, pero debemos encargar-
nos de que este sistema no sea susceptible
de ser usado como arma politica.
may translate as
It is good reach an agreement on proce-
dures, but we must encargarnos that this
system is not susceptible to be usado as
political weapon.
</bodyText>
<figure confidence="0.9954996">
100
40
30
20
90
80
70
60
50
10
0
unigrams
bigrams
trigrams
4-grams
</figure>
<page confidence="0.888607">
18
</page>
<figureCaption confidence="0.988684">
Figure 2: Using a bilingual parallel corpus to extract paraphrases
</figureCaption>
<bodyText confidence="0.996709133333333">
what is more, the relevant cost dynamic is completely under control
im Ÿbrigen ist die diesbezŸgliche kostenentwicklung völlig unter kontrolle
wir sind es den steuerzahlern schuldig die kosten
unter kontrolle
zu haben
we owe it to the taxpayers to keep
in check
the costs
The strategy that we employ for dealing with un-
known source language words is to substitute para-
phrases of those words, and then translate the para-
phrases. Table 1 gives examples of paraphrases and
their translations. If we had learned a translation of
garantizar we could translate it instead of encargar-
nos, and similarly for utilizado instead of usado.
</bodyText>
<sectionHeader confidence="0.962224" genericHeader="method">
3 Acquiring Paraphrases
</sectionHeader>
<bodyText confidence="0.999915517241379">
Paraphrases are alternative ways of expressing the
same information within one language. The auto-
matic generation of paraphrases has been the focus
of a significant amount of research lately. Many
methods for extracting paraphrases (Barzilay and
McKeown, 2001; Pang et al., 2003) make use of
monolingual parallel corpora, such as multiple trans-
lations of classic French novels into English, or the
multiple reference translations used by many auto-
matic evaluation metrics for machine translation.
Bannard and Callison-Burch (2005) use bilin-
gual parallel corpora to generate paraphrases. Para-
phrases are identified by pivoting through phrases in
another language. The foreign language translations
of an English phrase are identified, all occurrences
of those foreign phrases are found, and all English
phrases that they translate back to are treated as po-
tential paraphrases of the original English phrase.
Figure 2 illustrates how a German phrase can be
used as a point of identification for English para-
phrases in this way.
The method defined in Bannard and Callison-
Burch (2005) has several features that make it an
ideal candidate for incorporation into statistical ma-
chine translation system. Firstly, it can easily be ap-
plied to any language for which we have one or more
parallel corpora. Secondly, it defines a paraphrase
probability, p(e2|e1), which can be incorporated into
the probabilistic framework of SMT.
</bodyText>
<subsectionHeader confidence="0.999481">
3.1 Paraphrase probabilities
</subsectionHeader>
<bodyText confidence="0.999959">
The paraphrase probability p(e2|e1) is defined
in terms of two translation model probabilities:
p(f|e1), the probability that the original English
phrase e1 translates as a particular phrase f in the
other language, and p(e2|f), the probability that the
candidate paraphrase e2 translates as the foreign lan-
guage phrase. Since e1 can translate as multiple for-
eign language phrases, we marginalize f out:
</bodyText>
<equation confidence="0.944345">
�p(e2|e1) = p(f|e1)p(e2|f) (1)
f
</equation>
<bodyText confidence="0.999961666666667">
The translation model probabilities can be com-
puted using any standard formulation from phrase-
based machine translation. For example, p(e2|f)
can be calculated straightforwardly using maximum
likelihood estimation by counting how often the
phrases e and f were aligned in the parallel corpus:
</bodyText>
<equation confidence="0.990130666666667">
count(e2, f) (2)
p(e2|f) ≈
Eel count(e2, f)
</equation>
<bodyText confidence="0.99974925">
There is nothing that limits us to estimating para-
phrases probabilities from a single parallel corpus.
We can extend the definition of the paraphrase prob-
ability to include multiple corpora, as follows:
</bodyText>
<equation confidence="0.982798">
p(e2|e1) ≈ E,-EC Ef in ICI f |e1)p(e2|f) (3)
</equation>
<bodyText confidence="0.99981">
where c is a parallel corpus from a set of paral-
lel corpora C. Thus multiple corpora may be used
</bodyText>
<page confidence="0.995385">
19
</page>
<bodyText confidence="0.995561666666667">
by summing over all paraphrase probabilities calcu-
lated from a single corpus (as in Equation 1) and
normalized by the number of parallel corpora.
</bodyText>
<sectionHeader confidence="0.995324" genericHeader="method">
4 Experimental Design
</sectionHeader>
<bodyText confidence="0.999684">
We examined the application of paraphrases to deal
with unknown phrases when translating from Span-
ish and French into English. We used the pub-
licly available Europarl multilingual parallel corpus
(Koehn, 2005) to create six training corpora for the
two language pairs, and used the standard Europarl
development and test sets.
</bodyText>
<subsectionHeader confidence="0.949497">
4.1 Baseline
</subsectionHeader>
<bodyText confidence="0.9870445">
For a baseline system we produced a phrase-based
statistical machine translation system based on the
log-linear formulation described in (Och and Ney,
2002)
</bodyText>
<equation confidence="0.999066333333333">
e� = argmaxp(elf) (4)
e
amhm(e, f) (5)
</equation>
<bodyText confidence="0.999858478260869">
The baseline model had a total of eight feature
functions, hm(e, f): a language model probabil-
ity, a phrase translation probability, a reverse phrase
translation probability, lexical translation probabil-
ity, a reverse lexical translation probability, a word
penalty, a phrase penalty, and a distortion cost. To
set the weights, am, we performed minimum error
rate training (Och, 2003) on the development set us-
ing Bleu (Papineni et al., 2002) as the objective func-
tion.
The phrase translation probabilities were deter-
mined using maximum likelihood estimation over
phrases induced from word-level alignments pro-
duced by performing Giza++ training on each of the
three training corpora. We used the Pharaoh beam-
search decoder (Koehn, 2004) to produce the trans-
lations after all of the model parameters had been
set.
When the baseline system encountered unknown
words in the test set, its behavior was simply to re-
produce the foreign word in the translated output.
This is the default behavior for many systems, as
noted in Section 2.1.
</bodyText>
<subsectionHeader confidence="0.998632">
4.2 Translation with paraphrases
</subsectionHeader>
<bodyText confidence="0.995558925925926">
We extracted all source language (Spanish and
French) phrases up to length 10 from the test and
development sets which did not have translations in
phrase tables that were generated for the three train-
ing corpora. For each of these phrases we gener-
ated a list of paraphrases using all of the parallel cor-
pora from Europarl aside from the Spanish-English
and French-English corpora. We used bitexts be-
tween Spanish and Danish, Dutch, Finnish, French,
German, Italian, Portuguese, and Swedish to gener-
ate our Spanish paraphrases, and did similarly for
the French paraphrases. We manage the parallel
corpora with a suffix array -based data structure
(Callison-Burch et al., 2005). We calculated para-
phrase probabilities using the Bannard and Callison-
Burch (2005) method, summarized in Equation 3.
Source language phrases that included names and
numbers were not paraphrased.
For each paraphrase that had translations in the
phrase table, we added additional entries in the
phrase table containing the original phrase and the
paraphrase’s translations. We augmented the base-
line model by incorporating the paraphrase probabil-
ity into an additional feature function which assigns
values as follows:
p(f2|f1) If phrase table entry (e, f1)
is generated from (e, f2)
</bodyText>
<sectionHeader confidence="0.980401" genericHeader="method">
1 Otherwise
</sectionHeader>
<bodyText confidence="0.99997975">
Just as we did in the baseline system, we performed
minimum error rate training to set the weights of the
nine feature functions in our translation model that
exploits paraphrases.
We tested the usefulness of the paraphrase fea-
ture function by performing an additional experi-
ment where the phrase table was expanded but the
paraphrase probability was omitted.
</bodyText>
<subsectionHeader confidence="0.982957">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999971285714286">
We evaluated the efficacy of using paraphrases in
three ways: by calculating the Bleu score for the
translated output, by measuring the increase in cov-
erage when including paraphrases, and through a tar-
geted manual evaluation of the phrasal translations
of unseen phrases to determine how many of the
newly covered phrases were accurately translated.
</bodyText>
<equation confidence="0.835562333333333">
= arg max
e
M
E
m=1
h(e,f1) = I
</equation>
<page confidence="0.847677">
20
</page>
<figureCaption confidence="0.970275">
Figure 3: Test sentences and reference translations
</figureCaption>
<bodyText confidence="0.996561833333333">
were manually word-aligned. This allowed us to
equate unseen phrases with their corresponding En-
glish phrase. In this case enumeradas with listed.
Although Bleu is currently the standard metric for
MT evaluation, we believe that it may not meaning-
fully measure translation improvements in our setup.
By substituting a paraphrase for an unknown source
phrase there is a strong chance that its translation
may also be a paraphrase of the equivalent target
language phrase. Bleu relies on exact matches of
n-grams in a reference translation. Thus if our trans-
lation is a paraphrase of the reference, Bleu will fail
to score it correctly.
Because Bleu is potentially insensitive to the type
of changes that we were making to the translations,
we additionally performed a focused manual evalu-
ation (Callison-Burch et al., 2006). To do this, had
bilingual speakers create word-level alignments for
the first 150 and 250 sentence in the Spanish-English
and French-English test corpora, as shown in Figure
3. We were able to use these alignments to extract
the translations of the Spanish and French words that
we were applying our paraphrase method to.
Knowing this correspondence between foreign
phrases and their English counterparts allowed us to
directly analyze whether translations that were be-
ing produced from paraphrases remained faithful to
the meaning of the reference translation. When pro-
The article combats discrimination and inequality
in the treatment of citizens for the reasons listed
therein.
The article combats discrimination and the dif-
ferent treatment of citizens for the reasons men-
tioned in the same.
The article fights against uneven and the treatment
of citizens for the reasons enshrined in the same.
The article is countering discrimination and the
unequal treatment of citizens for the reasons that
in the same.
Figure 4: Judges were asked whether the highlighted
phrase retained the same meaning as the highlighted
phrase in the reference translation (top)
ducing our translations using the Pharaoh decoder
we employed its “trace” facility, which tells which
source sentence span each target phrase was derived
from. This allowed us to identify which elements
in the machine translated output corresponded to the
paraphrased foreign phrase. We asked a monolin-
gual judge whether the phrases in the machine trans-
lated output had the same meaning as of the refer-
ence phrase. This is illustrated in Figure 4.
In addition to judging the accuracy of 100 phrases
for each of the translated sets, we measured how
much our paraphrase method increased the cover-
age of the translation system. Because we focus
on words that the system was previously unable to
translate, the increase in coverage and the transla-
tion quality of the newly covered phrases are the
two most relevant indicators as to the efficacy of the
method.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999969090909091">
We produced translations under five conditions for
each of our training corpora: a set of baseline
translations without any additional entries in the
phrase table, a condition where we added the trans-
lations of paraphrases for unseen source words along
with paraphrase probabilities, a condition where we
added the translations of paraphrases of multi-word
phrases along with paraphrase probabilities, and two
additional conditions where we added the transla-
tions of paraphrases of single and multi-word para-
phrase without paraphrase probabilities.
</bodyText>
<figure confidence="0.99099025">
The
article
combats
discrimination
and
inequality
in
the
treatment
of
citizens
for
the
reasons
therein.
listed
El
art’culo
combate
la
Alignment Tool
discriminaci—n
y
el
trato
desigual
de
los
ciudadanos
por
las
causas
enumeradas
en
el
mismo.
</figure>
<page confidence="0.995613">
21
</page>
<table confidence="0.9995684">
Spanish-English French-English
Corpus size 10k 20k 40k 80k 160k 320k 10k 20k 40k 80k 160k 320k
Baseline 22.6 25.0 26.5 26.5 28.7 30.0 21.9 24.3 26.3 27.8 28.8 29.5
Single word 23.1 25.2 26.6 28.0 29.0 30.0 22.7 24.2 26.9 27.7 28.9 29.8
Multi-word 23.3 26.0 27.2 28.0 28.8 29.7 23.7 25.1 27.1 28.5 29.1 29.8
</table>
<tableCaption confidence="0.983187">
Table 2: Bleu scores for the various training corpora, including baseline results without paraphrasing, results
for only paraphrasing unknown words, and results for paraphrasing any unseen phrase. Corpus size is
measured in sentences.
</tableCaption>
<table confidence="0.999740666666667">
Corpus size 10k 20k 40k 80k 160k 320k 10k 20k 40k 80k 160k 320k
Single w/o-ff 23.0 25.1 26.7 28.0 29.0 29.9 22.5 24.1 26.0 27.6 28.8 29.6
Multi w/o-ff 20.6 22.6 21.9 24.0 25.4 27.5 19.7 22.1 24.3 25.6 26.0 28.1
</table>
<tableCaption confidence="0.99956">
Table 3: Bleu scores for the various training corpora, when the paraphrase feature function is not included
</tableCaption>
<subsectionHeader confidence="0.995682">
5.1 Bleu scores
</subsectionHeader>
<bodyText confidence="0.999965115384616">
Table 2 gives the Bleu scores for each of these con-
ditions. We were able to measure a translation im-
provement for all sizes of training corpora, under
both the single word and multi-word conditions, ex-
cept for the largest Spanish-English corpus. For the
single word condition, it would have been surprising
if we had seen a decrease in Bleu score. Because we
are translating words that were previously untrans-
latable it would be unlikely that we could do any
worse. In the worst case we would be replacing one
word that did not occur in the reference translation
with another, and thus have no effect on Bleu.
More interesting is the fact that by paraphrasing
unseen multi-word units we get an increase in qual-
ity above and beyond the single word paraphrases.
These multi-word units may not have been observed
in the training data as a unit, but each of the compo-
nent words may have been. In this case translating
a paraphrase would not be guaranteed to received
an improved or identical Bleu score, as in the single
word case. Thus the improved Bleu score is notable.
Table 3 shows that incorporating the paraphrase
probability into the model’s feature functions plays a
critical role. Without it, the multi-word paraphrases
harm translation performance when compared to the
baseline.
</bodyText>
<subsectionHeader confidence="0.999052">
5.2 Manual evaluation
</subsectionHeader>
<bodyText confidence="0.999992483870968">
We performed a manual evaluation by judging the
accuracy of phrases for 100 paraphrased translations
from each of the sets using the manual word align-
ments.1 Table 4 gives the percentage of time that
each of the translations of paraphrases were judged
to have the same meaning as the equivalent target
phrase. In the case of the translations of single word
paraphrases for the Spanish accuracy ranged from
just below 50% to just below 70%. This number
is impressive in light of the fact that none of those
items are correctly translated in the baseline model,
which simply inserts the foreign language word. As
with the Bleu scores, the translations of multi-word
paraphrases were judged to be more accurate than
the translations of single word paraphrases.
In performing the manual evaluation we were ad-
ditionally able to determine how often Bleu was ca-
pable of measuring an actual improvement in trans-
lation. For those items judged to have the same
meaning as the gold standard phrases we could
track how many would have contributed to a higher
Bleu score (that is, which of them were exactly
the same as the reference translation phrase, or had
some words in common with the reference trans-
lation phrase). By counting how often a correct
phrase would have contributed to an increased Bleu
score, and how often it would fail to increase the
Bleu score we were able to determine with what fre-
quency Bleu was sensitive to our improvements. We
found that Bleu was insensitive to our translation im-
provements between 60-75% of the time, thus re-
</bodyText>
<footnote confidence="0.9925605">
1Note that for the larger training corpora fewer than 100
paraphrases occurred in the first 150 and 250 sentence pairs.
</footnote>
<page confidence="0.988315">
22
</page>
<table confidence="0.9992635">
Spanish-English French-English
Corpus size 10k 20k 40k 80k 160k 320k 10k 20k 40k 80k 160k 320k
Single word 48% 53% 57% 67%* 33%* 50%* 54% 49% 45% 50% 39%* 21%*
Multi-word 64% 65% 66% 71% 76%* 71%* 60% 67% 63% 58% 65% 42%*
</table>
<tableCaption confidence="0.893746666666667">
Table 4: Percent of time that the translation of a paraphrase was judged to retain the same meaning as the
corresponding phrase in the gold standard. Starred items had fewer than 100 judgments and should not be
taken as reliable estimates.
</tableCaption>
<table confidence="0.999870714285714">
Size 1-gram 2-gram 3-gram 4-gram
10k 48% 25% 10% 3%
20k 60% 35% 15% 6%
40k 71% 45% 22% 9%
80k 80% 55% 29% 12%
160k 86% 64% 37% 17%
320k 91% 71% 45% 22%
</table>
<tableCaption confidence="0.96987">
Table 5: The percent of the unique test set phrases
which have translations in each of the Spanish-
English training corpora prior to paraphrasing
</tableCaption>
<table confidence="0.999846714285714">
Size 1-gram 2-gram 3-gram 4-gram
10k 90% 67% 37% 16%
20k 90% 69% 39% 17%
40k 91% 71% 41% 18%
80k 92% 73% 44% 20%
160k 92% 75% 46% 22%
320k 93% 77% 50% 25%
</table>
<tableCaption confidence="0.988676">
Table 6: The percent of the unique test set phrases
</tableCaption>
<bodyText confidence="0.899313">
which have translations in each of the Spanish-
English training corpora after paraphrasing
inforcing our belief that it is not an appropriate mea-
sure for translation improvements of this sort.
</bodyText>
<subsectionHeader confidence="0.981133">
5.3 Increase in coverage
</subsectionHeader>
<bodyText confidence="0.999927916666667">
As illustrated in Figure 1, translation models suffer
from sparse data. When only a very small paral-
lel corpus is available for training, translations are
learned for very few of the unique phrases in a test
set. If we exclude 451 words worth of names, num-
bers, and foreign language text in 2,000 sentences
that comprise the Spanish portion of the Europarl
test set, then the number of unique n-grams in text
are: 7,331 unigrams, 28,890 bigrams, 44,194 tri-
grams, and 48,259 4-grams. Table 5 gives the per-
centage of these which have translations in each of
the three training corpora, if we do not use para-
phrasing.
In contrast after expanding the phrase table using
the translations of paraphrases, the coverage of the
unique test set phrases goes up dramatically (shown
in Table 6). For the first training corpus with 10,000
sentence pairs and roughly 200,000 words of text in
each language, the coverage goes up from less than
50% of the vocabulary items being covered to 90%.
The coverage of unique 4-grams jumps from 3% to
16% – a level reached only after observing more
than 100,000 sentence pairs, or roughly three mil-
lion words of text, without using paraphrases.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999893954545454">
Previous research on trying to overcome data spar-
sity issues in statistical machine translation has
largely focused on introducing morphological anal-
ysis as a way of reducing the number of types ob-
served in a training text. For example, Nissen and
Ney (2004) apply morphological analyzers to En-
glish and German and are able to reduce the amount
of training data needed to reach a certain level
of translation quality. Goldwater and McClosky
(2005) find that stemming Czech and using lemmas
improves the word-to-word correspondences when
training Czech-English alignment models. Koehn
and Knight (2003) show how monolingual texts and
parallel corpora can be used to figure out appropriate
places to split German compounds.
Still other approaches focus on ways of acquiring
data. Resnik and Smith (2003) develop a method
for gathering parallel corpora from the web. Oard
et al. (2003) describe various methods employed
for quickly gathering resources to create a machine
translation system for a language with no initial re-
sources.
</bodyText>
<page confidence="0.99748">
23
</page>
<sectionHeader confidence="0.999257" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9998644">
In this paper we have shown that significant gains in
coverage and translation quality can be had by inte-
grating paraphrases into statistical machine transla-
tion. In effect, paraphrases introduce some amount
ofgeneralization into statistical machine translation.
Whereas before we relied on having observed a par-
ticular word or phrase in the training set in order to
produce a translation of it, we are no longer tied to
having seen every word in advance. We can exploit
knowledge that is external to the translation model
about what words have similar meanings and use
that in the process of translation. This method is
particularly pertinent to small data conditions, which
are plagued by sparse data problems.
In future work, we plan to determine how much
data is required to learn useful paraphrases. The sce-
nario described in this paper was very favorable to
creating high quality paraphrases. The large number
of parallel corpora between Spanish and the other
languages present in the Europarl corpus allowed
us to generate high quality, in domain data. While
this is a realistic scenario, in that many new official
languages have been added to the European Union,
some of which do not yet have extensive parallel cor-
pora, we realize that this may be a slightly idealized
scenario.
Finally, we plan to formalize our targeted manual
evaluation method, in the hopes of creating a eval-
uation methodology for machine translation that is
more thorough and elucidating than Bleu.
</bodyText>
<sectionHeader confidence="0.998367" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995768">
Thank you to Alexandra Birch and Stephanie Van-
damme for creating the word alignments.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857155172414">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL-2005.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In ACL-2001.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311, June.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings ofACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation. In Proceedings of EACL.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proceedings ofEMNLP.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of EACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings ofHLT/NAACL.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings ofAMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings ofMT-Summit.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings ofEMNLP.
Sonja Nissen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic analysis. Computational Linguistics,
30(2):181–204.
Doug Oard, David Doermann, Bonnie Dorr, Daqing He,
Phillip Resnik, William Byrne, Sanjeeve Khudanpur,
David Yarowsky, Anton Leuski, Philipp Koehn, and
Kevin Knight. 2003. Desperately seeking Cebuano.
In Proceedings ofHLT-NAACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings ofACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings ofACL.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings ofHLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings ofACL.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349–
380, September.
</reference>
<page confidence="0.999178">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.932425">
<title confidence="0.999823">Improved Statistical Machine Translation Using Paraphrases</title>
<author confidence="0.999624">Chris Callison-Burch Philipp Koehn Miles</author>
<affiliation confidence="0.989412333333333">School of University of 2 Buccleuch</affiliation>
<address confidence="0.997629">Edinburgh, EH8</address>
<email confidence="0.998602">callison-burch@ed.ac.uk</email>
<abstract confidence="0.99831025">Parallel corpora are crucial for training SMT systems. However, for many language pairs they are available only in very limited quantities. For these language pairs a huge portion of phrases encountered at run-time will be unknown. We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. Our results show that augmenting a stateof-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In ACL-2005.</booktitle>
<contexts>
<context position="7799" citStr="Bannard and Callison-Burch (2005)" startWordPosition="1229" endWordPosition="1232">slate it instead of encargarnos, and similarly for utilizado instead of usado. 3 Acquiring Paraphrases Paraphrases are alternative ways of expressing the same information within one language. The automatic generation of paraphrases has been the focus of a significant amount of research lately. Many methods for extracting paraphrases (Barzilay and McKeown, 2001; Pang et al., 2003) make use of monolingual parallel corpora, such as multiple translations of classic French novels into English, or the multiple reference translations used by many automatic evaluation metrics for machine translation. Bannard and Callison-Burch (2005) use bilingual parallel corpora to generate paraphrases. Paraphrases are identified by pivoting through phrases in another language. The foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate back to are treated as potential paraphrases of the original English phrase. Figure 2 illustrates how a German phrase can be used as a point of identification for English paraphrases in this way. The method defined in Bannard and CallisonBurch (2005) has several features that make it an ideal candid</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In ACL-2001.</booktitle>
<contexts>
<context position="7528" citStr="Barzilay and McKeown, 2001" startWordPosition="1189" endWordPosition="1192">that we employ for dealing with unknown source language words is to substitute paraphrases of those words, and then translate the paraphrases. Table 1 gives examples of paraphrases and their translations. If we had learned a translation of garantizar we could translate it instead of encargarnos, and similarly for utilizado instead of usado. 3 Acquiring Paraphrases Paraphrases are alternative ways of expressing the same information within one language. The automatic generation of paraphrases has been the focus of a significant amount of research lately. Many methods for extracting paraphrases (Barzilay and McKeown, 2001; Pang et al., 2003) make use of monolingual parallel corpora, such as multiple translations of classic French novels into English, or the multiple reference translations used by many automatic evaluation metrics for machine translation. Bannard and Callison-Burch (2005) use bilingual parallel corpora to generate paraphrases. Paraphrases are identified by pivoting through phrases in another language. The foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate back to are treated as potent</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2001. Extracting paraphrases from a parallel corpus. In ACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1080" citStr="Brown et al., 1993" startWordPosition="161" endWordPosition="164">ow how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. Our results show that augmenting a stateof-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches. 1 Introduction As with many other statistical natural language processing tasks, statistical machine translation (Brown et al., 1993) produces high quality results when ample training data is available. This is problematic for so called “low density” language pairs which do not have very large parallel corpora. For example, when words occur infrequently in a parallel corpus parameter estimates for word-level alignments can be inaccurate, which can in turn lead to inaccurate phrase translations. Limited amounts of training data can further lead to a problem of low coverage in that many phrases encountered at run-time are not ob17 served in the training data and therefore their translations will not be learned. Here we addres</context>
<context position="3643" citStr="Brown et al., 1993" startWordPosition="568" endWordPosition="571">rican Chapter of the ACL, pages 17–24, New York, June 2006. c�2006 Association for Computational Linguistics Test Set Items with Translations (%) 10000 100000 1e+06 1e+07 Training Corpus Size (num words) Figure 1: Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation, phrase-based machine translation does away with many of the problems associated with the original word-based formulation of statistical machine translation (Brown et al., 1993). For instance, with multiword units less re-ordering needs to occur since local dependencies are frequently captured. For example, common adjective-noun alternations are memorized. However, since this linguistic information is not explicitly and generatively encoded in the model, unseen adjective noun pairs may still be handled incorrectly. Thus, having observed phrases in the past dramatically increases the chances that they will be translated correctly in the future. However, for any given test set, a huge amount of training data has to be observed before translations are learned for a reas</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="12286" citStr="Callison-Burch et al., 2005" startWordPosition="1942" endWordPosition="1945">ench) phrases up to length 10 from the test and development sets which did not have translations in phrase tables that were generated for the three training corpora. For each of these phrases we generated a list of paraphrases using all of the parallel corpora from Europarl aside from the Spanish-English and French-English corpora. We used bitexts between Spanish and Danish, Dutch, Finnish, French, German, Italian, Portuguese, and Swedish to generate our Spanish paraphrases, and did similarly for the French paraphrases. We manage the parallel corpora with a suffix array -based data structure (Callison-Burch et al., 2005). We calculated paraphrase probabilities using the Bannard and CallisonBurch (2005) method, summarized in Equation 3. Source language phrases that included names and numbers were not paraphrased. For each paraphrase that had translations in the phrase table, we added additional entries in the phrase table containing the original phrase and the paraphrase’s translations. We augmented the baseline model by incorporating the paraphrase probability into an additional feature function which assigns values as follows: p(f2|f1) If phrase table entry (e, f1) is generated from (e, f2) 1 Otherwise Just </context>
</contexts>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Chris Callison-Burch, Colin Bannard, and Josh Schroeder. 2005. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of bleu in machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="14516" citStr="Callison-Burch et al., 2006" startWordPosition="2297" endWordPosition="2300">for MT evaluation, we believe that it may not meaningfully measure translation improvements in our setup. By substituting a paraphrase for an unknown source phrase there is a strong chance that its translation may also be a paraphrase of the equivalent target language phrase. Bleu relies on exact matches of n-grams in a reference translation. Thus if our translation is a paraphrase of the reference, Bleu will fail to score it correctly. Because Bleu is potentially insensitive to the type of changes that we were making to the translations, we additionally performed a focused manual evaluation (Callison-Burch et al., 2006). To do this, had bilingual speakers create word-level alignments for the first 150 and 250 sentence in the Spanish-English and French-English test corpora, as shown in Figure 3. We were able to use these alignments to extract the translations of the Spanish and French words that we were applying our paraphrase method to. Knowing this correspondence between foreign phrases and their English counterparts allowed us to directly analyze whether translations that were being produced from paraphrases remained faithful to the meaning of the reference translation. When proThe article combats discrimi</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of bleu in machine translation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="24000" citStr="Goldwater and McClosky (2005)" startWordPosition="3899" endWordPosition="3902">grams jumps from 3% to 16% – a level reached only after observing more than 100,000 sentence pairs, or roughly three million words of text, without using paraphrases. 6 Related Work Previous research on trying to overcome data sparsity issues in statistical machine translation has largely focused on introducing morphological analysis as a way of reducing the number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 23 7 Discussion I</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="24154" citStr="Koehn and Knight (2003)" startWordPosition="3919" endWordPosition="3922">rases. 6 Related Work Previous research on trying to overcome data sparsity issues in statistical machine translation has largely focused on introducing morphological analysis as a way of reducing the number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 23 7 Discussion In this paper we have shown that significant gains in coverage and translation quality can be had by integrating paraphrases into statistical machine trans</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="2927" citStr="Koehn et al., 2003" startWordPosition="460" endWordPosition="463"> marked improvement in coverage and translation quality, especially in the case of unknown words which to date have been left untranslated. • Argue that while we observe an improvement in Bleu score, this metric is particularly poorly suited to measuring the sort of improvements that we achieve. • Present an alternative methodology for targeted manual evaluation that may be useful in other research projects. 2 The Problem of Coverage in SMT Statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004). By Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 17–24, New York, June 2006. c�2006 Association for Computational Linguistics Test Set Items with Translations (%) 10000 100000 1e+06 1e+07 Training Corpus Size (num words) Figure 1: Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation, phrase-based machine translation does away with many of the </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="11280" citStr="Koehn, 2004" startWordPosition="1779" endWordPosition="1780">bility, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, am, we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default behavior for many systems, as noted in Section 2.1. 4.2 Translation with paraphrases We extracted all source language (Spanish and French) phrases up to length 10 from the test and development sets which did not have translations in phrase tables that were generated for the three training corpora. For each of these phrases we generated a list of paraphr</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofMT-Summit.</booktitle>
<contexts>
<context position="10216" citStr="Koehn, 2005" startWordPosition="1614" endWordPosition="1615">end the definition of the paraphrase probability to include multiple corpora, as follows: p(e2|e1) ≈ E,-EC Ef in ICI f |e1)p(e2|f) (3) where c is a parallel corpus from a set of parallel corpora C. Thus multiple corpora may be used 19 by summing over all paraphrase probabilities calculated from a single corpus (as in Equation 1) and normalized by the number of parallel corpora. 4 Experimental Design We examined the application of paraphrases to deal with unknown phrases when translating from Spanish and French into English. We used the publicly available Europarl multilingual parallel corpus (Koehn, 2005) to create six training corpora for the two language pairs, and used the standard Europarl development and test sets. 4.1 Baseline For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) e� = argmaxp(elf) (4) e amhm(e, f) (5) The baseline model had a total of eight feature functions, hm(e, f): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phra</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. A parallel corpus for statistical machine translation. In Proceedings ofMT-Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="2907" citStr="Marcu and Wong, 2002" startWordPosition="456" endWordPosition="459">raphrases we achieve a marked improvement in coverage and translation quality, especially in the case of unknown words which to date have been left untranslated. • Argue that while we observe an improvement in Bleu score, this metric is particularly poorly suited to measuring the sort of improvements that we achieve. • Present an alternative methodology for targeted manual evaluation that may be useful in other research projects. 2 The Problem of Coverage in SMT Statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004). By Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 17–24, New York, June 2006. c�2006 Association for Computational Linguistics Test Set Items with Translations (%) 10000 100000 1e+06 1e+07 Training Corpus Size (num words) Figure 1: Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation, phrase-based machine translation does aw</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nissen</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation with scarce resources using morpho-syntatic analysis.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="23811" citStr="Nissen and Ney (2004)" startWordPosition="3868" endWordPosition="3871">00 sentence pairs and roughly 200,000 words of text in each language, the coverage goes up from less than 50% of the vocabulary items being covered to 90%. The coverage of unique 4-grams jumps from 3% to 16% – a level reached only after observing more than 100,000 sentence pairs, or roughly three million words of text, without using paraphrases. 6 Related Work Previous research on trying to overcome data sparsity issues in statistical machine translation has largely focused on introducing morphological analysis as a way of reducing the number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the</context>
</contexts>
<marker>Nissen, Ney, 2004</marker>
<rawString>Sonja Nissen and Hermann Ney. 2004. Statistical machine translation with scarce resources using morpho-syntatic analysis. Computational Linguistics, 30(2):181–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Oard</author>
<author>David Doermann</author>
<author>Bonnie Dorr</author>
<author>Daqing He</author>
<author>Phillip Resnik</author>
<author>William Byrne</author>
<author>Sanjeeve Khudanpur</author>
<author>David Yarowsky</author>
<author>Anton Leuski</author>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Desperately seeking Cebuano.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="24435" citStr="Oard et al. (2003)" startWordPosition="3965" endWordPosition="3968"> morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 23 7 Discussion In this paper we have shown that significant gains in coverage and translation quality can be had by integrating paraphrases into statistical machine translation. In effect, paraphrases introduce some amount ofgeneralization into statistical machine translation. Whereas before we relied on having observed a particular word or phrase in the training set in order to produce a translation of it, we are no longer tied to having seen eve</context>
</contexts>
<marker>Oard, Doermann, Dorr, He, Resnik, Byrne, Khudanpur, Yarowsky, Leuski, Koehn, Knight, 2003</marker>
<rawString>Doug Oard, David Doermann, Bonnie Dorr, Daqing He, Phillip Resnik, William Byrne, Sanjeeve Khudanpur, David Yarowsky, Anton Leuski, Philipp Koehn, and Kevin Knight. 2003. Desperately seeking Cebuano. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10503" citStr="Och and Ney, 2002" startWordPosition="1656" endWordPosition="1659">alculated from a single corpus (as in Equation 1) and normalized by the number of parallel corpora. 4 Experimental Design We examined the application of paraphrases to deal with unknown phrases when translating from Spanish and French into English. We used the publicly available Europarl multilingual parallel corpus (Koehn, 2005) to create six training corpora for the two language pairs, and used the standard Europarl development and test sets. 4.1 Baseline For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) e� = argmaxp(elf) (4) e amhm(e, f) (5) The baseline model had a total of eight feature functions, hm(e, f): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, am, we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation o</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation. Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="2947" citStr="Och and Ney, 2004" startWordPosition="464" endWordPosition="467">in coverage and translation quality, especially in the case of unknown words which to date have been left untranslated. • Argue that while we observe an improvement in Bleu score, this metric is particularly poorly suited to measuring the sort of improvements that we achieve. • Present an alternative methodology for targeted manual evaluation that may be useful in other research projects. 2 The Problem of Coverage in SMT Statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004). By Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 17–24, New York, June 2006. c�2006 Association for Computational Linguistics Test Set Items with Translations (%) 10000 100000 1e+06 1e+07 Training Corpus Size (num words) Figure 1: Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation, phrase-based machine translation does away with many of the problems associated </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10927" citStr="Och, 2003" startWordPosition="1724" endWordPosition="1725">t and test sets. 4.1 Baseline For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) e� = argmaxp(elf) (4) e amhm(e, f) (5) The baseline model had a total of eight feature functions, hm(e, f): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, am, we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default be</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="7548" citStr="Pang et al., 2003" startWordPosition="1193" endWordPosition="1196">ith unknown source language words is to substitute paraphrases of those words, and then translate the paraphrases. Table 1 gives examples of paraphrases and their translations. If we had learned a translation of garantizar we could translate it instead of encargarnos, and similarly for utilizado instead of usado. 3 Acquiring Paraphrases Paraphrases are alternative ways of expressing the same information within one language. The automatic generation of paraphrases has been the focus of a significant amount of research lately. Many methods for extracting paraphrases (Barzilay and McKeown, 2001; Pang et al., 2003) make use of monolingual parallel corpora, such as multiple translations of classic French novels into English, or the multiple reference translations used by many automatic evaluation metrics for machine translation. Bannard and Callison-Burch (2005) use bilingual parallel corpora to generate paraphrases. Paraphrases are identified by pivoting through phrases in another language. The foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate back to are treated as potential paraphrases of t</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10985" citStr="Papineni et al., 2002" startWordPosition="1733" endWordPosition="1736">ystem we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) e� = argmaxp(elf) (4) e amhm(e, f) (5) The baseline model had a total of eight feature functions, hm(e, f): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, am, we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default behavior for many systems, as noted in Section 2.1. 4.2 Tran</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<pages>380</pages>
<contexts>
<context position="24354" citStr="Resnik and Smith (2003)" startWordPosition="3951" endWordPosition="3954"> number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 23 7 Discussion In this paper we have shown that significant gains in coverage and translation quality can be had by integrating paraphrases into statistical machine translation. In effect, paraphrases introduce some amount ofgeneralization into statistical machine translation. Whereas before we relied on having observed a particular word or phrase in the training set </context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349– 380, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>