<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000428">
<title confidence="0.998822">
Detecting Novelty in the context of Progressive Summarization
</title>
<author confidence="0.991161">
Praveen Bysani
</author>
<affiliation confidence="0.815236">
Language Technologies Research Center
IIIT Hyderabad
</affiliation>
<email confidence="0.985547">
lvsnpraveen@research.iiit.ac.in
</email>
<sectionHeader confidence="0.998512" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994831875">
A Progressive summary helps a user to moni-
tor changes in evolving news topics over a pe-
riod of time. Detecting novel information is
the essential part of progressive summariza-
tion that differentiates it from normal multi
document summarization. In this work, we
explore the possibility of detecting novelty at
various stages of summarization. New scoring
features, Re-ranking criterions and filtering
strategies are proposed to identify “relevant
novel” information. We compare these tech-
niques using an automated evaluation frame-
work ROUGE, and determine the best. Over-
all, our summarizer is able to perform on par
with existing prime methods in progressive
summarization.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959">
Summarization is the process of condensing text to
its most essential facts. Summarization is challeng-
ing for its associated cognitive task and interesting
because of its practical usage. It has been success-
fully applied for text content such as news articles 1,
scientific papers (Teufel and Moens, 2002) that fol-
low a discourse structure. Update summarization is
an emerging area with in summarization, acquiring
significant research focus during recent times. The
task was introduced at DUC 20072 and continued
during TAC 2008, 20093. We refer to update sum-
mariztion as “Progressive Summarization” in rest of
</bodyText>
<footnote confidence="0.999945">
1http://newsblaster.cs.columbia.edu/
2http://duc.nist.gov/duc2007/tasks.html
3http://www.nist.gov/tac
</footnote>
<page confidence="0.997014">
13
</page>
<bodyText confidence="0.997737787878788">
this paper, as summaries are produced periodically
in a progressive manner and the latter title is more
apt to the task. Progressive summaries contain infor-
mation which is both relevant and novel, since they
are produced under the assumption that user has al-
ready read some previous documents/articles on the
topic. Such summaries are extremely useful in track-
ing news stories, tracing new product reviews etc.
Unlike dynamic summarization (Jatowt, 2004)
where a single summary transforms periodically, re-
flecting changes in source text, Progressive summa-
rizer produce multiple summaries at specific time
intervals updating user knowledge. Temporal Sum-
marization (Allan et al., 2001) generate summaries,
similar to progressive summaries by ranking sen-
tences as combination of relevant and new scores.
In this work, summaries are produced not just by
reforming ranking scheme but also altering scoring
and extraction stages of summarization.
Progressive summarization requires differentiat-
ing Relevant and Novel Vs Non-Relevant and Novel
Vs Relevant and Redundant information. Such dis-
crimination is feasible only with efficient Novelty
detection techniques. We define Novelty detection
as identifying relevant sentences containing new in-
formation. This task shares similarity with TREC
Novelty Track 4, that is designed to investigate sys-
tems abilities to locate sentences containing relevant
and/or new information given the topic and a set of
relevant documents ordered by date. A progressive
summarizer needs to identify, score and then finally
rank “relevant novel” sentences to produce a sum-
mary.
</bodyText>
<footnote confidence="0.982399">
4http://trec.nist.gov/data/novelty.html
</footnote>
<note confidence="0.446755">
Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 13–18,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999505277777778">
Previous approaches to Novelty detection at
TREC (Soboroff, 2004) include cosine filter-
ing (Abdul-Jaleel et al., 2004), where a sentence
having maximum cosine similarity value with pre-
vious set of sentences, lower than a preset thresh-
old is considered novel. Alternatively, (Schiffman
and McKeown, 2004) considered previously unseen
words as an evidence of Novelty. (Eichmannac et
al., 2004) expanded all noun phrases in a sentence
using wordnet and used corresponding sysnsets for
novelty comparisions.
Our work targets exploring the effect of detect-
ing novelty at different stages of summarization on
the quality of progressive summaries. Unlike most
of the previous work (Li et al., 2009) (Zhang et
al., 2009) in progressive summarization, we em-
ploy multiple novelty detection techniques at differ-
ent stages and analyze them all to find the best.
</bodyText>
<sectionHeader confidence="0.977685" genericHeader="method">
2 Document Summarization
</sectionHeader>
<bodyText confidence="0.9998236">
The Focus of this paper is only on extrac-
tive summarization, henceforth term summariza-
tion/summarizer implies sentence extractive multi
document summarization. Our Summarizer has 4
major stages as shown in Figure 1,
</bodyText>
<figureCaption confidence="0.997106">
Figure 1: Stages in a Multi Document Summarizer
</figureCaption>
<bodyText confidence="0.999856214285714">
Every news article/document is cleaned from
news heads, HTML tags and split into sentences dur-
ing Pre-processing stage. At scoring, several sen-
tence scoring features assign scores for each sen-
tence, reflecting its topic relevance. Feature scores
are combined to get a final rank for the sentence
in ranking stage. Rank of a sentence is predicted
from regression model built on feature vectors of
sentences in the training data using support vector
machine as explained in (Schilder and Kondadandi,
2008). Finally during summary extraction, a sub-
set of ranked sentences are selected to produce sum-
mary after a redundancy check to filter duplicate
sentences.
</bodyText>
<subsectionHeader confidence="0.985587">
2.1 Normal Summarizers
</subsectionHeader>
<bodyText confidence="0.992487928571428">
Two normal summarizers (DocSumm, TacBaseline)
are developed in a similar fashion described in
Figure 1.
DocSumm produce summaries with two scoring
features, Document Frequency Score (DF) (Schilder
and Kondadandi, 2008) and Sentence Position
(SP). DocSumm serves as a baseline to depict the
effect of novelty detection techniques described
in Section 3 on normal summarizers. Document
frequency (DF), of a word (w) in the document set
(docs) is defined as ratio of number of documents in
which it occured to the total number of documents.
Normalized DF score of all content words in a
sentence is considered its feature score.
</bodyText>
<equation confidence="0.987788">
DFdo.(w) = JJdJl c d}
E
</equation>
<bodyText confidence="0.9996805">
Sentence Position (SP) assigns positional index (n)
of a sentence (sn) in the document (d) it occurs as
its feature score. Training model will learn the opti-
mum sentence position for the dataset.
</bodyText>
<equation confidence="0.865609">
SP(snd) = n
</equation>
<bodyText confidence="0.9996986">
TacBaseline is a conventional baseline at TAC, that
creates a n word length summary from first n words
of the most recent article. It provides a lower bound
on what can be achieved with automatic multi docu-
ment summarizers.
</bodyText>
<sectionHeader confidence="0.996027" genericHeader="method">
3 Novelty Detection
</sectionHeader>
<bodyText confidence="0.999829833333333">
Progressive summaries are generated at regular time
intervals to update user knowledge on a particular
news topic. Imagine a set of articles published on
a evolving news topic over time period T, with td
being publishing timestamp of article d. All the arti-
cles published from time 0 to time t are assumed to
</bodyText>
<page confidence="0.994392">
14
</page>
<bodyText confidence="0.997457">
have been read previously, hence prior knowledge,
pdocs. Articles published in the interval t to T that
contain new information are considered ndocs.
</bodyText>
<equation confidence="0.9481985">
ndocs = {d : td &gt; t}
pdocs = {d : td &lt;= t}
</equation>
<bodyText confidence="0.999193">
Progressive summarization needs a novelty detec-
tion technique to identify sentences that contain rel-
evant new information. The task of detecting nov-
elty can be carried out at 3 stages of summarization
shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999817">
3.1 At Scoring
</subsectionHeader>
<bodyText confidence="0.9999814">
New Sentence scoring features are devised to
capture sentence novelty along with its relevance.
Two features Novelty Factor (NF) (Varma et al.,
2009), and New Words (NW) are used at scoring
level.
</bodyText>
<subsectionHeader confidence="0.465216">
Novelty Factor (NF)
</subsectionHeader>
<bodyText confidence="0.9992255">
NF measures both topic relevancy of a sentence
and its novelty given prior knowledge of the user
through pdocs. NF score for a word w is calculated
as,
</bodyText>
<equation confidence="0.98296325">
NF(w) = |ndt|
|pdt |+ |ndocs|
ndt= {d : w ∈ d ∧ d ∈ ndocs}
pdt= {d : w ∈ d ∧ d ∈ pdocs}
</equation>
<bodyText confidence="0.99959275">
|ndt |captures the relevancy of w, and |pdt |elevates
the novelty by penalizing words occurring fre-
quently in pdocs. Score of a sentence is the average
NF value of its content words.
</bodyText>
<subsectionHeader confidence="0.749015">
New Words (NW)
</subsectionHeader>
<bodyText confidence="0.999817857142857">
Unlike NF, NW captures only novelty of a sentence.
Novelty of a sentence is assessed by the amount of
new words it contains. Words that never occurred
before in pdocs are considered new. Normalized
term frequency of a word (w) is used in calculating
feature score of sentence. Score of a sentence(s) is
given by,
</bodyText>
<subsectionHeader confidence="0.999589">
3.2 At Ranking
</subsectionHeader>
<bodyText confidence="0.9986795">
Ranked sentence set is re-ordered using Maximal
Marginal relevance (Carbonell and Goldstein, 1998)
criterion, such that prior knowledge is neglected and
sentences with new information are promoted in the
ranked list. Final rank (“Rank”) of a sentence is
computed as,
</bodyText>
<equation confidence="0.9970805">
Rank = relweight ∗ rank −
(1 − relweight) ∗ redundancy score
</equation>
<bodyText confidence="0.99192025">
Where “rank” is the original sentence rank predicted
by regression model as described in section 2, and
“redundancy score” is an estimate for the amount
of prior information a sentence contains. Parameter
“relweight” adjusts relevancy and novelty of a
sentence. Two similarity measures ITSim, CoSim
are used for calculating redundancy score.
Information Theoretic Similarity (ITSim)
According to information theory, Entropy quantifies
the amount of information carried with a message.
Extending this analogy to text content, Entropy
I(w) of a word w is calculated as,
</bodyText>
<equation confidence="0.9999745">
I(w) = −p(w) ∗ log(p(w))
p(w) = n/N
</equation>
<bodyText confidence="0.99972">
Motivated by the information theoretic definition of
similarity by (Lin, 1998), we define similarity be-
tween two sentences s1 and s2 as,
</bodyText>
<equation confidence="0.995355">
2 ∗ EwEs1�s2 I(w)
IT5im(s1, s2) =
</equation>
<subsectionHeader confidence="0.763598">
Cosine Similarity (CoSim)
</subsectionHeader>
<bodyText confidence="0.999048833333333">
Cosine similarity is a popular technique in TREC
Novelty track to compute sentence similarity.
Sentences are viewed as tf-idf vectors (Salton
and Buckley, 1987) of words they contain in a n-
dimension space. Similarity between two sentences
is measured as,
</bodyText>
<equation confidence="0.927669">
s1.s2
Co5im(s1, s2) = cos(Θ) =
|s1||s2|
</equation>
<bodyText confidence="0.997797666666667">
Average similarity value of a sentence with all sen-
tences in pdocs is considered as its redundancy
score.
</bodyText>
<equation confidence="0.992326125">
|s|
NW (w) = 0 if w ∈ pdocs
= n/N else
n is frequency of w in ndocs
N is total term frequency of ndocs
E 5core(s) = wEs
NW (w)
EwEs1 I(w) + EwEs2 I(w)
</equation>
<bodyText confidence="0.960338666666667">
Numerator is proportional to the commonality
between s1 and s2 and denominator reflects differ-
ences between them.
</bodyText>
<page confidence="0.993697">
15
</page>
<subsectionHeader confidence="0.745326">
3.3 At summary extraction
Novelty Pool (NP)
</subsectionHeader>
<bodyText confidence="0.9999166">
Sentences that possibly contain prior information
are filtered out from summary by creating Novelty
Pool (NP), a pool of sentences containing one or
more novelwords. Two sets of “dominant” words
are generated one for each pdocs and ndocs.
</bodyText>
<equation confidence="0.8220575">
domndocs = {w : DFndocs(w) &gt; threshold}
dompdocs = {w : DFpdocs(w) &gt; threshold}
</equation>
<bodyText confidence="0.923700833333333">
A word is considered dominant if it appears in more
than a predefined “threshold ” of articles, thus mea-
suring its topic relevance. Difference of the two dom
sets gives us a list of novelwords that are both rele-
vant and new.
novelwords = domndocs − dompdocs
</bodyText>
<sectionHeader confidence="0.998235" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999923111111111">
We conducted all the experiments on TAC 2009 Up-
date Summarization dataset. It consists of 48 topics,
each having 20 documents divided into two clusters
“A” and “B” based on their chronological coverage
of topic. It serves as an ideal setting for evaluat-
ing our progressive summaries. Summary for clus-
ter A (pdocs) is a normal multi document summary
where as summary for cluster B (ndocs) is a Pro-
gressive summary, both of length 100 words. Each
topic has associated 4 model summaries written by
human assessors. TAC 2008 Update summarization
data that follow similar structure is used to build
training model for support vectors as mentioned in
Section 2. Thresholds for domndocs, dompdocs are
set to 0.6, 0.3 respectively and relweight to 0.8 for
optimal results.
Summaries are evaluated using ROUGE (Lin,
2004), a recall oriented metric that automatically
assess machine generated summaries based on their
overlap with models. ROUGE-2 and ROUGE-SU4
are standard measures for automated summary
evaluation. In Table 1 ROUGE scores of baseline
systems(Section 2.1) are presented.
Five progressive runs are generated, each having a
novelty detection scheme at either scoring, ranking
or summary extraction stages. ROUGE scores of
these runs are presented in Table 2.
</bodyText>
<equation confidence="0.845330666666667">
ROUGE-2 ROUGE-SU4
0.09346 0.13233
0.05865 0.09333
</equation>
<tableCaption confidence="0.995281">
Table 1: Average ROUGE-2, ROUGE-SU4 recall scores
of baselines for TAC 2009, cluster B
</tableCaption>
<bodyText confidence="0.954714470588235">
NF+DocSumm : Sentence scoring is done with an
additional feature NF, along with default features of
DocSumm
NW+DocSumm : An additional feature NW is
used to score sentences for DocSumm
ITSim+DocSumm : ITSim is used for computing
similarity between a sentence in ndocs and set of all
sentences in pdocs. Maximum similarity value is
considered as redundancy score. Re-ordered ranked
list is used for summary extraction
Cosim+DocSumm : CoSim is used as a similarity
measure instead of ITSim
NP+DocSumm : Only members of NP are consid-
ered while extracting DocSumm summaries
Results of top systems at TAC 2009, ICSI (Gillick
et al., 2009) and THUSUM (Long et al., 2009) are
also provided for comparison.
</bodyText>
<table confidence="0.9982715">
ROUGE-2 ROUGE-SU4
ICSI 0.10417 0.13959
NF+DocSumm 0.10273 0.13922
NW+DocSumm 0.09645 0.13955
NP+DocSumm 0.09873 0.13977
THUSUM 0.09608 0.13499
ITSim+DocSumm 0.09461 0.13306
Cosim+DocSumm 0.08338 0.12607
</table>
<tableCaption confidence="0.967777">
Table 2: Average ROUGE-2, ROUGE-SU4 recall scores
for TAC 2009, cluster B
</tableCaption>
<bodyText confidence="0.9986572">
Next level of experiments are carried out on combi-
nation of these techniques. Each run is produced by
combining two or more of the above(Section 3) de-
scribed techniques in conjunction with DocSumm.
Results of these runs are presented in table 3
</bodyText>
<footnote confidence="0.624558166666667">
NF+NW : Both NF and NW are used for sentence
scoring along with default features of DocSumm
NF+NW+ITSim : Sentences scored in NF+NW are
re-ranked by their ITSim score
NF+NW+NP : Only members of NP are selected
while extracting NF+NW summaries
</footnote>
<table confidence="0.838100090909091">
DocSumm
TacBaseline
16
NF+NW+ITSim+NP : Sentences are selected from
NP during extraction of NF+NW+ITSim summaries
ROUGE-2 ROUGE-SU4
NF+NW 0.09807 0.14058
NF+NW+ITSim 0.09704 0.13978
NF+NW+NP 0.09875 0.14010
{NP+NW+
ITSim+NP} 0.09664 0.13812
</table>
<tableCaption confidence="0.9749965">
Table 3: Average ROUGE-2, ROUGE-SU4 recall scores
for TAC 2009, cluster B
</tableCaption>
<sectionHeader confidence="0.982742" genericHeader="conclusions">
5 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999991">
Experimental results prove that proposed Novelty
Detection techniques, particularly at scoring stage
are very effective in the context of progressive sum-
marization. Both NF, a language modeling tech-
nique and NW, a heuristic based feature are able
to capture relevant novelty successfully. An ap-
proximate 6% increase in ROUGE-2 and 3% in-
crease in ROUGE-SU4 scores over DocSumm sup-
port our argument. Scores of NF+DocSumm and
NW+DocSumm are comparable with existing best
approaches. Since CoSim is a word overlap mea-
sure, and novel information is often embedded
within a sentence containing formerly known infor-
mation, quality of progressive summaries declined.
ITSim performs better than Cosim because it con-
siders entropy of a word in similarity computations,
which is a better estimate of information. There is a
need for improved similarity measures that can cap-
ture semantic relatedness between sentences. Nov-
elty pool (NP) is a simple filtering technique, that
improved quality of progressive summaries by dis-
carding probable redundant sentences into summary.
From the results in Table 2, it can be hypothesized
that Novelty is best captured at sentence scoring
stage of summarization, rather than at ranking or
summary extraction.
A slight improvement of ROUGE scores is ob-
served in table 3, when novelty detection techniques
at scoring, ranking and extracting stages are com-
bined together. As Novel sentences are already
scored high through NF and NW, the effect of Re-
Ranking and Filtering is not significant in the com-
bination.
The major contribution of this work is to iden-
tify the possibility of novelty detection at different
stages of summarization. Two new sentence scoring
features (NF and NW), a filtering strategy (NP), a
sentence similarity measure (ITSim) are introduced
to capture relevant novelty. Although proposed ap-
proaches are simple, we hope that this novel treat-
ment could inspire new methodologies in progres-
sive summarization. Nevertheless, the problem of
progressive summarization is far from being solved
given the complexity involved in novelty detection.
</bodyText>
<sectionHeader confidence="0.9964" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996768">
I would like to thank Dr. Vasudeva Varma at IIIT
Hyderabad, for his support and guidance throughout
this work. I also thank Rahul Katragadda at Yahoo
Research and other anonymous reviewers, for their
valuable suggestions and comments.
</bodyText>
<sectionHeader confidence="0.998856" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996537533333334">
Nasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fer-
nando Diaz, Leah Larkey, and Xiaoyan Li. 2004.
Umass at trec 2004: Novelty and hard.
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of news topics.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In SIGIR ’98: Pro-
ceedings of the 21st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 335–336, New York, NY, USA.
ACM.
David Eichmannac, Yi Zhangb, Shannon Bradshawbc,
Xin Ying Qiub, Padmini Srinivasanabc, and Aditya
Kumar. 2004. Novelty, question answering and ge-
nomics: The university of iowa response.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd
summarization system at tac 2009.
Adam Jatowt. 2004. Web page summarization using dy-
namic content. In WWW Alt. ’04: Proceedings of the
13th international World Wide Web conference on Al-
ternate track papers and posters, pages 344–345, New
York, NY, USA. ACM.
Sujian Li, Wei Wang, and Yongwei Zhang. 2009. Tac
2009 update summarization with unsupervised meth-
ods.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In ICML ’98: Proceedings of the Fif-
teenth International Conference on Machine Learn-
</reference>
<page confidence="0.987964">
17
</page>
<reference confidence="0.999763382352941">
ing, pages 296–304, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. pages 74–81, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Chong Long, Minlie Huang, and Xiaoyan Zhu. 2009.
Tsinghua university at tac 2009: Summarizing multi-
documents by information distance.
Gerard Salton and Chris Buckley. 1987. Term weight-
ing approaches in automatic text retrieval. Technical
report, Ithaca, NY, USA.
Barry Schiffman and Kathleen R. McKeown. 2004.
Columbia university in the novelty track at trec 2004.
Frank Schilder and Ravikumar Kondadandi. 2008. Fast-
sum: fast and accurate query-based multi-document
summarization. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics on Human Language Technologies. Human Lan-
guage Technology Conference.
Ian Soboroff. 2004. Overview of the trec 2004 novelty
track. National Institute of Standards and Technol-
ogy,Gaithersburg, MD 20899.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409–445.
Vasudeva Varma, Praveen Bysani, Kranthi Reddy, Vijay
Bharat, Santosh GSK, Karuna Kumar, Sudheer Kove-
lamudi, Kiran Kumar N, and Nitin Maganti. 2009. iiit
hyderabad at tac 2009. Technical report, Gaithersburg,
Maryland USA.
Jin Zhang, Pan Du, Hongbo Xu, and Xueqi Cheng. 2009.
Ictgrasper at tac2009: Temporal preferred update sum-
marization.
</reference>
<page confidence="0.999287">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.129478">
<title confidence="0.999881">Detecting Novelty in the context of Progressive Summarization</title>
<author confidence="0.193479">Praveen</author>
<affiliation confidence="0.295618">Language Technologies Research IIIT</affiliation>
<email confidence="0.699217">lvsnpraveen@research.iiit.ac.in</email>
<abstract confidence="0.995995588235294">A Progressive summary helps a user to monitor changes in evolving news topics over a period of time. Detecting novel information is the essential part of progressive summarization that differentiates it from normal multi document summarization. In this work, we explore the possibility of detecting novelty at various stages of summarization. New scoring features, Re-ranking criterions and filtering strategies are proposed to identify “relevant novel” information. We compare these techniques using an automated evaluation framework ROUGE, and determine the best. Overall, our summarizer is able to perform on par with existing prime methods in progressive summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nasreen Abdul-Jaleel</author>
<author>James Allan</author>
<author>W Bruce Croft</author>
<author>Fernando Diaz</author>
<author>Leah Larkey</author>
<author>Xiaoyan Li</author>
</authors>
<title>Umass at trec 2004: Novelty and hard.</title>
<date>2004</date>
<contexts>
<context position="3496" citStr="Abdul-Jaleel et al., 2004" startWordPosition="493" endWordPosition="496"> Track 4, that is designed to investigate systems abilities to locate sentences containing relevant and/or new information given the topic and a set of relevant documents ordered by date. A progressive summarizer needs to identify, score and then finally rank “relevant novel” sentences to produce a summary. 4http://trec.nist.gov/data/novelty.html Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 13–18, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Previous approaches to Novelty detection at TREC (Soboroff, 2004) include cosine filtering (Abdul-Jaleel et al., 2004), where a sentence having maximum cosine similarity value with previous set of sentences, lower than a preset threshold is considered novel. Alternatively, (Schiffman and McKeown, 2004) considered previously unseen words as an evidence of Novelty. (Eichmannac et al., 2004) expanded all noun phrases in a sentence using wordnet and used corresponding sysnsets for novelty comparisions. Our work targets exploring the effect of detecting novelty at different stages of summarization on the quality of progressive summaries. Unlike most of the previous work (Li et al., 2009) (Zhang et al., 2009) in pr</context>
</contexts>
<marker>Abdul-Jaleel, Allan, Croft, Diaz, Larkey, Li, 2004</marker>
<rawString>Nasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fernando Diaz, Leah Larkey, and Xiaoyan Li. 2004. Umass at trec 2004: Novelty and hard.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Rahul Gupta</author>
<author>Vikas Khandelwal</author>
</authors>
<title>Temporal summaries of news topics.</title>
<date>2001</date>
<contexts>
<context position="2260" citStr="Allan et al., 2001" startWordPosition="320" endWordPosition="323">ive manner and the latter title is more apt to the task. Progressive summaries contain information which is both relevant and novel, since they are produced under the assumption that user has already read some previous documents/articles on the topic. Such summaries are extremely useful in tracking news stories, tracing new product reviews etc. Unlike dynamic summarization (Jatowt, 2004) where a single summary transforms periodically, reflecting changes in source text, Progressive summarizer produce multiple summaries at specific time intervals updating user knowledge. Temporal Summarization (Allan et al., 2001) generate summaries, similar to progressive summaries by ranking sentences as combination of relevant and new scores. In this work, summaries are produced not just by reforming ranking scheme but also altering scoring and extraction stages of summarization. Progressive summarization requires differentiating Relevant and Novel Vs Non-Relevant and Novel Vs Relevant and Redundant information. Such discrimination is feasible only with efficient Novelty detection techniques. We define Novelty detection as identifying relevant sentences containing new information. This task shares similarity with TR</context>
</contexts>
<marker>Allan, Gupta, Khandelwal, 2001</marker>
<rawString>James Allan, Rahul Gupta, and Vikas Khandelwal. 2001. Temporal summaries of news topics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8120" citStr="Carbonell and Goldstein, 1998" startWordPosition="1266" endWordPosition="1269">docs} |ndt |captures the relevancy of w, and |pdt |elevates the novelty by penalizing words occurring frequently in pdocs. Score of a sentence is the average NF value of its content words. New Words (NW) Unlike NF, NW captures only novelty of a sentence. Novelty of a sentence is assessed by the amount of new words it contains. Words that never occurred before in pdocs are considered new. Normalized term frequency of a word (w) is used in calculating feature score of sentence. Score of a sentence(s) is given by, 3.2 At Ranking Ranked sentence set is re-ordered using Maximal Marginal relevance (Carbonell and Goldstein, 1998) criterion, such that prior knowledge is neglected and sentences with new information are promoted in the ranked list. Final rank (“Rank”) of a sentence is computed as, Rank = relweight ∗ rank − (1 − relweight) ∗ redundancy score Where “rank” is the original sentence rank predicted by regression model as described in section 2, and “redundancy score” is an estimate for the amount of prior information a sentence contains. Parameter “relweight” adjusts relevancy and novelty of a sentence. Two similarity measures ITSim, CoSim are used for calculating redundancy score. Information Theoretic Simila</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335–336, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Eichmannac</author>
<author>Yi Zhangb</author>
<author>Shannon Bradshawbc</author>
</authors>
<title>Xin Ying Qiub, Padmini Srinivasanabc, and Aditya Kumar.</title>
<date>2004</date>
<contexts>
<context position="3769" citStr="Eichmannac et al., 2004" startWordPosition="534" endWordPosition="537">” sentences to produce a summary. 4http://trec.nist.gov/data/novelty.html Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 13–18, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Previous approaches to Novelty detection at TREC (Soboroff, 2004) include cosine filtering (Abdul-Jaleel et al., 2004), where a sentence having maximum cosine similarity value with previous set of sentences, lower than a preset threshold is considered novel. Alternatively, (Schiffman and McKeown, 2004) considered previously unseen words as an evidence of Novelty. (Eichmannac et al., 2004) expanded all noun phrases in a sentence using wordnet and used corresponding sysnsets for novelty comparisions. Our work targets exploring the effect of detecting novelty at different stages of summarization on the quality of progressive summaries. Unlike most of the previous work (Li et al., 2009) (Zhang et al., 2009) in progressive summarization, we employ multiple novelty detection techniques at different stages and analyze them all to find the best. 2 Document Summarization The Focus of this paper is only on extractive summarization, henceforth term summarization/summarizer implies senten</context>
</contexts>
<marker>Eichmannac, Zhangb, Bradshawbc, 2004</marker>
<rawString>David Eichmannac, Yi Zhangb, Shannon Bradshawbc, Xin Ying Qiub, Padmini Srinivasanabc, and Aditya Kumar. 2004. Novelty, question answering and genomics: The university of iowa response.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-Tur</author>
<author>Berndt Bohnet</author>
<author>Yang Liu</author>
<author>Shasha Xie</author>
</authors>
<title>The icsi/utd summarization system at tac</title>
<date>2009</date>
<contexts>
<context position="12502" citStr="Gillick et al., 2009" startWordPosition="1982" endWordPosition="1985"> Sentence scoring is done with an additional feature NF, along with default features of DocSumm NW+DocSumm : An additional feature NW is used to score sentences for DocSumm ITSim+DocSumm : ITSim is used for computing similarity between a sentence in ndocs and set of all sentences in pdocs. Maximum similarity value is considered as redundancy score. Re-ordered ranked list is used for summary extraction Cosim+DocSumm : CoSim is used as a similarity measure instead of ITSim NP+DocSumm : Only members of NP are considered while extracting DocSumm summaries Results of top systems at TAC 2009, ICSI (Gillick et al., 2009) and THUSUM (Long et al., 2009) are also provided for comparison. ROUGE-2 ROUGE-SU4 ICSI 0.10417 0.13959 NF+DocSumm 0.10273 0.13922 NW+DocSumm 0.09645 0.13955 NP+DocSumm 0.09873 0.13977 THUSUM 0.09608 0.13499 ITSim+DocSumm 0.09461 0.13306 Cosim+DocSumm 0.08338 0.12607 Table 2: Average ROUGE-2, ROUGE-SU4 recall scores for TAC 2009, cluster B Next level of experiments are carried out on combination of these techniques. Each run is produced by combining two or more of the above(Section 3) described techniques in conjunction with DocSumm. Results of these runs are presented in table 3 NF+NW : Both</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, Bohnet, Liu, Xie, 2009</marker>
<rawString>Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd summarization system at tac 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Jatowt</author>
</authors>
<title>Web page summarization using dynamic content.</title>
<date>2004</date>
<booktitle>In WWW Alt. ’04: Proceedings of the 13th international World Wide Web conference on Alternate track papers and posters,</booktitle>
<pages>344--345</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2031" citStr="Jatowt, 2004" startWordPosition="290" endWordPosition="291">ummariztion as “Progressive Summarization” in rest of 1http://newsblaster.cs.columbia.edu/ 2http://duc.nist.gov/duc2007/tasks.html 3http://www.nist.gov/tac 13 this paper, as summaries are produced periodically in a progressive manner and the latter title is more apt to the task. Progressive summaries contain information which is both relevant and novel, since they are produced under the assumption that user has already read some previous documents/articles on the topic. Such summaries are extremely useful in tracking news stories, tracing new product reviews etc. Unlike dynamic summarization (Jatowt, 2004) where a single summary transforms periodically, reflecting changes in source text, Progressive summarizer produce multiple summaries at specific time intervals updating user knowledge. Temporal Summarization (Allan et al., 2001) generate summaries, similar to progressive summaries by ranking sentences as combination of relevant and new scores. In this work, summaries are produced not just by reforming ranking scheme but also altering scoring and extraction stages of summarization. Progressive summarization requires differentiating Relevant and Novel Vs Non-Relevant and Novel Vs Relevant and R</context>
</contexts>
<marker>Jatowt, 2004</marker>
<rawString>Adam Jatowt. 2004. Web page summarization using dynamic content. In WWW Alt. ’04: Proceedings of the 13th international World Wide Web conference on Alternate track papers and posters, pages 344–345, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujian Li</author>
<author>Wei Wang</author>
<author>Yongwei Zhang</author>
</authors>
<title>Tac</title>
<date>2009</date>
<contexts>
<context position="4069" citStr="Li et al., 2009" startWordPosition="581" endWordPosition="584">osine filtering (Abdul-Jaleel et al., 2004), where a sentence having maximum cosine similarity value with previous set of sentences, lower than a preset threshold is considered novel. Alternatively, (Schiffman and McKeown, 2004) considered previously unseen words as an evidence of Novelty. (Eichmannac et al., 2004) expanded all noun phrases in a sentence using wordnet and used corresponding sysnsets for novelty comparisions. Our work targets exploring the effect of detecting novelty at different stages of summarization on the quality of progressive summaries. Unlike most of the previous work (Li et al., 2009) (Zhang et al., 2009) in progressive summarization, we employ multiple novelty detection techniques at different stages and analyze them all to find the best. 2 Document Summarization The Focus of this paper is only on extractive summarization, henceforth term summarization/summarizer implies sentence extractive multi document summarization. Our Summarizer has 4 major stages as shown in Figure 1, Figure 1: Stages in a Multi Document Summarizer Every news article/document is cleaned from news heads, HTML tags and split into sentences during Pre-processing stage. At scoring, several sentence sco</context>
</contexts>
<marker>Li, Wang, Zhang, 2009</marker>
<rawString>Sujian Li, Wei Wang, and Yongwei Zhang. 2009. Tac 2009 update summarization with unsupervised methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In ICML ’98: Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="9032" citStr="Lin, 1998" startWordPosition="1410" endWordPosition="1411"> as described in section 2, and “redundancy score” is an estimate for the amount of prior information a sentence contains. Parameter “relweight” adjusts relevancy and novelty of a sentence. Two similarity measures ITSim, CoSim are used for calculating redundancy score. Information Theoretic Similarity (ITSim) According to information theory, Entropy quantifies the amount of information carried with a message. Extending this analogy to text content, Entropy I(w) of a word w is calculated as, I(w) = −p(w) ∗ log(p(w)) p(w) = n/N Motivated by the information theoretic definition of similarity by (Lin, 1998), we define similarity between two sentences s1 and s2 as, 2 ∗ EwEs1�s2 I(w) IT5im(s1, s2) = Cosine Similarity (CoSim) Cosine similarity is a popular technique in TREC Novelty track to compute sentence similarity. Sentences are viewed as tf-idf vectors (Salton and Buckley, 1987) of words they contain in a ndimension space. Similarity between two sentences is measured as, s1.s2 Co5im(s1, s2) = cos(Θ) = |s1||s2| Average similarity value of a sentence with all sentences in pdocs is considered as its redundancy score. |s| NW (w) = 0 if w ∈ pdocs = n/N else n is frequency of w in ndocs N is total t</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In ICML ’98: Proceedings of the Fifteenth International Conference on Machine Learning, pages 296–304, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<pages>74--81</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona, Spain,</location>
<contexts>
<context position="11279" citStr="Lin, 2004" startWordPosition="1795" endWordPosition="1796">of topic. It serves as an ideal setting for evaluating our progressive summaries. Summary for cluster A (pdocs) is a normal multi document summary where as summary for cluster B (ndocs) is a Progressive summary, both of length 100 words. Each topic has associated 4 model summaries written by human assessors. TAC 2008 Update summarization data that follow similar structure is used to build training model for support vectors as mentioned in Section 2. Thresholds for domndocs, dompdocs are set to 0.6, 0.3 respectively and relweight to 0.8 for optimal results. Summaries are evaluated using ROUGE (Lin, 2004), a recall oriented metric that automatically assess machine generated summaries based on their overlap with models. ROUGE-2 and ROUGE-SU4 are standard measures for automated summary evaluation. In Table 1 ROUGE scores of baseline systems(Section 2.1) are presented. Five progressive runs are generated, each having a novelty detection scheme at either scoring, ranking or summary extraction stages. ROUGE scores of these runs are presented in Table 2. ROUGE-2 ROUGE-SU4 0.09346 0.13233 0.05865 0.09333 Table 1: Average ROUGE-2, ROUGE-SU4 recall scores of baselines for TAC 2009, cluster B NF+DocSumm</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Long</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Tsinghua university at tac 2009: Summarizing multidocuments by information distance.</title>
<date>2009</date>
<contexts>
<context position="12533" citStr="Long et al., 2009" startWordPosition="1988" endWordPosition="1991">additional feature NF, along with default features of DocSumm NW+DocSumm : An additional feature NW is used to score sentences for DocSumm ITSim+DocSumm : ITSim is used for computing similarity between a sentence in ndocs and set of all sentences in pdocs. Maximum similarity value is considered as redundancy score. Re-ordered ranked list is used for summary extraction Cosim+DocSumm : CoSim is used as a similarity measure instead of ITSim NP+DocSumm : Only members of NP are considered while extracting DocSumm summaries Results of top systems at TAC 2009, ICSI (Gillick et al., 2009) and THUSUM (Long et al., 2009) are also provided for comparison. ROUGE-2 ROUGE-SU4 ICSI 0.10417 0.13959 NF+DocSumm 0.10273 0.13922 NW+DocSumm 0.09645 0.13955 NP+DocSumm 0.09873 0.13977 THUSUM 0.09608 0.13499 ITSim+DocSumm 0.09461 0.13306 Cosim+DocSumm 0.08338 0.12607 Table 2: Average ROUGE-2, ROUGE-SU4 recall scores for TAC 2009, cluster B Next level of experiments are carried out on combination of these techniques. Each run is produced by combining two or more of the above(Section 3) described techniques in conjunction with DocSumm. Results of these runs are presented in table 3 NF+NW : Both NF and NW are used for sentenc</context>
</contexts>
<marker>Long, Huang, Zhu, 2009</marker>
<rawString>Chong Long, Minlie Huang, and Xiaoyan Zhu. 2009. Tsinghua university at tac 2009: Summarizing multidocuments by information distance.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chris Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1987</date>
<tech>Technical report,</tech>
<location>Ithaca, NY, USA.</location>
<contexts>
<context position="9311" citStr="Salton and Buckley, 1987" startWordPosition="1453" endWordPosition="1456">core. Information Theoretic Similarity (ITSim) According to information theory, Entropy quantifies the amount of information carried with a message. Extending this analogy to text content, Entropy I(w) of a word w is calculated as, I(w) = −p(w) ∗ log(p(w)) p(w) = n/N Motivated by the information theoretic definition of similarity by (Lin, 1998), we define similarity between two sentences s1 and s2 as, 2 ∗ EwEs1�s2 I(w) IT5im(s1, s2) = Cosine Similarity (CoSim) Cosine similarity is a popular technique in TREC Novelty track to compute sentence similarity. Sentences are viewed as tf-idf vectors (Salton and Buckley, 1987) of words they contain in a ndimension space. Similarity between two sentences is measured as, s1.s2 Co5im(s1, s2) = cos(Θ) = |s1||s2| Average similarity value of a sentence with all sentences in pdocs is considered as its redundancy score. |s| NW (w) = 0 if w ∈ pdocs = n/N else n is frequency of w in ndocs N is total term frequency of ndocs E 5core(s) = wEs NW (w) EwEs1 I(w) + EwEs2 I(w) Numerator is proportional to the commonality between s1 and s2 and denominator reflects differences between them. 15 3.3 At summary extraction Novelty Pool (NP) Sentences that possibly contain prior informati</context>
</contexts>
<marker>Salton, Buckley, 1987</marker>
<rawString>Gerard Salton and Chris Buckley. 1987. Term weighting approaches in automatic text retrieval. Technical report, Ithaca, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Schiffman</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Columbia university in the novelty track at trec</title>
<date>2004</date>
<contexts>
<context position="3681" citStr="Schiffman and McKeown, 2004" startWordPosition="521" endWordPosition="524">ate. A progressive summarizer needs to identify, score and then finally rank “relevant novel” sentences to produce a summary. 4http://trec.nist.gov/data/novelty.html Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 13–18, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Previous approaches to Novelty detection at TREC (Soboroff, 2004) include cosine filtering (Abdul-Jaleel et al., 2004), where a sentence having maximum cosine similarity value with previous set of sentences, lower than a preset threshold is considered novel. Alternatively, (Schiffman and McKeown, 2004) considered previously unseen words as an evidence of Novelty. (Eichmannac et al., 2004) expanded all noun phrases in a sentence using wordnet and used corresponding sysnsets for novelty comparisions. Our work targets exploring the effect of detecting novelty at different stages of summarization on the quality of progressive summaries. Unlike most of the previous work (Li et al., 2009) (Zhang et al., 2009) in progressive summarization, we employ multiple novelty detection techniques at different stages and analyze them all to find the best. 2 Document Summarization The Focus of this paper is o</context>
</contexts>
<marker>Schiffman, McKeown, 2004</marker>
<rawString>Barry Schiffman and Kathleen R. McKeown. 2004. Columbia university in the novelty track at trec 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Schilder</author>
<author>Ravikumar Kondadandi</author>
</authors>
<title>Fastsum: fast and accurate query-based multi-document summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies. Human Language Technology Conference.</booktitle>
<contexts>
<context position="5020" citStr="Schilder and Kondadandi, 2008" startWordPosition="730" endWordPosition="733">ocument summarization. Our Summarizer has 4 major stages as shown in Figure 1, Figure 1: Stages in a Multi Document Summarizer Every news article/document is cleaned from news heads, HTML tags and split into sentences during Pre-processing stage. At scoring, several sentence scoring features assign scores for each sentence, reflecting its topic relevance. Feature scores are combined to get a final rank for the sentence in ranking stage. Rank of a sentence is predicted from regression model built on feature vectors of sentences in the training data using support vector machine as explained in (Schilder and Kondadandi, 2008). Finally during summary extraction, a subset of ranked sentences are selected to produce summary after a redundancy check to filter duplicate sentences. 2.1 Normal Summarizers Two normal summarizers (DocSumm, TacBaseline) are developed in a similar fashion described in Figure 1. DocSumm produce summaries with two scoring features, Document Frequency Score (DF) (Schilder and Kondadandi, 2008) and Sentence Position (SP). DocSumm serves as a baseline to depict the effect of novelty detection techniques described in Section 3 on normal summarizers. Document frequency (DF), of a word (w) in the do</context>
</contexts>
<marker>Schilder, Kondadandi, 2008</marker>
<rawString>Frank Schilder and Ravikumar Kondadandi. 2008. Fastsum: fast and accurate query-based multi-document summarization. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies. Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Soboroff</author>
</authors>
<title>Overview of the trec 2004 novelty track.</title>
<date>2004</date>
<booktitle>National Institute of Standards and Technology,Gaithersburg, MD</booktitle>
<pages>20899</pages>
<contexts>
<context position="3443" citStr="Soboroff, 2004" startWordPosition="487" endWordPosition="488">s task shares similarity with TREC Novelty Track 4, that is designed to investigate systems abilities to locate sentences containing relevant and/or new information given the topic and a set of relevant documents ordered by date. A progressive summarizer needs to identify, score and then finally rank “relevant novel” sentences to produce a summary. 4http://trec.nist.gov/data/novelty.html Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 13–18, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Previous approaches to Novelty detection at TREC (Soboroff, 2004) include cosine filtering (Abdul-Jaleel et al., 2004), where a sentence having maximum cosine similarity value with previous set of sentences, lower than a preset threshold is considered novel. Alternatively, (Schiffman and McKeown, 2004) considered previously unseen words as an evidence of Novelty. (Eichmannac et al., 2004) expanded all noun phrases in a sentence using wordnet and used corresponding sysnsets for novelty comparisions. Our work targets exploring the effect of detecting novelty at different stages of summarization on the quality of progressive summaries. Unlike most of the previ</context>
</contexts>
<marker>Soboroff, 2004</marker>
<rawString>Ian Soboroff. 2004. Overview of the trec 2004 novelty track. National Institute of Standards and Technology,Gaithersburg, MD 20899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Comput. Linguist.,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="1165" citStr="Teufel and Moens, 2002" startWordPosition="166" endWordPosition="169">king criterions and filtering strategies are proposed to identify “relevant novel” information. We compare these techniques using an automated evaluation framework ROUGE, and determine the best. Overall, our summarizer is able to perform on par with existing prime methods in progressive summarization. 1 Introduction Summarization is the process of condensing text to its most essential facts. Summarization is challenging for its associated cognitive task and interesting because of its practical usage. It has been successfully applied for text content such as news articles 1, scientific papers (Teufel and Moens, 2002) that follow a discourse structure. Update summarization is an emerging area with in summarization, acquiring significant research focus during recent times. The task was introduced at DUC 20072 and continued during TAC 2008, 20093. We refer to update summariztion as “Progressive Summarization” in rest of 1http://newsblaster.cs.columbia.edu/ 2http://duc.nist.gov/duc2007/tasks.html 3http://www.nist.gov/tac 13 this paper, as summaries are produced periodically in a progressive manner and the latter title is more apt to the task. Progressive summaries contain information which is both relevant an</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: experiments with relevance and rhetorical status. Comput. Linguist., 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasudeva Varma</author>
<author>Praveen Bysani</author>
<author>Kranthi Reddy</author>
<author>Vijay Bharat</author>
<author>Santosh GSK</author>
<author>Karuna Kumar</author>
<author>Sudheer Kovelamudi</author>
<author>Kiran Kumar N</author>
<author>Nitin Maganti</author>
</authors>
<title>iiit hyderabad at tac</title>
<date>2009</date>
<tech>Technical report,</tech>
<location>Gaithersburg, Maryland USA.</location>
<contexts>
<context position="7188" citStr="Varma et al., 2009" startWordPosition="1094" endWordPosition="1097">d from time 0 to time t are assumed to 14 have been read previously, hence prior knowledge, pdocs. Articles published in the interval t to T that contain new information are considered ndocs. ndocs = {d : td &gt; t} pdocs = {d : td &lt;= t} Progressive summarization needs a novelty detection technique to identify sentences that contain relevant new information. The task of detecting novelty can be carried out at 3 stages of summarization shown in Figure 1. 3.1 At Scoring New Sentence scoring features are devised to capture sentence novelty along with its relevance. Two features Novelty Factor (NF) (Varma et al., 2009), and New Words (NW) are used at scoring level. Novelty Factor (NF) NF measures both topic relevancy of a sentence and its novelty given prior knowledge of the user through pdocs. NF score for a word w is calculated as, NF(w) = |ndt| |pdt |+ |ndocs| ndt= {d : w ∈ d ∧ d ∈ ndocs} pdt= {d : w ∈ d ∧ d ∈ pdocs} |ndt |captures the relevancy of w, and |pdt |elevates the novelty by penalizing words occurring frequently in pdocs. Score of a sentence is the average NF value of its content words. New Words (NW) Unlike NF, NW captures only novelty of a sentence. Novelty of a sentence is assessed by the am</context>
</contexts>
<marker>Varma, Bysani, Reddy, Bharat, GSK, Kumar, Kovelamudi, N, Maganti, 2009</marker>
<rawString>Vasudeva Varma, Praveen Bysani, Kranthi Reddy, Vijay Bharat, Santosh GSK, Karuna Kumar, Sudheer Kovelamudi, Kiran Kumar N, and Nitin Maganti. 2009. iiit hyderabad at tac 2009. Technical report, Gaithersburg, Maryland USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Zhang</author>
<author>Pan Du</author>
<author>Hongbo Xu</author>
<author>Xueqi Cheng</author>
</authors>
<title>Ictgrasper at tac2009: Temporal preferred update summarization.</title>
<date>2009</date>
<contexts>
<context position="4090" citStr="Zhang et al., 2009" startWordPosition="585" endWordPosition="588">bdul-Jaleel et al., 2004), where a sentence having maximum cosine similarity value with previous set of sentences, lower than a preset threshold is considered novel. Alternatively, (Schiffman and McKeown, 2004) considered previously unseen words as an evidence of Novelty. (Eichmannac et al., 2004) expanded all noun phrases in a sentence using wordnet and used corresponding sysnsets for novelty comparisions. Our work targets exploring the effect of detecting novelty at different stages of summarization on the quality of progressive summaries. Unlike most of the previous work (Li et al., 2009) (Zhang et al., 2009) in progressive summarization, we employ multiple novelty detection techniques at different stages and analyze them all to find the best. 2 Document Summarization The Focus of this paper is only on extractive summarization, henceforth term summarization/summarizer implies sentence extractive multi document summarization. Our Summarizer has 4 major stages as shown in Figure 1, Figure 1: Stages in a Multi Document Summarizer Every news article/document is cleaned from news heads, HTML tags and split into sentences during Pre-processing stage. At scoring, several sentence scoring features assign </context>
</contexts>
<marker>Zhang, Du, Xu, Cheng, 2009</marker>
<rawString>Jin Zhang, Pan Du, Hongbo Xu, and Xueqi Cheng. 2009. Ictgrasper at tac2009: Temporal preferred update summarization.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>