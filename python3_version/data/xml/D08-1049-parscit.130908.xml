<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.98884">
Multimodal Subjectivity Analysis of Multiparty Conversation
</title>
<author confidence="0.930668">
Stephan Raaijmakers
</author>
<affiliation confidence="0.827365666666667">
TNO Information and
Communication Technology
Delft, The Netherlands
</affiliation>
<note confidence="0.478554">
Khiet Truong
TNO Defense, Security and Safety
Soesterberg, The Netherlands
</note>
<email confidence="0.983309">
khiet.truong@tno.nl
</email>
<author confidence="0.993432">
Theresa Wilson
</author>
<affiliation confidence="0.9983875">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.539189">
Edingburgh, UK
</address>
<email confidence="0.9767355">
twilson@inf.ed.ac.uk
stephan.raaijmakers@tno.nl
</email>
<sectionHeader confidence="0.994932" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999788833333333">
We investigate the combination of several
sources of information for the purpose of sub-
jectivity recognition and polarity classification
in meetings. We focus on features from two
modalities, transcribed words and acoustics,
and we compare the performance of three dif-
ferent textual representations: words, charac-
ters, and phonemes. Our experiments show
that character-level features outperform word-
level features for these tasks, and that a care-
ful fusion of all features yields the best perfor-
mance. 1
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.951178346938775">
Opinions, sentiments and other types of subjective
content are an important part of any meeting. Meet-
ing participants express pros and cons about ideas,
they support or oppose decisions, and they make
suggestions that may or may not be adopted. When
recorded and archived, meetings become a part of
the organizational knowledge, but their value is lim-
ited by the ability of tools to search and summa-
rize meeting content, including subjective content.
While progress has been made on recognizing pri-
marily objective meeting content, for example, in-
formation about the topics that are discussed (Hsueh
and Moore, 2006) and who is assigned to work on
given tasks (Purver et al., 2006), there has been
&apos;This work was supported by the Dutch BSIK-project Mul-
timediaN, and the European IST Programme Project FP6-
0033812. This paper only reflects the authors’ views and fund-
ing agencies are not liable for any use that may be made of the
information contained herein.
fairly little work specifically directed toward recog-
nizing subjective content.
In contrast, there has been a wealth of research
over the past several years on automatic subjectiv-
ity and sentiment analysis in text, including on-line
media. Partly inspired by the rapid growth of so-
cial media, such as blogs, as well as on-line news
and reviews, researchers are now actively address-
ing a wide variety of new tasks, ranging from blog
mining (e.g., finding opinion leaders in an on-line
community), to reputation management (e.g. find-
ing negative opinions about a company on the web),
to opinion-oriented summarization and question an-
swering. Yet many challenges remain, including
how best to represent and combine linguistic infor-
mation for subjectivity analysis. With the additional
modalities that are present when working with face-
to-face spoken communication, these challenges are
even more pronounced.
The work in this paper focuses on two tasks: (1)
recognizing subjective utterances and (2) discrimi-
nating between positive and negative subjective ut-
terances. An utterance may be subjective because
the speaker is expressing an opinion, because the
speaker is discussing someone else’s opinion, or be-
cause the speaker is eliciting the opinion of someone
else with a question.
We approach the above tasks as supervised ma-
chine learning problems, with the specific goal of
finding answers to the following research questions:
</bodyText>
<listItem confidence="0.87017575">
• Given a variety of information sources, such
as text arising from (transcribed) speech,
phoneme representations of the words in an ut-
terance, and acoustic features extracted from
</listItem>
<page confidence="0.993425">
466
</page>
<note confidence="0.956152">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 466–474,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.619623">
the audio layer, which of these sources are par-
ticularly valuable for subjectivity analysis in
multiparty conversation?
</bodyText>
<listItem confidence="0.877662">
• Does the combination of these sources lead to
further improvement?
• What are the optimal representations of these
information sources in terms of feature design
for a machine learning component?
</listItem>
<bodyText confidence="0.999121666666667">
A central tenet of our approach is that subword
representations, such as character and phoneme n-
grams, are beneficial for the tasks at hand.
</bodyText>
<sectionHeader confidence="0.990809" genericHeader="method">
2 Subword Features
</sectionHeader>
<bodyText confidence="0.999383072727273">
Previous work has demonstrated that textual units
below the word level, such as character n-grams,
are valuable sources of information for various
text classification tasks. An example of character
n-grams is the set of 3-grams {#se, sen, ent,
nti, tim, ime, men, ent, nt#, t#a,
#an, ana, nal, aly, lys, ysi, sis,
is#} for the two-word phrase sentiment analysis.
The special symbol # represents a word boundary.
While it is not directly obvious that there is much
information in these truncated substrings, character
n-grams have successfully been used for fine-
grained classification tasks, such as named-entity
recognition (Klein et al., 2003) and subjective
sentence recognition (Raaijmakers and Kraaij,
2008), as well as a variety of document-level tasks
(Stamatatos, 2006; Zhang and Lee, 2006; Kanaris
and Stamatatos, 2007).
The informativeness of these low-level features
comes in part from a form of attenuation (Eisner,
1996): a slight abstraction of the underlying data
that leads to the formation of string equivalence
classes. For instance, words in a sentence will in-
variably share many character n-grams. Since ev-
ery unique character n-gram in an utterance consti-
tutes a separate feature, this leads to the formation
of string classes, which is a form of abstraction. For
example, Zhang and Lee (2006) investigate similar
subword representations, called key substring group
features. By compressing substrings in a corpus in a
trie (a prefix tree), and labeling entire sets of distri-
butionally equivalent substrings with one group la-
bel, an attenuation effect is obtained that proves very
beneficial for a number of text classification tasks.
Aside from attenuation effects, character n-
grams, especially those that represent word bound-
aries, have additional benefits. Treating word
boundaries as characters captures micro-phrasal in-
formation: short strings that express the transition
of one word to another. Stemming occurs naturally
within the set of initial character n-grams of a word,
where the suffix is left out. Also, some part-of-
speech information is captured. For example, the
modals could, would, should can be represented by
the 4-gram, ould, and the set of adverbs ending in
-ly can be represented by the 3-gram ly#.
A challenging thought is to extend the use of n-
grams to the level of phonemes, which comprise
the first symbolic level in the process of sound to
grapheme conversion. If n-grams of phonemes com-
pare favorably to word n-grams for the purpose of
sentiment classification, then significant speedups
can be obtained for online sentiment classification,
since tokenization of the raw speech signal can make
a halt at the phoneme level.
</bodyText>
<sectionHeader confidence="0.996648" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999959681818182">
For this work we use 13 meetings from the AMI
Meeting Corpus (Carletta et al., 2005). Each meet-
ing has four participants and is approximately 30
minutes long. The participants play specific roles
(e.g., Project Manager, Marketing Expert) and to-
gether function as a design team. Within the set of
13 meetings, there are a total of 20 participants, with
each participant taking part in two or three meet-
ings as part of the same design team. Meetings with
the same set of participants represent different stages
in the design process (e.g., Conceptual Design, De-
tailed Design).
The meetings used in the experiments have been
annotated for subjective content using the AMIDA
annotation scheme (Wilson, 2008). Table 1 lists
the types of annotations that are marked in the data.
There are three main categories of annotations, sub-
jective utterances, subjective questions, and objec-
tive polar utterances. A subjective utterance is a
span of words (or possibly sounds) where a pri-
vate state is being expressed either through choice of
words or prosody. A private state (Quirk et al., 1985)
</bodyText>
<page confidence="0.998835">
467
</page>
<bodyText confidence="0.8538668">
is an internal mental or emotional state, including
opinions, beliefs, sentiments, emotions, evaluations,
uncertainties, and speculations, among others. Al-
though typically when a private state is expressed
it is the private state of the speaker, as in example
(1) below, an utterance may also be subjective be-
cause the speaker is talking about the private state
of someone else. For example, in (2) the negative
opinion attributed to the company is what makes the
utterance subjective.
</bodyText>
<listItem confidence="0.935389">
(1) Finding them is really a pain, you know
(2) The company’s decided that teletext is out-
dated
</listItem>
<bodyText confidence="0.998312">
Subjective questions are questions in which the
speaker is eliciting the private state of someone else.
In other words, the speaker is asking about what
someone else thinks, feels, wants, likes, etc., and the
speaker is expecting a response in which the other
person expresses what he or she thinks, feels, wants,
or likes. For example, both (3) and (4) below are
subjective questions.
</bodyText>
<listItem confidence="0.999509">
(3) Do you like the large buttons?
(4) What do you think about the large buttons?
</listItem>
<bodyText confidence="0.997261409090909">
Objective polar utterances are statements or phrases
that describe positive or negative factual information
about something without conveying a private state.
The sentence The camera broke the first time I used
it gives an example of negative factual information;
generally, something breaking the first time it is used
is not good.
For the work in this paper, we focus on recog-
nizing subjectivity in general and distinguishing be-
tween positive and negative subjective utterances.
Positive subjective utterances are those in which any
of the following types of private states are expressed:
agreements, positive sentiments, positive sugges-
tions, arguing for something, beliefs from which
positive sentiments can be inferred, and positive re-
sponses to subjective questions. Negative subjective
utterances express private states that are the oppo-
site of those represented by the positive subjective
category: disagreements, negative sentiments, nega-
tive suggestions, arguing against something, beliefs
from which negative sentiments can be inferred, and
negative responses to subjective questions. Example
</bodyText>
<listItem confidence="0.973867">
(5) below contains two positive subjective utterances
</listItem>
<tableCaption confidence="0.998826">
Table 1: AMIDA Subjectivity Annotation Types
</tableCaption>
<table confidence="0.993692285714286">
Subjective Utterances
positive subjective
negative subjective
positive and negative subjective
uncertainty
other subjective
subjective fragment
Subjective Questions
positive subjective question
negative subjective question
general subjective question
Objective Polar Utterances
positive objective
negative objective
</table>
<bodyText confidence="0.9328745">
and one negative subjective utterance. Each annota-
tion is indicated by a pair of angle brackets.
</bodyText>
<listItem confidence="0.97824125">
(5) Um (POS-SUBJ it’s very easy to use).
Um (NEG-SUBJ but unfortunately it does
lack the advanced functions) (POS-SUBJ
which I I quite like having on the controls).
</listItem>
<bodyText confidence="0.927596666666667">
The positive and negative subjective category is for
marking cases of positive and negative subjectivity
that are so closely interconnected that it is difficult
or impossible to separate the two. For example, (6)
below is marked as both positive and negative sub-
jective.
</bodyText>
<listItem confidence="0.63310125">
(6) Um (POS-AND-NEG-SUBJ they’ve also
suggested that we um we only use the remote
control to control the television, not the VCR,
DVD or anything else).
</listItem>
<bodyText confidence="0.9997395">
In (Wilson, 2008), agreement is measured for each
class separately at the level of dialogue act segments.
If a dialogue act overlaps with an annotation of a
particular type, then the segment is considered to
be labelled with that type. Table 2 gives the Kappa
(Cohen, 1960) and % agreement for subjective seg-
ments, positive and negative subjective segments,2
and subjective questions.
</bodyText>
<footnote confidence="0.8593665">
2A positive subjective segment is any dialogue act segment
that overlaps with a positive subjective utterance or a positive-
and-negative subjective utterance. The negative subjective seg-
ments are defined similarly.
</footnote>
<page confidence="0.999179">
468
</page>
<tableCaption confidence="0.9891875">
Table 2: Interannotator agreement for the AMIDA sub-
jectivity annotations
</tableCaption>
<table confidence="0.9992488">
Kappa % Agree
Subjective 0.56 79
Pos Subjective 0.58 84
Neg Subjective 0.62 92
Subjective Question 0.56 95
</table>
<sectionHeader confidence="0.997328" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999970742857143">
We conduct two sets of classification experiments.
For the first set of experiments (Task 1), we auto-
matically distinguish between subjective and non-
subjective utterances. For the second set of ex-
periments (Task 2), we focus on distinguishing be-
tween positive and negative subjective utterances.
For both tasks, we use the manual dialogue act seg-
ments available as part of the AMI Corpus as the unit
of classification. For Task 1, a segment is considered
subjective if it overlaps with either a subjective utter-
ance or subjective question annotation. For Task 2,
the segments being classified are those that overlap
with positive or negative subjective utterances. For
this task, we exclude segments that are both positive
and negative. Although limiting the set of segments
to be classified to just those that are positive or nega-
tive makes the task somewhat artificial, it also allows
us to focus in on the performance of features specifi-
cally for this task.3 We use 6226 subjective and 8707
non-subjective dialog acts for Task 1 (with an aver-
age duration of 1.9s, standard deviation of 2.0s), and
3157 positive subjective and 1052 negative subjec-
tive dialog acts for Task 2 (average duration of 2.6s,
standard deviation of 2.3s).
The experiments are performed using 13-fold
cross validation. Each meeting constitutes a separate
fold for testing, e.g., all the segments from meeting 1
make up the test set for fold 1. Then, for a given fold,
the segments from the remaining 12 meetings are
used for training and parameter tuning, with roughly
a 85%, 7%, and 8% split between training, tuning,
and testing sets for each fold. The assignment to
training versus tuning set was random, with the only
constraint being that a segment could only be in the
tuning set for one fold of the data.
</bodyText>
<footnote confidence="0.9635">
3In practice, this excludes about 7% of the positive/negative
segments.
</footnote>
<bodyText confidence="0.999611285714286">
The experiments we perform involve two steps.
First, we train and optimize a classifier for each type
of feature using BoosTexter (Schapire and Singer,
2000) AdaBoost.MH. Then, we investigate the per-
formance of all possible combinations of features
using linear combinations of the individual feature
classifiers.
</bodyText>
<subsectionHeader confidence="0.795831">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.990607882352941">
The two modalities that are investigated, prosodic,
and textual, are represented by four different
sets of features: prosody (PROS), word n-
grams (WORDS), character n-grams (CHARS), and
phoneme n-grams (PHONES).
Based on previous research on prosody modelling
in a meeting context (Wrede and Shriberg, 2003)
and on the literature in emotion research (Banse and
Scherer, 1996) we extract PROS features that are
mainly based on pitch, energy and the distribution of
energy in the long-term averaged spectrum (LTAS)
(see Table 3). These features are extracted at the
word level and aggregated to the dialogue-act level
by taking the average over the words per dialogue
act. We then normalize the features per speaker per
meeting by converting the raw feature values to z-
scores (z = (x − p)/Q).
</bodyText>
<tableCaption confidence="0.998344">
Table 3: Prosodic features used in experiments.
</tableCaption>
<table confidence="0.99680025">
pitch mean, standard deviation, min-
imum, maximum, range, mean
absolute slope
intensity (en- mean, standard deviation, min-
ergy) imum, maximum, range, RMS
energy
distribution en- slope, Hammerberg index, cen-
ergy in LTAS tre of gravity, skewness
</table>
<bodyText confidence="0.9792276">
The textual features, WORDS and CHARS, and
the PHONES features are based on a manual tran-
scription of the speech. The PHONES were pro-
duced through dictionary lookup on the words in the
reference transcription. Both CHARS and PHONES
representations include word boundaries as informa-
tive tokens. The textual features for a given seg-
ment are simply all the WORDS/CHARS/PHONES
in that segment. Selection of n-grams is performed
by the learning algorithm.
</bodyText>
<page confidence="0.998102">
469
</page>
<subsectionHeader confidence="0.986465">
4.2 Single Source Classifiers
</subsectionHeader>
<bodyText confidence="0.99999748">
We train four single source classifiers using BoosT-
exter, one for each type of feature. For the WORDS,
CHARS, and PHONES, we optimize the classi-
fier by performing a grid search over the parame-
ter space, varying the number of rounds of boosting
(100, 500, 1000, 2000, 5000), the length of the n-
gram (1, 2, 3, 4, 5), and the type of n-gram. Boos-
Texter can be run with three different n-gram con-
figurations: n-gram, s-gram, and f-gram. For the
default configuration (n-gram), BoosTexter searches
for n-grams up to length n. For example, if n = 3,
BoosTexter will consider 1-grams, 2-grams, and 3-
grams. For the s-gram configuration, BoosTexter
will in addition consider sparse n-grams (i.e., n-
grams containing wildcards), such as the * idea. For
the f-gram configuration, BoosTexter will only con-
sider n-grams of a maximum fixed length, e.g., if
n = 3 BoosTexter will only consider 3-grams. For
the PROS classifier, only the number of rounds of
boosting was varied. The parameters are selected
for each fold separately; the parameter set that pro-
duces the highest subjective F1 score on the tuning
set for Task 1, and the highest positive subjective F1
score for Task 2, is used to train the final classifier
for that fold.
</bodyText>
<subsectionHeader confidence="0.993106">
4.3 Classifier combination
</subsectionHeader>
<bodyText confidence="0.998820565217391">
After the single source classifiers have been trained,
they have to be combined into an aggregate classi-
fier. To this end, we decided to apply a simple linear
interpolation strategy. Linear interpolation of mod-
els is the weighted combination of simple models to
form complex models, and has its roots in generative
language models (Jelinek and Mercer, 1980). (Raai-
jmakers, 2007) has demonstrated its use for discrim-
inative machine learning.
In the present binary class setting, BoosTexter
produces two decision values, one for every class.
For every individual single-source classifier (i.e.,
PROS, WORDS, CHARS and PHONES), separate
weights are estimated that are applied to the decision
values for the two classes produced by these classi-
fiers. These weights express the relative importance
of the single-source classifiers.
The prediction of an aggregate classifier for a
class c is then simply the sum of all weights for
all participating single-source classifiers applied to
the decision values these classifiers produce for this
class. The class with the maximum score wins, just
as in the simple non-aggregate case.
Formally, then, this linear interpolation strategy
finds for n single-source classifiers n interpolation
weights A1,... An that minimize the empirical loss
(measured by a loss function L), with Aj the weight
of classifier j (A E [0,1]), and Cjc(xi) the decision
value of class c produced by classifier j for datum xi
(a feature vector). The two classes are denoted with
0, 1. The true class for datum xi is denoted with xi.
The loss function is in our case based on subjective
F-measure (Task 1) or positive subjective F-measure
(Task 2) measured on heldout development training
and test data.
The aggregate prediction xi for datum xi on the
basis of n single-source classifiers then becomes
The search process for these weights can easily be
implemented with a simple grid search over admis-
sible ranges.
In the experiments described below, we investi-
gate all possible combinations of the four differ-
ent sets of features (PROS, WORDS, CHARS, and
PHONES) to determine which combination yields
the best performance for subjectivity and subjective
polarity recognition.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999808777777778">
Results for the two tasks are given in Tables 4 and 5
and in Figures 1 and 2. We use two baselines, listed
at the top of each table. The bullets in a given row
indicate the features that are being evaluated for a
given experiment. In Table 4, subjective F1, recall,
and precision are reported as well as overall accu-
racy. In Table 4, the F1, recall, and precision scores
are for the positive subjective class. All values in the
tables are averages over the 13 folds.
</bodyText>
<equation confidence="0.83800925">
xi = arg max(
c
n
Aj - Cjc=0(xi), E
j=1
Aj - Cjc=1(xi))
(1)
n
</equation>
<figure confidence="0.691288">
E
j=1
and the lambdas are defined as
An j = arg min k L(xi, xi; Aj, ... , An) (2)
a; c[0,1] i
</figure>
<page confidence="0.995443">
470
</page>
<tableCaption confidence="0.999494">
Table 4: Results Task 1: Subjective vs. Non-Subjective.
</tableCaption>
<table confidence="0.902416">
PROS WORDS CHARS PHONES F1 PREC REC ACC
BASE-SUBJ always chooses subjective class 60.3 43.4 100 43.4
BASE-RAND randomly chooses a class based on priors 41.8 42.9 41.3 50.6
• 54.6 55.3 54.5 63.1
• 60.5 68.5 54.5 71.0
single • 61.7 67.5 57.2 71.1
• 60.3 66.4 55.5 70.2
</table>
<figure confidence="0.755205818181818">
• • 63.9 72.1 57.6 73.4
• • 65.6 71.9 60.3 74.0
double • • • • 64.6 72.3 58.4 73.7
66.2 73.8 60.1 74.9
• • 65.2 73.2 58.8 74.3
• • 66.1 72.8 60.7 74.5
• • • 66.5 74.3 60.3 75.1
triple • • • • 65.5 73.5 59.0 74.5
• • 66.5 73.3 60.8 74.8
• • • 66.9 74.3 60.9 75.3
quartet • • • • 67.1 74.5 61.2 75.4
</figure>
<tableCaption confidence="0.962094">
Table 5: Results Task 2: Positive Subjective vs. Negative Subjective.
</tableCaption>
<table confidence="0.862459714285714">
PROS WORDS CHARS PHONES F1 PREC REC ACC
BASE-POS-SUBJ always chooses positive subjective class 85.6 75.0 100 75.0
BASE-RAND randomly chooses a class based on priors 75.1 74.4 76.1 62.4
single • • • • 84.8 74.8 98.1 73.9
85.6 79.6 93.1 76.8
85.9 81.9 90.5 78.0
85.5 80.5 91.3 77.0
</table>
<figure confidence="0.761548454545455">
double • • • • 88.7 83.0 95.4 81.9
• • • • 88.7 83.1 95.1 81.8
• • • • 88.5 83.3 94.4 81.6
89.5 84.2 95.7 83.3
89.2 83.7 95.5 82.8
89.0 84.2 94.6 82.6
triple • • • • 89.6 84.0 96.1 83.4
• • • • 89.3 83.6 95.8 82.8
• • • • 89.2 83.7 95.5 82.7
89.8 84.4 96.0 83.8
quartet • • • • 89.9 84.4 96.2 83.8
</figure>
<bodyText confidence="0.979478952380953">
It is quite obvious that the combination of differ-
ent sources of information is beneficial, and in gen-
eral, the more information the better the results. The
best performing classifier for Task 1 uses all the fea-
tures, achieving a subjective F1 of 67.1. For Task 2,
the best performing classifier also uses all the fea-
tures, although it does not perform significantly bet-
ter than the classifier using only WORDS, CHARS,
and PHONES.4 This classifier achieves a positive-
subjective F1 of 89.9.
We measured the effects of adding more infor-
mation to the single source classifiers. These re-
sults are listed in Table 6. Of the various feature
types, prosody seems to be the least informative for
both subjectivity and polarity classification. In ad-
dition to producing the single-source classifier with
the lowest performance for both tasks, Table 6 shows
that when prosody is added, of all the features it is
least likely to yield significant improvements.
4We measured significance with the non-parametric
Wilcoxon signed rank test, P &lt; 0.05.
Throughout the experiments, adding an additional
type of textual feature always yields higher results.
In all cases but two, these improvements are sig-
nificant. The best performing of the features are
the character n-grams. Of the single-source exper-
iments, the character n-grams achieve the best per-
formance, with significant improvements in F1 over
the other single-source classifiers for both Task 1
and Task 2. Also, adding character n-grams to other
feature combinations always gives significant im-
provements in performance.
An obvious question that remains is what the ef-
fect is of classifier interpolation on the results. To
answer this question, we conducted two additional
experiments for both tasks. First, we investigated
the performance of an uninterpolated combination
of the four single-source classifiers. In essence, this
combines the separate feature spaces without explic-
itly weighting them. Second, we investigated the re-
sults of training a single BoosTexter model using all
the features, essentially merging all feature spaces
</bodyText>
<page confidence="0.999364">
471
</page>
<tableCaption confidence="0.99015">
Table 6: Addition of features separately (for Task 1 and 2): ‘+’ for a row-column pair (r, c) means that the addition
of column feature c to the row features r significantly improved r’s F1; ‘-’ indicates no significant improvement; ‘X’
means ‘not applicable’
</tableCaption>
<table confidence="0.8952493125">
Task +PROS + WORDS +CHARS +PHONES
1 2 1 2 1 2 1 2
PROS X X + + + + + +
WORDS - + X X + + + +
CHARS - + - + X X - +
PHONES - + + + + + X X
PROS+WORDS X X X X + + + +
PROS+CHARS X X + + X X + +
PROS+PHONES X X + + + + X X
WORDS+CHARS + - X X X X + +
WORDS+PHONES + - X X + + X X
CHARS+PHONES + + + + X X X X
PROS+WORDS+CHARS X X X X X X + +
PROS+WORDS+PHONES X X X X + + X X
PROS+CHARS+PHONES X X + + X X X X
WORDS+CHARS+PHONES + - X X X X X X
</table>
<figure confidence="0.8595">
F1 measure
40 45 50 55 60 65 70
F1 measure
70 75 80 85 90 95 100
</figure>
<figureCaption confidence="0.99962375">
Figure 1: Results (F1) experiment 1: subjective vs. non-
subjective.
Figure 2: Results (F1) experiment 2: positive subjective
vs. negative subjective.
</figureCaption>
<bodyText confidence="0.999956571428572">
into one agglomerate feature space. The results for
these experiments are given in Table 7, along with
the results from the all-feature interpolated classifi-
cation for comparison.
The results in Table 7 show that interpolation
outperforms both the unweighted and single-model
combinations for both tasks. For Task 1, the ef-
fect of interpolation compared to a single model is
marginal (a .03 point difference in F1). However,
compared to the uninterpolated combination, inter-
polation gives a clear 3.1 points improvement of F1.
For Task 2, interpolation outperforms both the unin-
terpolated and single-model classifiers, with 2 and 3
points improvements in F1, respectively.
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999878714285714">
Previous work has demonstrated that textual units
below the word level, such as character n-grams,
are valuable sources of information. Character-
level models have successfully been used for named-
entity recognition (Klein et al., 2003), predicting
authorship (Keselj et al., 2003; Stamatatos, 2006),
text categorization (Zhang and Lee, 2006), web page
genre identification (Kanaris and Stamatatos, 2007),
and sentence-level subjectivity recognition (Raaij-
makers and Kraaij, 2008) In spoken-language data,
Hsueh (2008) achieves good results using chains
of phonemes to automatically segment meetings ac-
cording to topic. However, to the best of our knowl-
edge there has been no investigation to date on the
</bodyText>
<page confidence="0.998753">
472
</page>
<tableCaption confidence="0.983674">
Table 7: Results of interpolated classifiers compared to
uninterpolated and single-model classifiers for all fea-
tures.
</tableCaption>
<table confidence="0.996046285714286">
Task Combination ACC REC PREC F1
interpolated 75.4 61.2 74.5 67.1
1 uninterpolated 73.0 58.7 70.6 64.0
single model 74.7 62.1 72.7 66.8
interpolated 83.8 96.2 84.4 89.9
2 uninterpolated 79.8 98.0 79.7 87.9
single model 79.5 91.0 83.3 86.9
</table>
<bodyText confidence="0.99933821875">
combination of character-level, phoneme-level, and
word-level models for any natural language classifi-
cation tasks.
In text, there has been a significant amount of
research on subjectivity and sentiment recognition,
ranging from work at the phrase level to work on
classifying sentences and documents. Sentence-
level subjectivity classification (e.g., (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and
sentiment classification (e.g., (Yu and Hatzivas-
siloglou, 2003; Kim and Hovy, 2004; Hu and Liu,
2004; Popescu and Etzioni, 2005)) is the research
in text most closely related to our work. Of the
sentence-level research, the most similar is work
by Raaijmakers and Kraaij (2008) comparing word-
spanning character n-grams to word-internal char-
acter n-grams for subjectivity classification in news
data. They found that character n-grams spanning
words perform the best.
Research on recognizing subjective content in
multiparty conversation includes work by Somasun-
daran et al. (2007) on recognizing sentiments and
arguing in meetings, work by Neiberg el al. (2006)
on recognizing positive, negative, and neutral emo-
tions in meetings, work on recognizing agreements
and disagreements in meetings (Hillard et al., 2003;
Galley et al., 2004; Hahn et al., 2006), and work
by Wrede and Shriberg (2003) on recognizing meet-
ing hotspots. Somasundaran et al. use lexical and
discourse features to recognize sentences and turns
where meeting participants express sentiments or ar-
guing. They also use the AMI corpus in their work;
however, the use of different annotations and task
definitions makes it impossible to directly compare
their results and ours. Neiberg et al. use acoustic–
prosodic features (Mel-frequency Cepstral Coeffi-
cients (MFCCs) and pitch features) and lexical n-
grams for recognizing emotions in the ISL Meeting
Corpus (Laskowski and Burger, 2006).
Agreements and disagreements are a subset of the
private states represented by the positive and neg-
ative subjective categories used in this work. To
recognise agreements and disagreements automati-
cally, Hillard et al. train 3-way decision tree clas-
sifiers (agreement, disagreement, other) using both
word-based and prosodic features. Galley et al.
model this task as a sequence tagging problem, and
investigate whether features capturing speaker inter-
actions are useful for recognizing agreements and
disagreements. Hahn et al. investigate the use of
contrast classifiers (Peng et al., 2003) for the task,
using only lexical features.
Hotspots are places in a meeting in which the par-
ticipants are highly involved in the discussion. Al-
though high involvement does not necessarily equate
subjective content, in practice, we expect more sen-
timents, opinions, and arguments to be expressed
when participants are highly involved in the discus-
sion. In their work on recognizing meeting hotspots,
Wrede and Shriberg focus on evaluating the contri-
bution of various prosodic features, ignoring lexi-
cal features completely. The results of their study
helped to inform our choice of prosodic features for
the experiments in this paper.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999966117647059">
In this paper, we investigated the use of prosodic
features, word n-grams, character n-grams, and
phoneme n-grams for subjectivity recognition and
polarity classification of dialog acts in multiparty
conversation. We show that character n-grams
outperform prosodic features, word n-grams and
phoneme n-grams in subjectiviy recognition and po-
larity classification. Combining these features sig-
nificantly improves performance. Comparing the
additive value of the four information sources avail-
able, prosodic information seem to be least in-
formative while character-level information indeed
proves to be a very valuable source. For subjectiv-
ity recognition, a combination of prosodic, word-
level, character-level, and phoneme-level informa-
tion yields the best performance. For polarity clas-
sification, the best performance is achieved with a
</bodyText>
<page confidence="0.997929">
473
</page>
<bodyText confidence="0.934896">
combination of words, characters and phonemes.
</bodyText>
<sectionHeader confidence="0.98711" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874254901961">
R. Banse and K. R. Scherer. 1996. Acoustic profiles in
vocal emotion expression. Journal of Personality and
Social Psychology, pages 614–636.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Wellner.
2005. The AMI meeting corpus. In Proceedings of
the Measuring Behavior Symposium on “Annotating
and Measuring Meeting Behavior”.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37–46.
J. Eisner. 1996. An empirical comparison of probability
models for dependency grammar. In Technical Report
IRCS-96-11, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use ofbayesian networks to model
pragmatic dependencies. In Proceedings ofACL.
S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agree-
ment/disagreement classification: Exploiting unla-
beled data using contrast classifiers. In Proceedings
ofHLT/NAACL.
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. De-
tection of agreement vs. disagreement in meetings:
Training with unlabeled data. In Proceedings of
HLT/NAACL.
P. Hsueh and J. Moore. 2006. Automatic topic seg-
mentation and lablelling in multiparty dialogue. In
Proceedings of IEEE/ACM Workshop on Spoken Lan-
guage Technology.
P. Hsueh. 2008. Audio-based unsupervised segmentation
of meeting dialogue. In Proceedings ofICASSP.
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proceedings ofKDD.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data.
In Proceedings, Workshop on Pattern Recognition in
Practice, pages 381–397.
I. Kanaris and E. Stamatatos. 2007. Webpage genre iden-
tification using variable-length character n-grams. In
Proceedings ofICTAI.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-
gram-based author profiles for authorship attribution.
In Proceedings of PACLING.
S. Kim and Eduard Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of Coling.
D. Klein, J. Smarr, H. Nguyen, and C.D. Manning. 2003.
Named entity recognition with character-level models.
In Proceedings of CoNLL.
K. Laskowski and S. Burger. 2006. Annotation and anal-
ysis of emotionally relevant behavior in the ISL meet-
ing corpus. In Proceedings ofLREC 2006.
D. Neiberg, K. Elenius, and K. Laskowski. 2006. Emo-
tion recognition in spontaneous speech using GMMs.
In Proceedings ofINTERSPEECH.
K. Peng, S. Vucetic, B. Han, H. Xie, and Z Obradovic.
2003. Exploiting unlabeled data for improving ac-
curacy of predictive data mining. In Proceedings of
ICDM.
A. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings of
HLT/EMNLP.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting
action items in multi-party meetings: Annotation and
initial experiments. In Proceedings ofMLMI.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
S. Raaijmakers and W. Kraaij. 2008. A shallow ap-
proach to subjectivity classification. In Proceedings
ofICWSM.
S. Raaijmakers. 2007. Sentiment classification with in-
terpolated information diffusion kernels. In Proceed-
ings of the First International Workshop on Data Min-
ing and Audience Intelligence for Advertising (AD-
KDD’07).
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
EMNLP.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135–168.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In Pro-
ceedings of SIGdial.
E. Stamatatos. 2006. Ensemble-based author identifica-
tion using character n-grams. In Proceedings of TIR.
T. Wilson. 2008. Annotating subjective content in meet-
ings. In Proceedings ofLREC.
B. Wrede and E. Shriberg. 2003. Spotting “hot spots”
in meetings: Human judgments and prosodic cues. In
Proceedings of EUROSPEECH.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings ofEMNLP.
D. Zhang and W. S. Lee. 2006. Extracting key-substring-
group features for text classification. In Proceedings
ofKDD.
</reference>
<page confidence="0.998978">
474
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.117950">
<title confidence="0.999974">Multimodal Subjectivity Analysis of Multiparty Conversation</title>
<author confidence="0.871052">Stephan</author>
<affiliation confidence="0.7072245">TNO Information Communication</affiliation>
<address confidence="0.577071">Delft, The Netherlands</address>
<author confidence="0.877401">Khiet Truong</author>
<affiliation confidence="0.722161">TNO Defense, Security and Safety</affiliation>
<address confidence="0.808782">Soesterberg, The Netherlands</address>
<email confidence="0.973745">khiet.truong@tno.nl</email>
<author confidence="0.95595">Theresa</author>
<affiliation confidence="0.999137">School of University of</affiliation>
<address confidence="0.735">Edingburgh, UK</address>
<email confidence="0.9548055">twilson@inf.ed.ac.ukstephan.raaijmakers@tno.nl</email>
<abstract confidence="0.955956833333333">We investigate the combination of several sources of information for the purpose of subjectivity recognition and polarity classification in meetings. We focus on features from two modalities, transcribed words and acoustics, and we compare the performance of three different textual representations: words, characters, and phonemes. Our experiments show that character-level features outperform wordlevel features for these tasks, and that a careful fusion of all features yields the best perfor-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Banse</author>
<author>K R Scherer</author>
</authors>
<title>Acoustic profiles in vocal emotion expression.</title>
<date>1996</date>
<journal>Journal of Personality and Social Psychology,</journal>
<pages>614--636</pages>
<contexts>
<context position="14500" citStr="Banse and Scherer, 1996" startWordPosition="2248" endWordPosition="2251">classifier for each type of feature using BoosTexter (Schapire and Singer, 2000) AdaBoost.MH. Then, we investigate the performance of all possible combinations of features using linear combinations of the individual feature classifiers. 4.1 Features The two modalities that are investigated, prosodic, and textual, are represented by four different sets of features: prosody (PROS), word ngrams (WORDS), character n-grams (CHARS), and phoneme n-grams (PHONES). Based on previous research on prosody modelling in a meeting context (Wrede and Shriberg, 2003) and on the literature in emotion research (Banse and Scherer, 1996) we extract PROS features that are mainly based on pitch, energy and the distribution of energy in the long-term averaged spectrum (LTAS) (see Table 3). These features are extracted at the word level and aggregated to the dialogue-act level by taking the average over the words per dialogue act. We then normalize the features per speaker per meeting by converting the raw feature values to zscores (z = (x − p)/Q). Table 3: Prosodic features used in experiments. pitch mean, standard deviation, minimum, maximum, range, mean absolute slope intensity (en- mean, standard deviation, minergy) imum, max</context>
</contexts>
<marker>Banse, Scherer, 1996</marker>
<rawString>R. Banse and K. R. Scherer. 1996. Acoustic profiles in vocal emotion expression. Journal of Personality and Social Psychology, pages 614–636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>W Kraaij</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>I McCowan</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The AMI meeting corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the Measuring Behavior Symposium on “Annotating and Measuring Meeting Behavior”.</booktitle>
<contexts>
<context position="6885" citStr="Carletta et al., 2005" startWordPosition="1056" endWordPosition="1059">nted by the 4-gram, ould, and the set of adverbs ending in -ly can be represented by the 3-gram ly#. A challenging thought is to extend the use of ngrams to the level of phonemes, which comprise the first symbolic level in the process of sound to grapheme conversion. If n-grams of phonemes compare favorably to word n-grams for the purpose of sentiment classification, then significant speedups can be obtained for online sentiment classification, since tokenization of the raw speech signal can make a halt at the phoneme level. 3 Data For this work we use 13 meetings from the AMI Meeting Corpus (Carletta et al., 2005). Each meeting has four participants and is approximately 30 minutes long. The participants play specific roles (e.g., Project Manager, Marketing Expert) and together function as a design team. Within the set of 13 meetings, there are a total of 20 participants, with each participant taking part in two or three meetings as part of the same design team. Meetings with the same set of participants represent different stages in the design process (e.g., Conceptual Design, Detailed Design). The meetings used in the experiments have been annotated for subjective content using the AMIDA annotation sc</context>
</contexts>
<marker>Carletta, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kraaij, Kronenthal, Lathoud, Lincoln, Lisowska, McCowan, Post, Reidsma, Wellner, 2005</marker>
<rawString>J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2005. The AMI meeting corpus. In Proceedings of the Measuring Behavior Symposium on “Annotating and Measuring Meeting Behavior”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="11415" citStr="Cohen, 1960" startWordPosition="1760" endWordPosition="1761">gative subjectivity that are so closely interconnected that it is difficult or impossible to separate the two. For example, (6) below is marked as both positive and negative subjective. (6) Um (POS-AND-NEG-SUBJ they’ve also suggested that we um we only use the remote control to control the television, not the VCR, DVD or anything else). In (Wilson, 2008), agreement is measured for each class separately at the level of dialogue act segments. If a dialogue act overlaps with an annotation of a particular type, then the segment is considered to be labelled with that type. Table 2 gives the Kappa (Cohen, 1960) and % agreement for subjective segments, positive and negative subjective segments,2 and subjective questions. 2A positive subjective segment is any dialogue act segment that overlaps with a positive subjective utterance or a positiveand-negative subjective utterance. The negative subjective segments are defined similarly. 468 Table 2: Interannotator agreement for the AMIDA subjectivity annotations Kappa % Agree Subjective 0.56 79 Pos Subjective 0.58 84 Neg Subjective 0.62 92 Subjective Question 0.56 95 4 Experiments We conduct two sets of classification experiments. For the first set of expe</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>An empirical comparison of probability models for dependency grammar. In</title>
<date>1996</date>
<tech>Technical Report IRCS-96-11,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<contexts>
<context position="5054" citStr="Eisner, 1996" startWordPosition="763" endWordPosition="764">for the two-word phrase sentiment analysis. The special symbol # represents a word boundary. While it is not directly obvious that there is much information in these truncated substrings, character n-grams have successfully been used for finegrained classification tasks, such as named-entity recognition (Klein et al., 2003) and subjective sentence recognition (Raaijmakers and Kraaij, 2008), as well as a variety of document-level tasks (Stamatatos, 2006; Zhang and Lee, 2006; Kanaris and Stamatatos, 2007). The informativeness of these low-level features comes in part from a form of attenuation (Eisner, 1996): a slight abstraction of the underlying data that leads to the formation of string equivalence classes. For instance, words in a sentence will invariably share many character n-grams. Since every unique character n-gram in an utterance constitutes a separate feature, this leads to the formation of string classes, which is a form of abstraction. For example, Zhang and Lee (2006) investigate similar subword representations, called key substring group features. By compressing substrings in a corpus in a trie (a prefix tree), and labeling entire sets of distributionally equivalent substrings with</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. An empirical comparison of probability models for dependency grammar. In Technical Report IRCS-96-11, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use ofbayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="27055" citStr="Galley et al., 2004" startWordPosition="4403" endWordPosition="4406">rch, the most similar is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing positive, negative, and neutral emotions in meetings, work on recognizing agreements and disagreements in meetings (Hillard et al., 2003; Galley et al., 2004; Hahn et al., 2006), and work by Wrede and Shriberg (2003) on recognizing meeting hotspots. Somasundaran et al. use lexical and discourse features to recognize sentences and turns where meeting participants express sentiments or arguing. They also use the AMI corpus in their work; however, the use of different annotations and task definitions makes it impossible to directly compare their results and ours. Neiberg et al. use acoustic– prosodic features (Mel-frequency Cepstral Coefficients (MFCCs) and pitch features) and lexical ngrams for recognizing emotions in the ISL Meeting Corpus (Laskows</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use ofbayesian networks to model pragmatic dependencies. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hahn</author>
<author>R Ladner</author>
<author>M Ostendorf</author>
</authors>
<title>Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="27075" citStr="Hahn et al., 2006" startWordPosition="4407" endWordPosition="4410"> is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing positive, negative, and neutral emotions in meetings, work on recognizing agreements and disagreements in meetings (Hillard et al., 2003; Galley et al., 2004; Hahn et al., 2006), and work by Wrede and Shriberg (2003) on recognizing meeting hotspots. Somasundaran et al. use lexical and discourse features to recognize sentences and turns where meeting participants express sentiments or arguing. They also use the AMI corpus in their work; however, the use of different annotations and task definitions makes it impossible to directly compare their results and ours. Neiberg et al. use acoustic– prosodic features (Mel-frequency Cepstral Coefficients (MFCCs) and pitch features) and lexical ngrams for recognizing emotions in the ISL Meeting Corpus (Laskowski and Burger, 2006)</context>
</contexts>
<marker>Hahn, Ladner, Ostendorf, 2006</marker>
<rawString>S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>E Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: Training with unlabeled data.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="27034" citStr="Hillard et al., 2003" startWordPosition="4399" endWordPosition="4402">e sentence-level research, the most similar is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing positive, negative, and neutral emotions in meetings, work on recognizing agreements and disagreements in meetings (Hillard et al., 2003; Galley et al., 2004; Hahn et al., 2006), and work by Wrede and Shriberg (2003) on recognizing meeting hotspots. Somasundaran et al. use lexical and discourse features to recognize sentences and turns where meeting participants express sentiments or arguing. They also use the AMI corpus in their work; however, the use of different annotations and task definitions makes it impossible to directly compare their results and ours. Neiberg et al. use acoustic– prosodic features (Mel-frequency Cepstral Coefficients (MFCCs) and pitch features) and lexical ngrams for recognizing emotions in the ISL Me</context>
</contexts>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>D. Hillard, M. Ostendorf, and E. Shriberg. 2003. Detection of agreement vs. disagreement in meetings: Training with unlabeled data. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hsueh</author>
<author>J Moore</author>
</authors>
<title>Automatic topic segmentation and lablelling in multiparty dialogue.</title>
<date>2006</date>
<booktitle>In Proceedings of IEEE/ACM Workshop on Spoken Language Technology.</booktitle>
<contexts>
<context position="1511" citStr="Hsueh and Moore, 2006" startWordPosition="213" endWordPosition="216">ction Opinions, sentiments and other types of subjective content are an important part of any meeting. Meeting participants express pros and cons about ideas, they support or oppose decisions, and they make suggestions that may or may not be adopted. When recorded and archived, meetings become a part of the organizational knowledge, but their value is limited by the ability of tools to search and summarize meeting content, including subjective content. While progress has been made on recognizing primarily objective meeting content, for example, information about the topics that are discussed (Hsueh and Moore, 2006) and who is assigned to work on given tasks (Purver et al., 2006), there has been &apos;This work was supported by the Dutch BSIK-project MultimediaN, and the European IST Programme Project FP6- 0033812. This paper only reflects the authors’ views and funding agencies are not liable for any use that may be made of the information contained herein. fairly little work specifically directed toward recognizing subjective content. In contrast, there has been a wealth of research over the past several years on automatic subjectivity and sentiment analysis in text, including on-line media. Partly inspired</context>
</contexts>
<marker>Hsueh, Moore, 2006</marker>
<rawString>P. Hsueh and J. Moore. 2006. Automatic topic segmentation and lablelling in multiparty dialogue. In Proceedings of IEEE/ACM Workshop on Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hsueh</author>
</authors>
<title>Audio-based unsupervised segmentation of meeting dialogue.</title>
<date>2008</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<contexts>
<context position="25260" citStr="Hsueh (2008)" startWordPosition="4133" endWordPosition="4134"> single-model classifiers, with 2 and 3 points improvements in F1, respectively. 6 Related Work Previous work has demonstrated that textual units below the word level, such as character n-grams, are valuable sources of information. Characterlevel models have successfully been used for namedentity recognition (Klein et al., 2003), predicting authorship (Keselj et al., 2003; Stamatatos, 2006), text categorization (Zhang and Lee, 2006), web page genre identification (Kanaris and Stamatatos, 2007), and sentence-level subjectivity recognition (Raaijmakers and Kraaij, 2008) In spoken-language data, Hsueh (2008) achieves good results using chains of phonemes to automatically segment meetings according to topic. However, to the best of our knowledge there has been no investigation to date on the 472 Table 7: Results of interpolated classifiers compared to uninterpolated and single-model classifiers for all features. Task Combination ACC REC PREC F1 interpolated 75.4 61.2 74.5 67.1 1 uninterpolated 73.0 58.7 70.6 64.0 single model 74.7 62.1 72.7 66.8 interpolated 83.8 96.2 84.4 89.9 2 uninterpolated 79.8 98.0 79.7 87.9 single model 79.5 91.0 83.3 86.9 combination of character-level, phoneme-level, and </context>
</contexts>
<marker>Hsueh, 2008</marker>
<rawString>P. Hsueh. 2008. Audio-based unsupervised segmentation of meeting dialogue. In Proceedings ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings ofKDD.</booktitle>
<contexts>
<context position="26321" citStr="Hu and Liu, 2004" startWordPosition="4293" endWordPosition="4296"> interpolated 83.8 96.2 84.4 89.9 2 uninterpolated 79.8 98.0 79.7 87.9 single model 79.5 91.0 83.3 86.9 combination of character-level, phoneme-level, and word-level models for any natural language classification tasks. In text, there has been a significant amount of research on subjectivity and sentiment recognition, ranging from work at the phrase level to work on classifying sentences and documents. Sentencelevel subjectivity classification (e.g., (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and sentiment classification (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005)) is the research in text most closely related to our work. Of the sentence-level research, the most similar is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing positive, negative, and</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings ofKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data. In</title>
<date>1980</date>
<booktitle>Proceedings, Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<contexts>
<context position="17299" citStr="Jelinek and Mercer, 1980" startWordPosition="2712" endWordPosition="2715">e selected for each fold separately; the parameter set that produces the highest subjective F1 score on the tuning set for Task 1, and the highest positive subjective F1 score for Task 2, is used to train the final classifier for that fold. 4.3 Classifier combination After the single source classifiers have been trained, they have to be combined into an aggregate classifier. To this end, we decided to apply a simple linear interpolation strategy. Linear interpolation of models is the weighted combination of simple models to form complex models, and has its roots in generative language models (Jelinek and Mercer, 1980). (Raaijmakers, 2007) has demonstrated its use for discriminative machine learning. In the present binary class setting, BoosTexter produces two decision values, one for every class. For every individual single-source classifier (i.e., PROS, WORDS, CHARS and PHONES), separate weights are estimated that are applied to the decision values for the two classes produced by these classifiers. These weights express the relative importance of the single-source classifiers. The prediction of an aggregate classifier for a class c is then simply the sum of all weights for all participating single-source </context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Proceedings, Workshop on Pattern Recognition in Practice, pages 381–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kanaris</author>
<author>E Stamatatos</author>
</authors>
<title>Webpage genre identification using variable-length character n-grams.</title>
<date>2007</date>
<booktitle>In Proceedings ofICTAI.</booktitle>
<contexts>
<context position="4949" citStr="Kanaris and Stamatatos, 2007" startWordPosition="745" endWordPosition="748">n-grams is the set of 3-grams {#se, sen, ent, nti, tim, ime, men, ent, nt#, t#a, #an, ana, nal, aly, lys, ysi, sis, is#} for the two-word phrase sentiment analysis. The special symbol # represents a word boundary. While it is not directly obvious that there is much information in these truncated substrings, character n-grams have successfully been used for finegrained classification tasks, such as named-entity recognition (Klein et al., 2003) and subjective sentence recognition (Raaijmakers and Kraaij, 2008), as well as a variety of document-level tasks (Stamatatos, 2006; Zhang and Lee, 2006; Kanaris and Stamatatos, 2007). The informativeness of these low-level features comes in part from a form of attenuation (Eisner, 1996): a slight abstraction of the underlying data that leads to the formation of string equivalence classes. For instance, words in a sentence will invariably share many character n-grams. Since every unique character n-gram in an utterance constitutes a separate feature, this leads to the formation of string classes, which is a form of abstraction. For example, Zhang and Lee (2006) investigate similar subword representations, called key substring group features. By compressing substrings in a </context>
<context position="25146" citStr="Kanaris and Stamatatos, 2007" startWordPosition="4117" endWordPosition="4120">nation, interpolation gives a clear 3.1 points improvement of F1. For Task 2, interpolation outperforms both the uninterpolated and single-model classifiers, with 2 and 3 points improvements in F1, respectively. 6 Related Work Previous work has demonstrated that textual units below the word level, such as character n-grams, are valuable sources of information. Characterlevel models have successfully been used for namedentity recognition (Klein et al., 2003), predicting authorship (Keselj et al., 2003; Stamatatos, 2006), text categorization (Zhang and Lee, 2006), web page genre identification (Kanaris and Stamatatos, 2007), and sentence-level subjectivity recognition (Raaijmakers and Kraaij, 2008) In spoken-language data, Hsueh (2008) achieves good results using chains of phonemes to automatically segment meetings according to topic. However, to the best of our knowledge there has been no investigation to date on the 472 Table 7: Results of interpolated classifiers compared to uninterpolated and single-model classifiers for all features. Task Combination ACC REC PREC F1 interpolated 75.4 61.2 74.5 67.1 1 uninterpolated 73.0 58.7 70.6 64.0 single model 74.7 62.1 72.7 66.8 interpolated 83.8 96.2 84.4 89.9 2 unint</context>
</contexts>
<marker>Kanaris, Stamatatos, 2007</marker>
<rawString>I. Kanaris and E. Stamatatos. 2007. Webpage genre identification using variable-length character n-grams. In Proceedings ofICTAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Keselj</author>
<author>F Peng</author>
<author>N Cercone</author>
<author>C Thomas</author>
</authors>
<title>Ngram-based author profiles for authorship attribution.</title>
<date>2003</date>
<booktitle>In Proceedings of PACLING.</booktitle>
<contexts>
<context position="25022" citStr="Keselj et al., 2003" startWordPosition="4101" endWordPosition="4104">mpared to a single model is marginal (a .03 point difference in F1). However, compared to the uninterpolated combination, interpolation gives a clear 3.1 points improvement of F1. For Task 2, interpolation outperforms both the uninterpolated and single-model classifiers, with 2 and 3 points improvements in F1, respectively. 6 Related Work Previous work has demonstrated that textual units below the word level, such as character n-grams, are valuable sources of information. Characterlevel models have successfully been used for namedentity recognition (Klein et al., 2003), predicting authorship (Keselj et al., 2003; Stamatatos, 2006), text categorization (Zhang and Lee, 2006), web page genre identification (Kanaris and Stamatatos, 2007), and sentence-level subjectivity recognition (Raaijmakers and Kraaij, 2008) In spoken-language data, Hsueh (2008) achieves good results using chains of phonemes to automatically segment meetings according to topic. However, to the best of our knowledge there has been no investigation to date on the 472 Table 7: Results of interpolated classifiers compared to uninterpolated and single-model classifiers for all features. Task Combination ACC REC PREC F1 interpolated 75.4 6</context>
</contexts>
<marker>Keselj, Peng, Cercone, Thomas, 2003</marker>
<rawString>V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. Ngram-based author profiles for authorship attribution. In Proceedings of PACLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling.</booktitle>
<contexts>
<context position="26303" citStr="Kim and Hovy, 2004" startWordPosition="4289" endWordPosition="4292"> 74.7 62.1 72.7 66.8 interpolated 83.8 96.2 84.4 89.9 2 uninterpolated 79.8 98.0 79.7 87.9 single model 79.5 91.0 83.3 86.9 combination of character-level, phoneme-level, and word-level models for any natural language classification tasks. In text, there has been a significant amount of research on subjectivity and sentiment recognition, ranging from work at the phrase level to work on classifying sentences and documents. Sentencelevel subjectivity classification (e.g., (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and sentiment classification (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005)) is the research in text most closely related to our work. Of the sentence-level research, the most similar is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing posit</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>S. Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>J Smarr</author>
<author>H Nguyen</author>
<author>C D Manning</author>
</authors>
<title>Named entity recognition with character-level models.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="4766" citStr="Klein et al., 2003" startWordPosition="719" endWordPosition="722">rated that textual units below the word level, such as character n-grams, are valuable sources of information for various text classification tasks. An example of character n-grams is the set of 3-grams {#se, sen, ent, nti, tim, ime, men, ent, nt#, t#a, #an, ana, nal, aly, lys, ysi, sis, is#} for the two-word phrase sentiment analysis. The special symbol # represents a word boundary. While it is not directly obvious that there is much information in these truncated substrings, character n-grams have successfully been used for finegrained classification tasks, such as named-entity recognition (Klein et al., 2003) and subjective sentence recognition (Raaijmakers and Kraaij, 2008), as well as a variety of document-level tasks (Stamatatos, 2006; Zhang and Lee, 2006; Kanaris and Stamatatos, 2007). The informativeness of these low-level features comes in part from a form of attenuation (Eisner, 1996): a slight abstraction of the underlying data that leads to the formation of string equivalence classes. For instance, words in a sentence will invariably share many character n-grams. Since every unique character n-gram in an utterance constitutes a separate feature, this leads to the formation of string class</context>
<context position="24978" citStr="Klein et al., 2003" startWordPosition="4095" endWordPosition="4098">. For Task 1, the effect of interpolation compared to a single model is marginal (a .03 point difference in F1). However, compared to the uninterpolated combination, interpolation gives a clear 3.1 points improvement of F1. For Task 2, interpolation outperforms both the uninterpolated and single-model classifiers, with 2 and 3 points improvements in F1, respectively. 6 Related Work Previous work has demonstrated that textual units below the word level, such as character n-grams, are valuable sources of information. Characterlevel models have successfully been used for namedentity recognition (Klein et al., 2003), predicting authorship (Keselj et al., 2003; Stamatatos, 2006), text categorization (Zhang and Lee, 2006), web page genre identification (Kanaris and Stamatatos, 2007), and sentence-level subjectivity recognition (Raaijmakers and Kraaij, 2008) In spoken-language data, Hsueh (2008) achieves good results using chains of phonemes to automatically segment meetings according to topic. However, to the best of our knowledge there has been no investigation to date on the 472 Table 7: Results of interpolated classifiers compared to uninterpolated and single-model classifiers for all features. Task Com</context>
</contexts>
<marker>Klein, Smarr, Nguyen, Manning, 2003</marker>
<rawString>D. Klein, J. Smarr, H. Nguyen, and C.D. Manning. 2003. Named entity recognition with character-level models. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Laskowski</author>
<author>S Burger</author>
</authors>
<title>Annotation and analysis of emotionally relevant behavior in the ISL meeting corpus.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC</booktitle>
<contexts>
<context position="27675" citStr="Laskowski and Burger, 2006" startWordPosition="4499" endWordPosition="4502">., 2004; Hahn et al., 2006), and work by Wrede and Shriberg (2003) on recognizing meeting hotspots. Somasundaran et al. use lexical and discourse features to recognize sentences and turns where meeting participants express sentiments or arguing. They also use the AMI corpus in their work; however, the use of different annotations and task definitions makes it impossible to directly compare their results and ours. Neiberg et al. use acoustic– prosodic features (Mel-frequency Cepstral Coefficients (MFCCs) and pitch features) and lexical ngrams for recognizing emotions in the ISL Meeting Corpus (Laskowski and Burger, 2006). Agreements and disagreements are a subset of the private states represented by the positive and negative subjective categories used in this work. To recognise agreements and disagreements automatically, Hillard et al. train 3-way decision tree classifiers (agreement, disagreement, other) using both word-based and prosodic features. Galley et al. model this task as a sequence tagging problem, and investigate whether features capturing speaker interactions are useful for recognizing agreements and disagreements. Hahn et al. investigate the use of contrast classifiers (Peng et al., 2003) for th</context>
</contexts>
<marker>Laskowski, Burger, 2006</marker>
<rawString>K. Laskowski and S. Burger. 2006. Annotation and analysis of emotionally relevant behavior in the ISL meeting corpus. In Proceedings ofLREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Neiberg</author>
<author>K Elenius</author>
<author>K Laskowski</author>
</authors>
<title>Emotion recognition in spontaneous speech using GMMs.</title>
<date>2006</date>
<booktitle>In Proceedings ofINTERSPEECH.</booktitle>
<marker>Neiberg, Elenius, Laskowski, 2006</marker>
<rawString>D. Neiberg, K. Elenius, and K. Laskowski. 2006. Emotion recognition in spontaneous speech using GMMs. In Proceedings ofINTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Peng</author>
<author>S Vucetic</author>
<author>B Han</author>
<author>H Xie</author>
<author>Z Obradovic</author>
</authors>
<title>Exploiting unlabeled data for improving accuracy of predictive data mining.</title>
<date>2003</date>
<booktitle>In Proceedings of ICDM.</booktitle>
<contexts>
<context position="28268" citStr="Peng et al., 2003" startWordPosition="4586" endWordPosition="4589">skowski and Burger, 2006). Agreements and disagreements are a subset of the private states represented by the positive and negative subjective categories used in this work. To recognise agreements and disagreements automatically, Hillard et al. train 3-way decision tree classifiers (agreement, disagreement, other) using both word-based and prosodic features. Galley et al. model this task as a sequence tagging problem, and investigate whether features capturing speaker interactions are useful for recognizing agreements and disagreements. Hahn et al. investigate the use of contrast classifiers (Peng et al., 2003) for the task, using only lexical features. Hotspots are places in a meeting in which the participants are highly involved in the discussion. Although high involvement does not necessarily equate subjective content, in practice, we expect more sentiments, opinions, and arguments to be expressed when participants are highly involved in the discussion. In their work on recognizing meeting hotspots, Wrede and Shriberg focus on evaluating the contribution of various prosodic features, ignoring lexical features completely. The results of their study helped to inform our choice of prosodic features </context>
</contexts>
<marker>Peng, Vucetic, Han, Xie, Obradovic, 2003</marker>
<rawString>K. Peng, S. Vucetic, B. Han, H. Xie, and Z Obradovic. 2003. Exploiting unlabeled data for improving accuracy of predictive data mining. In Proceedings of ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="26349" citStr="Popescu and Etzioni, 2005" startWordPosition="4297" endWordPosition="4300"> 96.2 84.4 89.9 2 uninterpolated 79.8 98.0 79.7 87.9 single model 79.5 91.0 83.3 86.9 combination of character-level, phoneme-level, and word-level models for any natural language classification tasks. In text, there has been a significant amount of research on subjectivity and sentiment recognition, ranging from work at the phrase level to work on classifying sentences and documents. Sentencelevel subjectivity classification (e.g., (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and sentiment classification (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005)) is the research in text most closely related to our work. Of the sentence-level research, the most similar is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing positive, negative, and neutral emotions in meeting</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A. Popescu and O. Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>P Ehlen</author>
<author>J Niekrasz</author>
</authors>
<title>Detecting action items in multi-party meetings: Annotation and initial experiments.</title>
<date>2006</date>
<booktitle>In Proceedings ofMLMI.</booktitle>
<contexts>
<context position="1576" citStr="Purver et al., 2006" startWordPosition="226" endWordPosition="229">e an important part of any meeting. Meeting participants express pros and cons about ideas, they support or oppose decisions, and they make suggestions that may or may not be adopted. When recorded and archived, meetings become a part of the organizational knowledge, but their value is limited by the ability of tools to search and summarize meeting content, including subjective content. While progress has been made on recognizing primarily objective meeting content, for example, information about the topics that are discussed (Hsueh and Moore, 2006) and who is assigned to work on given tasks (Purver et al., 2006), there has been &apos;This work was supported by the Dutch BSIK-project MultimediaN, and the European IST Programme Project FP6- 0033812. This paper only reflects the authors’ views and funding agencies are not liable for any use that may be made of the information contained herein. fairly little work specifically directed toward recognizing subjective content. In contrast, there has been a wealth of research over the past several years on automatic subjectivity and sentiment analysis in text, including on-line media. Partly inspired by the rapid growth of social media, such as blogs, as well as o</context>
</contexts>
<marker>Purver, Ehlen, Niekrasz, 2006</marker>
<rawString>M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting action items in multi-party meetings: Annotation and initial experiments. In Proceedings ofMLMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="7882" citStr="Quirk et al., 1985" startWordPosition="1219" endWordPosition="1222">set of participants represent different stages in the design process (e.g., Conceptual Design, Detailed Design). The meetings used in the experiments have been annotated for subjective content using the AMIDA annotation scheme (Wilson, 2008). Table 1 lists the types of annotations that are marked in the data. There are three main categories of annotations, subjective utterances, subjective questions, and objective polar utterances. A subjective utterance is a span of words (or possibly sounds) where a private state is being expressed either through choice of words or prosody. A private state (Quirk et al., 1985) 467 is an internal mental or emotional state, including opinions, beliefs, sentiments, emotions, evaluations, uncertainties, and speculations, among others. Although typically when a private state is expressed it is the private state of the speaker, as in example (1) below, an utterance may also be subjective because the speaker is talking about the private state of someone else. For example, in (2) the negative opinion attributed to the company is what makes the utterance subjective. (1) Finding them is really a pain, you know (2) The company’s decided that teletext is outdated Subjective qu</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Raaijmakers</author>
<author>W Kraaij</author>
</authors>
<title>A shallow approach to subjectivity classification.</title>
<date>2008</date>
<booktitle>In Proceedings ofICWSM.</booktitle>
<contexts>
<context position="4833" citStr="Raaijmakers and Kraaij, 2008" startWordPosition="727" endWordPosition="730">aracter n-grams, are valuable sources of information for various text classification tasks. An example of character n-grams is the set of 3-grams {#se, sen, ent, nti, tim, ime, men, ent, nt#, t#a, #an, ana, nal, aly, lys, ysi, sis, is#} for the two-word phrase sentiment analysis. The special symbol # represents a word boundary. While it is not directly obvious that there is much information in these truncated substrings, character n-grams have successfully been used for finegrained classification tasks, such as named-entity recognition (Klein et al., 2003) and subjective sentence recognition (Raaijmakers and Kraaij, 2008), as well as a variety of document-level tasks (Stamatatos, 2006; Zhang and Lee, 2006; Kanaris and Stamatatos, 2007). The informativeness of these low-level features comes in part from a form of attenuation (Eisner, 1996): a slight abstraction of the underlying data that leads to the formation of string equivalence classes. For instance, words in a sentence will invariably share many character n-grams. Since every unique character n-gram in an utterance constitutes a separate feature, this leads to the formation of string classes, which is a form of abstraction. For example, Zhang and Lee (200</context>
<context position="25222" citStr="Raaijmakers and Kraaij, 2008" startWordPosition="4125" endWordPosition="4129">, interpolation outperforms both the uninterpolated and single-model classifiers, with 2 and 3 points improvements in F1, respectively. 6 Related Work Previous work has demonstrated that textual units below the word level, such as character n-grams, are valuable sources of information. Characterlevel models have successfully been used for namedentity recognition (Klein et al., 2003), predicting authorship (Keselj et al., 2003; Stamatatos, 2006), text categorization (Zhang and Lee, 2006), web page genre identification (Kanaris and Stamatatos, 2007), and sentence-level subjectivity recognition (Raaijmakers and Kraaij, 2008) In spoken-language data, Hsueh (2008) achieves good results using chains of phonemes to automatically segment meetings according to topic. However, to the best of our knowledge there has been no investigation to date on the 472 Table 7: Results of interpolated classifiers compared to uninterpolated and single-model classifiers for all features. Task Combination ACC REC PREC F1 interpolated 75.4 61.2 74.5 67.1 1 uninterpolated 73.0 58.7 70.6 64.0 single model 74.7 62.1 72.7 66.8 interpolated 83.8 96.2 84.4 89.9 2 uninterpolated 79.8 98.0 79.7 87.9 single model 79.5 91.0 83.3 86.9 combination o</context>
<context position="26498" citStr="Raaijmakers and Kraaij (2008)" startWordPosition="4322" endWordPosition="4325">l models for any natural language classification tasks. In text, there has been a significant amount of research on subjectivity and sentiment recognition, ranging from work at the phrase level to work on classifying sentences and documents. Sentencelevel subjectivity classification (e.g., (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and sentiment classification (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005)) is the research in text most closely related to our work. Of the sentence-level research, the most similar is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing positive, negative, and neutral emotions in meetings, work on recognizing agreements and disagreements in meetings (Hillard et al., 2003; Galley et al., 2004; Hahn et al., 2006), and work by Wrede and</context>
</contexts>
<marker>Raaijmakers, Kraaij, 2008</marker>
<rawString>S. Raaijmakers and W. Kraaij. 2008. A shallow approach to subjectivity classification. In Proceedings ofICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Raaijmakers</author>
</authors>
<title>Sentiment classification with interpolated information diffusion kernels.</title>
<date>2007</date>
<booktitle>In Proceedings of the First International Workshop on Data Mining and Audience Intelligence for Advertising (ADKDD’07).</booktitle>
<contexts>
<context position="17320" citStr="Raaijmakers, 2007" startWordPosition="2716" endWordPosition="2718">arately; the parameter set that produces the highest subjective F1 score on the tuning set for Task 1, and the highest positive subjective F1 score for Task 2, is used to train the final classifier for that fold. 4.3 Classifier combination After the single source classifiers have been trained, they have to be combined into an aggregate classifier. To this end, we decided to apply a simple linear interpolation strategy. Linear interpolation of models is the weighted combination of simple models to form complex models, and has its roots in generative language models (Jelinek and Mercer, 1980). (Raaijmakers, 2007) has demonstrated its use for discriminative machine learning. In the present binary class setting, BoosTexter produces two decision values, one for every class. For every individual single-source classifier (i.e., PROS, WORDS, CHARS and PHONES), separate weights are estimated that are applied to the decision values for the two classes produced by these classifiers. These weights express the relative importance of the single-source classifiers. The prediction of an aggregate classifier for a class c is then simply the sum of all weights for all participating single-source classifiers applied t</context>
</contexts>
<marker>Raaijmakers, 2007</marker>
<rawString>S. Raaijmakers. 2007. Sentiment classification with interpolated information diffusion kernels. In Proceedings of the First International Workshop on Data Mining and Audience Intelligence for Advertising (ADKDD’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="26183" citStr="Riloff and Wiebe, 2003" startWordPosition="4272" endWordPosition="4275">eatures. Task Combination ACC REC PREC F1 interpolated 75.4 61.2 74.5 67.1 1 uninterpolated 73.0 58.7 70.6 64.0 single model 74.7 62.1 72.7 66.8 interpolated 83.8 96.2 84.4 89.9 2 uninterpolated 79.8 98.0 79.7 87.9 single model 79.5 91.0 83.3 86.9 combination of character-level, phoneme-level, and word-level models for any natural language classification tasks. In text, there has been a significant amount of research on subjectivity and sentiment recognition, ranging from work at the phrase level to work on classifying sentences and documents. Sentencelevel subjectivity classification (e.g., (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and sentiment classification (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005)) is the research in text most closely related to our work. Of the sentence-level research, the most similar is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasund</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>E. Riloff and J. Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="13956" citStr="Schapire and Singer, 2000" startWordPosition="2169" endWordPosition="2172"> up the test set for fold 1. Then, for a given fold, the segments from the remaining 12 meetings are used for training and parameter tuning, with roughly a 85%, 7%, and 8% split between training, tuning, and testing sets for each fold. The assignment to training versus tuning set was random, with the only constraint being that a segment could only be in the tuning set for one fold of the data. 3In practice, this excludes about 7% of the positive/negative segments. The experiments we perform involve two steps. First, we train and optimize a classifier for each type of feature using BoosTexter (Schapire and Singer, 2000) AdaBoost.MH. Then, we investigate the performance of all possible combinations of features using linear combinations of the individual feature classifiers. 4.1 Features The two modalities that are investigated, prosodic, and textual, are represented by four different sets of features: prosody (PROS), word ngrams (WORDS), character n-grams (CHARS), and phoneme n-grams (PHONES). Based on previous research on prosody modelling in a meeting context (Wrede and Shriberg, 2003) and on the literature in emotion research (Banse and Scherer, 1996) we extract PROS features that are mainly based on pitch</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Ruppenhofer</author>
<author>J Wiebe</author>
</authors>
<title>Detecting arguing and sentiment in meetings.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGdial.</booktitle>
<contexts>
<context position="26801" citStr="Somasundaran et al. (2007)" startWordPosition="4363" endWordPosition="4367">be, 2003; Yu and Hatzivassiloglou, 2003)) and sentiment classification (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005)) is the research in text most closely related to our work. Of the sentence-level research, the most similar is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing positive, negative, and neutral emotions in meetings, work on recognizing agreements and disagreements in meetings (Hillard et al., 2003; Galley et al., 2004; Hahn et al., 2006), and work by Wrede and Shriberg (2003) on recognizing meeting hotspots. Somasundaran et al. use lexical and discourse features to recognize sentences and turns where meeting participants express sentiments or arguing. They also use the AMI corpus in their work; however, the use of different annotations and task definitions </context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2007</marker>
<rawString>S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007. Detecting arguing and sentiment in meetings. In Proceedings of SIGdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
</authors>
<title>Ensemble-based author identification using character n-grams.</title>
<date>2006</date>
<booktitle>In Proceedings of</booktitle>
<contexts>
<context position="4897" citStr="Stamatatos, 2006" startWordPosition="739" endWordPosition="740">ication tasks. An example of character n-grams is the set of 3-grams {#se, sen, ent, nti, tim, ime, men, ent, nt#, t#a, #an, ana, nal, aly, lys, ysi, sis, is#} for the two-word phrase sentiment analysis. The special symbol # represents a word boundary. While it is not directly obvious that there is much information in these truncated substrings, character n-grams have successfully been used for finegrained classification tasks, such as named-entity recognition (Klein et al., 2003) and subjective sentence recognition (Raaijmakers and Kraaij, 2008), as well as a variety of document-level tasks (Stamatatos, 2006; Zhang and Lee, 2006; Kanaris and Stamatatos, 2007). The informativeness of these low-level features comes in part from a form of attenuation (Eisner, 1996): a slight abstraction of the underlying data that leads to the formation of string equivalence classes. For instance, words in a sentence will invariably share many character n-grams. Since every unique character n-gram in an utterance constitutes a separate feature, this leads to the formation of string classes, which is a form of abstraction. For example, Zhang and Lee (2006) investigate similar subword representations, called key subst</context>
<context position="25041" citStr="Stamatatos, 2006" startWordPosition="4105" endWordPosition="4106">del is marginal (a .03 point difference in F1). However, compared to the uninterpolated combination, interpolation gives a clear 3.1 points improvement of F1. For Task 2, interpolation outperforms both the uninterpolated and single-model classifiers, with 2 and 3 points improvements in F1, respectively. 6 Related Work Previous work has demonstrated that textual units below the word level, such as character n-grams, are valuable sources of information. Characterlevel models have successfully been used for namedentity recognition (Klein et al., 2003), predicting authorship (Keselj et al., 2003; Stamatatos, 2006), text categorization (Zhang and Lee, 2006), web page genre identification (Kanaris and Stamatatos, 2007), and sentence-level subjectivity recognition (Raaijmakers and Kraaij, 2008) In spoken-language data, Hsueh (2008) achieves good results using chains of phonemes to automatically segment meetings according to topic. However, to the best of our knowledge there has been no investigation to date on the 472 Table 7: Results of interpolated classifiers compared to uninterpolated and single-model classifiers for all features. Task Combination ACC REC PREC F1 interpolated 75.4 61.2 74.5 67.1 1 uni</context>
</contexts>
<marker>Stamatatos, 2006</marker>
<rawString>E. Stamatatos. 2006. Ensemble-based author identification using character n-grams. In Proceedings of TIR. T. Wilson. 2008. Annotating subjective content in meetings. In Proceedings ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wrede</author>
<author>E Shriberg</author>
</authors>
<title>Spotting “hot spots” in meetings: Human judgments and prosodic cues.</title>
<date>2003</date>
<booktitle>In Proceedings of EUROSPEECH.</booktitle>
<contexts>
<context position="14432" citStr="Wrede and Shriberg, 2003" startWordPosition="2237" endWordPosition="2240">riments we perform involve two steps. First, we train and optimize a classifier for each type of feature using BoosTexter (Schapire and Singer, 2000) AdaBoost.MH. Then, we investigate the performance of all possible combinations of features using linear combinations of the individual feature classifiers. 4.1 Features The two modalities that are investigated, prosodic, and textual, are represented by four different sets of features: prosody (PROS), word ngrams (WORDS), character n-grams (CHARS), and phoneme n-grams (PHONES). Based on previous research on prosody modelling in a meeting context (Wrede and Shriberg, 2003) and on the literature in emotion research (Banse and Scherer, 1996) we extract PROS features that are mainly based on pitch, energy and the distribution of energy in the long-term averaged spectrum (LTAS) (see Table 3). These features are extracted at the word level and aggregated to the dialogue-act level by taking the average over the words per dialogue act. We then normalize the features per speaker per meeting by converting the raw feature values to zscores (z = (x − p)/Q). Table 3: Prosodic features used in experiments. pitch mean, standard deviation, minimum, maximum, range, mean absolu</context>
<context position="27114" citStr="Wrede and Shriberg (2003)" startWordPosition="4414" endWordPosition="4417">ij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing positive, negative, and neutral emotions in meetings, work on recognizing agreements and disagreements in meetings (Hillard et al., 2003; Galley et al., 2004; Hahn et al., 2006), and work by Wrede and Shriberg (2003) on recognizing meeting hotspots. Somasundaran et al. use lexical and discourse features to recognize sentences and turns where meeting participants express sentiments or arguing. They also use the AMI corpus in their work; however, the use of different annotations and task definitions makes it impossible to directly compare their results and ours. Neiberg et al. use acoustic– prosodic features (Mel-frequency Cepstral Coefficients (MFCCs) and pitch features) and lexical ngrams for recognizing emotions in the ISL Meeting Corpus (Laskowski and Burger, 2006). Agreements and disagreements are a su</context>
</contexts>
<marker>Wrede, Shriberg, 2003</marker>
<rawString>B. Wrede and E. Shriberg. 2003. Spotting “hot spots” in meetings: Human judgments and prosodic cues. In Proceedings of EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="26215" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="4276" endWordPosition="4279">n ACC REC PREC F1 interpolated 75.4 61.2 74.5 67.1 1 uninterpolated 73.0 58.7 70.6 64.0 single model 74.7 62.1 72.7 66.8 interpolated 83.8 96.2 84.4 89.9 2 uninterpolated 79.8 98.0 79.7 87.9 single model 79.5 91.0 83.3 86.9 combination of character-level, phoneme-level, and word-level models for any natural language classification tasks. In text, there has been a significant amount of research on subjectivity and sentiment recognition, ranging from work at the phrase level to work on classifying sentences and documents. Sentencelevel subjectivity classification (e.g., (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and sentiment classification (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005)) is the research in text most closely related to our work. Of the sentence-level research, the most similar is work by Raaijmakers and Kraaij (2008) comparing wordspanning character n-grams to word-internal character n-grams for subjectivity classification in news data. They found that character n-grams spanning words perform the best. Research on recognizing subjective content in multiparty conversation includes work by Somasundaran et al. (2007) on recognizin</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>W S Lee</author>
</authors>
<title>Extracting key-substringgroup features for text classification.</title>
<date>2006</date>
<booktitle>In Proceedings ofKDD.</booktitle>
<contexts>
<context position="4918" citStr="Zhang and Lee, 2006" startWordPosition="741" endWordPosition="744">example of character n-grams is the set of 3-grams {#se, sen, ent, nti, tim, ime, men, ent, nt#, t#a, #an, ana, nal, aly, lys, ysi, sis, is#} for the two-word phrase sentiment analysis. The special symbol # represents a word boundary. While it is not directly obvious that there is much information in these truncated substrings, character n-grams have successfully been used for finegrained classification tasks, such as named-entity recognition (Klein et al., 2003) and subjective sentence recognition (Raaijmakers and Kraaij, 2008), as well as a variety of document-level tasks (Stamatatos, 2006; Zhang and Lee, 2006; Kanaris and Stamatatos, 2007). The informativeness of these low-level features comes in part from a form of attenuation (Eisner, 1996): a slight abstraction of the underlying data that leads to the formation of string equivalence classes. For instance, words in a sentence will invariably share many character n-grams. Since every unique character n-gram in an utterance constitutes a separate feature, this leads to the formation of string classes, which is a form of abstraction. For example, Zhang and Lee (2006) investigate similar subword representations, called key substring group features. </context>
<context position="25084" citStr="Zhang and Lee, 2006" startWordPosition="4109" endWordPosition="4112">in F1). However, compared to the uninterpolated combination, interpolation gives a clear 3.1 points improvement of F1. For Task 2, interpolation outperforms both the uninterpolated and single-model classifiers, with 2 and 3 points improvements in F1, respectively. 6 Related Work Previous work has demonstrated that textual units below the word level, such as character n-grams, are valuable sources of information. Characterlevel models have successfully been used for namedentity recognition (Klein et al., 2003), predicting authorship (Keselj et al., 2003; Stamatatos, 2006), text categorization (Zhang and Lee, 2006), web page genre identification (Kanaris and Stamatatos, 2007), and sentence-level subjectivity recognition (Raaijmakers and Kraaij, 2008) In spoken-language data, Hsueh (2008) achieves good results using chains of phonemes to automatically segment meetings according to topic. However, to the best of our knowledge there has been no investigation to date on the 472 Table 7: Results of interpolated classifiers compared to uninterpolated and single-model classifiers for all features. Task Combination ACC REC PREC F1 interpolated 75.4 61.2 74.5 67.1 1 uninterpolated 73.0 58.7 70.6 64.0 single mode</context>
</contexts>
<marker>Zhang, Lee, 2006</marker>
<rawString>D. Zhang and W. S. Lee. 2006. Extracting key-substringgroup features for text classification. In Proceedings ofKDD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>