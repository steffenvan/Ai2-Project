<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000156">
<title confidence="0.999409">
A Single Generative Model
for Joint Morphological Segmentation and Syntactic Parsing
</title>
<author confidence="0.996083">
Yoav Goldberg
</author>
<affiliation confidence="0.998099">
Ben Gurion University of the Negev
Department of Computer Science
</affiliation>
<address confidence="0.865154">
POB 653 Be’er Sheva, 84105, Israel
</address>
<email confidence="0.996416">
yoavg@cs.bgu.ac.il
</email>
<author confidence="0.91325">
Reut Tsarfaty
</author>
<affiliation confidence="0.9623405">
Institute for Logic Language and Computation
University of Amsterdam
</affiliation>
<address confidence="0.533627">
Plantage Muidergracht 24, Amsterdam, NL
</address>
<email confidence="0.996676">
rtsarfat@science.uva.nl
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997348444444444">
Morphological processes in Semitic languages
deliver space-delimited words which intro-
duce multiple, distinct, syntactic units into the
structure of the input sentence. These words
are in turn highly ambiguous, breaking the
assumption underlying most parsers that the
yield of a tree for a given sentence is known in
advance. Here we propose a single joint model
for performing both morphological segmenta-
tion and syntactic disambiguation which by-
passes the associated circularity. Using a tree-
bank grammar, a data-driven lexicon, and a
linguistically motivated unknown-tokens han-
dling technique our model outperforms previ-
ous pipelined, integrated or factorized systems
for Hebrew morphological and syntactic pro-
cessing, yielding an error reduction of 12%
over the best published results so far.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9990230625">
Current state-of-the-art broad-coverage parsers as-
sume a direct correspondence between the lexical
items ingrained in the proposed syntactic analyses
(the yields of syntactic parse-trees) and the space-
delimited tokens (henceforth, ‘tokens’) that consti-
tute the unanalyzed surface forms (utterances). In
Semitic languages the situation is very different.
In Modern Hebrew (Hebrew), a Semitic language
with very rich morphology, particles marking con-
junctions, prepositions, complementizers and rela-
tivizers are bound elements prefixed to the word
(Glinert, 1989). The Hebrew token ‘bcl’1, for ex-
ample, stands for the complete prepositional phrase
&apos;We adopt here the transliteration of (Sima’an et al., 2001).
“in the shadow”. This token may further embed
into a larger utterance, e.g., ‘bcl hneim’ (literally
“in-the-shadow the-pleasant”, meaning roughly “in
the pleasant shadow”) in which the dominated Noun
is modified by a proceeding space-delimited adjec-
tive. It should be clear from the onset that the parti-
cle b (“in”) in ‘bcl’ may then attach higher than the
bare noun cl (“shadow”). This leads to word- and
constituent-boundaries discrepancy, which breaks
the assumptions underlying current state-of-the-art
statistical parsers.
One way to approach this discrepancy is to as-
sume a preceding phase of morphological segmen-
tation for extracting the different lexical items that
exist at the token level (as is done, to the best of
our knowledge, in all parsing related work on Arabic
and its dialects (Chiang et al., 2006)). The input for
the segmentation task is however highly ambiguous
for Semitic languages, and surface forms (tokens)
may admit multiple possible analyses as in (Bar-
Haim et al., 2007; Adler and Elhadad, 2006). The
aforementioned surface form bcl, for example, may
also stand for the lexical item “onion”, a Noun. The
implication of this ambiguity for a parser is that the
yield of syntactic trees no longer consists of space-
delimited tokens, and the expected number of leaves
in the syntactic analysis in not known in advance.
Tsarfaty (2006) argues that for Semitic languages
determining the correct morphological segmentation
is dependent on syntactic context and shows that in-
creasing information sharing between the morpho-
logical and the syntactic components leads to im-
proved performance on the joint task. Cohen and
Smith (2007) followed up on these results and pro-
</bodyText>
<page confidence="0.96723">
371
</page>
<note confidence="0.696691">
Proceedings ofACL-08: HLT, pages 371–379,
</note>
<page confidence="0.492669">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.9990752">
posed a system for joint inference of morphological
and syntactic structures using factored models each
designed and trained on its own.
Here we push the single-framework conjecture
across the board and present a single model that
performs morphological segmentation and syntac-
tic disambiguation in a fully generative framework.
We claim that no particular morphological segmen-
tation is a-priory more likely for surface forms be-
fore exploring the compositional nature of syntac-
tic structures, including manifestations of various
long-distance dependencies. Morphological seg-
mentation decisions in our model are delegated to a
lexeme-based PCFG and we show that using a sim-
ple treebank grammar, a data-driven lexicon, and
a linguistically motivated unknown-tokens handling
our model outperforms (Tsarfaty, 2006) and (Co-
hen and Smith, 2007) on the joint task and achieves
state-of-the-art results on a par with current respec-
tive standalone models.2
</bodyText>
<sectionHeader confidence="0.991049" genericHeader="method">
2 Modern Hebrew Structure
</sectionHeader>
<bodyText confidence="0.993560916666667">
Segmental morphology Hebrew consists of
seven particles m(“from”) f(“when”/“who”/“that”)
h(“the”) w(“and”) k(“like”) l(“to”) and b(“in”).
which may never appear in isolation and must
always attach as prefixes to the following open-class
category item we refer to as stem. Several such
particles may be prefixed onto a single stem, in
which case the affixation is subject to strict linear
precedence constraints. Co-occurrences among the
particles themselves are subject to further syntactic
and lexical constraints relative to the stem.
While the linear precedence of segmental mor-
phemes within a token is subject to constraints, the
dominance relations among their mother and sister
constituents is rather free. The relativizer f(“that”)
for example, may attach to an arbitrarily long rela-
tive clause that goes beyond token boundaries. The
attachment in such cases encompasses a long dis-
tance dependency that cannot be captured by Marko-
vian processes that are typically used for morpho-
logical disambiguation. The same argument holds
for resolving PP attachment of a prefixed preposition
or marking conjunction of elements of any kind.
A less canonical representation of segmental mor-
</bodyText>
<subsectionHeader confidence="0.894804">
2Standalone parsing models assume a segmentation Oracle.
</subsectionHeader>
<bodyText confidence="0.995696553191489">
phology is triggered by a morpho-phonological pro-
cess of omitting the definite article h when occur-
ring after the particles b or l. This process triggers
ambiguity as for the definiteness status of Nouns
following these particles.We refer to such cases
in which the concatenation of elements does not
strictly correspond to the original surface form as
super-segmental morphology. An additional case of
super-segmental morphology is the case of Pronom-
inal Clitics. Inflectional features marking pronom-
inal elements may be attached to different kinds of
categories marking their pronominal complements.
The additional morphological material in such cases
appears after the stem and realizes the extended
meaning. The current work treats both segmental
and super-segmental phenomena, yet we note that
there may be more adequate ways to treat super-
segmental phenomena assuming Word-Based mor-
phology as we explore in (Tsarfaty and Goldberg,
2008).
Lexical and Morphological Ambiguity The rich
morphological processes for deriving Hebrew stems
give rise to a high degree of ambiguity for Hebrew
space-delimited tokens. The form fmnh, for exam-
ple, can be understood as the verb “lubricated”, the
possessed noun “her oil”, the adjective “fat” or the
verb “got fat”. Furthermore, the systematic way in
which particles are prefixed to one another and onto
an open-class category gives rise to a distinct sort
of morphological ambiguity: space-delimited tokens
may be ambiguous between several different seg-
mentation possibilities. The same form fmnh can be
segmented as f-mnh, f (“that”) functioning as a rele-
tivizer with the form mnh. The form mnh itself can
be read as at least three different verbs (“counted”,
“appointed”, “was appointed”), a noun (“a portion”),
and a possessed noun (“her kind”).
Such ambiguities cause discrepancies between
token boundaries (indexed as white spaces) and
constituent boundaries (imposed by syntactic cate-
gories) with respect to a surface form. Such discrep-
ancies can be aligned via an intermediate level of
PoS tags. PoS tags impose a unique morphological
segmentation on surface tokens and present a unique
valid yield for syntactic trees. The correct ambigu-
ity resolution of the syntactic level therefore helps to
resolve the morphological one, and vice versa.
</bodyText>
<page confidence="0.998816">
372
</page>
<sectionHeader confidence="0.987955" genericHeader="method">
3 Previous Work on Hebrew Processing
</sectionHeader>
<bodyText confidence="0.999891326086957">
Morphological analyzers for Hebrew that analyze a
surface form in isolation have been proposed by Se-
gal (2000), Yona and Wintner (2005), and recently
by the knowledge center for processing Hebrew (Itai
et al., 2006). Such analyzers propose multiple seg-
mentation possibilities and their corresponding anal-
yses for a token in isolation but have no means to
determine the most likely ones. Morphological dis-
ambiguators that consider a token in context (an ut-
terance) and propose the most likely morphologi-
cal analysis of an utterance (including segmentation)
were presented by Bar-Haim et al. (2005), Adler
and Elhadad (2006), Shacham and Wintner (2007),
and achieved good results (the best segmentation re-
sult so far is around 98%).
The development of the very first Hebrew Tree-
bank (Sima’an et al., 2001) called for the exploration
of general statistical parsing methods, but the appli-
cation was at first limited. Sima’an et al. (2001) pre-
sented parsing results for a DOP tree-gram model
using a small data set (500 sentences) and semi-
automatic morphological disambiguation. Tsarfaty
(2006) was the first to demonstrate that fully auto-
matic Hebrew parsing is feasible using the newly
available 5000 sentences treebank. Tsarfaty and
Sima’an (2007) have reported state-of-the-art results
on Hebrew unlexicalized parsing (74.41%) albeit as-
suming oracle morphological segmentation.
The joint morphological and syntactic hypothesis
was first discussed in (Tsarfaty, 2006; Tsarfaty and
Sima’an, 2004) and empirically explored in (Tsar-
faty, 2006). Tsarfaty (2006) used a morphological
analyzer (Segal, 2000), a PoS tagger (Bar-Haim et
al., 2005), and a general purpose parser (Schmid,
2000) in an integrated framework in which morpho-
logical and syntactic components interact to share
information, leading to improved performance on
the joint task. Cohen and Smith (2007) later on
based a system for joint inference on factored, inde-
pendent, morphological and syntactic components
of which scores are combined to cater for the joint
inference task. Both (Tsarfaty, 2006; Cohen and
Smith, 2007) have shown that a single integrated
framework outperforms a completely streamlined
implementation, yet neither has shown a single gen-
erative model which handles both tasks.
</bodyText>
<sectionHeader confidence="0.996318" genericHeader="method">
4 Model Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.999884">
4.1 The Status Space-Delimited Tokens
</subsectionHeader>
<bodyText confidence="0.999883933333333">
A Hebrew surface token may have several readings,
each of which corresponding to a sequence of seg-
ments and their corresponding PoS tags. We refer
to different readings as different analyses whereby
the segments are deterministic given the sequence of
PoS tags. We refer to a segment and its assigned PoS
tag as a lexeme, and so analyses are in fact sequences
of lexemes. For brevity we omit the segments from
the analysis, and so analysis of the form “fmnh” as
f/REL mnh/VB is represented simply as REL VB.
Such tag sequences are often treated as “complex
tags” (e.g. REL+VB) (cf. (Bar-Haim et al., 2007;
Habash and Rambow, 2005)) and probabilities are
assigned to different analyses in accordance with
the likelihood of their tags (e.g., “fmnh is 30%
likely to be tagged NN and 70% likely to be tagged
REL+VB”). Here we do not submit to this view.
When a token fmnh is to be interpreted as the lex-
eme sequence f/REL mnh/VB, the analysis intro-
duces two distinct entities, the relativizer f (“that”)
and the verb mnh (“counted”), and not as the com-
plex entity “that counted”. When the same token
is to be interpreted as a single lexeme fmnh, it may
function as a single adjective “fat”. There is no re-
lation between these two interpretations other then
the fact that their surface forms coincide, and we ar-
gue that the only reason to prefer one analysis over
the other is compositional. A possible probabilistic
model for assigning probabilities to complex analy-
ses of a surface form may be
</bodyText>
<equation confidence="0.7609555">
P(REL, VB|fmnh, context) _
P(REL|f)P(VB|mnh, REL)P(REL, VB |context)
</equation>
<bodyText confidence="0.999852428571429">
and indeed recent sequential disambiguation models
for Hebrew (Adler and Elhadad, 2006) and Arabic
(Smith et al., 2005) present similar models.
We suggest that in unlexicalized PCFGs the syn-
tactic context may be explicitly modeled in the
derivation probabilities. Hence, we take the prob-
ability of the event fmnh analyzed as REL VB to be
</bodyText>
<equation confidence="0.617279">
P(REL → f|REL) × P(VB → mnh|VB)
</equation>
<bodyText confidence="0.990429">
This means that we generate f and mnh indepen-
dently depending on their corresponding PoS tags,
</bodyText>
<page confidence="0.995909">
373
</page>
<bodyText confidence="0.9991448">
and the context (as well as the syntactic relation be-
tween the two) is modeled via the derivation result-
ing in a sequence REL VB spanning the form fmnh.
based on linear context. In our model, however, all
lattice paths are taken to be a-priori equally likely.
</bodyText>
<sectionHeader confidence="0.792292" genericHeader="method">
5 A Generative PCFG Model
</sectionHeader>
<subsectionHeader confidence="0.999352">
4.2 Lattice Representation
</subsectionHeader>
<bodyText confidence="0.999923541666667">
We represent all morphological analyses of a given
utterance using a lattice structure. Each lattice arc
corresponds to a segment and its corresponding PoS
tag, and a path through the lattice corresponds to
a specific morphological segmentation of the utter-
ance. This is by now a fairly standard representa-
tion for multiple morphological segmentation of He-
brew utterances (Adler, 2001; Bar-Haim et al., 2005;
Smith et al., 2005; Cohen and Smith, 2007; Adler,
2007). Figure 1 depicts the lattice for a 2-words
sentence bclm hneim. We use double-circles to in-
dicate the space-delimited token boundaries. Note
that in our construction arcs can never cross token
boundaries. Every token is independent of the oth-
ers, and the sentence lattice is in fact a concatena-
tion of smaller lattices, one for each token. Fur-
thermore, some of the arcs represent lexemes not
present in the input tokens (e.g. h/DT, fl/POS), how-
ever these are parts of valid analyses of the token (cf.
super-segmental morphology section 2). Segments
with the same surface form but different PoS tags
are treated as different lexemes, and are represented
as separate arcs (e.g. the two arcs labeled neim from
node 6 to 7).
</bodyText>
<figure confidence="0.605154">
bclm/NNP
</figure>
<figureCaption confidence="0.999547">
Figure 1: The Lattice for the Hebrew Phrase bclm hneim
</figureCaption>
<bodyText confidence="0.995874058823529">
A similar structure is used in speech recognition.
There, a lattice is used to represent the possible sen-
tences resulting from an interpretation of an acoustic
model. In speech recognition the arcs of the lattice
are typically weighted in order to indicate the prob-
ability of specific transitions. Given that weights on
all outgoing arcs sum up to one, weights induce a
probability distribution on the lattice paths. In se-
quential tagging models such as (Adler and Elhadad,
2006; Bar-Haim et al., 2007; Smith et al., 2005)
weights are assigned according to a language model
The input for the joint task is a sequence W =
w1, ... , wn of space-delimited tokens. Each token
may admit multiple analyses, each of which a se-
quence of one or more lexemes (we use li to denote
a lexeme) belonging a presupposed Hebrew lexicon
LEX. The entries in such a lexicon may be thought
of as meaningful surface segments paired up with
their PoS tags li = (si, pi), but note that a surface
segment s need not be a space-delimited token.
The Input The set of analyses for a token is thus
represented as a lattice in which every arc corre-
sponds to a specific lexeme l, as shown in Figure
1. A morphological analyzer M : W—* L is a
function mapping sentences in Hebrew (W E W)
to their corresponding lattices (M(W) = L E L).
We define the lattice L to be the concatenation of the
lattices Li corresponding to the input words wi (s.t.
M(wi) = Li). Each connected path (l1 ... lk) E
L corresponds to one morphological segmentation
possibility of W.
The Parser Given a sequence of input tokens
W = w1 ... wn and a morphological analyzer, we
look for the most probable parse tree π s.t.
</bodyText>
<equation confidence="0.9174665">
π� = arg max
π
</equation>
<bodyText confidence="0.998977">
Since the lattice L for a given sentence W is deter-
mined by the morphological analyzer M we have
</bodyText>
<equation confidence="0.977228166666667">
P(π|W, M, L)
Hence, our parser searches for a parse tree π over
lexemes (l1 ... lk) s.t. li = (si, pi) E LEX,
(l1 ... lk) E L and M(W) = L. So we remain with
π� = arg max
π
</equation>
<bodyText confidence="0.999896285714286">
which is precisely the formula corresponding to the
so-called lattice parsing familiar from speech recog-
nition. Every parse π selects a specific morphologi-
cal segmentation (l1...lk) (a path through the lattice).
This is akin to PoS tags sequences induced by dif-
ferent parses in the setup familiar from English and
explored in e.g. (Charniak et al., 1996).
</bodyText>
<figure confidence="0.95770747368421">
5
clm/NN
clm/VB
hneim/VB
0
b/IN
1
h/DT
3
bcl/NN
cl/NN
h/DT
neim/VB
6 neim/JJ
7
2
fl/POS
4
clm/NN
</figure>
<equation confidence="0.8031636">
hm/PRP
P(π|W, M)
π� = arg max
π
P(π|L)
</equation>
<page confidence="0.988511">
374
</page>
<bodyText confidence="0.99985755">
Our use of an unweighted lattice reflects our be-
lief that all the segmentations of the given input sen-
tence are a-priori equally likely; the only reason to
prefer one segmentation over the another is due to
the overall syntactic context which is modeled via
the PCFG derivations. A compatible view is pre-
sented by Charniak et al. (1996) who consider the
kind of probabilities a generative parser should get
from a PoS tagger, and concludes that these should
be P(w|t) “and nothing fancier”.3 In our setting,
therefore, the Lattice is not used to induce a proba-
bility distribution on a linear context, but rather, it is
used as a common-denominator of state-indexation
of all segmentations possibilities of a surface form.
This is a unique object for which we are able to de-
fine a proper probability model. Thus our proposed
model is a proper model assigning probability mass
to all (7r, L) pairs, where 7r is a parse tree and L is
the one and only lattice that a sequence of characters
(and spaces) W over our alpha-beth gives rise to.
</bodyText>
<equation confidence="0.9943355">
� P(7r, L) = 1; L uniquely index W
7r,L
</equation>
<bodyText confidence="0.976053436619719">
The Grammar Our parser looks for the most
likely tree spanning a single path through the lat-
tice of which the yield is a sequence of lexemes.
This is done using a simple PCFG which is lexeme-
based. This means that the rules in our grammar
are of two kinds: (a) syntactic rules relating non-
terminals to a sequence of non-terminals and/or PoS
tags, and (b) lexical rules relating PoS tags to lattice
arcs (lexemes). The possible analyses of a surface
token pose constraints on the analyses of specific
segments. In order to pass these constraints onto the
parser, the lexical rules in the grammar are of the
form pi —* (si, pi)
Parameter Estimation The grammar probabili-
ties are estimated from the corpus using simple rela-
tive frequency estimates. Lexical rules are estimated
in a similar manner. We smooth Prf(p —* (s, p)) for
rare and OOV segments (s E l, l E L, s unseen) us-
ing a “per-tag” probability distribution over rare seg-
ments which we estimate using relative frequency
estimates for once-occurring segments.
3An English sentence with ambiguous PoS assignment can
be trivially represented as a lattice similar to our own, where
every pair of consecutive nodes correspond to a word, and every
possible PoS assignment for this word is a connecting arc.
Handling Unknown tokens When handling un-
known tokens in a language such as Hebrew various
important aspects have to be borne in mind. Firstly,
Hebrew unknown tokens are doubly unknown: each
unknown token may correspond to several segmen-
tation possibilities, and each segment in such se-
quences may be able to admit multiple PoS tags.
Secondly, some segments in a proposed segment se-
quence may in fact be seen lexical events, i.e., for
some p tag Prf(p —* (s, p)) &gt; 0, while other seg-
ments have never been observed as a lexical event
before. The latter arcs correspond to OOV words
in English. Finally, the assignments of PoS tags to
OOV segments is subject to language specific con-
straints relative to the token it was originated from.
Our smoothing procedure takes into account all
the aforementioned aspects and works as follows.
We first make use of our morphological analyzer to
find all segmentation possibilities by chopping off
all prefix sequence possibilities (including the empty
prefix) and construct a lattice off of them. The re-
maining arcs are marked OOV. At this stage the lat-
tice path corresponds to segments only, with no PoS
assigned to them. In turn we use two sorts of heuris-
tics, orthogonal to one another, to prune segmenta-
tion possibilities based on lexical and grammatical
constraints. We simulate lexical constraints by using
an external lexical resource against which we verify
whether OOV segments are in fact valid Hebrew lex-
emes. This heuristics is used to prune all segmenta-
tion possibilities involving “lexically improper” seg-
ments. For the remaining arcs, if the segment is in
fact a known lexeme it is tagged as usual, but for the
OOV arcs which are valid Hebrew entries lacking
tags assignment, we assign all possible tags and then
simulate a grammatical constraint. Here, all token-
internal collocations of tags unseen in our training
data are pruned away. From now on all lattice arcs
are tagged segments and the assignment of probabil-
ity P(p —* (s, p)) to lattice arcs proceeds as usual.4
A rather pathological case is when our lexical
heuristics prune away all segmentation possibilities
and we remain with an empty lattice. In such cases
we use the non-pruned lattice including all (possibly
ungrammatical) segmentation, and let the statistics
(including OOV) decide. We empirically control for
</bodyText>
<footnote confidence="0.989906">
4Our heuristics may slightly alter E,,, P(ir, L) Pz� 1
</footnote>
<page confidence="0.998951">
375
</page>
<bodyText confidence="0.996053">
the effect of our heuristics to make sure our pruning
does not undermine the objectives of our joint task.
</bodyText>
<sectionHeader confidence="0.998244" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999826416666667">
Previous work on morphological and syntactic dis-
ambiguation in Hebrew used different sets of data,
different splits, differing annotation schemes, and
different evaluation measures. Our experimental
setup therefore is designed to serve two goals. Our
primary goal is to exploit the resources that are most
appropriate for the task at hand, and our secondary
goal is to allow for comparison of our models’ per-
formance against previously reported results. When
a comparison against previous results requires addi-
tional pre-processing, we state it explicitly to allow
for the reader to replicate the reported results.
Data We use the Hebrew Treebank, (Sima’an
et al., 2001), provided by the knowledge center
for processing Hebrew, in which sentences from
the daily newspaper “Ha’aretz” are morphologically
segmented and syntactically annotated. The tree-
bank has two versions, v1.0 and v2.0, containing
5001 and 6501 sentences respectively. We use v1.0
mainly because previous studies on joint inference
reported results w.r.t. v1.0 only.5 We expect that
using the same setup on v2.0 will allow a cross-
treebank comparison.6 We used the first 500 sen-
tences as our dev set and the rest 4500 for training
and report our main results on this split. To facili-
tate the comparison of our results to those reported
by (Cohen and Smith, 2007) we use their data set in
which 177 empty and “malformed”7 were removed.
The first 3770 trees of the resulting set then were
used for training, and the last 418 are used testing.
(we ignored the 419 trees in their development set.)
Morphological Analyzer Ideally, we would use
an of-the-shelf morphological analyzer for mapping
each input token to its possible analyses. Such re-
sources exist for Hebrew (Itai et al., 2006), but un-
fortunately use a tagging scheme which is incom-
</bodyText>
<tableCaption confidence="0.475363">
5The comparison to performance on version 2.0 is meaning-
less not only because of the change in size, but also conceptual
changes in the annotation scheme
6Unfortunatley running our setup on the v2.0 data set is cur-
rently not possible due to missing tokens-morphemes alignment
in the v2.0 treebank.
7We thank Shay Cohen for providing us with their data set
and evaluation Software.
</tableCaption>
<bodyText confidence="0.99862396875">
patible with the one of the Hebrew Treebank.s For
this reason, we use a data-driven morphological an-
alyzer derived from the training data similar to (Co-
hen and Smith, 2007). We construct a mapping from
all the space-delimited tokens seen in the training
sentences to their corresponding analyses.
Lexicon and OOV Handling Our data-driven
morphological-analyzer proposes analyses for un-
known tokens as described in Section 5. We use the
HSPELL9 (Har’el and Kenigsberg, 2004) wordlist
as a lexeme-based lexicon for pruning segmenta-
tions involving invalid segments. Models that em-
ploy this strategy are denoted hsp. To control for
the effect of the HSPELL-based pruning, we also ex-
perimented with a morphological analyzer that does
not perform this pruning. For these models we limit
the options provided for OOV words by not consid-
ering the entire token as a valid segmentation in case
at least some prefix segmentation exists. This ana-
lyzer setting is similar to that of (Cohen and Smith,
2007), and models using it are denoted nohsp,
Parser and Grammar We used BitPar (Schmid,
2004), an efficient general purpose parser,10 together
with various treebank grammars to parse the in-
put sentences and propose compatible morpholog-
ical segmentation and syntactic analysis.
We experimented with increasingly rich gram-
mars read off of the treebank. Our first model is
GTplain, a PCFG learned from the treebank after
removing all functional features from the syntactic
categories. In our second model GTvpi we also
distinguished finite and non-finite verbs and VPs as
</bodyText>
<subsectionHeader confidence="0.6727835">
MMapping between the two schemes involves non-
deterministic many-to-many mappings, and in some cases re-
quire a change in the syntactic trees.
9An open-source Hebrew spell-checker.
</subsectionHeader>
<bodyText confidence="0.982702866666667">
10Lattice parsing can be performed by special initialization
of the chart in a CKY parser (Chappelier et al., 1999). We
currently simulate this by crafting a WCFG and feeding it to
BitPar. Given a PCFG grammar G and a lattice L with nodes
n1 ... nk, we construct the weighted grammar GL as follows:
for every arc (lexeme) l E L from node ni to node nj, we add
to GL the rule [l --+ tni, tni+1, ... , tnj_1] with a probability of
1 (this indicates the lexeme l spans from node ni to node nj).
GL is then used to parse the string tn1 ... tnk_1, where tni is
a terminal corresponding to the lattice span between node ni
and ni+1. Removing the leaves from the resulting tree yields a
parse for L under G, with the desired probabilities. We use a
patched version of BitPar allowing for direct input of probabili-
ties instead of counts. We thank Felix Hageloh (Hageloh, 2006)
for providing us with this version.
</bodyText>
<page confidence="0.997738">
376
</page>
<bodyText confidence="0.999875447368421">
proposed in (Tsarfaty, 2006). In our third model
GTppp we also add the distinction between gen-
eral PPs and possessive PPs following Goldberg and
Elhadad (2007). In our forth model GTnph we
add the definiteness status of constituents follow-
ing Tsarfaty and Sima’an (2007). Finally, model
GTv = 2 includes parent annotation on top of the
various state-splits, as is done also in (Tsarfaty and
Sima’an, 2007; Cohen and Smith, 2007). For all
grammars, we use fine-grained PoS tags indicating
various morphological features annotated therein.
Evaluation We use 8 different measures to eval-
uate the performance of our system on the joint dis-
ambiguation task. To evaluate the performance on
the segmentation task, we report SEG, the stan-
dard harmonic means for segmentation Precision
and Recall F1 (as defined in Bar-Haim et al. (2005);
Tsarfaty (2006)) as well as the segmentation ac-
curacy SEGTok measure indicating the percentage
of input tokens assigned the correct exact segmen-
tation (as reported by Cohen and Smith (2007)).
SEGTok(noH) is the segmentation accuracy ignor-
ing mistakes involving the implicit definite article
h.11 To evaluate our performance on the tagging
task we report CPOS and FPOS corresponding
to coarse- and fine-grained PoS tagging results (F1)
measure. Evaluating parsing results in our joint
framework, as argued by Tsarfaty (2006), is not triv-
ial under the joint disambiguation task, as the hy-
pothesized yield need not coincide with the correct
one. Our parsing performance measures (SY N)
thus report the PARSEVAL extension proposed in
Tsarfaty (2006). We further report SYNCS, the
parsing metric of Cohen and Smith (2007), to fa-
cilitate the comparison. We report the F1 value of
both measures. Finally, our U (unparsed) measure
is used to report the number of sentences to which
our system could not propose a joint analysis.
</bodyText>
<sectionHeader confidence="0.998605" genericHeader="evaluation">
7 Results and Analysis
</sectionHeader>
<bodyText confidence="0.996867">
The accuracy results for segmentation, tagging and
parsing using our different models and our standard
data split are summarized in Table 1. In addition
we report for each model its performance on gold-
segmented input (GS) to indicate the upper bound
11Overt definiteness errors may be seen as a wrong feature
rather than as wrong constituent and it is by now an accepted
standard to report accuracy with and without such errors.
for the grammars’ performance on the parsing task.
The table makes clear that enriching our grammar
improves the syntactic performance as well as mor-
phological disambiguation (segmentation and POS
tagging) accuracy. This supports our main thesis that
decisions taken by single, improved, grammar are
beneficial for both tasks. When using the segmen-
tation pruning (using HSPELL) for unseen tokens,
performance improves for all tasks as well. Yet we
note that the better grammars without pruning out-
perform the poorer grammars using this technique,
indicating that the syntactic context aids, to some
extent, the disambiguation of unknown tokens.
Table 2 compares the performance of our system
on the setup of Cohen and Smith (2007) to the best
results reported by them for the same tasks.
</bodyText>
<table confidence="0.9993107">
Model SEGTok CPOS FPOS SYNCS
GTnohsp/pln 89.50 81.00 77.65 62.22
GTnohsp/···+nph 89.58 81.26 77.82 64.30
CSpl. 91.10 80.40 75.60 64.00
CS„=2 90.90 80.50 75.40 64.40
GThsp/pln 93.13 83.12 79.12 64.46
GTnohsp/···+v=2 89.66 82.85 78.92 66.31
Oracle CSpl, 91.80 83.20 79.10 66.50
Oracle CS„=2 91.70 83.00 78.70 67.40
GThsp/···+v=2 93.38 85.08 80.11 69.11
</table>
<tableCaption confidence="0.938195333333333">
Table 2: Segmentation, Parsing and Tagging Results us-
ing the Setup of (Cohen and Smith, 2007) (sentence
length &lt; 40). The Models’ are Ordered by Performance.
</tableCaption>
<bodyText confidence="0.9986558">
We first note that the accuracy results of our
system are overall higher on their setup, on all
measures, indicating that theirs may be an easier
dataset. Secondly, for all our models we provide
better fine- and coarse-grained POS-tagging accu-
racy, and all pruned models outperform the Ora-
cle results reported by them.12 In terms of syn-
tactic disambiguation, even the simplest grammar
pruned with HSPELL outperforms their non-Oracle
results. Without HSPELL-pruning, our simpler
grammars are somewhat lagging behind, but as the
grammars improve the gap is bridged. The addi-
tion of vertical markovization enables non-pruned
models to outperform all previously reported re-
12Cohen and Smith (2007) make use of a parameter (α)
which is tuned separately for each of the tasks. This essentially
means that their model does not result in a true joint inference,
as executions for different tasks involve tuning a parameter sep-
arately. In our model there are no such hyper-parameters, and
the performance is the result of truly joint disambiguation.
</bodyText>
<page confidence="0.992249">
377
</page>
<table confidence="0.995248454545455">
Model U SEGTok / no H SEGF CPOS FPOS SYN / SYNCS GS SYN
GTnohsp/pln 7 89.77 / 93.18 91.80 80.36 76.77 60.41 / 61.66 65.00
···+vpi 7 89.80 / 93.18 91.84 80.37 76.74 61.16 / 62.41 66.70
···+ppp 7 89.79 / 93.20 91.86 80.43 76.79 61.47 / 62.86 67.22
···+nph 7 89.78 / 93.20 91.86 80.43 76.87 61.85 / 63.06 68.23
···+v=2 9 89.12 / 92.45 91.77 82.02 77.86 64.53 / 66.02 70.82
GThsp/pln 11 92.00 / 94.81 94.52 82.35 78.11 62.10 / 64.17 65.00
···+vpi 11 92.03 / 94.82 94.58 82.39 78.23 63.00 / 65.06 66.70
···+ppp 11 92.02 / 94.85 94.58 82.48 78.33 63.26 / 65.42 67.22
···+nph 11 92.14 / 94.91 94.73 82.58 78.47 63.98 / 65.98 68.23
···+v=2 13 91.42 / 94.10 94.67 84.23 79.25 66.60 / 68.79 70.82
</table>
<tableCaption confidence="0.999896">
Table 1: Segmentation, tagging and parsing results on the Standard dev/train Split, for all Sentences
</tableCaption>
<bodyText confidence="0.999959060606061">
sults. Furthermore, the combination of pruning and
vertical markovization of the grammar outperforms
the Oracle results reported by Cohen and Smith.
This essentially means that a better grammar tunes
the joint model for optimized syntactic disambigua-
tion at least in as much as their hyper parameters
do. An interesting observation is that while vertical
markovization benefits all our models, its effect is
less evident in Cohen and Smith.
On the surface, our model may seem as a special
case of Cohen and Smith in which α = 0. How-
ever, there is a crucial difference: the morphological
probabilities in their model come from discrimina-
tive models based on linear context. Many morpho-
logical decisions are based on long distance depen-
dencies, and when the global syntactic evidence dis-
agrees with evidence based on local linear context,
the two models compete with one another, despite
the fact that the PCFG takes also local context into
account. In addition, as the CRF and PCFG look at
similar sorts of information from within two inher-
ently different models, they are far from independent
and optimizing their product is meaningless. Cohen
and Smith approach this by introducing the α hy-
perparameter, which performs best when optimized
independently for each sentence (cf. Oracle results).
In contrast, our morphological probabilities are
based on a unigram, lexeme-based model, and all
other (local and non-local) contextual considerations
are delegated to the PCFG. This fully generative
model caters for real interaction between the syn-
tactic and morphological levels as a part of a single
coherent process.
</bodyText>
<sectionHeader confidence="0.997793" genericHeader="discussions">
8 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999689736842105">
Employing a PCFG-based generative framework to
make both syntactic and morphological disambigua-
tion decisions is not only theoretically clean and
linguistically justified and but also probabilistically
apropriate and empirically sound. The overall per-
formance of our joint framework demonstrates that
a probability distribution obtained over mere syn-
tactic contexts using a Treebank grammar and a
data-driven lexicon outperforms upper bounds pro-
posed by previous joint disambiguation systems and
achieves segmentation and parsing results on a par
with state-of-the-art standalone applications results.
Better grammars are shown here to improve per-
formance on both morphological and syntactic tasks,
providing support for the advantage of a joint frame-
work over pipelined or factorized ones. We conjec-
ture that this trend may continue by incorporating
additional information, e.g., three-dimensional mod-
els as proposed by Tsarfaty and Sima’an (2007). In
the current work morphological analyses and lexi-
cal probabilities are derived from a small Treebank,
which is by no means the best way to go. Using
a wide-coverage morphological analyzer based on
(Itai et al., 2006) should cater for a better cover-
age, and incorporating lexical probabilities learned
from a big (unannotated) corpus (cf. (Levinger et
al., 1995; Goldberg et al., ; Adler et al., 2008)) will
make the parser more robust and suitable for use in
more realistic scenarios.
Acknowledgments We thank Meni Adler and
Michael Elhadad (BGU) for helpful comments and
discussion. We further thank Khalil Simaan (ILLC-
UvA) for his careful advise concerning the formal
details of the proposal. The work of the first au-
thor was supported by the Lynn and William Frankel
Center for Computer Sciences. The work of the sec-
ond author as well as collaboration visits to Israel
was financed by NWO, grant number 017.001.271.
</bodyText>
<page confidence="0.998252">
378
</page>
<sectionHeader confidence="0.998327" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99970664">
Meni Adler and Michael Elhadad. 2006. An Unsuper-
vised Morpheme-Based HMM for Hebrew Morpho-
logical Disambiguation. In Proceeding of COLING-
ACL-06, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised Lexicon-Based Reso-
lution of Unknown Words for Full Morpholological
Analysis. In Proceedings ofACL-08.
Meni Adler. 2001. Hidden Markov Model for Hebrew
Part-of-Speech Tagging. Master’s thesis, Ben-Gurion
University of the Negev.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Roy Bar-Haim, Khalil Sima’an, and Yoad Winter. 2005.
Choosing an optimal architecture for segmentation and
pos- tagging of modern Hebrew. In Proceedings of
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Roy Bar-Haim, Khalil Sima’an, and Yoad Winter. 2007.
Part-of-speech tagging of Modern Hebrew text. Natu-
ral Language Engineering, 14(02):223–251.
J. Chappelier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice Parsing for Speech Recognition.
Eugene Charniak, Glenn Carroll, John Adcock, An-
thony R. Cassandra, Yoshihiko Gotoh, Jeremy Katz,
Michael L. Littman, and John McCann. 1996. Tag-
gers for Parsers. AI, 85(1-2):45–57.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic Di-
alects. In Proceedings ofEACL-06.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL-07, pages 208–217.
Lewis Glinert. 1989. The Grammar of Modern Hebrew.
Cambridge University Press.
Yoav Goldberg and Michael Elhadad. 2007. SVM Model
Tampering and Anchored Learning: A Case Study
in Hebrew NP Chunking. In Proceeding of ACL-07,
Prague, Czech Republic.
Yoav Goldberg, Meni Adler, and Michael Elhadad. EM
Can Find Pretty G]ood HMM POS-Taggers (When
Given a Good Start), booktitle = Proceedings of ACL-
08, year = 2008,.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceeding of
ACL-05.
Felix Hageloh. 2006. Parsing Using Transforms over
Treebanks. Master’s thesis, University of Amsterdam.
Nadav Har’el and Dan Kenigsberg. 2004. HSpell - the
free Hebrew Spell Checker and Morphological Ana-
lyzer. Israeli Seminar on Computational Linguistics.
Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A
Computational Lexicon of Contemporary Hebrew. In
Proceedings ofLREC-06.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing Morpholexical Probabilities from an Untagged
Corpus with an Application to Hebrew. Computa-
tional Linguistics, 21:383–404.
Helmut Schmid, 2000. LoPar: Design and Implementa-
tion. Institute for Computational Linguistics, Univer-
sity of Stuttgart.
Helmut Schmid. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit Vector. In
Proceedings of COLING-04.
Erel Segal. 2000. Hebrew Morphological Analyzer for
Hebrew Undotted Texts. Master’s thesis, Technion,
Haifa, Israel.
Danny Shacham and Shuly Wintner. 2007. Morpho-
logical Disambiguation of Hebrew: A Case Study in
Classifier Combination. In Proceedings of EMNLP-
CoNLL-07, pages 439–447.
Khalil Sima’an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues, volume 42.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of HLT-05, pages
475–482, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Reut Tsarfaty and Yoav Goldberg. 2008. Word-Based or
Morpheme-Based? Annotation Strategies for Modern
Hebrew Clitics. In Proceedings ofLREC-08.
Reut Tsarfaty and Khalil Sima’an. 2004. An Integrated
Model for Morphological and Syntactic Disambigua-
tion in Modern Hebrew. MOZAIEK detailed proposal,
NWO Mozaiek scheme.
Reut Tsarfaty and Khalil Sima’an. 2007. Three-
Dimensional Parametrization for Parsing Morphologi-
cally Rich Languages. In Proceedings ofIWPT-07.
Reut Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceedings ofACL-SRW-06.
Shlomo Yona and Shuly Wintner. 2005. A Finite-
state Morphological Grammar of Hebrew. In Proceed-
ings of the ACL-05 Workshop on Computational Ap-
proaches to Semitic Languages.
</reference>
<page confidence="0.999177">
379
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.523764">
<title confidence="0.999573">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</title>
<author confidence="0.831778">Yoav Goldberg Ben Gurion University of the Negev</author>
<affiliation confidence="0.999259">Department of Computer Science</affiliation>
<address confidence="0.984049">POB 653 Be’er Sheva, 84105, Israel</address>
<email confidence="0.999088">yoavg@cs.bgu.ac.il</email>
<affiliation confidence="0.946365666666667">Reut Tsarfaty Institute for Logic Language and Computation University of Amsterdam</affiliation>
<address confidence="0.97132">Plantage Muidergracht 24, Amsterdam, NL</address>
<email confidence="0.988826">rtsarfat@science.uva.nl</email>
<abstract confidence="0.999455894736842">Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>An Unsupervised Morpheme-Based HMM for Hebrew Morphological Disambiguation.</title>
<date>2006</date>
<booktitle>In Proceeding of COLINGACL-06,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2920" citStr="Adler and Elhadad, 2006" startWordPosition="425" endWordPosition="428"> and constituent-boundaries discrepancy, which breaks the assumptions underlying current state-of-the-art statistical parsers. One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint ta</context>
<context position="8881" citStr="Adler and Elhadad (2006)" startWordPosition="1331" endWordPosition="1334">g Morphological analyzers for Hebrew that analyze a surface form in isolation have been proposed by Segal (2000), Yona and Wintner (2005), and recently by the knowledge center for processing Hebrew (Itai et al., 2006). Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones. Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so far is around 98%). The development of the very first Hebrew Treebank (Sima’an et al., 2001) called for the exploration of general statistical parsing methods, but the application was at first limited. Sima’an et al. (2001) presented parsing results for a DOP tree-gram model using a small data set (500 sentences) and semiautomatic morphological disambiguation. Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank. Tsar</context>
<context position="12224" citStr="Adler and Elhadad, 2006" startWordPosition="1866" endWordPosition="1869">ted”), and not as the complex entity “that counted”. When the same token is to be interpreted as a single lexeme fmnh, it may function as a single adjective “fat”. There is no relation between these two interpretations other then the fact that their surface forms coincide, and we argue that the only reason to prefer one analysis over the other is compositional. A possible probabilistic model for assigning probabilities to complex analyses of a surface form may be P(REL, VB|fmnh, context) _ P(REL|f)P(VB|mnh, REL)P(REL, VB |context) and indeed recent sequential disambiguation models for Hebrew (Adler and Elhadad, 2006) and Arabic (Smith et al., 2005) present similar models. We suggest that in unlexicalized PCFGs the syntactic context may be explicitly modeled in the derivation probabilities. Hence, we take the probability of the event fmnh analyzed as REL VB to be P(REL → f|REL) × P(VB → mnh|VB) This means that we generate f and mnh independently depending on their corresponding PoS tags, 373 and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context. In our model, however, all lattice paths</context>
<context position="14648" citStr="Adler and Elhadad, 2006" startWordPosition="2274" endWordPosition="2277"> represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7). bclm/NNP Figure 1: The Lattice for the Hebrew Phrase bclm hneim A similar structure is used in speech recognition. There, a lattice is used to represent the possible sentences resulting from an interpretation of an acoustic model. In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions. Given that weights on all outgoing arcs sum up to one, weights induce a probability distribution on the lattice paths. In sequential tagging models such as (Adler and Elhadad, 2006; Bar-Haim et al., 2007; Smith et al., 2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1, ... , wn of space-delimited tokens. Each token may admit multiple analyses, each of which a sequence of one or more lexemes (we use li to denote a lexeme) belonging a presupposed Hebrew lexicon LEX. The entries in such a lexicon may be thought of as meaningful surface segments paired up with their PoS tags li = (si, pi), but note that a surface segment s need not be a space-delimited token. The Input The set of analyses for a token is thus represent</context>
</contexts>
<marker>Adler, Elhadad, 2006</marker>
<rawString>Meni Adler and Michael Elhadad. 2006. An Unsupervised Morpheme-Based HMM for Hebrew Morphological Disambiguation. In Proceeding of COLINGACL-06, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meni Adler</author>
<author>Yoav Goldberg</author>
<author>David Gabay</author>
<author>Michael Elhadad</author>
</authors>
<title>Unsupervised Lexicon-Based Resolution of Unknown Words for Full Morpholological Analysis.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08.</booktitle>
<marker>Adler, Goldberg, Gabay, Elhadad, 2008</marker>
<rawString>Meni Adler, Yoav Goldberg, David Gabay, and Michael Elhadad. 2008. Unsupervised Lexicon-Based Resolution of Unknown Words for Full Morpholological Analysis. In Proceedings ofACL-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meni Adler</author>
</authors>
<title>Hidden Markov Model for Hebrew Part-of-Speech Tagging. Master’s thesis,</title>
<date>2001</date>
<journal>Ben-Gurion University of the Negev.</journal>
<contexts>
<context position="13303" citStr="Adler, 2001" startWordPosition="2050" endWordPosition="2051"> derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context. In our model, however, all lattice paths are taken to be a-priori equally likely. 5 A Generative PCFG Model 4.2 Lattice Representation We represent all morphological analyses of a given utterance using a lattice structure. Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance. This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler, 2001; Bar-Haim et al., 2005; Smith et al., 2005; Cohen and Smith, 2007; Adler, 2007). Figure 1 depicts the lattice for a 2-words sentence bclm hneim. We use double-circles to indicate the space-delimited token boundaries. Note that in our construction arcs can never cross token boundaries. Every token is independent of the others, and the sentence lattice is in fact a concatenation of smaller lattices, one for each token. Furthermore, some of the arcs represent lexemes not present in the input tokens (e.g. h/DT, fl/POS), however these are parts of valid analyses of the token (cf. super-segmental m</context>
</contexts>
<marker>Adler, 2001</marker>
<rawString>Meni Adler. 2001. Hidden Markov Model for Hebrew Part-of-Speech Tagging. Master’s thesis, Ben-Gurion University of the Negev.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meni Adler</author>
</authors>
<title>Hebrew Morphological Disambiguation: An Unsupervised Stochastic Word-based Approach.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Ben-Gurion University of the Negev,</institution>
<location>Beer-Sheva, Israel.</location>
<contexts>
<context position="13383" citStr="Adler, 2007" startWordPosition="2064" endWordPosition="2065">ear context. In our model, however, all lattice paths are taken to be a-priori equally likely. 5 A Generative PCFG Model 4.2 Lattice Representation We represent all morphological analyses of a given utterance using a lattice structure. Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance. This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler, 2001; Bar-Haim et al., 2005; Smith et al., 2005; Cohen and Smith, 2007; Adler, 2007). Figure 1 depicts the lattice for a 2-words sentence bclm hneim. We use double-circles to indicate the space-delimited token boundaries. Note that in our construction arcs can never cross token boundaries. Every token is independent of the others, and the sentence lattice is in fact a concatenation of smaller lattices, one for each token. Furthermore, some of the arcs represent lexemes not present in the input tokens (e.g. h/DT, fl/POS), however these are parts of valid analyses of the token (cf. super-segmental morphology section 2). Segments with the same surface form but different PoS tags</context>
</contexts>
<marker>Adler, 2007</marker>
<rawString>Meni Adler. 2007. Hebrew Morphological Disambiguation: An Unsupervised Stochastic Word-based Approach. Ph.D. thesis, Ben-Gurion University of the Negev, Beer-Sheva, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Khalil Sima’an</author>
<author>Yoad Winter</author>
</authors>
<title>Choosing an optimal architecture for segmentation and pos- tagging of modern Hebrew.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05 Workshop on Computational Approaches to Semitic Languages.</booktitle>
<marker>Bar-Haim, Sima’an, Winter, 2005</marker>
<rawString>Roy Bar-Haim, Khalil Sima’an, and Yoad Winter. 2005. Choosing an optimal architecture for segmentation and pos- tagging of modern Hebrew. In Proceedings of ACL-05 Workshop on Computational Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Khalil Sima’an</author>
<author>Yoad Winter</author>
</authors>
<title>Part-of-speech tagging of Modern Hebrew text.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>02</issue>
<marker>Bar-Haim, Sima’an, Winter, 2007</marker>
<rawString>Roy Bar-Haim, Khalil Sima’an, and Yoad Winter. 2007. Part-of-speech tagging of Modern Hebrew text. Natural Language Engineering, 14(02):223–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chappelier</author>
<author>M Rajman</author>
<author>R Aragues</author>
<author>A Rozenknop</author>
</authors>
<title>Lattice Parsing for Speech Recognition.</title>
<date>1999</date>
<contexts>
<context position="25505" citStr="Chappelier et al., 1999" startWordPosition="4128" endWordPosition="4131"> segmentation and syntactic analysis. We experimented with increasingly rich grammars read off of the treebank. Our first model is GTplain, a PCFG learned from the treebank after removing all functional features from the syntactic categories. In our second model GTvpi we also distinguished finite and non-finite verbs and VPs as MMapping between the two schemes involves nondeterministic many-to-many mappings, and in some cases require a change in the syntactic trees. 9An open-source Hebrew spell-checker. 10Lattice parsing can be performed by special initialization of the chart in a CKY parser (Chappelier et al., 1999). We currently simulate this by crafting a WCFG and feeding it to BitPar. Given a PCFG grammar G and a lattice L with nodes n1 ... nk, we construct the weighted grammar GL as follows: for every arc (lexeme) l E L from node ni to node nj, we add to GL the rule [l --+ tni, tni+1, ... , tnj_1] with a probability of 1 (this indicates the lexeme l spans from node ni to node nj). GL is then used to parse the string tn1 ... tnk_1, where tni is a terminal corresponding to the lattice span between node ni and ni+1. Removing the leaves from the resulting tree yields a parse for L under G, with the desir</context>
</contexts>
<marker>Chappelier, Rajman, Aragues, Rozenknop, 1999</marker>
<rawString>J. Chappelier, M. Rajman, R. Aragues, and A. Rozenknop. 1999. Lattice Parsing for Speech Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Glenn Carroll</author>
<author>John Adcock</author>
<author>Anthony R Cassandra</author>
<author>Yoshihiko Gotoh</author>
<author>Jeremy Katz</author>
<author>Michael L Littman</author>
<author>John McCann</author>
</authors>
<title>Taggers for Parsers.</title>
<date>1996</date>
<pages>85--1</pages>
<publisher>AI,</publisher>
<contexts>
<context position="16471" citStr="Charniak et al., 1996" startWordPosition="2621" endWordPosition="2624">nce the lattice L for a given sentence W is determined by the morphological analyzer M we have P(π|W, M, L) Hence, our parser searches for a parse tree π over lexemes (l1 ... lk) s.t. li = (si, pi) E LEX, (l1 ... lk) E L and M(W) = L. So we remain with π� = arg max π which is precisely the formula corresponding to the so-called lattice parsing familiar from speech recognition. Every parse π selects a specific morphological segmentation (l1...lk) (a path through the lattice). This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g. (Charniak et al., 1996). 5 clm/NN clm/VB hneim/VB 0 b/IN 1 h/DT 3 bcl/NN cl/NN h/DT neim/VB 6 neim/JJ 7 2 fl/POS 4 clm/NN hm/PRP P(π|W, M) π� = arg max π P(π|L) 374 Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations. A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger, and concludes that these should </context>
</contexts>
<marker>Charniak, Carroll, Adcock, Cassandra, Gotoh, Katz, Littman, McCann, 1996</marker>
<rawString>Eugene Charniak, Glenn Carroll, John Adcock, Anthony R. Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael L. Littman, and John McCann. 1996. Taggers for Parsers. AI, 85(1-2):45–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Safiullah Shareef</author>
</authors>
<title>Parsing Arabic Dialects.</title>
<date>2006</date>
<booktitle>In Proceedings ofEACL-06.</booktitle>
<contexts>
<context position="2713" citStr="Chiang et al., 2006" startWordPosition="392" endWordPosition="395">n is modified by a proceeding space-delimited adjective. It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”). This leads to word- and constituent-boundaries discrepancy, which breaks the assumptions underlying current state-of-the-art statistical parsers. One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct </context>
</contexts>
<marker>Chiang, Diab, Habash, Rambow, Shareef, 2006</marker>
<rawString>David Chiang, Mona Diab, Nizar Habash, Owen Rambow, and Safiullah Shareef. 2006. Parsing Arabic Dialects. In Proceedings ofEACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Joint morphological and syntactic disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL-07,</booktitle>
<pages>208--217</pages>
<contexts>
<context position="3546" citStr="Cohen and Smith (2007)" startWordPosition="524" endWordPosition="527">e aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task. Cohen and Smith (2007) followed up on these results and pro371 Proceedings ofACL-08: HLT, pages 371–379, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics posed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own. Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework. We claim that no particular morphological segmentation is a-priory more likely for surface forms before exp</context>
<context position="10121" citStr="Cohen and Smith (2007)" startWordPosition="1517" endWordPosition="1520">2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation. The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty, 2006; Tsarfaty and Sima’an, 2004) and empirically explored in (Tsarfaty, 2006). Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task. Cohen and Smith (2007) later on based a system for joint inference on factored, independent, morphological and syntactic components of which scores are combined to cater for the joint inference task. Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks. 4 Model Preliminaries 4.1 The Status Space-Delimited Tokens A Hebrew surface token may have several readings, each of which corresponding to a sequence of segments and their corresponding PoS tags. We ref</context>
<context position="13369" citStr="Cohen and Smith, 2007" startWordPosition="2060" endWordPosition="2063">form fmnh. based on linear context. In our model, however, all lattice paths are taken to be a-priori equally likely. 5 A Generative PCFG Model 4.2 Lattice Representation We represent all morphological analyses of a given utterance using a lattice structure. Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance. This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler, 2001; Bar-Haim et al., 2005; Smith et al., 2005; Cohen and Smith, 2007; Adler, 2007). Figure 1 depicts the lattice for a 2-words sentence bclm hneim. We use double-circles to indicate the space-delimited token boundaries. Note that in our construction arcs can never cross token boundaries. Every token is independent of the others, and the sentence lattice is in fact a concatenation of smaller lattices, one for each token. Furthermore, some of the arcs represent lexemes not present in the input tokens (e.g. h/DT, fl/POS), however these are parts of valid analyses of the token (cf. super-segmental morphology section 2). Segments with the same surface form but diff</context>
<context position="22795" citStr="Cohen and Smith, 2007" startWordPosition="3691" endWordPosition="3694">r processing Hebrew, in which sentences from the daily newspaper “Ha’aretz” are morphologically segmented and syntactically annotated. The treebank has two versions, v1.0 and v2.0, containing 5001 and 6501 sentences respectively. We use v1.0 mainly because previous studies on joint inference reported results w.r.t. v1.0 only.5 We expect that using the same setup on v2.0 will allow a crosstreebank comparison.6 We used the first 500 sentences as our dev set and the rest 4500 for training and report our main results on this split. To facilitate the comparison of our results to those reported by (Cohen and Smith, 2007) we use their data set in which 177 empty and “malformed”7 were removed. The first 3770 trees of the resulting set then were used for training, and the last 418 are used testing. (we ignored the 419 trees in their development set.) Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses. Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incom5The comparison to performance on version 2.0 is meaningless not only because of the change in size, but also conceptual ch</context>
<context position="24646" citStr="Cohen and Smith, 2007" startWordPosition="3996" endWordPosition="3999">nalyses for unknown tokens as described in Section 5. We use the HSPELL9 (Har’el and Kenigsberg, 2004) wordlist as a lexeme-based lexicon for pruning segmentations involving invalid segments. Models that employ this strategy are denoted hsp. To control for the effect of the HSPELL-based pruning, we also experimented with a morphological analyzer that does not perform this pruning. For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists. This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis. We experimented with increasingly rich grammars read off of the treebank. Our first model is GTplain, a PCFG learned from the treebank after removing all functional features from the syntactic categories. In our second model GTvpi we also distinguished finite and non-finite verbs and VPs as MMapping between the two schemes in</context>
<context position="26727" citStr="Cohen and Smith, 2007" startWordPosition="4351" endWordPosition="4354">ed probabilities. We use a patched version of BitPar allowing for direct input of probabilities instead of counts. We thank Felix Hageloh (Hageloh, 2006) for providing us with this version. 376 proposed in (Tsarfaty, 2006). In our third model GTppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007). In our forth model GTnph we add the definiteness status of constituents following Tsarfaty and Sima’an (2007). Finally, model GTv = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima’an, 2007; Cohen and Smith, 2007). For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein. Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task. To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)). SEGTok</context>
<context position="29332" citStr="Cohen and Smith (2007)" startWordPosition="4766" endWordPosition="4769"> the syntactic performance as well as morphological disambiguation (segmentation and POS tagging) accuracy. This supports our main thesis that decisions taken by single, improved, grammar are beneficial for both tasks. When using the segmentation pruning (using HSPELL) for unseen tokens, performance improves for all tasks as well. Yet we note that the better grammars without pruning outperform the poorer grammars using this technique, indicating that the syntactic context aids, to some extent, the disambiguation of unknown tokens. Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks. Model SEGTok CPOS FPOS SYNCS GTnohsp/pln 89.50 81.00 77.65 62.22 GTnohsp/···+nph 89.58 81.26 77.82 64.30 CSpl. 91.10 80.40 75.60 64.00 CS„=2 90.90 80.50 75.40 64.40 GThsp/pln 93.13 83.12 79.12 64.46 GTnohsp/···+v=2 89.66 82.85 78.92 66.31 Oracle CSpl, 91.80 83.20 79.10 66.50 Oracle CS„=2 91.70 83.00 78.70 67.40 GThsp/···+v=2 93.38 85.08 80.11 69.11 Table 2: Segmentation, Parsing and Tagging Results using the Setup of (Cohen and Smith, 2007) (sentence length &lt; 40). The Models’ are Ordered by Performance. We first note that the accuracy r</context>
<context position="30592" citStr="Cohen and Smith (2007)" startWordPosition="4961" endWordPosition="4965">r on their setup, on all measures, indicating that theirs may be an easier dataset. Secondly, for all our models we provide better fine- and coarse-grained POS-tagging accuracy, and all pruned models outperform the Oracle results reported by them.12 In terms of syntactic disambiguation, even the simplest grammar pruned with HSPELL outperforms their non-Oracle results. Without HSPELL-pruning, our simpler grammars are somewhat lagging behind, but as the grammars improve the gap is bridged. The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks. This essentially means that their model does not result in a true joint inference, as executions for different tasks involve tuning a parameter separately. In our model there are no such hyper-parameters, and the performance is the result of truly joint disambiguation. 377 Model U SEGTok / no H SEGF CPOS FPOS SYN / SYNCS GS SYN GTnohsp/pln 7 89.77 / 93.18 91.80 80.36 76.77 60.41 / 61.66 65.00 ···+vpi 7 89.80 / 93.18 91.84 80.37 76.74 61.16 / 62.41 66.70 ···+ppp 7 89.79 / 93.20 91.86 80.43 76.79 61.47 / 62.86 67.22 ··</context>
</contexts>
<marker>Cohen, Smith, 2007</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2007. Joint morphological and syntactic disambiguation. In Proceedings of EMNLP-CoNLL-07, pages 208–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lewis Glinert</author>
</authors>
<title>The Grammar of Modern Hebrew.</title>
<date>1989</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1748" citStr="Glinert, 1989" startWordPosition="241" endWordPosition="242">er the best published results so far. 1 Introduction Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, ‘tokens’) that constitute the unanalyzed surface forms (utterances). In Semitic languages the situation is very different. In Modern Hebrew (Hebrew), a Semitic language with very rich morphology, particles marking conjunctions, prepositions, complementizers and relativizers are bound elements prefixed to the word (Glinert, 1989). The Hebrew token ‘bcl’1, for example, stands for the complete prepositional phrase &apos;We adopt here the transliteration of (Sima’an et al., 2001). “in the shadow”. This token may further embed into a larger utterance, e.g., ‘bcl hneim’ (literally “in-the-shadow the-pleasant”, meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective. It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”). This leads to word- and constituent-boundaries discrepancy, which break</context>
</contexts>
<marker>Glinert, 1989</marker>
<rawString>Lewis Glinert. 1989. The Grammar of Modern Hebrew. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>SVM Model Tampering and Anchored Learning: A Case Study</title>
<date>2007</date>
<booktitle>in Hebrew NP Chunking. In Proceeding of ACL-07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="26458" citStr="Goldberg and Elhadad (2007)" startWordPosition="4306" endWordPosition="4309">cates the lexeme l spans from node ni to node nj). GL is then used to parse the string tn1 ... tnk_1, where tni is a terminal corresponding to the lattice span between node ni and ni+1. Removing the leaves from the resulting tree yields a parse for L under G, with the desired probabilities. We use a patched version of BitPar allowing for direct input of probabilities instead of counts. We thank Felix Hageloh (Hageloh, 2006) for providing us with this version. 376 proposed in (Tsarfaty, 2006). In our third model GTppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007). In our forth model GTnph we add the definiteness status of constituents following Tsarfaty and Sima’an (2007). Finally, model GTv = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima’an, 2007; Cohen and Smith, 2007). For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein. Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task. To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segme</context>
</contexts>
<marker>Goldberg, Elhadad, 2007</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2007. SVM Model Tampering and Anchored Learning: A Case Study in Hebrew NP Chunking. In Proceeding of ACL-07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<date>2008</date>
<booktitle>EM Can Find Pretty G]ood HMM POS-Taggers (When Given a Good Start), booktitle = Proceedings of ACL08, year =</booktitle>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. EM Can Find Pretty G]ood HMM POS-Taggers (When Given a Good Start), booktitle = Proceedings of ACL08, year = 2008,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proceeding of ACL-05.</booktitle>
<contexts>
<context position="11207" citStr="Habash and Rambow, 2005" startWordPosition="1694" endWordPosition="1697">ew surface token may have several readings, each of which corresponding to a sequence of segments and their corresponding PoS tags. We refer to different readings as different analyses whereby the segments are deterministic given the sequence of PoS tags. We refer to a segment and its assigned PoS tag as a lexeme, and so analyses are in fact sequences of lexemes. For brevity we omit the segments from the analysis, and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB. Such tag sequences are often treated as “complex tags” (e.g. REL+VB) (cf. (Bar-Haim et al., 2007; Habash and Rambow, 2005)) and probabilities are assigned to different analyses in accordance with the likelihood of their tags (e.g., “fmnh is 30% likely to be tagged NN and 70% likely to be tagged REL+VB”). Here we do not submit to this view. When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB, the analysis introduces two distinct entities, the relativizer f (“that”) and the verb mnh (“counted”), and not as the complex entity “that counted”. When the same token is to be interpreted as a single lexeme fmnh, it may function as a single adjective “fat”. There is no relation between these two inte</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In Proceeding of ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hageloh</author>
</authors>
<title>Parsing Using Transforms over Treebanks. Master’s thesis,</title>
<date>2006</date>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="26258" citStr="Hageloh, 2006" startWordPosition="4275" endWordPosition="4276">onstruct the weighted grammar GL as follows: for every arc (lexeme) l E L from node ni to node nj, we add to GL the rule [l --+ tni, tni+1, ... , tnj_1] with a probability of 1 (this indicates the lexeme l spans from node ni to node nj). GL is then used to parse the string tn1 ... tnk_1, where tni is a terminal corresponding to the lattice span between node ni and ni+1. Removing the leaves from the resulting tree yields a parse for L under G, with the desired probabilities. We use a patched version of BitPar allowing for direct input of probabilities instead of counts. We thank Felix Hageloh (Hageloh, 2006) for providing us with this version. 376 proposed in (Tsarfaty, 2006). In our third model GTppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007). In our forth model GTnph we add the definiteness status of constituents following Tsarfaty and Sima’an (2007). Finally, model GTv = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima’an, 2007; Cohen and Smith, 2007). For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein. Evaluation We use 8 d</context>
</contexts>
<marker>Hageloh, 2006</marker>
<rawString>Felix Hageloh. 2006. Parsing Using Transforms over Treebanks. Master’s thesis, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadav Har’el and Dan Kenigsberg</author>
</authors>
<title>HSpell - the free Hebrew Spell Checker and Morphological Analyzer. Israeli Seminar on Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="24126" citStr="Kenigsberg, 2004" startWordPosition="3911" endWordPosition="3912"> to missing tokens-morphemes alignment in the v2.0 treebank. 7We thank Shay Cohen for providing us with their data set and evaluation Software. patible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007). We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses. Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5. We use the HSPELL9 (Har’el and Kenigsberg, 2004) wordlist as a lexeme-based lexicon for pruning segmentations involving invalid segments. Models that employ this strategy are denoted hsp. To control for the effect of the HSPELL-based pruning, we also experimented with a morphological analyzer that does not perform this pruning. For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists. This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schm</context>
</contexts>
<marker>Kenigsberg, 2004</marker>
<rawString>Nadav Har’el and Dan Kenigsberg. 2004. HSpell - the free Hebrew Spell Checker and Morphological Analyzer. Israeli Seminar on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Itai</author>
<author>Shuly Wintner</author>
<author>Shlomo Yona</author>
</authors>
<title>A Computational Lexicon of Contemporary Hebrew.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC-06.</booktitle>
<contexts>
<context position="8474" citStr="Itai et al., 2006" startWordPosition="1268" endWordPosition="1271">ategories) with respect to a surface form. Such discrepancies can be aligned via an intermediate level of PoS tags. PoS tags impose a unique morphological segmentation on surface tokens and present a unique valid yield for syntactic trees. The correct ambiguity resolution of the syntactic level therefore helps to resolve the morphological one, and vice versa. 372 3 Previous Work on Hebrew Processing Morphological analyzers for Hebrew that analyze a surface form in isolation have been proposed by Segal (2000), Yona and Wintner (2005), and recently by the knowledge center for processing Hebrew (Itai et al., 2006). Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones. Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so far is around 98%). The development of the very first Hebrew Treebank (Sima’an et al., 2001) called for </context>
<context position="23217" citStr="Itai et al., 2006" startWordPosition="3762" endWordPosition="3765">irst 500 sentences as our dev set and the rest 4500 for training and report our main results on this split. To facilitate the comparison of our results to those reported by (Cohen and Smith, 2007) we use their data set in which 177 empty and “malformed”7 were removed. The first 3770 trees of the resulting set then were used for training, and the last 418 are used testing. (we ignored the 419 trees in their development set.) Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses. Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incom5The comparison to performance on version 2.0 is meaningless not only because of the change in size, but also conceptual changes in the annotation scheme 6Unfortunatley running our setup on the v2.0 data set is currently not possible due to missing tokens-morphemes alignment in the v2.0 treebank. 7We thank Shay Cohen for providing us with their data set and evaluation Software. patible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smit</context>
<context position="34544" citStr="Itai et al., 2006" startWordPosition="5602" endWordPosition="5605">h state-of-the-art standalone applications results. Better grammars are shown here to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones. We conjecture that this trend may continue by incorporating additional information, e.g., three-dimensional models as proposed by Tsarfaty and Sima’an (2007). In the current work morphological analyses and lexical probabilities are derived from a small Treebank, which is by no means the best way to go. Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf. (Levinger et al., 1995; Goldberg et al., ; Adler et al., 2008)) will make the parser more robust and suitable for use in more realistic scenarios. Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion. We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal. The work of the first author was supported by the Lynn and William Frankel Center for Computer Sciences. The work of the </context>
</contexts>
<marker>Itai, Wintner, Yona, 2006</marker>
<rawString>Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A Computational Lexicon of Contemporary Hebrew. In Proceedings ofLREC-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Levinger</author>
<author>Uzi Ornan</author>
<author>Alon Itai</author>
</authors>
<title>Learning Morpholexical Probabilities from an Untagged Corpus with an Application to Hebrew. Computational Linguistics,</title>
<date>1995</date>
<pages>21--383</pages>
<marker>Levinger, Ornan, Itai, 1995</marker>
<rawString>Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learning Morpholexical Probabilities from an Untagged Corpus with an Application to Hebrew. Computational Linguistics, 21:383–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>LoPar: Design and Implementation.</title>
<date>2000</date>
<institution>Institute for Computational Linguistics, University of Stuttgart.</institution>
<contexts>
<context position="9941" citStr="Schmid, 2000" startWordPosition="1492" endWordPosition="1493">ion. Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank. Tsarfaty and Sima’an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation. The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty, 2006; Tsarfaty and Sima’an, 2004) and empirically explored in (Tsarfaty, 2006). Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task. Cohen and Smith (2007) later on based a system for joint inference on factored, independent, morphological and syntactic components of which scores are combined to cater for the joint inference task. Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks. 4 Model Preliminaries 4.</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>Helmut Schmid, 2000. LoPar: Design and Implementation. Institute for Computational Linguistics, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vector.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04.</booktitle>
<contexts>
<context position="24735" citStr="Schmid, 2004" startWordPosition="4013" endWordPosition="4014">004) wordlist as a lexeme-based lexicon for pruning segmentations involving invalid segments. Models that employ this strategy are denoted hsp. To control for the effect of the HSPELL-based pruning, we also experimented with a morphological analyzer that does not perform this pruning. For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists. This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis. We experimented with increasingly rich grammars read off of the treebank. Our first model is GTplain, a PCFG learned from the treebank after removing all functional features from the syntactic categories. In our second model GTvpi we also distinguished finite and non-finite verbs and VPs as MMapping between the two schemes involves nondeterministic many-to-many mappings, and in some cases require a change in the </context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vector. In Proceedings of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erel Segal</author>
</authors>
<title>Hebrew Morphological Analyzer for Hebrew Undotted Texts. Master’s thesis,</title>
<date>2000</date>
<location>Technion, Haifa,</location>
<contexts>
<context position="8369" citStr="Segal (2000)" startWordPosition="1252" endWordPosition="1254">tween token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form. Such discrepancies can be aligned via an intermediate level of PoS tags. PoS tags impose a unique morphological segmentation on surface tokens and present a unique valid yield for syntactic trees. The correct ambiguity resolution of the syntactic level therefore helps to resolve the morphological one, and vice versa. 372 3 Previous Work on Hebrew Processing Morphological analyzers for Hebrew that analyze a surface form in isolation have been proposed by Segal (2000), Yona and Wintner (2005), and recently by the knowledge center for processing Hebrew (Itai et al., 2006). Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones. Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so</context>
<context position="9858" citStr="Segal, 2000" startWordPosition="1478" endWordPosition="1479">using a small data set (500 sentences) and semiautomatic morphological disambiguation. Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank. Tsarfaty and Sima’an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation. The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty, 2006; Tsarfaty and Sima’an, 2004) and empirically explored in (Tsarfaty, 2006). Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task. Cohen and Smith (2007) later on based a system for joint inference on factored, independent, morphological and syntactic components of which scores are combined to cater for the joint inference task. Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has</context>
</contexts>
<marker>Segal, 2000</marker>
<rawString>Erel Segal. 2000. Hebrew Morphological Analyzer for Hebrew Undotted Texts. Master’s thesis, Technion, Haifa, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danny Shacham</author>
<author>Shuly Wintner</author>
</authors>
<title>Morphological Disambiguation of Hebrew: A Case Study in Classifier Combination.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL-07,</booktitle>
<pages>439--447</pages>
<contexts>
<context position="8909" citStr="Shacham and Wintner (2007)" startWordPosition="1335" endWordPosition="1338">for Hebrew that analyze a surface form in isolation have been proposed by Segal (2000), Yona and Wintner (2005), and recently by the knowledge center for processing Hebrew (Itai et al., 2006). Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones. Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so far is around 98%). The development of the very first Hebrew Treebank (Sima’an et al., 2001) called for the exploration of general statistical parsing methods, but the application was at first limited. Sima’an et al. (2001) presented parsing results for a DOP tree-gram model using a small data set (500 sentences) and semiautomatic morphological disambiguation. Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank. Tsarfaty and Sima’an (2007) have</context>
</contexts>
<marker>Shacham, Wintner, 2007</marker>
<rawString>Danny Shacham and Shuly Wintner. 2007. Morphological Disambiguation of Hebrew: A Case Study in Classifier Combination. In Proceedings of EMNLPCoNLL-07, pages 439–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
<author>Alon Itai</author>
<author>Yoad Winter</author>
<author>Alon Altman</author>
<author>Noa Nativ</author>
</authors>
<title>Building a Tree-Bank for Modern Hebrew Text.</title>
<date>2001</date>
<booktitle>In Traitement Automatique des Langues,</booktitle>
<volume>42</volume>
<marker>Sima’an, Itai, Winter, Altman, Nativ, 2001</marker>
<rawString>Khalil Sima’an, Alon Itai, Yoad Winter, Alon Altman, and Noa Nativ. 2001. Building a Tree-Bank for Modern Hebrew Text. In Traitement Automatique des Langues, volume 42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-based morphological disambiguation with random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-05,</booktitle>
<pages>475--482</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="12256" citStr="Smith et al., 2005" startWordPosition="1872" endWordPosition="1875">“that counted”. When the same token is to be interpreted as a single lexeme fmnh, it may function as a single adjective “fat”. There is no relation between these two interpretations other then the fact that their surface forms coincide, and we argue that the only reason to prefer one analysis over the other is compositional. A possible probabilistic model for assigning probabilities to complex analyses of a surface form may be P(REL, VB|fmnh, context) _ P(REL|f)P(VB|mnh, REL)P(REL, VB |context) and indeed recent sequential disambiguation models for Hebrew (Adler and Elhadad, 2006) and Arabic (Smith et al., 2005) present similar models. We suggest that in unlexicalized PCFGs the syntactic context may be explicitly modeled in the derivation probabilities. Hence, we take the probability of the event fmnh analyzed as REL VB to be P(REL → f|REL) × P(VB → mnh|VB) This means that we generate f and mnh independently depending on their corresponding PoS tags, 373 and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context. In our model, however, all lattice paths are taken to be a-priori equall</context>
<context position="14692" citStr="Smith et al., 2005" startWordPosition="2282" endWordPosition="2285"> labeled neim from node 6 to 7). bclm/NNP Figure 1: The Lattice for the Hebrew Phrase bclm hneim A similar structure is used in speech recognition. There, a lattice is used to represent the possible sentences resulting from an interpretation of an acoustic model. In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions. Given that weights on all outgoing arcs sum up to one, weights induce a probability distribution on the lattice paths. In sequential tagging models such as (Adler and Elhadad, 2006; Bar-Haim et al., 2007; Smith et al., 2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1, ... , wn of space-delimited tokens. Each token may admit multiple analyses, each of which a sequence of one or more lexemes (we use li to denote a lexeme) belonging a presupposed Hebrew lexicon LEX. The entries in such a lexicon may be thought of as meaningful surface segments paired up with their PoS tags li = (si, pi), but note that a surface segment s need not be a space-delimited token. The Input The set of analyses for a token is thus represented as a lattice in which every arc correspon</context>
</contexts>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Noah A. Smith, David A. Smith, and Roy W. Tromble. 2005. Context-based morphological disambiguation with random fields. In Proceedings of HLT-05, pages 475–482, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Yoav Goldberg</author>
</authors>
<title>Word-Based or Morpheme-Based? Annotation Strategies for Modern Hebrew Clitics.</title>
<date>2008</date>
<booktitle>In Proceedings ofLREC-08.</booktitle>
<contexts>
<context position="6867" citStr="Tsarfaty and Goldberg, 2008" startWordPosition="1016" endWordPosition="1019">espond to the original surface form as super-segmental morphology. An additional case of super-segmental morphology is the case of Pronominal Clitics. Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements. The additional morphological material in such cases appears after the stem and realizes the extended meaning. The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008). Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens. The form fmnh, for example, can be understood as the verb “lubricated”, the possessed noun “her oil”, the adjective “fat” or the verb “got fat”. Furthermore, the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities. The same form fmnh c</context>
</contexts>
<marker>Tsarfaty, Goldberg, 2008</marker>
<rawString>Reut Tsarfaty and Yoav Goldberg. 2008. Word-Based or Morpheme-Based? Annotation Strategies for Modern Hebrew Clitics. In Proceedings ofLREC-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Khalil Sima’an</author>
</authors>
<title>An Integrated Model for Morphological and Syntactic Disambiguation in Modern Hebrew. MOZAIEK detailed proposal, NWO Mozaiek scheme.</title>
<date>2004</date>
<marker>Tsarfaty, Sima’an, 2004</marker>
<rawString>Reut Tsarfaty and Khalil Sima’an. 2004. An Integrated Model for Morphological and Syntactic Disambiguation in Modern Hebrew. MOZAIEK detailed proposal, NWO Mozaiek scheme.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Khalil Sima’an</author>
</authors>
<title>ThreeDimensional Parametrization for Parsing Morphologically Rich Languages.</title>
<date>2007</date>
<booktitle>In Proceedings ofIWPT-07.</booktitle>
<marker>Tsarfaty, Sima’an, 2007</marker>
<rawString>Reut Tsarfaty and Khalil Sima’an. 2007. ThreeDimensional Parametrization for Parsing Morphologically Rich Languages. In Proceedings ofIWPT-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
</authors>
<title>Integrated Morphological and Syntactic Disambiguation for Modern Hebrew.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL-SRW-06.</booktitle>
<contexts>
<context position="3254" citStr="Tsarfaty (2006)" startWordPosition="483" endWordPosition="484"> all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task. Cohen and Smith (2007) followed up on these results and pro371 Proceedings ofACL-08: HLT, pages 371–379, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics posed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own. Here we</context>
<context position="4520" citStr="Tsarfaty, 2006" startWordPosition="665" endWordPosition="666">ard and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework. We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures, including manifestations of various long-distance dependencies. Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2 2 Modern Hebrew Structure Segmental morphology Hebrew consists of seven particles m(“from”) f(“when”/“who”/“that”) h(“the”) w(“and”) k(“like”) l(“to”) and b(“in”). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem. Several such particles may be prefixed onto a single stem, in which case the affixation is subject to strict linear precedence constraints. Co-occurrences among the</context>
<context position="9348" citStr="Tsarfaty (2006)" startWordPosition="1408" endWordPosition="1409">e the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so far is around 98%). The development of the very first Hebrew Treebank (Sima’an et al., 2001) called for the exploration of general statistical parsing methods, but the application was at first limited. Sima’an et al. (2001) presented parsing results for a DOP tree-gram model using a small data set (500 sentences) and semiautomatic morphological disambiguation. Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank. Tsarfaty and Sima’an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation. The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty, 2006; Tsarfaty and Sima’an, 2004) and empirically explored in (Tsarfaty, 2006). Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an </context>
<context position="26327" citStr="Tsarfaty, 2006" startWordPosition="4286" endWordPosition="4287">l E L from node ni to node nj, we add to GL the rule [l --+ tni, tni+1, ... , tnj_1] with a probability of 1 (this indicates the lexeme l spans from node ni to node nj). GL is then used to parse the string tn1 ... tnk_1, where tni is a terminal corresponding to the lattice span between node ni and ni+1. Removing the leaves from the resulting tree yields a parse for L under G, with the desired probabilities. We use a patched version of BitPar allowing for direct input of probabilities instead of counts. We thank Felix Hageloh (Hageloh, 2006) for providing us with this version. 376 proposed in (Tsarfaty, 2006). In our third model GTppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007). In our forth model GTnph we add the definiteness status of constituents following Tsarfaty and Sima’an (2007). Finally, model GTv = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima’an, 2007; Cohen and Smith, 2007). For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein. Evaluation We use 8 different measures to evaluate the performance of our system on the jo</context>
<context position="27652" citStr="Tsarfaty (2006)" startWordPosition="4495" endWordPosition="4496">monic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)). SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure. Evaluating parsing results in our joint framework, as argued by Tsarfaty (2006), is not trivial under the joint disambiguation task, as the hypothesized yield need not coincide with the correct one. Our parsing performance measures (SY N) thus report the PARSEVAL extension proposed in Tsarfaty (2006). We further report SYNCS, the parsing metric of Cohen and Smith (2007), to facilitate the comparison. We report the F1 value of both measures. Finally, our U (unparsed) measure is used to report the number of sentences to which our system could not propose a joint analysis. 7 Results and Analysis The accuracy results for segmentation, tagging and parsing using our different </context>
</contexts>
<marker>Tsarfaty, 2006</marker>
<rawString>Reut Tsarfaty. 2006. Integrated Morphological and Syntactic Disambiguation for Modern Hebrew. In Proceedings ofACL-SRW-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Yona</author>
<author>Shuly Wintner</author>
</authors>
<title>A Finitestate Morphological Grammar of Hebrew.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-05 Workshop on Computational Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="8394" citStr="Yona and Wintner (2005)" startWordPosition="1255" endWordPosition="1258">undaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form. Such discrepancies can be aligned via an intermediate level of PoS tags. PoS tags impose a unique morphological segmentation on surface tokens and present a unique valid yield for syntactic trees. The correct ambiguity resolution of the syntactic level therefore helps to resolve the morphological one, and vice versa. 372 3 Previous Work on Hebrew Processing Morphological analyzers for Hebrew that analyze a surface form in isolation have been proposed by Segal (2000), Yona and Wintner (2005), and recently by the knowledge center for processing Hebrew (Itai et al., 2006). Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones. Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so far is around 98%). The </context>
</contexts>
<marker>Yona, Wintner, 2005</marker>
<rawString>Shlomo Yona and Shuly Wintner. 2005. A Finitestate Morphological Grammar of Hebrew. In Proceedings of the ACL-05 Workshop on Computational Approaches to Semitic Languages.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>