<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002523">
<title confidence="0.994523">
Unsupervised Semantic Role Induction with Global Role Ordering
</title>
<author confidence="0.99736">
Nikhil Garg James Henderson
</author>
<affiliation confidence="0.809318">
University of Geneva University of Geneva
Switzerland Switzerland
</affiliation>
<email confidence="0.997126">
nikhil.garg@unige.ch james.henderson@unige.ch
</email>
<sectionHeader confidence="0.995605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999412454545455">
We propose a probabilistic generative model
for unsupervised semantic role induction,
which integrates local role assignment deci-
sions and a global role ordering decision in a
unified model. The role sequence is divided
into intervals based on the notion of primary
roles, and each interval generates a sequence
of secondary roles and syntactic constituents
using local features. The global role ordering
consists of the sequence of primary roles only,
thus making it a partial ordering.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999481714285714">
Unsupervised semantic role induction has gained
significant interest recently (Lang and Lapata,
2011b) due to limited amounts of annotated corpora.
A Semantic Role Labeling (SRL) system should
provide consistent argument labels across different
syntactic realizations of the same verb (Palmer et al.,
2005), as in
</bodyText>
<figure confidence="0.466248">
(a.) [ Mark ]A0 drove [ the car ]A1
(b.) [ The car ]A1 was driven by [ Mark ]A0
</figure>
<bodyText confidence="0.999948311111111">
This simple example also shows that while certain
local syntactic and semantic features could provide
clues to the semantic role label of a constituent, non-
local features such as predicate voice could provide
information about the expected semantic role se-
quence. Sentence a is in active voice with sequence
(A0, PREDICATE, A1) and sentence b is in passive
voice with sequence (A1, PREDICATE, A0). Addi-
tional global preferences, such as arguments A0 and
A1 rarely repeat in a frame (as seen in the corpus),
could also be useful in addition to local features.
Supervised SRL systems have mostly used local
classifiers that assign a role to each constituent inde-
pendently of others, and only modeled limited cor-
relations among roles in a sequence (Toutanova et
al., 2008). The correlations have been modeled via
role sets (Gildea and Jurafsky, 2002), role repeti-
tion constraints (Punyakanok et al., 2004), language
model over roles (Thompson et al., 2003; Pradhan
et al., 2005), and global role sequence (Toutanova
et al., 2008). Unsupervised SRL systems have ex-
plored even fewer correlations. Lang and Lapata
(2011a; 2011b) use the relative position (left/right)
of the argument w.r.t. the predicate. Grenager and
Manning (2006) use an ordering of the linking of se-
mantic roles and syntactic relations. However, as the
space of possible linkings is large, language-specific
knowledge is used to constrain this space.
Similar to Toutanova et al. (2008), we propose to
use global role ordering preferences but in a gener-
ative model in contrast to their discriminative one.
Further, unlike Grenager and Manning (2006), we
do not explicitly generate the linking of semantic
roles and syntactic relations, thus keeping the pa-
rameter space tractable. The main contribution of
this work is an unsupervised model that uses global
role ordering and repetition preferences without as-
suming any language-specific constraints.
Following Gildea and Jurafsky (2002), previous
work has typically broken the SRL task into (i) argu-
ment identification, and (ii) argument classification
(M`arquez et al., 2008). The latter is our focus in this
work. Given the dependency parse tree of a sentence
with correctly identified arguments, the aim is to as-
sign a semantic role label to each argument.
</bodyText>
<page confidence="0.992971">
145
</page>
<note confidence="0.8175955">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 145–149,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.974967222222222">
Algorithm 1 Generative process
PARAMETERS
for all predicate p do
for all voice vc ∈ {active, passive} do
draw �order
p,vc ∼ Dirichlet(αorder)
for all interval I do
draw �SR
p,I ∼ Dirichlet(αSR)
for all adjacency adj ∈ {0, 1} do
draw �ST OP
p,I,adj ∼ Beta(αSTOP)
for all role r ∈ PR ∪ SR do
for all feature type f do
draw BFp,r,f ∼ Dirichlet(αF )
DATA
given a predicate p with voice vc:
choose an ordering o ∼ Multinomial(�order
p,vc )
for all interval I ∈ o do
draw an indicators ∼ Binomial(BpSTI OP)
,0
while s # STOP do
choose a SR r ∼ Multinomial(BSR
p,I )
draw an indicator s ∼ Binomial(BSTOP)
p,I,1
</figure>
<bodyText confidence="0.548573333333333">
for all generated roles r do
for all feature type f do
choose a value vf ∼ Multinomial(BFp,r,f)
</bodyText>
<sectionHeader confidence="0.967678" genericHeader="method">
2 Proposed Model
</sectionHeader>
<bodyText confidence="0.996710166666667">
We assume the roles to be predicate-specific. We
begin by introducing a few terms:
Primary Role (PR) For every predicate, we assume
the existence of K primary roles (PRs) denoted by
P1, P2, ..., PK. These roles are not allowed to re-
peat in a frame and serve as “anchor points” in the
global role ordering. Intuitively, the model attempts
to choose PRs such that they occur with high fre-
quency, do not repeat, and their ordering influences
the positioning of other roles. Note that a PR may
correspond to either a core role or a modifier role.
For ease of explication, we create 3 additional PRs:
START denoting the start of the role sequence, END
denoting its end, and PRED denoting the predicate.
Secondary Role (SR) The roles that are not PRs are
called secondary roles (SRs). Given N roles in total,
there are (N − K) SRs, denoted by S1, S2, ..., SN_K.
Unlike PRs, SRs are not constrained to occur only
once in a frame and do not participate in the global
role ordering.
Interval An interval is a sequence of SRs bounded
by PRs, for instance (P2, S3, S5, PRED).
Ordering An ordering is the sequence of PRs ob-
served in a frame. For example, if the complete role
</bodyText>
<figureCaption confidence="0.997741">
Figure 1: Proposed model. Shaded and unshaded
nodes represent visible and hidden variables resp.
</figureCaption>
<bodyText confidence="0.98186296969697">
sequence is (START, P2, S1, S1, PRED, S3, END), the
ordering is defined as (START, P2, PRED, END).
Features We have explored 1 frame level (global)
feature (i) voice: active/passive, and 3 argument
level (local) features (i) deprel: dependency relation
of an argument to its head in the dependency parse
tree, (ii) head: head word of the argument, and (iii)
pos-head: Part-of-Speech tag of head.
Algorithm 1 describes the generative story of our
model and Figure 1 illustrates it graphically. Given a
predicate and its voice, an ordering is selected from
a multinomial. This ordering gives us the sequence
of PRs (PR1, PR2, ..., PRN). Each pair of consec-
utive PRs, PRi, PRi+1, in an ordering corresponds
to an interval Ii. For each such interval, we generate
0 or more SRs (SRi1, SRi2, ...SRim) as follows.
Generate an indicator variable: CONTINUE/STOP
from a binomial distribution. If CONTINUE, gen-
erate a SR from the multinomial corresponding to
the interval. Generate another indicator variable and
continue the process till a STOP has been generated.
In addition to the interval, the indicator variable also
depends on whether we are generating the first SR
(adj = 0) or a subsequent one (adj = 1). For each
role, primary as well as secondary, we now generate
the corresponding constituent by generating each of
its features independently (F1, F2,..., FT ).
Given a frame instance with predicate p and voice
vc, Figure 2 gives (i) Eq. 1: the joint distribution
of the ordering o, role sequence r, and constituent
sequence f, and (ii) Eq. 2: the marginal distribution
of an instance. The likelihood of the whole corpus
is the product of marginals of individual instances.
</bodyText>
<page confidence="0.992772">
146
</page>
<table confidence="0.852762625">
P(o, r, f|p, vc) = P(o|p, vc)
{z
ordering
* Π{riErnPR}P(fi|ri,p)
{z
Primary Roles
* Π{IEo}P(r(I),f(I)|I,p)
 |{z }
Intervals
(1)
Y P(continue|I, p, adj) P (ri|I, p) P(fi|ri,p) * P(stop|I, p, adj)
where P(r(I), f(I)|I, p) =  |{z }   ||{z }  |{z }
riEr(I) generate indicator {z } generate features end of the interval
generate SR
and P(fi|ri,p) = ΠtP(fi,t|ri,p)
P(f|p, vc) = ΣoΣ{rEseq(o)}P(o, r, f|p, vc) where seq(o) = {role sequences allowed under ordering o} (2)
</table>
<figureCaption confidence="0.9936925">
Figure 2: ri and fi denote the role and features at position i respectively, and r(I) and f(I) respectively
denote the SR sequence and feature sequence in interval I. fi,t denotes the value of feature t at position i.
</figureCaption>
<bodyText confidence="0.998909111111111">
This particular choice of model is inspired from
different sources. Firstly, making the role order-
ing dependent only on PRs aligns with the obser-
vation by Pradhan et al. (2005) and Toutanova et
al. (2008) that including the ordering information
of only core roles helped improve the SRL perfor-
mance as opposed to the complete role sequence.
Although our assumption here is softer in that we
assume the existence of some roles which define
the ordering which may or may not correspond to
core roles. Secondly, generating the SRs indepen-
dently of each other given the interval is based on
the intuition that knowing the core roles informs
us about the expected non-core roles that occur be-
tween them. This intuition is supported by the statis-
tics in the annotated data, where we found that if we
consider the core roles as PRs, then most of the in-
tervals tend to have only a few types of SRs and a
given SR tends to occur only in a few types of in-
tervals. The concept of intervals is also related to
the linguistic theory of topological fields (Diderich-
sen, 1966; Drach, 1937). This simplifying assump-
tion that given the PRs at the interval boundary, the
SRs in that interval are independent of the other
roles in the sequence, keeps the parameter space lim-
ited, which helps unsupervised learning. Thirdly,
not allowing some or all roles to repeat has been
employed as a useful constraint in previous work
(Punyakanok et al., 2004; Lang and Lapata, 2011b),
which we use here for PRs. Lastly, conditioning the
(STOP/CONTINUE) indicator variable on the adja-
cency value (adj) is inspired from the DMV model
(Klein and Manning, 2004) for unsupervised depen-
dency parsing. We found in the annotated corpus
that if we map core roles to PRs, then most of the
time the intervals do not generate any SRs at all. So,
the probability to STOP should be very high when
generating the first SR.
We use an EM procedure to train the model. In
the E-step, we calculate the expected counts of all
the hidden variables in our model using the Inside-
Outside algorithm (Baker, 1979). In the M-step, we
add the counts corresponding to the Bayesian priors
to the expected counts and use the resulting counts
to calculate the MAP estimate of the parameters.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9998696875">
Following the experimental settings of Lang and La-
pata (2011b), we use the CoNLL 2008 shared task
dataset (Surdeanu et al., 2008), only consider ver-
bal predicates, and run unsupervised training on the
standard training set. The evaluation measures are
also the same: (i) Purity (PU) that measures how
well an induced cluster corresponds to a single gold
role, (ii) Collocation (CO) that measures how well
a gold role corresponds to a single induced cluster,
and (iii) F1 which is the harmonic mean of PU and
CO. Final scores are computed by weighting each
predicate by the number of its argument instances.
We chose a uniform Dirichlet prior with concentra-
tion parameter as 0.1 for all the model parameters
in Algorithm 1 (set roughly, without optimization1).
50 training iterations were used.
</bodyText>
<subsectionHeader confidence="0.683148">
3.1 Results
</subsectionHeader>
<bodyText confidence="0.999913">
Since the dataset has 21 semantic roles in total, we
fix the total number of roles in our model to be 21.
Further, we set the number of PRs to 2 (excluding
START, END and PRED), and SRs to 21-2=19.
</bodyText>
<footnote confidence="0.970545">
1Removing the Bayesian priors completely, resulted in the
EM algorithm getting to a local maxima quite early, giving a
substantially lower performance.
</footnote>
<page confidence="0.990446">
147
</page>
<table confidence="0.9989245">
Model Features PU CO F1
0 Baseline d 81.6 78.1 79.8
1a Proposed d 82.3 78.6 80.4
1b Proposed d,h 82.7 77.2 79.9
1c Proposed d,p-h 83.5 78.5 80.9
1d Proposed d,p-h,h 83.2 77.1 80.0
</table>
<tableCaption confidence="0.861428">
Table 1: Evaluation. d refers to deprel, h refers to
head and p-h refers to pos-head.
Table 1 gives the results using different feature
</tableCaption>
<bodyText confidence="0.99745056">
combinations. Line 0 reports the performance of
Lang and Lapata (2011b)’s baseline, which has been
shown difficult to outperform. This baseline maps
20 most frequent deprel to a role each, and the rest
are mapped to the 21st role. By just using deprel as
a feature, the proposed model outperforms the base-
line by 0.6 points in terms of F1 score. In this con-
figuration, the only addition over the baseline is the
ordering model. Adding head as a feature leads to
sparsity, which results in a substantial decrease in
collocation (lines 1b and 1d). However, just adding
pos-head (line 1c) does not cause this problem and
gives the best F1 score. To address sparsity, we in-
duced a distributed hidden representation for each
word via a neural network, capturing the semantic
similarity between words. Preliminary experiments
improved the F1 score when using this word repre-
sentation as a feature instead of the word directly.
Lang and Lapata (2011b) give the results of three
methods on this task. In terms of F1 score, the La-
tent Logistic and Graph Partitioning methods result
in slight reduction in performance over the baseline,
while the Split-Merge method results in an improve-
ment of 0.6 points. Table 1, line 1c achieves an im-
provement of 1.1 points over the baseline.
</bodyText>
<subsectionHeader confidence="0.999279">
3.2 Further Evaluation
</subsectionHeader>
<bodyText confidence="0.998859285714286">
Table 2 shows the variation in performance w.r.t.
the number of PRs3 in the best performing config-
uration (Table 1, line 1c). On one extreme, when
there are 0 PRs, there are only two possible in-
tervals: (START, PRED) and (PRED, END) which
means that the only context information a SR has
is whether it is to the left or right of the predicate.
</bodyText>
<footnote confidence="0.979272">
2The baseline F1 reported by Lang and Lapata (2011b) is
79.5 due to a bug in their system (personal communication).
3Note that the system might not use all available PRs to label
a given frame instance. #PRs refers to the max #PRs.
</footnote>
<table confidence="0.999257333333333">
# PRs PU CO F1
0 81.67 78.07 79.83
1 82.91 78.99 80.90
2 83.54 78.47 80.93
3 83.68 78.23 80.87
4 83.72 78.08 80.80
</table>
<tableCaption confidence="0.898711">
Table 2: Performance variation with the number of
PRs (excluding START, END and PRED)
</tableCaption>
<bodyText confidence="0.999960166666667">
With only this additional ordering information, the
performance is the same as the baseline. Adding just
1 PR leads to a big increase in both purity and col-
location. Increasing the number of PRs beyond 1
leads to a gradual increase in purity and decline in
collocation, with the best F1 score at 2 PRs. This
behavior could be explained by the fact that increas-
ing the number of PRs also increases the number of
intervals, which makes the probability distributions
more sparse. In the extreme case, where all the roles
are PRs and there are no SRs, the model would just
learn the complete sequence of roles, which would
make the parameter space too large to be tractable.
For calculating purity, each induced cluster (or
role) is mapped to a particular gold role that has
the maximum instances in the cluster. Analyzing the
output of our model (line 1c in Table 1), we found
that about 98% of the PRs and 40% of the SRs got
mapped to the gold core roles (A0,A1, etc.). This
suggests that the model is indeed following the intu-
ition that (i) the ordering of core roles is important
information for SRL systems, and (ii) the intervals
bounded by core roles provide good context infor-
mation for classification of other roles.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999895">
We propose a unified generative model for unsu-
pervised semantic role induction that incorporates
global role correlations as well as local feature infor-
mation. The results indicate that a small number of
ordered primary roles (PRs) is a good representation
of global ordering constraints for SRL. This repre-
sentation keeps the parameter space small enough
for unsupervised learning.
</bodyText>
<sectionHeader confidence="0.998305" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9753305">
This work was funded by the Swiss NSF grant
200021 125137 and EC FP7 grant PARLANCE.
</bodyText>
<page confidence="0.99749">
148
</page>
<sectionHeader confidence="0.989398" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999205881355932">
J.K. Baker. 1979. Trainable grammars for speech recog-
nition. The Journal of the Acoustical Society ofAmer-
ica,65:S132.
P. Diderichsen. 1966. Elementary Danish Grammar.
Gyldendal, Copenhagen.
E. Drach. 1937. Grundstellung der Deutschen Satzlehre.
Diesterweg, Frankfurt.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245–288.
T. Grenager and C.D. Manning. 2006. Unsupervised dis-
covery of a statistical verb lexicon. In Proceedings of
the 2006 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1–8. Association for
Computational Linguistics.
D. Klein and C.D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency
and constituency. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, page 478. Association for Computational Linguis-
tics.
J. Lang and M. Lapata. 2011a. Unsupervised semantic
role induction via split-merge clustering. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics, Portland, Oregon.
J. Lang and M. Lapata. 2011b. Unsupervised seman-
tic role induction with graph partitioning. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 1320–1331, Ed-
inburgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
L. M`arquez, X. Carreras, K.C. Litkowski, and S. Steven-
son. 2008. Semantic role labeling: an introduc-
tion to the special issue. Computational linguistics,
34(2):145–159.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J.H. Mar-
tin, and D. Jurafsky. 2005. Support vector learning for
semantic argument classification. Machine Learning,
60(1):11–39.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear programming
inference. In Proceedings of the 20th international
conference on Computational Linguistics, page 1346.
Association for Computational Linguistics.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and
J. Nivre. 2008. The conll-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159–177.
Association for Computational Linguistics.
C. Thompson, R. Levy, and C. Manning. 2003. A gen-
erative model for semantic role labeling. Machine
Learning: ECML 2003, pages 397–408.
K. Toutanova, A. Haghighi, and C.D. Manning. 2008. A
global joint model for semantic role labeling. Compu-
tational Linguistics, 34(2):161–191.
</reference>
<page confidence="0.998976">
149
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526077">
<title confidence="0.999633">Unsupervised Semantic Role Induction with Global Role Ordering</title>
<author confidence="0.99951">Nikhil Garg James Henderson</author>
<affiliation confidence="0.93184">University of Geneva University of Geneva Switzerland Switzerland</affiliation>
<email confidence="0.65246">nikhil.garg@unige.chjames.henderson@unige.ch</email>
<abstract confidence="0.996570416666667">We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. The role sequence is divided on the notion of and each interval generates a sequence roles syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<journal>The Journal of the Acoustical Society ofAmerica,65:S132.</journal>
<contexts>
<context position="9971" citStr="Baker, 1979" startWordPosition="1674" endWordPosition="1675">, 2011b), which we use here for PRs. Lastly, conditioning the (STOP/CONTINUE) indicator variable on the adjacency value (adj) is inspired from the DMV model (Klein and Manning, 2004) for unsupervised dependency parsing. We found in the annotated corpus that if we map core roles to PRs, then most of the time the intervals do not generate any SRs at all. So, the probability to STOP should be very high when generating the first SR. We use an EM procedure to train the model. In the E-step, we calculate the expected counts of all the hidden variables in our model using the InsideOutside algorithm (Baker, 1979). In the M-step, we add the counts corresponding to the Bayesian priors to the expected counts and use the resulting counts to calculate the MAP estimate of the parameters. 3 Experiments Following the experimental settings of Lang and Lapata (2011b), we use the CoNLL 2008 shared task dataset (Surdeanu et al., 2008), only consider verbal predicates, and run unsupervised training on the standard training set. The evaluation measures are also the same: (i) Purity (PU) that measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) that measures how well a gold r</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J.K. Baker. 1979. Trainable grammars for speech recognition. The Journal of the Acoustical Society ofAmerica,65:S132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Diderichsen</author>
</authors>
<title>Elementary Danish Grammar.</title>
<date>1966</date>
<location>Gyldendal, Copenhagen.</location>
<contexts>
<context position="8974" citStr="Diderichsen, 1966" startWordPosition="1501" endWordPosition="1503">ordering which may or may not correspond to core roles. Secondly, generating the SRs independently of each other given the interval is based on the intuition that knowing the core roles informs us about the expected non-core roles that occur between them. This intuition is supported by the statistics in the annotated data, where we found that if we consider the core roles as PRs, then most of the intervals tend to have only a few types of SRs and a given SR tends to occur only in a few types of intervals. The concept of intervals is also related to the linguistic theory of topological fields (Diderichsen, 1966; Drach, 1937). This simplifying assumption that given the PRs at the interval boundary, the SRs in that interval are independent of the other roles in the sequence, keeps the parameter space limited, which helps unsupervised learning. Thirdly, not allowing some or all roles to repeat has been employed as a useful constraint in previous work (Punyakanok et al., 2004; Lang and Lapata, 2011b), which we use here for PRs. Lastly, conditioning the (STOP/CONTINUE) indicator variable on the adjacency value (adj) is inspired from the DMV model (Klein and Manning, 2004) for unsupervised dependency pars</context>
</contexts>
<marker>Diderichsen, 1966</marker>
<rawString>P. Diderichsen. 1966. Elementary Danish Grammar. Gyldendal, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Drach</author>
</authors>
<title>Grundstellung der Deutschen Satzlehre.</title>
<date>1937</date>
<location>Diesterweg, Frankfurt.</location>
<contexts>
<context position="8988" citStr="Drach, 1937" startWordPosition="1504" endWordPosition="1505">or may not correspond to core roles. Secondly, generating the SRs independently of each other given the interval is based on the intuition that knowing the core roles informs us about the expected non-core roles that occur between them. This intuition is supported by the statistics in the annotated data, where we found that if we consider the core roles as PRs, then most of the intervals tend to have only a few types of SRs and a given SR tends to occur only in a few types of intervals. The concept of intervals is also related to the linguistic theory of topological fields (Diderichsen, 1966; Drach, 1937). This simplifying assumption that given the PRs at the interval boundary, the SRs in that interval are independent of the other roles in the sequence, keeps the parameter space limited, which helps unsupervised learning. Thirdly, not allowing some or all roles to repeat has been employed as a useful constraint in previous work (Punyakanok et al., 2004; Lang and Lapata, 2011b), which we use here for PRs. Lastly, conditioning the (STOP/CONTINUE) indicator variable on the adjacency value (adj) is inspired from the DMV model (Klein and Manning, 2004) for unsupervised dependency parsing. We found </context>
</contexts>
<marker>Drach, 1937</marker>
<rawString>E. Drach. 1937. Grundstellung der Deutschen Satzlehre. Diesterweg, Frankfurt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1956" citStr="Gildea and Jurafsky, 2002" startWordPosition="300" endWordPosition="303">about the expected semantic role sequence. Sentence a is in active voice with sequence (A0, PREDICATE, A1) and sentence b is in passive voice with sequence (A1, PREDICATE, A0). Additional global preferences, such as arguments A0 and A1 rarely repeat in a frame (as seen in the corpus), could also be useful in addition to local features. Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space. Similar to Toutanova et al. (20</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<title>Unsupervised discovery of a statistical verb lexicon.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2336" citStr="Grenager and Manning (2006)" startWordPosition="357" endWordPosition="360">ocal classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space. Similar to Toutanova et al. (2008), we propose to use global role ordering preferences but in a generative model in contrast to their discriminative one. Further, unlike Grenager and Manning (2006), we do not explicitly generate the linking of semantic roles and syntactic relations, thus keeping the parameter space tractable. The main contribution of this work is an unsupervised model that uses global role o</context>
</contexts>
<marker>Grenager, Manning, 2006</marker>
<rawString>T. Grenager and C.D. Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>478</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9541" citStr="Klein and Manning, 2004" startWordPosition="1593" endWordPosition="1596">inguistic theory of topological fields (Diderichsen, 1966; Drach, 1937). This simplifying assumption that given the PRs at the interval boundary, the SRs in that interval are independent of the other roles in the sequence, keeps the parameter space limited, which helps unsupervised learning. Thirdly, not allowing some or all roles to repeat has been employed as a useful constraint in previous work (Punyakanok et al., 2004; Lang and Lapata, 2011b), which we use here for PRs. Lastly, conditioning the (STOP/CONTINUE) indicator variable on the adjacency value (adj) is inspired from the DMV model (Klein and Manning, 2004) for unsupervised dependency parsing. We found in the annotated corpus that if we map core roles to PRs, then most of the time the intervals do not generate any SRs at all. So, the probability to STOP should be very high when generating the first SR. We use an EM procedure to train the model. In the E-step, we calculate the expected counts of all the hidden variables in our model using the InsideOutside algorithm (Baker, 1979). In the M-step, we add the counts corresponding to the Bayesian priors to the expected counts and use the resulting counts to calculate the MAP estimate of the parameter</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C.D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lang</author>
<author>M Lapata</author>
</authors>
<title>Unsupervised semantic role induction via split-merge clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Portland, Oregon.</location>
<contexts>
<context position="815" citStr="Lang and Lapata, 2011" startWordPosition="109" endWordPosition="112">unige.ch Abstract We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering. 1 Introduction Unsupervised semantic role induction has gained significant interest recently (Lang and Lapata, 2011b) due to limited amounts of annotated corpora. A Semantic Role Labeling (SRL) system should provide consistent argument labels across different syntactic realizations of the same verb (Palmer et al., 2005), as in (a.) [ Mark ]A0 drove [ the car ]A1 (b.) [ The car ]A1 was driven by [ Mark ]A0 This simple example also shows that while certain local syntactic and semantic features could provide clues to the semantic role label of a constituent, nonlocal features such as predicate voice could provide information about the expected semantic role sequence. Sentence a is in active voice with sequenc</context>
<context position="2222" citStr="Lang and Lapata (2011" startWordPosition="341" endWordPosition="344">n the corpus), could also be useful in addition to local features. Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space. Similar to Toutanova et al. (2008), we propose to use global role ordering preferences but in a generative model in contrast to their discriminative one. Further, unlike Grenager and Manning (2006), we do not explicitly generate the linking of semantic roles and syntactic relations, thus keeping </context>
<context position="9365" citStr="Lang and Lapata, 2011" startWordPosition="1565" endWordPosition="1568"> most of the intervals tend to have only a few types of SRs and a given SR tends to occur only in a few types of intervals. The concept of intervals is also related to the linguistic theory of topological fields (Diderichsen, 1966; Drach, 1937). This simplifying assumption that given the PRs at the interval boundary, the SRs in that interval are independent of the other roles in the sequence, keeps the parameter space limited, which helps unsupervised learning. Thirdly, not allowing some or all roles to repeat has been employed as a useful constraint in previous work (Punyakanok et al., 2004; Lang and Lapata, 2011b), which we use here for PRs. Lastly, conditioning the (STOP/CONTINUE) indicator variable on the adjacency value (adj) is inspired from the DMV model (Klein and Manning, 2004) for unsupervised dependency parsing. We found in the annotated corpus that if we map core roles to PRs, then most of the time the intervals do not generate any SRs at all. So, the probability to STOP should be very high when generating the first SR. We use an EM procedure to train the model. In the E-step, we calculate the expected counts of all the hidden variables in our model using the InsideOutside algorithm (Baker,</context>
<context position="11703" citStr="Lang and Lapata (2011" startWordPosition="1969" endWordPosition="1972">set the number of PRs to 2 (excluding START, END and PRED), and SRs to 21-2=19. 1Removing the Bayesian priors completely, resulted in the EM algorithm getting to a local maxima quite early, giving a substantially lower performance. 147 Model Features PU CO F1 0 Baseline d 81.6 78.1 79.8 1a Proposed d 82.3 78.6 80.4 1b Proposed d,h 82.7 77.2 79.9 1c Proposed d,p-h 83.5 78.5 80.9 1d Proposed d,p-h,h 83.2 77.1 80.0 Table 1: Evaluation. d refers to deprel, h refers to head and p-h refers to pos-head. Table 1 gives the results using different feature combinations. Line 0 reports the performance of Lang and Lapata (2011b)’s baseline, which has been shown difficult to outperform. This baseline maps 20 most frequent deprel to a role each, and the rest are mapped to the 21st role. By just using deprel as a feature, the proposed model outperforms the baseline by 0.6 points in terms of F1 score. In this configuration, the only addition over the baseline is the ordering model. Adding head as a feature leads to sparsity, which results in a substantial decrease in collocation (lines 1b and 1d). However, just adding pos-head (line 1c) does not cause this problem and gives the best F1 score. To address sparsity, we in</context>
<context position="13322" citStr="Lang and Lapata (2011" startWordPosition="2250" endWordPosition="2253">esult in slight reduction in performance over the baseline, while the Split-Merge method results in an improvement of 0.6 points. Table 1, line 1c achieves an improvement of 1.1 points over the baseline. 3.2 Further Evaluation Table 2 shows the variation in performance w.r.t. the number of PRs3 in the best performing configuration (Table 1, line 1c). On one extreme, when there are 0 PRs, there are only two possible intervals: (START, PRED) and (PRED, END) which means that the only context information a SR has is whether it is to the left or right of the predicate. 2The baseline F1 reported by Lang and Lapata (2011b) is 79.5 due to a bug in their system (personal communication). 3Note that the system might not use all available PRs to label a given frame instance. #PRs refers to the max #PRs. # PRs PU CO F1 0 81.67 78.07 79.83 1 82.91 78.99 80.90 2 83.54 78.47 80.93 3 83.68 78.23 80.87 4 83.72 78.08 80.80 Table 2: Performance variation with the number of PRs (excluding START, END and PRED) With only this additional ordering information, the performance is the same as the baseline. Adding just 1 PR leads to a big increase in both purity and collocation. Increasing the number of PRs beyond 1 leads to a gr</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>J. Lang and M. Lapata. 2011a. Unsupervised semantic role induction via split-merge clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lang</author>
<author>M Lapata</author>
</authors>
<title>Unsupervised semantic role induction with graph partitioning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1320--1331</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="815" citStr="Lang and Lapata, 2011" startWordPosition="109" endWordPosition="112">unige.ch Abstract We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering. 1 Introduction Unsupervised semantic role induction has gained significant interest recently (Lang and Lapata, 2011b) due to limited amounts of annotated corpora. A Semantic Role Labeling (SRL) system should provide consistent argument labels across different syntactic realizations of the same verb (Palmer et al., 2005), as in (a.) [ Mark ]A0 drove [ the car ]A1 (b.) [ The car ]A1 was driven by [ Mark ]A0 This simple example also shows that while certain local syntactic and semantic features could provide clues to the semantic role label of a constituent, nonlocal features such as predicate voice could provide information about the expected semantic role sequence. Sentence a is in active voice with sequenc</context>
<context position="2222" citStr="Lang and Lapata (2011" startWordPosition="341" endWordPosition="344">n the corpus), could also be useful in addition to local features. Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space. Similar to Toutanova et al. (2008), we propose to use global role ordering preferences but in a generative model in contrast to their discriminative one. Further, unlike Grenager and Manning (2006), we do not explicitly generate the linking of semantic roles and syntactic relations, thus keeping </context>
<context position="9365" citStr="Lang and Lapata, 2011" startWordPosition="1565" endWordPosition="1568"> most of the intervals tend to have only a few types of SRs and a given SR tends to occur only in a few types of intervals. The concept of intervals is also related to the linguistic theory of topological fields (Diderichsen, 1966; Drach, 1937). This simplifying assumption that given the PRs at the interval boundary, the SRs in that interval are independent of the other roles in the sequence, keeps the parameter space limited, which helps unsupervised learning. Thirdly, not allowing some or all roles to repeat has been employed as a useful constraint in previous work (Punyakanok et al., 2004; Lang and Lapata, 2011b), which we use here for PRs. Lastly, conditioning the (STOP/CONTINUE) indicator variable on the adjacency value (adj) is inspired from the DMV model (Klein and Manning, 2004) for unsupervised dependency parsing. We found in the annotated corpus that if we map core roles to PRs, then most of the time the intervals do not generate any SRs at all. So, the probability to STOP should be very high when generating the first SR. We use an EM procedure to train the model. In the E-step, we calculate the expected counts of all the hidden variables in our model using the InsideOutside algorithm (Baker,</context>
<context position="11703" citStr="Lang and Lapata (2011" startWordPosition="1969" endWordPosition="1972">set the number of PRs to 2 (excluding START, END and PRED), and SRs to 21-2=19. 1Removing the Bayesian priors completely, resulted in the EM algorithm getting to a local maxima quite early, giving a substantially lower performance. 147 Model Features PU CO F1 0 Baseline d 81.6 78.1 79.8 1a Proposed d 82.3 78.6 80.4 1b Proposed d,h 82.7 77.2 79.9 1c Proposed d,p-h 83.5 78.5 80.9 1d Proposed d,p-h,h 83.2 77.1 80.0 Table 1: Evaluation. d refers to deprel, h refers to head and p-h refers to pos-head. Table 1 gives the results using different feature combinations. Line 0 reports the performance of Lang and Lapata (2011b)’s baseline, which has been shown difficult to outperform. This baseline maps 20 most frequent deprel to a role each, and the rest are mapped to the 21st role. By just using deprel as a feature, the proposed model outperforms the baseline by 0.6 points in terms of F1 score. In this configuration, the only addition over the baseline is the ordering model. Adding head as a feature leads to sparsity, which results in a substantial decrease in collocation (lines 1b and 1d). However, just adding pos-head (line 1c) does not cause this problem and gives the best F1 score. To address sparsity, we in</context>
<context position="13322" citStr="Lang and Lapata (2011" startWordPosition="2250" endWordPosition="2253">esult in slight reduction in performance over the baseline, while the Split-Merge method results in an improvement of 0.6 points. Table 1, line 1c achieves an improvement of 1.1 points over the baseline. 3.2 Further Evaluation Table 2 shows the variation in performance w.r.t. the number of PRs3 in the best performing configuration (Table 1, line 1c). On one extreme, when there are 0 PRs, there are only two possible intervals: (START, PRED) and (PRED, END) which means that the only context information a SR has is whether it is to the left or right of the predicate. 2The baseline F1 reported by Lang and Lapata (2011b) is 79.5 due to a bug in their system (personal communication). 3Note that the system might not use all available PRs to label a given frame instance. #PRs refers to the max #PRs. # PRs PU CO F1 0 81.67 78.07 79.83 1 82.91 78.99 80.90 2 83.54 78.47 80.93 3 83.68 78.23 80.87 4 83.72 78.08 80.80 Table 2: Performance variation with the number of PRs (excluding START, END and PRED) With only this additional ordering information, the performance is the same as the baseline. Adding just 1 PR leads to a big increase in both purity and collocation. Increasing the number of PRs beyond 1 leads to a gr</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>J. Lang and M. Lapata. 2011b. Unsupervised semantic role induction with graph partitioning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320–1331, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M`arquez</author>
<author>X Carreras</author>
<author>K C Litkowski</author>
<author>S Stevenson</author>
</authors>
<title>Semantic role labeling: an introduction to the special issue.</title>
<date>2008</date>
<journal>Computational linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>L. M`arquez, X. Carreras, K.C. Litkowski, and S. Stevenson. 2008. Semantic role labeling: an introduction to the special issue. Computational linguistics, 34(2):145–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1021" citStr="Palmer et al., 2005" startWordPosition="139" endWordPosition="142">. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering. 1 Introduction Unsupervised semantic role induction has gained significant interest recently (Lang and Lapata, 2011b) due to limited amounts of annotated corpora. A Semantic Role Labeling (SRL) system should provide consistent argument labels across different syntactic realizations of the same verb (Palmer et al., 2005), as in (a.) [ Mark ]A0 drove [ the car ]A1 (b.) [ The car ]A1 was driven by [ Mark ]A0 This simple example also shows that while certain local syntactic and semantic features could provide clues to the semantic role label of a constituent, nonlocal features such as predicate voice could provide information about the expected semantic role sequence. Sentence a is in active voice with sequence (A0, PREDICATE, A1) and sentence b is in passive voice with sequence (A1, PREDICATE, A0). Additional global preferences, such as arguments A0 and A1 rarely repeat in a frame (as seen in the corpus), could</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>K Hacioglu</author>
<author>V Krugler</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<issue>1</issue>
<contexts>
<context position="2084" citStr="Pradhan et al., 2005" startWordPosition="320" endWordPosition="323">ve voice with sequence (A1, PREDICATE, A0). Additional global preferences, such as arguments A0 and A1 rarely repeat in a frame (as seen in the corpus), could also be useful in addition to local features. Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space. Similar to Toutanova et al. (2008), we propose to use global role ordering preferences but in a generative model in contrast to their discriminative one. Furth</context>
<context position="8090" citStr="Pradhan et al. (2005)" startWordPosition="1340" endWordPosition="1343"> } |{z } riEr(I) generate indicator {z } generate features end of the interval generate SR and P(fi|ri,p) = ΠtP(fi,t|ri,p) P(f|p, vc) = ΣoΣ{rEseq(o)}P(o, r, f|p, vc) where seq(o) = {role sequences allowed under ordering o} (2) Figure 2: ri and fi denote the role and features at position i respectively, and r(I) and f(I) respectively denote the SR sequence and feature sequence in interval I. fi,t denotes the value of feature t at position i. This particular choice of model is inspired from different sources. Firstly, making the role ordering dependent only on PRs aligns with the observation by Pradhan et al. (2005) and Toutanova et al. (2008) that including the ordering information of only core roles helped improve the SRL performance as opposed to the complete role sequence. Although our assumption here is softer in that we assume the existence of some roles which define the ordering which may or may not correspond to core roles. Secondly, generating the SRs independently of each other given the interval is based on the intuition that knowing the core roles informs us about the expected non-core roles that occur between them. This intuition is supported by the statistics in the annotated data, where we</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J.H. Martin, and D. Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning, 60(1):11–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1346</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="2011" citStr="Punyakanok et al., 2004" startWordPosition="308" endWordPosition="311">in active voice with sequence (A0, PREDICATE, A1) and sentence b is in passive voice with sequence (A1, PREDICATE, A0). Additional global preferences, such as arguments A0 and A1 rarely repeat in a frame (as seen in the corpus), could also be useful in addition to local features. Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space. Similar to Toutanova et al. (2008), we propose to use global role ordering preferences</context>
<context position="9342" citStr="Punyakanok et al., 2004" startWordPosition="1561" endWordPosition="1564">e core roles as PRs, then most of the intervals tend to have only a few types of SRs and a given SR tends to occur only in a few types of intervals. The concept of intervals is also related to the linguistic theory of topological fields (Diderichsen, 1966; Drach, 1937). This simplifying assumption that given the PRs at the interval boundary, the SRs in that interval are independent of the other roles in the sequence, keeps the parameter space limited, which helps unsupervised learning. Thirdly, not allowing some or all roles to repeat has been employed as a useful constraint in previous work (Punyakanok et al., 2004; Lang and Lapata, 2011b), which we use here for PRs. Lastly, conditioning the (STOP/CONTINUE) indicator variable on the adjacency value (adj) is inspired from the DMV model (Klein and Manning, 2004) for unsupervised dependency parsing. We found in the annotated corpus that if we map core roles to PRs, then most of the time the intervals do not generate any SRs at all. So, the probability to STOP should be very high when generating the first SR. We use an EM procedure to train the model. In the E-step, we calculate the expected counts of all the hidden variables in our model using the InsideOu</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proceedings of the 20th international conference on Computational Linguistics, page 1346. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The conll-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>159--177</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. 2008. The conll-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 159–177. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Thompson</author>
<author>R Levy</author>
<author>C Manning</author>
</authors>
<title>A generative model for semantic role labeling.</title>
<date>2003</date>
<booktitle>Machine Learning: ECML</booktitle>
<pages>397--408</pages>
<contexts>
<context position="2061" citStr="Thompson et al., 2003" startWordPosition="316" endWordPosition="319"> sentence b is in passive voice with sequence (A1, PREDICATE, A0). Additional global preferences, such as arguments A0 and A1 rarely repeat in a frame (as seen in the corpus), could also be useful in addition to local features. Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space. Similar to Toutanova et al. (2008), we propose to use global role ordering preferences but in a generative model in contrast to their di</context>
</contexts>
<marker>Thompson, Levy, Manning, 2003</marker>
<rawString>C. Thompson, R. Levy, and C. Manning. 2003. A generative model for semantic role labeling. Machine Learning: ECML 2003, pages 397–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>A Haghighi</author>
<author>C D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="1878" citStr="Toutanova et al., 2008" startWordPosition="288" endWordPosition="291">tuent, nonlocal features such as predicate voice could provide information about the expected semantic role sequence. Sentence a is in active voice with sequence (A0, PREDICATE, A1) and sentence b is in passive voice with sequence (A1, PREDICATE, A0). Additional global preferences, such as arguments A0 and A1 rarely repeat in a frame (as seen in the corpus), could also be useful in addition to local features. Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-speci</context>
<context position="8118" citStr="Toutanova et al. (2008)" startWordPosition="1345" endWordPosition="1348">indicator {z } generate features end of the interval generate SR and P(fi|ri,p) = ΠtP(fi,t|ri,p) P(f|p, vc) = ΣoΣ{rEseq(o)}P(o, r, f|p, vc) where seq(o) = {role sequences allowed under ordering o} (2) Figure 2: ri and fi denote the role and features at position i respectively, and r(I) and f(I) respectively denote the SR sequence and feature sequence in interval I. fi,t denotes the value of feature t at position i. This particular choice of model is inspired from different sources. Firstly, making the role ordering dependent only on PRs aligns with the observation by Pradhan et al. (2005) and Toutanova et al. (2008) that including the ordering information of only core roles helped improve the SRL performance as opposed to the complete role sequence. Although our assumption here is softer in that we assume the existence of some roles which define the ordering which may or may not correspond to core roles. Secondly, generating the SRs independently of each other given the interval is based on the intuition that knowing the core roles informs us about the expected non-core roles that occur between them. This intuition is supported by the statistics in the annotated data, where we found that if we consider t</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>K. Toutanova, A. Haghighi, and C.D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34(2):161–191.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>