<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002575">
<title confidence="0.870712">
AUEB: Two Stage Sentiment Analysis of Social Network Messages
</title>
<author confidence="0.865822">
Rafael Michael Karampatsis, John Pavlopoulos and Prodromos Malakasiotis
</author>
<email confidence="0.823278">
mpatsis13@gmail.com, annis@aueb.gr, rulller@aueb.gr
</email>
<affiliation confidence="0.9799305">
Department of Informatics
Athens University of Economics and Business
</affiliation>
<address confidence="0.61659">
Patission 76, GR-104 34 Athens, Greece
</address>
<sectionHeader confidence="0.945784" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879">
This paper describes the system submit-
ted for the Sentiment Analysis in Twitter
Task of SEMEVAL 2014 and specifically
the Message Polarity Classification sub-
task. We used a 2–stage pipeline approach
employing a linear SVM classifier at each
stage and several features including mor-
phological features, POS tags based fea-
tures and lexicon based features.
</bodyText>
<sectionHeader confidence="0.998793" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895272727273">
Recently, Twitter has gained significant popularity
among the social network services. Lots of users
often use Twitter to express feelings or opinions
about a variety of subjects. Analysing this kind of
content can lead to useful information for fields,
such as personalized marketing or social profiling.
However such a task is not trivial, because the lan-
guage used in Twitter is often informal presenting
new challenges to text analysis.
In this paper we focus on sentiment analysis,
the field of study that analyzes people’s sentiment
and opinions from written language (Liu, 2012).
Given some text (e.g., tweet), sentiment analysis
systems return a sentiment label, which most often
is positive, negative, or neutral. This classification
can be performed directly or in two stages; in the
first stage the system examines whether the text
carries sentiment and in the second stage, the sys-
tem decides for the sentiment’s polarity (i.e., posi-
tive or negative).1 This decomposition is based on
the assumption that subjectivity detection and sen-
timent polarity detection are different problems.
</bodyText>
<footnote confidence="0.887215375">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1For instance a 2–stage approach is better suited to sys-
tems that focus on subjectivity detection; e.g., aspect based
sentiment analysis systems which extract aspect terms only
from evaluative texts.
</footnote>
<bodyText confidence="0.999981583333333">
We choose to follow the 2–stage approach, be-
cause it allows us to focus on each of the two prob-
lems separately (e.g., features, tuning, etc.). In the
following we will describe the system with which
we participated in the Message Polarity Classi-
fication subtask of Sentiment Analysis in Twit-
ter (Task 9) of SEMEVAL 2014 (Rosenthal et al.,
2014). Specifically Section 2 describes the data
provided by the organizers of the task. Sections 3
and 4 present our system and its performance re-
spectively. Finally, Section 5 concludes and pro-
vides hints for future work.
</bodyText>
<sectionHeader confidence="0.987938" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999248">
At first, we describe the data used for this year’s
task. For system tuning the organizers released the
training and development data of SEMEVAL 2013
Task 2 (Wilson et al., 2013). Both these sets are
allowed to be used for training. The organizers
also provided the test data of the same Task to be
used for development only. As argued in (Malaka-
siotis et al., 2013) these data suffer from class im-
balance. Concerning the test data, they contained
8987 messages broken down in the following 5
datasets:
</bodyText>
<listItem confidence="0.9914942">
– LJ14: 2000 sentences from LIVEJOURNAL.
– SMS13: SMS test data from last year.
– TW13: Twitter test data from last year.
– TW14: 2000 new tweets.
– TWSARC14: 100 tweets containing sarcasm.
</listItem>
<bodyText confidence="0.999953142857143">
The details of the test data were made available to
the participants only after the end of the Task. Re-
call that SMS13 and TW13 were also provided as
development data. In this way the organizers were
able to check, i) the progress of the systems since
last year’s task, and ii) the generalization capabil-
ity of the participating systems.
</bodyText>
<page confidence="0.980152">
114
</page>
<note confidence="0.8284685">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 114–118,
Dublin, Ireland, August 23-24, 2014.
</note>
<sectionHeader confidence="0.968096" genericHeader="method">
3 System Overview
</sectionHeader>
<bodyText confidence="0.9999840625">
The main objective of our system is to detect
whether a message M expresses positive, negative
or no sentiment. To achieve that we follow a 2–
stage approach. During the first stage we detect
whether M expresses sentiment (“subjective”) or
not; this process is called subjectivity detection.
In the second stage we classify the “subjective”
messages of the first stage as “positive” or “neg-
ative”. Both stages utilize a Support Vector Ma-
chine (SVM (Vapnik, 1998)) classifier with lin-
ear kernel.2 Similar approaches have also been
proposed in (Pang and Lee, 2004; Wilson et al.,
2005; Barbosa and Feng, 2010; Malakasiotis et al.,
2013). Finally, we note that the 2–stage approach,
in datasets such the one here (Malakasiotis et al.,
2013), alleviates the class imbalance problem.
</bodyText>
<subsectionHeader confidence="0.999683">
3.1 Data preprocessing
</subsectionHeader>
<bodyText confidence="0.999986857142857">
A very essential part of our system is data pre-
processing. At first, each message M is passed
through a twitter specific tokenizer and part-of-
speech (POS) tagger (Owoputi et al., 2013) to ob-
tain the tokens and the corresponding POS tags,
which are necessary for some sets of features.3
We then use a dictionary to replace any slang with
the actual text.4 We also normalize the text of
each message by combining a trie data structure
(De La Briandais, 1959) with an English dictio-
nary.5 In more detail, we replace every token of M
not in the dictionary with the most similar word of
the dictionary. Finally, we obtain POS tags of all
the new tokens.
</bodyText>
<subsectionHeader confidence="0.999849">
3.2 Sentiment lexicons
</subsectionHeader>
<bodyText confidence="0.9721325">
Another key attribute of our system is the use of
sentiment lexicons. We have used the following:
</bodyText>
<listItem confidence="0.9890085">
– HL (Hu and Liu, 2004).
– SENTIWORDNET (Baccianella et al., 2010).
– SENTIWORDNET lexicon with POS tags
(Baccianella et al., 2010).
– AFINN (Nielsen, 2011).
– MPQA (Wilson et al., 2005).
</listItem>
<footnote confidence="0.9992026">
2We used the LIBLINEAR distribution (Fan et al., 2008)
3Tokens could be words, emoticons, hashtags, etc. No
lemmatization or stemming has been applied
4Seehttp://www.noslang.com/dictionary/.
5We used the OPENOFFICE dictionary
</footnote>
<bodyText confidence="0.824168090909091">
– NRC Emotion lexicon (Mohammad and Tur-
ney, 2013).
– NRC S140 lexicon (Mohammad et al.,
2013).
– NRC Hashtag lexicon (Mohammad et al.,
2013).
– The three lexicons created from the training
data in (Malakasiotis et al., 2013).
Note that concerning the MPQA Lexicon we
applied preprocessing similar to Malakasiotis et al.
(2013) to obtain the following sub–lexicons:
</bodyText>
<listItem confidence="0.689072125">
S+ : Contains strong subjective expressions with
positive prior polarity.
S_ : Contains strong subjective expressions with
negative prior polarity.
Sf : Contains strong subjective expressions with
either positive or negative prior polarity.
S0 : Contains strong subjective expressions with
neutral prior polarity.
W+ : Contains weak subjective expressions with
positive prior polarity.
W_ : Contains weak subjective expressions with
negative prior polarity.
Wf : Contains weak subjective expressions with
either positive or negative prior polarity.
W0 : Contains weak subjective expressions with
neutral prior polarity.
</listItem>
<subsectionHeader confidence="0.999703">
3.3 Feature engineering
</subsectionHeader>
<bodyText confidence="0.999969">
Our system employs several types of features
based on morphological attributes of the mes-
sages, POS tags, and lexicons of section 3.2.6
</bodyText>
<subsectionHeader confidence="0.939685">
3.3.1 Morphological features
</subsectionHeader>
<bodyText confidence="0.9970116">
– The existence of elongated tokens (e.g.,
“baaad”).
– The number of elongated tokens.
– The existence of date references.
– The existence of time references.
</bodyText>
<footnote confidence="0.94419">
6All the features are normalized to [−1, 1]
</footnote>
<page confidence="0.99734">
115
</page>
<bodyText confidence="0.9970945">
– The number of tokens that contain only upper
case letters.
– The number of tokens that contain both upper
and lower case letters.
– The number of tokens that start with an upper
case letter.
– The number of exclamation marks.
– The number of question marks.
– The sum of exclamation and question marks.
– The number of tokens containing only excla-
mation marks.
– The number of tokens containing only ques-
tion marks.
– The number of tokens containing only excla-
mation or question marks.
– The number of tokens containing only ellip-
sis (...).
– The existence of a subjective (i.e., positive or
negative) emoticon at the message’s end.
– The existence of an ellipsis and a link at the
message’s end.
– The existence of an exclamation mark at the
message’s end.
– The existence of a question mark at the mes-
sage’s end.
– The existence of a question or an exclamation
mark at the message’s end.
– The existence of slang.
</bodyText>
<subsectionHeader confidence="0.908004">
3.3.2 POS based features
</subsectionHeader>
<bodyText confidence="0.991367">
– The number of adjectives.
– The number of adverbs.
– The number of interjections.
– The number of verbs.
– The number of nouns.
– The number of proper nouns.
– The number of urls.
– The number of subjective emoticons.7
– The number of positive emoticons.8
– The number of negative emoticons.9
– The average, maximum and minimum F1
scores of the message’s POS bigrams for the
subjective and the neutral classes.10
– The average, maximum and minimum F1
scores of the message’s POS bigrams for the
positive and the negative classes.11
For a bigram b and a class c, F1 is calculated as:
</bodyText>
<equation confidence="0.99104">
2 · Pre(b, c) · Rec(b, c)
F1(b, c) = (1) Pre(b, c) + Rec(b, c)
where:
Pre(b, c) = #messages of c containing b (2) #messages containing b
Rec(b, c) = #messages of c containing b (3) #messages of c
</equation>
<subsectionHeader confidence="0.8874">
3.3.3 Sentiment lexicon based features
</subsectionHeader>
<bodyText confidence="0.999433666666667">
For each lexicon we use seven different features
based on the scores provided by the lexicon for
each word present in the message.12
</bodyText>
<listItem confidence="0.983738">
– Sum of scores.
– Maximum of scores.
– Minimum of scores.
– Average of scores.
</listItem>
<bodyText confidence="0.9073795">
– The count of words with scores.
– The score of the last word of the message that
appears in the lexicon.
– The score of the last word of the message.
</bodyText>
<footnote confidence="0.779254833333333">
7This feature is used only for subjectivity detection.
8This feature is used only for polarity detection.
9This feature is used only for polarity detection.
10This feature is used only for subjectivity detection.
11This feature is used only for polarity detection.
12If a word does not appear in the lexicon it is assigned
with a score of 0 and it is not considered in the calculation of
the average, maximum, minimum and count scores. Also, we
have removed from SENTIWORDNET any instances having
positive and negative scores that sum to zero. Moreover, the
MPQA lexicon does not provide scores, so, for each word in
the lexicon we assume a score equal to 1.
</footnote>
<page confidence="0.998476">
116
</page>
<bodyText confidence="0.99999">
We also created features based on the Pre and
F1 scores of MPQA and the train data generated
lexicons in a similar manner to that described in
(Malakasiotis et al., 2013), with the difference that
the features are stage dependent. Thus, for subjec-
tivity detection we use the subjective and neutral
classes and for polarity detection we use the posi-
tive and negative classes to compute the scores.
</bodyText>
<subsectionHeader confidence="0.758864">
3.3.4 Miscellaneous features
</subsectionHeader>
<bodyText confidence="0.995058263157895">
Negation. Negation not only is a good subjec-
tivity indicator but it also may change the
polarity of a message. We therefore add 7
more features, one indicating the existence
of negation, and the remaining six indicat-
ing the existence of negation that precedes
words from lexicons Sf, S+, S−, Wf, W+
and W−.13 Each feature is used in the appro-
priate stage.14 We have not implement this
type of feature for other lexicons but it might
be a good addition to the system.
Carnegie Mellon University’s Twitter clusters.
Owoputi et al. (2013) released a dataset of
938 clusters containing words coming from
tweets. Words of the same clusters share
similar attributes. We try to exploit this
observation by adding 938 features, each of
which indicates if a message’s token appears
or not in the corresponding attributes.
</bodyText>
<subsectionHeader confidence="0.990248">
3.4 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999086571428571">
To allow our model to better scale on unseen data
we have performed feature selection. More specif-
ically, we first merged training and development
data of SEMEVAL 2013 Task 2. Then, we ranked
the features with respect to their information gain
(Quinlan, 1986) on this dataset. To obtain the best
set of features we started with a set containing the
top 50 features and we kept adding batches of 50
features until we have added all of them. At each
step we evaluated the corresponding feature set on
the TW13 and SMS13 datasets and chose the fea-
ture set with the best performance. This resulted in
a system which used the top 900 features for Stage
1 and the top 1150 features for Stage 2.
</bodyText>
<footnote confidence="0.99785475">
13We use a list of words with negation. We assume that a
token precedes a word if it is in a distance of at most 5 tokens.
14The features concerning S± and W± are used in subjec-
tivity detection and the remaining four in polarity detection.
</footnote>
<table confidence="0.999557875">
Test Set AUEB Median Best
LJ14 70.75 65.48 74.84
SMS13 64.32 57.53 70.28
TW13 63.92 62.88 72.12
TW14 66.38 63.03 70.96
TWSARC14 56.16 45.77 58.16
AVGall 64.31 56.56 68.78
AVG14 64.43 57.97 67.62
</table>
<tableCaption confidence="0.986526">
Table 1: F1(±) scores per dataset.
</tableCaption>
<table confidence="0.998815125">
Test Set Ranking
LJ14 9/50
SMS13 8/50
TW13 21/50
TW14 14/50
TWSARC14 4/50
AVGall 6/50
AVG14 5/50
</table>
<tableCaption confidence="0.99834">
Table 2: Rankings of our system.
</tableCaption>
<sectionHeader confidence="0.990284" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999931875">
The official measure of the Task is the average F1
score of the positive and negative classes (F1(±)).
Table 1 illustrates the F1(±) score per evaluation
dataset achieved by our system along with the me-
dian and best F1(±). In the same table AVGall
corresponds to the average F1(±) across the five
datasets while AVG14 corresponds to the average
F1(±) across LJ14, TW14 and TWSARC14. We
observe that in all cases our results are above the
median. Table 2 illustrates the ranking of our sys-
tem according to F1(±). Our system ranked 6th
according to AVGall and 5th according to AVG14
among the 50 participating systems. Note that our
best results were achieved on the new test sets
(LJ14, TW14, TWSARC14) meaning that our sys-
tem has a good generalization ability.
</bodyText>
<sectionHeader confidence="0.908551" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.997961888888889">
In this paper we presented our approach for the
Message Polarity Classification subtask of the
Sentiment Analysis in Twitter Task of SEMEVAL
2014. We proposed a 2–stage pipeline approach,
which first detects sentiment and then decides
about its polarity. The results indicate that our sys-
tem handles well the class imbalance problem and
has a good generalization ability. A possible ex-
planation is that we do not use bag-of-words fea-
</bodyText>
<page confidence="0.992767">
117
</page>
<bodyText confidence="0.9999704">
tures which often suffer from over–fitting. Never-
theless, there is still some room for improvement.
A promising direction would be to improve the
1st stage (subjectivity detection) either by adding
more data or by adding more features, mostly be-
cause the performance of stage 1 greatly affects
that of stage 2. Finally, the addition of more data
for the negative class on stage 2 might be a good
improvement because it would further reduce the
class imbalance of the training data for this stage.
</bodyText>
<sectionHeader confidence="0.998457" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979543678161">
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opin-
ion mining. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC’10), Valletta, Malta,
may.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ’10, pages 36–44, Beijing, China.
Rene De La Briandais. 1959. File searching using
variable length keys. In Papers Presented at the the
March 3-5, 1959, Western Joint Computer Confer-
ence, IRE-AIEE-ACM ’59 (Western), pages 295–
298, New York, NY, USA.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.
Prodromos Malakasiotis, Rafael Michael Karampat-
sis, Konstantina Makrynioti, and John Pavlopoulos.
2013. nlp.cs.aueb.gr: Two stage sentiment analysis.
In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM), Volume 2: Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 562–567,
Atlanta, Georgia, June.
Saif Mohammad and Peter Turney. 2013. Crowdsourc-
ing a word-emotion association lexicon. 29(3):436–
465.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), Atlanta, Georgia, USA,
June.
Finn ˚Arup Nielsen. 2011. A new anew: evaluation of
a word list for sentiment analysis in microblogs. In
Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie,
and Mariann Hardey, editors, Proceedings of the
ESWC2011 Workshop on ’Making Sense of Micro-
posts’: Big things come in small packages, volume
718 of CEUR Workshop Proceedings, pages 93–98,
May.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ’04, Barcelona, Spain.
Ross Quinlan. 1986. Induction of decision trees.
Mach. Learn., 1(1):81–106, March.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’14, Dublin, Ireland.
Vladimir Vapnik. 1998. Statistical learning theory.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 347–354.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. SemEval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ’13, June.
</reference>
<page confidence="0.996179">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.532480">
<title confidence="0.999717">AUEB: Two Stage Sentiment Analysis of Social Network Messages</title>
<author confidence="0.995743">Michael Karampatsis</author>
<author confidence="0.995743">John Pavlopoulos Malakasiotis</author>
<email confidence="0.990056">mpatsis13@gmail.com,annis@aueb.gr,rulller@aueb.gr</email>
<affiliation confidence="0.9893925">Department of Informatics Athens University of Economics and</affiliation>
<note confidence="0.585864">Patission 76, GR-104 34 Athens, Greece</note>
<abstract confidence="0.994299">This paper describes the system submitted for the Sentiment Analysis in Twitter of and specifically the Message Polarity Classification subtask. We used a 2–stage pipeline approach employing a linear SVM classifier at each stage and several features including morphological features, POS tags based features and lexicon based features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="5579" citStr="Baccianella et al., 2010" startWordPosition="895" endWordPosition="898">he corresponding POS tags, which are necessary for some sets of features.3 We then use a dictionary to replace any slang with the actual text.4 We also normalize the text of each message by combining a trie data structure (De La Briandais, 1959) with an English dictionary.5 In more detail, we replace every token of M not in the dictionary with the most similar word of the dictionary. Finally, we obtain POS tags of all the new tokens. 3.2 Sentiment lexicons Another key attribute of our system is the use of sentiment lexicons. We have used the following: – HL (Hu and Liu, 2004). – SENTIWORDNET (Baccianella et al., 2010). – SENTIWORDNET lexicon with POS tags (Baccianella et al., 2010). – AFINN (Nielsen, 2011). – MPQA (Wilson et al., 2005). 2We used the LIBLINEAR distribution (Fan et al., 2008) 3Tokens could be words, emoticons, hashtags, etc. No lemmatization or stemming has been applied 4Seehttp://www.noslang.com/dictionary/. 5We used the OPENOFFICE dictionary – NRC Emotion lexicon (Mohammad and Turney, 2013). – NRC S140 lexicon (Mohammad et al., 2013). – NRC Hashtag lexicon (Mohammad et al., 2013). – The three lexicons created from the training data in (Malakasiotis et al., 2013). Note that concerning the M</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>36--44</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4547" citStr="Barbosa and Feng, 2010" startWordPosition="717" endWordPosition="720">System Overview The main objective of our system is to detect whether a message M expresses positive, negative or no sentiment. To achieve that we follow a 2– stage approach. During the first stage we detect whether M expresses sentiment (“subjective”) or not; this process is called subjectivity detection. In the second stage we classify the “subjective” messages of the first stage as “positive” or “negative”. Both stages utilize a Support Vector Machine (SVM (Vapnik, 1998)) classifier with linear kernel.2 Similar approaches have also been proposed in (Pang and Lee, 2004; Wilson et al., 2005; Barbosa and Feng, 2010; Malakasiotis et al., 2013). Finally, we note that the 2–stage approach, in datasets such the one here (Malakasiotis et al., 2013), alleviates the class imbalance problem. 3.1 Data preprocessing A very essential part of our system is data preprocessing. At first, each message M is passed through a twitter specific tokenizer and part-ofspeech (POS) tagger (Owoputi et al., 2013) to obtain the tokens and the corresponding POS tags, which are necessary for some sets of features.3 We then use a dictionary to replace any slang with the actual text.4 We also normalize the text of each message by com</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 36–44, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rene De La Briandais</author>
</authors>
<title>File searching using variable length keys.</title>
<date>1959</date>
<booktitle>In Papers Presented at the the March 3-5, 1959, Western Joint Computer Conference, IRE-AIEE-ACM ’59 (Western),</booktitle>
<pages>295--298</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="5199" citStr="Briandais, 1959" startWordPosition="829" endWordPosition="830">y, we note that the 2–stage approach, in datasets such the one here (Malakasiotis et al., 2013), alleviates the class imbalance problem. 3.1 Data preprocessing A very essential part of our system is data preprocessing. At first, each message M is passed through a twitter specific tokenizer and part-ofspeech (POS) tagger (Owoputi et al., 2013) to obtain the tokens and the corresponding POS tags, which are necessary for some sets of features.3 We then use a dictionary to replace any slang with the actual text.4 We also normalize the text of each message by combining a trie data structure (De La Briandais, 1959) with an English dictionary.5 In more detail, we replace every token of M not in the dictionary with the most similar word of the dictionary. Finally, we obtain POS tags of all the new tokens. 3.2 Sentiment lexicons Another key attribute of our system is the use of sentiment lexicons. We have used the following: – HL (Hu and Liu, 2004). – SENTIWORDNET (Baccianella et al., 2010). – SENTIWORDNET lexicon with POS tags (Baccianella et al., 2010). – AFINN (Nielsen, 2011). – MPQA (Wilson et al., 2005). 2We used the LIBLINEAR distribution (Fan et al., 2008) 3Tokens could be words, emoticons, hashtags</context>
</contexts>
<marker>Briandais, 1959</marker>
<rawString>Rene De La Briandais. 1959. File searching using variable length keys. In Papers Presented at the the March 3-5, 1959, Western Joint Computer Conference, IRE-AIEE-ACM ’59 (Western), pages 295– 298, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="5755" citStr="Fan et al., 2008" startWordPosition="924" endWordPosition="927">ge by combining a trie data structure (De La Briandais, 1959) with an English dictionary.5 In more detail, we replace every token of M not in the dictionary with the most similar word of the dictionary. Finally, we obtain POS tags of all the new tokens. 3.2 Sentiment lexicons Another key attribute of our system is the use of sentiment lexicons. We have used the following: – HL (Hu and Liu, 2004). – SENTIWORDNET (Baccianella et al., 2010). – SENTIWORDNET lexicon with POS tags (Baccianella et al., 2010). – AFINN (Nielsen, 2011). – MPQA (Wilson et al., 2005). 2We used the LIBLINEAR distribution (Fan et al., 2008) 3Tokens could be words, emoticons, hashtags, etc. No lemmatization or stemming has been applied 4Seehttp://www.noslang.com/dictionary/. 5We used the OPENOFFICE dictionary – NRC Emotion lexicon (Mohammad and Turney, 2013). – NRC S140 lexicon (Mohammad et al., 2013). – NRC Hashtag lexicon (Mohammad et al., 2013). – The three lexicons created from the training data in (Malakasiotis et al., 2013). Note that concerning the MPQA Lexicon we applied preprocessing similar to Malakasiotis et al. (2013) to obtain the following sub–lexicons: S+ : Contains strong subjective expressions with positive prior</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="5536" citStr="Hu and Liu, 2004" startWordPosition="889" endWordPosition="892">., 2013) to obtain the tokens and the corresponding POS tags, which are necessary for some sets of features.3 We then use a dictionary to replace any slang with the actual text.4 We also normalize the text of each message by combining a trie data structure (De La Briandais, 1959) with an English dictionary.5 In more detail, we replace every token of M not in the dictionary with the most similar word of the dictionary. Finally, we obtain POS tags of all the new tokens. 3.2 Sentiment lexicons Another key attribute of our system is the use of sentiment lexicons. We have used the following: – HL (Hu and Liu, 2004). – SENTIWORDNET (Baccianella et al., 2010). – SENTIWORDNET lexicon with POS tags (Baccianella et al., 2010). – AFINN (Nielsen, 2011). – MPQA (Wilson et al., 2005). 2We used the LIBLINEAR distribution (Fan et al., 2008) 3Tokens could be words, emoticons, hashtags, etc. No lemmatization or stemming has been applied 4Seehttp://www.noslang.com/dictionary/. 5We used the OPENOFFICE dictionary – NRC Emotion lexicon (Mohammad and Turney, 2013). – NRC S140 lexicon (Mohammad et al., 2013). – NRC Hashtag lexicon (Mohammad et al., 2013). – The three lexicons created from the training data in (Malakasioti</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1258" citStr="Liu, 2012" startWordPosition="183" endWordPosition="184">features. 1 Introduction Recently, Twitter has gained significant popularity among the social network services. Lots of users often use Twitter to express feelings or opinions about a variety of subjects. Analysing this kind of content can lead to useful information for fields, such as personalized marketing or social profiling. However such a task is not trivial, because the language used in Twitter is often informal presenting new challenges to text analysis. In this paper we focus on sentiment analysis, the field of study that analyzes people’s sentiment and opinions from written language (Liu, 2012). Given some text (e.g., tweet), sentiment analysis systems return a sentiment label, which most often is positive, negative, or neutral. This classification can be performed directly or in two stages; in the first stage the system examines whether the text carries sentiment and in the second stage, the system decides for the sentiment’s polarity (i.e., positive or negative).1 This decomposition is based on the assumption that subjectivity detection and sentiment polarity detection are different problems. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Pag</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prodromos Malakasiotis</author>
<author>Rafael Michael Karampatsis</author>
<author>Konstantina Makrynioti</author>
<author>John Pavlopoulos</author>
</authors>
<title>nlp.cs.aueb.gr: Two stage sentiment analysis.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>562--567</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="3117" citStr="Malakasiotis et al., 2013" startWordPosition="479" endWordPosition="483">EVAL 2014 (Rosenthal et al., 2014). Specifically Section 2 describes the data provided by the organizers of the task. Sections 3 and 4 present our system and its performance respectively. Finally, Section 5 concludes and provides hints for future work. 2 Data At first, we describe the data used for this year’s task. For system tuning the organizers released the training and development data of SEMEVAL 2013 Task 2 (Wilson et al., 2013). Both these sets are allowed to be used for training. The organizers also provided the test data of the same Task to be used for development only. As argued in (Malakasiotis et al., 2013) these data suffer from class imbalance. Concerning the test data, they contained 8987 messages broken down in the following 5 datasets: – LJ14: 2000 sentences from LIVEJOURNAL. – SMS13: SMS test data from last year. – TW13: Twitter test data from last year. – TW14: 2000 new tweets. – TWSARC14: 100 tweets containing sarcasm. The details of the test data were made available to the participants only after the end of the Task. Recall that SMS13 and TW13 were also provided as development data. In this way the organizers were able to check, i) the progress of the systems since last year’s task, and</context>
<context position="4575" citStr="Malakasiotis et al., 2013" startWordPosition="721" endWordPosition="724"> objective of our system is to detect whether a message M expresses positive, negative or no sentiment. To achieve that we follow a 2– stage approach. During the first stage we detect whether M expresses sentiment (“subjective”) or not; this process is called subjectivity detection. In the second stage we classify the “subjective” messages of the first stage as “positive” or “negative”. Both stages utilize a Support Vector Machine (SVM (Vapnik, 1998)) classifier with linear kernel.2 Similar approaches have also been proposed in (Pang and Lee, 2004; Wilson et al., 2005; Barbosa and Feng, 2010; Malakasiotis et al., 2013). Finally, we note that the 2–stage approach, in datasets such the one here (Malakasiotis et al., 2013), alleviates the class imbalance problem. 3.1 Data preprocessing A very essential part of our system is data preprocessing. At first, each message M is passed through a twitter specific tokenizer and part-ofspeech (POS) tagger (Owoputi et al., 2013) to obtain the tokens and the corresponding POS tags, which are necessary for some sets of features.3 We then use a dictionary to replace any slang with the actual text.4 We also normalize the text of each message by combining a trie data structure</context>
<context position="6151" citStr="Malakasiotis et al., 2013" startWordPosition="983" endWordPosition="986"> Liu, 2004). – SENTIWORDNET (Baccianella et al., 2010). – SENTIWORDNET lexicon with POS tags (Baccianella et al., 2010). – AFINN (Nielsen, 2011). – MPQA (Wilson et al., 2005). 2We used the LIBLINEAR distribution (Fan et al., 2008) 3Tokens could be words, emoticons, hashtags, etc. No lemmatization or stemming has been applied 4Seehttp://www.noslang.com/dictionary/. 5We used the OPENOFFICE dictionary – NRC Emotion lexicon (Mohammad and Turney, 2013). – NRC S140 lexicon (Mohammad et al., 2013). – NRC Hashtag lexicon (Mohammad et al., 2013). – The three lexicons created from the training data in (Malakasiotis et al., 2013). Note that concerning the MPQA Lexicon we applied preprocessing similar to Malakasiotis et al. (2013) to obtain the following sub–lexicons: S+ : Contains strong subjective expressions with positive prior polarity. S_ : Contains strong subjective expressions with negative prior polarity. Sf : Contains strong subjective expressions with either positive or negative prior polarity. S0 : Contains strong subjective expressions with neutral prior polarity. W+ : Contains weak subjective expressions with positive prior polarity. W_ : Contains weak subjective expressions with negative prior polarity. W</context>
<context position="10269" citStr="Malakasiotis et al., 2013" startWordPosition="1691" endWordPosition="1694">tion. 11This feature is used only for polarity detection. 12If a word does not appear in the lexicon it is assigned with a score of 0 and it is not considered in the calculation of the average, maximum, minimum and count scores. Also, we have removed from SENTIWORDNET any instances having positive and negative scores that sum to zero. Moreover, the MPQA lexicon does not provide scores, so, for each word in the lexicon we assume a score equal to 1. 116 We also created features based on the Pre and F1 scores of MPQA and the train data generated lexicons in a similar manner to that described in (Malakasiotis et al., 2013), with the difference that the features are stage dependent. Thus, for subjectivity detection we use the subjective and neutral classes and for polarity detection we use the positive and negative classes to compute the scores. 3.3.4 Miscellaneous features Negation. Negation not only is a good subjectivity indicator but it also may change the polarity of a message. We therefore add 7 more features, one indicating the existence of negation, and the remaining six indicating the existence of negation that precedes words from lexicons Sf, S+, S−, Wf, W+ and W−.13 Each feature is used in the appropr</context>
</contexts>
<marker>Malakasiotis, Karampatsis, Makrynioti, Pavlopoulos, 2013</marker>
<rawString>Prodromos Malakasiotis, Rafael Michael Karampatsis, Konstantina Makrynioti, and John Pavlopoulos. 2013. nlp.cs.aueb.gr: Two stage sentiment analysis. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 562–567, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Peter Turney</author>
</authors>
<title>Crowdsourcing a word-emotion association lexicon.</title>
<date>2013</date>
<pages>29--3</pages>
<contexts>
<context position="5976" citStr="Mohammad and Turney, 2013" startWordPosition="952" endWordPosition="956">we obtain POS tags of all the new tokens. 3.2 Sentiment lexicons Another key attribute of our system is the use of sentiment lexicons. We have used the following: – HL (Hu and Liu, 2004). – SENTIWORDNET (Baccianella et al., 2010). – SENTIWORDNET lexicon with POS tags (Baccianella et al., 2010). – AFINN (Nielsen, 2011). – MPQA (Wilson et al., 2005). 2We used the LIBLINEAR distribution (Fan et al., 2008) 3Tokens could be words, emoticons, hashtags, etc. No lemmatization or stemming has been applied 4Seehttp://www.noslang.com/dictionary/. 5We used the OPENOFFICE dictionary – NRC Emotion lexicon (Mohammad and Turney, 2013). – NRC S140 lexicon (Mohammad et al., 2013). – NRC Hashtag lexicon (Mohammad et al., 2013). – The three lexicons created from the training data in (Malakasiotis et al., 2013). Note that concerning the MPQA Lexicon we applied preprocessing similar to Malakasiotis et al. (2013) to obtain the following sub–lexicons: S+ : Contains strong subjective expressions with positive prior polarity. S_ : Contains strong subjective expressions with negative prior polarity. Sf : Contains strong subjective expressions with either positive or negative prior polarity. S0 : Contains strong subjective expressions</context>
</contexts>
<marker>Mohammad, Turney, 2013</marker>
<rawString>Saif Mohammad and Peter Turney. 2013. Crowdsourcing a word-emotion association lexicon. 29(3):436– 465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013),</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="6020" citStr="Mohammad et al., 2013" startWordPosition="961" endWordPosition="964">ntiment lexicons Another key attribute of our system is the use of sentiment lexicons. We have used the following: – HL (Hu and Liu, 2004). – SENTIWORDNET (Baccianella et al., 2010). – SENTIWORDNET lexicon with POS tags (Baccianella et al., 2010). – AFINN (Nielsen, 2011). – MPQA (Wilson et al., 2005). 2We used the LIBLINEAR distribution (Fan et al., 2008) 3Tokens could be words, emoticons, hashtags, etc. No lemmatization or stemming has been applied 4Seehttp://www.noslang.com/dictionary/. 5We used the OPENOFFICE dictionary – NRC Emotion lexicon (Mohammad and Turney, 2013). – NRC S140 lexicon (Mohammad et al., 2013). – NRC Hashtag lexicon (Mohammad et al., 2013). – The three lexicons created from the training data in (Malakasiotis et al., 2013). Note that concerning the MPQA Lexicon we applied preprocessing similar to Malakasiotis et al. (2013) to obtain the following sub–lexicons: S+ : Contains strong subjective expressions with positive prior polarity. S_ : Contains strong subjective expressions with negative prior polarity. Sf : Contains strong subjective expressions with either positive or negative prior polarity. S0 : Contains strong subjective expressions with neutral prior polarity. W+ : Contains </context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn ˚Arup Nielsen</author>
</authors>
<title>A new anew: evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>Proceedings of the ESWC2011 Workshop on ’Making Sense of Microposts’: Big things come in small packages, volume 718 of CEUR Workshop Proceedings,</booktitle>
<pages>93--98</pages>
<editor>In Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, and Mariann Hardey, editors,</editor>
<contexts>
<context position="5669" citStr="Nielsen, 2011" startWordPosition="911" endWordPosition="912">o replace any slang with the actual text.4 We also normalize the text of each message by combining a trie data structure (De La Briandais, 1959) with an English dictionary.5 In more detail, we replace every token of M not in the dictionary with the most similar word of the dictionary. Finally, we obtain POS tags of all the new tokens. 3.2 Sentiment lexicons Another key attribute of our system is the use of sentiment lexicons. We have used the following: – HL (Hu and Liu, 2004). – SENTIWORDNET (Baccianella et al., 2010). – SENTIWORDNET lexicon with POS tags (Baccianella et al., 2010). – AFINN (Nielsen, 2011). – MPQA (Wilson et al., 2005). 2We used the LIBLINEAR distribution (Fan et al., 2008) 3Tokens could be words, emoticons, hashtags, etc. No lemmatization or stemming has been applied 4Seehttp://www.noslang.com/dictionary/. 5We used the OPENOFFICE dictionary – NRC Emotion lexicon (Mohammad and Turney, 2013). – NRC S140 lexicon (Mohammad et al., 2013). – NRC Hashtag lexicon (Mohammad et al., 2013). – The three lexicons created from the training data in (Malakasiotis et al., 2013). Note that concerning the MPQA Lexicon we applied preprocessing similar to Malakasiotis et al. (2013) to obtain the f</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn ˚Arup Nielsen. 2011. A new anew: evaluation of a word list for sentiment analysis in microblogs. In Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, and Mariann Hardey, editors, Proceedings of the ESWC2011 Workshop on ’Making Sense of Microposts’: Big things come in small packages, volume 718 of CEUR Workshop Proceedings, pages 93–98, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="4927" citStr="Owoputi et al., 2013" startWordPosition="778" endWordPosition="781">age as “positive” or “negative”. Both stages utilize a Support Vector Machine (SVM (Vapnik, 1998)) classifier with linear kernel.2 Similar approaches have also been proposed in (Pang and Lee, 2004; Wilson et al., 2005; Barbosa and Feng, 2010; Malakasiotis et al., 2013). Finally, we note that the 2–stage approach, in datasets such the one here (Malakasiotis et al., 2013), alleviates the class imbalance problem. 3.1 Data preprocessing A very essential part of our system is data preprocessing. At first, each message M is passed through a twitter specific tokenizer and part-ofspeech (POS) tagger (Owoputi et al., 2013) to obtain the tokens and the corresponding POS tags, which are necessary for some sets of features.3 We then use a dictionary to replace any slang with the actual text.4 We also normalize the text of each message by combining a trie data structure (De La Briandais, 1959) with an English dictionary.5 In more detail, we replace every token of M not in the dictionary with the most similar word of the dictionary. Finally, we obtain POS tags of all the new tokens. 3.2 Sentiment lexicons Another key attribute of our system is the use of sentiment lexicons. We have used the following: – HL (Hu and L</context>
<context position="11060" citStr="Owoputi et al. (2013)" startWordPosition="1824" endWordPosition="1827">the positive and negative classes to compute the scores. 3.3.4 Miscellaneous features Negation. Negation not only is a good subjectivity indicator but it also may change the polarity of a message. We therefore add 7 more features, one indicating the existence of negation, and the remaining six indicating the existence of negation that precedes words from lexicons Sf, S+, S−, Wf, W+ and W−.13 Each feature is used in the appropriate stage.14 We have not implement this type of feature for other lexicons but it might be a good addition to the system. Carnegie Mellon University’s Twitter clusters. Owoputi et al. (2013) released a dataset of 938 clusters containing words coming from tweets. Words of the same clusters share similar attributes. We try to exploit this observation by adding 938 features, each of which indicates if a message’s token appears or not in the corresponding attributes. 3.4 Feature Selection To allow our model to better scale on unseen data we have performed feature selection. More specifically, we first merged training and development data of SEMEVAL 2013 Task 2. Then, we ranked the features with respect to their information gain (Quinlan, 1986) on this dataset. To obtain the best set </context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="4502" citStr="Pang and Lee, 2004" startWordPosition="709" endWordPosition="712">, Dublin, Ireland, August 23-24, 2014. 3 System Overview The main objective of our system is to detect whether a message M expresses positive, negative or no sentiment. To achieve that we follow a 2– stage approach. During the first stage we detect whether M expresses sentiment (“subjective”) or not; this process is called subjectivity detection. In the second stage we classify the “subjective” messages of the first stage as “positive” or “negative”. Both stages utilize a Support Vector Machine (SVM (Vapnik, 1998)) classifier with linear kernel.2 Similar approaches have also been proposed in (Pang and Lee, 2004; Wilson et al., 2005; Barbosa and Feng, 2010; Malakasiotis et al., 2013). Finally, we note that the 2–stage approach, in datasets such the one here (Malakasiotis et al., 2013), alleviates the class imbalance problem. 3.1 Data preprocessing A very essential part of our system is data preprocessing. At first, each message M is passed through a twitter specific tokenizer and part-ofspeech (POS) tagger (Owoputi et al., 2013) to obtain the tokens and the corresponding POS tags, which are necessary for some sets of features.3 We then use a dictionary to replace any slang with the actual text.4 We a</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<journal>Mach. Learn.,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="11619" citStr="Quinlan, 1986" startWordPosition="1916" endWordPosition="1917">on University’s Twitter clusters. Owoputi et al. (2013) released a dataset of 938 clusters containing words coming from tweets. Words of the same clusters share similar attributes. We try to exploit this observation by adding 938 features, each of which indicates if a message’s token appears or not in the corresponding attributes. 3.4 Feature Selection To allow our model to better scale on unseen data we have performed feature selection. More specifically, we first merged training and development data of SEMEVAL 2013 Task 2. Then, we ranked the features with respect to their information gain (Quinlan, 1986) on this dataset. To obtain the best set of features we started with a set containing the top 50 features and we kept adding batches of 50 features until we have added all of them. At each step we evaluated the corresponding feature set on the TW13 and SMS13 datasets and chose the feature set with the best performance. This resulted in a system which used the top 900 features for Stage 1 and the top 1150 features for Stage 2. 13We use a list of words with negation. We assume that a token precedes a word if it is in a distance of at most 5 tokens. 14The features concerning S± and W± are used in</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>Ross Quinlan. 1986. Induction of decision trees. Mach. Learn., 1(1):81–106, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval ’14,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2525" citStr="Rosenthal et al., 2014" startWordPosition="376" endWordPosition="379">y the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1For instance a 2–stage approach is better suited to systems that focus on subjectivity detection; e.g., aspect based sentiment analysis systems which extract aspect terms only from evaluative texts. We choose to follow the 2–stage approach, because it allows us to focus on each of the two problems separately (e.g., features, tuning, etc.). In the following we will describe the system with which we participated in the Message Polarity Classification subtask of Sentiment Analysis in Twitter (Task 9) of SEMEVAL 2014 (Rosenthal et al., 2014). Specifically Section 2 describes the data provided by the organizers of the task. Sections 3 and 4 present our system and its performance respectively. Finally, Section 5 concludes and provides hints for future work. 2 Data At first, we describe the data used for this year’s task. For system tuning the organizers released the training and development data of SEMEVAL 2013 Task 2 (Wilson et al., 2013). Both these sets are allowed to be used for training. The organizers also provided the test data of the same Task to be used for development only. As argued in (Malakasiotis et al., 2013) these d</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval ’14, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<date>1998</date>
<note>Statistical learning theory.</note>
<contexts>
<context position="4403" citStr="Vapnik, 1998" startWordPosition="695" endWordPosition="696">eedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 114–118, Dublin, Ireland, August 23-24, 2014. 3 System Overview The main objective of our system is to detect whether a message M expresses positive, negative or no sentiment. To achieve that we follow a 2– stage approach. During the first stage we detect whether M expresses sentiment (“subjective”) or not; this process is called subjectivity detection. In the second stage we classify the “subjective” messages of the first stage as “positive” or “negative”. Both stages utilize a Support Vector Machine (SVM (Vapnik, 1998)) classifier with linear kernel.2 Similar approaches have also been proposed in (Pang and Lee, 2004; Wilson et al., 2005; Barbosa and Feng, 2010; Malakasiotis et al., 2013). Finally, we note that the 2–stage approach, in datasets such the one here (Malakasiotis et al., 2013), alleviates the class imbalance problem. 3.1 Data preprocessing A very essential part of our system is data preprocessing. At first, each message M is passed through a twitter specific tokenizer and part-ofspeech (POS) tagger (Owoputi et al., 2013) to obtain the tokens and the corresponding POS tags, which are necessary fo</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik. 1998. Statistical learning theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="4523" citStr="Wilson et al., 2005" startWordPosition="713" endWordPosition="716">ugust 23-24, 2014. 3 System Overview The main objective of our system is to detect whether a message M expresses positive, negative or no sentiment. To achieve that we follow a 2– stage approach. During the first stage we detect whether M expresses sentiment (“subjective”) or not; this process is called subjectivity detection. In the second stage we classify the “subjective” messages of the first stage as “positive” or “negative”. Both stages utilize a Support Vector Machine (SVM (Vapnik, 1998)) classifier with linear kernel.2 Similar approaches have also been proposed in (Pang and Lee, 2004; Wilson et al., 2005; Barbosa and Feng, 2010; Malakasiotis et al., 2013). Finally, we note that the 2–stage approach, in datasets such the one here (Malakasiotis et al., 2013), alleviates the class imbalance problem. 3.1 Data preprocessing A very essential part of our system is data preprocessing. At first, each message M is passed through a twitter specific tokenizer and part-ofspeech (POS) tagger (Owoputi et al., 2013) to obtain the tokens and the corresponding POS tags, which are necessary for some sets of features.3 We then use a dictionary to replace any slang with the actual text.4 We also normalize the tex</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<contexts>
<context position="2929" citStr="Wilson et al., 2013" startWordPosition="445" endWordPosition="448">tuning, etc.). In the following we will describe the system with which we participated in the Message Polarity Classification subtask of Sentiment Analysis in Twitter (Task 9) of SEMEVAL 2014 (Rosenthal et al., 2014). Specifically Section 2 describes the data provided by the organizers of the task. Sections 3 and 4 present our system and its performance respectively. Finally, Section 5 concludes and provides hints for future work. 2 Data At first, we describe the data used for this year’s task. For system tuning the organizers released the training and development data of SEMEVAL 2013 Task 2 (Wilson et al., 2013). Both these sets are allowed to be used for training. The organizers also provided the test data of the same Task to be used for development only. As argued in (Malakasiotis et al., 2013) these data suffer from class imbalance. Concerning the test data, they contained 8987 messages broken down in the following 5 datasets: – LJ14: 2000 sentences from LIVEJOURNAL. – SMS13: SMS test data from last year. – TW13: Twitter test data from last year. – TW14: 2000 new tweets. – TWSARC14: 100 tweets containing sarcasm. The details of the test data were made available to the participants only after the e</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013. SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>