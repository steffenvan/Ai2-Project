<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016221">
<title confidence="0.990495">
Grammatical Role Labeling with Integer Linear Programming
</title>
<author confidence="0.899119">
Manfred Klenner
</author>
<affiliation confidence="0.9012035">
Institute of Computational Linguistics
University of Zurich
</affiliation>
<email confidence="0.980211">
klenner@cl.unizh.ch
</email>
<sectionHeader confidence="0.993508" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999622">
In this paper, we present a formalization
of grammatical role labeling within the
framework of Integer Linear Programming
(ILP). We focus on the integration of sub-
categorization information into the deci-
sion making process. We present a first
empirical evaluation that achieves compet-
itive precision and recall rates.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998445894736842">
An often stressed point is that the most widely
used classifiers such as Naive Bayes, HMM, and
Memory-based Learners are restricted to local de-
cisions only. With grammatical role labeling, for
example, there is no way to explicitly express
global constraints that, say, the verb “to give” must
have 3 arguments of a particular grammatical role.
Among the approaches to overcome this restric-
tion, i.e. that allow for global, theory based con-
straints, Integer Linear Programming (ILP) has
been applied to NLP (Punyakanok et al., 2004) .
We apply ILP to the problem of grammatical re-
lation labeling, i.e. given two chunks.1 (e.g. a
verb and a np), what is the grammatical relation
between them (if there is any). We have trained a
maximum entropy classifier on vectors with mor-
phological, syntactic and positional information.
Its output is utilized as weights to the ILP com-
ponent which generates equations to solve the fol-
lowing problem: Given subcategorization frames
(expressed in functional roles, e.g. subject), and
given a sentence with verbs, (auxiliary, modal,
finite, non-finite, ..), and chunks, ( , ), label
all pairs ( ) with a grammatical role2.
In this paper, we are pursuing two empirical sce-
narios. The first is to collapse all subcategoriza-
1Currently, we use perfect chunks, that is, chunks stem-
ming from automatically flattening a treebank.
2Most of these pairs do not stand in a proper grammatical
relation, they get a null class assignment.
tion frames of a verb into a single one, comprising
all subcategorized roles of the verb but not nec-
essarily forming a valid subcategorization frame
of that verb at all. For example, the verb “to be-
lieve” subcategorizes for a subject and a preposi-
tional complement (“He believes in magic”) or for
a subject and a clausal complement (“She believes
that he is dreaming”), but there is no frame that
combines a subject, a prepositional object and a
clausal object. Nevertheless, the set of valid gram-
matical roles of a verb can serve as a filter operat-
ing upon the output of a statistical classifier. The
typical errors being made by classifiers with only
local decisions are: a constituent is assigned to a
grammatical role more than once and a grammat-
ical role (e.g. of a verb) is instantiated more than
once. The worst example in our tests was a verb
that receives from the maxent classifier two sub-
jects and three clausal objects. Here, such a role
filter will help to improve the results.
The second setting is to provide ILP with the
correct subcategorization frame of the verb. The
results of such an oracle setting define the upper
bound of the performance our ILP approach can
achieve. Future work will be to let ILP find the
optimal subcategorization frame given all frames
of a verb.
</bodyText>
<sectionHeader confidence="0.974938" genericHeader="introduction">
2 The ILP Specification
</sectionHeader>
<bodyText confidence="0.984718083333333">
Integer Linear Programming (ILP) is the name of
a class of constraint satisfaction algorithms which
are restricted to a numerical representation of the
problem to be solved. The objective is to optimize
(minimize or maximize) the numerical solution of
linear equations (see the objective function in Fig.
1). The general form of an ILP specification is
given in Fig. 1 (here: maximization). The goal is
to maximize a -ary function , which is defined
as the sum of the variables .
Assignment decisions (e.g. grammatical role la-
beling) can be modeled in the following way:
</bodyText>
<page confidence="0.996838">
187
</page>
<sectionHeader confidence="0.587011" genericHeader="method">
Objective Function:
Constraints:
</sectionHeader>
<bodyText confidence="0.9995505">
represents the weighted sum of all adjunct at-
tachments. is the weighted sum of all attributive
(“the book in her hand ..”) and genitive
attachments (“die Frau des Professors ” [the
wife of the professor]). represents the weighted
sum of all unassigned objects.3 is the weighted
sum of the case frame instantiations of all verbs in
the sentence. It is defined as follows:
</bodyText>
<figure confidence="0.9012925">
(2)
are variables, , and are constants.
</figure>
<figureCaption confidence="0.999851">
Figure 1: ILP Specification
</figureCaption>
<bodyText confidence="0.996960882352941">
are binary class variables that indicate the (non-)
assignment of a constituent to the grammatical
function (e.g. subject) of a verb . To rep-
resent this, three indices are needed. Thus, is
a complex variable name, e.g. . For the sake
of readability, we add some mnemotechnical sugar
and use instead or for a constituent
being (or not) the subject of verb ( thus
is an instantiation of ) . If the value of such
a class variable is set to 1 in the course
of the maximization task, the attachment was suc-
cessful, otherwise ( ) it failed. from
Fig. 1 are weights that represent the impact of an
assignment (or a constraint); they provide an em-
pirically based numerical justification of the as-
signment (we don”t need the ). For example,
we represent the impact of =1 by
</bodyText>
<sectionHeader confidence="0.9978" genericHeader="method">
3 Formalization
</sectionHeader>
<bodyText confidence="0.999935777777778">
We restrict our formalization to the following set
of grammatical functions: subject (), direct (i.e.
accusative) object ( ), indirect (i.e. dative) object
(▲), clausal complement ( ), prepositional com-
plement ( ), attributive (np or pp) attachment ( )
and adjunct ( ). The set of grammatical relations
of a verb (verb complements) is denoted with , it
comprises, ,, and .
The objective function is:
</bodyText>
<subsectionHeader confidence="0.711261">
This sums up over all verbs. For each verb,
</subsectionHeader>
<bodyText confidence="0.99676472972973">
each grammatical role ( is the set of such
roles) is instantiated from the stock of all con-
stituents ( , which includes all np and pp
constituents but also the verbs as potential heads
of clausal objects). is a variable that in-
dicates the assignment of a constituent to the
grammatical function of verb . is the
weight of such an assignment. The (binary) value
of each is to be determined in the course
of the constraint satisfaction process, the weight is
taken from the maximum entropy model.
is the function for weighted attributive attach-
ments:
where is the weight of an assignment
of constituent to constituent and is a
binary variable indicating the classification deci-
sion whether actually modifies . In contrast to
, does not include verbs.
The function for weighted adjunct attachments,
, is:
where is the set of constituents of
the sentence. is the weight given to a clas-
sification of a as an adjunct of a clause with
as verbal head.
The function for the weighted assignment to the
null class, , is:
This represents the impact of assigning a con-
stituent neither to a verb (as a complement) nor
.
These weights are derived from a maximum en-
tropy model trained on a treebank (see section 5).
is used to set up numerical constraints. For ex-
ample that a constituent can only be the filler of
one grammatical role. The decision, which of the
class variables are to be “on” or “off” is based on
the weights and the constraints an overall solution
must obey to. ILP seeks to optimize the solution.
</bodyText>
<footnote confidence="0.6399255">
3Not every set of chunks can form a valid dependency tree
- introduces robustness.
</footnote>
<page confidence="0.8828665">
(1)
188
</page>
<bodyText confidence="0.999946823529412">
to another constituent (as an attributive modifier).
means that the constituent has got no
head (e.g. a finite verb as part of a sentential co-
ordination), although it might be the head of other
.
The equations from 1 to 5 are devoted to the
maximization task, i.e. which constituent is at-
tached to which grammatical function and with
which impact. Of course, without any further re-
strictions, every constituent would get assigned to
every grammatical role - because there are no co-
occurrence restrictions. Exactly this would lead to
a maximal sum. In order to assure a valid distribu-
tion, restrictions have to be formulated, e.g. that a
grammatical role can have at most one filler object
and that a constituent can be at most the filler of
one grammatical role.
</bodyText>
<sectionHeader confidence="0.999239" genericHeader="method">
4 Constraints
</sectionHeader>
<bodyText confidence="0.988606791666667">
A constituent must either be bound as an at-
tribute, an adjunct, a verb complement or by the
null class. This is to say that all class variables
with sum up to exactly 1; then is consumed.
(6)
Here, is an index over all constituents and is
one of the grammatical roles of verb ( ).
No two constituents can be attached to each
other symmetrically (being head and modifier of
each other at the same time), i.e. (among oth-
ers) is defined to be asymmetric.
Finally, we must restrict the number of filler
objects a grammatical role can have. Here, we
have to distinguish among our two settings. In
setting one (all case roles of all frames of a verb
are collapsed into a single set of case roles), we
can’t require all grammatical roles to be instanti-
ated (since we have an artificial case frame, not
necessarily a proper one). This is expressed as
in equation 8.
In setting two (the actual case frame is given),
we require that every grammatical role of the
verb ( ) must be instantiated exactly
once:
</bodyText>
<sectionHeader confidence="0.942522" genericHeader="method">
5 The Weighting Scheme
</sectionHeader>
<bodyText confidence="0.9996404">
A maximum entropy model was used to fix a prob-
ability model that serves as the basis for the ILP
weights. The model was trained on the Tiger tree-
bank (Brants et al., 2002) with feature vectors
stemming from the following set of features: the
part of speech tags of the two candidate chunks,
the distance between them in phrases, the number
of verbs between them, the number of punctuation
marks between them, the person, case and num-
ber of the candidates, their heads, the direction of
the attachment (left or right) and a passive/active
voice flag.
The output of the maxent model is for each pair
of chunks (represented by their feature vectors) a
probability vector. Each entry in this probability
vector represents the probability (used as a weight)
that the two chunks are in a particular grammat-
ical relation (including the “non-grammatical re-
lation”, ) . For example, the weight for an
adjunct assignment, , of two chunks (a
verb) and (a or a ) is given by the cor-
responding entry in the probability vector of the
maximum entropy model. The vector also pro-
vides values for a subject assignment of these two
chunks etc.
</bodyText>
<sectionHeader confidence="0.984315" genericHeader="method">
6 Empirical Results
</sectionHeader>
<bodyText confidence="0.999974173913044">
The overall precision of the maximum entropy
classifier is 87.46%. Since candidate pairs are
generated almost without restrictions, most pairs
do not realize a proper grammatical relation. In
the training set these examples are labeled with
the non-grammatical relation label (which
is the basis of ILPs null class ). Since maximum
entropy modeling seeks to sharpen the classifier
with respect to the most prominent class,
gets a strong bias. So things are getting worse, if
we focus on the proper grammatical relations. The
precision then is low, namely 62.73%, the recall is
85.76%, the f-measure is 72.46 %. ILP improves
the precision by almost 20% (in the “all frames in
one setting” the precision is 81.31%).
We trained on 40,000 sentences, which gives
about 700,000 vectors (90% training, 10% test, in-
cluding negative and positive pairings). Our first
experiment was devoted to fix an upper bound for
the ILP approach: we selected from the set of sub-
categorization frames of a verb the correct one (ac-
cording to the gold standard). The set of licenced
grammatical relations then is reduced to the cor-
</bodyText>
<page confidence="0.994611">
189
</page>
<table confidence="0.797802777777778">
F F
Prec Rec F-Mea
91.4 86.1 88.7
90.4 83.3 86.7
88.5 76.9 82.3
79.3 73.7 76.4
98.6 94.1 96.3
76.7 75.6 76.1
75.7 76.9 76.3
</table>
<figure confidence="0.929970176470588">
Rec -Mea
85.7 87.7
79.7 79.1
62.1 67.3
43.6 55.9
96.6 89.3
78.9 76.5
79.9 76.7
Prec
89.8
78.6
73.5
75.6
82.9
74.2
73.6
F
</figure>
<bodyText confidence="0.98304225">
rect subcategorized GR and the non-governable
GR (adjunct) and (attribute). The results are
given in Fig. 2 under F (cf. section 3 for GR
shortcuts, e.g. for subject).
</bodyText>
<figureCaption confidence="0.983533">
Figure 2: Correct Frame and Collapsed Frames
</figureCaption>
<bodyText confidence="0.999740482758621">
The results of the governable GR ( down to
) are quite good, only the results for preposi-
tional complements ( ) are low (the f-measure is
76.4%). From the 36509 grammatical relations,
37173 were found and 31680 were correct. Over-
all precision is 85.23%, recall is 86.77% and the
f-measure is 85.99%. The most dominant error
being made here is the coherent but wrong assign-
ment of constituents to grammatical roles (e.g. the
subject is taken to be object). This is not a prob-
lem with ILP or the subcategorization frames, but
one of the statistical model (and the feature vec-
tors). It does not discriminate well among alter-
natives. Any improvement of the statistical model
will push the precision of ILP.
The results of the second setting, i.e. to collapse
all grammatical roles of the verb frames to a sin-
gle role set (cf. Fig. 2, F ), are astonishingly
good. The f-measures comes close to the results
of (Buchholz, 1999). Overall precision is 79.99%,
recall 82.67% and f-measure is 81.31%. As ex-
pected, the values of the governable GR decrease
(e.g. recall for prepositional objects by 30.1%).
The third setting will be to let ILP choose
among all subcategorization frames of a verb
(there are up to 20 frames per verb). First experi-
ments have shown that the results are between the
and results. The question then is, how
close can we come to the upper bound.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999953466666667">
ILP has been applied to various NLP problems,
including semantic role labeling (Punyakanok et
al., 2004), extraction of predicates from parse trees
(Klenner, 2005) and discourse ordering in genera-
tion (Althaus et al., 2004). (Roth and Yih, 2005)
discuss how to utilize ILP with Conditional Ran-
dom Fields.
Grammatical relation labeling has been coped
with in a couple of articles, e.g. (Buchholz,
1999). There, a cascaded model (of classifiers)
has been proposed (using various tools around
TIMBL). The f-measure (perfect test data) was
83.5%. However, the set of grammatical relations
differs from the one we use, which makes it diffi-
cult to compare the results.
</bodyText>
<sectionHeader confidence="0.983565" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.993089578947368">
In this paper, we argue for the integration of top
down (theory based) information into NLP. One
kind of information that is well known but have
been used only in a data driven manner within
statistical approaches (e.g. the Collins parser) is
subcategorization information (or case frames). If
subcategorization information turns out to be use-
ful at all, it might become so only under the strict
control of a global constraint mechanism. We are
currently testing an ILP formalization where all
subcategorization frames of a verb are competing
with each other. The benefits will be to have the in-
stantiation not only of licensed grammatical roles
of a verb, but of a consistent and coherent instan-
tiation of a single case frame.
Acknowledgment. I would like to thank Markus Dreyer
for fruitful (“long distance”) discussions and a number of
(steadily improved) maximum entropy models. Also, the de-
tailed comments of the reviewers have been very helpful.
</bodyText>
<sectionHeader confidence="0.999222" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999816611111111">
Ernst Althaus, Nikiforos Karamanis, and Alexander Koller.
2004. Computing Locally Coherent Discourses. Proceed-
ings of the ACL. 2004.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius and George Smith. 2002. The TIGER Treebank.
Proceedings of the Workshop on Treebanks and Linguistic
Theories.
Sabine Buchholz, Jorn Veenstra and Walter Daelemans.
1999. Cascaded Grammatical Relation Assignment.
EMNLP-VLC’99, the Joint SIGDAT Conference on Em-
pirical Methods in NLP and Very Large Corpora.
Manfred Klenner. 2005. Extracting Predicate Structures
from Parse Trees. Proceedings of the RANLP 2005.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dave Zi-
mak. 2004. Role Labeling via Integer Linear Program-
ming Inference. Proceedings of the 20th COLING.
Dan Roth and Wen-tau Yih. 2005. ILP Inference for Condi-
tional Random Fields. Proceedings of the ICML, 2005.
</reference>
<page confidence="0.997871">
190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.952699">
<title confidence="0.999866">Grammatical Role Labeling with Integer Linear Programming</title>
<author confidence="0.999651">Manfred Klenner</author>
<affiliation confidence="0.999926">Institute of Computational Linguistics University of Zurich</affiliation>
<email confidence="0.969808">klenner@cl.unizh.ch</email>
<abstract confidence="0.998098666666667">In this paper, we present a formalization of grammatical role labeling within the framework of Integer Linear Programming (ILP). We focus on the integration of subcategorization information into the decision making process. We present a first empirical evaluation that achieves competitive precision and recall rates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ernst Althaus</author>
<author>Nikiforos Karamanis</author>
<author>Alexander Koller</author>
</authors>
<title>Computing Locally Coherent Discourses.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL.</booktitle>
<contexts>
<context position="13243" citStr="Althaus et al., 2004" startWordPosition="2280" endWordPosition="2283">e is 81.31%. As expected, the values of the governable GR decrease (e.g. recall for prepositional objects by 30.1%). The third setting will be to let ILP choose among all subcategorization frames of a verb (there are up to 20 frames per verb). First experiments have shown that the results are between the and results. The question then is, how close can we come to the upper bound. 7 Related Work ILP has been applied to various NLP problems, including semantic role labeling (Punyakanok et al., 2004), extraction of predicates from parse trees (Klenner, 2005) and discourse ordering in generation (Althaus et al., 2004). (Roth and Yih, 2005) discuss how to utilize ILP with Conditional Random Fields. Grammatical relation labeling has been coped with in a couple of articles, e.g. (Buchholz, 1999). There, a cascaded model (of classifiers) has been proposed (using various tools around TIMBL). The f-measure (perfect test data) was 83.5%. However, the set of grammatical relations differs from the one we use, which makes it difficult to compare the results. 8 Conclusion and Future Work In this paper, we argue for the integration of top down (theory based) information into NLP. One kind of information that is well k</context>
</contexts>
<marker>Althaus, Karamanis, Koller, 2004</marker>
<rawString>Ernst Althaus, Nikiforos Karamanis, and Alexander Koller. 2004. Computing Locally Coherent Discourses. Proceedings of the ACL. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<date>2002</date>
<booktitle>The TIGER Treebank. Proceedings of the Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="9103" citStr="Brants et al., 2002" startWordPosition="1566" endWordPosition="1569">our two settings. In setting one (all case roles of all frames of a verb are collapsed into a single set of case roles), we can’t require all grammatical roles to be instantiated (since we have an artificial case frame, not necessarily a proper one). This is expressed as in equation 8. In setting two (the actual case frame is given), we require that every grammatical role of the verb ( ) must be instantiated exactly once: 5 The Weighting Scheme A maximum entropy model was used to fix a probability model that serves as the basis for the ILP weights. The model was trained on the Tiger treebank (Brants et al., 2002) with feature vectors stemming from the following set of features: the part of speech tags of the two candidate chunks, the distance between them in phrases, the number of verbs between them, the number of punctuation marks between them, the person, case and number of the candidates, their heads, the direction of the attachment (left or right) and a passive/active voice flag. The output of the maxent model is for each pair of chunks (represented by their feature vectors) a probability vector. Each entry in this probability vector represents the probability (used as a weight) that the two chunk</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius and George Smith. 2002. The TIGER Treebank. Proceedings of the Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Jorn Veenstra</author>
<author>Walter Daelemans</author>
</authors>
<date>1999</date>
<booktitle>Cascaded Grammatical Relation Assignment. EMNLP-VLC’99, the Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.</booktitle>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>Sabine Buchholz, Jorn Veenstra and Walter Daelemans. 1999. Cascaded Grammatical Relation Assignment. EMNLP-VLC’99, the Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Klenner</author>
</authors>
<title>Extracting Predicate Structures from Parse Trees.</title>
<date>2005</date>
<booktitle>Proceedings of the RANLP</booktitle>
<contexts>
<context position="13183" citStr="Klenner, 2005" startWordPosition="2272" endWordPosition="2273">erall precision is 79.99%, recall 82.67% and f-measure is 81.31%. As expected, the values of the governable GR decrease (e.g. recall for prepositional objects by 30.1%). The third setting will be to let ILP choose among all subcategorization frames of a verb (there are up to 20 frames per verb). First experiments have shown that the results are between the and results. The question then is, how close can we come to the upper bound. 7 Related Work ILP has been applied to various NLP problems, including semantic role labeling (Punyakanok et al., 2004), extraction of predicates from parse trees (Klenner, 2005) and discourse ordering in generation (Althaus et al., 2004). (Roth and Yih, 2005) discuss how to utilize ILP with Conditional Random Fields. Grammatical relation labeling has been coped with in a couple of articles, e.g. (Buchholz, 1999). There, a cascaded model (of classifiers) has been proposed (using various tools around TIMBL). The f-measure (perfect test data) was 83.5%. However, the set of grammatical relations differs from the one we use, which makes it difficult to compare the results. 8 Conclusion and Future Work In this paper, we argue for the integration of top down (theory based) </context>
</contexts>
<marker>Klenner, 2005</marker>
<rawString>Manfred Klenner. 2005. Extracting Predicate Structures from Parse Trees. Proceedings of the RANLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
<author>Dave Zimak</author>
</authors>
<title>Role Labeling via Integer Linear Programming Inference.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th COLING.</booktitle>
<contexts>
<context position="1028" citStr="Punyakanok et al., 2004" startWordPosition="150" endWordPosition="153">t empirical evaluation that achieves competitive precision and recall rates. 1 Introduction An often stressed point is that the most widely used classifiers such as Naive Bayes, HMM, and Memory-based Learners are restricted to local decisions only. With grammatical role labeling, for example, there is no way to explicitly express global constraints that, say, the verb “to give” must have 3 arguments of a particular grammatical role. Among the approaches to overcome this restriction, i.e. that allow for global, theory based constraints, Integer Linear Programming (ILP) has been applied to NLP (Punyakanok et al., 2004) . We apply ILP to the problem of grammatical relation labeling, i.e. given two chunks.1 (e.g. a verb and a np), what is the grammatical relation between them (if there is any). We have trained a maximum entropy classifier on vectors with morphological, syntactic and positional information. Its output is utilized as weights to the ILP component which generates equations to solve the following problem: Given subcategorization frames (expressed in functional roles, e.g. subject), and given a sentence with verbs, (auxiliary, modal, finite, non-finite, ..), and chunks, ( , ), label all pairs ( ) w</context>
<context position="13124" citStr="Punyakanok et al., 2004" startWordPosition="2262" endWordPosition="2265">od. The f-measures comes close to the results of (Buchholz, 1999). Overall precision is 79.99%, recall 82.67% and f-measure is 81.31%. As expected, the values of the governable GR decrease (e.g. recall for prepositional objects by 30.1%). The third setting will be to let ILP choose among all subcategorization frames of a verb (there are up to 20 frames per verb). First experiments have shown that the results are between the and results. The question then is, how close can we come to the upper bound. 7 Related Work ILP has been applied to various NLP problems, including semantic role labeling (Punyakanok et al., 2004), extraction of predicates from parse trees (Klenner, 2005) and discourse ordering in generation (Althaus et al., 2004). (Roth and Yih, 2005) discuss how to utilize ILP with Conditional Random Fields. Grammatical relation labeling has been coped with in a couple of articles, e.g. (Buchholz, 1999). There, a cascaded model (of classifiers) has been proposed (using various tools around TIMBL). The f-measure (perfect test data) was 83.5%. However, the set of grammatical relations differs from the one we use, which makes it difficult to compare the results. 8 Conclusion and Future Work In this pape</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dave Zimak. 2004. Role Labeling via Integer Linear Programming Inference. Proceedings of the 20th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>ILP Inference for Conditional Random Fields.</title>
<date>2005</date>
<booktitle>Proceedings of the ICML,</booktitle>
<contexts>
<context position="13265" citStr="Roth and Yih, 2005" startWordPosition="2284" endWordPosition="2287">, the values of the governable GR decrease (e.g. recall for prepositional objects by 30.1%). The third setting will be to let ILP choose among all subcategorization frames of a verb (there are up to 20 frames per verb). First experiments have shown that the results are between the and results. The question then is, how close can we come to the upper bound. 7 Related Work ILP has been applied to various NLP problems, including semantic role labeling (Punyakanok et al., 2004), extraction of predicates from parse trees (Klenner, 2005) and discourse ordering in generation (Althaus et al., 2004). (Roth and Yih, 2005) discuss how to utilize ILP with Conditional Random Fields. Grammatical relation labeling has been coped with in a couple of articles, e.g. (Buchholz, 1999). There, a cascaded model (of classifiers) has been proposed (using various tools around TIMBL). The f-measure (perfect test data) was 83.5%. However, the set of grammatical relations differs from the one we use, which makes it difficult to compare the results. 8 Conclusion and Future Work In this paper, we argue for the integration of top down (theory based) information into NLP. One kind of information that is well known but have been use</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>Dan Roth and Wen-tau Yih. 2005. ILP Inference for Conditional Random Fields. Proceedings of the ICML, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>