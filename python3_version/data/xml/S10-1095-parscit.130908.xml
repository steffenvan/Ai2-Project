<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000298">
<title confidence="0.928146">
UMCC-DLSI: Integrative resource for disambiguation task
</title>
<author confidence="0.840434">
Yoan Guti´errez and Antonio Fern´andez
</author>
<affiliation confidence="0.737833">
DI, University of Matanzas
Autopista a Varadero km 31/2
</affiliation>
<address confidence="0.439736">
Matanzas, Cuba
</address>
<email confidence="0.923335">
yoan.gutierrez,antonio.fernandez@umcc.cu
</email>
<author confidence="0.997963">
Andr´es Montoyo and Sonia V´azquez
</author>
<affiliation confidence="0.752199333333333">
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
</affiliation>
<email confidence="0.995234">
montoyo,svazquez@dlsi.ua.es
</email>
<sectionHeader confidence="0.996615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999625846153846">
This paper describes the UMCC-DLSI
system in SemEval-2010 task number 17
(All-words Word Sense Disambiguation
on Specific Domain). The main purpose
of this work is to evaluate and compare
our computational resource of WordNet’s
mappings using 3 different methods:
Relevant Semantic Tree, Relevant
Semantic Tree 2 and an Adaptation of
k-clique’s Technique. Our proposal is
a non-supervised and knowledge-based
system that uses Domains Ontology and
SUMO.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99985065625">
Ambiguity is the task of building up multiple
alternative linguistic structures for a single
input (Kozareva et al., 2007). Word Sense
Disambiguation (WSD) is a key enabling-
technology that automatically chooses the
intended sense of a word in context. In this task,
one of the most used lexical data base is WordNet
(WN) (Fellbaum, 1998). WN is an online lexical
reference system whose design is inspired by
current psycholinguistic theories of human lexical
memory. Due to the great popularity of WN
in Natural Language Processing (NLP), several
authors (Magnini and Cavaglia, 2000), (Niles
and Pease, 2001), (Niles and Pease, 2003),
(Valitutti, 2004) have proposed to incorporate to
the semantic net of WN, some taxonomies that
characterize, in one or several concepts, the senses
of each word. In spite of the fact that there have
been developed a lot of WordNet’s mappings,
there isn’t one unique resource to integrate all
of them in a single system approach. To solve
this need we have developed a resource that joins
WN1, the SUMO Ontology2, WordNet Domains3
and WordNet Affect4. Our purpose is to test the
advantages of having all the resources together for
the resolution of the WSD task.
The rest of the paper is organized as follows.
In Section 2 we describe the architecture of the
integrative resource. Our approach is shown in
Section 3. Next section presents the obtained
results and a discussion. And finally the
conclusions in Section 5.
</bodyText>
<sectionHeader confidence="0.913947" genericHeader="introduction">
2 Background and techniques
</sectionHeader>
<subsectionHeader confidence="0.99925">
2.1 Architecture of the integrative resource
</subsectionHeader>
<bodyText confidence="0.999707333333333">
Our integrative model takes WN 1.6 as nucleus
and links to it the SUMO resource. Moreover,
WordNet Domains 2.0 (WND) and WordNet
Affect 1.1 (WNAffects) are also integrated but
mapped instead to WN 2.0. From the model
showed in Figure 1, a computational resource has
been built in order to integrate the mappings above
mentioned.
The model integrator’s proposal provides
a software that incorporates bookstores of
programming classes, capable to navigate inside
the semantic graph and to apply any type of
possible algorithm to a net. The software
architecture allows to update WN’s version.
In order to maintain the compatibility with other
resources mapped to WN, we have decided to use
WN 1.6 version. However, the results can be
offered in anyone of WN’s versions.
</bodyText>
<footnote confidence="0.99939675">
1http://www.cogsci.princeton.edu/ wn/
2http://suo.ieee.org
3http://wndomains.fbk.eu/
427 4http://wndomains.fbk.eu/wnaffect.html
</footnote>
<note confidence="0.899225">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 427–432,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999467">
Figure 1: WordNet integrative model
</figureCaption>
<subsectionHeader confidence="0.994863">
2.2 The k-clique’s Technique
</subsectionHeader>
<bodyText confidence="0.999832363636364">
Formally, a clique is the maximum number of
actors who have all possible ties presented among
themselves. A “Maximal complete sub-graph” is
such a grouping, expanded to include as many
actors as possible.
“A k-clique is a subset of vertices C such that,
for every i, j E C, the distance d(i, j)k. The 1-
clique is identical to a clique, because the distance
between the vertices is one edge. The 2-clique
is the maximal complete sub-graph with a path
length of one or two edges”. (Cavique et al., 2009)
</bodyText>
<sectionHeader confidence="0.988568" genericHeader="method">
3 The Proposal
</sectionHeader>
<bodyText confidence="0.9973284">
Our proposal consists in accomplishing three runs
with different algorithms. Both first utilize the
domain’s vectors; the third method utilizes k-
cliques’ techniques.
This work is divided in several stages:
</bodyText>
<listItem confidence="0.983304766666667">
1. Pre-processing of the corpus (lemmatization
with Freeling) (Atserias et al., 2006).
2. Context selection (For the first (3.1), and
the third (3.3) run the context window was
constituted by the sentence that contains
the word to disambiguate; in the second
run the context window was constituted
by the sentence that contains the word to
disambiguate, the previous sentence and the
next one).
3. Obtaining the domain vector, this vector is
used in first and the second runs (when
the lemma of the words in the analyzed
sentence is obtained, the integrative resource
of WordNet’s Mappings is used to get the
respective senses from each lemma).
4. Obtaining the all resource vector: SUMO,
Affects, and Domain resource. This is only
for the third run (3.3).
5. Relevant Semantic Tree construction
(Addition of concepts parents to the vectors.
For the first (3.1) and second (3.2) runs only
Domain resource is used; for the third (3.3)
run all the resources are used).
6. Selection of the correct senses (the first and
the second runs use the same way to do the
selection; the third run is different. We make
an exception: For the verb “be” we select the
sense with the higher frequency according to
Freeling.
</listItem>
<subsectionHeader confidence="0.994585">
3.1 Relevant Semantic Tree
</subsectionHeader>
<bodyText confidence="0.998753909090909">
With this proposal we measure how much a
concept is correlated to the sentence, similar to
Reuters Vector (Magnini et al., 2002), but with
a different equation. This proposal has a partial
similarity with the Conceptual Density (Agirre
and Rigau, 1996) and DRelevant (V´azquez et al.,
2004) to get the concepts from a hierarchy that
they associate with the sentence.
In order to determine the Association Ratio
(RA) of a domain in relation to the sentence, the
Equation 1 is used.
</bodyText>
<equation confidence="0.984597">
n
RA(D, f) = RA(D, fz) (1)
z=1
where:
P(D, w)
RA(D, w) = P(D, w) � lo�2 P (D) (2)
</equation>
<bodyText confidence="0.994957333333333">
f: is a set of words w.
fz: is a i-th word of the phrase f.
P(D, w): is joint probability distribution.
P(D): is marginal probability.
From now, vectors are created using the
Senseval-2’s corpus. Next, we show an example:
For the phrase: “But it is unfair to dump
on teachers as distinct from the educational
establishment”.
By means of the process Pres-processing
analyzed in previous stage 1 we get the lemma and
the following vector.
</bodyText>
<page confidence="0.985926">
428
</page>
<bodyText confidence="0.948515">
Phrase [unfair; dump; teacher, distinct,
educational; establishment]
Each lemma is looked for in WordNet’s
integrative resource of mappings and it is
correlated with concepts of WND.
</bodyText>
<figure confidence="0.996175818181818">
Vector
RA Domains
0.9 Pedagogy
0.9 Administration
0.36 Buildings
0.36 Politics
0.36 Environment
0.36 Commerce
0.36 Quality
0.36 Psychoanalysis
0.36 Economy
</figure>
<tableCaption confidence="0.993513">
Table 1: Initial Domain Vector
</tableCaption>
<table confidence="0.999403">
Vector
RA Domains
1.63 Social Science
0.9 Administration
0.9 Pedagogy
0.8 RootDomain
0.36 Psychoanalysis
0.36 Economy
0.36 Quality
0.36 Politics
0.36 Buildings
0.36 Commerce
0.36 Environment
0.11 Factotum
0.11 Psychology
0.11 Architecture
0.11 Pure Science
</table>
<tableCaption confidence="0.989846">
Table 2: Final Domain Vector
</tableCaption>
<bodyText confidence="0.992012">
After obtaining the Initial Domain Vector we
apply the Equation 3 in order to build the Relevant
Semantic Tree related to the phrase.
</bodyText>
<equation confidence="0.83157">
DN(CI, Df) = RA CI − MP(TD, Df) (3)
Where DN: is a normalized distance
</equation>
<bodyText confidence="0.941654">
CI: is the Initial Concept which you want to
add the ancestors.
Df: is Parent Domain.
RA CI: is a Association Ratio of the child
Concept.
TD: is Depth of the hierarchic tree of the
resource to use.
MP: is Minimal Path.
Applying the Equation 3 the algorithm to decide
which parent domain will be added to the vector is
shown here:
</bodyText>
<construct confidence="0.692052">
if (DN(CI, Df) &gt; 0)
</construct>
<bodyText confidence="0.782182583333333">
{
if ( Df not exist)
Df is added to the vector with DN value;
else
Df value = Df value + DN;
}
As a result the Table 2 is obtained.
This vector represents the Domain tree
associated to the phrase.
After the Relevant Semantic Tree is obtained,
the Domain Factotum is eliminated from the tree.
Due to the large amount of WordNet synsets,
</bodyText>
<figureCaption confidence="0.997144">
Figure 2: Relevant semantic tree
</figureCaption>
<bodyText confidence="0.998834928571428">
that do not belong to a specific domain, but
rather they can appear in almost all of them, the
Factotum domain has been created. It basically
includes two types of synsets: Generic synsets,
which are hard to classify in a particular domain;
and Stop Senses synsets which appear frequently
in different contexts, such as numbers, week
days, colors, etc. (Magnini and Cavaglia, 2000),
(Magnini et al., 2002). Words that contain this
synsets are frequently in the phrases, therefore the
senses associated to this domain are not selected.
After processing the patterns that characterize
the sentence, the following stage is to determine
the correct senses, so that the next steps ensue:
</bodyText>
<footnote confidence="0.527261666666667">
1. Senses that do not coincide with the
grammatical category of Freeling are
removed.
</footnote>
<page confidence="0.988665">
429
</page>
<bodyText confidence="0.8835938">
2. For each word to disambiguate all candidate
senses are obtained. Of each sense
the relevant vector are obtained using the
Equation 4, and according to the previous
Equation 3 parent concepts are added.
</bodyText>
<equation confidence="0.86035125">
P(D, s)
RA(D, s) = P(D, s) � log2 P (D) (4)
where s: is a sense of word.
P(D, s): is joint probability distribution
</equation>
<bodyText confidence="0.981544818181818">
between Domain concept D and the sense s.
P(D): is marginal probability of the Domain
concept.
3. The one that accumulates the bigger value of
relevance is assigned as correct sense. The
following process is applied:
For each coincidence of the elements in the
senses’ domain vector with the domain vector
of the sentence, the RA value of the analyzed
elements is accumulated. The process is
described in the Equation 5.
</bodyText>
<equation confidence="0.9769705">
�k V RA[V sk]
AC(s, V RA) = (5) i�1V RAi
</equation>
<bodyText confidence="0.977418857142857">
where AC: The RA value accumulated for
the analyzed elements.
VRA: Vector of relevant domains of the
sentence with the format: VRA [domain —
value RA].
V s: Vector of relevant domain of the sense
with the format: V s [domain].
</bodyText>
<equation confidence="0.95391">
V sk: Is a k-th domain of the vector V s.
</equation>
<bodyText confidence="0.935394">
VRA[Vsk]: Represents the value of RA
assigned to the domain V sk for the value
VRA.
The &amp;_1 V RAi term normalizes the result.
</bodyText>
<subsectionHeader confidence="0.995511">
3.2 Relevant Semantic Tree 2
</subsectionHeader>
<bodyText confidence="0.9997786">
This run is the same as the first one with a
little difference, the context window is constituted
by the sentence that contains the word to
disambiguate, the previous sentence and the next
one.
</bodyText>
<subsectionHeader confidence="0.879778">
3.3 Adaptation of k-clique’s technique to the
WSD
</subsectionHeader>
<bodyText confidence="0.999950294117647">
They are applied, of the section 3, the steps from
the 1 to the 5, where the semantic trees of concepts
are obtained.
Then they are already obtained for all the
words of the context, all the senses discriminated
according to Freeling (Atserias et al., 2006).
Then a sentence’s net of knowledge is built
by means of minimal paths among each sense
and each concept at trees. Next the k-clique’s
technique is applied to the net of knowledge to
obtain cohesive subsets of nodes.
To obtain the correct sense of each word it is
looked, as proposed sense, the sense belonging to
the subset containing more quantities of nodes and
if it has more than a sense for the same word,
the more frequent sense is chosen according to
Freeling.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.994798571428571">
The conducted experiments measure the
influence of the aforementioned resources in
the disambiguation task. We have evaluated them
individually and as a whole. In the Table 3 it
is represented each one of the inclusions and
combinations experimented with the Relevant
Semantic Tree method.
</bodyText>
<table confidence="0.999419444444445">
Resources Precision Recall Attempted
WNAffect 0.242 0.237 97.78%
SUMO 0.267 0.261 98.5%
WND 0.328 0.322 98.14%
WND &amp; 0.308 0.301 97.78%
SUMO
WND &amp; 0.308 0.301 97.78%
SUMO &amp;
WNAffect
</table>
<tableCaption confidence="0.999947">
Table 3: Evaluation of integrated resources
</tableCaption>
<bodyText confidence="0.999897727272727">
As it can be observed, in the evaluation for
specific domain corpus the best results are reached
when only domain resource is used. But this
is not a conclusion about the resources inclusion
because the use of this method for global domain,
for example with the task English All words from
Senseval-2 (Agirre et al., 2010), the experiment
adding all the resources showed good results. This
is due to the fact that the global domain includes
information of different contexts, exactly what
is representing in the mentioned resources. For
</bodyText>
<page confidence="0.988471">
430
</page>
<bodyText confidence="0.999962044444445">
this reason, in the experiment with global domain
and the inclusion of all the resource obtained
better results than using this method with specific
domain, 42% of recall and 45% of precision
(Guti´errez, 2010).
For example, with the k-clique’s technique,
utilizing the English All word task from Senseval-
2´s corpus, the results for the test with global
dominion were: with single domain inclusion 40
% of precision and recall; but with the three
resources 41.7 % for both measures.
Table 4 shows the obtained results for the test
data set. The average performance of our system
is 32% and we ranked on 27-th position from
27 participating systems. Although, we have
used different sources of information and various
approximations, in the future we have to surmount
a number of obstacles.
One of the limitations comes from the usage
of the POS-tagger Freeling which introduces
some errors in the grammatical discrimination.
Representing a loss of 3.7% in the precision of our
system.
The base of knowledge utilized in the task was
WordNet 1.6; but the competition demanded the
results with WordNet 3.0. In order to achieve
this we utilized mappings among versions where
119 of 1398 resulting senses emitted by Semeval-
2 were did not found. This represents an 8.5%.
In our proposal, the sense belonging to the
Factotum Domain was eliminated, what disabled
that the senses linked to this domain went
candidates to be recovered. 777 senses of 1398
annotated like correct for Semeval-2 belong to
domain Factotum, what represents that the 66%
were not recovered by our system. Considering
the senses that are not correlated to Factotum,
that is, that correlate to another domains, we are
speaking about 621 senses to define; The system
would emit results of a 72,4%. Senses selected
correctly were 450, representing a 32%. However,
189 kept on like second candidates to be elected.
This represents a 13.5%. If a technique of more
precise decision takes effect, the results of the
system could be increased largely.
</bodyText>
<sectionHeader confidence="0.983392" genericHeader="conclusions">
5 Conclusion and future works
</sectionHeader>
<bodyText confidence="0.98401825">
For our participation in the Semeval-2 task
17 (All-words Word Sense Disambiguation on
Specific Domain), we presented three methods
for disambiguation approach which uses an
</bodyText>
<table confidence="0.997981888888889">
Methods Precision Recall Attempted
Relevant 0.328 0.322 98.14%
Domains
Tree
Relevant 0.321 0.315 98.14%
Semantic
Tree 2
Relevant 0.312 0.303 97.35%
Cliques
</table>
<tableCaption confidence="0.999712">
Table 4: Evaluation results
</tableCaption>
<bodyText confidence="0.999929954545454">
integrative resource of WordNet mappings. We
conducted an experimental study with the trail
data set, according to which the Relevant Semantic
Tree reaches the best performance. Our current
approach can be improved with the incorporation
of more granularities in the hierarchy of WordNet
Domains. Because it was demonstrated that
to define correct senses associated to specific
domains an improvement of 72.4% is obtained.
At this moment, only domain information is used
in our first and second method. Besides was
demonstrated for specific domains, the inclusion
of several resources worsened the results with the
first and second proposal method, the third one has
been not experimented yet. Despite the fact that
we have knowledge of SUMO, WordNet-Affect
and WordNet Domain in our third method we still
not obtain a relevant result.
It would be convenient to enrich our resource
with other resources like Frame-Net, Concept-Net
or others with the objective of characterizing even
more the senses of the words.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999916333333333">
This paper has been supported partially by
Ministerio de Ciencia e Innovaci´on - Spanish
Government (grant no. TIN2009-13391-C04-
01), and Conselleria d’Educaci´o - Generalitat
Valenciana (grant no. PROMETEO/2009/119 and
ACOMP/2010/288).
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996148105263158">
Eneko Agirre and German Rigau. 1996. Word
sense disambiguation using conceptual density. In
Proceedings of the 16th International Conference
on Computational Linguistic (COLING´96),
Copenhagen, Denmark.
Eneko Agirre, Oier Lopez de Lacalle, Christiane
Fellbaum, Shu-kai Hsieh, Maurizio Tesconi, Monica
431
Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word
sense disambiguation on a specific domain. In
Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2010), Association
for Computational Linguistics.
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonz´alez, Lluis Padr´o, and Muntsa Padr´o.
2006. Freeling 1.3: Syntactic and semantic services
in an open-source nlp library. In Proceedings
of the fifth international conference on Language
Resources and Evaluation (LREC 2006), ELRA.
Luis Cavique, Armando B. Mendes, and Jorge M.
Santos. 2009. An algorithm to discover the k-
clique cover in networks. In EPIA ’09: Proceedings
of the 14th Portuguese Conference on Artificial
Intelligence, pages 363–373, Berlin, Heidelberg.
Springer-Verlag.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Yoan Guti´errez. 2010. Resoluci´on de ambiguedad
sem´antica mediante el uso de vectores de conceptos
relevantes.
Zornitsa Kozareva, Sonia V´azquez, and Andr´es
Montoyo. 2007. Ua-zsa: Web page clustering on
the basis of name disambiguation. In Semeval I. 4th
International Wordshop on Semantic Evaluations.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating subject field codes into wordnet. In
Proceedings of Third International Conference on
Language Resources and Evaluation (LREC-2000).
Bernardo Magnini, Carlo Strapparava, Giovanni
Pezzulo, and Alfio Gliozzo. 2002. Comparing
ontology-based and corpus-based domain
annotations in wordnet. In Proceedings of the
First International WordNet Conference, pages
146–154.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In FOIS, pages 2–9.
Ian Niles and Adam Pease. 2003. Linking lexicons
and ontologies: Mapping wordnet to the suggested
upper merged ontology. In IKE, pages 412–416.
Ro Valitutti. 2004. Wordnet-affect: an affective
extension of wordnet. In Proceedings of the 4th
International Conference on Language Resources
and Evaluation, pages 1083–1086.
Sonia V´azquez, Andr´es Montoyo, and German Rigau.
2004. Using relevant domains resource for word
sense disambiguation. In IC-AI, pages 784–789.
</reference>
<page confidence="0.998556">
432
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.171261">
<title confidence="0.998299">UMCC-DLSI: Integrative resource for disambiguation task</title>
<author confidence="0.99898">Guti´errez Fern´andez</author>
<affiliation confidence="0.8615075">DI, University of Matanzas a Varadero km</affiliation>
<address confidence="0.964483">Matanzas, Cuba</address>
<email confidence="0.997413">yoan.gutierrez,antonio.fernandez@umcc.cu</email>
<author confidence="0.995436">Montoyo V´azquez</author>
<affiliation confidence="0.999691">DLSI, University of Alicante</affiliation>
<address confidence="0.612155">Carretera de San Vicente S/N Alicante, Spain</address>
<email confidence="0.98796">montoyo,svazquez@dlsi.ua.es</email>
<abstract confidence="0.972669285714286">This paper describes the UMCC-DLSI system in SemEval-2010 task number 17 (All-words Word Sense Disambiguation on Specific Domain). The main purpose of this work is to evaluate and compare our computational resource of WordNet’s mappings using 3 different methods: Relevant Semantic Tree, Relevant Semantic Tree 2 and an Adaptation of k-clique’s Technique. Our proposal is a non-supervised and knowledge-based system that uses Domains Ontology and SUMO.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>German Rigau</author>
</authors>
<title>Word sense disambiguation using conceptual density.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistic (COLING´96),</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="5698" citStr="Agirre and Rigau, 1996" startWordPosition="889" endWordPosition="892">d second (3.2) runs only Domain resource is used; for the third (3.3) run all the resources are used). 6. Selection of the correct senses (the first and the second runs use the same way to do the selection; the third run is different. We make an exception: For the verb “be” we select the sense with the higher frequency according to Freeling. 3.1 Relevant Semantic Tree With this proposal we measure how much a concept is correlated to the sentence, similar to Reuters Vector (Magnini et al., 2002), but with a different equation. This proposal has a partial similarity with the Conceptual Density (Agirre and Rigau, 1996) and DRelevant (V´azquez et al., 2004) to get the concepts from a hierarchy that they associate with the sentence. In order to determine the Association Ratio (RA) of a domain in relation to the sentence, the Equation 1 is used. n RA(D, f) = RA(D, fz) (1) z=1 where: P(D, w) RA(D, w) = P(D, w) � lo�2 P (D) (2) f: is a set of words w. fz: is a i-th word of the phrase f. P(D, w): is joint probability distribution. P(D): is marginal probability. From now, vectors are created using the Senseval-2’s corpus. Next, we show an example: For the phrase: “But it is unfair to dump on teachers as distinct f</context>
</contexts>
<marker>Agirre, Rigau, 1996</marker>
<rawString>Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In Proceedings of the 16th International Conference on Computational Linguistic (COLING´96), Copenhagen, Denmark.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez de Lacalle, Christiane Fellbaum,</title>
<journal>Shu-kai Hsieh, Maurizio Tesconi, Monica</journal>
<volume>431</volume>
<marker>Agirre, </marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, Christiane Fellbaum, Shu-kai Hsieh, Maurizio Tesconi, Monica 431</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piek Vossen Monachini</author>
<author>Roxanne Segers</author>
</authors>
<title>Semeval-2010 task 17: All-words word sense disambiguation on a specific domain.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Association for Computational Linguistics.</booktitle>
<marker>Monachini, Segers, 2010</marker>
<rawString>Monachini, Piek Vossen, and Roxanne Segers. 2010. Semeval-2010 task 17: All-words word sense disambiguation on a specific domain. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Atserias</author>
<author>Bernardino Casas</author>
<author>Elisabet Comelles</author>
<author>Meritxell Gonz´alez</author>
<author>Lluis Padr´o</author>
<author>Muntsa Padr´o</author>
</authors>
<title>Freeling 1.3: Syntactic and semantic services in an open-source nlp library.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC</booktitle>
<pages>ELRA.</pages>
<marker>Atserias, Casas, Comelles, Gonz´alez, Padr´o, Padr´o, 2006</marker>
<rawString>Jordi Atserias, Bernardino Casas, Elisabet Comelles, Meritxell Gonz´alez, Lluis Padr´o, and Muntsa Padr´o. 2006. Freeling 1.3: Syntactic and semantic services in an open-source nlp library. In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006), ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Cavique</author>
<author>Armando B Mendes</author>
<author>Jorge M Santos</author>
</authors>
<title>An algorithm to discover the kclique cover in networks.</title>
<date>2009</date>
<booktitle>In EPIA ’09: Proceedings of the 14th Portuguese Conference on Artificial Intelligence,</booktitle>
<pages>363--373</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="3989" citStr="Cavique et al., 2009" startWordPosition="607" endWordPosition="610">010. c�2010 Association for Computational Linguistics Figure 1: WordNet integrative model 2.2 The k-clique’s Technique Formally, a clique is the maximum number of actors who have all possible ties presented among themselves. A “Maximal complete sub-graph” is such a grouping, expanded to include as many actors as possible. “A k-clique is a subset of vertices C such that, for every i, j E C, the distance d(i, j)k. The 1- clique is identical to a clique, because the distance between the vertices is one edge. The 2-clique is the maximal complete sub-graph with a path length of one or two edges”. (Cavique et al., 2009) 3 The Proposal Our proposal consists in accomplishing three runs with different algorithms. Both first utilize the domain’s vectors; the third method utilizes kcliques’ techniques. This work is divided in several stages: 1. Pre-processing of the corpus (lemmatization with Freeling) (Atserias et al., 2006). 2. Context selection (For the first (3.1), and the third (3.3) run the context window was constituted by the sentence that contains the word to disambiguate; in the second run the context window was constituted by the sentence that contains the word to disambiguate, the previous sentence an</context>
</contexts>
<marker>Cavique, Mendes, Santos, 2009</marker>
<rawString>Luis Cavique, Armando B. Mendes, and Jorge M. Santos. 2009. An algorithm to discover the kclique cover in networks. In EPIA ’09: Proceedings of the 14th Portuguese Conference on Artificial Intelligence, pages 363–373, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1158" citStr="Fellbaum, 1998" startWordPosition="162" endWordPosition="163">r computational resource of WordNet’s mappings using 3 different methods: Relevant Semantic Tree, Relevant Semantic Tree 2 and an Adaptation of k-clique’s Technique. Our proposal is a non-supervised and knowledge-based system that uses Domains Ontology and SUMO. 1 Introduction Ambiguity is the task of building up multiple alternative linguistic structures for a single input (Kozareva et al., 2007). Word Sense Disambiguation (WSD) is a key enablingtechnology that automatically chooses the intended sense of a word in context. In this task, one of the most used lexical data base is WordNet (WN) (Fellbaum, 1998). WN is an online lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. Due to the great popularity of WN in Natural Language Processing (NLP), several authors (Magnini and Cavaglia, 2000), (Niles and Pease, 2001), (Niles and Pease, 2003), (Valitutti, 2004) have proposed to incorporate to the semantic net of WN, some taxonomies that characterize, in one or several concepts, the senses of each word. In spite of the fact that there have been developed a lot of WordNet’s mappings, there isn’t one unique resource to integrate all of them in</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoan Guti´errez</author>
</authors>
<title>Resoluci´on de ambiguedad sem´antica mediante el uso de vectores de conceptos relevantes.</title>
<date>2010</date>
<marker>Guti´errez, 2010</marker>
<rawString>Yoan Guti´errez. 2010. Resoluci´on de ambiguedad sem´antica mediante el uso de vectores de conceptos relevantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Sonia V´azquez</author>
<author>Andr´es Montoyo</author>
</authors>
<title>Ua-zsa: Web page clustering on the basis of name disambiguation.</title>
<date>2007</date>
<booktitle>In Semeval I. 4th International Wordshop on Semantic Evaluations.</booktitle>
<marker>Kozareva, V´azquez, Montoyo, 2007</marker>
<rawString>Zornitsa Kozareva, Sonia V´azquez, and Andr´es Montoyo. 2007. Ua-zsa: Web page clustering on the basis of name disambiguation. In Semeval I. 4th International Wordshop on Semantic Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavaglia</author>
</authors>
<title>Integrating subject field codes into wordnet.</title>
<date>2000</date>
<booktitle>In Proceedings of Third International Conference on Language Resources and Evaluation (LREC-2000).</booktitle>
<contexts>
<context position="1404" citStr="Magnini and Cavaglia, 2000" startWordPosition="197" endWordPosition="200">s Domains Ontology and SUMO. 1 Introduction Ambiguity is the task of building up multiple alternative linguistic structures for a single input (Kozareva et al., 2007). Word Sense Disambiguation (WSD) is a key enablingtechnology that automatically chooses the intended sense of a word in context. In this task, one of the most used lexical data base is WordNet (WN) (Fellbaum, 1998). WN is an online lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. Due to the great popularity of WN in Natural Language Processing (NLP), several authors (Magnini and Cavaglia, 2000), (Niles and Pease, 2001), (Niles and Pease, 2003), (Valitutti, 2004) have proposed to incorporate to the semantic net of WN, some taxonomies that characterize, in one or several concepts, the senses of each word. In spite of the fact that there have been developed a lot of WordNet’s mappings, there isn’t one unique resource to integrate all of them in a single system approach. To solve this need we have developed a resource that joins WN1, the SUMO Ontology2, WordNet Domains3 and WordNet Affect4. Our purpose is to test the advantages of having all the resources together for the resolution of </context>
<context position="8411" citStr="Magnini and Cavaglia, 2000" startWordPosition="1357" endWordPosition="1360">btained. This vector represents the Domain tree associated to the phrase. After the Relevant Semantic Tree is obtained, the Domain Factotum is eliminated from the tree. Due to the large amount of WordNet synsets, Figure 2: Relevant semantic tree that do not belong to a specific domain, but rather they can appear in almost all of them, the Factotum domain has been created. It basically includes two types of synsets: Generic synsets, which are hard to classify in a particular domain; and Stop Senses synsets which appear frequently in different contexts, such as numbers, week days, colors, etc. (Magnini and Cavaglia, 2000), (Magnini et al., 2002). Words that contain this synsets are frequently in the phrases, therefore the senses associated to this domain are not selected. After processing the patterns that characterize the sentence, the following stage is to determine the correct senses, so that the next steps ensue: 1. Senses that do not coincide with the grammatical category of Freeling are removed. 429 2. For each word to disambiguate all candidate senses are obtained. Of each sense the relevant vector are obtained using the Equation 4, and according to the previous Equation 3 parent concepts are added. P(D</context>
</contexts>
<marker>Magnini, Cavaglia, 2000</marker>
<rawString>Bernardo Magnini and Gabriela Cavaglia. 2000. Integrating subject field codes into wordnet. In Proceedings of Third International Conference on Language Resources and Evaluation (LREC-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Giovanni Pezzulo</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Comparing ontology-based and corpus-based domain annotations in wordnet.</title>
<date>2002</date>
<booktitle>In Proceedings of the First International WordNet Conference,</booktitle>
<pages>146--154</pages>
<contexts>
<context position="5574" citStr="Magnini et al., 2002" startWordPosition="870" endWordPosition="873">ird run (3.3). 5. Relevant Semantic Tree construction (Addition of concepts parents to the vectors. For the first (3.1) and second (3.2) runs only Domain resource is used; for the third (3.3) run all the resources are used). 6. Selection of the correct senses (the first and the second runs use the same way to do the selection; the third run is different. We make an exception: For the verb “be” we select the sense with the higher frequency according to Freeling. 3.1 Relevant Semantic Tree With this proposal we measure how much a concept is correlated to the sentence, similar to Reuters Vector (Magnini et al., 2002), but with a different equation. This proposal has a partial similarity with the Conceptual Density (Agirre and Rigau, 1996) and DRelevant (V´azquez et al., 2004) to get the concepts from a hierarchy that they associate with the sentence. In order to determine the Association Ratio (RA) of a domain in relation to the sentence, the Equation 1 is used. n RA(D, f) = RA(D, fz) (1) z=1 where: P(D, w) RA(D, w) = P(D, w) � lo�2 P (D) (2) f: is a set of words w. fz: is a i-th word of the phrase f. P(D, w): is joint probability distribution. P(D): is marginal probability. From now, vectors are created </context>
<context position="8435" citStr="Magnini et al., 2002" startWordPosition="1361" endWordPosition="1364">s the Domain tree associated to the phrase. After the Relevant Semantic Tree is obtained, the Domain Factotum is eliminated from the tree. Due to the large amount of WordNet synsets, Figure 2: Relevant semantic tree that do not belong to a specific domain, but rather they can appear in almost all of them, the Factotum domain has been created. It basically includes two types of synsets: Generic synsets, which are hard to classify in a particular domain; and Stop Senses synsets which appear frequently in different contexts, such as numbers, week days, colors, etc. (Magnini and Cavaglia, 2000), (Magnini et al., 2002). Words that contain this synsets are frequently in the phrases, therefore the senses associated to this domain are not selected. After processing the patterns that characterize the sentence, the following stage is to determine the correct senses, so that the next steps ensue: 1. Senses that do not coincide with the grammatical category of Freeling are removed. 429 2. For each word to disambiguate all candidate senses are obtained. Of each sense the relevant vector are obtained using the Equation 4, and according to the previous Equation 3 parent concepts are added. P(D, s) RA(D, s) = P(D, s) </context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo, and Alfio Gliozzo. 2002. Comparing ontology-based and corpus-based domain annotations in wordnet. In Proceedings of the First International WordNet Conference, pages 146–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Niles</author>
<author>Adam Pease</author>
</authors>
<title>Towards a standard upper ontology.</title>
<date>2001</date>
<booktitle>In FOIS,</booktitle>
<pages>2--9</pages>
<contexts>
<context position="1429" citStr="Niles and Pease, 2001" startWordPosition="201" endWordPosition="204"> Introduction Ambiguity is the task of building up multiple alternative linguistic structures for a single input (Kozareva et al., 2007). Word Sense Disambiguation (WSD) is a key enablingtechnology that automatically chooses the intended sense of a word in context. In this task, one of the most used lexical data base is WordNet (WN) (Fellbaum, 1998). WN is an online lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. Due to the great popularity of WN in Natural Language Processing (NLP), several authors (Magnini and Cavaglia, 2000), (Niles and Pease, 2001), (Niles and Pease, 2003), (Valitutti, 2004) have proposed to incorporate to the semantic net of WN, some taxonomies that characterize, in one or several concepts, the senses of each word. In spite of the fact that there have been developed a lot of WordNet’s mappings, there isn’t one unique resource to integrate all of them in a single system approach. To solve this need we have developed a resource that joins WN1, the SUMO Ontology2, WordNet Domains3 and WordNet Affect4. Our purpose is to test the advantages of having all the resources together for the resolution of the WSD task. The rest of</context>
</contexts>
<marker>Niles, Pease, 2001</marker>
<rawString>Ian Niles and Adam Pease. 2001. Towards a standard upper ontology. In FOIS, pages 2–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Niles</author>
<author>Adam Pease</author>
</authors>
<title>Linking lexicons and ontologies: Mapping wordnet to the suggested upper merged ontology.</title>
<date>2003</date>
<booktitle>In IKE,</booktitle>
<pages>412--416</pages>
<contexts>
<context position="1454" citStr="Niles and Pease, 2003" startWordPosition="205" endWordPosition="208">s the task of building up multiple alternative linguistic structures for a single input (Kozareva et al., 2007). Word Sense Disambiguation (WSD) is a key enablingtechnology that automatically chooses the intended sense of a word in context. In this task, one of the most used lexical data base is WordNet (WN) (Fellbaum, 1998). WN is an online lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. Due to the great popularity of WN in Natural Language Processing (NLP), several authors (Magnini and Cavaglia, 2000), (Niles and Pease, 2001), (Niles and Pease, 2003), (Valitutti, 2004) have proposed to incorporate to the semantic net of WN, some taxonomies that characterize, in one or several concepts, the senses of each word. In spite of the fact that there have been developed a lot of WordNet’s mappings, there isn’t one unique resource to integrate all of them in a single system approach. To solve this need we have developed a resource that joins WN1, the SUMO Ontology2, WordNet Domains3 and WordNet Affect4. Our purpose is to test the advantages of having all the resources together for the resolution of the WSD task. The rest of the paper is organized a</context>
</contexts>
<marker>Niles, Pease, 2003</marker>
<rawString>Ian Niles and Adam Pease. 2003. Linking lexicons and ontologies: Mapping wordnet to the suggested upper merged ontology. In IKE, pages 412–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ro Valitutti</author>
</authors>
<title>Wordnet-affect: an affective extension of wordnet.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1083--1086</pages>
<contexts>
<context position="1473" citStr="Valitutti, 2004" startWordPosition="209" endWordPosition="210"> multiple alternative linguistic structures for a single input (Kozareva et al., 2007). Word Sense Disambiguation (WSD) is a key enablingtechnology that automatically chooses the intended sense of a word in context. In this task, one of the most used lexical data base is WordNet (WN) (Fellbaum, 1998). WN is an online lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. Due to the great popularity of WN in Natural Language Processing (NLP), several authors (Magnini and Cavaglia, 2000), (Niles and Pease, 2001), (Niles and Pease, 2003), (Valitutti, 2004) have proposed to incorporate to the semantic net of WN, some taxonomies that characterize, in one or several concepts, the senses of each word. In spite of the fact that there have been developed a lot of WordNet’s mappings, there isn’t one unique resource to integrate all of them in a single system approach. To solve this need we have developed a resource that joins WN1, the SUMO Ontology2, WordNet Domains3 and WordNet Affect4. Our purpose is to test the advantages of having all the resources together for the resolution of the WSD task. The rest of the paper is organized as follows. In Secti</context>
</contexts>
<marker>Valitutti, 2004</marker>
<rawString>Ro Valitutti. 2004. Wordnet-affect: an affective extension of wordnet. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pages 1083–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonia V´azquez</author>
<author>Andr´es Montoyo</author>
<author>German Rigau</author>
</authors>
<title>Using relevant domains resource for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In IC-AI,</booktitle>
<pages>784--789</pages>
<marker>V´azquez, Montoyo, Rigau, 2004</marker>
<rawString>Sonia V´azquez, Andr´es Montoyo, and German Rigau. 2004. Using relevant domains resource for word sense disambiguation. In IC-AI, pages 784–789.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>