<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.9986215">
Improving Name Tagging by
Reference Resolution and Relation Detection
</title>
<author confidence="0.999421">
Heng Ji Ralph Grishman
</author>
<affiliation confidence="0.999924">
Department of Computer Science
</affiliation>
<address confidence="0.765132">
New York University
New York, NY, 10003, USA
</address>
<email confidence="0.999268">
hengji@cs.nyu.edu grishman@cs.nyu.edu
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998021875">
Information extraction systems incorpo-
rate multiple stages of linguistic analysis.
Although errors are typically compounded
from stage to stage, it is possible to re-
duce the errors in one stage by harnessing
the results of the other stages. We dem-
onstrate this by using the results of
coreference analysis and relation extrac-
tion to reduce the errors produced by a
Chinese name tagger. We use an N-best
approach to generate multiple hypotheses
and have them re-ranked by subsequent
stages of processing. We obtained
thereby a reduction of 24% in spurious
and incorrect name tags, and a reduction
of 14% in missed tags.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947693877551">
Systems which extract relations or events from a
document typically perform a number of types of
linguistic analysis in preparation for information
extraction. These include name identification and
classification, parsing (or partial parsing), semantic
classification of noun phrases, and coreference
analysis. These tasks are reflected in the evalua-
tion tasks introduced for MUC-6 (named entity,
coreference, template element) and MUC-7 (tem-
plate relation).
In most extraction systems, these stages of
analysis are arranged sequentially, with each stage
using the results of prior stages and generating a
single analysis that gets enriched by each stage.
This provides a simple modular organization for
the extraction system.
Unfortunately, each stage also introduces a cer-
tain level of error into the analysis. Furthermore,
these errors are compounded – for example, errors
in name recognition may lead to errors in parsing.
The net result is that the final output (relations or
events) may be quite inaccurate.
This paper considers how interactions between
the stages can be exploited to reduce the error rate.
For example, the results of coreference analysis or
relation identification may be helpful in name clas-
sification, and the results of relation or event ex-
traction may be helpful in coreference.
Such interactions are not easily exploited in a
simple sequential model ... if name classification
is performed at the beginning of the pipeline, it
cannot make use of the results of subsequent stages.
It may even be difficult to use this information im-
plicitly, by using features which are also used in
later stages, because the representation used in the
initial stages is too limited.
To address these limitations, some recent sys-
tems have used more parallel designs, in which a
single classifier (incorporating a wide range of fea-
tures) encompasses what were previously several
separate stages (Kambhatla, 2004; Zelenko et al.,
2004). This can reduce the compounding of errors
of the sequential design. However, it leads to a
very large feature space and makes it difficult to
select linguistically appropriate features for par-
ticular analysis tasks. Furthermore, because these
decisions are being made in parallel, it becomes
much harder to express interactions between the
levels of analysis based on linguistic intuitions.
</bodyText>
<page confidence="0.99249">
411
</page>
<note confidence="0.9916375">
Proceedings of the 43rd Annual Meeting of the ACL, pages 411–418,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999797">
In order to capture these interactions more ex-
plicitly, we have employed a sequential design in
which multiple hypotheses are forwarded from
each stage to the next, with hypotheses being rer-
anked and pruned using the information from later
stages. We shall apply this design to show how
named entity classification can be improved by
‘feedback’ from coreference analysis and relation
extraction. We shall show that this approach can
capture these interactions in a natural and efficient
manner, yielding a substantial improvement in
name identification and classification.
</bodyText>
<sectionHeader confidence="0.996093" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.99959025">
A wide variety of trainable models have been ap-
plied to the name tagging task, including HMMs
(Bikel et al., 1997), maximum entropy models
(Borthwick, 1999), support vector machines
(SVMs), and conditional random fields. People
have spent considerable effort in engineering ap-
propriate features to improve performance; most of
these involve internal name structure or the imme-
diate local context of the name.
Some other named entity systems have explored
global information for name tagging. (Borthwick,
1999) made a second tagging pass which uses in-
formation on token sequences tagged in the first
pass; (Chieu and Ng, 2002) used as features infor-
mation about features assigned to other instances
of the same token.
Recently, in (Ji and Grishman, 2004) we pro-
posed a name tagging method which applied an
SVM based on coreference information to filter the
names with low confidence, and used coreference
rules to correct and recover some names. One limi-
tation of this method is that in the process of dis-
carding many incorrect names, it also discarded
some correct names. We attempted to recover
some of these names by heuristic rules which are
quite language specific. In addition, this single-
hypothesis method placed an upper bound on recall.
Traditional statistical name tagging methods
have generated a single name hypothesis. BBN
proposed the N-Best algorithm for speech recogni-
tion in (Chow and Schwartz, 1989). Since then N-
Best methods have been widely used by other re-
searchers (Collins, 2002; Zhai et al., 2004).
In this paper, we tried to combine the advan-
tages of the prior work, and incorporate broader
knowledge into a more general re-ranking model.
</bodyText>
<sectionHeader confidence="0.813558" genericHeader="method">
3 Task and Terminology
</sectionHeader>
<bodyText confidence="0.852671">
Our experiments were conducted in the context of
the ACE Information Extraction evaluations, and
we will use the terminology of these evaluations:
entity: an object or a set of objects in one of the
semantic categories of interest
</bodyText>
<listItem confidence="0.837904428571429">
mention: a reference to an entity (typically, a noun
phrase)
name mention: a reference by name to an entity
nominal mention: a reference by a common noun
or noun phrase to an entity
relation: one of a specified set of relationships be-
tween a pair of entities
</listItem>
<bodyText confidence="0.9999080625">
The 2004 ACE evaluation had 7 types of entities,
of which the most common were PER (persons),
ORG (organizations), and GPE (‘geo-political enti-
ties’ – locations which are also political units, such
as countries, counties, and cities). There were 7
types of relations, with 23 subtypes. Examples of
these relations are “the CEO of Microsoft” (an em-
ploy-exec relation), “Fred’s wife” (a family rela-
tion), and “a military base in Germany” (a located
relation).
In this paper we look at the problem of identify-
ing name mentions in Chinese text and classifying
them as persons, organizations, or GPEs. Because
Chinese has neither capitalization nor overt word
boundaries, it poses particular problems for name
identification.
</bodyText>
<sectionHeader confidence="0.998323" genericHeader="method">
4 Baseline System
</sectionHeader>
<subsectionHeader confidence="0.999201">
4.1 Baseline Name Tagger
</subsectionHeader>
<bodyText confidence="0.999909">
Our baseline name tagger consists of a HMM tag-
ger augmented with a set of post-processing rules.
The HMM tagger generally follows the Nymble
model (Bikel et al, 1997), but with multiple hy-
potheses as output and a larger number of states
(12) to handle name prefixes and suffixes, and
transliterated foreign names separately. It operates
on the output of a word segmenter from Tsinghua
University.
Within each of the name class states, a statistical
bigram model is employed, with the usual one-
word-per-state emission. The various probabilities
involve word co-occurrence, word features, and
class probabilities. Then it uses A* search decod-
ing to generate multiple hypotheses. Since these
probabilities are estimated based on observations
</bodyText>
<page confidence="0.997627">
412
</page>
<bodyText confidence="0.999979857142857">
seen in a corpus, “back-off models” are used to
reflect the strength of support for a given statistic,
as for the Nymble system.
We also add post-processing rules to correct
some omissions and systematic errors using name
lists (for example, a list of all Chinese last names;
lists of organization and location suffixes) and par-
ticular contextual patterns (for example, verbs oc-
curring with people’s names). They also deal with
abbreviations and nested organization names.
The ITMM tagger also computes the margin –
the difference between the log probabilities of the
top two hypotheses. This is used as a rough meas-
ure of confidence in the top hypothesis (see sec-
tions 5.3 and 6.2, below).
The name tagger used for these experiments
identifies the three main ACE entity types: Person
(PER), Organization (ORG), and GPE (names of
the other ACE types are identified by a separate
component of our system, not involved in the ex-
periments reported here).
</bodyText>
<subsectionHeader confidence="0.944678">
4.2 Nominal Mention Tagger
</subsectionHeader>
<bodyText confidence="0.99997125">
Our nominal mention tagger (noun group recog-
nizer) is a maximum entropy tagger trained on the
Chinese TreeBank from the University of Pennsyl-
vania, supplemented by list matching.
</bodyText>
<subsectionHeader confidence="0.999724">
4.3 Reference Resolver
</subsectionHeader>
<bodyText confidence="0.999997285714286">
Our baseline reference resolver goes through two
successive stages: first, coreference rules will iden-
tify some high-confidence positive and negative
mention pairs, in training data and test data; then
the remaining samples will be used as input of a
maximum entropy tagger. The features used in this
tagger involve distance, string matching, lexical
information, position, semantics, etc. We separate
the task into different classifiers for different men-
tion types (name / noun / pronoun). Then we in-
corporate the results from the relation tagger to
adjust the probabilities from the classifiers. Finally
we apply a clustering algorithm to combine them
into entities (sets of coreferring mentions).
</bodyText>
<subsectionHeader confidence="0.999087">
4.4 Relation Tagger
</subsectionHeader>
<bodyText confidence="0.948974148148148">
The relation tagger uses a k-nearest-neighbor algo-
rithm. For both training and test, we consider all
pairs of entity mentions where there is at most one
other mention between the heads of the two men-
tions of interest1. Each training / test example con-
sists of the pair of mentions and the sequence of
intervening words. Associated with each training
example is either one of the ACE relation types or
no relation at all. We defined a distance metric be-
tween two examples based on
❑ whether the heads of the mentions match
❑ whether the ACE types of the heads of the mentions
match (for example, both are people or both are or-
ganizations)
❑ whether the intervening words match
To tag a test example, we find the k nearest
training examples (where k = 3) and use the dis-
tance to weight each neighbor, then select the most
common class in the weighted neighbor set.
To provide a crude measure of the confidence of
our relation tagger, we define two thresholds, Dnear
and Dfar. If the average distance d to the nearest
neighbors d &lt; Dnear, we consider this a definite re-
lation. If Dnear &lt; d &lt; Dfar, we consider this a possi-
ble relation. If d &gt; Dfar, the tagger assumes that no
relation exists (regardless of the class of the nearest
neighbor).
</bodyText>
<sectionHeader confidence="0.957404" genericHeader="method">
5 Information from Coreference and Re-
lations
</sectionHeader>
<bodyText confidence="0.9949152">
Our system is processing a document consisting of
multiple sentences. For each sentence, the name
recognizer generates multiple hypotheses, each of
which is an NE tagging of the entire sentence. The
names in the hypothesis, plus the nouns in the
categories of interest constitute the mention set for
that hypothesis. Coreference resolution links these
mentions, assigning each to an entity. In symbols:
Si is the i-th sentence in the document.
Hi is the hypotheses set for Si
hij is the j-th hypothesis in Si
Mij is the mention set for hij
mijk is the k-th mention in Mij
eijk is the entity which mijk belongs to according to
the current reference resolution results
</bodyText>
<subsectionHeader confidence="0.992057">
5.1 Coreference Features
</subsectionHeader>
<bodyText confidence="0.999058">
For each mention we compute seven quantities
based on the results of name tagging and reference
resolution:
</bodyText>
<footnote confidence="0.539844333333333">
1 This constraint is relaxed for parallel structures such as “mention1, mention2,
[and] mention3....”; in such cases there can be more than one intervening men-
tion.
</footnote>
<page confidence="0.99768">
413
</page>
<bodyText confidence="0.894400346153846">
CorefNumijk is the number of mentions in eijk
WeightSumijk is the sum of all the link weights be-
tween mijk and other mentions in eijk , 0.8 for
name-name coreference; 0.5 for apposition;
0.3 for other name-nominal coreference
FirstMentionijk is 1 if mijk is the first name mention
in the entity; otherwise 0
Headijk is 1 if mijk includes the head word of name;
otherwise 0
Withoutidiomijk is 1 if mijk is not part of an idiom;
otherwise 0
PERContextijk is the number of PER context words
around a PER name such as a title or an ac-
tion verb involving a PER
ORGSuffixijk is 1 if ORGmijk includes a suffix word;
otherwise 0
The first three capture evidence of the correct-
ness of a name provided by reference resolution;
for example, a name which is coreferenced with
more other mentions is more likely to be correct.
The last four capture local or name-internal evi-
dence; for instance, that an organization name in-
cludes an explicit, organization-indicating suffix.
We then compute, for each of these seven quan-
tities, the sum over all mentions k in a sentence,
obtaining values for CorefNumij, WeightSumij, etc.:
</bodyText>
<equation confidence="0.993033333333333">
CorefNumij = ∑ etc.
CorefNum ijk
k
</equation>
<bodyText confidence="0.99979875">
Finally, we determine, for a given sentence and
hypothesis, for each of these seven quantities,
whether this quantity achieves the maximum of its
values for this hypothesis:
</bodyText>
<equation confidence="0.9171155">
BestCorefNumij ≡
CorefNumij = maxq CorefNumiq etc.
</equation>
<bodyText confidence="0.999954">
We will use these properties of the hypothesis as
features in assessing the quality of a hypothesis.
</bodyText>
<subsectionHeader confidence="0.997183">
5.2 Relation Word Clusters
</subsectionHeader>
<bodyText confidence="0.999760318181818">
In addition to using relation information for
reranking name hypotheses, we used the relation
training corpus to build word clusters which could
more directly improve name tagging. Name tag-
gers rely heavily on words in the immediate con-
text to identify and classify names; for example,
specific job titles, occupations, or family relations
can be used to identify people names. Such words
are learned individually from the name tagger’s
training corpus. If we can provide the name tagger
with clusters of related words, the tagger will be
able to generalize from the examples in the training
corpus to other words in the cluster.
The set of ACE relations includes several in-
volving employment, social, and family relations.
We gathered the words appearing as an argument
of one of these relations in the training corpus,
eliminated low-frequency terms and manually ed-
ited the ten resulting clusters to remove inappro-
priate terms. These were then combined with lists
(of titles, organization name suffixes, location suf-
fixes) used in the baseline tagger.
</bodyText>
<subsectionHeader confidence="0.990611">
5.3 Relation Features
</subsectionHeader>
<bodyText confidence="0.999879181818182">
Because the performance of our relation tagger
is not as good as our coreference resolver, we have
used the results of relation detection in a relatively
simple way to enhance name detection. The basic
intuition is that a name which has been correctly
identified is more likely to participate in a relation
than one which has been erroneously identified.
For a given range of margins (from the HMM),
the probability that a name in the first hypothesis is
correct is shown in the following table, for names
participating and not participating in a relation:
</bodyText>
<table confidence="0.99993325">
Margin In Relation(%) Not in Relation(%)
&lt;4 90.7 55.3
&lt;3 89.0 50.1
&lt;2 86.9 42.2
&lt;1.5 81.3 28.9
&lt;1.2 78.8 23.1
&lt;1 75.7 19.0
&lt;0.5 66.5 14.3
</table>
<tableCaption confidence="0.8737845">
Table 1 Probability of a name being correct
Table 1 confirms that names participating in re-
</tableCaption>
<bodyText confidence="0.989537272727273">
lations are much more likely to be correct than
names that do not participate in relations. We also
see, not surprisingly, that these probabilities are
strongly affected by the HMM hypothesis margin
(the difference in log probabilities) between the
first hypothesis and the second hypothesis. So it is
natural to use participation in a relation (coupled
with a margin value) as a valuable feature for re-
ranking name hypotheses.
Let mijk be the k-th name mention for hypothe-
sis hij of sentence; then we define:
</bodyText>
<page confidence="0.979096">
414
</page>
<bodyText confidence="0.803650333333333">
Inrelationijk = 1 if mijk is in a definite relation
= 0 if mijk is in a possible relation
= -1 if mijk is not in a relation
</bodyText>
<equation confidence="0.98873575">
Inrelationij = ∑ Inrelation
ijk
k
Mostrelatedij ≡ (Inrelationij = max9 Inrelationi9 )
</equation>
<bodyText confidence="0.97750025">
Finally, to capture the interaction with the margin,
we let zi = the margin for sentence Si and divide
the range of values of zi into six intervals Mar1, ...
Mar6. And we define the hypothesis ranking in-
formation: FirstHypothesisij = 1 if j =1; otherwise 0.
We will use as features for ranking hij the con-
junction of Mostrelatedij, zi ∈ Marp (p = 1, ..., 6),
and FirstHypothesisij .
</bodyText>
<sectionHeader confidence="0.997911" genericHeader="method">
6 Using the Information from Corefer-
ence and Relations
</sectionHeader>
<subsectionHeader confidence="0.999219">
6.1 Word Clustering based on Relations
</subsectionHeader>
<bodyText confidence="0.999618909090909">
As we described in section 5.2, we can generate
word clusters based on relation information. If a
word is not part of a relation cluster, we consider it
an independent (1-word) cluster.
The Nymble name tagger (Bikel et al., 1999) re-
lies on a multi-level linear interpolation model for
backoff. We extended this model by adding a level
from word to cluster, so as to estimate more reli-
able probabilities for words in these clusters. Table
2 shows the extended backoff model for each of
the three probabilities used by Nymble.
</bodyText>
<table confidence="0.863375285714286">
Transition First-Word Non-First-Word
Probability Emission Emission
Probability Probability
P(NC2|NC1, P(&lt;w2,f2&gt; |P(&lt;w2,f2&gt;|
&lt;w1, f1&gt;) NC1, NC2) &lt;w1,f1&gt;, NC2)
P(&lt;Cluster2,f2&gt; |P(&lt;Cluster2,f2&gt;|
NC1, NC2) &lt;w1,f1&gt;, NC2)
P(NC2|NC1, P(&lt;Cluster2,f2&gt; |P(&lt;Cluster2,f2&gt;|
&lt;Cluster1, &lt;+begin+, other&gt;, &lt;Cluster1,f1&gt;,
f1&gt;) NC2) NC2)
P(NC2|NC1) P(&lt;Cluster2, f2&gt;|NC2)
P(NC2) P(Cluster2|NC2) * P(f2|NC2)
1/#(name 1/#(cluster) * 1/#(word features)
classes)
</table>
<sectionHeader confidence="0.776665" genericHeader="method">
Table2 Extended Backoff Model
</sectionHeader>
<subsectionHeader confidence="0.999882">
6.2 Pre-pruning by Margin
</subsectionHeader>
<bodyText confidence="0.999989090909091">
The HMM tagger produces the N best hypotheses
for each sentence.2 In order to decide when we
need to rely on global (coreference and relation)
information for name tagging, we want to have
some assessment of the confidence that the name
tagger has in the first hypothesis. In this paper, we
use the margin for this purpose. A large margin
indicates greater confidence that the first hypothe-
sis is correct.3 So if the margin of a sentence is
above a threshold, we select the first hypothesis,
dropping the others and by-passing the reranking.
</bodyText>
<subsectionHeader confidence="0.99972">
6.3 Re-ranking based on Coreference
</subsectionHeader>
<bodyText confidence="0.965127166666667">
We described in section 5.1, above, the coreference
features which will be used for reranking the hy-
potheses after pre-pruning. A maximum entropy
model for re-ranking these hypotheses is then
trained and applied as follows:
Training
</bodyText>
<listItem confidence="0.686397">
1. Use K-fold cross-validation to generate multi-
</listItem>
<bodyText confidence="0.760820222222222">
ple name tagging hypotheses for each docu-
ment in the training data Dtrain (in each of the K
iterations, we use K-1 subsets to train the
HMM and then generate hypotheses from the
Kth subset).
2. For each document d in Dtrain, where d includes
n sentences S1...Sn
For i = 1...n, let m = the number of hy-
potheses for Si
</bodyText>
<listItem confidence="0.998917214285714">
(1) Pre-prune the candidate hypotheses us-
ing the HMM margin
(2) For each hypothesis hij, j = 1...m
(a) Compare hij with the key, set the
prediction Valueij “Best” or “Not
Best”
(b) Run the Coreference Resolver on
hij and the best hypothesis for each
of the other sentences, generate
entity results for each candidate
name in hij
(c) Generate a coreference feature vec-
tor Vij for hij
(d) Output Vij and Valueij
</listItem>
<footnote confidence="0.925752">
2 We set different N = 5, 10, 20 or 30 for different margin ranges, by cross-
validation checking the training data about the ranking position of the best
hypothesis for each sentence. With this N, optimal reranking (selecting the best
hypothesis among the N best) would yield Precision = 96.9 Recall = 94.5 F =
95.7 on our test corpus.
3 Similar methods based on HMM margins were used by (Scheffer et al., 2001).
</footnote>
<page confidence="0.998228">
415
</page>
<figure confidence="0.994059545454545">
3. Train Maxent Re-ranking system on all Vij and
Valueij
)
exp( probij
W=
ij
exp( )
prob iq
Z
q
Test
</figure>
<listItem confidence="0.999074291666667">
1. Run the baseline name tagger to generate mul-
tiple name tagging hypotheses for each docu-
ment in the test data Dtest
2. For each document d in Dtest, where d includes
n sentences S1...Sn
(1) Initialize: Dynamic input of coreference re-
solver H = {hi-best  |i = 1...n, hi-best is the
current best hypothesis for Si}
(2) For i = 1...n, assume m = the number of
hypotheses for Si
(a) Pre-prune the candidate hypotheses us-
ing the HMM margin
(b) For each hypothesis hij, j = 1...m
• hi-best = hij
• Run the Coreference Resolver on H,
generate entity results for each name
candidate in hij
• Generate a coreference feature vec-
tor Vij for hij
• Run Maxent Re-ranking system on
Vij, produce Probij of “Best” value
(c) hi-best = the hypothesis with highest
Probij of “Best” value, update H and
output hi-best
</listItem>
<subsectionHeader confidence="0.999088">
6.4 Re-ranking based on Relations
</subsectionHeader>
<bodyText confidence="0.994189782608696">
From the above first-stage re-ranking by corefer-
ence, for each hypothesis we got the probability of
its being the best one. By using these results and
relation information we proceed to a second-stage
re-ranking. As we described in section 5.3, the in-
formation of “in relation or not” can be used to-
gether with margin as another important measure
of confidence.
In addition, we apply the mechanism of weighted
voting among hypotheses (Zhai et al., 2004) as an
additional feature in this second-stage re-ranking.
This approach allows all hypotheses to vote on a
possible name output. A recognized name is con-
sidered correct only when it occurs in more than 30
percent of the hypotheses (weighted by their prob-
ability).
In our experiments we use the probability pro-
duced by the HMM, probij , for hypothesis hij . We
normalize this probability weight as:
For each name mention mijk in hi j, we define:
Occurq (mji k) = 1 if mijk occurs in hq
= 0 otherwise
Then we count its voting value as follows:
</bodyText>
<equation confidence="0.977507125">
Votingijk is 1 if Wiq Occurq mijk
Z x ( ) &gt;0.3;
q
otherwise 0.
The voting value of hij is:
Votingij = Z
Votingijk
k
</equation>
<bodyText confidence="0.991586666666667">
Finally we define the following voting feature:
BestVotingij ≡ (Votingij = maxq Votingiq )
This feature is used, together with the features
described at the end of section 5.3 and the prob-
ability score from the first stage, for the second-
stage maxent re-ranking model.
One appeal of the above two re-ranking algo-
rithms is its flexibility in incorporating features
into a learning model: essentially any coreference
or relation features which might be useful in dis-
criminating good from bad structures can be in-
cluded.
</bodyText>
<sectionHeader confidence="0.928202" genericHeader="method">
7 System Pipeline
</sectionHeader>
<bodyText confidence="0.999611">
Combining all the methods presented above, the
flow of our final system is shown in figure 1.
</bodyText>
<sectionHeader confidence="0.993976" genericHeader="evaluation">
8 Evaluation Results
</sectionHeader>
<subsectionHeader confidence="0.998013">
8.1 Training and Test Data
</subsectionHeader>
<bodyText confidence="0.999986769230769">
We took 346 documents from the 2004 ACE train-
ing corpus and official test set, including both
broadcast news and newswire, as our blind test set.
To train our name tagger, we used the Beijing Uni-
versity Insititute of Computational Linguistics cor-
pus – 2978 documents from the People’s Daily in
1998 – and 667 texts in the training corpus for the
2003 &amp; 2004 ACE evaluation. Our reference re-
solver is trained on these 667 ACE texts. The rela-
tion tagger is trained on 546 ACE 2004 texts, from
which we also extracted the relation clusters. The
test set included 11715 names: 3551 persons, 5100
GPEs and 3064 organizations.
</bodyText>
<page confidence="0.997245">
416
</page>
<figure confidence="0.5862795">
Post-processing
by heuristic rules
</figure>
<figureCaption confidence="0.944093">
Figure 1 System Flow
</figureCaption>
<subsectionHeader confidence="0.982915">
8.2 Overall Performance Comparison
</subsectionHeader>
<bodyText confidence="0.999373178571429">
Table 3 shows the performance of the baseline sys-
tem; Table 4 is the system with relation word clus-
ters; Table 5 is the system with both relation
clusters and re-ranking based on coreference fea-
tures; and Table 6 is the whole system with sec-
ond-stage re-ranking using relations.
The results indicate that relation word clusters
help to improve the precision and recall of most
name types. Although the overall gain in F-score is
small (0.7%), we believe further gain can be
achieved if the relation corpus is enlarged in the
future. The re-ranking using the coreference fea-
tures had the largest impact, improving precision
and recall consistently for all types. Compared to
our system in (Ji and Grishman, 2004), it helps to
distinguish the good and bad hypotheses without
any loss of recall. The second-stage re-ranking us-
ing the relation participation feature yielded a
small further gain in F score for each type, improv-
ing precision at a slight cost in recall.
The overall system achieves a 24.1% relative re-
duction on the spurious and incorrect tags, and
14.3% reduction in the missing rate over a state-of-
the-art baseline HMM trained on the same material.
Furthermore, it helps to disambiguate many name
type errors: the number of cases of type confusion
in name classification was reduced from 191 to
102.
</bodyText>
<table confidence="0.9997532">
Name Precision Recall F
PER 88.6 89.2 88.9
GPE 88.1 84.9 86.5
ORG 88.8 87.3 88.0
ALL 88.4 86.7 87.5
</table>
<tableCaption confidence="0.79975">
Table 3 Baseline Name Tagger
</tableCaption>
<table confidence="0.99995">
Name Precision Recall F
PER 89.4 90.1 89.7
GPE 88.9 85.8 89.4
ORG 88.7 87.4 88.0
ALL 89.0 87.4 88.2
</table>
<tableCaption confidence="0.993166">
Table 4 Baseline + Word Clustering by Relation
</tableCaption>
<table confidence="0.9999324">
Name Precision Recall F
PER 90.1 91.2 90.5
GPE 89.7 86.8 88.2
ORG 90.6 89.8 90.2
ALL 90.0 88.8 89.4
</table>
<tableCaption confidence="0.8280215">
Table 5 Baseline + Word Clustering by Relation +
Re-ranking by Coreference
</tableCaption>
<table confidence="0.999949">
Name Precision Recall F
PER 90.7 91.0 90.8
GPE 91.2 86.9 89.0
ORG 91.7 89.1 90.4
ALL 91.2 88.6 89.9
</table>
<tableCaption confidence="0.966442333333333">
Table 6 Baseline + Word Clustering by Relation +
Re-ranking by Coreference +
Re-ranking by Relation
</tableCaption>
<bodyText confidence="0.900805533333333">
In order to check how robust these methods are,
we conducted significance testing (sign test) on the
346 documents. We split them into 5 folders, 70
documents in each of the first four folders and 66
in the fifth folder. We found that each enhance-
ment (word clusters, coreference reranking, rela-
tion reranking) produced an improvement in F
score for each folder, allowing us to reject the hy-
pothesis that these improvements were random at a
95% confidence level. The overall F-measure im-
provements (using all enhancements) for the 5
folders were: 2.3%, 1.6%, 2.1%, 3.5%, and 2.1%.
HMM Name Tagger, word
clustering based on rela-
tions, pruned by margin
</bodyText>
<figure confidence="0.998545111111111">
Multiple name
hypotheses
Coreference
Resolver
Relation
Tagger
Input
Maxent Re-ranking
by coreference
Maxent Re-ranking
by relation
Nominal
Mentions
Nominal
Mention
Tagger
Single name
hypothesis
</figure>
<page confidence="0.991944">
417
</page>
<sectionHeader confidence="0.996102" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999977921052632">
This paper explored methods for exploiting the
interaction of analysis components in an informa-
tion extraction system to reduce the error rate of
individual components. The ACE task hierarchy
provided a good opportunity to explore these inter-
actions, including the one presented here between
reference resolution/relation detection and name
tagging. We demonstrated its effectiveness for
Chinese name tagging, obtaining an absolute im-
provement of 2.4% in F-measure (a reduction of
19% in the (1 – F) error rate). These methods are
quite low-cost because we don’t need any extra
resources or components compared to the baseline
information extraction system.
Because no language-specific rules are involved
and no additional training resources are required,
we expect that the approach described here can be
straightforwardly applied to other languages. It
should also be possible to extend this re-ranking
framework to other levels of analysis in informa-
tion extraction –- for example, to use event detec-
tion to improve name tagging; to incorporate
subtype tagging results to improve name tagging;
and to combine name tagging, reference resolution
and relation detection to improve nominal mention
tagging. For Chinese (and other languages without
overt word segmentation) it could also be extended
to do character-based name tagging, keeping mul-
tiple segmentations among the N-Best hypotheses.
Also, as information extraction is extended to cap-
ture cross-document information, we should expect
further improvements in performance of the earlier
stages of analysis, including in particular name
identification.
For some levels of analysis, such as name tag-
ging, it will be natural to apply lattice techniques to
organize the multiple hypotheses, at some gain in
efficiency.
</bodyText>
<sectionHeader confidence="0.996549" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998745428571429">
This research was supported by the Defense Ad-
vanced Research Projects Agency under Grant
N66001-04-1-8920 from SPAWAR San Diego,
and by the National Science Foundation under
Grant 03-25657. This paper does not necessarily
reflect the position or the policy of the U.S. Gov-
ernment.
</bodyText>
<sectionHeader confidence="0.996344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99915945">
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder. Proc. Fifth
Conf. on Applied Natural Language Processing,
Washington, D.C.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. Disser-
tation, Dept. of Computer Science, New York
University.
Hai Leong Chieu and Hwee Tou Ng. 2002. Named En-
tity Recognition: A Maximum Entropy Approach Us-
ing Global Information. Proc.: 17th Int’l Conf. on
Computational Linguistics (COLING 2002), Taipei,
Taiwan.
Yen-Lu Chow and Richard Schwartz. 1989. The N-Best
Algorithm: An efficient Procedure for Finding Top N
Sentence Hypotheses. Proc. DARPA Speech and
Natural Language Workshop
Michael Collins. 2002. Ranking Algorithms for Named-
Entity Extraction: Boosting and the Voted Percep-
tron. Proc. ACL 2002
Heng Ji and Ralph Grishman. 2004. Applying Corefer-
ence to Improve Name Recognition. Proc. ACL 2004
Workshop on Reference Resolution and Its Applica-
tions, Barcelona, Spain
N. Kambhatla. 2004. Combining Lexical, Syntactic, and
Semantic Features with Maximum Entropy Models
for Extracting Relations. Proc. ACL 2004.
Tobias Scheffer, Christian Decomain, and Stefan
Wrobel. 2001. Active Hidden Markov Models for In-
formation Extraction. Proc. Int’l Symposium on In-
telligent Data Analysis (IDA-2001).
Dmitry Zelenko, Chinatsu Aone, and Jason Tibbets.
2004. Binary Integer Programming for Information
Extraction. ACE Evaluation Meeting, September
2004, Alexandria, VA.
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine
Carpuat, and Dekai Wu. 2004. Using N-best Lists for
Named Entity Recognition from Chinese Speech.
Proc. NAACL 2004 (Short Papers)
</reference>
<page confidence="0.992953">
418
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945554">
<title confidence="0.9990675">Improving Name Tagging by Reference Resolution and Relation Detection</title>
<author confidence="0.999922">Heng Ji Ralph Grishman</author>
<affiliation confidence="0.991848">Department of Computer Science New York University</affiliation>
<address confidence="0.999817">New York, NY, 10003, USA</address>
<email confidence="0.999784">hengji@cs.nyu.edugrishman@cs.nyu.edu</email>
<abstract confidence="0.997814352941176">Information extraction systems incorporate multiple stages of linguistic analysis. Although errors are typically compounded from stage to stage, it is possible to reduce the errors in one stage by harnessing the results of the other stages. We demonstrate this by using the results of coreference analysis and relation extraction to reduce the errors produced by a Chinese name tagger. We use an N-best approach to generate multiple hypotheses and have them re-ranked by subsequent stages of processing. We obtained thereby a reduction of 24% in spurious and incorrect name tags, and a reduction of 14% in missed tags.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Scott Miller</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: a highperformance Learning Name-finder.</title>
<date>1997</date>
<booktitle>Proc. Fifth Conf. on Applied Natural Language Processing,</booktitle>
<location>Washington, D.C.</location>
<contexts>
<context position="4029" citStr="Bikel et al., 1997" startWordPosition="620" endWordPosition="623">tial design in which multiple hypotheses are forwarded from each stage to the next, with hypotheses being reranked and pruned using the information from later stages. We shall apply this design to show how named entity classification can be improved by ‘feedback’ from coreference analysis and relation extraction. We shall show that this approach can capture these interactions in a natural and efficient manner, yielding a substantial improvement in name identification and classification. 2 Prior Work A wide variety of trainable models have been applied to the name tagging task, including HMMs (Bikel et al., 1997), maximum entropy models (Borthwick, 1999), support vector machines (SVMs), and conditional random fields. People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name. Some other named entity systems have explored global information for name tagging. (Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same toke</context>
<context position="7021" citStr="Bikel et al, 1997" startWordPosition="1108" endWordPosition="1111">s are “the CEO of Microsoft” (an employ-exec relation), “Fred’s wife” (a family relation), and “a military base in Germany” (a located relation). In this paper we look at the problem of identifying name mentions in Chinese text and classifying them as persons, organizations, or GPEs. Because Chinese has neither capitalization nor overt word boundaries, it poses particular problems for name identification. 4 Baseline System 4.1 Baseline Name Tagger Our baseline name tagger consists of a HMM tagger augmented with a set of post-processing rules. The HMM tagger generally follows the Nymble model (Bikel et al, 1997), but with multiple hypotheses as output and a larger number of states (12) to handle name prefixes and suffixes, and transliterated foreign names separately. It operates on the output of a word segmenter from Tsinghua University. Within each of the name class states, a statistical bigram model is employed, with the usual oneword-per-state emission. The various probabilities involve word co-occurrence, word features, and class probabilities. Then it uses A* search decoding to generate multiple hypotheses. Since these probabilities are estimated based on observations 412 seen in a corpus, “back</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>Daniel M. Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997. Nymble: a highperformance Learning Name-finder. Proc. Fifth Conf. on Applied Natural Language Processing, Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. Dissertation,</tech>
<institution>Dept. of Computer Science, New York University.</institution>
<contexts>
<context position="4071" citStr="Borthwick, 1999" startWordPosition="627" endWordPosition="628">forwarded from each stage to the next, with hypotheses being reranked and pruned using the information from later stages. We shall apply this design to show how named entity classification can be improved by ‘feedback’ from coreference analysis and relation extraction. We shall show that this approach can capture these interactions in a natural and efficient manner, yielding a substantial improvement in name identification and classification. 2 Prior Work A wide variety of trainable models have been applied to the name tagging task, including HMMs (Bikel et al., 1997), maximum entropy models (Borthwick, 1999), support vector machines (SVMs), and conditional random fields. People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name. Some other named entity systems have explored global information for name tagging. (Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token. Recently, in (Ji and Grishman, 2004) we</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. Dissertation, Dept. of Computer Science, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named Entity Recognition: A Maximum Entropy Approach Using Global Information.</title>
<date>2002</date>
<booktitle>Proc.: 17th Int’l Conf. on Computational Linguistics (COLING</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4540" citStr="Chieu and Ng, 2002" startWordPosition="697" endWordPosition="700"> variety of trainable models have been applied to the name tagging task, including HMMs (Bikel et al., 1997), maximum entropy models (Borthwick, 1999), support vector machines (SVMs), and conditional random fields. People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name. Some other named entity systems have explored global information for name tagging. (Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token. Recently, in (Ji and Grishman, 2004) we proposed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names. One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names. We attempted to recover some of these names by heuristic rules which are quite language specific. In addition, this singlehypothesis method placed an upp</context>
</contexts>
<marker>Chieu, Ng, 2002</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2002. Named Entity Recognition: A Maximum Entropy Approach Using Global Information. Proc.: 17th Int’l Conf. on Computational Linguistics (COLING 2002), Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yen-Lu Chow</author>
<author>Richard Schwartz</author>
</authors>
<title>The N-Best Algorithm: An efficient Procedure for Finding Top N Sentence Hypotheses.</title>
<date>1989</date>
<booktitle>Proc. DARPA Speech and Natural Language Workshop</booktitle>
<contexts>
<context position="5331" citStr="Chow and Schwartz, 1989" startWordPosition="826" endWordPosition="829">h applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names. One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names. We attempted to recover some of these names by heuristic rules which are quite language specific. In addition, this singlehypothesis method placed an upper bound on recall. Traditional statistical name tagging methods have generated a single name hypothesis. BBN proposed the N-Best algorithm for speech recognition in (Chow and Schwartz, 1989). Since then NBest methods have been widely used by other researchers (Collins, 2002; Zhai et al., 2004). In this paper, we tried to combine the advantages of the prior work, and incorporate broader knowledge into a more general re-ranking model. 3 Task and Terminology Our experiments were conducted in the context of the ACE Information Extraction evaluations, and we will use the terminology of these evaluations: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) name mention: a reference by name to a</context>
</contexts>
<marker>Chow, Schwartz, 1989</marker>
<rawString>Yen-Lu Chow and Richard Schwartz. 1989. The N-Best Algorithm: An efficient Procedure for Finding Top N Sentence Hypotheses. Proc. DARPA Speech and Natural Language Workshop</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking Algorithms for NamedEntity Extraction: Boosting and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>Proc. ACL</booktitle>
<contexts>
<context position="5415" citStr="Collins, 2002" startWordPosition="843" endWordPosition="844">used coreference rules to correct and recover some names. One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names. We attempted to recover some of these names by heuristic rules which are quite language specific. In addition, this singlehypothesis method placed an upper bound on recall. Traditional statistical name tagging methods have generated a single name hypothesis. BBN proposed the N-Best algorithm for speech recognition in (Chow and Schwartz, 1989). Since then NBest methods have been widely used by other researchers (Collins, 2002; Zhai et al., 2004). In this paper, we tried to combine the advantages of the prior work, and incorporate broader knowledge into a more general re-ranking model. 3 Task and Terminology Our experiments were conducted in the context of the ACE Information Extraction evaluations, and we will use the terminology of these evaluations: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) name mention: a reference by name to an entity nominal mention: a reference by a common noun or noun phrase to an entity r</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking Algorithms for NamedEntity Extraction: Boosting and the Voted Perceptron. Proc. ACL 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Applying Coreference to Improve Name Recognition.</title>
<date>2004</date>
<booktitle>Proc. ACL 2004 Workshop on Reference Resolution and Its Applications,</booktitle>
<location>Barcelona, Spain</location>
<contexts>
<context position="4668" citStr="Ji and Grishman, 2004" startWordPosition="718" endWordPosition="721"> models (Borthwick, 1999), support vector machines (SVMs), and conditional random fields. People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name. Some other named entity systems have explored global information for name tagging. (Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token. Recently, in (Ji and Grishman, 2004) we proposed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names. One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names. We attempted to recover some of these names by heuristic rules which are quite language specific. In addition, this singlehypothesis method placed an upper bound on recall. Traditional statistical name tagging methods have generated a single name hypothesis. BBN proposed the N-Bes</context>
<context position="23489" citStr="Ji and Grishman, 2004" startWordPosition="3896" endWordPosition="3899"> word clusters; Table 5 is the system with both relation clusters and re-ranking based on coreference features; and Table 6 is the whole system with second-stage re-ranking using relations. The results indicate that relation word clusters help to improve the precision and recall of most name types. Although the overall gain in F-score is small (0.7%), we believe further gain can be achieved if the relation corpus is enlarged in the future. The re-ranking using the coreference features had the largest impact, improving precision and recall consistently for all types. Compared to our system in (Ji and Grishman, 2004), it helps to distinguish the good and bad hypotheses without any loss of recall. The second-stage re-ranking using the relation participation feature yielded a small further gain in F score for each type, improving precision at a slight cost in recall. The overall system achieves a 24.1% relative reduction on the spurious and incorrect tags, and 14.3% reduction in the missing rate over a state-ofthe-art baseline HMM trained on the same material. Furthermore, it helps to disambiguate many name type errors: the number of cases of type confusion in name classification was reduced from 191 to 102</context>
</contexts>
<marker>Ji, Grishman, 2004</marker>
<rawString>Heng Ji and Ralph Grishman. 2004. Applying Coreference to Improve Name Recognition. Proc. ACL 2004 Workshop on Reference Resolution and Its Applications, Barcelona, Spain</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kambhatla</author>
</authors>
<title>Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Extracting Relations.</title>
<date>2004</date>
<booktitle>Proc. ACL</booktitle>
<contexts>
<context position="2772" citStr="Kambhatla, 2004" startWordPosition="427" endWordPosition="428"> Such interactions are not easily exploited in a simple sequential model ... if name classification is performed at the beginning of the pipeline, it cannot make use of the results of subsequent stages. It may even be difficult to use this information implicitly, by using features which are also used in later stages, because the representation used in the initial stages is too limited. To address these limitations, some recent systems have used more parallel designs, in which a single classifier (incorporating a wide range of features) encompasses what were previously several separate stages (Kambhatla, 2004; Zelenko et al., 2004). This can reduce the compounding of errors of the sequential design. However, it leads to a very large feature space and makes it difficult to select linguistically appropriate features for particular analysis tasks. Furthermore, because these decisions are being made in parallel, it becomes much harder to express interactions between the levels of analysis based on linguistic intuitions. 411 Proceedings of the 43rd Annual Meeting of the ACL, pages 411–418, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics In order to capture these interactions more</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>N. Kambhatla. 2004. Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Extracting Relations. Proc. ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Scheffer</author>
<author>Christian Decomain</author>
<author>Stefan Wrobel</author>
</authors>
<title>Active Hidden Markov Models for Information Extraction.</title>
<date>2001</date>
<booktitle>Proc. Int’l Symposium on Intelligent Data Analysis (IDA-2001).</booktitle>
<contexts>
<context position="19332" citStr="Scheffer et al., 2001" startWordPosition="3164" endWordPosition="3167">ce Resolver on hij and the best hypothesis for each of the other sentences, generate entity results for each candidate name in hij (c) Generate a coreference feature vector Vij for hij (d) Output Vij and Valueij 2 We set different N = 5, 10, 20 or 30 for different margin ranges, by crossvalidation checking the training data about the ranking position of the best hypothesis for each sentence. With this N, optimal reranking (selecting the best hypothesis among the N best) would yield Precision = 96.9 Recall = 94.5 F = 95.7 on our test corpus. 3 Similar methods based on HMM margins were used by (Scheffer et al., 2001). 415 3. Train Maxent Re-ranking system on all Vij and Valueij ) exp( probij W= ij exp( ) prob iq Z q Test 1. Run the baseline name tagger to generate multiple name tagging hypotheses for each document in the test data Dtest 2. For each document d in Dtest, where d includes n sentences S1...Sn (1) Initialize: Dynamic input of coreference resolver H = {hi-best |i = 1...n, hi-best is the current best hypothesis for Si} (2) For i = 1...n, assume m = the number of hypotheses for Si (a) Pre-prune the candidate hypotheses using the HMM margin (b) For each hypothesis hij, j = 1...m • hi-best = hij • </context>
</contexts>
<marker>Scheffer, Decomain, Wrobel, 2001</marker>
<rawString>Tobias Scheffer, Christian Decomain, and Stefan Wrobel. 2001. Active Hidden Markov Models for Information Extraction. Proc. Int’l Symposium on Intelligent Data Analysis (IDA-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Jason Tibbets</author>
</authors>
<title>Binary Integer Programming for Information Extraction.</title>
<date>2004</date>
<journal>ACE Evaluation Meeting,</journal>
<location>Alexandria, VA.</location>
<contexts>
<context position="2795" citStr="Zelenko et al., 2004" startWordPosition="429" endWordPosition="432">s are not easily exploited in a simple sequential model ... if name classification is performed at the beginning of the pipeline, it cannot make use of the results of subsequent stages. It may even be difficult to use this information implicitly, by using features which are also used in later stages, because the representation used in the initial stages is too limited. To address these limitations, some recent systems have used more parallel designs, in which a single classifier (incorporating a wide range of features) encompasses what were previously several separate stages (Kambhatla, 2004; Zelenko et al., 2004). This can reduce the compounding of errors of the sequential design. However, it leads to a very large feature space and makes it difficult to select linguistically appropriate features for particular analysis tasks. Furthermore, because these decisions are being made in parallel, it becomes much harder to express interactions between the levels of analysis based on linguistic intuitions. 411 Proceedings of the 43rd Annual Meeting of the ACL, pages 411–418, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics In order to capture these interactions more explicitly, we have em</context>
</contexts>
<marker>Zelenko, Aone, Tibbets, 2004</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Jason Tibbets. 2004. Binary Integer Programming for Information Extraction. ACE Evaluation Meeting, September 2004, Alexandria, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lufeng Zhai</author>
<author>Pascale Fung</author>
<author>Richard Schwartz</author>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Using N-best Lists for Named Entity Recognition from Chinese Speech.</title>
<date>2004</date>
<booktitle>Proc. NAACL</booktitle>
<tech>(Short Papers)</tech>
<contexts>
<context position="5435" citStr="Zhai et al., 2004" startWordPosition="845" endWordPosition="848">e rules to correct and recover some names. One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names. We attempted to recover some of these names by heuristic rules which are quite language specific. In addition, this singlehypothesis method placed an upper bound on recall. Traditional statistical name tagging methods have generated a single name hypothesis. BBN proposed the N-Best algorithm for speech recognition in (Chow and Schwartz, 1989). Since then NBest methods have been widely used by other researchers (Collins, 2002; Zhai et al., 2004). In this paper, we tried to combine the advantages of the prior work, and incorporate broader knowledge into a more general re-ranking model. 3 Task and Terminology Our experiments were conducted in the context of the ACE Information Extraction evaluations, and we will use the terminology of these evaluations: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) name mention: a reference by name to an entity nominal mention: a reference by a common noun or noun phrase to an entity relation: one of a sp</context>
<context position="20725" citStr="Zhai et al., 2004" startWordPosition="3415" endWordPosition="3418"> Vij, produce Probij of “Best” value (c) hi-best = the hypothesis with highest Probij of “Best” value, update H and output hi-best 6.4 Re-ranking based on Relations From the above first-stage re-ranking by coreference, for each hypothesis we got the probability of its being the best one. By using these results and relation information we proceed to a second-stage re-ranking. As we described in section 5.3, the information of “in relation or not” can be used together with margin as another important measure of confidence. In addition, we apply the mechanism of weighted voting among hypotheses (Zhai et al., 2004) as an additional feature in this second-stage re-ranking. This approach allows all hypotheses to vote on a possible name output. A recognized name is considered correct only when it occurs in more than 30 percent of the hypotheses (weighted by their probability). In our experiments we use the probability produced by the HMM, probij , for hypothesis hij . We normalize this probability weight as: For each name mention mijk in hi j, we define: Occurq (mji k) = 1 if mijk occurs in hq = 0 otherwise Then we count its voting value as follows: Votingijk is 1 if Wiq Occurq mijk Z x ( ) &gt;0.3; q otherwi</context>
</contexts>
<marker>Zhai, Fung, Schwartz, Carpuat, Wu, 2004</marker>
<rawString>Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine Carpuat, and Dekai Wu. 2004. Using N-best Lists for Named Entity Recognition from Chinese Speech. Proc. NAACL 2004 (Short Papers)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>