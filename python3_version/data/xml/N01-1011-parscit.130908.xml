<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001657">
<title confidence="0.999215">
A Decision Tree of Bigrams
is an Accurate Predictor of Word Sense
</title>
<author confidence="0.999545">
Ted Pedersen
</author>
<affiliation confidence="0.999651">
Department of Computer Science
University of Minnesota Duluth
</affiliation>
<address confidence="0.501438">
Duluth, MN 55812 USA
</address>
<email confidence="0.997148">
tpederse@d.umn.edu
</email>
<sectionHeader confidence="0.98002" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989222222222">
This paper presents a corpus-based approach to
word sense disambiguation where a decision tree as-
signs a sense to an ambiguous word based on the
bigrams that occur nearby. This approach is evalu-
ated using the sense-tagged corpora from the 1998
SENSEVAL word sense disambiguation exercise. It
is more accurate than the average results reported
for 30 of 36 words, and is more accurate than the
best results for 19 of 36 words.
</bodyText>
<sectionHeader confidence="0.995495" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.968298278481013">
Word sense disambiguation is the process of selecting
the most appropriate meaning for a word, based on
the context in which it occurs. For our purposes it is
assumed that the set of possible meanings, i.e., the
sense inventory, has already been determined. For
example, suppose bill has the following set of possi-
ble meanings: a piece of currency, pending legisla-
tion, or a bird jaw. When used in the context of The
Senate bill is under consideration, a human reader
immediately understands that bill is being used in
the legislative sense. However, a computer program
attempting to perform the same task faces a difficult
problem since it does not have the benefit of innate
common—sense or linguistic knowledge.
Rather than attempting to provide computer pro-
grams with real—world knowledge comparable to
that of humans, natural language processing has
turned to corpus—based methods. These approaches
use techniques from statistics and machine learn-
ing to induce models of language usage from large
samples of text. These models are trained to per-
form particular tasks, usually via supervised learn-
ing. This paper describes an approach where a deci-
sion tree is learned from some number of sentences
where each instance of an ambiguous word has been
manually annotated with a sense—tag that denotes
the most appropriate sense for that context.
Prior to learning, the sense—tagged corpus must be
converted into a more regular form suitable for auto-
matic processing. Each sense—tagged occurrence of
an ambiguous word is converted into a feature vec-
tor, where each feature represents some property of
the surrounding text that is considered to be relevant
to the disambiguation process. Given the flexibility
and complexity of human language, there is poten-
tially an infinite set of features that could be utilized.
However, in corpus—based approaches features usu-
ally consist of information that can be readily iden-
tified in the text, without relying on extensive exter-
nal knowledge sources. These typically include the
part—of—speech of surrounding words, the presence
of certain key words within some window of context,
and various syntactic properties of the sentence and
the ambiguous word.
The approach in this paper relies upon a feature
set made up of bigrams, two word sequences that
occur in a text. The context in which an ambiguous
word occurs is represented by some number of binary
features that indicate whether or not a particular
bigram has occurred within approximately 50 words
to the left or right of the word being disambiguated.
We take this approach since surface lexical fea-
tures like bigrams, collocations, and co—occurrences
often contribute a great deal to disambiguation ac-
curacy. It is not clear how much disambiguation ac-
curacy is improved through the use of features that
are identified by more complex pre—processing such
as part—of—speech tagging, parsing, or anaphora res-
olution. One of our objectives is to establish a clear
upper bounds on the accuracy of disambiguation us-
ing feature sets that do not impose substantial pre—
processing requirements.
This paper continues with a discussion of our
methods for identifying the bigrams that should be
included in the feature set for learning. Then the
decision tree learning algorithm is described, as are
some benchmark learning algorithms that are in-
cluded for purposes of comparison. The experimen-
tal data is discussed, and then the empirical results
are presented. We close with an analysis of our find-
ings and a discussion of related work.
2 Building a Feature Set of Bigrams
We have developed an approach to word sense dis-
ambiguation that represents text entirely in terms of
the occurrence of bigrams, which we define to be two
cat —cat totals
big n11= 10 n12= 20 n1+= 30
—big n21= 40 n22= 930 n2+= 970
totals n+1=50 n+2=950 n++=1000
</bodyText>
<figureCaption confidence="0.999887">
Figure 1: Representation of Bigram Counts
</figureCaption>
<bodyText confidence="0.9976863">
consecutive words that occur in a text. The distri-
butional characteristics of bigrams are fairly consis-
tent across corpora; a majority of them only occur
one time. Given the sparse and skewed nature of
this data, the statistical methods used to select in-
teresting bigrams must be carefully chosen. We ex-
plore two alternatives, the power divergence family
of goodness of fit statistics and the Dice Coefficient,
an information theoretic measure related to point-
wise Mutual Information.
Figure 1 summarizes the notation for word and
bigram counts used in this paper by way of a 2 x 2
contingency table. The value of n11 shows how many
times the bigram big cat occurs in the corpus. The
value of n12 shows how often bigrams occur where
big is the first word and cat is not the second. The
counts in n+1 and n1+ indicate how often words big
and cat occur as the first and second words of any
bigram in the corpus. The total number of bigrams
in the corpus is represented by n++.
</bodyText>
<subsectionHeader confidence="0.993081">
2.1 The Power Divergence Family
</subsectionHeader>
<bodyText confidence="0.999817744186047">
(Cressie and Read, 1984) introduce the power diver-
gence family of goodness of fit statistics. A number
of well known statistics belong to this family, includ-
ing the likelihood ratio statistic G2 and Pearson&apos;s X2
statistic.
These measure the divergence of the observed
(nij) and expected (mij) bigram counts, where mij
is estimated based on the assumption that the com-
ponent words in the bigram occur together strictly
by chance:
data distributions. However, (Cressie and Read,
1984) suggest that there are cases where Pearson&apos;s
statistic is more reliable than the likelihood ratio and
that one test should not always be preferred over
the other. In light of this, (Pedersen, 1996) presents
Fisher&apos;s exact test as an alternative since it does not
rely on the distributional assumptions that underly
both Pearson&apos;s test and the likelihood ratio.
Unfortunately it is usually not clear which test
is most appropriate for a particular sample of data.
We take the following approach, based on the obser-
vation that all tests should assign approximately the
same measure of statistical significance when the bi-
gram counts in the contingency table do not violate
any of the distributional assumptions that underly
the goodness of fit statistics. We perform tests us-
ing X2, G2, and Fisher&apos;s exact test for each bigram.
If the resulting measures of statistical significance
differ, then the distribution of the bigram counts is
causing at least one of the tests to become unreli-
able. When this occurs we rely upon the value from
Fisher&apos;s exact test since it makes fewer assumptions
about the underlying distribution of data.
For the experiments in this paper, we identified
the top 100 ranked bigrams that occur more than 5
times in the training corpus associated with a word.
There were no cases where rankings produced by
G2, X2, and Fisher&apos;s exact test disagreed, which is
not altogether surprising given that low frequency
bigrams were excluded. Since all of these statistics
produced the same rankings, hereafter we make no
distinction among them and simply refer to them
generically as the power divergence statistic.
</bodyText>
<subsectionHeader confidence="0.997153">
2.2 Dice Coefficient
</subsectionHeader>
<bodyText confidence="0.999914285714286">
The Dice Coefficient is a descriptive statistic that
provides a measure of association among two words
in a corpus. It is similar to pointwise Mutual Infor-
mation, a widely used measure that was first intro-
duced for identifying lexical relationships in (Church
and Hanks, 1990). Pointwise Mutual Information
can be defined as follows:
</bodyText>
<equation confidence="0.982803181818182">
n+1 * n1+
mij =
ni+ * n+j
n++
n11 * n++
MI(w1� w2) = �o�2
Given this value, G2 and X2 are calculated as:
G2=2Y�
i,j
(nij — mij)2
mij
</equation>
<bodyText confidence="0.998660428571428">
(Dunning, 1993) argues in favor of G2 over X2, es-
pecially when dealing with very sparse and skewed
where w1 and w2 represent the two words that make
up the bigram.
Pointwise Mutual Information quantifies how of-
ten two words occur together in a bigram (the nu-
merator) relative to how often they occur overall in
the corpus (the denominator). However, there is a
curious limitation to pointwise Mutual Information.
A bigram w1w2 that occurs n11 times in the corpus,
and whose component words w1 and w2 only occur
as a part of that bigram, will result in increasingly
strong measures of association as the value of n11
decreases. Thus, the maximum pointwise Mutual
</bodyText>
<equation confidence="0.648131">
nij
nij * log
mij
X2=Y�
i,j
</equation>
<bodyText confidence="0.999880375">
Information in a given corpus will be assigned to bi-
grams that occur one time, and whose component
words never occur outside that bigram. These are
usually not the bigrams that prove most useful for
disambiguation, yet they will dominate a ranked list
as determined by pointwise Mutual Information.
The Dice Coefficient overcomes this limitation,
and can be defined as follows:
</bodyText>
<equation confidence="0.999653">
Dice(w1i w2) =
n+1 + n1+
</equation>
<bodyText confidence="0.978357833333333">
When n11 = n1+ = n+1 the value of Dice(w1, w2)
will be 1 for all values n11. When the value of n11
is less than either of the marginal totals (the more
typical case) the rankings produced by the Dice Co-
efficient are similar to those of Mutual Information.
The relationship between pointwise Mutual Infor-
mation and the Dice Coefficient is also discussed in
(Smadja et al., 1996).
We have developed the Bigram Statistics Package
to produce ranked lists of bigrams using a range of
tests. This software is written in Perl and is freely
available from www.d.umn.edu/~tpederse.
</bodyText>
<sectionHeader confidence="0.975842" genericHeader="introduction">
3 Learning Decision Trees
</sectionHeader>
<bodyText confidence="0.999971931034483">
Decision trees are among the most widely used ma-
chine learning algorithms. They perform a general
to specific search of a feature space, adding the most
informative features to a tree structure as the search
proceeds. The objective is to select a minimal set of
features that efficiently partitions the feature space
into classes of observations and assemble them into
a tree. In our case, the observations are manually
sense—tagged examples of an ambiguous word in con-
text and the partitions correspond to the different
possible senses.
Each feature selected during the search process is
represented by a node in the learned decision tree.
Each node represents a choice point between a num-
ber of different possible values for a feature. Learn-
ing continues until all the training examples are ac-
counted for by the decision tree. In general, such
a tree will be overly specific to the training data
and not generalize well to new examples. Therefore
learning is followed by a pruning step where some
nodes are eliminated or reorganized to produce a
tree that can generalize to new circumstances.
Test instances are disambiguated by finding a path
through the learned decision tree from the root to a
leaf node that corresponds with the observed fea-
tures. An instance of an ambiguous word is dis-
ambiguated by passing it through a series of tests,
where each test asks if a particular bigram occurs in
the available window of context.
We also include three benchmark learning algo-
rithms in this study: the majority classifier, the de-
cision stump, and the Naive Bayesian classifier.
The majority classifier assigns the most common
sense in the training data to every instance in the
test data. A decision stump is a one node decision
tree(Holte, 1993) that is created by stopping the de-
cision tree learner after the single most informative
feature is added to the tree.
The Naive Bayesian classifier (Duda and Hart,
1973) is based on certain blanket assumptions about
the interactions among features in a corpus. There
is no search of the feature space performed to build
a representative model as is the case with decision
trees. Instead, all features are included in the classi-
fier and assumed to be relevant to the task at hand.
There is a further assumption that each feature is
conditionally independent of all other features, given
the sense of the ambiguous word. It is most often
used with a bag of words feature set, where every
word in the training sample is represented by a bi-
nary feature that indicates whether or not it occurs
in the window of context surrounding the ambiguous
word.
We use the Weka (Witten and Frank, 2000) imple-
mentations of the C4.5 decision tree learner (known
as J48), the decision stump, and the Naive Bayesian
classifier. Weka is written in Java and is freely avail-
able from www.cs.waikato.ac.nz/~ml.
</bodyText>
<sectionHeader confidence="0.995158" genericHeader="method">
4 Experimental Data
</sectionHeader>
<bodyText confidence="0.985253411764706">
Our empirical study utilizes the training and test
data from the 1998 SENSEVAL evaluation of word
sense disambiguation systems. Ten teams partic-
ipated in the supervised learning portion of this
event. Additional details about the exercise, in-
cluding the data and results referred to in this
paper, can be found at the SENSEVAL web site
(www.itri.bton.ac.uk/events/senseval/) and in (Kil-
garriff and Palmer, 2000).
We included all 36 tasks from SENSEVAL for
which training and test data were provided. Each
task requires that the occurrences of a particular
word in the test data be disambiguated based on
a model learned from the sense—tagged instances in
the training data. Some words were used in multiple
tasks as different parts of speech. For example, there
were two tasks associated with bet, one for its use as
a noun and the other as a verb. Thus, there are
36 tasks involving the disambiguation of 29 different
words.
The words and part of speech associated with each
task are shown in Table 1 in column 1. Note that
the parts of speech are encoded as n for noun, a
for adjective, v for verb, and p for words where the
part of speech was not provided. The number of
test and training instances for each task are shown
in columns 2 and 4. Each instance consists of the
sentence in which the ambiguous word occurs as well
2 * n11
as one or two surrounding sentences. In general the
total context available for each ambiguous word is
less than 100 surrounding words. The number of
distinct senses in the test data for each task is shown
in column 3.
</bodyText>
<sectionHeader confidence="0.991209" genericHeader="method">
5 Experimental Method
</sectionHeader>
<bodyText confidence="0.999982">
The following process is repeated for each task. Cap-
italization and punctuation are removed from the
training and test data. Two feature sets are selected
from the training data based on the top 100 ranked
bigrams according to the power divergence statistic
and the Dice Coefficient. The bigram must have oc-
curred 5 or more times to be included as a feature.
This step filters out a large number of possible bi-
grams and allows the decision tree learner to focus
on a small number of candidate bigrams that are
likely to be helpful in the disambiguation process.
The training and test data are converted to fea-
ture vectors where each feature represents the occur-
rence of one of the bigrams that belong in the feature
set. This representation of the training data is the
actual input to the learning algorithms. Decision
tree and decision stump learning is performed twice,
once using the feature set determined by the power
divergence statistic and again using the feature set
identified by the Dice Coefficient. The majority clas-
sifier simply determines the most frequent sense in
the training data and assigns that to all instances
in the test data. The Naive Bayesian classifier is
based on a feature set where every word that occurs
5 or more times in the training data is included as a
feature.
All of these learned models are used to disam-
biguate the test data. The test data is kept separate
until this stage. We employ a fine grained scoring
method, where a word is counted as correctly disam-
biguated only when the assigned sense tag exactly
matches the true sense tag. No partial credit is as-
signed for near misses.
</bodyText>
<sectionHeader confidence="0.998434" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999981203389831">
The accuracy attained by each of the learning algo-
rithms is shown in Table 1. Column 5 reports the
accuracy of the majority classifier, columns 6 and 7
show the best and average accuracy reported by the
10 participating SENSEVAL teams. The evaluation
at SENSEVAL was based on precision and recall, so
we converted those scores to accuracy by taking their
product. However, the best precision and recall may
have come from different teams, so the best accuracy
shown in column 6 may actually be higher than that
of any single participating SENSEVAL system. The
average accuracy in column 7 is the product of the
average precision and recall reported for the par-
ticipating SENSEVAL teams. Column 8 shows the
accuracy of the decision tree using the J48 learning
algorithm and the features identified by a power di-
vergence statistic. Column 10 shows the accuracy
of the decision tree when the Dice Coefficient selects
the features. Columns 9 and 11 show the accuracy
of the decision stump based on the power divergence
statistic and the Dice Coefficient respectively. Fi-
nally, column 13 shows the accuracy of the Naive
Bayesian classifier based on a bag of words feature
set.
The most accurate method is the decision tree
based on a feature set determined by the power di-
vergence statistic. The last line of Table 1 shows
the win-tie-loss score of the decision tree/power di-
vergence method relative to every other method. A
win shows it was more accurate than the method in
the column, a loss means it was less accurate, and
a tie means it was equally accurate. The decision
tree/power divergence method was more accurate
than the best reported SENSEVAL results for 19
of the 36 tasks, and more accurate for 30 of the 36
tasks when compared to the average reported accu-
racy. The decision stumps also fared well, proving to
be more accurate than the best SENSEVAL results
for 14 of the 36 tasks.
In general the feature sets selected by the power
divergence statistic result in more accurate decision
trees than those selected by the Dice Coefficient.
The power divergence tests prove to be more reliable
since they account for all possible events surround-
ing two words w1 and w2; when they occur as bigram
w1w2, when w1 or w2 occurs in a bigram without the
other, and when a bigram consists of neither. The
Dice Coefficient is based strictly on the event where
w1 and w2 occur together in a bigram.
There are 6 tasks where the decision tree / power
divergence approach is less accurate than the SEN-
SEVAL average; promise-n, scrap-n, shirt-n, amaze-
v, bitter-p, and sanction-p. The most dramatic dif-
ference occurred with amaze-v, where the SENSE-
VAL average was 92.4% and the decision tree accu-
racy was 58.6%. However, this was an unusual task
where every instance in the test data belonged to a
single sense that was a minority sense in the training
data.
</bodyText>
<sectionHeader confidence="0.893402" genericHeader="method">
7 Analysis of Experimental Results
</sectionHeader>
<bodyText confidence="0.998903090909091">
The characteristics of the decision trees and deci-
sion stumps learned for each word are shown in
Table 2. Column 1 shows the word and part of
speech. Columns 2, 3, and 4 are based on the feature
set selected by the power divergence statistic while
columns 5, 6, and 7 are based on the Dice Coeffi-
cient. Columns 2 and 5 show the node selected to
serve as the decision stump. Columns 3 and 6 show
the number of leaf nodes in the learned decision tree
relative to the number of total nodes. Columns 4
and 7 show the number of bigram features selected
</bodyText>
<tableCaption confidence="0.99573">
Table 1: Experimental Results
</tableCaption>
<table confidence="0.999160875">
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12)
word-pos test senses train maj best avg j48 stump j48 stump naive
in test pow pow dice dice bayes
accident-n 267 8 227 75.3 87.1 79.6 85.0 77.2 83.9 77.2 83.1
behaviour-n 279 3 994 94.3 92.9 90.2 95.7 95.7 95.7 95.7 93.2
bet-n 274 15 106 18.2 50.7 39.6 41.8 34.5 41.8 34.5 39.3
excess-n 186 8 251 1.1 75.9 63.7 65.1 38.7 60.8 38.7 64.5
float-n 75 12 61 45.3 66.1 45.0 52.0 50.7 52.0 50.7 56.0
giant-n 118 7 355 49.2 67.6 56.6 68.6 59.3 66.1 59.3 70.3
knee-n 251 22 435 48.2 67.4 56.0 71.3 60.2 70.5 60.2 64.1
onion-n 214 4 26 82.7 84.8 75.7 82.7 82.7 82.7 82.7 82.2
promise-n 113 8 845 62.8 75.2 56.9 48.7 63.7 55.8 62.8 78.0
sack-n 82 7 97 50.0 77.1 59.3 80.5 58.5 80.5 58.5 74.4
scrap-n 156 14 27 41.7 51.6 35.1 26.3 16.7 26.3 16.7 26.7
shirt-n 184 8 533 43.5 77.4 59.8 46.7 43.5 51.1 43.5 60.9
amaze-v 70 1 316 0.0 100.0 92.4 58.6 12.9 60.0 12.9 71.4
bet-v 117 9 60 43.2 60.5 44.0 50.8 58.5 52.5 50.8 58.5
bother-v 209 8 294 75.0 59.2 50.7 69.9 55.0 64.6 55.0 62.2
bury-v 201 14 272 38.3 32.7 22.9 48.8 38.3 44.8 38.3 42.3
calculate-v 218 5 249 83.9 85.0 75.5 90.8 88.5 89.9 88.5 80.7
consume-v 186 6 67 39.8 25.2 20.2 36.0 34.9 39.8 34.9 31.7
derive-v 217 6 259 47.9 44.1 36.0 82.5 52.1 82.5 52.1 72.4
float-v 229 16 183 33.2 30.8 22.5 30.1 22.7 30.1 22.7 56.3
invade-v 207 6 64 40.1 30.9 25.5 28.0 40.1 28.0 40.1 31.0
promise-v 224 6 1160 85.7 82.1 74.6 85.7 84.4 81.7 81.3 85.3
sack-v 178 3 185 97.8 95.6 95.6 97.8 97.8 97.8 97.8 97.2
scrap-v 186 3 30 85.5 80.6 68.6 85.5 85.5 85.5 85.5 82.3
seize-v 259 11 291 21.2 51.0 42.1 52.9 25.1 49.4 25.1 51.7
brilliant-a 229 10 442 45.9 31.7 26.5 55.9 45.9 51.1 45.9 58.1
floating-a 47 5 41 57.4 49.3 27.4 57.4 57.4 57.4 57.4 55.3
generous-a 227 6 307 28.2 37.5 30.9 44.9 32.6 46.3 32.6 48.9
giant-a 97 5 302 94.8 98.0 93.5 95.9 95.9 94.8 94.8 94.8
modest-a 270 9 374 61.5 49.6 44.9 72.2 64.4 73.0 64.4 68.1
slight-a 218 6 385 91.3 92.7 81.4 91.3 91.3 91.3 91.3 91.3
wooden-a 196 4 362 93.9 81.7 71.3 96.9 96.9 96.9 96.9 93.9
band-p 302 29 1326 77.2 81.7 75.9 86.1 84.4 79.8 77.2 83.1
bitter-p 373 14 144 27.0 44.6 39.8 36.4 31.3 36.4 31.3 32.6
sanction-p 431 7 96 57.5 74.8 62.4 57.5 57.5 57.1 57.5 56.8
shake-p 356 36 963 23.6 56.7 47.1 52.2 23.6 50.0 23.6 46.6
win-tie-loss (j48-pow vs. X) 23-7-6 19-0-17 30-0-6 28-9-3 14-15-7 28-9-3 24-1-11
</table>
<bodyText confidence="0.9886186">
to represent the training data.
This table shows that there is little difference in
the decision stump nodes selected from feature sets
determined by the power divergence statistics versus
the Dice Coefficient. This is to be expected since the
top ranked bigrams for each measure are consistent,
and the decision stump node is generally chosen from
among those.
However, there are differences between the feature
sets selected by the power divergence statistics and
the Dice Coefficient. These are reflected in the dif-
ferent sized trees that are learned based on these
feature sets. The number of leaf nodes and the total
number of nodes for each learned tree is shown in
columns 3 and 6. The number of internal nodes is
simply the difference between the total nodes and
the leaf nodes. Each leaf node represents the end
of a path through the decision tree that makes a
sense distinction. Since a bigram feature can only
appear once in the decision tree, the number of inter-
</bodyText>
<tableCaption confidence="0.921479">
Table 2: Decision Tree and Stump Characteristics
</tableCaption>
<figure confidence="0.99822915625">
(5)
stump node
by accident
best behaviour
betting shop
in excess
the float
the giants
kneeinjury
in the
a promising
the sack
scrap of
shirt and
amazed at
i bet
be bothered
buried in
calculated to
on the
derived from
floated on
toinvade
promise you
return to
of the
to seize
a brilliant
in the
a generous
a giant
a modest
the slightest
wooden spoon
the band
a bitter
south africa
his head
(6) (7)
leaf/total features
12/23 112
2/3 104
20/39 50
11/21 102
7/13 13
14/27 78
20/39 104
1/1 7
49/97 107
5/9 31
7/13 8
55/109 101
11/21 102
4/7 10
20/39 106
32/63 103
5/9 103
4/7 20
10/19 104
24/47 80
66/127 108
5/9 106
1/1 91
1/1 7
57/113 104
42/83 103
7/13 10
56/111 102
1/1 101
10/19 105
2/3 105
2/3 101
21/41 117
22/43 54
12/23 52
81/161 105
dice coefficient
power divergence
(3) (4)
leaf/total features
8/15 101
2/3 100
20/39 50
13/25 104
7/13 13
16/31 103
23/45 102
1/1 7
95/189 100
5/9 31
7/13 8
38/75 101
11/21 102
4/7 10
19/37 101
28/55 103
5/9 103
4/7 20
10/19 104
24/47 80
55/109 107
3/5 100
1/1 91
1/1 7
26/51 104
26/51 101
7/13 10
57/113 103
2/3 102
14/27 101
2/3 105
2/3 104
14/27 100
22/43 54
12/23 52
90/179 100
(1)
word-pos
accident-n
behaviour-n
bet-n
excess-n
float-n
giant-n
knee-n
onion-n
promise-n
sack-n
scrap-n
shirt-n
amaze-v
bet-v
bother-v
bury-v
calculate-v
consume-v
derive-v
float-v
invade-v
promise-v
sack-v
scrap-v
seize-v
brilliant-a
floating-a
generous-a
giant-a
modest-a
slight-a
wooden-a
band-p
bitter-p
sanction-p
shake-p
(2)
stump node
by accident
best behaviour
betting shop
in excess
the float
the giants
kneeinjury
in the
promise of
the sack
scrap of
shirt and
amazed at
i bet
be bothered
buried in
calculated to
on the
derived from
floated on
toinvade
promise to
return to
of the
to seize
a brilliant
in the
a generous
the giant
a modest
the slightest
wooden spoon
band of
a bitter
south africa
his head
</figure>
<bodyText confidence="0.987492225">
nal nodes represents the number of bigram features
selected by the decision tree learner.
One of our original hypotheses was that accurate
decision trees of bigrams will include a relatively
small number of features. This was motivated by
the success of decision stumps in performing disam-
biguation based on a single bigram feature. In these
experiments, there were no decision trees that used
all of the bigram features identified by the filtering
step, and for many words the decision tree learner
went on to eliminate most of the candidate features.
This can be seen by comparing the number of inter-
nal nodes with the number of candidate features as
shown in columns 4 or 7.1
It is also noteworthy that the bigrams ultimately
selected by the decision tree learner for inclusion in
the tree do not always include those bigrams ranked
most highly by the power divergence statistic or the
Dice Coefficient. This is to be expected, since the
selection of the bigrams from raw text is only mea-
&apos;For most words the 100 top ranked bigrams form the set
of candidate features presented to the decision tree learner. If
there are ties in the top 100 rankings then there may be more
than 100 features, and if the there were fewer than 100 bi-
grams that occurred more than 5 times then all such bigrams
are included in the feature set.
suring the association between two words, while the
decision tree seeks bigrams that partition instances
of the ambiguous word into into distinct senses. In
particular, the decision tree learner makes decisions
as to what bigram to include as nodes in the tree
using the gain ratio, a measure based on the over-
all Mutual Information between the bigram and a
particular word sense.
Finally, note that the smallest decision trees are
functionally equivalent to our benchmark methods.
A decision tree with 1 leaf node and no internal
nodes (1/1) acts as a majority classifier. A deci-
sion tree with 2 leaf nodes and 1 internal node (2/3)
has the structure of a decision stump.
</bodyText>
<sectionHeader confidence="0.997891" genericHeader="method">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999969861111111">
One of our long-term objectives is to identify a core
set of features that will be useful for disambiguat-
ing a wide class of words using both supervised and
unsupervised methodologies.
We have presented an ensemble approach to word
sense disambiguation (Pedersen, 2000) where mul-
tiple Naive Bayesian classifiers, each based on co—
occurrence features from varying sized windows of
context, is shown to perform well on the widely stud-
ied nouns interest and line. While the accuracy of
this approach was as good as any previously pub-
lished results, the learned models were complex and
difficult to interpret, in effect acting as very accurate
black boxes.
Our experience has been that variations in learn-
ing algorithms are far less significant contributors
to disambiguation accuracy than are variations in
the feature set. In other words, an informative fea-
ture set will result in accurate disambiguation when
used with a wide range of learning algorithms, but
there is no learning algorithm that can perform well
given an uninformative or misleading set of features.
Therefore, our focus is on developing and discover-
ing feature sets that make distinctions among word
senses. Our learning algorithms must not only pro-
duce accurate models, but they should also shed new
light on the relationships among features and allow
us to continue refining and understanding our fea-
ture sets.
We believe that decision trees meet these criteria.
A wide range of implementations are available, and
they are known to be robust and accurate across a
range of domains. Most important, their structure
is easy to interpret and may provide insights into
the relationships that exist among features and more
general rules of disambiguation.
</bodyText>
<sectionHeader confidence="0.999089" genericHeader="method">
9 Related Work
</sectionHeader>
<bodyText confidence="0.999982742857143">
Bigrams have been used as features for word sense
disambiguation, particularly in the form of colloca-
tions where the ambiguous word is one component
of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng
and Lee, 1996), (Yarowsky, 1995)). While some of
the bigrams we identify are collocations that include
the word being disambiguated, there is no require-
ment that this be the case.
Decision trees have been used in supervised learn-
ing approaches to word sense disambiguation, and
have fared well in a number of comparative studies
(e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).
In the former they were used with the bag of word
feature sets and in the latter they were used with a
mixed feature set that included the part-of-speech of
neighboring words, three collocations, and the mor-
phology of the ambiguous word. We believe that
the approach in this paper is the first time that de-
cision trees based strictly on bigram features have
been employed.
The decision list is a closely related approach that
has also been applied to word sense disambigua-
tion (e.g., (Yarowsky, 1994), (Wilks and Stevenson,
1998), (Yarowsky, 2000)). Rather than building and
traversing a tree to perform disambiguation, a list is
employed. In the general case a decision list may suf-
fer from less fragmentation during learning than de-
cision trees; as a practical matter this means that the
decision list is less likely to be over—trained. How-
ever, we believe that fragmentation also reflects on
the feature set used for learning. Ours consists of
at most approximately 100 binary features. This re-
sults in a relatively small feature space that is not
as likely to suffer from fragmentation as are larger
spaces.
</bodyText>
<sectionHeader confidence="0.997874" genericHeader="method">
10 Future Work
</sectionHeader>
<bodyText confidence="0.999993095238095">
There are a number of immediate extensions to this
work. The first is to ease the requirement that bi-
grams be made up of two consecutive words. Rather,
we will search for bigrams where the component
words may be separated by other words in the text.
The second is to eliminate the filtering step by which
candidate bigrams are selected by a power diver-
gence statistic. Instead, the decision tree learner
would consider all possible bigrams. Despite increas-
ing the danger of fragmentation, this is an interest-
ing issue since the bigrams judged most informative
by the decision tree learner are not always ranked
highly in the filtering step. In particular, we will
determine if the filtering process ever eliminates bi-
grams that could be significant sources of disam-
biguation information.
In the longer term, we hope to adapt this approach
to unsupervised learning, where disambiguation is
performed without the benefit of sense tagged text.
We are optimistic that this is viable, since bigram
features are easy to identify in raw text.
</bodyText>
<sectionHeader confidence="0.951136" genericHeader="conclusions">
11 Conclusion
</sectionHeader>
<bodyText confidence="0.999863111111111">
This paper shows that the combination of a simple
feature set made up of bigrams and a standard deci-
sion tree learning algorithm results in accurate word
sense disambiguation. The results of this approach
are compared with those from the 1998 SENSEVAL
word sense disambiguation exercise and show that
the bigram based decision tree approach is more ac-
curate than the best SENSEVAL results for 19 of 36
words.
</bodyText>
<sectionHeader confidence="0.979175" genericHeader="acknowledgments">
12 Acknowledgments
</sectionHeader>
<bodyText confidence="0.997778833333333">
The Bigram Statistics Package has been imple-
mented by Satanjeev Banerjee, who is supported by
a Grant-in-Aid of Research, Artistry and Scholar-
ship from the Office of the Vice President for Re-
search and the Dean of the Graduate School of the
University of Minnesota. We would like to thank
the SENSEVAL organizers for making the data and
results from the 1998 event freely available. The
comments of three anonymous reviewers were very
helpful in preparing the final version of this paper.
A preliminary version of this paper appears in (Ped-
ersen, 2001).
</bodyText>
<sectionHeader confidence="0.997886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999186518518519">
R. Bruce and J. Wiebe. 1994. Word-sense disam-
biguation using decomposable models. In Proceed-
ings of the 32nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 139-
146.
K. Church and P. Hanks. 1990. Word association
norms, mutual information and lexicography. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics, pages
76-83.
N. Cressie and T. Read. 1984. Multinomial good-
ness of fit tests. Journal of the Royal Statistics
Society Series B, 46:440-464.
R. Duda and P. Hart. 1973. Pattern Classification
and Scene Analysis. Wiley, New York, NY.
T. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61-74.
R. Holte. 1993. Very simple classification rules per-
form well on most commonly used datasets. Ma-
chine Learning, 11:63-91.
A. Kilgarriff and M. Palmer. 2000. Special issue on
SENSEVAL: Evaluating word sense disambigua-
tion programs. Computers and the Humanities,
34(1-2).
R. Mooney. 1996. Comparative experiments on dis-
ambiguating word senses: An illustration of the
role of bias in machine learning. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 82-91, May.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense:
An exemplar-based approach. In Proceedings of
the 34th Annual Meeting of the Association for
Computational Linguistics, pages 40-47.
T. Pedersen and R. Bruce. 1997. A new supervised
learning algorithm for word sense disambiguation.
In Proceedings of the Fourteenth National Con-
ference on Artificial Intelligence, pages 604-609,
Providence, RI, July.
T. Pedersen. 1996. Fishing for exactness. In Pro-
ceedings of the South Central SAS User&apos;s Group
(SCSUG-96) Conference, pages 188-200, Austin,
TX, October.
T. Pedersen. 2000. A simple approach to building
ensembles of naive bayesian classifiers for word
sense disambiguation. In Proceedings of the First
Annual Meeting of the North American Chapter
of the Association for Computational Linguistics,
pages 63-69, Seattle, WA, May.
T. Pedersen. 2001. Lexical semantic ambiguity res-
olution with bigram-based decision trees. In Pro-
ceedings of the Second International Conference
on Intelligent Text Processing and Computational
Linguistics, pages 157-168, Mexico City, Febru-
ary.
F. Smadja, K. McKeown, and V. Hatzivassiloglou.
1996. Translating collocations for bilingual lexi-
cons: A statistical approach. Computational Lin-
guistics, 22(1):1-38.
Y. Wilks and M. Stevenson. 1998. Word
sense disambiguation using optimised combina-
tions of knowledge sources. In Proceedings of
COLING/ACL-98.
I. Witten and E. Frank. 2000. Data Mining - Practi-
cal Machine Learning Tools and Techniques with
Java Implementations. Morgan-Kaufmann, San
Francisco, CA.
D. Yarowsky. 1994. Decision lists for lexical amgi-
guity resolution: Application to accent resotration
in Spanish and French. In Proceedings of the 32nd
Annual Meeting of the Association for Computa-
tional Linguistics.
D. Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 189-
196, Cambridge, MA.
D. Yarowsky. 2000. Hierarchical decision lists for
word sense disambiguation. Computers and the
Humanities, 34(1-2).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933912">
<title confidence="0.9969815">A Decision Tree of is an Accurate Predictor of Word Sense</title>
<author confidence="0.992788">Ted</author>
<affiliation confidence="0.9999585">Department of Computer University of Minnesota</affiliation>
<address confidence="0.973743">Duluth, MN 55812 USA</address>
<email confidence="0.999807">tpederse@d.umn.edu</email>
<abstract confidence="0.9948337">This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby. This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Word-sense disambiguation using decomposable models.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="28480" citStr="Bruce and Wiebe, 1994" startWordPosition="4928" endWordPosition="4931">res and allow us to continue refining and understanding our feature sets. We believe that decision trees meet these criteria. A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. 9 Related Work Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morpholog</context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>R. Bruce and J. Wiebe. 1994. Word-sense disambiguation using decomposable models. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 139-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="7913" citStr="Church and Hanks, 1990" startWordPosition="1298" endWordPosition="1301">here rankings produced by G2, X2, and Fisher&apos;s exact test disagreed, which is not altogether surprising given that low frequency bigrams were excluded. Since all of these statistics produced the same rankings, hereafter we make no distinction among them and simply refer to them generically as the power divergence statistic. 2.2 Dice Coefficient The Dice Coefficient is a descriptive statistic that provides a measure of association among two words in a corpus. It is similar to pointwise Mutual Information, a widely used measure that was first introduced for identifying lexical relationships in (Church and Hanks, 1990). Pointwise Mutual Information can be defined as follows: n+1 * n1+ mij = ni+ * n+j n++ n11 * n++ MI(w1� w2) = �o�2 Given this value, G2 and X2 are calculated as: G2=2Y� i,j (nij — mij)2 mij (Dunning, 1993) argues in favor of G2 over X2, especially when dealing with very sparse and skewed where w1 and w2 represent the two words that make up the bigram. Pointwise Mutual Information quantifies how often two words occur together in a bigram (the numerator) relative to how often they occur overall in the corpus (the denominator). However, there is a curious limitation to pointwise Mutual Informati</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. Church and P. Hanks. 1990. Word association norms, mutual information and lexicography. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 76-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cressie</author>
<author>T Read</author>
</authors>
<title>Multinomial goodness of fit tests.</title>
<date>1984</date>
<journal>Journal of the Royal Statistics Society Series B,</journal>
<pages>46--440</pages>
<contexts>
<context position="5527" citStr="Cressie and Read, 1984" startWordPosition="911" endWordPosition="914">t, an information theoretic measure related to pointwise Mutual Information. Figure 1 summarizes the notation for word and bigram counts used in this paper by way of a 2 x 2 contingency table. The value of n11 shows how many times the bigram big cat occurs in the corpus. The value of n12 shows how often bigrams occur where big is the first word and cat is not the second. The counts in n+1 and n1+ indicate how often words big and cat occur as the first and second words of any bigram in the corpus. The total number of bigrams in the corpus is represented by n++. 2.1 The Power Divergence Family (Cressie and Read, 1984) introduce the power divergence family of goodness of fit statistics. A number of well known statistics belong to this family, including the likelihood ratio statistic G2 and Pearson&apos;s X2 statistic. These measure the divergence of the observed (nij) and expected (mij) bigram counts, where mij is estimated based on the assumption that the component words in the bigram occur together strictly by chance: data distributions. However, (Cressie and Read, 1984) suggest that there are cases where Pearson&apos;s statistic is more reliable than the likelihood ratio and that one test should not always be pref</context>
</contexts>
<marker>Cressie, Read, 1984</marker>
<rawString>N. Cressie and T. Read. 1984. Multinomial goodness of fit tests. Journal of the Royal Statistics Society Series B, 46:440-464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Duda</author>
<author>P Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="11713" citStr="Duda and Hart, 1973" startWordPosition="1950" endWordPosition="1953">guated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context. We also include three benchmark learning algorithms in this study: the majority classifier, the decision stump, and the Naive Bayesian classifier. The majority classifier assigns the most common sense in the training data to every instance in the test data. A decision stump is a one node decision tree(Holte, 1993) that is created by stopping the decision tree learner after the single most informative feature is added to the tree. The Naive Bayesian classifier (Duda and Hart, 1973) is based on certain blanket assumptions about the interactions among features in a corpus. There is no search of the feature space performed to build a representative model as is the case with decision trees. Instead, all features are included in the classifier and assumed to be relevant to the task at hand. There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word. It is most often used with a bag of words feature set, where every word in the training sample is represented by a binary feature that indicates wheth</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. Duda and P. Hart. 1973. Pattern Classification and Scene Analysis. Wiley, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="8119" citStr="Dunning, 1993" startWordPosition="1341" endWordPosition="1342">fter we make no distinction among them and simply refer to them generically as the power divergence statistic. 2.2 Dice Coefficient The Dice Coefficient is a descriptive statistic that provides a measure of association among two words in a corpus. It is similar to pointwise Mutual Information, a widely used measure that was first introduced for identifying lexical relationships in (Church and Hanks, 1990). Pointwise Mutual Information can be defined as follows: n+1 * n1+ mij = ni+ * n+j n++ n11 * n++ MI(w1� w2) = �o�2 Given this value, G2 and X2 are calculated as: G2=2Y� i,j (nij — mij)2 mij (Dunning, 1993) argues in favor of G2 over X2, especially when dealing with very sparse and skewed where w1 and w2 represent the two words that make up the bigram. Pointwise Mutual Information quantifies how often two words occur together in a bigram (the numerator) relative to how often they occur overall in the corpus (the denominator). However, there is a curious limitation to pointwise Mutual Information. A bigram w1w2 that occurs n11 times in the corpus, and whose component words w1 and w2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n11</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Holte</author>
</authors>
<title>Very simple classification rules perform well on most commonly used datasets.</title>
<date>1993</date>
<booktitle>Machine Learning,</booktitle>
<pages>11--63</pages>
<contexts>
<context position="11543" citStr="Holte, 1993" startWordPosition="1923" endWordPosition="1924">ing a path through the learned decision tree from the root to a leaf node that corresponds with the observed features. An instance of an ambiguous word is disambiguated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context. We also include three benchmark learning algorithms in this study: the majority classifier, the decision stump, and the Naive Bayesian classifier. The majority classifier assigns the most common sense in the training data to every instance in the test data. A decision stump is a one node decision tree(Holte, 1993) that is created by stopping the decision tree learner after the single most informative feature is added to the tree. The Naive Bayesian classifier (Duda and Hart, 1973) is based on certain blanket assumptions about the interactions among features in a corpus. There is no search of the feature space performed to build a representative model as is the case with decision trees. Instead, all features are included in the classifier and assumed to be relevant to the task at hand. There is a further assumption that each feature is conditionally independent of all other features, given the sense of </context>
</contexts>
<marker>Holte, 1993</marker>
<rawString>R. Holte. 1993. Very simple classification rules perform well on most commonly used datasets. Machine Learning, 11:63-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>M Palmer</author>
</authors>
<title>Special issue on SENSEVAL: Evaluating word sense disambiguation programs.</title>
<date>2000</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="13062" citStr="Kilgarriff and Palmer, 2000" startWordPosition="2172" endWordPosition="2176">lementations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesian classifier. Weka is written in Java and is freely available from www.cs.waikato.ac.nz/~ml. 4 Experimental Data Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems. Ten teams participated in the supervised learning portion of this event. Additional details about the exercise, including the data and results referred to in this paper, can be found at the SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) and in (Kilgarriff and Palmer, 2000). We included all 36 tasks from SENSEVAL for which training and test data were provided. Each task requires that the occurrences of a particular word in the test data be disambiguated based on a model learned from the sense—tagged instances in the training data. Some words were used in multiple tasks as different parts of speech. For example, there were two tasks associated with bet, one for its use as a noun and the other as a verb. Thus, there are 36 tasks involving the disambiguation of 29 different words. The words and part of speech associated with each task are shown in Table 1 in column</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>A. Kilgarriff and M. Palmer. 2000. Special issue on SENSEVAL: Evaluating word sense disambiguation programs. Computers and the Humanities, 34(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>82--91</pages>
<contexts>
<context position="28836" citStr="Mooney, 1996" startWordPosition="4988" endWordPosition="4989">atures and more general rules of disambiguation. 9 Related Work Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the first time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)). Rather than buildin</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>R. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 82-91, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="28500" citStr="Ng and Lee, 1996" startWordPosition="4932" endWordPosition="4935">nue refining and understanding our feature sets. We believe that decision trees meet these criteria. A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. 9 Related Work Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous w</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>H.T. Ng and H.B. Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
</authors>
<title>A new supervised learning algorithm for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>604--609</pages>
<location>Providence, RI,</location>
<contexts>
<context position="28864" citStr="Pedersen and Bruce, 1997" startWordPosition="4990" endWordPosition="4993">general rules of disambiguation. 9 Related Work Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the first time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)). Rather than building and traversing a tree to p</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>T. Pedersen and R. Bruce. 1997. A new supervised learning algorithm for word sense disambiguation. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 604-609, Providence, RI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Fishing for exactness.</title>
<date>1996</date>
<booktitle>In Proceedings of the South Central SAS User&apos;s Group (SCSUG-96) Conference,</booktitle>
<pages>188--200</pages>
<location>Austin, TX,</location>
<contexts>
<context position="6183" citStr="Pedersen, 1996" startWordPosition="1019" endWordPosition="1020"> goodness of fit statistics. A number of well known statistics belong to this family, including the likelihood ratio statistic G2 and Pearson&apos;s X2 statistic. These measure the divergence of the observed (nij) and expected (mij) bigram counts, where mij is estimated based on the assumption that the component words in the bigram occur together strictly by chance: data distributions. However, (Cressie and Read, 1984) suggest that there are cases where Pearson&apos;s statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other. In light of this, (Pedersen, 1996) presents Fisher&apos;s exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson&apos;s test and the likelihood ratio. Unfortunately it is usually not clear which test is most appropriate for a particular sample of data. We take the following approach, based on the observation that all tests should assign approximately the same measure of statistical significance when the bigram counts in the contingency table do not violate any of the distributional assumptions that underly the goodness of fit statistics. We perform tests using X2, G2, and Fisher&apos;s</context>
</contexts>
<marker>Pedersen, 1996</marker>
<rawString>T. Pedersen. 1996. Fishing for exactness. In Proceedings of the South Central SAS User&apos;s Group (SCSUG-96) Conference, pages 188-200, Austin, TX, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>63--69</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="26820" citStr="Pedersen, 2000" startWordPosition="4661" endWordPosition="4662"> the bigram and a particular word sense. Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods. A decision tree with 1 leaf node and no internal nodes (1/1) acts as a majority classifier. A decision tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump. 8 Discussion One of our long-term objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies. We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classifiers, each based on co— occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. While the accuracy of this approach was as good as any previously published results, the learned models were complex and difficult to interpret, in effect acting as very accurate black boxes. Our experience has been that variations in learning algorithms are far less significant contributors to disambiguation accuracy than are variations in the feature set. In other words, an informative feature set will</context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>T. Pedersen. 2000. A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation. In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 63-69, Seattle, WA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Lexical semantic ambiguity resolution with bigram-based decision trees.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>157--168</pages>
<location>Mexico City,</location>
<marker>Pedersen, 2001</marker>
<rawString>T. Pedersen. 2001. Lexical semantic ambiguity resolution with bigram-based decision trees. In Proceedings of the Second International Conference on Intelligent Text Processing and Computational Linguistics, pages 157-168, Mexico City, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
<author>K McKeown</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="9574" citStr="Smadja et al., 1996" startWordPosition="1594" endWordPosition="1597">the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information. The Dice Coefficient overcomes this limitation, and can be defined as follows: Dice(w1i w2) = n+1 + n1+ When n11 = n1+ = n+1 the value of Dice(w1, w2) will be 1 for all values n11. When the value of n11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Coefficient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice Coefficient is also discussed in (Smadja et al., 1996). We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests. This software is written in Perl and is freely available from www.d.umn.edu/~tpederse. 3 Learning Decision Trees Decision trees are among the most widely used machine learning algorithms. They perform a general to specific search of a feature space, adding the most informative features to a tree structure as the search proceeds. The objective is to select a minimal set of features that efficiently partitions the feature space into classes of observations and assemble them into a tree. I</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>F. Smadja, K. McKeown, and V. Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>M Stevenson</author>
</authors>
<title>Word sense disambiguation using optimised combinations of knowledge sources.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-98.</booktitle>
<contexts>
<context position="29396" citStr="Wilks and Stevenson, 1998" startWordPosition="5081" endWordPosition="5084">ared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the first time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)). Rather than building and traversing a tree to perform disambiguation, a list is employed. In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over—trained. However, we believe that fragmentation also reflects on the feature set used for learning. Ours consists of at most approximately 100 binary features. This results in a relatively small feature space that is not as likely to suffer from fragmentation as are larger spaces. 10 Future Work Th</context>
</contexts>
<marker>Wilks, Stevenson, 1998</marker>
<rawString>Y. Wilks and M. Stevenson. 1998. Word sense disambiguation using optimised combinations of knowledge sources. In Proceedings of COLING/ACL-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining -</title>
<date>2000</date>
<booktitle>Practical Machine Learning Tools and Techniques with Java Implementations. Morgan-Kaufmann,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="12430" citStr="Witten and Frank, 2000" startWordPosition="2076" endWordPosition="2079">ere is no search of the feature space performed to build a representative model as is the case with decision trees. Instead, all features are included in the classifier and assumed to be relevant to the task at hand. There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word. It is most often used with a bag of words feature set, where every word in the training sample is represented by a binary feature that indicates whether or not it occurs in the window of context surrounding the ambiguous word. We use the Weka (Witten and Frank, 2000) implementations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesian classifier. Weka is written in Java and is freely available from www.cs.waikato.ac.nz/~ml. 4 Experimental Data Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems. Ten teams participated in the supervised learning portion of this event. Additional details about the exercise, including the data and results referred to in this paper, can be found at the SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) and </context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>I. Witten and E. Frank. 2000. Data Mining - Practical Machine Learning Tools and Techniques with Java Implementations. Morgan-Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical amgiguity resolution: Application to accent resotration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="29367" citStr="Yarowsky, 1994" startWordPosition="5079" endWordPosition="5080">uation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the first time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)). Rather than building and traversing a tree to perform disambiguation, a list is employed. In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over—trained. However, we believe that fragmentation also reflects on the feature set used for learning. Ours consists of at most approximately 100 binary features. This results in a relatively small feature space that is not as likely to suffer from fragmentation as are lar</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision lists for lexical amgiguity resolution: Application to accent resotration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="28518" citStr="Yarowsky, 1995" startWordPosition="4936" endWordPosition="4937">erstanding our feature sets. We believe that decision trees meet these criteria. A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. 9 Related Work Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe th</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189-196, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Hierarchical decision lists for word sense disambiguation. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="29414" citStr="Yarowsky, 2000" startWordPosition="5085" endWordPosition="5086">arative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the first time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)). Rather than building and traversing a tree to perform disambiguation, a list is employed. In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over—trained. However, we believe that fragmentation also reflects on the feature set used for learning. Ours consists of at most approximately 100 binary features. This results in a relatively small feature space that is not as likely to suffer from fragmentation as are larger spaces. 10 Future Work There are a number o</context>
</contexts>
<marker>Yarowsky, 2000</marker>
<rawString>D. Yarowsky. 2000. Hierarchical decision lists for word sense disambiguation. Computers and the Humanities, 34(1-2).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>