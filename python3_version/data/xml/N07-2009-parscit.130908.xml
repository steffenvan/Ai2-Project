<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011841">
<title confidence="0.996866">
Generalized Graphical Abstractions for Statistical Machine Translation
</title>
<author confidence="0.99503">
Karim Filali and Jeff Bilmes*
</author>
<affiliation confidence="0.998427">
Departments of Computer Science &amp; Engineering and Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.752114">
Seattle, WA 98195, USA
</address>
<email confidence="0.984054">
{karim@cs,bilmes@ee}.washington.edu
</email>
<sectionHeader confidence="0.998485" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991">
We introduce a novel framework for the
expression, rapid-prototyping, and eval-
uation of statistical machine-translation
(MT) systems using graphical mod-
els. The framework extends dynamic
Bayesian networks with multiple con-
nected different-length streams, switching
variable existence and dependence mech-
anisms, and constraint factors. We have
implemented a new general-purpose MT
training/decoding system in this frame-
work, and have tested this on a variety of
existing MT models (including the 4 IBM
models), and some novel ones as well,
all using Europarl as a test corpus. We
describe the semantics of our representa-
tion, and present preliminary evaluations,
showing that it is possible to prototype
novel MT ideas in a short amount of time.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999789">
We present a unified graphical model framework
based on (Filali and Bilmes, 2006) for statistical ma-
chine translation. Graphical models utilize graphical
descriptions of probabilistic processes, and are capa-
ble of quickly describing a wide variety of different
sets of model assumptions. In our approach, either
phrases or words can be used as the unit of transla-
tion, but as a first step, we have only implemented
word-based models since our main goal is to show
</bodyText>
<footnote confidence="0.5395045">
This material was supported by NSF under Grant No. ISS-
0326276.
</footnote>
<page confidence="0.993241">
33
</page>
<bodyText confidence="0.99537025">
the viability of our graphical model representation
and new software system.
There are several important advantages to a uni-
fied probabilistic framework for MT including: (1)
the same codebase can be used for training and de-
coding without having to implement a separate de-
coder for each model; (2) new models can be pro-
totyped quickly; (3) combining models (such as in
a speech-MT system) is easier when they are en-
coded in the same framework; (4) sharing algo-
rithms across different disciplines (e.g., the MT and
the constraint-satisfaction community) is facilitated.
</bodyText>
<sectionHeader confidence="0.998653" genericHeader="method">
2 Graphical Model Framework
</sectionHeader>
<bodyText confidence="0.999415952380952">
A Graphical Model (GM) represents a factorization
of a family of joint probability distributions over a
set of random variables using a graph. The graph
specifies conditional independence relationships be-
tween the variables, and parameters of the model
are associated with each variable or group thereof.
There are many types of graphical models. For ex-
ample, Bayesian networks use an acyclic directed
graph and their parameters are conditional probabili-
ties of each variable given its parents. Various forms
of GM and their conditional independence proper-
ties are defined in (Lauritzen, 1996).
Our graphical representation, which we call
Multi-dynamic Bayesian Networks (MDBNs) (Filali
and Bilmes, 2006), is a generalization of dynamic
Bayesian networks (DBNs) (Dean and Kanazawa,
1988). DBNs are an appropriate representation for
sequential (for example, temporal) stochastic pro-
cesses, but can be very difficult to apply when de-
pendencies have arbitrary time-span and the exis-
tence of random variables is contingent on the val-
</bodyText>
<note confidence="0.360848">
Proceedings of NAACL HLT 2007, Companion Volume, pages 33–36,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.758686571428571">
ues of certain variables in the network. In (Filali
and Bilmes, 2006), we discuss inference and learn-
ing in MDBNs. Here we focus on representation
and the evaluation of our new implementation and
framework. Below, we summarize key features un-
derlying our framework. In section 3, we explain
how these features apply to a specific MT model.
</bodyText>
<listItem confidence="0.98847365">
• Multiple DBNs can be represented along with
rules for how they are interconnected — the rule de-
scription lengths are fixed (they do not grow with the
length of the DBNs).
• Switching dependencies (Geiger and Hecker-
man, 1996): a variable X is a switching parent of
Y if X influences what type of dependencies Y has
with respect to its other parents, e.g., an alignment
variable in IBM Models 1 and 2 is switching.
• Switching existence: A variable X is “switch-
ing existence” with respect to variable Y if the value
of X determines whether Y exists. An example is a
fertility variable in IBM Models 3 and above.
• Constraints and aggregation: BN semantics can
encode various types of constraints between groups
of variables (Pearl, 1988). For example, in the con-
struct A —* B +— C where B is observed, B can
constrain A and C to be unequal. We extend those
semantics to support a more efficient evaluation of
constraints under some variable order conditions.
</listItem>
<sectionHeader confidence="0.957765" genericHeader="method">
3 GM Representation of IBM MT Models
</sectionHeader>
<bodyText confidence="0.999988981481481">
In this section we present a GM representation for
IBM model 3 (Brown et al., 1993) in fig. 1. Model 3
is intricate enough to showcase some of the features
of our graphical representation but not as complex
as, and thus is easier to describe, than model 4. Our
choice of representing IBM models is not because
we believe they are state of the art MT models—
although they are still widely used in producing
alignments and as features in log-linear models—
but because they provide a good initial testbed for
our architecture.
The topmost random variable (RV), E, is a hid-
den switching existence variable corresponding to
the length of the English string. The box abutting
E includes all the nodes whose existence depends on
the value of E. In the figure, E = 3, thus resulting
in three English words e1, ..., e3, connected using a
second-order Markov chain. To each English word
ei corresponds a conditionally dependent fertility 0i,
which indicates how many times ei is used by words
in the French string. Each 0i in turn grants existence
to a set of RVs under it. Given the fertilities (the fig-
ure depicts the case 01 = 3, 02 = 1, 03 = 0), for
each word ei, 0i French word RVs are granted exis-
tence and are denoted by the tablet Ti1, Ti2, ... , Ti0i
of ei. The values of T variables need to match the
actual observed French sequence f1, ... , fm. This is
represented as a shared constraint between all the f,
7r, and T variables which have incoming edges into
the observed variable v. v’s conditional probability
table is such that it is one only when the associated
constraint is satisfied. The variable 7ri,k is a switch-
ing dependency parent with respect to the constraint
variable v and determines which fj participates in
an equality constraint with Ti,k.
In the null word sub-model, the constraint that
successive permutation variables be ordered is im-
plemented using the observed child w of 7r0i and
7r0(i+1). The probability of w being unity is one only
when the constraint is satisfied and zero otherwise.
The bottom variable m is a switching existence
node (observed to be 6 in the figure) with cor-
responding French word sequence and alignment
variables. The French sequence participates in the
v constraint described above, while the alignment
variables aj E {1, ... , E}, j E 1, ... , m constrain
the fertilities to take their unique allowable values
(for the given alignment). Alignments also restrict
the domain of permutation variables, 7r, using the
constraint variable x. Finally, the domain size of
each aj has to lie in the interval [0, E] and that is en-
forced by the variable u. The dashed edges connect-
ing the alignment a variables represent an extension
to implement an M3/M-HMM hybrid.1
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999850888888889">
We have developed (in C++) a new entirely self-
contained general-purpose MT training/decoding
system based on our framework, of which we pro-
vide a preliminary evaluation in this section. Al-
though the framework is perfectly capable of rep-
resenting phrase-based models, we restrict ourselves
to word-based models to show the viability of graph-
ical models for MT and will consider different trans-
lation units in future work. We perform MT ex-
</bodyText>
<footnote confidence="0.970952">
1We refer to the HMM MT model in (Vogel et al., 1996) as
M-HMM to avoid any confusion.
</footnote>
<page confidence="0.99421">
34
</page>
<figureCaption confidence="0.924662">
Figure 1: Unrolled Model 3 graphical model with fertility
assignment φ0 = 2, φ, = 3,φ2 = 1, φ3 = 0.
</figureCaption>
<bodyText confidence="0.99982019047619">
periments on a English-French subset of the Eu-
roparl corpus used for the ACL 2005 SMT evalu-
ations (Koehn and Monz, 2005). We train an En-
glish language model on the whole training set us-
ing the SRILM toolkit (Stolcke, 2002) and train
MT models mainly on a 10k sentence pair sub-
set of the ACL training set. We test on the 2000
sentence test set used for the same evaluations.
For comparison, we use the MT training program,
GIZA++ (Och and Ney, 2003), the phrase-base de-
coder, Pharaoh (Koehn et al., 2003), and the word-
based decoder, Rewrite (Germann, 2003).
For inference we use a backtracking depth-first
search inference method with memoization that ex-
tends Value Elimination (Bacchus et al., 2003). The
same inference engine is used for both training and
decoding. As an admissible heuristic for decod-
ing, we compute, for each node V with Conditional
Probability Table c, the largest value of c over all
possible configurations of V and its parents (Filali
and Bilmes, 2006).
</bodyText>
<table confidence="0.9988106">
Decoder BLEU (%)
500 1000 1500 2000
Rewrite 25.3 22.3 21.7 22.01
Pharaoh 20.4 18.1 17.7 18.05
M-HMM 19.9 16.9 15.6 12.5
</table>
<tableCaption confidence="0.623331">
Table 1: BLEU scores on first 500, 1000, 1500, and
2000 sentences (ordered from shortest to longest) of
the ACL05 English-French 2000 sentence test set us-
ing a 700k sent train set. The last row is our MDBN
system’s simulation of a M-HMM model.
Table 1 compares MT performance between (1)
Pharaoh (which uses beam search), (2) our system,
and (3) Rewrite (hill-climbing). (1) and (2) make
</tableCaption>
<bodyText confidence="0.999681777777778">
use of a fixed lexical table2 learned using an M-
HMM model specified using our tool, and neither
uses minimum error rate training. (3) uses Model
4 parameters learned using GIZA++. This compari-
son is informative because Rewrite is a special pur-
pose model 4 decoder and we would expect it to
perform at least as well as decoders not written for
a specific IBM model. Pharaoh is more general in
that it only requires, as input, a lexical table from
any given model.3 Our MDBN system is not tai-
lored for the translation task. Pharaoh was able to
decode the 2000 sentences of the test set in 5000s
on a 3.2GHz machine; Rewrite took 84000s, and we
allotted 400000s for our engine (200s per sentence).
We attribute the difference in speed and BLEU score
between our system and Pharaoh to the fact Value
Elimination searches in a depth-first fashion over
the space of partial configurations of RVs, while
Pharaoh expands partial translation hypotheses in a
best-first search manner. Thus, Pharaoh can take ad-
vantage of knowledge about the MT problem’s hy-
pothesis space while the GM is agnostic with respect
to the structure of the problem—something that is
desirable from our perspective since generality is
a main concern of ours. Moreover, the MDBN’s
heuristic and caching of previously explored sub-
trees have not yet proven able to defray the cost,
associated with depth-first search, of exploring sub-
trees that do not contain any “good” configurations.
Table 2 shows BLEU scores of different MT mod-
els trained using our system. We decode using
Pharaoh because the above speed difference in its
favor allowed us to run more experiments and fo-
cus on the training aspect of different models. M1,
M2, M-HMM, M3, and M4 are the standard IBM
models. M2d and M-Hd are variants in which
the distortion between the French and English po-
sitions is used instead of the absolute alignment po-
sition. M-Hdd is a second-order M-HMM model
(with distortion). M3H (see fig 1) is a variant of
model 3 that uses first-order dependencies between
alignment variables. M-Hr is another HMM model
that uses the relative distortion between the current
alignment and the previous one. This is similar
to the model implemented by GIZA except we did
</bodyText>
<footnote confidence="0.910737333333333">
2Pharaoh’s phrases are single words only.
3It does, however, use simple hard-coded distortion and fer-
tility models.
</footnote>
<figure confidence="0.998372826086956">
u
m
v
π01 w π02 π11 π12 π13 π21
τ01 τ02 τ11 τ12 τ13 τ21
m’
a1
f1
φ0 φ1 φ2 φ3
a2
f2
ℓ
a3
f3
e1 e2 e3
a4
f4
a5
f5
a6
f6
x
y
</figure>
<page confidence="0.988351">
35
</page>
<table confidence="0.999554642857143">
BLEU(%)
Giza train MDBN train
10k 700k 10k 700k
M1 15.67 18.04 14.53 17.74
M2 15.84 18.52 15.74
M2d NA NA 15.75
M-HMM NA NA 15.87
M-Hd NA NA 15.99 18.05
M-Hdd NA NA 15.55
M-Hr 16.98 19.57 16.04
M3 16.78 19.38 15.32
M3H NA NA 15.67
M4 16.81 19.51 15.00
M4H NA NA 15.20
</table>
<tableCaption confidence="0.985621333333333">
Table 2: BLEU scores for various models trained
using GM and GIZA (when applicable). All models
are decoded using Pharaoh.
</tableCaption>
<bodyText confidence="0.99995512">
not include the English word class dependency. Fi-
nally, model M4H is a simplified model 4, in which
only distortions within each tablet are modeled but a
Markov dependency is also used between the align-
ment variables.
Table 2 also shows BLEU scores obtained by
training equivalent IBM models using GIZA and
the standard training regimen of initializing higher
models with lower ones (we use the same sched-
ules for our GM training, but only transfer lexical ta-
bles). The main observation is that GIZA-trained M-
HMM, M3 and 4 have about 1% better BLEU scores
than their corresponding MDBN versions. We at-
tribute the difference in M3/4 scores to the fact we
use a Viterbi-like training procedure (i.e., we con-
sider a single configuration of the hidden variables
in EM training) while GIZA uses pegging (Brown et
al., 1993) to sum over a set of likely hidden variable
configurations in EM.
While these preliminary results do not show im-
proved MT performance, nor would we expect them
to since they are on simulated IBM models, we find
very promising the fact that this general-purpose
graphical model-based system produces competitive
MT results on a computationally challenging task.
</bodyText>
<sectionHeader confidence="0.99716" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999978388888889">
We have described a new probabilistic framework
for doing statistical machine translation. We have
focused so far on word-based translation. In fu-
ture work, we intend to implement phrase-based MT
models. We also plan to design better approximate
inference strategies for training highly connected
graphs such as IBM models 3 and 4, and some novel
extensions. We are also working on new best-first
search generalizations of our depth-first search in-
ference to improve decoding time. As there has been
increased interest in end-to-end task such as speech
translation, dialog systems, and multilingual search,
a new challenge is how best to combine the complex
components of these systems into one framework.
We believe that, in addition to the finite-state trans-
ducer approach, a graphical model framework such
as ours would be well suited for this scientific and
engineering endeavor.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999797">
F. Bacchus, S. Dalmao, and T. Pitassi. 2003. Value elimina-
tion: Bayesian inference via backtracking search. In UAI-03,
pages 20–28, San Francisco, CA. Morgan Kaufmann.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263–311.
T. Dean and K. Kanazawa. 1988. Probabilistic temporal rea-
soning. AAAI, pages 524–528.
Karim Filali and Jeff Bilmes. 2006. Multi-dynamic Bayesian
networks. In Advances in Neural Information Processing
Systems 19. MIT Press, Cambridge, MA.
Dan Geiger and David Heckerman. 1996. Knowledge repre-
sentation and inference in similarity networks and bayesian
multinets. Artif. Intell., 82(1-2):45–74.
Ulrich Germann. 2003. Greedy decoding for statistical Ma-
chine Translation in almost linear time. In HLT-NAACL,
pages 72–79, Edmonton, Canada, May. ACL.
P. Koehn and C. Monz. 2005. Shared task: Statistical machine
translation between European languages. pages 119–124,
Ann Arbor, MI, June. ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, May.
S.L. Lauritzen. 1996. Graphical Models. Oxford Science Pub-
lications.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19–51.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference. Morgan Kaufmann.
A. Stolcke. 2002. SRILM – an extensible language modeling
toolkit. In Proc. Int. Conf. on Spoken Language Processing.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In COLING, pages 836–
841.
</reference>
<page confidence="0.998936">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.833948">
<title confidence="0.99852">Generalized Graphical Abstractions for Statistical Machine Translation</title>
<author confidence="0.857412">Filali</author>
<affiliation confidence="0.9966545">Departments of Computer Science &amp; Engineering and Electrical University of</affiliation>
<address confidence="0.999532">Seattle, WA 98195,</address>
<abstract confidence="0.99872215">We introduce a novel framework for the expression, rapid-prototyping, and evaluation of statistical machine-translation (MT) systems using graphical models. The framework extends dynamic Bayesian networks with multiple connected different-length streams, switching variable existence and dependence mechanisms, and constraint factors. We have implemented a new general-purpose MT training/decoding system in this framework, and have tested this on a variety of existing MT models (including the 4 IBM models), and some novel ones as well, all using Europarl as a test corpus. We describe the semantics of our representation, and present preliminary evaluations, showing that it is possible to prototype novel MT ideas in a short amount of time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Bacchus</author>
<author>S Dalmao</author>
<author>T Pitassi</author>
</authors>
<title>Value elimination: Bayesian inference via backtracking search.</title>
<date>2003</date>
<booktitle>In UAI-03,</booktitle>
<pages>20--28</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="8708" citStr="Bacchus et al., 2003" startWordPosition="1446" endWordPosition="1449">005 SMT evaluations (Koehn and Monz, 2005). We train an English language model on the whole training set using the SRILM toolkit (Stolcke, 2002) and train MT models mainly on a 10k sentence pair subset of the ACL training set. We test on the 2000 sentence test set used for the same evaluations. For comparison, we use the MT training program, GIZA++ (Och and Ney, 2003), the phrase-base decoder, Pharaoh (Koehn et al., 2003), and the wordbased decoder, Rewrite (Germann, 2003). For inference we use a backtracking depth-first search inference method with memoization that extends Value Elimination (Bacchus et al., 2003). The same inference engine is used for both training and decoding. As an admissible heuristic for decoding, we compute, for each node V with Conditional Probability Table c, the largest value of c over all possible configurations of V and its parents (Filali and Bilmes, 2006). Decoder BLEU (%) 500 1000 1500 2000 Rewrite 25.3 22.3 21.7 22.01 Pharaoh 20.4 18.1 17.7 18.05 M-HMM 19.9 16.9 15.6 12.5 Table 1: BLEU scores on first 500, 1000, 1500, and 2000 sentences (ordered from shortest to longest) of the ACL05 English-French 2000 sentence test set using a 700k sent train set. The last row is our </context>
</contexts>
<marker>Bacchus, Dalmao, Pitassi, 2003</marker>
<rawString>F. Bacchus, S. Dalmao, and T. Pitassi. 2003. Value elimination: Bayesian inference via backtracking search. In UAI-03, pages 20–28, San Francisco, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4740" citStr="Brown et al., 1993" startWordPosition="749" endWordPosition="752">hing existence” with respect to variable Y if the value of X determines whether Y exists. An example is a fertility variable in IBM Models 3 and above. • Constraints and aggregation: BN semantics can encode various types of constraints between groups of variables (Pearl, 1988). For example, in the construct A —* B +— C where B is observed, B can constrain A and C to be unequal. We extend those semantics to support a more efficient evaluation of constraints under some variable order conditions. 3 GM Representation of IBM MT Models In this section we present a GM representation for IBM model 3 (Brown et al., 1993) in fig. 1. Model 3 is intricate enough to showcase some of the features of our graphical representation but not as complex as, and thus is easier to describe, than model 4. Our choice of representing IBM models is not because we believe they are state of the art MT models— although they are still widely used in producing alignments and as features in log-linear models— but because they provide a good initial testbed for our architecture. The topmost random variable (RV), E, is a hidden switching existence variable corresponding to the length of the English string. The box abutting E includes </context>
<context position="13162" citStr="Brown et al., 1993" startWordPosition="2233" endWordPosition="2236">ignment variables. Table 2 also shows BLEU scores obtained by training equivalent IBM models using GIZA and the standard training regimen of initializing higher models with lower ones (we use the same schedules for our GM training, but only transfer lexical tables). The main observation is that GIZA-trained MHMM, M3 and 4 have about 1% better BLEU scores than their corresponding MDBN versions. We attribute the difference in M3/4 scores to the fact we use a Viterbi-like training procedure (i.e., we consider a single configuration of the hidden variables in EM training) while GIZA uses pegging (Brown et al., 1993) to sum over a set of likely hidden variable configurations in EM. While these preliminary results do not show improved MT performance, nor would we expect them to since they are on simulated IBM models, we find very promising the fact that this general-purpose graphical model-based system produces competitive MT results on a computationally challenging task. 5 Conclusion and Future Work We have described a new probabilistic framework for doing statistical machine translation. We have focused so far on word-based translation. In future work, we intend to implement phrase-based MT models. We al</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dean</author>
<author>K Kanazawa</author>
</authors>
<title>Probabilistic temporal reasoning.</title>
<date>1988</date>
<pages>524--528</pages>
<publisher>AAAI,</publisher>
<contexts>
<context position="2941" citStr="Dean and Kanazawa, 1988" startWordPosition="442" endWordPosition="445">conditional independence relationships between the variables, and parameters of the model are associated with each variable or group thereof. There are many types of graphical models. For example, Bayesian networks use an acyclic directed graph and their parameters are conditional probabilities of each variable given its parents. Various forms of GM and their conditional independence properties are defined in (Lauritzen, 1996). Our graphical representation, which we call Multi-dynamic Bayesian Networks (MDBNs) (Filali and Bilmes, 2006), is a generalization of dynamic Bayesian networks (DBNs) (Dean and Kanazawa, 1988). DBNs are an appropriate representation for sequential (for example, temporal) stochastic processes, but can be very difficult to apply when dependencies have arbitrary time-span and the existence of random variables is contingent on the valProceedings of NAACL HLT 2007, Companion Volume, pages 33–36, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ues of certain variables in the network. In (Filali and Bilmes, 2006), we discuss inference and learning in MDBNs. Here we focus on representation and the evaluation of our new implementation and framework. Below, we sum</context>
</contexts>
<marker>Dean, Kanazawa, 1988</marker>
<rawString>T. Dean and K. Kanazawa. 1988. Probabilistic temporal reasoning. AAAI, pages 524–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Filali</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-dynamic Bayesian networks.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 19.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1108" citStr="Filali and Bilmes, 2006" startWordPosition="154" endWordPosition="157">e connected different-length streams, switching variable existence and dependence mechanisms, and constraint factors. We have implemented a new general-purpose MT training/decoding system in this framework, and have tested this on a variety of existing MT models (including the 4 IBM models), and some novel ones as well, all using Europarl as a test corpus. We describe the semantics of our representation, and present preliminary evaluations, showing that it is possible to prototype novel MT ideas in a short amount of time. 1 Introduction We present a unified graphical model framework based on (Filali and Bilmes, 2006) for statistical machine translation. Graphical models utilize graphical descriptions of probabilistic processes, and are capable of quickly describing a wide variety of different sets of model assumptions. In our approach, either phrases or words can be used as the unit of translation, but as a first step, we have only implemented word-based models since our main goal is to show This material was supported by NSF under Grant No. ISS0326276. 33 the viability of our graphical model representation and new software system. There are several important advantages to a unified probabilistic framewor</context>
<context position="2858" citStr="Filali and Bilmes, 2006" startWordPosition="430" endWordPosition="433">ty distributions over a set of random variables using a graph. The graph specifies conditional independence relationships between the variables, and parameters of the model are associated with each variable or group thereof. There are many types of graphical models. For example, Bayesian networks use an acyclic directed graph and their parameters are conditional probabilities of each variable given its parents. Various forms of GM and their conditional independence properties are defined in (Lauritzen, 1996). Our graphical representation, which we call Multi-dynamic Bayesian Networks (MDBNs) (Filali and Bilmes, 2006), is a generalization of dynamic Bayesian networks (DBNs) (Dean and Kanazawa, 1988). DBNs are an appropriate representation for sequential (for example, temporal) stochastic processes, but can be very difficult to apply when dependencies have arbitrary time-span and the existence of random variables is contingent on the valProceedings of NAACL HLT 2007, Companion Volume, pages 33–36, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ues of certain variables in the network. In (Filali and Bilmes, 2006), we discuss inference and learning in MDBNs. Here we focus on repre</context>
<context position="8985" citStr="Filali and Bilmes, 2006" startWordPosition="1493" endWordPosition="1496">the same evaluations. For comparison, we use the MT training program, GIZA++ (Och and Ney, 2003), the phrase-base decoder, Pharaoh (Koehn et al., 2003), and the wordbased decoder, Rewrite (Germann, 2003). For inference we use a backtracking depth-first search inference method with memoization that extends Value Elimination (Bacchus et al., 2003). The same inference engine is used for both training and decoding. As an admissible heuristic for decoding, we compute, for each node V with Conditional Probability Table c, the largest value of c over all possible configurations of V and its parents (Filali and Bilmes, 2006). Decoder BLEU (%) 500 1000 1500 2000 Rewrite 25.3 22.3 21.7 22.01 Pharaoh 20.4 18.1 17.7 18.05 M-HMM 19.9 16.9 15.6 12.5 Table 1: BLEU scores on first 500, 1000, 1500, and 2000 sentences (ordered from shortest to longest) of the ACL05 English-French 2000 sentence test set using a 700k sent train set. The last row is our MDBN system’s simulation of a M-HMM model. Table 1 compares MT performance between (1) Pharaoh (which uses beam search), (2) our system, and (3) Rewrite (hill-climbing). (1) and (2) make use of a fixed lexical table2 learned using an MHMM model specified using our tool, and ne</context>
</contexts>
<marker>Filali, Bilmes, 2006</marker>
<rawString>Karim Filali and Jeff Bilmes. 2006. Multi-dynamic Bayesian networks. In Advances in Neural Information Processing Systems 19. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Geiger</author>
<author>David Heckerman</author>
</authors>
<title>Knowledge representation and inference in similarity networks and bayesian multinets.</title>
<date>1996</date>
<journal>Artif. Intell.,</journal>
<pages>82--1</pages>
<contexts>
<context position="3887" citStr="Geiger and Heckerman, 1996" startWordPosition="594" endWordPosition="598">April 2007. c�2007 Association for Computational Linguistics ues of certain variables in the network. In (Filali and Bilmes, 2006), we discuss inference and learning in MDBNs. Here we focus on representation and the evaluation of our new implementation and framework. Below, we summarize key features underlying our framework. In section 3, we explain how these features apply to a specific MT model. • Multiple DBNs can be represented along with rules for how they are interconnected — the rule description lengths are fixed (they do not grow with the length of the DBNs). • Switching dependencies (Geiger and Heckerman, 1996): a variable X is a switching parent of Y if X influences what type of dependencies Y has with respect to its other parents, e.g., an alignment variable in IBM Models 1 and 2 is switching. • Switching existence: A variable X is “switching existence” with respect to variable Y if the value of X determines whether Y exists. An example is a fertility variable in IBM Models 3 and above. • Constraints and aggregation: BN semantics can encode various types of constraints between groups of variables (Pearl, 1988). For example, in the construct A —* B +— C where B is observed, B can constrain A and C </context>
</contexts>
<marker>Geiger, Heckerman, 1996</marker>
<rawString>Dan Geiger and David Heckerman. 1996. Knowledge representation and inference in similarity networks and bayesian multinets. Artif. Intell., 82(1-2):45–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Greedy decoding for statistical Machine Translation in almost linear time.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>72--79</pages>
<publisher>ACL.</publisher>
<location>Edmonton, Canada,</location>
<contexts>
<context position="8564" citStr="Germann, 2003" startWordPosition="1427" endWordPosition="1428">l with fertility assignment φ0 = 2, φ, = 3,φ2 = 1, φ3 = 0. periments on a English-French subset of the Europarl corpus used for the ACL 2005 SMT evaluations (Koehn and Monz, 2005). We train an English language model on the whole training set using the SRILM toolkit (Stolcke, 2002) and train MT models mainly on a 10k sentence pair subset of the ACL training set. We test on the 2000 sentence test set used for the same evaluations. For comparison, we use the MT training program, GIZA++ (Och and Ney, 2003), the phrase-base decoder, Pharaoh (Koehn et al., 2003), and the wordbased decoder, Rewrite (Germann, 2003). For inference we use a backtracking depth-first search inference method with memoization that extends Value Elimination (Bacchus et al., 2003). The same inference engine is used for both training and decoding. As an admissible heuristic for decoding, we compute, for each node V with Conditional Probability Table c, the largest value of c over all possible configurations of V and its parents (Filali and Bilmes, 2006). Decoder BLEU (%) 500 1000 1500 2000 Rewrite 25.3 22.3 21.7 22.01 Pharaoh 20.4 18.1 17.7 18.05 M-HMM 19.9 16.9 15.6 12.5 Table 1: BLEU scores on first 500, 1000, 1500, and 2000 s</context>
</contexts>
<marker>Germann, 2003</marker>
<rawString>Ulrich Germann. 2003. Greedy decoding for statistical Machine Translation in almost linear time. In HLT-NAACL, pages 72–79, Edmonton, Canada, May. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Shared task: Statistical machine translation between European languages.</title>
<date>2005</date>
<pages>119--124</pages>
<publisher>ACL.</publisher>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="8129" citStr="Koehn and Monz, 2005" startWordPosition="1346" endWordPosition="1349"> which we provide a preliminary evaluation in this section. Although the framework is perfectly capable of representing phrase-based models, we restrict ourselves to word-based models to show the viability of graphical models for MT and will consider different translation units in future work. We perform MT ex1We refer to the HMM MT model in (Vogel et al., 1996) as M-HMM to avoid any confusion. 34 Figure 1: Unrolled Model 3 graphical model with fertility assignment φ0 = 2, φ, = 3,φ2 = 1, φ3 = 0. periments on a English-French subset of the Europarl corpus used for the ACL 2005 SMT evaluations (Koehn and Monz, 2005). We train an English language model on the whole training set using the SRILM toolkit (Stolcke, 2002) and train MT models mainly on a 10k sentence pair subset of the ACL training set. We test on the 2000 sentence test set used for the same evaluations. For comparison, we use the MT training program, GIZA++ (Och and Ney, 2003), the phrase-base decoder, Pharaoh (Koehn et al., 2003), and the wordbased decoder, Rewrite (Germann, 2003). For inference we use a backtracking depth-first search inference method with memoization that extends Value Elimination (Bacchus et al., 2003). The same inference </context>
</contexts>
<marker>Koehn, Monz, 2005</marker>
<rawString>P. Koehn and C. Monz. 2005. Shared task: Statistical machine translation between European languages. pages 119–124, Ann Arbor, MI, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<contexts>
<context position="8512" citStr="Koehn et al., 2003" startWordPosition="1417" endWordPosition="1420">y confusion. 34 Figure 1: Unrolled Model 3 graphical model with fertility assignment φ0 = 2, φ, = 3,φ2 = 1, φ3 = 0. periments on a English-French subset of the Europarl corpus used for the ACL 2005 SMT evaluations (Koehn and Monz, 2005). We train an English language model on the whole training set using the SRILM toolkit (Stolcke, 2002) and train MT models mainly on a 10k sentence pair subset of the ACL training set. We test on the 2000 sentence test set used for the same evaluations. For comparison, we use the MT training program, GIZA++ (Och and Ney, 2003), the phrase-base decoder, Pharaoh (Koehn et al., 2003), and the wordbased decoder, Rewrite (Germann, 2003). For inference we use a backtracking depth-first search inference method with memoization that extends Value Elimination (Bacchus et al., 2003). The same inference engine is used for both training and decoding. As an admissible heuristic for decoding, we compute, for each node V with Conditional Probability Table c, the largest value of c over all possible configurations of V and its parents (Filali and Bilmes, 2006). Decoder BLEU (%) 500 1000 1500 2000 Rewrite 25.3 22.3 21.7 22.01 Pharaoh 20.4 18.1 17.7 18.05 M-HMM 19.9 16.9 15.6 12.5 Table</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrasebased translation. In HLT-NAACL, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Lauritzen</author>
</authors>
<title>Graphical Models.</title>
<date>1996</date>
<publisher>Oxford Science Publications.</publisher>
<contexts>
<context position="2747" citStr="Lauritzen, 1996" startWordPosition="418" endWordPosition="419">phical Model Framework A Graphical Model (GM) represents a factorization of a family of joint probability distributions over a set of random variables using a graph. The graph specifies conditional independence relationships between the variables, and parameters of the model are associated with each variable or group thereof. There are many types of graphical models. For example, Bayesian networks use an acyclic directed graph and their parameters are conditional probabilities of each variable given its parents. Various forms of GM and their conditional independence properties are defined in (Lauritzen, 1996). Our graphical representation, which we call Multi-dynamic Bayesian Networks (MDBNs) (Filali and Bilmes, 2006), is a generalization of dynamic Bayesian networks (DBNs) (Dean and Kanazawa, 1988). DBNs are an appropriate representation for sequential (for example, temporal) stochastic processes, but can be very difficult to apply when dependencies have arbitrary time-span and the existence of random variables is contingent on the valProceedings of NAACL HLT 2007, Companion Volume, pages 33–36, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ues of certain variables i</context>
</contexts>
<marker>Lauritzen, 1996</marker>
<rawString>S.L. Lauritzen. 1996. Graphical Models. Oxford Science Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8457" citStr="Och and Ney, 2003" startWordPosition="1408" endWordPosition="1411"> MT model in (Vogel et al., 1996) as M-HMM to avoid any confusion. 34 Figure 1: Unrolled Model 3 graphical model with fertility assignment φ0 = 2, φ, = 3,φ2 = 1, φ3 = 0. periments on a English-French subset of the Europarl corpus used for the ACL 2005 SMT evaluations (Koehn and Monz, 2005). We train an English language model on the whole training set using the SRILM toolkit (Stolcke, 2002) and train MT models mainly on a 10k sentence pair subset of the ACL training set. We test on the 2000 sentence test set used for the same evaluations. For comparison, we use the MT training program, GIZA++ (Och and Ney, 2003), the phrase-base decoder, Pharaoh (Koehn et al., 2003), and the wordbased decoder, Rewrite (Germann, 2003). For inference we use a backtracking depth-first search inference method with memoization that extends Value Elimination (Bacchus et al., 2003). The same inference engine is used for both training and decoding. As an admissible heuristic for decoding, we compute, for each node V with Conditional Probability Table c, the largest value of c over all possible configurations of V and its parents (Filali and Bilmes, 2006). Decoder BLEU (%) 500 1000 1500 2000 Rewrite 25.3 22.3 21.7 22.01 Phara</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<booktitle>In Proc. Int. Conf. on Spoken Language Processing.</booktitle>
<publisher>Morgan</publisher>
<contexts>
<context position="4398" citStr="Pearl, 1988" startWordPosition="687" endWordPosition="688">(they do not grow with the length of the DBNs). • Switching dependencies (Geiger and Heckerman, 1996): a variable X is a switching parent of Y if X influences what type of dependencies Y has with respect to its other parents, e.g., an alignment variable in IBM Models 1 and 2 is switching. • Switching existence: A variable X is “switching existence” with respect to variable Y if the value of X determines whether Y exists. An example is a fertility variable in IBM Models 3 and above. • Constraints and aggregation: BN semantics can encode various types of constraints between groups of variables (Pearl, 1988). For example, in the construct A —* B +— C where B is observed, B can constrain A and C to be unequal. We extend those semantics to support a more efficient evaluation of constraints under some variable order conditions. 3 GM Representation of IBM MT Models In this section we present a GM representation for IBM model 3 (Brown et al., 1993) in fig. 1. Model 3 is intricate enough to showcase some of the features of our graphical representation but not as complex as, and thus is easier to describe, than model 4. Our choice of representing IBM models is not because we believe they are state of th</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>J. Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann. A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. Int. Conf. on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="7872" citStr="Vogel et al., 1996" startWordPosition="1296" endWordPosition="1299"> u. The dashed edges connecting the alignment a variables represent an extension to implement an M3/M-HMM hybrid.1 4 Experiments We have developed (in C++) a new entirely selfcontained general-purpose MT training/decoding system based on our framework, of which we provide a preliminary evaluation in this section. Although the framework is perfectly capable of representing phrase-based models, we restrict ourselves to word-based models to show the viability of graphical models for MT and will consider different translation units in future work. We perform MT ex1We refer to the HMM MT model in (Vogel et al., 1996) as M-HMM to avoid any confusion. 34 Figure 1: Unrolled Model 3 graphical model with fertility assignment φ0 = 2, φ, = 3,φ2 = 1, φ3 = 0. periments on a English-French subset of the Europarl corpus used for the ACL 2005 SMT evaluations (Koehn and Monz, 2005). We train an English language model on the whole training set using the SRILM toolkit (Stolcke, 2002) and train MT models mainly on a 10k sentence pair subset of the ACL training set. We test on the 2000 sentence test set used for the same evaluations. For comparison, we use the MT training program, GIZA++ (Och and Ney, 2003), the phrase-ba</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING, pages 836– 841.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>