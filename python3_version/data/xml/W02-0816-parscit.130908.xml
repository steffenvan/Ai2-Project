<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038067">
<note confidence="0.996630666666667">
Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
Disambiguation: Recent Successes and Future Directions, Philadelphia,
July 2002, pp. 109-115. Association for Computational Linguistics.
</note>
<bodyText confidence="0.99913134920635">
the all words English task and the Italian lexi-
cal sample task 1. Both the lexical sample and
all words task shared the gold-standard sense
tagging methodology. The task of WSD is iso-
lated and no application specific machinery is
required. The Dutch all words task additionally
required participants to extract the inventory
which was implicit in the sense tags supplied
with the training data. The problem that faces
the organisers of a task using this methodology
is which inventory should be used, and why.
The Japanese translation task was different in
that participation focussed on a specific and rel-
evant application. The participants were given
a mapping between the Japanese test items and
possible English equivalents. Thus the inven-
tory was selected for a purpose for which WSD
is necessary, and participants were asked to pro-
vide the translations, given this inventory. The
senses that the systems had to choose between
were selected because they corresponded to dis-
tinct translations from Japanese into English.
Sense distinctions with different forms in dif-
ferent languages are more likely to correspond
to coarser grained distinctions than sense dis-
tinctions in a monolingual dictionary (Resnik
and Yarowsky, 2000). Disambiguating these dif-
ferences has relevance for art application. In
contrast, much WSD work has gone on with-
out a goal-driven inventory (Kilgarriff, 1997).
Whilst it is of interest to investigate if a machine
can distinguish senses that lexicographers have
thought up, and also whether human annota-
tors can themselves distinguish these senses, the
questions still remains which inventory should
be used to discover these distinctions, since in-
ventories vary considerably in the distinctions
that they make, and why. If we are not inter-
ested in which human distinctions WSD systems
can make, but we want to apply our systems to
some task then we need some rationale behind
the inventory that we use.
The inventories that we choose bias systems.
One rationale for the use of Hector (Atkins,
1993) for the English task at the original SEN-
lAlthough systems could use freely available sense
tagged data, such as SemCor.
SEVAL was that because no systems were using
it, everyone would be penalised (Kilgarriff and
Rosenzweig, 2000). A mapping between Word-
Net and Hector was provided so that partici-
pants with systems built around WordNet could
share a common mapping. However, the map-
ping itself then creates biases (Agirre et al.,
2000; Carroll and McCarthy, 2000). In SENSE-
VAL-2, WordNet was used as the inventory for
both English tasks, because WordNet is widely
available, and Hector much less so. WordNet
does not however link related senses and there
is no clear level of coarse sense distinctions. This
was probably one of the reasons that the re-
sults for the English SENSEVAL-2 were notice-
ably lower than those of the original SENSEVAL.
There seems to be no easy way out of bias-
ing systems, unless we do not prescribe a spe-
cific inventory. There are systems which disam-
biguate according to the inventories which they
themselves detect in the data (Schiitze, 1998).
An evaluation task which did not assume a par-
ticular inventory might be easier for such sys-
tems than the traditional gold-standard tasks. If
no inventory is supplied then the gold-standard
sense-tagging methodology becomes hard if not
impossible to adopt because of the work involved
for a human annotator recognising a wide vari-
ety of sense tags. SENSEVAL participants were
at liberty to merge senses from the given inven-
tory, and supply more than one sense tag, but
they would then be penalised if the human an-
notators didn&apos;t make the same decisions.
What is needed is at least one more appli-
cation oriented task, where the inventory used
should be as relevant to the overall goal as pos-
sible. Whilst we could still use the traditional
methodology for investigating which sense-tags
from a given inventory are easier to tag, we
could also see the performance of systems in
the context of an application, and explore how
the choice of inventory affects this performance.
The translation methodology is a good one, in
that WSD has been shown to improve perfor-
mance (Brown et al., 1991). It would be fair
to allow systems to compete on more than one
application platform, since some systems are de-
signed for purposes other than machine trans-
lation, such as information retrieval (Schiitze,
1998). We would particularly like to explore
art application which is flexible enough to al-
low systems to respond using their own inven-
tory, though evaluating the system responses
will then be harder.
Another criteria we have for art application-
oriented task is that it should allow participants
to focus ort the WSD task within the overall
application, rather than trying to evaluate too
many different subtasks at the same time. For
the machine translation task, effort was focussed
just ort translation of words, thus focusing ort
the WSD task. One way to evaluate WSD in the
context of art application would be where a core
system was provided which required a WSD mod-
ule and could read in the responses from partic-
ipants&apos; systems. The difference in performance
with and without the WSD module could then
be measured. However, if it is the case that or-
ganisers are not able to provide such a system
for participants, then it would certainly not be
fair to expect participants to produce the archi-
tecture themselves.
So what applications are there, in addition to
the machine translation task, that would be suit-
able for objective evaluation without requiring
a whole host of other activities? WSD systems
have been suggested as being of use for many ap-
plications, although aside from machine transla-
tion the benefits have yet to be proved.
</bodyText>
<sectionHeader confidence="0.610753" genericHeader="method">
2 Possible Applications for WSD
</sectionHeader>
<bodyText confidence="0.999931830508475">
What applications can WSD be applied to and
which of these would be suitable for art evalu-
ation exercise? Kilgarriff (1997) identified ma-
chine translation, lexicography and information
retrieval as being applications which can benefit
from WSD. Machine translation is the clearest
case where WSD is required for art NLP appli-
cation. Whilst WSD has been shown to bene-
fit information retrieval (Schiitze, 1998), the ef-
fect is diminished when longer queries are sup-
plied. The combination of words in the query
go a long way to ranking documents in order
of sense relevance. Lexicography is not art NLP
task, but one where sense-tagged data is useful
to lexicographers developing dictionaries. The
traditional sense tagging methodology of SEN-
SEVAL, with the production of gold-standard re-
sources clearly feeds into this. Presumably it is
also useful for lexicographers to be aware where
inter-tagger agreement is low, and where and
why WSD systems fall down ort the same items.
There are other applications which might ben-
efit from WSD. One application which we think
would benefit from WSD is text simplification,
which is just one variation of a more general lexi-
cal substitution task where a word is replaced by
another word for a particular application. For
example, in PSET (Devlin and Tait, 1998) the
main enterprise was to simplify newspaper text
for aphasic adults. One subtask was to sub-
stitute words with more familiar, or more fre-
quent, synonyms, for example learn might be
used in place of memorize. It makes clear sense
to substitute with synonyms from the appropri-
ate sense rather than from arty of the synonyms
for the target word form. Thus if we are going
to simplify scheme in the sentence:
A recent government study singled out the
scheme as an example to others
one would want to use strategy rather than dodge
as a replacement for scheme in this context.
Having identified the sense of the target word,
lexical choice is required for generating the re-
placement as a given sense may have more than
one synonym. Typically one would want words
which were near synonyms, or less specific re-
placements. For text simplification there is a
particular agenda as regards the requirements of
the word used for replacement. The word should
be easier to understand for the target audience.
There are other possible uses for lexical sub-
stitution. Text summarisation might benefit
from a module which identifies the sense of a
word and is able to suggest a number of alterna-
tive expressions (Banko et al., 2000). Informa-
tion retrieval may also benefit from lexical sub-
stitution in term expansion, assuming the user
is interested in documents without the exact key
words supplied.
</bodyText>
<sectionHeader confidence="0.971422" genericHeader="method">
3 Lexical Substitution as an
Application Oriented WSD Task
</sectionHeader>
<bodyText confidence="0.999973111111111">
The text substitution task is rather like the
translation task, in that there is a mapping be-
tween the target form and one or more sets of
substitutions. Cases with only one set will arise
for monosemous words. Whilst we could con-
strain the systems to select from a given inven-
tory of sets, there is a more appealing option of
letting them generate the sets themselves. This
would create additional work for participants,
although they could use man-made inventories
such as WordNet. Crucially, it would allow sys-
tems which produce their own inventories into
the arena, and permit users of predefined inven-
tories to merge senses and create coarser grained
inventories where it makes sense to do so. Pos-
sibilities for the inventory and how we might ac-
tually evaluate the system answers are outlined
in the next two subsections.
</bodyText>
<subsectionHeader confidence="0.994799">
3.1 The Inventory
</subsectionHeader>
<bodyText confidence="0.999884">
For a lexical substitution task, we can choose
whether we restrict users to a given inventory,
or allow them to select their own. Whilst not
specifying a predefined inventory makes human
annotation much harder we contend that this
would reduce bias and encourage participation
from users who build their own classifications.
Systems that deal in semantic space (Schiitze,
1998) could then participate, as well as systems
that are committed to a given inventory.
The substitutions will depend ort the type of
inventory used. As regards man-made invento-
ries, a thesaurus like WordNet lends itself more
easily to a task like this than a dictionary like
Hector, since it is organised ort the basis of se-
mantic relationships, rather than alphabetically,
though useful replacements might well be found
in dictionary definitions. WordNet provides syn-
onyms, or near synonyms together in synsets,
and these synsets are related to other synsets
with relationships such as ltyponymy. For verbs
and nouns one could use synonyms, or words in
ltypernym classes. For adjectives one could use
the &amp;quot;similar to&amp;quot; relation. For many word senses
in WordNet there are no synonyms supplied.
</bodyText>
<table confidence="0.998543">
ltypernyms
word sensel sense2 sense3
cascade arrange descend
discordant discrepant dissonant
church service building faith
</table>
<tableCaption confidence="0.9563225">
Table 1: Hypernyms for different senses of target
words
</tableCaption>
<bodyText confidence="0.9989306">
Instead ltypernyms might provide adequate re-
placements. For example, table 1 shows alter-
native ltypernyms for some target words from
the SENSEVAL-2 data, depending ort the sense
ill which they are used. 2
</bodyText>
<subsectionHeader confidence="0.999144">
3.2 Evaluating the Responses
</subsectionHeader>
<bodyText confidence="0.999774473684211">
So how do we evaluate responses to a lexical
substitution task? We would need to provide
either a gold-standard of possible substitutions
or a task-based evaluation. Possibilities for task-
based evaluation might be readability of the out-
put, as determined by human judges, or perfor-
mance ort art information retrieval task. We dis-
cuss possibilities for producing a gold-standard
and leave open for discussion the question of
whether it would be appropriate, and indeed
possible to supply art application for evaluation.
Providing a gold-standard for evaluating a
wide variety of possible lexical replacements is
harder than specifying art exact match criterion
for senses, at least in terms of scoring. How-
ever, for the human annotators at least it may
be that selecting replacement words might be
easier than identifying senses. Many issues re-
main for evaluation:
</bodyText>
<listItem confidence="0.967648111111111">
1. could annotators be asked to supply a gold-
standard and training data in advance of
the evaluation?
2. do we have a binary response to whether
something is a suitable replacement, or can
we rank lexical choice?
3. should participants choose more than one
replacement, and should these be seen dis-
junctively, or conjunctively?
</listItem>
<bodyText confidence="0.97415948">
2We use the prerelease 1.7 version of WordNet used
for SENSEVAL-2 in this paper.
Once we remove the restriction to use a given
inventory then we need to allow for a wide-range
of responses. We could provide annotators with
potential replacements and get them to select
from these in advance, permitting them to add
their own, however we should expect to have to
check systems&apos; responses which not are in the
set of potential replacements after submission.
The criteria that are used to judge responses
is critical. It may well be that a good substi-
tute in terms of the senses of the word may not
fit syntactically. For example, one sense of ser-
vice, in WordNet is listed with Itypernyms assist
help assistance aid and a gloss: an act of help
or assistance; &amp;quot;he did them a service&amp;quot;. Whilst
the Itypernyms might bear a strong semantic re-
semblance to this sense of service, they would
be syntactically anomalous in a phrase such as
did them a service. There are also collocational
constraints to deal with, strong would be a bet-
ter substitute for potent than powerful in the
phrase potent tea. We could either require our
systems to meet syntactic and collocational cri-
teria, or instruct our annotators to ignore these
constraints if we want to put as little non WSD
burden on participants. If we opt for a task-
based evaluation, rather than a gold-standard,
then the participants would need to consider
these constraints. The choices for lexical sub-
stitution within a full application would depend
on the goals of that application, for example
whether the text is to be summarized, simpli-
fied or expanded.
For a gold-standard evaluation the criteria we
might ask the annotators to use is semantic co-
hesion of the target and the replacement. We
could avoid grading responses and count any
valid substitution as correct where valid substi-
tutions are those which are semantically close,
not antonyms such as cold in place of hot, not
more specific than the target e.g. not alsation
for dog, and not too general so as to be ambigu-
ous e.g. thing for chicken.
There are several issues as to whether and
how participants should be allowed to supply
multiple choices for a given test item. lit the
previous SENSEVALS, participants were allowed
to supply more than one sense tag per item,
and allowed to specify a probability distribution
with their choices or accept a default uniform
distribution. If more than one sense tag was
correct, then the scoring was performed for all
tags, since there was no way for participants to
specify whether the answers were expected to
be conjuncts (both apply) or disjuncts (one of
the disjuncts applies, but the system cannot dis-
criminate between them). We could likewise al-
low more than one replacement. Again the issue
arises, are these answers to be seen as conjuncts
or disjuncts? It makes perfect sense that a sys-
tem might want to supply both. We advocate
that the ambiguity be resolved by asking par-
ticipants to supply brackets around conjuncts,
and attach a probability score to the disjuncts.
Thus, for example door was marked in the gold-
standard as having two senses in the English all
words task.
d00 d00.s04.t15 door%1:06:00:: door%1:06:01::
in
The parishioners of St. Michael and All Angels
stop to chat at the church door,. . .
In SENSEVAL-2 A participant might have re-
sponded:
</bodyText>
<equation confidence="0.564512714285714">
d00 d00.s04.t15 door%1:06:00::
or
d00 d00.s04.t15 door%1:06:01::
or
d00 d00.s04.t15 door%1:06:00:: door%1:06:01::
or
d00 d00.s04.t15 door%1:06:00:: 0.6 door%1:06:01:: 0.4
</equation>
<bodyText confidence="0.99981625">
We suggest that participants could bracket the
choices to indicate a conjunct or leave the
choices unbracketed as before to indicate a dis-
junct, with attached probability distribution if
the default is not required.
Lexical substitution responses using related
senses in the WordNet inventory pictured in fig-
ure I might then look like :
</bodyText>
<table confidence="0.890146285714286">
d00 d00.s04.t15 barrier doorway
or
d00 d00.s04.t15 barrier 0.6 doorway 0.4
or
d00 d00.s04.t15 (barrier doorway)
or
d00 d00.s04.t15 (barrier doorway threshold room_access entrance)
</table>
<bodyText confidence="0.998565">
If more than one replacement for a given tar-
get is offered, we should divide the credit for
the test item (i), by the number of replacements
offered. The probability distribution supplied
with disjuncts could be used to weight this, or
the default uniform distribution:
</bodyText>
<figure confidence="0.997726809523809">
artefact artifact
moveable_barrier
door doorway
threshold room_access
door
entity
structure
construction
obstructorimpediment
obstruction impedimenta
obstructor
barrier
way
access
approach
entrance entranceway
entry entree entryway
physical_objectobject
unit
whole
whole_thing
</figure>
<sectionHeader confidence="0.821062" genericHeader="method">
6 Further Work Needed
</sectionHeader>
<bodyText confidence="0.999970545454546">
Further work is required to determine if a lexical
substitution task, such as the one we propose, is
feasible. We need to investigate if human anno-
tators could be asked to provide in advance a
gold-standard of possible substitutes, given ac-
cess to appropriate inventories. We need to as-
certain how time consuming and costly this pro-
cess would be. Is it more or less time-consuming
than sense tagging? Is selecting from a pool of
suggested replacements a valid possibility? Can
human annotators readily ignore syntactic and
collocational constraints in favour of semantic
resemblance or would it be appropriate to re-
quire participants to adhere to such constraints?
How likely is it that a good replacement would
not be thought of in advance, thereby requiring
a thorough check ort non-valid responses after-
wards and could we supply training data for a
task such as this?
We acknowledge that our examples have been
from English. Work is required to see whether
and how this approach might work in other lan-
guages, and whether man-made resources exist
which might be appropriate to the task.
One large by-product of the past SENSEVAL
exercises has been the production of sense la-
belled data sets for further evaluation. If a good
deal of time is to be set up into creating data re-
sources supplied with valid lexical replacements
it would be good to know if and how such a re-
source might be used by the WSD community,
other researchers in NLP and as a resource for
lexicographers.
</bodyText>
<sectionHeader confidence="0.998474" genericHeader="method">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9897394">
This work was supported by the EPSRC-funded
RASP project (grant GR/N36493), and the Eli
5th Framework project MEANING — Develop-
ing Multilingual Web-scale Language Technolo-
gies (IST-2001-34460).
</bodyText>
<sectionHeader confidence="0.99559" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998933595744681">
Eneko Agirre, German Rigau, and J. Atserias. 2000.
Combining supervised and unsupervised lexical
knowledge methods for word sense disambigua-
tion. Computers and the Humanities. Senseval
Special Issue, 34(1-2):103-108.
Sue Atkins. 1993. Tools for computer-aided lexicog-
raphy: the Hector project. In Papers in Compu-
tational Lexicography: COMPLEX 93, Budapest.
Michele Banko, Vibhu Mittal, and Michael Wit-
brock. 2000. Headline generation based on statis-
tical translation. In Proceedings of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pages 318-325.
P. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L.
Mercer. 1991. Word-sense disambiguation using
statistical methods. In Proceedings of the 29th An-
nual Meeting of the Association for Computational
Linguistics, pages 264-270.
John Carroll and Diana McCarthy. 2000. Word
sense disambiguation using automatically ac-
quired verbal preferences. Computers and the Hu-
manities. Senseval Special Issue, 34(1-2):109-114.
Siobhan Devlin and John Tait. 1998. The use of a
psycholinguistic database in the simplification of
text for aphasic readers. In John Nerbonne, ed-
itor, Linguistic Databases, volume CSLI Lecture
Notes Number 77, pages 161-173. CSLI Publica-
tions, Stanford CA.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
Framework and results for english SENSEVAL.
Computers and the Humanities. Senseval Special
Issue, 34(1-2):15-48.
Adam Kilgarriff. 1997. What is word sense dis-
ambiguation good for? In Proceedings of Natu-
ral Language Processing in the Pacific Rim, pages
209-214.
Adam Kilgarriff. 1998. Gold standard datasets for
evaluating word sense disambiguation programs.
Computer Speech and Language, 12(3):453-472.
Philip Resnik and David Yarowsky. 2000. Distin-
guishing systems and distinguishing senses: New
evaluation methods for word sense disambigua-
tion. Natural Language Engineering, 5(3):113—
133.
Hinrich Schiitze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97—
123.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001418">
<note confidence="0.73169775">Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Philadelphia, July 2002, pp. 109-115. Association for Computational Linguistics. the all words English task and the Italian lexi-</note>
<abstract confidence="0.999680262790699">sample task Both the lexical sample and all words task shared the gold-standard sense methodology. The task of is isolated and no application specific machinery is required. The Dutch all words task additionally required participants to extract the inventory which was implicit in the sense tags supplied with the training data. The problem that faces the organisers of a task using this methodology is which inventory should be used, and why. The Japanese translation task was different in that participation focussed on a specific and relevant application. The participants were given a mapping between the Japanese test items and possible English equivalents. Thus the invenwas selected for a purpose for which is necessary, and participants were asked to provide the translations, given this inventory. The senses that the systems had to choose between were selected because they corresponded to distinct translations from Japanese into English. Sense distinctions with different forms in different languages are more likely to correspond to coarser grained distinctions than sense distinctions in a monolingual dictionary (Resnik and Yarowsky, 2000). Disambiguating these differences has relevance for art application. In much has gone on without a goal-driven inventory (Kilgarriff, 1997). Whilst it is of interest to investigate if a machine can distinguish senses that lexicographers have thought up, and also whether human annotators can themselves distinguish these senses, the questions still remains which inventory should be used to discover these distinctions, since inventories vary considerably in the distinctions that they make, and why. If we are not interin which human distinctions can make, but we want to apply our systems to some task then we need some rationale behind the inventory that we use. The inventories that we choose bias systems. One rationale for the use of Hector (Atkins, for the English task at the original SENsystems could use freely available sense tagged data, such as SemCor. that because no systems were using it, everyone would be penalised (Kilgarriff and Rosenzweig, 2000). A mapping between Word- Net and Hector was provided so that participants with systems built around WordNet could share a common mapping. However, the mapping itself then creates biases (Agirre et al., 2000; Carroll and McCarthy, 2000). In SENSE- VAL-2, WordNet was used as the inventory for both English tasks, because WordNet is widely available, and Hector much less so. WordNet does not however link related senses and there is no clear level of coarse sense distinctions. This was probably one of the reasons that the refor the English noticeably lower than those of the original SENSEVAL. There seems to be no easy way out of biasing systems, unless we do not prescribe a specific inventory. There are systems which disambiguate according to the inventories which they themselves detect in the data (Schiitze, 1998). An evaluation task which did not assume a particular inventory might be easier for such systems than the traditional gold-standard tasks. If no inventory is supplied then the gold-standard sense-tagging methodology becomes hard if not impossible to adopt because of the work involved for a human annotator recognising a wide variety of sense tags. SENSEVAL participants were at liberty to merge senses from the given inventory, and supply more than one sense tag, but they would then be penalised if the human annotators didn&apos;t make the same decisions. What is needed is at least one more application oriented task, where the inventory used should be as relevant to the overall goal as possible. Whilst we could still use the traditional methodology for investigating which sense-tags from a given inventory are easier to tag, we could also see the performance of systems in the context of an application, and explore how the choice of inventory affects this performance. The translation methodology is a good one, in been shown to improve performance (Brown et al., 1991). It would be fair to allow systems to compete on more than one application platform, since some systems are defor purposes other than machine translation, such as information retrieval (Schiitze, 1998). We would particularly like to explore art application which is flexible enough to allow systems to respond using their own inventory, though evaluating the system responses will then be harder. Another criteria we have for art applicationoriented task is that it should allow participants to focus ort the WSD task within the overall application, rather than trying to evaluate too many different subtasks at the same time. For the machine translation task, effort was focussed just ort translation of words, thus focusing ort the WSD task. One way to evaluate WSD in the context of art application would be where a core system was provided which required a WSD module and could read in the responses from participants&apos; systems. The difference in performance with and without the WSD module could then be measured. However, if it is the case that organisers are not able to provide such a system for participants, then it would certainly not be fair to expect participants to produce the architecture themselves. So what applications are there, in addition to the machine translation task, that would be suitable for objective evaluation without requiring a whole host of other activities? WSD systems have been suggested as being of use for many applications, although aside from machine translation the benefits have yet to be proved. 2 Possible Applications for WSD What applications can WSD be applied to and which of these would be suitable for art evaluation exercise? Kilgarriff (1997) identified machine translation, lexicography and information retrieval as being applications which can benefit from WSD. Machine translation is the clearest case where WSD is required for art NLP application. Whilst WSD has been shown to benefit information retrieval (Schiitze, 1998), the effect is diminished when longer queries are supplied. The combination of words in the query go a long way to ranking documents in order of sense relevance. Lexicography is not art NLP task, but one where sense-tagged data is useful to lexicographers developing dictionaries. The traditional sense tagging methodology of SEN- SEVAL, with the production of gold-standard resources clearly feeds into this. Presumably it is also useful for lexicographers to be aware where inter-tagger agreement is low, and where and why WSD systems fall down ort the same items. There are other applications which might benefit from WSD. One application which we think would benefit from WSD is text simplification, which is just one variation of a more general lexical substitution task where a word is replaced by another word for a particular application. For example, in PSET (Devlin and Tait, 1998) the main enterprise was to simplify newspaper text for aphasic adults. One subtask was to substitute words with more familiar, or more fresynonyms, for example be in place of makes clear sense to substitute with synonyms from the appropriate sense rather than from arty of the synonyms for the target word form. Thus if we are going simplify the sentence: government study singled out the scheme as an example to others would want to use than a replacement for this context. Having identified the sense of the target word, lexical choice is required for generating the replacement as a given sense may have more than one synonym. Typically one would want words which were near synonyms, or less specific replacements. For text simplification there is a particular agenda as regards the requirements of the word used for replacement. The word should be easier to understand for the target audience. There are other possible uses for lexical substitution. Text summarisation might benefit from a module which identifies the sense of a word and is able to suggest a number of alternative expressions (Banko et al., 2000). Information retrieval may also benefit from lexical substitution in term expansion, assuming the user is interested in documents without the exact key words supplied. 3 Lexical Substitution as an Application Oriented WSD Task The text substitution task is rather like the translation task, in that there is a mapping between the target form and one or more sets of substitutions. Cases with only one set will arise for monosemous words. Whilst we could constrain the systems to select from a given inventory of sets, there is a more appealing option of letting them generate the sets themselves. This would create additional work for participants, although they could use man-made inventories such as WordNet. Crucially, it would allow systems which produce their own inventories into the arena, and permit users of predefined inventories to merge senses and create coarser grained inventories where it makes sense to do so. Possibilities for the inventory and how we might actually evaluate the system answers are outlined in the next two subsections. 3.1 The Inventory For a lexical substitution task, we can choose whether we restrict users to a given inventory, or allow them to select their own. Whilst not specifying a predefined inventory makes human annotation much harder we contend that this would reduce bias and encourage participation from users who build their own classifications. Systems that deal in semantic space (Schiitze, 1998) could then participate, as well as systems that are committed to a given inventory. The substitutions will depend ort the type of inventory used. As regards man-made inventories, a thesaurus like WordNet lends itself more easily to a task like this than a dictionary like Hector, since it is organised ort the basis of semantic relationships, rather than alphabetically, though useful replacements might well be found in dictionary definitions. WordNet provides synonyms, or near synonyms together in synsets, and these synsets are related to other synsets with relationships such as ltyponymy. For verbs and nouns one could use synonyms, or words in ltypernym classes. For adjectives one could use the &amp;quot;similar to&amp;quot; relation. For many word senses in WordNet there are no synonyms supplied. ltypernyms word sensel sense2 sense3 cascade arrange descend discordant discrepant dissonant church service building faith Table 1: Hypernyms for different senses of target words Instead ltypernyms might provide adequate replacements. For example, table 1 shows alternative ltypernyms for some target words from depending ort the sense which they are used. 2 3.2 Evaluating the Responses So how do we evaluate responses to a lexical substitution task? We would need to provide either a gold-standard of possible substitutions or a task-based evaluation. Possibilities for taskbased evaluation might be readability of the output, as determined by human judges, or performance ort art information retrieval task. We discuss possibilities for producing a gold-standard and leave open for discussion the question of whether it would be appropriate, and indeed possible to supply art application for evaluation. Providing a gold-standard for evaluating a wide variety of possible lexical replacements is harder than specifying art exact match criterion for senses, at least in terms of scoring. However, for the human annotators at least it may be that selecting replacement words might be easier than identifying senses. Many issues remain for evaluation: 1. could annotators be asked to supply a goldstandard and training data in advance of the evaluation? 2. do we have a binary response to whether something is a suitable replacement, or can we rank lexical choice? 3. should participants choose more than one replacement, and should these be seen disjunctively, or conjunctively? use the prerelease 1.7 version of WordNet used for SENSEVAL-2 in this paper. Once we remove the restriction to use a given inventory then we need to allow for a wide-range of responses. We could provide annotators with potential replacements and get them to select from these in advance, permitting them to add their own, however we should expect to have to check systems&apos; responses which not are in the set of potential replacements after submission. The criteria that are used to judge responses is critical. It may well be that a good substitute in terms of the senses of the word may not syntactically. For example, one sense of ser- WordNet is listed with Itypernyms assistance aid a gloss: act of help assistance; &amp;quot;he did them a service&amp;quot;. the Itypernyms might bear a strong semantic reto this sense of would be syntactically anomalous in a phrase such as them a service. are also collocational to deal with, be a betsubstitute for the tea. could either require our systems to meet syntactic and collocational criteria, or instruct our annotators to ignore these constraints if we want to put as little non WSD burden on participants. If we opt for a taskbased evaluation, rather than a gold-standard, then the participants would need to consider these constraints. The choices for lexical substitution within a full application would depend on the goals of that application, for example whether the text is to be summarized, simplified or expanded. For a gold-standard evaluation the criteria we might ask the annotators to use is semantic cohesion of the target and the replacement. We could avoid grading responses and count any valid substitution as correct where valid substitutions are those which are semantically close, antonyms such as place of specific than the target e.g. not not too general so as to be ambigue.g. There are several issues as to whether and how participants should be allowed to supply multiple choices for a given test item. lit the previous SENSEVALS, participants were allowed to supply more than one sense tag per item, and allowed to specify a probability distribution with their choices or accept a default uniform distribution. If more than one sense tag was correct, then the scoring was performed for all tags, since there was no way for participants to specify whether the answers were expected to be conjuncts (both apply) or disjuncts (one of the disjuncts applies, but the system cannot discriminate between them). We could likewise allow more than one replacement. Again the issue arises, are these answers to be seen as conjuncts or disjuncts? It makes perfect sense that a system might want to supply both. We advocate that the ambiguity be resolved by asking participants to supply brackets around conjuncts, and attach a probability score to the disjuncts. for example marked in the goldstandard as having two senses in the English all words task. d00 d00.s04.t15 door%1:06:00:: door%1:06:01:: in The parishioners of St. Michael and All Angels stop to chat at the church door,. . . participant might have responded: d00 d00.s04.t15 door%1:06:00:: or d00 d00.s04.t15 door%1:06:01:: or d00 d00.s04.t15 door%1:06:00:: door%1:06:01:: or d00 d00.s04.t15 door%1:06:00:: 0.6 door%1:06:01:: 0.4 We suggest that participants could bracket the choices to indicate a conjunct or leave the choices unbracketed as before to indicate a disjunct, with attached probability distribution if the default is not required. Lexical substitution responses using related senses in the WordNet inventory pictured in figure I might then look like : d00 d00.s04.t15 barrier doorway or d00 d00.s04.t15 barrier 0.6 doorway 0.4 or d00 d00.s04.t15 (barrier doorway) or d00 d00.s04.t15 (barrier doorway threshold room_access entrance) If more than one replacement for a given target is offered, we should divide the credit for the test item (i), by the number of replacements offered. The probability distribution supplied with disjuncts could be used to weight this, or the default uniform distribution: artefact artifact moveable_barrier door doorway threshold room_access door entity structure construction obstructorimpediment obstruction impedimenta obstructor barrier way access approach entrance entranceway entry entree entryway physical_objectobject unit whole whole_thing 6 Further Work Needed Further work is required to determine if a lexical substitution task, such as the one we propose, is feasible. We need to investigate if human annotators could be asked to provide in advance a gold-standard of possible substitutes, given access to appropriate inventories. We need to ascertain how time consuming and costly this process would be. Is it more or less time-consuming than sense tagging? Is selecting from a pool of suggested replacements a valid possibility? Can human annotators readily ignore syntactic and collocational constraints in favour of semantic resemblance or would it be appropriate to require participants to adhere to such constraints? How likely is it that a good replacement would not be thought of in advance, thereby requiring a thorough check ort non-valid responses afterwards and could we supply training data for a task such as this? We acknowledge that our examples have been from English. Work is required to see whether and how this approach might work in other languages, and whether man-made resources exist which might be appropriate to the task. One large by-product of the past SENSEVAL exercises has been the production of sense labelled data sets for further evaluation. If a good deal of time is to be set up into creating data resources supplied with valid lexical replacements it would be good to know if and how such a resource might be used by the WSD community, other researchers in NLP and as a resource for lexicographers.</abstract>
<note confidence="0.888331733333333">7 Acknowledgments This work was supported by the EPSRC-funded RASP project (grant GR/N36493), and the Eli 5th Framework project MEANING — Developing Multilingual Web-scale Language Technologies (IST-2001-34460). References Eneko Agirre, German Rigau, and J. Atserias. 2000. Combining supervised and unsupervised lexical methods for word sense disambiguaand the Humanities. Senseval Issue, Sue Atkins. 1993. Tools for computer-aided lexicogthe Hector project. In in Compu- Lexicography: COMPLEX 93,</note>
<author confidence="0.952412">Michele Banko</author>
<author confidence="0.952412">Vibhu Mittal</author>
<author confidence="0.952412">Michael Wit-</author>
<abstract confidence="0.893187533333333">brock. 2000. Headline generation based on statistranslation. In of the 38th Annual Meeting of the Association for Computational 318-325. P. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mercer. 1991. Word-sense disambiguation using methods. In of the 29th Annual Meeting of the Association for Computational 264-270. John Carroll and Diana McCarthy. 2000. Word sense disambiguation using automatically acverbal preferences. and the Hu- Senseval Special Issue, Siobhan Devlin and John Tait. 1998. The use of a psycholinguistic database in the simplification of</abstract>
<note confidence="0.626664153846154">text for aphasic readers. In John Nerbonne, ed- Databases, CSLI Lecture Notes Number 77, pages 161-173. CSLI Publications, Stanford CA. Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework and results for english SENSEVAL. Computers and the Humanities. Senseval Special Adam Kilgarriff. 1997. What is word sense disgood for? In of Natu- Language Processing in the Pacific Rim, 209-214. Adam Kilgarriff. 1998. Gold standard datasets for evaluating word sense disambiguation programs.</note>
<title confidence="0.597743">Speech and Language,</title>
<author confidence="0.621644">Distin-</author>
<abstract confidence="0.796336">guishing systems and distinguishing senses: New evaluation methods for word sense disambigua-</abstract>
<note confidence="0.783091">Language Engineering, 133. Hinrich Schiitze. 1998. Automatic word sense dis- Linguistics, 123.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>German Rigau</author>
<author>J Atserias</author>
</authors>
<title>Combining supervised and unsupervised lexical knowledge methods for word sense disambiguation. Computers and the Humanities. Senseval Special Issue,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="2659" citStr="Agirre et al., 2000" startWordPosition="416" endWordPosition="419"> our systems to some task then we need some rationale behind the inventory that we use. The inventories that we choose bias systems. One rationale for the use of Hector (Atkins, 1993) for the English task at the original SENlAlthough systems could use freely available sense tagged data, such as SemCor. SEVAL was that because no systems were using it, everyone would be penalised (Kilgarriff and Rosenzweig, 2000). A mapping between WordNet and Hector was provided so that participants with systems built around WordNet could share a common mapping. However, the mapping itself then creates biases (Agirre et al., 2000; Carroll and McCarthy, 2000). In SENSEVAL-2, WordNet was used as the inventory for both English tasks, because WordNet is widely available, and Hector much less so. WordNet does not however link related senses and there is no clear level of coarse sense distinctions. This was probably one of the reasons that the results for the English SENSEVAL-2 were noticeably lower than those of the original SENSEVAL. There seems to be no easy way out of biasing systems, unless we do not prescribe a specific inventory. There are systems which disambiguate according to the inventories which they themselves </context>
</contexts>
<marker>Agirre, Rigau, Atserias, 2000</marker>
<rawString>Eneko Agirre, German Rigau, and J. Atserias. 2000. Combining supervised and unsupervised lexical knowledge methods for word sense disambiguation. Computers and the Humanities. Senseval Special Issue, 34(1-2):103-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sue Atkins</author>
</authors>
<title>Tools for computer-aided lexicography: the Hector project.</title>
<date>1993</date>
<booktitle>In Papers in Computational Lexicography: COMPLEX 93,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="2223" citStr="Atkins, 1993" startWordPosition="346" endWordPosition="347">to investigate if a machine can distinguish senses that lexicographers have thought up, and also whether human annotators can themselves distinguish these senses, the questions still remains which inventory should be used to discover these distinctions, since inventories vary considerably in the distinctions that they make, and why. If we are not interested in which human distinctions WSD systems can make, but we want to apply our systems to some task then we need some rationale behind the inventory that we use. The inventories that we choose bias systems. One rationale for the use of Hector (Atkins, 1993) for the English task at the original SENlAlthough systems could use freely available sense tagged data, such as SemCor. SEVAL was that because no systems were using it, everyone would be penalised (Kilgarriff and Rosenzweig, 2000). A mapping between WordNet and Hector was provided so that participants with systems built around WordNet could share a common mapping. However, the mapping itself then creates biases (Agirre et al., 2000; Carroll and McCarthy, 2000). In SENSEVAL-2, WordNet was used as the inventory for both English tasks, because WordNet is widely available, and Hector much less so</context>
</contexts>
<marker>Atkins, 1993</marker>
<rawString>Sue Atkins. 1993. Tools for computer-aided lexicography: the Hector project. In Papers in Computational Lexicography: COMPLEX 93, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu Mittal</author>
<author>Michael Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>318--325</pages>
<contexts>
<context position="8475" citStr="Banko et al., 2000" startWordPosition="1398" endWordPosition="1401">ense of the target word, lexical choice is required for generating the replacement as a given sense may have more than one synonym. Typically one would want words which were near synonyms, or less specific replacements. For text simplification there is a particular agenda as regards the requirements of the word used for replacement. The word should be easier to understand for the target audience. There are other possible uses for lexical substitution. Text summarisation might benefit from a module which identifies the sense of a word and is able to suggest a number of alternative expressions (Banko et al., 2000). Information retrieval may also benefit from lexical substitution in term expansion, assuming the user is interested in documents without the exact key words supplied. 3 Lexical Substitution as an Application Oriented WSD Task The text substitution task is rather like the translation task, in that there is a mapping between the target form and one or more sets of substitutions. Cases with only one set will arise for monosemous words. Whilst we could constrain the systems to select from a given inventory of sets, there is a more appealing option of letting them generate the sets themselves. Th</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Michele Banko, Vibhu Mittal, and Michael Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 318-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>Word-sense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>264--270</pages>
<contexts>
<context position="4381" citStr="Brown et al., 1991" startWordPosition="710" endWordPosition="713">t they would then be penalised if the human annotators didn&apos;t make the same decisions. What is needed is at least one more application oriented task, where the inventory used should be as relevant to the overall goal as possible. Whilst we could still use the traditional methodology for investigating which sense-tags from a given inventory are easier to tag, we could also see the performance of systems in the context of an application, and explore how the choice of inventory affects this performance. The translation methodology is a good one, in that WSD has been shown to improve performance (Brown et al., 1991). It would be fair to allow systems to compete on more than one application platform, since some systems are designed for purposes other than machine translation, such as information retrieval (Schiitze, 1998). We would particularly like to explore art application which is flexible enough to allow systems to respond using their own inventory, though evaluating the system responses will then be harder. Another criteria we have for art applicationoriented task is that it should allow participants to focus ort the WSD task within the overall application, rather than trying to evaluate too many di</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>P. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mercer. 1991. Word-sense disambiguation using statistical methods. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 264-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Diana McCarthy</author>
</authors>
<title>Word sense disambiguation using automatically acquired verbal preferences. Computers and the Humanities. Senseval Special Issue,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="2688" citStr="Carroll and McCarthy, 2000" startWordPosition="420" endWordPosition="423">task then we need some rationale behind the inventory that we use. The inventories that we choose bias systems. One rationale for the use of Hector (Atkins, 1993) for the English task at the original SENlAlthough systems could use freely available sense tagged data, such as SemCor. SEVAL was that because no systems were using it, everyone would be penalised (Kilgarriff and Rosenzweig, 2000). A mapping between WordNet and Hector was provided so that participants with systems built around WordNet could share a common mapping. However, the mapping itself then creates biases (Agirre et al., 2000; Carroll and McCarthy, 2000). In SENSEVAL-2, WordNet was used as the inventory for both English tasks, because WordNet is widely available, and Hector much less so. WordNet does not however link related senses and there is no clear level of coarse sense distinctions. This was probably one of the reasons that the results for the English SENSEVAL-2 were noticeably lower than those of the original SENSEVAL. There seems to be no easy way out of biasing systems, unless we do not prescribe a specific inventory. There are systems which disambiguate according to the inventories which they themselves detect in the data (Schiitze,</context>
</contexts>
<marker>Carroll, McCarthy, 2000</marker>
<rawString>John Carroll and Diana McCarthy. 2000. Word sense disambiguation using automatically acquired verbal preferences. Computers and the Humanities. Senseval Special Issue, 34(1-2):109-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>The use of a psycholinguistic database in the simplification of text for aphasic readers. In</title>
<date>1998</date>
<booktitle>Linguistic Databases, volume CSLI Lecture Notes Number 77,</booktitle>
<pages>161--173</pages>
<editor>John Nerbonne, editor,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford CA.</location>
<contexts>
<context position="7259" citStr="Devlin and Tait, 1998" startWordPosition="1188" endWordPosition="1191">dictionaries. The traditional sense tagging methodology of SENSEVAL, with the production of gold-standard resources clearly feeds into this. Presumably it is also useful for lexicographers to be aware where inter-tagger agreement is low, and where and why WSD systems fall down ort the same items. There are other applications which might benefit from WSD. One application which we think would benefit from WSD is text simplification, which is just one variation of a more general lexical substitution task where a word is replaced by another word for a particular application. For example, in PSET (Devlin and Tait, 1998) the main enterprise was to simplify newspaper text for aphasic adults. One subtask was to substitute words with more familiar, or more frequent, synonyms, for example learn might be used in place of memorize. It makes clear sense to substitute with synonyms from the appropriate sense rather than from arty of the synonyms for the target word form. Thus if we are going to simplify scheme in the sentence: A recent government study singled out the scheme as an example to others one would want to use strategy rather than dodge as a replacement for scheme in this context. Having identified the sens</context>
</contexts>
<marker>Devlin, Tait, 1998</marker>
<rawString>Siobhan Devlin and John Tait. 1998. The use of a psycholinguistic database in the simplification of text for aphasic readers. In John Nerbonne, editor, Linguistic Databases, volume CSLI Lecture Notes Number 77, pages 161-173. CSLI Publications, Stanford CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Framework and results for english SENSEVAL. Computers and the Humanities. Senseval Special Issue,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="2454" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="381" endWordPosition="384">d be used to discover these distinctions, since inventories vary considerably in the distinctions that they make, and why. If we are not interested in which human distinctions WSD systems can make, but we want to apply our systems to some task then we need some rationale behind the inventory that we use. The inventories that we choose bias systems. One rationale for the use of Hector (Atkins, 1993) for the English task at the original SENlAlthough systems could use freely available sense tagged data, such as SemCor. SEVAL was that because no systems were using it, everyone would be penalised (Kilgarriff and Rosenzweig, 2000). A mapping between WordNet and Hector was provided so that participants with systems built around WordNet could share a common mapping. However, the mapping itself then creates biases (Agirre et al., 2000; Carroll and McCarthy, 2000). In SENSEVAL-2, WordNet was used as the inventory for both English tasks, because WordNet is widely available, and Hector much less so. WordNet does not however link related senses and there is no clear level of coarse sense distinctions. This was probably one of the reasons that the results for the English SENSEVAL-2 were noticeably lower than those of the origi</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework and results for english SENSEVAL. Computers and the Humanities. Senseval Special Issue, 34(1-2):15-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>What is word sense disambiguation good for?</title>
<date>1997</date>
<booktitle>In Proceedings of Natural Language Processing in the Pacific Rim,</booktitle>
<pages>209--214</pages>
<contexts>
<context position="1583" citStr="Kilgarriff, 1997" startWordPosition="240" endWordPosition="241">ose for which WSD is necessary, and participants were asked to provide the translations, given this inventory. The senses that the systems had to choose between were selected because they corresponded to distinct translations from Japanese into English. Sense distinctions with different forms in different languages are more likely to correspond to coarser grained distinctions than sense distinctions in a monolingual dictionary (Resnik and Yarowsky, 2000). Disambiguating these differences has relevance for art application. In contrast, much WSD work has gone on without a goal-driven inventory (Kilgarriff, 1997). Whilst it is of interest to investigate if a machine can distinguish senses that lexicographers have thought up, and also whether human annotators can themselves distinguish these senses, the questions still remains which inventory should be used to discover these distinctions, since inventories vary considerably in the distinctions that they make, and why. If we are not interested in which human distinctions WSD systems can make, but we want to apply our systems to some task then we need some rationale behind the inventory that we use. The inventories that we choose bias systems. One ration</context>
<context position="6084" citStr="Kilgarriff (1997)" startWordPosition="999" endWordPosition="1000">rovide such a system for participants, then it would certainly not be fair to expect participants to produce the architecture themselves. So what applications are there, in addition to the machine translation task, that would be suitable for objective evaluation without requiring a whole host of other activities? WSD systems have been suggested as being of use for many applications, although aside from machine translation the benefits have yet to be proved. 2 Possible Applications for WSD What applications can WSD be applied to and which of these would be suitable for art evaluation exercise? Kilgarriff (1997) identified machine translation, lexicography and information retrieval as being applications which can benefit from WSD. Machine translation is the clearest case where WSD is required for art NLP application. Whilst WSD has been shown to benefit information retrieval (Schiitze, 1998), the effect is diminished when longer queries are supplied. The combination of words in the query go a long way to ranking documents in order of sense relevance. Lexicography is not art NLP task, but one where sense-tagged data is useful to lexicographers developing dictionaries. The traditional sense tagging met</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>Adam Kilgarriff. 1997. What is word sense disambiguation good for? In Proceedings of Natural Language Processing in the Pacific Rim, pages 209-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Gold standard datasets for evaluating word sense disambiguation programs.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<pages>12--3</pages>
<marker>Kilgarriff, 1998</marker>
<rawString>Adam Kilgarriff. 1998. Gold standard datasets for evaluating word sense disambiguation programs. Computer Speech and Language, 12(3):453-472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>3</issue>
<pages>133</pages>
<contexts>
<context position="1424" citStr="Resnik and Yarowsky, 2000" startWordPosition="214" endWordPosition="217">relevant application. The participants were given a mapping between the Japanese test items and possible English equivalents. Thus the inventory was selected for a purpose for which WSD is necessary, and participants were asked to provide the translations, given this inventory. The senses that the systems had to choose between were selected because they corresponded to distinct translations from Japanese into English. Sense distinctions with different forms in different languages are more likely to correspond to coarser grained distinctions than sense distinctions in a monolingual dictionary (Resnik and Yarowsky, 2000). Disambiguating these differences has relevance for art application. In contrast, much WSD work has gone on without a goal-driven inventory (Kilgarriff, 1997). Whilst it is of interest to investigate if a machine can distinguish senses that lexicographers have thought up, and also whether human annotators can themselves distinguish these senses, the questions still remains which inventory should be used to discover these distinctions, since inventories vary considerably in the distinctions that they make, and why. If we are not interested in which human distinctions WSD systems can make, but </context>
</contexts>
<marker>Resnik, Yarowsky, 2000</marker>
<rawString>Philip Resnik and David Yarowsky. 2000. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation. Natural Language Engineering, 5(3):113— 133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<contexts>
<context position="3294" citStr="Schiitze, 1998" startWordPosition="527" endWordPosition="528">hy, 2000). In SENSEVAL-2, WordNet was used as the inventory for both English tasks, because WordNet is widely available, and Hector much less so. WordNet does not however link related senses and there is no clear level of coarse sense distinctions. This was probably one of the reasons that the results for the English SENSEVAL-2 were noticeably lower than those of the original SENSEVAL. There seems to be no easy way out of biasing systems, unless we do not prescribe a specific inventory. There are systems which disambiguate according to the inventories which they themselves detect in the data (Schiitze, 1998). An evaluation task which did not assume a particular inventory might be easier for such systems than the traditional gold-standard tasks. If no inventory is supplied then the gold-standard sense-tagging methodology becomes hard if not impossible to adopt because of the work involved for a human annotator recognising a wide variety of sense tags. SENSEVAL participants were at liberty to merge senses from the given inventory, and supply more than one sense tag, but they would then be penalised if the human annotators didn&apos;t make the same decisions. What is needed is at least one more applicati</context>
<context position="4590" citStr="Schiitze, 1998" startWordPosition="746" endWordPosition="747">oal as possible. Whilst we could still use the traditional methodology for investigating which sense-tags from a given inventory are easier to tag, we could also see the performance of systems in the context of an application, and explore how the choice of inventory affects this performance. The translation methodology is a good one, in that WSD has been shown to improve performance (Brown et al., 1991). It would be fair to allow systems to compete on more than one application platform, since some systems are designed for purposes other than machine translation, such as information retrieval (Schiitze, 1998). We would particularly like to explore art application which is flexible enough to allow systems to respond using their own inventory, though evaluating the system responses will then be harder. Another criteria we have for art applicationoriented task is that it should allow participants to focus ort the WSD task within the overall application, rather than trying to evaluate too many different subtasks at the same time. For the machine translation task, effort was focussed just ort translation of words, thus focusing ort the WSD task. One way to evaluate WSD in the context of art application</context>
<context position="6369" citStr="Schiitze, 1998" startWordPosition="1042" endWordPosition="1043">e host of other activities? WSD systems have been suggested as being of use for many applications, although aside from machine translation the benefits have yet to be proved. 2 Possible Applications for WSD What applications can WSD be applied to and which of these would be suitable for art evaluation exercise? Kilgarriff (1997) identified machine translation, lexicography and information retrieval as being applications which can benefit from WSD. Machine translation is the clearest case where WSD is required for art NLP application. Whilst WSD has been shown to benefit information retrieval (Schiitze, 1998), the effect is diminished when longer queries are supplied. The combination of words in the query go a long way to ranking documents in order of sense relevance. Lexicography is not art NLP task, but one where sense-tagged data is useful to lexicographers developing dictionaries. The traditional sense tagging methodology of SENSEVAL, with the production of gold-standard resources clearly feeds into this. Presumably it is also useful for lexicographers to be aware where inter-tagger agreement is low, and where and why WSD systems fall down ort the same items. There are other applications which</context>
<context position="9925" citStr="Schiitze, 1998" startWordPosition="1638" endWordPosition="1639">ories to merge senses and create coarser grained inventories where it makes sense to do so. Possibilities for the inventory and how we might actually evaluate the system answers are outlined in the next two subsections. 3.1 The Inventory For a lexical substitution task, we can choose whether we restrict users to a given inventory, or allow them to select their own. Whilst not specifying a predefined inventory makes human annotation much harder we contend that this would reduce bias and encourage participation from users who build their own classifications. Systems that deal in semantic space (Schiitze, 1998) could then participate, as well as systems that are committed to a given inventory. The substitutions will depend ort the type of inventory used. As regards man-made inventories, a thesaurus like WordNet lends itself more easily to a task like this than a dictionary like Hector, since it is organised ort the basis of semantic relationships, rather than alphabetically, though useful replacements might well be found in dictionary definitions. WordNet provides synonyms, or near synonyms together in synsets, and these synsets are related to other synsets with relationships such as ltyponymy. For </context>
</contexts>
<marker>Schiitze, 1998</marker>
<rawString>Hinrich Schiitze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97— 123.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>