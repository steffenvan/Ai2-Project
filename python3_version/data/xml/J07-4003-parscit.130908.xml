<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9963885">
Weighted and Probabilistic Context-Free
Grammars Are Equally Expressive
</title>
<author confidence="0.998371">
Noah A. Smith*
</author>
<affiliation confidence="0.992102">
Carnegie Mellon University
</affiliation>
<author confidence="0.984313">
Mark Johnson**
</author>
<affiliation confidence="0.82996">
Brown University
</affiliation>
<bodyText confidence="0.979595625">
This article studies the relationship between weighted context-free grammars (WCFGs), where
each production is associated with a positive real-valued weight, and probabilistic context-free
grammars (PCFGs), where the weights of the productions associated with a nonterminal are
constrained to sum to one. Because the class of WCFGs properly includes the PCFGs, one
might expect that WCFGs can describe distributions that PCFGs cannot. However, Z. Chi
(1999, Computational Linguistics, 25(1):131–160) and S. P. Abney, D. A. McAllester, and
P. Pereira (1999, In Proceedings of the 37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 542–549, College Park, MD) proved that every WCFG distribution
is equivalent to some PCFG distribution. We extend their results to conditional distributions,
and show that every WCFG conditional distribution of parses given strings is also the condi-
tional distribution defined by some PCFG, even when the WCFG’s partition function diverges.
This shows that any parsing or labeling accuracy improvement from conditional estimation of
WCFGs or conditional random fields (CRFs) over joint estimation of PCFGs or hidden Markov
models (HMMs) is due to the estimation procedure rather than the change in model class,
because PCFGs and HMMs are exactly as expressive as WCFGs and chain-structured CRFs,
respectively.
</bodyText>
<sectionHeader confidence="0.996951" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999953333333333">
In recent years the field of computational linguistics has turned to machine learning
to aid in the development of accurate tools for language processing. A widely used
example, applied to parsing and tagging tasks of various kinds, is a weighted grammar.
Adding weights to a formal grammar allows disambiguation (more generally, ranking
of analyses) and can lead to more efficient parsing. Machine learning comes in when we
wish to choose those weights empirically.
The predominant approach for many years was to select a probabilistic model—
such as a hidden Markov model (HMM) or probabilistic context-free grammar
(PCFG)—that defined a distribution over the structures allowed by a grammar. Given a
</bodyText>
<note confidence="0.655443">
* School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15217, USA.
</note>
<email confidence="0.993295">
E-mail: nasmith@cs.cmu.edu.
</email>
<note confidence="0.657093">
** Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912, USA.
</note>
<email confidence="0.708253">
E-mail: Mark Johnson@brown.edu.
</email>
<note confidence="0.89905425">
Submission received: 30 November 2005; revised submission received: 11 January 2007; accepted for
publication: 30 March 2007.
© 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.999624054054054">
treebank, maximum likelihood estimation can be applied to learn the probability values
in the model.
More recently, new machine learning methods have been developed or ex-
tended to handle models of grammatical structure. Notably, conditional estimation
(Ratnaparkhi, Roukos, and Ward 1994; Johnson et al. 1999; Lafferty, McCallum, and
Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised con-
trastive estimation (Smith and Eisner 2005) have been applied to structured models.
Weighted grammars learned in this way differ in two important ways from traditional,
generative models. First, the weights can be any positive value; they need not sum
to one. Second, features can “overlap,” and it can be difficult to design a generative
model that uses such features. The benefits of new features and discriminative training
methods are widely documented and recognized.
This article focuses specifically on the first of these differences. It compares the
expressive power of weighted context-free grammars (WCFGs), where each rule is
associated with a positive weight, to that of the corresponding PCFGs, that is, with
the same rules but where the weights of the rules expanding a nonterminal must sum
to one.
One might expect that because normalization removes one or more degrees of free-
dom, unnormalized models should be more expressive than normalized, probabilistic
models. Perhaps counterintuitively, previous work has shown that the classes of proba-
bility distributions defined by WCFGs and PCFGs are the same (Abney, McAllester, and
Pereira 1999; Chi 1999).
However, this result does not completely settle the question about the expressive
power of WCFGs and PCFGs. As we show herein, a WCFG can define a conditional
distribution from strings to trees even if it does not define a probability distribution
over trees. Because these conditional distributions are what are used in classification
tasks and related tasks such as parsing, we need to know the relationship between
the classes of conditional distributions defined by WCFGs and PCFGs. In fact we
extend the results of Chi and of Abney et al., and show that WCFGs and PCFGs both
define the same class of conditional distribution. Moreover, we present an algorithm
for converting an arbitrary WCFG that defines a conditional distribution over trees
given strings but possibly without a finite partition function into a PCFG with the
same rules as the WCFG and that defines the same conditional distribution over trees
given strings.
This means that maximum conditional likelihood WCFGs are non-identifiable, be-
cause there are an infinite number of rule weights all of which maximize the conditional
likelihood.
</bodyText>
<sectionHeader confidence="0.993618" genericHeader="keywords">
2. Weighted CFGs
</sectionHeader>
<bodyText confidence="0.9999565">
A CFG G is a tuple (N, S, E, R) where N is a finite set of nonterminal symbols, S E N is
the start symbol, E is a finite set of terminal symbols (disjoint from N), and R is a set of
production rules of the form X -4 α where X E N and α E (N U E)*. A WCFG associates
a positive number called the weight with each rule in R.1 We denote by θX→α the weight
attached to the rule X -4 α, and the vector of rule weights by © = {θA→α : A -4 α E R}.
A weighted grammar is the pair GΘ = (G, ©).
</bodyText>
<footnote confidence="0.921618">
1 Assigning a weight of zero to a rule equates to excluding it from R.
</footnote>
<page confidence="0.994536">
478
</page>
<note confidence="0.767872">
Smith and Johnson Weighted and Probabilistic CFGs
</note>
<bodyText confidence="0.627473333333333">
Unless otherwise specified, we assume a fixed underlying context-free grammar G.
Let Q(G) be the set of (finite) trees that G generates. For any τ G Q(G), the score so(τ) of
τ is defined as follows:
</bodyText>
<equation confidence="0.997524">
�so(τ) = (θX→α)f (X→α;τ) (1)
(X→α)∈R
</equation>
<bodyText confidence="0.997184">
where f (X -4 α; τ) is the number of times X -4 α is used in the derivation of the tree τ.
The partition function Z(©) is the sum of the scores of the trees in Q(G).
</bodyText>
<equation confidence="0.998075">
Z(©) = � so(τ)
τ∈Q(G)
</equation>
<bodyText confidence="0.99424875">
Because we have imposed no constraints on ©, the partition function need not equal
one; indeed, as we show subsequently the partition function need not even exist. If Z(©)
is finite then we say that the WCFG is convergent, and we can define a Gibbs probability
distribution over Q(G) by dividing by Z(©):
</bodyText>
<equation confidence="0.997815333333333">
so(τ)
Po(τ) =
Z(©)
</equation>
<bodyText confidence="0.836512">
A probabilistic CFG, or PCFG, is a WCFG in which the sum of the weights of the
rules expanding each nonterminal is one:
</bodyText>
<equation confidence="0.992977">
VXGN, � θX→α = 1 (2)
(X→α)∈R
</equation>
<bodyText confidence="0.999939052631579">
It is easy to show that if Go is a PCFG then Z(©) &lt; 1. A tight PCFG is a PCFG Go
for which Z(©) = 1. Necessary conditions and sufficient conditions for a PCFG to be
tight are given in several places, including Booth and Thompson (1973) and Wetherell
(1980).
We now describe the results of Chi (1999) and Abney, McAllester, and Pereira (1999).
Let g = {Go} denote the set of the WCFGs based on the CFG G (i.e., the WCFGs in g
all have the same underlying grammar G but differ in their rule weight vectors ©).
Let gZ&lt;∞ be the subset of g for which the partition function Z(©) is finite, and let
gZ=∞ = g \ gZ&lt;∞ be the subset of g with an infinite partition function. Further let gPCFG
denote the set of PCFGs based on G. In general, gPCFG is a proper subset of gZ&lt;∞, that
is, every PCFG is also a WCFG, but because there are weight vectors © that don’t obey
Equation 2, not all WCFGs are PCFGs.
However, this does not mean that WCFGs are more expressive than PCFGs. As
noted above, the WCFGs gZ&lt;∞ define Gibbs distributions. Again, for a fixed G, let
PZ&lt;∞ be the probability distributions over the trees Q(G) defined by the WCFGs gZ&lt;∞
and let PPCFG be the probability distributions defined by the PCFGs gPCFG. Chi (Propo-
sition 4) and Abney, McAllester, and Pereira (Lemma 5) showed that PZ&lt;∞ = PPCFG,
namely, that every WCFG probability distribution is in fact generated by some PCFG.
There is no “PZ=∞” because there is no finite normalizing term Z(©) for such WCFGs.
</bodyText>
<page confidence="0.996797">
479
</page>
<figure confidence="0.331531">
Computational Linguistics Volume 33, Number 4
</figure>
<subsectionHeader confidence="0.975817">
2.1 Chi’s Algorithm for Converting WCFGs to Equivalent PCFGs
</subsectionHeader>
<bodyText confidence="0.981172666666667">
Chi (1999) describes an algorithm for converting a WCFG to an equivalent PCFG. Let
Go be a WCFG in gZ&lt;∞. If X G N is a nonterminal, let QX(G) be the set of trees rooted
in X that can be built using G. Then define:
</bodyText>
<equation confidence="0.988578666666667">
ZX(©) = � so(τ)
τEQX(G)
For simplicity, let Zt(©) = 1 for all t G E. Chi demonstrated that Go G gZ&lt;∞ implies
that ZX(©) is finite for all X G N U E.
For every rule X -4 α in R define:
θX—α
Zαi(©)
11 |α|
i=1
ZX(©)
i
θX— α =
</equation>
<bodyText confidence="0.986574">
where αi is the ith element of α and α is the length of α. Chi proved that Goy is a PCFG
and that Po&apos;(τ) = so(τ)/Z(©) for all trees τ G Q(G).
Chi did not describe how to compute the nonterminal-specific partition functions
ZX(©). The ZX(©) are related by equations of the form
</bodyText>
<equation confidence="0.867670666666667">
� θX—α 11 |α |Zαi(©)
ZX(©) = i=1
α:X—αER
</equation>
<bodyText confidence="0.972315">
which constitute a set of nonlinear polynomial equations in ZX(©). Although a numeri-
cal solver might be employed to find the ZX(©), we have found that in practice iterative
propagation of weights following the method described by Stolcke (1995, Section 4.7.1)
converges quickly when Z(©) is finite.
</bodyText>
<sectionHeader confidence="0.798271" genericHeader="introduction">
3. Classifiers and Conditional Distributions
</sectionHeader>
<bodyText confidence="0.884723">
A common application of weighted grammars is parsing. One way to select a parse tree
for a sentence x is to choose the maximum weighted parse that is consistent with the
observation x:
</bodyText>
<equation confidence="0.9495335">
τ∗(x) = argmax so(τ) (3)
τEQ(G):y(τ)=x
</equation>
<bodyText confidence="0.9996994">
where y(τ) is the yield of τ. Other decision criteria exist, including minimum-loss de-
coding and re-ranked n-best decoding. All of these classifiers use some kind of dynamic
programming algorithm to optimize over trees, and they also exploit the conditional
distribution of trees given sentence observations. A WCFG defines such a conditional
distribution as follows:
</bodyText>
<equation confidence="0.9811775">
= so (τ) = so (τ)
Po(τ x) Eτ&apos;EQ(G):y(τ&apos;)=x so(τ�) Zx(©) (4)
</equation>
<page confidence="0.940425">
480
</page>
<note confidence="0.638847">
Smith and Johnson Weighted and Probabilistic CFGs
</note>
<bodyText confidence="0.999908888888889">
where Zx(Θ) is the sum of scores for all parses of x. Note that Equation (4) will be
ill-defined when Zx(Θ) diverges. Because Zx(Θ) is constant for a given x, solving Equa-
tion (3) is equivalent to choosing -r to maximize PΘ(-r  |x).
We turn now to classes of these conditional distribution families. Let CZ&lt;00 (CPCFG)
be the class of conditional distribution families that can be expressed by grammars in
9Z&lt;00 (9PCFG, respectively). It should be clear that, because PZ&lt;00 = PPCFG, CZ&lt;00 = CPCFG
since a conditional family is derived by normalizing a joint distribution by its marginals.
We now define another subset of 9. Let 9Zn&lt;00 contain every WCFG GΘ = (G, Θ)
such that, for all n &gt; 0,
</bodyText>
<equation confidence="0.950171">
Zn(Θ) = � sΘ(-r) &lt; oo (5)
τEQ(G):|y(τ)|=n
</equation>
<bodyText confidence="0.9132598">
(Note that, to be fully rigorous, we should quantify n in 9Zn&lt;00, writing “9ynZn(Θ)&lt;00.”
We use the abbreviated form to keep the notation crisp.) For any GΘ G 9Zn&lt;00, it also
follows that, for any x G L(G), Zx(Θ) &lt; oo; the converse holds as well.
It follows that any WCFG in 9Zn&lt;00 can be used to construct a conditional dis-
tribution of trees given the sentence, for any sentence x G L(G). To do so, we only
need to normalize sΘ(-r) by Zx(Θ) (Equation (4)). Let 9Zn=00 contain the WCFGs where
some Zn(Θ) diverge; this is a subset of 9Z=00.2 To see that 9Z=00 fl 9Zn&lt;00 =� 0, consider
Example 1.
Example 1
OA—,A A = 1, OA—,a = 1
This grammar produces binary structures over strings in a+. Every such tree receives
score 1. Because there are infinitely many trees, Z(Θ) diverges. But for any fixed string
an, the number of parse trees is finite. This grammar defines a uniform conditional
distribution over all binary trees, given the string.
For a grammar GΘ to be in 9Zn&lt;00, it is sufficient that, for every nonterminal X G N,
the sum of scores of all cyclic derivations X =&gt;+ X be finite. Conservatively, this can be
forced by eliminating epsilon rules and unary rules or cycles altogether, or by requiring
the sum of cyclic derivations for every nonterminal X to sum to strictly less than one.
Example 2 gives a grammar in 9Zn=00 with a unary cyclic derivation that does not
“dampen.”
Example 2
OA—,A A = 1, OA—,A = 1, OA—,a = 1
For any given an, there are infinitely many equally weighted parse trees, so even the
set of trees for an cannot be normalized into a distribution (Zn(Θ) = oo). Generally
speaking, if there exists a string x G L(G) such that the set of trees that derive x is not
</bodyText>
<page confidence="0.728962">
2 Here, full rigor would require quantification of n, writing “93nZn(Θ)=00.”
481
</page>
<note confidence="0.284852">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.974216571428572">
finite (i.e., there is no finite bound on the number of derivations for strings in L(G); the
grammar in Example 2 is a simple example), then GZn&lt;∞ and GZ&lt;∞ are separable.3
For a given CFG G, a conditional distribution over trees given strings is a function
Σ∗ → (Ω(G) → [0,1]). Our notation for the set of conditional distributions that can be
expressed by GZn&lt;∞ is CZn&lt;∞. Note that there is no “CZn=∞” because an infinite Zn(Θ)
implies an infinite Z(x) for some sentence x and therefore an ill-formed conditional fam-
ily. Indeed, it is difficult to imagine a scenario in computational linguistics in which non-
dampening cyclic derivations (WCFGs in GZn=∞) are desirable, because no linguistic
explanations depend crucially on arbitrary lengthening of cyclic derivations.
We now state our main theorem.
Theorem 1
For a given CFG G, CZn&lt;∞ = CZ&lt;∞.
Proof
Suppose we are given weights Θ for G such that GΘ ∈ GZn&lt;∞. We will show that the
sequence Z1(Θ), Z2(Θ), ... is bounded by an exponential function of n, then describe
a transformation on Θ resulting in a new grammar GΘy that is in GZ&lt;∞ and defines
the same family of conditional distributions (i.e., ∀-r ∈ Ω(G),∀x ∈ L(G), PΘ(-r  |x) =
PΘi(-r  |x)).
First we prove that for all n ≥ 1 there exists some c such that Zn(Θ) ≤ cn. Given GΘ,
we construct G¯Θ¯ in CNF that preserves the total score for any x ∈ L(G). The existence
of G¯Θ¯ was demonstrated by Goodman (1998, Section 2.6), who gives an algorithm for
constructing the value-preserving weighted grammar G¯Θ¯ from GΘ.
Note that G = (¯N, S, Σ, ¯R), containing possibly more nonterminals and rules than G.
The set of (finite) trees Ω (¯G) is different from Ω(G); the new trees must be binary and
may include new nonterminals.
Next, coll apse the nonterminals in N¯ into one nonterminal, S. The resulting gram-
mar is G˘Θ˘ = (({S}, S, Σ, ˘R), ˘Θ). R˘ contains the rule S → S S and rules of the form S → a
for a ∈ Σ. The weights of these rules are
</bodyText>
<equation confidence="0.99929175">
E˘�S→S S = R = max(1, ¯�X→Y Z) (6)
(X→Y Z)∈ R¯
E˘�S→a = v = max(1, ¯�X→b) (7)
(X→b)∈ R¯
</equation>
<bodyText confidence="0.9999174">
The grammar G˘Θ˘ will allow every tree allowed by G¯Θ¯ (modulo labels on nonterminal
nodes, which are now all S). It may allow some additional trees. The score of a tree
under G˘Θ˘ will be at least as great as the sum of scores of all structurally equivalent trees
under G¯¯Θ, because R and v are defined to be large enough to absorb all such scores. It
follows that, for all x ∈ L(G):
</bodyText>
<equation confidence="0.842163">
s ˘Θ(x) ≥ s ¯Θ(x) = sΘ(x) (8)
</equation>
<bodyText confidence="0.965561333333333">
3 We are grateful to an anonymous reviewer for pointing this out, and an even stronger point: for a given
G, G and GZn&lt;∞ have a nonempty set-difference if and only if G has infinite ambiguity (some x ∈ L(G)
has infinitely many parse trees).
</bodyText>
<page confidence="0.99486">
482
</page>
<note confidence="0.6959935">
Smith and Johnson Weighted and Probabilistic CFGs
Summing over all trees of any given yield length n, we have
</note>
<equation confidence="0.951148">
Zn( ˘©) ≥ Zn(¯©) = Zn(©) (9)
</equation>
<bodyText confidence="0.990285222222222">
G˘ generates all possible binary trees (with internal nodes undifferentiated) over
a given sentence x in L(G). Every tree generated by G˘ with yield length n will have
the same score: βn−1υn, because every binary tree with n terminals has exactly n − 1
nonterminals. Each tree corresponds to a way of bracketing n items, so the total number
of parse trees generated by G˘ for a string of length n is the number of different ways
of bracketing a sequence of n items. The total number of unlabeled binary bracketings
of an n-length sequence is the nth Catalan number Cn (Graham, Knuth, and Patashnik
1994), which in turn is bounded above by 4n (Vardi 1991). The total number of strings of
length n is |E|n. Therefore
</bodyText>
<equation confidence="0.87036275">
Zn( ˘©) = Cn|E|nβn−1υn ≤ 4n|E|nβn−1υn ≤ (4|E|βυ)n (10)
We now transform the original weights © as follows. For every rule (X → α) E R,
let
i
θX→α ←
(11)
(8|E|βυ)t(α)
θX→α
</equation>
<bodyText confidence="0.999719555555556">
where t(α) is the number of E symbols appearing in α. This transformation results in
every n-length sentence having its score divided by (8|E|βυ)n. The relative scores of
trees with the same yield are unaffected, because they are all scaled equally. Therefore
Goy defines the same conditional distribution over trees given sentences as Go, which
implies that Go and Goy have the same highest scoring parses. Note that any sufficiently
large value could stand in for 8|E|βυ to both (a) preserve the conditional distribution
and (b) force Zn(©) to converge. We have not found the minimum such value, but 8|E|βυ
is sufficiently large.
The sequence of Zn(©) now converges:
</bodyText>
<equation confidence="0.968258333333333">
i Zn(©) (1)n (12)
Zn(©) ≤ (8|E|βυ)n ≤ \2l
Hence Z(©&apos;) = E∞n_0 Zn(©&apos;) ≤ 2 and Goy E GZ&lt;∞. ■
</equation>
<sectionHeader confidence="0.344449" genericHeader="method">
Corollary 1
</sectionHeader>
<bodyText confidence="0.6782685">
Given a CFG G, CZn&lt;∞ = CPCFG.
Proof
</bodyText>
<listItem confidence="0.416885">
By Theorem 1, CZn&lt;∞ = CZ&lt;∞. We know that PZ&lt;∞ = PPCFG, from which it follows that
CZ&lt;∞ = CPCFG. Hence CZn&lt;∞ = CPCFG. To convert a WCFG in CZn&lt;∞ into a PCFG, first
apply the transformation in the proof of Theorem 1 to get a convergent WCFG, then
apply Chi’s method (our Section 2.1). ■
</listItem>
<page confidence="0.995793">
483
</page>
<figure confidence="0.809049">
Computational Linguistics Volume 33, Number 4
</figure>
<figureCaption confidence="0.996347">
Figure 1
</figureCaption>
<bodyText confidence="0.990414933333334">
A graphical depiction of the primary result of this article. Given a fixed set of productions, 9 is
the set of WCFGs with exactly those productions (i.e., they vary only in the production weights),
9Z&lt;∞ is the subset of 9 that defines (joint) probability distributions over trees (i.e., that have a
finite partition function Z) and PZ&lt;∞ is the set of probability distributions defined by grammars
in 9Z&lt;∞. Chi (1999) and Abney, McAllester, and Pereira (1999) proved that PZ&lt;∞ is
the same as PPCFG, the set of probability distributions defined by the PCFG 9PCFG with the same
productions as 9. Thus even though the set of WCFGs properly includes the set of PCFGs,
WCFGs define exactly the same probability distributions over trees as PCFGs. This article
extends these results to conditional distributions over trees conditioned on their strings. Even
though the set 9Zn n
&lt;� of WCFGs that define conditional distributions may be larger than 9Z&lt;∞
and properly includes �✓PCFG, the set of conditional distributions CZn&lt;∞ defined by 9Zn&lt;∞ is
equal to the set of conditional distributions CPCFG defined by PCFGs. Our proof is constructive:
we give an algorithm which takes as input a WCFG G E 9Zn&lt;∞ and returns a PCFG which
defines the same conditional distribution over trees given strings as G.
</bodyText>
<figureCaption confidence="0.6197835">
Figure 1 presents the main result graphically in the context of earlier results.
4. HMMs and Related Models
</figureCaption>
<bodyText confidence="0.999668428571429">
Hidden Markov models (HMMs) are a special case of PCFGs. The structures they
produce are labeled sequences, which are equivalent to right-branching trees. We can
write an HMM as a PCFG with restricted types of rules. We will refer to the unweighted,
finite-state grammars that HMMs stochasticize as “right-linear grammars.” Rather than
using the production rule notation of PCFGs, we will use more traditional HMM nota-
tion and refer to states (interchangeable with nonterminals) and paths (interchangeable
with parse trees).
In the rest of the article we distinguish between HMMs, which are probabilistic
finite-state automata locally normalized just like a PCFG, and chain-structured Markov
random fields (MRFs; Section 4.1), in which moves or transitions are associated with
positive weights and which are globally normalized like a WCFG.4 We also distinguish
two different types of dependency structures in these automata. Abusing the standard
terminology somewhat, in a Mealy automaton arcs are labeled with output or terminal
symbols, whereas in a Moore automaton the states emit terminal symbols.5
</bodyText>
<footnote confidence="0.985309">
4 We admit that these names are somewhat misleading, because as we will show, chain-structured MRFs
also have the Markov property and define the same joint and conditional distributions as HMMs.
5 In formal language theory both Mealy and Moore machines are finite-state transducers (Mealy 1955;
Moore 1956); we ignore the input symbols here.
</footnote>
<page confidence="0.993406">
484
</page>
<note confidence="0.868898">
Smith and Johnson Weighted and Probabilistic CFGs
</note>
<bodyText confidence="0.993455333333333">
A Mealy HMM defines a probability distribution over pairs (z, fr), where x� is a
length-n sequence (x1, x2, ..., xn) E En and π� = (π0, π1, π2, ..., πn) E Nn+1 is a state (or
nonterminal) path. The distribution is given by
</bodyText>
<equation confidence="0.999461">
� � �
PHMM(�x, �π) = p(xi,πi  |πi−1) p(STOP  |πn) (13)
i=1
</equation>
<bodyText confidence="0.999087714285714">
π0 is assumed, for simplicity, to be constant and known; we also assume that every
state transition emits a symbol (no a arcs), an assumption made in typical tagging and
chunking applications of HMMs. We can convert a Mealy HMM to a PCFG by including,
for every tuple (x, π, φ) (x E E and π, φ E N) such that p(x, π  |φ) &gt; 0, the rule π -4 x φ,
with the same probability as the corresponding HMM transition. For every π such that
p(STOP  |π), we include the rule π -4c, with probability p(STOP  |π).
A Moore HMM factors the distribution p(x, π  |φ) into p(x  |π) · p(π  |φ). A Moore
HMM can be converted to a PCFG by adding a new nonterminal π¯ for every state π
and including the rules φ -4 π¯ (with probability p(π  |φ)) and π¯ - 4x π (with probability
p(x  |π)). Stop probabilities are added as in the Mealy case. For a fixed number of states,
Moore HMMs are less probabilistically expressive than Mealy HMMs, though we can
convert between the two with a change in the number of states.
We consider Mealy HMMs primarily from here on. If we wish to define the distri-
bution over paths given words, we conditionalize
</bodyText>
<equation confidence="0.968324333333333">
7�7n
y) l 1i=1(( p(xi, πi  |πi−1) p(STOP  |πn )
PHMM(�π EA,ENn+1 `Hi 1 p(xi,�  |π�i−1)�p(STOP  |πn) (14)
</equation>
<bodyText confidence="0.9207586">
This is how scores are assigned when selecting the best path given a sequence.
For a grammar G that is right-linear, we can therefore talk about the set of HMM
(right-linear) grammars C✓HMM, the set of probability distributions PHMM defined by those
grammars, and CHMM, the set of conditional distributions over state paths (trees) that they
define.6
</bodyText>
<subsectionHeader confidence="0.996925">
4.1 Mealy Markov Random Fields
</subsectionHeader>
<bodyText confidence="0.993165833333333">
When the probabilities in Mealy HMMs are replaced by arbitrary positive weights, the
production rules can be seen as features in a Gibbs distribution. The resulting model
is a type of MRF with a chain structure; these have recently become popular in natural
language processing (Lafferty, McCallum, and Pereira 2001). Lafferty et al.’s formulation
defined a conditional distribution over paths given sequences by normalizing for each
sequence z:
</bodyText>
<equation confidence="0.992383375">
n
PCMRF(�π  |�x) =
~ n
rI �
θπi−1,xi,πi θπn,STOP
i=1
(15)
Zx(Θ)
</equation>
<page confidence="0.958965">
6 Of course, the right-linear grammar is a CFG, so we could also use the notation C✓PCFG, PPCFG, and CPCFG.
485
</page>
<note confidence="0.393936">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.715076">
Using a single normalizing term Z(Θ), we can also define a joint distribution over
states and paths:
</bodyText>
<equation confidence="0.9402115">
PCMRF(-x,-π) = (16)
Z(Θ)
</equation>
<bodyText confidence="0.9682124">
Let 9 = {GΘ} denote the set of weighted grammars based on the unweighted right-
linear grammar G. We call these weighted grammars “Mealy MRFs.” As in the WCFG
case, we can add the constraint Zn(Θ) &lt; oo (for all n), giving the class 9Zn&lt;∞.
Recall that, in the WCFG case, the move from 9 to 9Zn&lt;∞ had to do with cyclic
derivations. The analogous move in the right-linear grammar case involves c emis-
sions (production rules of the form X - 4Y). If, as in typical applications of finite-state
models to natural language processing, there are no rules of the form X - 4Y, then
9Zn&lt;∞ is empty and 9Zn&lt;∞ = 9. Our formulae, in fact, assume that there are no c
emissions.
Because Mealy MRFs are a special case of WCFGs, Theorem 1 applies to them.
This means that any random field using Mealy HMM features (Mealy MRF) such that
Vn, Zn(Θ) &lt; oo can be transformed into a Mealy HMM that defines the same conditional
distribution of tags given words.7
Corollary 2
For a given right-linear grammar G, CHMM = CZ&lt;∞ = CZn&lt;∞.
Lafferty, McCallum, and Pereira’s conditional random fields are typically trained to
optimize a different objective function than HMMs (conditional likelihood and joint
likelihood, respectively). Our result shows that optimizing either objective on the set
of Mealy HMMs as opposed to Mealy MRFs will achieve the same result, modulo
imperfections in the numerical search for parameter values.
</bodyText>
<subsectionHeader confidence="0.99125">
4.2 Maximum-Entropy Markov Models
</subsectionHeader>
<bodyText confidence="0.9959426">
While HMMs and chain MRFs represent the same set of conditional distributions, we
can show that the maximum-entropy Markov models (MEMMs) of McCallum, Freitag,
and Pereira (2000) represent a strictly smaller class of distributions.
An MEMM is a similar model with a different event structure. It defines the distri-
bution over paths given words as:
</bodyText>
<equation confidence="0.827141833333333">
� n
θπi−1,xi,πi θπn,STOP
i=1
PMEMM(-π  |-x) = ��n77 p(πi  |πi−1,xi) (17)
11
i=1
</equation>
<bodyText confidence="0.765597333333333">
Unlike an HMM, the MEMM does not define a distribution over output sequences x.
The name “maximum entropy Markov model” comes from the fact that the conditional
7 What if we allow additional features? It can be shown that, as long as the vocabulary Σ is finite and
known, we can convert any such MRF with potential functions on state transitions and emissions into
an HMM functioning equivalently as a classifier. If Σ is not fully known, then we cannot sum over all
emissions from each state, and we cannot use Chi’s method (Section 2.1) to convert to a PCFG (HMM).
</bodyText>
<page confidence="0.996567">
486
</page>
<note confidence="0.907364">
Smith and Johnson Weighted and Probabilistic CFGs
</note>
<bodyText confidence="0.93783">
distributions p(·  |π, x) typically have a log-linear form, rather than a multinomial form,
and are trained to maximize entropy.
Lemma 1
For every MEMM, there is a Mealy MRF that represents the same conditional distribu-
tion over paths given symbols.
</bodyText>
<subsectionHeader confidence="0.696297">
Proof
</subsectionHeader>
<bodyText confidence="0.9639417">
By definition, the features of the MRF include triples (πi−1,xi,πi). Assign to the
weight θπi,xj,πk the value PMEMM(πi  |πk,xj). Assign to θπi,STOP the value 1. In computing
PCMRF(π  |x) (Equation (15)), the normalizing term for each x will be equal to 1. ■
MEMMs, like HMMs, are defined by locally normalized conditional multinomial
distributions. This has computational advantages (no potentially infinite Z(Θ) terms to
compute). However, the set of conditional distributions of labels given terminals that
can be expressed by MEMMs is strictly smaller than those expressible by HMMs (and
by extension, Mealy MRFs).
Theorem 2
For a given right-linear grammar G, CMEMM ⊂ CHMM.
</bodyText>
<subsectionHeader confidence="0.577501">
Proof
</subsectionHeader>
<bodyText confidence="0.963708875">
We give an example of a Mealy HMM whose conditional distribution over paths (trees)
given sentences cannot be represented by an MEMM. We thank Michael Collins for
pointing out to us the existence of examples like this one. Define a Mealy HMM with
three states named 0, 1, and 2, over an alphabet {a, b, c}, as follows. State 0 is the start
state.
Example 3
Under this model, PHMM(0,1,1  |a,b) = PHMM(0,2,2  |a, c) = 1. These conditional dis-
tributions cannot both be met by any MEMM. To see why, consider
</bodyText>
<equation confidence="0.321788">
p(1  |0, a) · p(1  |1,b) = p(2  |0, a) · p(2  |2, c) = 1
This implies that
p(1  |0, a) = p(1  |1,b) = p(2  |0, a) = p(2  |2,c) = 1
</equation>
<page confidence="0.989897">
487
</page>
<note confidence="0.586929">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.996383">
But it is impossible for p(1  |0, a) = p(2  |0, a) = 1. This holds regardless of the form of
the distribution p(·  |π, x) (e.g., multinomial or log-linear).
Because P(0,1,1  |a, b) = P(0, 2,2  |a, c) cannot be met by any MEMM, there are
distributions in the family allowed by HMMs that cannot be expressed as MEMMs,
and the latter are less expressive. ■
It is important to note that this result applies to Mealy HMMs; our result compares
models with the same dependencies among random variables. If the HMM’s distribu-
tion p(xi,πi  |πi−1) is factored into p(xi  |πi) · p(πi  |πi−1) (i.e., it is a Moore HMM), then
there may exist an MEMM with the same number of states that can represent some
distributions that the Moore HMM cannot.8
One can also imagine MEMMs in which p(πi  |πi−1, xi, ...) is conditioned on more
surrounding context (xi−1 or xi+1, or the entire sequence z, for example). Conditioning
on more context can be done by increasing the order of the Markov model—all of
our models so far have been first-order, with a memory of only the previous state.
Our result can be extended to include higher-order MEMMs. Suppose we allow the
MEMM to “look ahead” n words, factoring its distribution into p(πi  |πi−1, xi, xi+1, ...,
xi+n).
</bodyText>
<table confidence="0.716427916666667">
Corollary 3
A first-order Mealy HMM can represent some classifiers that no MEMM with finite
lookahead can represent.
Proof
Consider again Example 3. Note that, for all m ≥ 1, it sets
PHMM(0, m 1’s
� �� �
1, ...,
1  |amb) = 1
PHMM(0, 2, ..., 2  |amc) = 1
\f J
m 2’s
</table>
<bodyText confidence="0.357942">
Suppose we wish to capture this in an MEMM with n symbols of look-ahead. Letting
</bodyText>
<equation confidence="0.998115428571429">
m = n + 1,
n
p(1  |0, an+1) · p(1  |1, anb) · p(1  |1,an−ib) = 1
i=1
n
p(2  |0, an+1) · p(2  |2, anc) · p(2  |2, an−ic) = 1
i=1
</equation>
<bodyText confidence="0.9873935">
The same issue arises as in the proof of Theorem 2: it cannot be that p(1  |0, an+1) =
p(2  |0, an+1) = 1, and so this MEMM does not exist. Note that even if we allow the
</bodyText>
<footnote confidence="0.742917">
8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to the
distribution.
</footnote>
<page confidence="0.994225">
488
</page>
<note confidence="0.90723">
Smith and Johnson Weighted and Probabilistic CFGs
</note>
<bodyText confidence="0.99456925">
MEMM to “look back” and condition on earlier symbols (or states), it cannot represent
the distribution in Example 3. ■
Generally speaking, this limitation of MEMMs has nothing to do with the estima-
tion procedure (we have committed to no estimation procedure in particular) but rather
with the conditional structure of the model. That some model structures work better
than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning
(2002). Our result—that the class of distributions allowed by MEMMs is a strict subset
of those allowed by Mealy HMMs—makes this unsurprising.
</bodyText>
<sectionHeader confidence="0.995177" genericHeader="method">
5. Practical Implications
</sectionHeader>
<bodyText confidence="0.999961272727273">
Our result is that weighted generalizations of classical probabilistic grammars (PCFGs
and HMMs) are no more powerful than the probabilistic models. This means that, inso-
far as log-linear models for NLP tasks like tagging and parsing are more successful
than their probabilistic cousins, it is due to either (a) additional features added to
the model, (b) improved estimation procedures (e.g., maximum conditional likelihood
estimation or contrastive estimation), or both. (Note that the choice of estimation proce-
dure (b) is in principle orthogonal to the choice of model, and conditional estimation
should not be conflated with log-linear modeling.) For a given estimation criterion,
weighted CFGs, and Mealy MRFs, in particular, cannot be expected to behave any
differently than PCFGs and HMMs, respectively, unless they are augmented with more
features.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="evaluation">
6. Related Work
</sectionHeader>
<bodyText confidence="0.999925294117647">
Abney, McAllester, and Pereira (1999) addressed the relationship between PCFGs and
probabilistic models based on push-down automaton operations (e.g., the structured
language model of Chelba and Jelinek, 1998). They proved that, although the conversion
may not be simple (indeed, a blow-up in the automaton’s size may be incurred), given
G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down
automata are weakly equivalent. Importantly, the standard conversion of a CFG into a
shift-reduce PDA, when applied in the stochastic case, does not always preserve the prob-
ability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further
work on the relationship between weighted CFGs and weighted PDAs is described in
Nederhof and Satta (2004).
MacKay (1996) proved that linear Boltzmann chains (a class of weighted models
that is essentially the same as Moore MRFs) express the same set of distributions as
Moore HMMs, under the condition that the Boltzmann chain has a single specific end
state. MacKay avoided the divergence problem by defining the Boltzmann chain always
to condition on the length of the sequence; he tacitly requires all of his models to be in
GZn&lt;∞. We have suggested a more applicable notion of model equivalence (equivalence
of the conditional distribution) and our Theorem 1 generalizes to context-free models.
</bodyText>
<sectionHeader confidence="0.956389" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.999733">
We have shown that weighted CFGs that define finite scores for all sentences in their
languages have no greater expressivity than PCFGs, when used to define distributions
</bodyText>
<page confidence="0.995333">
489
</page>
<note confidence="0.564792">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.9998215">
over trees given sentences. This implies that the standard Mealy MRF formalism is
no more powerful than Mealy HMMs, for instance. We have also related “maximum
entropy Markov models” to Mealy Markov random fields, showing that the former is a
strictly less expressive weighted formalism.
</bodyText>
<sectionHeader confidence="0.997663" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.976771916666667">
This work was supported by a Fannie and
John Hertz Foundation fellowship to
N. Smith at Johns Hopkins University. The
views expressed are not necessarily endorsed
by the sponsors. We are grateful to three
anonymous reviewers for feedback that
improved the article, to Michael Collins for
encouraging exploration of this matter and
helpful comments on a draft, and to Jason
Eisner and Dan Klein for insightful
conversations. Any errors are the sole
responsibility of the authors.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998201">
Abney, Steven P., David A. McAllester,
and Fernando Pereira. 1999. Relating
probabilistic grammars and automata.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics,
pages 542–549, College Park, MD.
Booth, Taylor L. and Richard A. Thompson.
1973. Applying probability measures to
abstract languages. IEEE Transactions on
Computers, 22(5):442–450.
Chelba, Ciprian and Frederick Jelinek.
1998. Exploiting syntactic structure for
language modeling. In Proceedings
of the 36th Annual Meeting of the
Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 325–331,
Montreal, Canada.
Chi, Zhiyi. 1999. Statistical properties of
probabilistic context-free grammars.
Computational Linguistics, 25(1):131–160.
Goodman, Joshua T. 1998. Parsing Inside-Out.
Ph.D. thesis, Harvard University,
Cambridge, MA.
Graham, Ronald L., Donald E. Knuth,
and Oren Patashnik.1994. Concrete
Mathematics. Addison-Wesley,
Reading, MA.
Johnson, Mark. 2001. Joint and conditional
estimation of tagging and parsing models.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics,
pages 314–321, Toulouse, France.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
“unification-based” grammars.
In Proceedings of the 37th Annual
Conference of the Association for
Computational Linguistics, pages 535–541,
College Park, MD.
Klein, Dan and Christopher D. Manning.
2002. Conditional structure versus
conditional estimation in NLP models.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 9–16, Philadelphia, PA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of the 18th International
Conference on Machine Learning,
pages 282–289, Williamstown, MA.
MacKay, David J. C. 1996. Equivalence of
linear Boltzmann chains and hidden
Markov models. Neural Computation,
8(1):178–181.
McCallum, Andrew, Dayne Freitag, and
Fernando Pereira. 2000. Maximum
entropy Markov models for information
extraction and segmentation. In
Proceedings of the 17th International
Conference on Machine Learning,
pages 591–598, Palo Alto, CA.
Mealy, G. H. 1955. A method for
synthesizing sequential circuits.
Bell System Technology Journal,
34:1045–1079.
Moore, Edward F. 1956. Gedanken-
experiments on sequential machines.
In Automata Studies, number 34 in
Annals of Mathematics Studies.
Princeton University Press, Princeton,
NJ, pages 129–153.
Nederhof, Mark-Jan and Giorgio Satta.
2004. Probabilistic parsing strategies.
In Proceedings of the 42nd Annual
Meeting of the Association for
Computational Linguistics, pages 543–550,
Barcelona, Spain.
Ratnaparkhi, Adwait, Salim Roukos,
and R. Todd Ward. 1994. A maximum
entropy model for parsing. In Proceedings
of the International Conference on Spoken
Language Processing, pages 803–806,
Yokohama, Japan.
Smith, Noah A. and Jason Eisner. 2005.
Contrastive estimation: Training
log-linear models on unlabeled data.
</reference>
<page confidence="0.992879">
490
</page>
<note confidence="0.904105">
Smith and Johnson Weighted and Probabilistic CFGs
</note>
<reference confidence="0.9986276">
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics, pages 354–362, Ann Arbor, MI.
Stolcke, Andreas. 1995. An efficient
probabilistic context-free parsing
algorithm that computes prefix
probabilities. Computational Linguistics,
21(2):165–201.
Taskar, Ben, Dan Klein, Michael Collins,
Daphne Koller, and Christopher Manning.
2004. Max-margin parsing. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 1–8,
Barcelona, Spain.
Vardi, Ilan. 1991. Computational Recreations in
Mathematica. Addison-Wesley, Redwood
City, CA.
Wetherell, C. S. 1980. Probabilistic languages:
A review and some open questions.
Computing Surveys, 12:361–379.
</reference>
<page confidence="0.998758">
491
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.287007">
<title confidence="0.997784">Weighted and Probabilistic Context-Free</title>
<author confidence="0.433293">Grammars Are Equally Expressive</author>
<affiliation confidence="0.850678333333333">A. Carnegie Mellon University Brown University</affiliation>
<abstract confidence="0.980270875">This article studies the relationship between weighted context-free grammars (WCFGs), where each production is associated with a positive real-valued weight, and probabilistic context-free grammars (PCFGs), where the weights of the productions associated with a nonterminal are constrained to sum to one. Because the class of WCFGs properly includes the PCFGs, one might expect that WCFGs can describe distributions that PCFGs cannot. However, Z. Chi (1999, Computational Linguistics, 25(1):131–160) and S. P. Abney, D. A. McAllester, and P. Pereira (1999, In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 542–549, College Park, MD) proved that every WCFG distribution is equivalent to some PCFG distribution. We extend their results to conditional distributions, and show that every WCFG conditional distribution of parses given strings is also the conditional distribution defined by some PCFG, even when the WCFG’s partition function diverges. This shows that any parsing or labeling accuracy improvement from conditional estimation of WCFGs or conditional random fields (CRFs) over joint estimation of PCFGs or hidden Markov models (HMMs) is due to the estimation procedure rather than the change in model class, because PCFGs and HMMs are exactly as expressive as WCFGs and chain-structured CRFs, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
<author>David A McAllester</author>
<author>Fernando Pereira</author>
</authors>
<title>Relating probabilistic grammars and automata.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>542--549</pages>
<location>College Park, MD.</location>
<marker>Abney, McAllester, Pereira, 1999</marker>
<rawString>Abney, Steven P., David A. McAllester, and Fernando Pereira. 1999. Relating probabilistic grammars and automata. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 542–549, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor L Booth</author>
<author>Richard A Thompson</author>
</authors>
<title>Applying probability measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>22</volume>
<issue>5</issue>
<contexts>
<context position="7149" citStr="Booth and Thompson (1973)" startWordPosition="1161" endWordPosition="1164">one; indeed, as we show subsequently the partition function need not even exist. If Z(©) is finite then we say that the WCFG is convergent, and we can define a Gibbs probability distribution over Q(G) by dividing by Z(©): so(τ) Po(τ) = Z(©) A probabilistic CFG, or PCFG, is a WCFG in which the sum of the weights of the rules expanding each nonterminal is one: VXGN, � θX→α = 1 (2) (X→α)∈R It is easy to show that if Go is a PCFG then Z(©) &lt; 1. A tight PCFG is a PCFG Go for which Z(©) = 1. Necessary conditions and sufficient conditions for a PCFG to be tight are given in several places, including Booth and Thompson (1973) and Wetherell (1980). We now describe the results of Chi (1999) and Abney, McAllester, and Pereira (1999). Let g = {Go} denote the set of the WCFGs based on the CFG G (i.e., the WCFGs in g all have the same underlying grammar G but differ in their rule weight vectors ©). Let gZ&lt;∞ be the subset of g for which the partition function Z(©) is finite, and let gZ=∞ = g \ gZ&lt;∞ be the subset of g with an infinite partition function. Further let gPCFG denote the set of PCFGs based on G. In general, gPCFG is a proper subset of gZ&lt;∞, that is, every PCFG is also a WCFG, but because there are weight vecto</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>Booth, Taylor L. and Richard A. Thompson. 1973. Applying probability measures to abstract languages. IEEE Transactions on Computers, 22(5):442–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>325--331</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="31101" citStr="Chelba and Jelinek, 1998" startWordPosition="5377" endWordPosition="5380">ion), or both. (Note that the choice of estimation procedure (b) is in principle orthogonal to the choice of model, and conditional estimation should not be conflated with log-linear modeling.) For a given estimation criterion, weighted CFGs, and Mealy MRFs, in particular, cannot be expected to behave any differently than PCFGs and HMMs, respectively, unless they are augmented with more features. 6. Related Work Abney, McAllester, and Pereira (1999) addressed the relationship between PCFGs and probabilistic models based on push-down automaton operations (e.g., the structured language model of Chelba and Jelinek, 1998). They proved that, although the conversion may not be simple (indeed, a blow-up in the automaton’s size may be incurred), given G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down automata are weakly equivalent. Importantly, the standard conversion of a CFG into a shift-reduce PDA, when applied in the stochastic case, does not always preserve the probability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further work on the relationship between weighted CFGs and weighted PDAs is described in Nederhof and Satta (2004). MacKay (1</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Chelba, Ciprian and Frederick Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 325–331, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyi Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="4277" citStr="Chi 1999" startWordPosition="626" endWordPosition="627">expressive power of weighted context-free grammars (WCFGs), where each rule is associated with a positive weight, to that of the corresponding PCFGs, that is, with the same rules but where the weights of the rules expanding a nonterminal must sum to one. One might expect that because normalization removes one or more degrees of freedom, unnormalized models should be more expressive than normalized, probabilistic models. Perhaps counterintuitively, previous work has shown that the classes of probability distributions defined by WCFGs and PCFGs are the same (Abney, McAllester, and Pereira 1999; Chi 1999). However, this result does not completely settle the question about the expressive power of WCFGs and PCFGs. As we show herein, a WCFG can define a conditional distribution from strings to trees even if it does not define a probability distribution over trees. Because these conditional distributions are what are used in classification tasks and related tasks such as parsing, we need to know the relationship between the classes of conditional distributions defined by WCFGs and PCFGs. In fact we extend the results of Chi and of Abney et al., and show that WCFGs and PCFGs both define the same cl</context>
<context position="7213" citStr="Chi (1999)" startWordPosition="1174" endWordPosition="1175">. If Z(©) is finite then we say that the WCFG is convergent, and we can define a Gibbs probability distribution over Q(G) by dividing by Z(©): so(τ) Po(τ) = Z(©) A probabilistic CFG, or PCFG, is a WCFG in which the sum of the weights of the rules expanding each nonterminal is one: VXGN, � θX→α = 1 (2) (X→α)∈R It is easy to show that if Go is a PCFG then Z(©) &lt; 1. A tight PCFG is a PCFG Go for which Z(©) = 1. Necessary conditions and sufficient conditions for a PCFG to be tight are given in several places, including Booth and Thompson (1973) and Wetherell (1980). We now describe the results of Chi (1999) and Abney, McAllester, and Pereira (1999). Let g = {Go} denote the set of the WCFGs based on the CFG G (i.e., the WCFGs in g all have the same underlying grammar G but differ in their rule weight vectors ©). Let gZ&lt;∞ be the subset of g for which the partition function Z(©) is finite, and let gZ=∞ = g \ gZ&lt;∞ be the subset of g with an infinite partition function. Further let gPCFG denote the set of PCFGs based on G. In general, gPCFG is a proper subset of gZ&lt;∞, that is, every PCFG is also a WCFG, but because there are weight vectors © that don’t obey Equation 2, not all WCFGs are PCFGs. Howeve</context>
<context position="8502" citStr="Chi (1999)" startWordPosition="1407" endWordPosition="1408">, the WCFGs gZ&lt;∞ define Gibbs distributions. Again, for a fixed G, let PZ&lt;∞ be the probability distributions over the trees Q(G) defined by the WCFGs gZ&lt;∞ and let PPCFG be the probability distributions defined by the PCFGs gPCFG. Chi (Proposition 4) and Abney, McAllester, and Pereira (Lemma 5) showed that PZ&lt;∞ = PPCFG, namely, that every WCFG probability distribution is in fact generated by some PCFG. There is no “PZ=∞” because there is no finite normalizing term Z(©) for such WCFGs. 479 Computational Linguistics Volume 33, Number 4 2.1 Chi’s Algorithm for Converting WCFGs to Equivalent PCFGs Chi (1999) describes an algorithm for converting a WCFG to an equivalent PCFG. Let Go be a WCFG in gZ&lt;∞. If X G N is a nonterminal, let QX(G) be the set of trees rooted in X that can be built using G. Then define: ZX(©) = � so(τ) τEQX(G) For simplicity, let Zt(©) = 1 for all t G E. Chi demonstrated that Go G gZ&lt;∞ implies that ZX(©) is finite for all X G N U E. For every rule X -4 α in R define: θX—α Zαi(©) 11 |α| i=1 ZX(©) i θX— α = where αi is the ith element of α and α is the length of α. Chi proved that Goy is a PCFG and that Po&apos;(τ) = so(τ)/Z(©) for all trees τ G Q(G). Chi did not describe how to com</context>
<context position="18135" citStr="Chi (1999)" startWordPosition="3158" endWordPosition="3159"> PCFG, first apply the transformation in the proof of Theorem 1 to get a convergent WCFG, then apply Chi’s method (our Section 2.1). ■ 483 Computational Linguistics Volume 33, Number 4 Figure 1 A graphical depiction of the primary result of this article. Given a fixed set of productions, 9 is the set of WCFGs with exactly those productions (i.e., they vary only in the production weights), 9Z&lt;∞ is the subset of 9 that defines (joint) probability distributions over trees (i.e., that have a finite partition function Z) and PZ&lt;∞ is the set of probability distributions defined by grammars in 9Z&lt;∞. Chi (1999) and Abney, McAllester, and Pereira (1999) proved that PZ&lt;∞ is the same as PPCFG, the set of probability distributions defined by the PCFG 9PCFG with the same productions as 9. Thus even though the set of WCFGs properly includes the set of PCFGs, WCFGs define exactly the same probability distributions over trees as PCFGs. This article extends these results to conditional distributions over trees conditioned on their strings. Even though the set 9Zn n &lt;� of WCFGs that define conditional distributions may be larger than 9Z&lt;∞ and properly includes �✓PCFG, the set of conditional distributions CZn&lt;</context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Chi, Zhiyi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>Parsing Inside-Out.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="14261" citStr="Goodman (1998" startWordPosition="2446" endWordPosition="2447">m. Theorem 1 For a given CFG G, CZn&lt;∞ = CZ&lt;∞. Proof Suppose we are given weights Θ for G such that GΘ ∈ GZn&lt;∞. We will show that the sequence Z1(Θ), Z2(Θ), ... is bounded by an exponential function of n, then describe a transformation on Θ resulting in a new grammar GΘy that is in GZ&lt;∞ and defines the same family of conditional distributions (i.e., ∀-r ∈ Ω(G),∀x ∈ L(G), PΘ(-r |x) = PΘi(-r |x)). First we prove that for all n ≥ 1 there exists some c such that Zn(Θ) ≤ cn. Given GΘ, we construct G¯Θ¯ in CNF that preserves the total score for any x ∈ L(G). The existence of G¯Θ¯ was demonstrated by Goodman (1998, Section 2.6), who gives an algorithm for constructing the value-preserving weighted grammar G¯Θ¯ from GΘ. Note that G = (¯N, S, Σ, ¯R), containing possibly more nonterminals and rules than G. The set of (finite) trees Ω (¯G) is different from Ω(G); the new trees must be binary and may include new nonterminals. Next, coll apse the nonterminals in N¯ into one nonterminal, S. The resulting grammar is G˘Θ˘ = (({S}, S, Σ, ˘R), ˘Θ). R˘ contains the rule S → S S and rules of the form S → a for a ∈ Σ. The weights of these rules are E˘�S→S S = R = max(1, ¯�X→Y Z) (6) (X→Y Z)∈ R¯ E˘�S→a = v = max(1, ¯</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>Goodman, Joshua T. 1998. Parsing Inside-Out. Ph.D. thesis, Harvard University, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ronald L Graham</author>
<author>Donald E Knuth</author>
<author>Oren Patashnik 1994</author>
</authors>
<title>Concrete Mathematics.</title>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<marker>Graham, Knuth, 1994, </marker>
<rawString>Graham, Ronald L., Donald E. Knuth, and Oren Patashnik.1994. Concrete Mathematics. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Joint and conditional estimation of tagging and parsing models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>314--321</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="29826" citStr="Johnson (2001)" startWordPosition="5191" endWordPosition="5192">te that even if we allow the 8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to the distribution. 488 Smith and Johnson Weighted and Probabilistic CFGs MEMM to “look back” and condition on earlier symbols (or states), it cannot represent the distribution in Example 3. ■ Generally speaking, this limitation of MEMMs has nothing to do with the estimation procedure (we have committed to no estimation procedure in particular) but rather with the conditional structure of the model. That some model structures work better than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning (2002). Our result—that the class of distributions allowed by MEMMs is a strict subset of those allowed by Mealy HMMs—makes this unsurprising. 5. Practical Implications Our result is that weighted generalizations of classical probabilistic grammars (PCFGs and HMMs) are no more powerful than the probabilistic models. This means that, insofar as log-linear models for NLP tasks like tagging and parsing are more successful than their probabilistic cousins, it is due to either (a) additional features added to the model, (b) improved estimation procedures (e.g., maximum condit</context>
</contexts>
<marker>Johnson, 2001</marker>
<rawString>Johnson, Mark. 2001. Joint and conditional estimation of tagging and parsing models. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 314–321, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<contexts>
<context position="3001" citStr="Johnson et al. 1999" startWordPosition="429" endWordPosition="432">ic Sciences, Brown University, Providence, RI 02912, USA. E-mail: Mark Johnson@brown.edu. Submission received: 30 November 2005; revised submission received: 11 January 2007; accepted for publication: 30 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 treebank, maximum likelihood estimation can be applied to learn the probability values in the model. More recently, new machine learning methods have been developed or extended to handle models of grammatical structure. Notably, conditional estimation (Ratnaparkhi, Roukos, and Ward 1994; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised contrastive estimation (Smith and Eisner 2005) have been applied to structured models. Weighted grammars learned in this way differ in two important ways from traditional, generative models. First, the weights can be any positive value; they need not sum to one. Second, features can “overlap,” and it can be difficult to design a generative model that uses such features. The benefits of new features and discriminative training methods are widely documented and recognized. This article focus</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 37th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>535--541</pages>
<location>College Park, MD.</location>
<marker></marker>
<rawString>In Proceedings of the 37th Annual Conference of the Association for Computational Linguistics, pages 535–541, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Conditional structure versus conditional estimation in NLP models.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>9--16</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="29855" citStr="Klein and Manning (2002)" startWordPosition="5194" endWordPosition="5197">allow the 8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to the distribution. 488 Smith and Johnson Weighted and Probabilistic CFGs MEMM to “look back” and condition on earlier symbols (or states), it cannot represent the distribution in Example 3. ■ Generally speaking, this limitation of MEMMs has nothing to do with the estimation procedure (we have committed to no estimation procedure in particular) but rather with the conditional structure of the model. That some model structures work better than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning (2002). Our result—that the class of distributions allowed by MEMMs is a strict subset of those allowed by Mealy HMMs—makes this unsurprising. 5. Practical Implications Our result is that weighted generalizations of classical probabilistic grammars (PCFGs and HMMs) are no more powerful than the probabilistic models. This means that, insofar as log-linear models for NLP tasks like tagging and parsing are more successful than their probabilistic cousins, it is due to either (a) additional features added to the model, (b) improved estimation procedures (e.g., maximum conditional likelihood estimation o</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2002. Conditional structure versus conditional estimation in NLP models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 9–16, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>Williamstown, MA.</location>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282–289, Williamstown, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
</authors>
<title>Equivalence of linear Boltzmann chains and hidden Markov models.</title>
<date>1996</date>
<journal>Neural Computation,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="31705" citStr="MacKay (1996)" startWordPosition="5472" endWordPosition="5473">ek, 1998). They proved that, although the conversion may not be simple (indeed, a blow-up in the automaton’s size may be incurred), given G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down automata are weakly equivalent. Importantly, the standard conversion of a CFG into a shift-reduce PDA, when applied in the stochastic case, does not always preserve the probability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further work on the relationship between weighted CFGs and weighted PDAs is described in Nederhof and Satta (2004). MacKay (1996) proved that linear Boltzmann chains (a class of weighted models that is essentially the same as Moore MRFs) express the same set of distributions as Moore HMMs, under the condition that the Boltzmann chain has a single specific end state. MacKay avoided the divergence problem by defining the Boltzmann chain always to condition on the length of the sequence; he tacitly requires all of his models to be in GZn&lt;∞. We have suggested a more applicable notion of model equivalence (equivalence of the conditional distribution) and our Theorem 1 generalizes to context-free models. 7. Conclusion We have</context>
</contexts>
<marker>MacKay, 1996</marker>
<rawString>MacKay, David J. C. 1996. Equivalence of linear Boltzmann chains and hidden Markov models. Neural Computation, 8(1):178–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<pages>591--598</pages>
<location>Palo Alto, CA.</location>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>McCallum, Andrew, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proceedings of the 17th International Conference on Machine Learning, pages 591–598, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H Mealy</author>
</authors>
<title>A method for synthesizing sequential circuits.</title>
<date>1955</date>
<journal>Bell System Technology Journal,</journal>
<pages>34--1045</pages>
<contexts>
<context position="20516" citStr="Mealy 1955" startWordPosition="3532" endWordPosition="3533">eights and which are globally normalized like a WCFG.4 We also distinguish two different types of dependency structures in these automata. Abusing the standard terminology somewhat, in a Mealy automaton arcs are labeled with output or terminal symbols, whereas in a Moore automaton the states emit terminal symbols.5 4 We admit that these names are somewhat misleading, because as we will show, chain-structured MRFs also have the Markov property and define the same joint and conditional distributions as HMMs. 5 In formal language theory both Mealy and Moore machines are finite-state transducers (Mealy 1955; Moore 1956); we ignore the input symbols here. 484 Smith and Johnson Weighted and Probabilistic CFGs A Mealy HMM defines a probability distribution over pairs (z, fr), where x� is a length-n sequence (x1, x2, ..., xn) E En and π� = (π0, π1, π2, ..., πn) E Nn+1 is a state (or nonterminal) path. The distribution is given by � � � PHMM(�x, �π) = p(xi,πi |πi−1) p(STOP |πn) (13) i=1 π0 is assumed, for simplicity, to be constant and known; we also assume that every state transition emits a symbol (no a arcs), an assumption made in typical tagging and chunking applications of HMMs. We can convert a</context>
</contexts>
<marker>Mealy, 1955</marker>
<rawString>Mealy, G. H. 1955. A method for synthesizing sequential circuits. Bell System Technology Journal, 34:1045–1079.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward F Moore</author>
</authors>
<title>Gedankenexperiments on sequential machines.</title>
<date>1956</date>
<booktitle>In Automata Studies, number 34 in Annals of Mathematics Studies.</booktitle>
<pages>129--153</pages>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ,</location>
<contexts>
<context position="20529" citStr="Moore 1956" startWordPosition="3534" endWordPosition="3535">hich are globally normalized like a WCFG.4 We also distinguish two different types of dependency structures in these automata. Abusing the standard terminology somewhat, in a Mealy automaton arcs are labeled with output or terminal symbols, whereas in a Moore automaton the states emit terminal symbols.5 4 We admit that these names are somewhat misleading, because as we will show, chain-structured MRFs also have the Markov property and define the same joint and conditional distributions as HMMs. 5 In formal language theory both Mealy and Moore machines are finite-state transducers (Mealy 1955; Moore 1956); we ignore the input symbols here. 484 Smith and Johnson Weighted and Probabilistic CFGs A Mealy HMM defines a probability distribution over pairs (z, fr), where x� is a length-n sequence (x1, x2, ..., xn) E En and π� = (π0, π1, π2, ..., πn) E Nn+1 is a state (or nonterminal) path. The distribution is given by � � � PHMM(�x, �π) = p(xi,πi |πi−1) p(STOP |πn) (13) i=1 π0 is assumed, for simplicity, to be constant and known; we also assume that every state transition emits a symbol (no a arcs), an assumption made in typical tagging and chunking applications of HMMs. We can convert a Mealy HMM to</context>
</contexts>
<marker>Moore, 1956</marker>
<rawString>Moore, Edward F. 1956. Gedankenexperiments on sequential machines. In Automata Studies, number 34 in Annals of Mathematics Studies. Princeton University Press, Princeton, NJ, pages 129–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Giorgio Satta</author>
</authors>
<title>Probabilistic parsing strategies.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>543--550</pages>
<location>Barcelona,</location>
<contexts>
<context position="31690" citStr="Nederhof and Satta (2004)" startWordPosition="5468" endWordPosition="5471">e model of Chelba and Jelinek, 1998). They proved that, although the conversion may not be simple (indeed, a blow-up in the automaton’s size may be incurred), given G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down automata are weakly equivalent. Importantly, the standard conversion of a CFG into a shift-reduce PDA, when applied in the stochastic case, does not always preserve the probability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further work on the relationship between weighted CFGs and weighted PDAs is described in Nederhof and Satta (2004). MacKay (1996) proved that linear Boltzmann chains (a class of weighted models that is essentially the same as Moore MRFs) express the same set of distributions as Moore HMMs, under the condition that the Boltzmann chain has a single specific end state. MacKay avoided the divergence problem by defining the Boltzmann chain always to condition on the length of the sequence; he tacitly requires all of his models to be in GZn&lt;∞. We have suggested a more applicable notion of model equivalence (equivalence of the conditional distribution) and our Theorem 1 generalizes to context-free models. 7. Con</context>
</contexts>
<marker>Nederhof, Satta, 2004</marker>
<rawString>Nederhof, Mark-Jan and Giorgio Satta. 2004. Probabilistic parsing strategies. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 543–550, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Salim Roukos</author>
<author>R Todd Ward</author>
</authors>
<title>A maximum entropy model for parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>803--806</pages>
<location>Yokohama, Japan.</location>
<marker>Ratnaparkhi, Roukos, Ward, 1994</marker>
<rawString>Ratnaparkhi, Adwait, Salim Roukos, and R. Todd Ward. 1994. A maximum entropy model for parsing. In Proceedings of the International Conference on Spoken Language Processing, pages 803–806, Yokohama, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>354--362</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="3153" citStr="Smith and Eisner 2005" startWordPosition="450" endWordPosition="453">eceived: 11 January 2007; accepted for publication: 30 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 treebank, maximum likelihood estimation can be applied to learn the probability values in the model. More recently, new machine learning methods have been developed or extended to handle models of grammatical structure. Notably, conditional estimation (Ratnaparkhi, Roukos, and Ward 1994; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised contrastive estimation (Smith and Eisner 2005) have been applied to structured models. Weighted grammars learned in this way differ in two important ways from traditional, generative models. First, the weights can be any positive value; they need not sum to one. Second, features can “overlap,” and it can be difficult to design a generative model that uses such features. The benefits of new features and discriminative training methods are widely documented and recognized. This article focuses specifically on the first of these differences. It compares the expressive power of weighted context-free grammars (WCFGs), where each rule is associ</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Smith, Noah A. and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 354–362, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="9490" citStr="Stolcke (1995" startWordPosition="1605" endWordPosition="1606">in R define: θX—α Zαi(©) 11 |α| i=1 ZX(©) i θX— α = where αi is the ith element of α and α is the length of α. Chi proved that Goy is a PCFG and that Po&apos;(τ) = so(τ)/Z(©) for all trees τ G Q(G). Chi did not describe how to compute the nonterminal-specific partition functions ZX(©). The ZX(©) are related by equations of the form � θX—α 11 |α |Zαi(©) ZX(©) = i=1 α:X—αER which constitute a set of nonlinear polynomial equations in ZX(©). Although a numerical solver might be employed to find the ZX(©), we have found that in practice iterative propagation of weights following the method described by Stolcke (1995, Section 4.7.1) converges quickly when Z(©) is finite. 3. Classifiers and Conditional Distributions A common application of weighted grammars is parsing. One way to select a parse tree for a sentence x is to choose the maximum weighted parse that is consistent with the observation x: τ∗(x) = argmax so(τ) (3) τEQ(G):y(τ)=x where y(τ) is the yield of τ. Other decision criteria exist, including minimum-loss decoding and re-ranked n-best decoding. All of these classifiers use some kind of dynamic programming algorithm to optimize over trees, and they also exploit the conditional distribution of t</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Stolcke, Andreas. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>Barcelona,</location>
<contexts>
<context position="3088" citStr="Taskar et al. 2004" startWordPosition="441" endWordPosition="444">u. Submission received: 30 November 2005; revised submission received: 11 January 2007; accepted for publication: 30 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 treebank, maximum likelihood estimation can be applied to learn the probability values in the model. More recently, new machine learning methods have been developed or extended to handle models of grammatical structure. Notably, conditional estimation (Ratnaparkhi, Roukos, and Ward 1994; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised contrastive estimation (Smith and Eisner 2005) have been applied to structured models. Weighted grammars learned in this way differ in two important ways from traditional, generative models. First, the weights can be any positive value; they need not sum to one. Second, features can “overlap,” and it can be difficult to design a generative model that uses such features. The benefits of new features and discriminative training methods are widely documented and recognized. This article focuses specifically on the first of these differences. It compares the expressive power of </context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Taskar, Ben, Dan Klein, Michael Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1–8, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilan Vardi</author>
</authors>
<date>1991</date>
<booktitle>Computational Recreations in Mathematica.</booktitle>
<publisher>Addison-Wesley,</publisher>
<location>Redwood City, CA.</location>
<contexts>
<context position="16335" citStr="Vardi 1991" startWordPosition="2839" endWordPosition="2840">h internal nodes undifferentiated) over a given sentence x in L(G). Every tree generated by G˘ with yield length n will have the same score: βn−1υn, because every binary tree with n terminals has exactly n − 1 nonterminals. Each tree corresponds to a way of bracketing n items, so the total number of parse trees generated by G˘ for a string of length n is the number of different ways of bracketing a sequence of n items. The total number of unlabeled binary bracketings of an n-length sequence is the nth Catalan number Cn (Graham, Knuth, and Patashnik 1994), which in turn is bounded above by 4n (Vardi 1991). The total number of strings of length n is |E|n. Therefore Zn( ˘©) = Cn|E|nβn−1υn ≤ 4n|E|nβn−1υn ≤ (4|E|βυ)n (10) We now transform the original weights © as follows. For every rule (X → α) E R, let i θX→α ← (11) (8|E|βυ)t(α) θX→α where t(α) is the number of E symbols appearing in α. This transformation results in every n-length sentence having its score divided by (8|E|βυ)n. The relative scores of trees with the same yield are unaffected, because they are all scaled equally. Therefore Goy defines the same conditional distribution over trees given sentences as Go, which implies that Go and Go</context>
</contexts>
<marker>Vardi, 1991</marker>
<rawString>Vardi, Ilan. 1991. Computational Recreations in Mathematica. Addison-Wesley, Redwood City, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wetherell</author>
</authors>
<title>Probabilistic languages: A review and some open questions. Computing Surveys,</title>
<date>1980</date>
<pages>12--361</pages>
<contexts>
<context position="7170" citStr="Wetherell (1980)" startWordPosition="1166" endWordPosition="1167">uently the partition function need not even exist. If Z(©) is finite then we say that the WCFG is convergent, and we can define a Gibbs probability distribution over Q(G) by dividing by Z(©): so(τ) Po(τ) = Z(©) A probabilistic CFG, or PCFG, is a WCFG in which the sum of the weights of the rules expanding each nonterminal is one: VXGN, � θX→α = 1 (2) (X→α)∈R It is easy to show that if Go is a PCFG then Z(©) &lt; 1. A tight PCFG is a PCFG Go for which Z(©) = 1. Necessary conditions and sufficient conditions for a PCFG to be tight are given in several places, including Booth and Thompson (1973) and Wetherell (1980). We now describe the results of Chi (1999) and Abney, McAllester, and Pereira (1999). Let g = {Go} denote the set of the WCFGs based on the CFG G (i.e., the WCFGs in g all have the same underlying grammar G but differ in their rule weight vectors ©). Let gZ&lt;∞ be the subset of g for which the partition function Z(©) is finite, and let gZ=∞ = g \ gZ&lt;∞ be the subset of g with an infinite partition function. Further let gPCFG denote the set of PCFGs based on G. In general, gPCFG is a proper subset of gZ&lt;∞, that is, every PCFG is also a WCFG, but because there are weight vectors © that don’t obey </context>
</contexts>
<marker>Wetherell, 1980</marker>
<rawString>Wetherell, C. S. 1980. Probabilistic languages: A review and some open questions. Computing Surveys, 12:361–379.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>