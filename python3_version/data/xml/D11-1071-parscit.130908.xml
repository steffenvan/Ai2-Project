<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.621599">
Collaborative Ranking: A Case Study on Entity Linking
</title>
<author confidence="0.982589">
Zheng Chen
</author>
<affiliation confidence="0.994101666666667">
Computer Science Department
Graduate Center
City University of New York
</affiliation>
<email confidence="0.995464">
zchen1@gc.cuny.edu
</email>
<sectionHeader confidence="0.994748" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996016">
In this paper, we present a new ranking
scheme, collaborative ranking (CR). In con-
trast to traditional non-collaborative ranking
scheme which solely relies on the strengths
of isolated queries and one stand-alone rank-
ing algorithm, the new scheme integrates the
strengths from multiple collaborators of a
query and the strengths from multiple ranking
algorithms. We elaborate three specific forms
of collaborative ranking, namely, micro col-
laborative ranking (MiCR), macro collabora-
tive ranking (MaCR) and micro-macro collab-
orative ranking (MiMaCR). Experiments on
entity linking task show that our proposed
scheme is indeed effective and promising.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999849238095238">
Many natural language processing tasks can be for-
malized as a ranking problem, namely to rank a
collection of candidate “objects” with respect to a
“query”. For example, intensive studies were de-
voted to parsing in which multiple possible pars-
ing trees or forests are ranked with respect to a sen-
tence (Collins, 2000; Charniak and Johnson, 2005;
Huang, 2008), machine translation in which multi-
ple translation hypotheses are ranked with respect to
a source sentence (Och, 2002; Shen et al., 2005),
anaphora resolution in which multiple antecedents
are ranked with respect to an anaphora (Yang et
al., 2008), and question answering in which mul-
tiple possible answers are ranked with respect to a
question (Ravichandran et al., 2003). Previous stud-
ies mainly focused on improving the ranking perfor-
mance using one stand-alone learning algorithm on
isolated queries.
Although a wide range of learning algorithms (un-
supervised, supervised or semi-supervised) is avail-
able, each with its strengths and weaknesses, there
</bodyText>
<note confidence="0.672988">
Heng Ji
Computer Science Department
Queens College and Graduate Center
City University of New York
</note>
<email confidence="0.984508">
hengji@cs.qc.cuny.edu
</email>
<bodyText confidence="0.999965193548387">
is not a learning algorithm that can work best on
all types of data. In such a situation, it would
be desirable to build a “collaborative” model by
integrating multiple models. Such an idea forms
the basis of ensemble methodology and it is well-
known that ensemble methods (e.g., bagging, boost-
ing) can improve the performance of many prob-
lems, in which classification is the most intensively
studied (Rokach, 2009). The other situation is re-
lated with isolated queries handled by learning al-
gorithms. The single query may not be formulated
with the best terms or the query itself may not con-
tain comprehensive information required for a high-
performance ranking algorithm. Therefore, tech-
niques of query expansion or query reformulation
can be introduced and previous research has shown
the effectiveness of those techniques in such applica-
tions as information retrieval and question answer-
ing (Manning et al., 2008; Riezler et al., 2007).
Nevertheless, previous research normally considers
query reformulation as a new query for the ranking
system, it would be more desirable to form a larger-
scale “collaborative” group for the query and make
a unified decision based on the group.
Inspired from human collaborative learning in
which two or more people form a group and ac-
complish work together, we propose a new ranking
scheme, collaborative ranking, which aims to imi-
tate human collaborative learning and enhance sys-
tem ranking performance. The main idea is to seek
collaborations for each query from two levels:
</bodyText>
<listItem confidence="0.992040428571428">
(1) query-level: search a group of query collabo-
rators, and make the joint decision from the group
together with the query using a stand-alone ranking
algorithm.
(2) ranker-level: design a group of multiple
rankers, and make the joint decision from the entire
group on a single query.
</listItem>
<page confidence="0.966008">
771
</page>
<note confidence="0.967082">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 771–781,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.7462445">
query query collaborator candidate object query collaboration group ranker ranking output
(a) non-collaborative ranking (b) micro collaborative ranking (c) macro collaborative ranking (d) micro-macro collaborative ranking
</figure>
<figureCaption confidence="0.999969">
Figure 1: Non-collaborative ranking and three collaborative ranking approaches.
</figureCaption>
<bodyText confidence="0.991623176470588">
Figure 1 presents an intuitive illustration of four
ranking approaches, including the traditional non-
collaborative ranking and three collaborative rank-
ing forms: micro collaborative ranking (MiCR),
macro collaborative ranking (MaCR), and micro-
macro collaborative ranking (MiMaCR).
Compared with the traditional non-collaborative
ranking that only leverages the information con-
tained in a single query and only applies one ranking
function (Figure 1 (a)), the three collaborative rank-
ing approaches have the following advantages:
(1)MiCR (corresponding to query-level collabo-
ration1) leverages the information contained in the
collaborators of a query. Figure 1 (b) demonstrates
that 6 query collaborators together with the query
form a query collaboration group.
(2)MaCR (corresponding to ranker-level collabo-
ration2) integrates the strengths from two or more
rankers. Figure 1 (c) demonstrates an example of 3
rankers.
(3)MiMaCR combines the advantages from
MiCR and MaCR as shown in Figure 1 (d).
In this paper, we will show the efficacy of collab-
orative ranking on the entity linking task defined in
the Knowledge Base Population (KBP) track (Ji et
al., 2010) at Text Analysis Conference (TAC). Each
query in the task is associated with a name string and
its context document. Traditional approaches for en-
tity linking only made use of the lexical or docu-
ment level information contained in the query, how-
ever, it may not be sufficient for the task. The intu-
ition why query-level collaboration may work is that
it leverages more comprehensive information about
the entity mention from multiple “collaborators” (re-
</bodyText>
<footnote confidence="0.9977125">
1Query is normally expressed by small-scale data structure,
so called micro.
2Ranker is normally implemented by large-scale algorithm,
so called macro.
</footnote>
<bodyText confidence="0.998658">
lated documents containing the name string). Fur-
thermore, previous work on this task mainly focused
on comparing one ranking algorithm with the oth-
ers, however, each ranking algorithm has its own
strengths, and therefore, ranker-level collaboration
can potentially improve the performance. Last, the
combination of query-level and ranker-level collab-
oration can lead to further performance gains.
</bodyText>
<sectionHeader confidence="0.973797" genericHeader="method">
2 Non-collaborative Ranking
</sectionHeader>
<equation confidence="0.857616">
{ }
Let q denote a query. Let o(q) = o(q)
1 , ... , o(q)
n(q)
</equation>
<bodyText confidence="0.9941698">
denote the object set associated with q, where n(q)
denotes the size of the o(q). The goal of non-
collaborative ranking is to seek a ranking function
f such that it computes ranking scores for the can-
didates in the object set, i.e., y(q) = f(o(q)) =
</bodyText>
<equation confidence="0.992618">
{ }
y(q)
1 , ... , y(q) .
n(q)
</equation>
<bodyText confidence="0.997745882352941">
Earlier studies on non-collaborative ranking
mainly explored unsupervised approaches, e.g., vec-
tor space model, link based algorithm such as
PageRank (Page et al., 1998). Unsupervised ap-
proaches are based on well-established statistical
and probability theory, nevertheless, they suffer
from some drawbacks, for example, it is hard to
tune parameters. Recently, supervised approaches
(named “learning to rank”) that automatically learn
ranking functions from training data become the fo-
cus of ranking research. In the literature, super-
vised approaches are categorized into three classes,
namely, pointwise, pairwise, and listwise. We sum-
marize a comparison of the three approaches in Ta-
ble 1. We use the following notations in the table.
Let 2 = {q1, ... , qN} denote the set of N queries
in the training data, each query qi is associated
</bodyText>
<equation confidence="0.653468">
{ }
with a set of objects o(qi) = o(qi)
1 , ... , o(qi)
n(qi)
</equation>
<bodyText confidence="0.845669">
and a set of ground-truth ranking scores y(qi) =
</bodyText>
<page confidence="0.99593">
772
</page>
<table confidence="0.994571545454546">
pointwise pairwise listwise
approach common: 1) use training samples; 2) learn the best ranking function by minimizing a given loss function;
overview 3) apply the ranking function at ranking step
transform ranking to regression or transform ranking to classification on ranking by learning from
classification on single objects object pairs lists of objects
training set ff (qi) (qi)l { x()*) &amp;()*) ./} {(x(qi) y(%&amp;))}/02,...,3
l(xj ,yj /�j=1,...,n(Qi); !&amp;quot;#,...,$ (J,-,&apos;&amp;quot;1,...,n(g0;k=1,....n(Qi):
-4&apos;; !&amp;quot;#,...,$
(%&amp;)
+1 !&amp;quot; #$ (%&amp;) &gt; #&apos;
. = 5 −1 !&amp;quot; #$ (%&amp;) ≤ #&apos; (%&amp;)
loss function pointwise loss, e.g., square pairwise loss, e.g., hinge loss(Zhang, listwise loss, e.g., cross
loss(Chen et al., 2009) 2004), exponential loss(Bartlett et al., entropy loss(Cao et al.,
2003), logistic loss(Lin, 2002) 2007),cosine loss(Qin et al.,
2007)
pros and pros: classification is well studied pros: classification is well studied pros: fully consider
cons cons:1) only consider one object cons:1) only consider pairwise relationship among objects
at a time ignoring relationship orders; 2) biased towards lists with cons: 1) less well studied in
among objects more objects theory
selected Discriminative model for IR SVM Ranking(Joachims, 2002); ListNet (Cao et al., 2007);
algorithms (Nallapati, 2004); RankBoost(Freund et al., 2003); RankCosine(Qin et al., 2007);
McRank (Li et al., 2007) RankNet(Burges et al., 2005) ListMLE (Xia et al., 2008)
</table>
<tableCaption confidence="0.999956">
Table 1: Comparison of pointwise, pairwise and listwise ranking approaches.
</tableCaption>
<figure confidence="0.304877">
Jy1qi), ... , y�( ) I. Let x3qi) = φ(qi, o(qi)j) denote
a feature vector associated with each query-object
pair (qi, o(qi)
j ).
</figure>
<sectionHeader confidence="0.96562" genericHeader="method">
3 Collaborative Ranking
</sectionHeader>
<subsectionHeader confidence="0.997482">
3.1 Micro Collaborative Ranking(MiCR)
</subsectionHeader>
<bodyText confidence="0.95667975">
Micro collaborative ranking is characterized by inte-
grating joint strengths from multiple query collabo-
rators and the query itself. It is based on the follow-
ing assumptions:
</bodyText>
<listItem confidence="0.99697">
• Expandability: Query is expandable, that is, it
is able to find potential collaborators.
• Redundancy: Collaborators and query may
share redundant information.
• Diversity: Collaborators exhibit multifaceted
information that may complement the information
contained in the query.
• Robustness: Noisy collaborators are allowable,
and they could be put under control.
</listItem>
<bodyText confidence="0.922940666666667">
Let cq(q) = {cq1, ... , cqk} be the k collabo-
rators of a query q. For each object oj(q) associ-
ated with q, we form k + 1 feature vectors x(q)
</bodyText>
<equation confidence="0.971938666666667">
j =
φ(q, o(q)
j ), x(cq1) = φ(cq1, o(cq1)
j ), ... , x(cqk) =
j j
φ(cqk, o(cqk)
</equation>
<bodyText confidence="0.687052">
j ) . Let f be a ranking function which
</bodyText>
<listItem confidence="0.882182571428571">
is obtained by either an unsupervised or supervised
approach. There are two important steps that dis-
tinguish MiCR from traditional non-collaborative
ranking approaches:
• Step (1): searching the best k collaborators of q.
• Step (2): simulating the interaction of k collab-
orators at the ranking step.
</listItem>
<bodyText confidence="0.9728275">
Solutions for step (1) can vary from case to case.
In our case study presented later, we transform
the collaborator searching problem into a clustering
problem. Collaborators of a query are then formed
by members (excluding the query) in a cluster which
contains the query and k is the size of the cluster mi-
nus one.
We transform the problem of step (2) into solv-
ing a function g1 such that a ranking score y(q)
j can
be computed for each object o(q)
j . One approach
to computing g1 is to firstly compute the ranking
scores of collaborators and query using the ranking
function f and then combine those ranking scores
in some way (Formula 1). The other approach is to
learn a supervised ranking function f′ which takes
collaborators and query as input (Formula 2).
</bodyText>
<equation confidence="0.959137333333333">
y�q) ( ( �q) 1 f ( �C9i)) f ( ��9k)11 l )
• = g1 f x x. ,..., x. J/ 1
773
)
yj = g1(•) = f′ (
(q) x(q)
j , x(cq1)
j ,..., x(cqk) (2)
j
</equation>
<bodyText confidence="0.99726775">
We present three specific forms of g1 in Formula
1, namely, max, min, and weighted. We can also
define a special case of weighted, called “average”
in which w0 = w1 ... = wk = 1/(k + 1).
</bodyText>
<equation confidence="0.879191909090909">
( ) ( )
• max: yj = max(f
(q) x(q) x(cqk)
, ... , f )
j j
( ) ( )
• min: y(q) x(q) x(cqk)
j = min(f , ... , f )
j j
• weighted: y(q) = w0 f (x�q) J 1 + r wif (x�C9i)1
iu1 J
</equation>
<bodyText confidence="0.7908725">
We will discuss three supervised versions of g1
(Formula 2) in section 4.4. A general algorithm for
MiCR is presented in Algorithm 1.
Algorithm 1 MiCR Algorithm.
Input:
a query q; a set of objects o(q); a function g1
Output:
a set of ranking scores y(q)
</bodyText>
<listItem confidence="0.859621111111111">
1: Search k collaborators of q:
cq(q) = {cq1, ... , cqk}.
2: for j = 1; j &lt;= n(q); j + + do
3: Form k + 1 feature vectors: x(q)
j , x(cq1) j, ... ,x(cqk) j.
4: Compute function y(q)
j = g1(•).
5: end for
6: return y(q)
</listItem>
<subsectionHeader confidence="0.99301">
3.2 Macro Collaborative Ranking(MaCR)
</subsectionHeader>
<bodyText confidence="0.998017666666667">
Macro collaborative ranking is characterized by in-
tegrating joint strengths from multiple rankers. It is
based on the following assumptions:
</bodyText>
<listItem confidence="0.998367285714286">
• Independence: Each ranker can make its own
ranking decisions.
• Diversity: Each ranker has its own strengths in
making ranking decisions.
• Collaboration: Rankers in the group could col-
laborate to make a consensus decision under some
mechanism.
</listItem>
<equation confidence="0.849738">
Let x(q)
� = φ(q, o(q)
</equation>
<bodyText confidence="0.969740142857143">
� ) be the feature vector formed
from the pair consisting of query q and an associated
object o(q)
� . Let F* = {f1, ... , fm} be m existing
ranking functions. We transform the computation of
collaboration among rankers into solving the follow-
ing composite function g2: (x(
</bodyText>
<equation confidence="0.9945075">
yjq) = g2 (f1 (x(q)) ... fm q)1) (3)
� �
</equation>
<bodyText confidence="0.9873715">
Similar with MiCR, g2 can be expressed by max,
min, weighted (average) respectively:
</bodyText>
<listItem confidence="0.999012666666667">
• max: yjq) = max{ fi (x�q)) 1m i=1
( )
• min: y(q) x(q)
</listItem>
<equation confidence="0.8585204">
j = min{fi 1m ji=1
�m
• weighted: y(q)
j =
i=1
</equation>
<bodyText confidence="0.912531">
It is worth noting that max and min can be use-
ful only if the ranking scores produced by various
rankers can be compared to each other directly, how-
ever, in practice, this can hardly be true.
A special form of ranking problem is that only
the best object is required as output. In this case, we
have another version of g2 which is called voting:
�m
</bodyText>
<listItem confidence="0.647384">
• voting: y(q)
j =
</listItem>
<equation confidence="0.96005">
i=1
</equation>
<bodyText confidence="0.836764">
in which sign(•) is an indicator function
{ 1 if fi outputs o(q)
j as the best object
</bodyText>
<equation confidence="0.890906">
sign(•) =
0 otherwise
</equation>
<bodyText confidence="0.800367375">
A general algorithm for MaCR is presented in Al-
gorithm 2.
Algorithm 2 MaCR Algorithm.
Input:
a query q; a set of objects o(q); a set of m rank-
ing functions F*; a composite function g2
Output:
a set of ranking scores y(q)
</bodyText>
<listItem confidence="0.9884604">
1: for j = 1; j &lt;= n(q); j + + do
2: Form a feature vector x(q)
j .
3: Compute ranking scores:f1(x(q)
j ), ... , fm(x(q)
j ).
4: Compute composite function: y(q)
j = g2(•).
5: end for
6: return y(q)
</listItem>
<subsectionHeader confidence="0.6813865">
3.3 Micro-Macro Collaborative Ranking
(MiMaCR)
</subsectionHeader>
<bodyText confidence="0.999970166666667">
The above two ranking approaches can be further
integrated into a joint model which is named Micro-
Macro Collaborative Ranking (MiMaCR). In order
to compute query-level and ranker-level collabora-
tion jointly, we solve the following complex com-
posite function g3:
</bodyText>
<equation confidence="0.9262865">
y� = g2(g1(•))
(q) (4)
</equation>
<bodyText confidence="0.980744">
in which, for each object o(q)
� , firstly we compute m
micro-ranking scores using m ranking functions on
query-level collaborators:
</bodyText>
<figure confidence="0.4995172">
wifi(x(q)) j
( )
x(q)
sign(fi )
j
</figure>
<page confidence="0.584228">
774
</page>
<equation confidence="0.9798988">
( (q) 1 f ( (cq1)1 f ( (cqk) 1)
g1 f1 xj J, 1 xj J,..., 1 xj J
m
( (9) 1 f ( (Cqi) 1 f ( (Cqk) 1)
g1 fm xj J , xj J x; J
</equation>
<bodyText confidence="0.738648636363636">
and secondly, we compute a macro-ranking score
using g2.
We can similarly define g1 and g2 as those in
MiCR and MaCR. A general algorithm for MiMaCR
is presented in Algorithm 3.
Algorithm 3 MiMaCR Algorithm.
Input:
a query q; a set of objects o(q); a set of ranking
functions F*; functions g1, g2
Output:
a set of ranking scores y(q)
</bodyText>
<listItem confidence="0.832061818181818">
1: Search k collaborators of q:
cq(q) = {cq1, ... , cqk}.
2: for j = 1; j &lt;= n(q); j + + do
3: Form k + 1 feature vectors: x�q)
j , x�cq1)
j , ... ,x�cqk)
j .
4: Compute m micro-ranking scores using F∗ and g1.
5: Compute the macro-ranking score using g2.
6: end for
7: return y(q)
</listItem>
<sectionHeader confidence="0.938534" genericHeader="method">
4 A Case Study on Entity Linking
</sectionHeader>
<bodyText confidence="0.999782125">
To demonstrate the efficacy of our collaborative
ranking scheme, we apply it to the entity linking
task defined in the TAC-KBP2010 program (Ji et
al., 2010) because there is a large amount of train-
ing and evaluation data available and various non-
collaborative ranking approaches have been pro-
posed, as summarized in (McNamee and Dang,
2009; Ji et al., 2010).
</bodyText>
<subsectionHeader confidence="0.98692">
4.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.999287">
The entity linking task aims to align a textual men-
tion of a named entity (person,organization or geo-
political) to an appropriate entry in a knowledge
base (KB), which may or may not contain the en-
tity. More formally, given a large corpus C, let q =
(q.id, q.string, q.text) denote a query in the task
which is a triple consisting of query id (q.id), name
string (q.string) and context document (q.text E
C). Let o(q) = {o(q)1, ... , o(q) I denote the candi-
date KB entries associated with the query. Each KB
entry is a tuple consisting of KB id, KB title, KB in-
fobox (a set of attribute-value pairs that summarize
or highlight the key features of the concept or sub-
ject of this entry) and KB text. The goal is to rank
the KB entries and determine whether the top en-
try id should be considered as the answer, otherwise
NIL should be returned.
A specific example of the task is as follows,
given a name string “Michael Jordan” and its con-
text document “...England Youth International goal-
keeper Michael Jordan...”. From the name string,
we retrieve a set of candidate KB entries includ-
ing “Michael Jordan (mycologist)”, “Michael Jor-
dan (footballer)”, etc. The entity linking system
should return the id of “Michael Jordan (footballer)”
as the answer, rather than the id of “Michael Jordan”
who is most well known as a basketball player.
</bodyText>
<subsectionHeader confidence="0.781225">
4.2 General Framework
</subsectionHeader>
<bodyText confidence="0.998597727272727">
A general framework of entity linking consists of
two crucial components, one for candidate gener-
ation, the other for candidate ranking, as shown
in Figure 2. In this paper, we developed the first
component by following the procedures described
in (Chen et al., 2010) which extensively leveraged
resources mined from Wikipedia. The performance
of the first component is 96.8% measured by recall
(the percentage of queries in which the candidates
cover the true answer). We then focus on the second
component.
</bodyText>
<figure confidence="0.776361">
Candidate Ranking
Answer
</figure>
<figureCaption confidence="0.968838">
Figure 2: A general framework of entity linking sys-
tem.
</figureCaption>
<subsectionHeader confidence="0.99616">
4.3 Baseline Rankers
</subsectionHeader>
<bodyText confidence="0.985617333333333">
We developed 8 baseline rankers, including 4 un-
supervised rankers (f1, f2, f3, f4) and 4 supervised
rankers(f5, f6, f7, f8).
</bodyText>
<figure confidence="0.994694166666667">
Knowledge
Base
Query
Query Expansion &amp;
Candidate Generation
I
</figure>
<page confidence="0.965593">
775
</page>
<listItem confidence="0.943513571428572">
•Naive (f1): since the answer for each query can
either be a KB id or NIL, the naive ranker simply
outputs NIL for all queries.
•Entity (f2): f2 is defined as weighted combina-
tion of entity similarities in three types (person, or-
ganization and geo-political). Name entities are ex-
tracted from q.text and KB text respectively using
Stanford NER toolkit3. The formulas to compute en-
tity similarities are defined in (Yoshida et al., 2010).
•Tfidf (f3): f3 is defined as cosine similarity be-
tween q.text and KB text using tfidf weights.
•Profile (f4): f4 is defined as profile similarity
between q.text and KB text (Chen et al., 2010).
We used a slot filling toolkit (Chen et al., 2011) to
generate the profile (attribute-value pairs) for each
query.
•Maxent (f5): a pointwise ranker implemented
using OpenNLP Maxent toolkit4 which is based on
maximum entropy model.
•SVM (f6): a pointwise ranker implemented us-
ing 5V M� ght (Joachims, 1999).
</listItem>
<bodyText confidence="0.886145333333333">
•SVM ranking (f7): a pairwise ranker imple-
mented using 5VMrank (Joachims, 2006).
•ListNet (f8): a listwise ranker presented in (Cao
et al., 2007).
The four supervised rankers apply exactly the
same set of features except that SVM ranking (f7)
needs to double expand the feature vector. The fea-
tures are categorized into three levels, surface fea-
tures (Dredze et al., 2010; Zheng et al., 2010), doc-
ument features (Dredze et al., 2010; Zheng et al.,
2010), and profiling features (entity slots that are ex-
tracted by the slot filling toolkit (Chen et al., 2011)).
</bodyText>
<subsectionHeader confidence="0.786294">
4.4 MiCR for Entity Linking
</subsectionHeader>
<bodyText confidence="0.9996015">
We convert the collaborator searching problem into
a clustering problem, i.e., for a given query q in the
task, we retrieve at most K = 300 documents from
the large corpus C, each of which contains q.string;
we then apply a clustering algorithm to generate
clusters over the documents, and form query collab-
orators (excluding q.text) from the cluster that con-
tains q.text.
We experimented the following two clustering ap-
proaches:
</bodyText>
<footnote confidence="0.999757">
3http://nlp.stanford.edu/software/CRF-NER.shtml
4http://maxent.sourceforge.net/about.html
</footnote>
<bodyText confidence="0.832700296296296">
(1)agglomerative clustering: it iteratively merges
clusters from singleton documents until a stop
threshold is reached. Document similarity is de-
fined as cosine similarity using tfidf weights. We ap-
plied group-average linking strategy to merge clus-
ters (Manning et al., 2008).
(2)graph-based clustering: it iteratively partitions
clusters from one single cluster until a stop threshold
is reached. Document similarity is similarly defined
as agglomerative clustering. We selected normalized
spectral clustering as our clustering algorithm (Shi
and Malik, 2000).
We first selected f3 as our basic ranking func-
tion, and investigated whether the ranker can ben-
efit from query collaborators formed by either ag-
glomerative clustering or graph clustering. We im-
plemented three versions of composite function g1
(max, min and average), and experimented their per-
formance using three unsupervised rankers f2, f3, f4
respectively.
Last, we implemented three supervised versions
of g1 (Maxent, SVM and ListNet respectively) by
adding cluster-level features and retraining the mod-
els in three supervised rankers f5, f6, f8 respec-
tively. Cluster-level features include maximum,
minimum, average tfidf/entity similarities between
the candidate and the query collaboration group.
</bodyText>
<subsectionHeader confidence="0.505279">
4.5 MaCR for Entity Linking
</subsectionHeader>
<bodyText confidence="0.999975571428571">
We implemented two versions of composite func-
tion g2, average and voting. Furthermore, we in-
vestigated how the performance can be affected by
incrementally adding more rankers into the ranker
set F*. To do so, we first sorted the 8 rankers ac-
cording to their performance on the development set
from the highest to the lowest, and starting with the
highest performance ranker, we added one ranker at
a time, until we have all the 8 rankers. It is worth
noting that, when there are even number of rankers
in the set F*, “ties” could take place using voting
function. In order to break the ties, we rank the
candidate higher if it is output as the answer from
a higher performance ranker.
</bodyText>
<subsectionHeader confidence="0.537239">
4.6 MiMaCR for Entity Linking
</subsectionHeader>
<bodyText confidence="0.971698333333333">
We investigated how the final performance can be
boosted by jointly computing micro-ranking scores
and macro-ranking score.
</bodyText>
<page confidence="0.998423">
776
</page>
<sectionHeader confidence="0.999117" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997885">
5.1 Data and Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.9684415">
We used TAC-KBP2009 evaluation data as our train-
ing (75%) and development set (25%), and used
TAC-KBP2010 evaluation data as our blind testing
set (shown in Table 2).
</bodyText>
<table confidence="0.99734475">
Corpus Queries
PER ORG GPE Total
Training&amp;Dev 627 2710 567 3904
Testing 750 750 750 2250
</table>
<tableCaption confidence="0.999752">
Table 2: Training, development and testing corpus.
</tableCaption>
<bodyText confidence="0.9994025">
The reference KB consists of 818,741 entries
which are extracted from an October 2008 dump of
English Wikipedia. The source text corpus (denoted
as C in section 4.1) consists of 1,777,888 documents
in 5 genres (mostly Newswire and Web Text).
We used the official evaluation metric for TAC-
KBP2010 entity linking task, that is, micro-averaged
accuracy. It is computed by
</bodyText>
<equation confidence="0.644488">
micro-averaged accuracy =
#correct answers
#queries
</equation>
<bodyText confidence="0.961200333333333">
An answer is considered as correct if the system
output (either a KB entry id or NIL) exactly matches
the key.
</bodyText>
<subsectionHeader confidence="0.996349">
5.2 Performance of 8 Baseline Rankers
</subsectionHeader>
<bodyText confidence="0.999979451612903">
Table 3 shows the performance of the 8 baseline
rankers in 4 columns: Overall for all queries, PER
for person queries, ORG for organization queries,
and GPE for geo-political queries. Each column
is further split into All, KB (for Non-NIL queries)
and NIL (for NIL queries). It shows that all the four
supervised rankers perform better than the four un-
supervised rankers. Naive ranker obtains the low-
est overall micro-average accuracy (54.5%) but the
highest NIL accuracy (100%). Among the four un-
supervised rankers, profile ranker performs the best,
which clearly shows that the extracted attributes of
entities are effective for disambiguating confusable
names. For example, our data analysis shows that
the attribute value of “per:alternative-name” from
the context document is particularly useful if a per-
son query is only mentioned by its last name. The
attribute “per:title” is another important indicator to
discriminate one person from the other. For geo-
political queries, if the query is a city name, at-
tribute “gpe:state” is useful to distinguish cities with
the same name but in different states or provinces.
Among the four supervised rankers, ListNet outper-
forms SVM ranking and then SVM ranking outper-
forms the two pointwise rankers. It may confirm
previous research findings that listwise ranking is
superior to pairwise ranking and pairwise ranking
is superior to pointwise ranking (Cao et al., 2007;
Zheng et al., 2010). The best baseline ranker (List-
Net) obtains an absolute overall accuracy gain of
26.6% over the naive ranker.
</bodyText>
<subsectionHeader confidence="0.998722">
5.3 Impact of MiCR
</subsectionHeader>
<bodyText confidence="0.999685088235294">
To study the impact of MiCR, we first select f3
(tfidf ranker) as our ranking function. Figure 3
shows the performance of applying different query
collaborator searching strategies (graph or agglom-
erative clustering) and different versions of g1 (av-
erage, max and min respectively). We intention-
ally adjust the meaning of threshold (x-axis) for both
graph clustering and agglomerative clustering, such
that at threshold 0, both clustering algorithms gen-
erate the largest number of clusters (i.e., each doc-
ument is a cluster), and at threshold 1, they gen-
erate only one cluster. We now take the average
function (Figure 3 (a)) into considerations, as graph
clustering algorithm gradually partitions from one
cluster (corresponding to threshold 1) to more clus-
ters, the number of query collaborators gradually re-
duces, meanwhile, the accuracy gradually increases
and reaches the highest (73.6%) at threshold of 0.45,
which clearly shows that removing noisy collabora-
tors in the query collaboration group can improve
the performance. As the threshold continues drop-
ping below 0.45, the number of query collaborators
reduces and the performance significantly drops un-
til it reaches the baseline performance of tfidf ranker
(68.3%). It clearly shows that maintaining a control-
lable number of query collaborators can improve the
performance. For the agglomerative clustering, it is
the other story. As it continues merging from sin-
gleton clusters (corresponding to threshold 0) to one
single cluster, the performance continues increasing
until in the end it reaches the highest accuracy of
72.6%. However, unlike graph clustering, a peak
never appears in the middle which implies that ag-
glomerative clustering is inferior to graph clustering.
</bodyText>
<page confidence="0.994081">
777
</page>
<table confidence="0.9996511">
Overall (%) PER (%) ORG (%) GPE (%)
All KB NIL All KB NIL All KB NIL All KB NIL
Naive 54.5 0.0 100 70.8 0.0 100 59.7 0 100 33.0 0 100
Entity 65.6 48.6 79.7 82.1 52.1 94.5 68.4 46.2 83.3 46.1 48.5 41.3
Tfidf 68.3 45.0 87.7 83.6 54.3 95.7 66.2 45.9 80.0 54.9 40.3 84.6
Profile 75.0 58.7 88.6 90.8 82.2 94.4 73.3 62.7 80.4 61.0 46.1 91.1
Maxent 77.4 72.3 81.6 86.5 82.6 94.4 73.3 62.7 80.4 61.0 71.5 72.1
SVM 78.1 73.0 82.3 91.1 81.7 94.9 78.7 70.0 84.6 64.4 71.1 51.0
SVM Rank 80.3 66.7 91.7 91.3 76.3 97.6 77.3 59.7 89.1 72.3 66.7 83.8
ListNet 81.1 69.7 90.6 90.8 77.6 96.2 79.0 64.0 89.1 73.5 69.7 81.4
</table>
<figureCaption confidence="0.9822505">
Figure 3: MiCR: comparison of average, max, and min functions combined with Graph and Agglomerative
(Aggr)-based query collaborator searching strategies (tfidf ranker).
</figureCaption>
<figure confidence="0.989496">
70.5
70.0
Micro-Average Accuracy (%)
72.6
69.5
69.0
68.5
68.0
67.5
67.0
(a) Average Function Threshold
Threshold
Threshold
(b) Max Function
(c) Min Function
Table 3: Comparison of 8 baseline rankers.
Graph-Ave
Aggr-Ave
73.6
68.3
68
72
71
70
69
Micro-Average Accuracy (%)
74
73
0.0 0.2 0.4 0.6 0.8 1.0
70.2
Graph-Max
Aggr-Max
69.0
68.3
67.4
0.0 0.2 0.4 0.6 0.8 1.0
Graph-Min
Aggr-Min
68.3
56.7
56
0.0 0.2 0.4 0.6 0.8 1.0
66
64
62
60
58
Micro-Average Accuracy (%)
70
</figure>
<page confidence="0.608416">
68
</page>
<bodyText confidence="0.999937085714286">
The max function (Figure 3 (b)) leverages the
strengths from the strongest collaborator in the
group, which can potentially improve KB accuracy,
but meanwhile hurt NIL accuracy. As shown in the
figure, as more collaborators join in the group, the
performance increases first for both graph and ag-
glomerative clustering, however, it starts to deterio-
rate when arriving at a threshold, and in the end, the
performance drops even lower than the baseline of
tfidf ranker.
The min function (Figure 3 (c)) leverages the
strengths from the weakest collaborator in the group,
which can potentially improve NIL accuracy, but
meanwhile hurt KB accuracy. Our data analysis
shows that the gain in NIL accuracy can not afford
the larger loss in non-NIL accuracy, therefore, the
performance continues dropping as the threshold in-
creases. Min function is a counter example showing
that searching query collaborators can not always
lead to benefits.
To summarize so far, the best strategy for tfidf
ranker in MiCR approach is graph-ave (applying
graph clustering and using average function) which
obtains overall accuracy gain of 5.3% over the base-
line (68.3%). We further validate the performance
of graph-ave using f2, f4 ranking functions, for en-
tity ranker, we obtain accuracy gain of 6.3%, and for
profile ranker, we obtain accuracy gain of 3.0%.
We then experiment the three supervised g1 func-
tions (ListNet, Maxent, and SVM respectively)
using graph clustering as the query collaborator
searching strategy. Figure 6 shows that ListNet,
Maxent, SVM rankers obtain accuracy gain of 1.4%,
4.6%, 4.2% respectively over the baselines (corre-
sponding to those points at threshold 0).
</bodyText>
<subsectionHeader confidence="0.999632">
5.4 Impact of MaCR
</subsectionHeader>
<bodyText confidence="0.999817272727273">
Figure 4 shows that the MaCR approach obtains
absolute accuracy gain of 1.3% (voting function)
and 0.5% (average function) over the best baseline
ranker (81.1%) when we add the 7th ranker (entity
ranker). The improvement of voting function is sta-
tistically significant at a 99.6% confidence level by
conducting Wilcoxon Matched-Pairs Signed-Ranks
Test on the 10 folds of the testing set. However, the
improvement of average function is not significant at
the 0.05 level which implies that average is inferior
to voting. We observe that the performance drops
</bodyText>
<page confidence="0.994856">
778
</page>
<bodyText confidence="0.999519789473684">
when there are even number of rankers in the ranker
set using voting function, which implies that our tie
breaking strategy is not very effective.
We also experimented the voting function on the
top 10 KBP2009 entity linking systems (each sys-
tem performance is shown in the table embedded in
Figure 5, and experiment is similarly done as de-
scribed in section 4.5). Figure 5 shows that it can
obtain absolute accuracy gain of 4.7% over the top
entity linking system (82.2%). The reasons why we
achieve relative smaller gains using our own ranker
set are as follows: (1) we use the same candidate
object set for all rankers, while different KBP2009
systems may use their own set of objects. (2) our
top 4 supervised rankers apply almost the same set
of features, while different KBP2009 systems may
apply more diversified features. Therefore, diversity
is a highly important factor that makes MaCR ap-
proach effective.
</bodyText>
<figureCaption confidence="0.9108475">
Figure 5: MaCR: applying voting function to the top
10 KBP2009 entity linking systems.
</figureCaption>
<subsectionHeader confidence="0.998641">
5.5 Impact of MiMaCR
</subsectionHeader>
<bodyText confidence="0.993995857142857">
We applied the following settings in our Mi-
MaCR approach: selecting graph clustering as the
query collaborator searching strategy, including five
rankers (tfidf, entity, Maxent, SVM and ListNet) in
the ranker set, using average function to compute
micro-ranking scores for the tfidf and entity ranker,
using the three corresponding supervised versions
of gi to compute micro-ranking scores for Maxent,
SVM and ListNet respectively, and finally apply-
ing voting function to compute the macro-ranking
score. In Figure 6, the curve of “MiMaCR” shows
how the performance of MiMaCR is affected by
the threshold in graph clustering. We obtain the
best micro-average accuracy of 83.7% at threshold
0.3, which is 2.6 % higher than the best baseline
ranker (81.1%). The improvement is statistically
significant at a 98.6% confidence level by conduct-
ing Wilcoxon Matched-Pairs Signed-Ranks Test on
the 10 folds of the testing set. The score reported
here is on par with the second best in the KBP2010
evaluation.
</bodyText>
<figureCaption confidence="0.728364333333333">
Figure 6: MiMaCR: Comparison of MiMaCR and
three supervised versions of gi (ListNet, Maxent,
and SVM respectively).
</figureCaption>
<sectionHeader confidence="0.999888" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999719761904762">
In the literature of information retrieval, query ex-
pansion is a useful technique that involves the pro-
cess of reformulating a query, and as a consequence,
is capable to extend the ability of a query and im-
prove the retrieval performance. Various approaches
for query expansion have been proposed, as summa-
rized in (Manning et al., 2008). The MiCR presented
in this paper is superior to query expansion in two
aspects, firstly, we leverage more information con-
tained in multiple query collaborators; secondly, we
place great emphasis on interactions among mem-
bers in the query collaboration group.
In the literature of machine learning, there
has been a considerable amount of research on
ensemble-based classification, which is to build a
predictive classification model by integrating multi-
ple classifiers. A comprehensive survey is presented
in (Rokach, 2009). In contrast, ensemble-based
ranking has only recently attracted research interests
(Hoi and Jin, 2008; Wei et al., 2010). Although the
MaCR presented here is in essence ensemble-based
</bodyText>
<figure confidence="0.96132">
0 1 2 3 4 5 6 7 8 9
#rankers
</figure>
<figureCaption confidence="0.957201">
Figure 4: MaCR: comparison of voting and average.
</figureCaption>
<figure confidence="0.999407000000001">
0 1 2 3 4 5 6 7 8 9 10 11
#systems (rankers)
Micro-Average Accuracy (%)
86.9
85.8
85.9
86.4
85
85
85.1
84.6
System ID Performance
1
82.2
84
2 80.3
3 79.8
4
78.8
83
5 76.7
6 73.5
7
71.1
82.2
82.2
82
7
83.
8 68.2
9 65.9
10 59.6
87
86
82.6
82.4
82.2
Micro-Average Accuracy(%)
82.0
81.8
81.6
81.4
81.2
81.0
80.8
81.7
81.7
81.2
81.4
81.1
81.3
81.2
82.2
82.4
81.8
81.6 81.6
81.1
81.0
80.8
voting
average
0.0 0.2 0.4 0.6 0.8 1.0
Threshold
84.0
83.5
83.0
82.5
Micro-Average Accuracy (%)
82.0
81.5
81.0
80.5
80.0
79.5
79.0
78.5
78.0
77.5
77.0
83.7
82.3
82.5
82.3
82.0
81.
1
78.
1
77.4
MiMaCR
ListNet
Maxent
SVM
</figure>
<page confidence="0.99085">
779
</page>
<bodyText confidence="0.957965369565218">
ranking, we extend it to MiMaCR which integrates plied, of the Army Research Laboratory or the U.S.
the strengths from both MiCR and MaCR. Government. The U.S. Government is authorized
It is worth noting that “collaborative ranking” pre- to reproduce and distribute reprints for Govern-
sented here should be distinguished from “collabo- ment purposes notwithstanding any copyright nota-
rative filtering” in that “collaborative filtering” uses tion hereon.
the known preferences of a group of users to gen- References
erate personalized recommendations while “collab- P. L. Bartlett, M. I. Jordan and J. D. McAuliffe. 2003.
orative ranking” leverages query collaborators and Convexity, classification, and risk bounds. Technical
ranker collaborators to enhance the overall ranking Report 638, Statistics Department, University of Cali-
performance. fornia, Berkeley.
There has been an increasing amount of research C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
on entity linking, especially through KBP2009 and N. Hamilton, and G. Hullender. 2005. Learning to
KBP2010. Various unsupervised or supervised ap- Rank Using Gradient Descent. In Proceedings of the
proaches have been proposed, as summarized in 22th International Conference on Machine Learning
(McNamee and Dang, 2009; Ji et al., 2010). How- (ICML 2005).
ever, most of the previous research mainly fo- Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai and H. Li. 2007.
cused on one or two ranking algorithms on isolated Learning to rank: from pairwise approach to listwise
queries. In this paper, we have extended the work approach In Proceedings of the 24th International
by systematically studying the possibility of perfor- Conference on Machine Learning (ICML 2007), pages
mance enhancement through query-level collabora- 129-136.
tion and ranker-level collaboration. E. Charniak and M. Johnson. 2005. Coarseto-fine-
7 Conclusions grained n-best parsing and discriminative reranking.
We presented a new ranking scheme called collab- In ACL-05, pages 173-180.
orative ranking with three specific forms, MiCR, W. Chen, T.-Y. Liu, Y. Lan, Z. Ma, and H. Li. 2009.
MaCR and MiMaCR and demonstrated its effective- Ranking measures and loss functions in learning to
ness on entity linking task. However, our scheme is rank. In Advances in Neural Information Processing
not restricted to this specific task and it is generally Systems 22 (NIPS 2009), pages 315-323.
applicable to many other other applications such as Z. Chen, S. Tamang, A. Lee, X. Li, W.-P. Lin, M. Snover,
question answering. In MiCR, effective searching J. Artiles, M. Passantino and H. Ji. 2010. CUN-
of query collaborators and active interplay among YBLENDER TAC-KBP2010 Entity Linking and Slot
members in the query collaboration group are two Filling System Description. In Proceedings of TextAn-
key factors that make MiCR successful. In MaCR, alytics Conference (TAC2010).
diversity is a highly important factor to make it suc- Z. Chen, S. Tamang, A. Lee and H. Ji. 2011. A Toolkit
cessful. Overall, MiMaCR can bootstrap the per- for Knowledge Base Population. In SIGIR.
formance to its maximum if integrating MiCR and M. Collins. 2000. Discriminative reranking for natural
MaCR properly. However, the better performance is language parsing. In Proceedings of the 17th Interna-
at the expense of much more computations. tional Conference on Machine Learning (ICML 2000),
Acknowledgments pages 175-182.
This work was supported by the U.S. Army Re- M. Dredze, P. McNamee, D. Rao, A. Gerber and T. Finin.
search Laboratory under Cooperative Agreement 2010. Entity Disambiguation for Knowledge Base
Number W911NF-09-2-0053, the U.S. NSF CA- Population. In Proc. COLING 2010.
REER Award under Grant IIS-0953149 and PSC- Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2003.
CUNY Research Program. The views and con- An efficient boosting algorithm for combining pref-
clusions contained in this document are those of erences. In Journal of Machine Learning Research,
the authors and should not be interpreted as repre- 4:933-969.
senting the official policies, either expressed or im- S. Hoi and R. Jin. 2008. Semi-supervised ensemble
</bodyText>
<reference confidence="0.987232141304348">
780 ranking. In Proc. of the 23rd AAAI Conf. on Artificial
Intelligence.
L. Huang. 2008. Forest Reranking: Discriminative Pars-
ing with Non-Local Features. In ACL-HLT-08, pages
586-594.
H. Ji, R. Grishman, H. T. Dang and K. Griffit. 2010. An
Overview of the TAC2010 Knowledge Base Popula-
tion Track. In Proceedings of Text Analytics Confer-
ence (TAC2010).
T. Joachims. 1999. Making large-Scale SVM Learn-
ing Practical. Advances in Kernel Methods - Support
Vector Learning, B. Sch¨olkopf and C. Burges and A.
Smola (ed.), MIT-Press, 1999.
T. Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the 8th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining(KDD 2002).
T. Joachims. 2006. Training Linear SVMs in Linear
Time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Y. Lan, T.-Y. Liu, T. Qin, Z. Ma, and H. Li. 2008. Query-
level stability and generalization in learning to rank.
In Proceedings of the 25th International Conference
on Machine Learning (ICML 2008), pages 512-519.
P. Li, C. Burges, and Q. Wu. 2007. Mcrank: Learn-
ing to rank using multiple classification and gradient
boosting In Advances in Neural Information Process-
ing Systems 20 (NIPS2007).
Y. Lin. 2002. Support vector machines and the bayes
rule in classification. In Data Mining and Knowledge
Discovery, pages 259-275.
C. D. Manning, P. Raghavan and H. Sch¨utze. 2008 . In-
troduction to Information Retrieval. Cambridge Uni-
versity Press.
P. McNamee and H. Dang. 2009. Overview of the TAC
2009 Knowledge Base Population Track. In Proceed-
ings of TAC.
R. Nallapati. 2004. Discriminative models for informa-
tion retrieval. In SIGIR.
F. J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, Computer Science Department, RWTH
Aachen, Germany, October.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The PageRank Citation Ranking: Bringing Order to
the Web. Technical report, Stanford Digital Library
Technologies Project.
T. Qin, X.-D. Zhang, M.-F. Tsai, D.-S. Wang, T.-Y. Liu
and H. Li. 2007. Query-level loss functions for infor-
mation retrieval. In Information Processing and Man-
agement.
T. Qin, T.-Y. Liu, X.-D. Zhang,D.-S. Wang, and H. Li.
2008. Global Ranking Using Continuous Conditional
Random Fields. In Advances in Neural Information
Processing Systems 21 (NIPS 2008).
D. Ravichandran, E. Hovy and F. J. Och. 2003. Statis-
tical QA - Classifier vs. Re-ranker: What’s the differ-
ence? In Proceedings of the ACL Workshop on Multi-
lingual Summarization and Question Answering.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal
and Y. Liu. 2007. Statistical Machine Translation for
Query Expansion in Answer Retrieval. In Proceedings
of ACL.
L. Rokach. 2009. Ensemble-based classifiers. Artif In-
tell Rev DOI 10.1007/s10462-009-9124-7.
L. Shen, A. Sarkar, and F. J. Och. 2005. Discriminative
reranking for machine translation. In Proceedings of
HLT-NAACL.
J. Shi and J. Malik. 2000. Normalized Cuts and Image
Segmentation. In Machine Intelligence, vol. 22, no. 8,
pages 888-905.
F. Wei, W. Li and S. Liu. 2010. iRANK: A rank-learn-
combine framework for unsupervised ensemble rank-
ing. In Journal of the American Society for Infor-
mation Science and Technology,61: 1232C1243. doi:
10.1002/asi.21296.
X. Yang and J. Su and C.L. Tan 2008. A Twin-Candidate
Model for Learning-based Anaphora Resolution. In
Computational Linguistics, vol. 34, no. 3, pages 327-
356.
M. Yoshida, M. Ikeda, S. Ono, I. Sato, and H. Nakagawa.
2010. Person name disambiguation by boostrapping.
In SIGIR.
T. Zhang. 2004. Statistical analysis of some multicate-
gory large margin classification methods. In Journal
of Machine Learning Research, 5, 1225-1251.
W. Zhang, J. Su, C. L. Tan and W.T. Wang. 2010. Entity
Linking Leveraging Automatically Generated Annota-
tion. In Proc. COLING 2010.
Z. Zheng, F. Li, M. Huang, X. Zhu. 2010. Learning to
Link Entities with Knowledge Base. In Proc. HLT-
NAACL2010.
</reference>
<page confidence="0.997891">
781
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953149">
<title confidence="0.999431">Collaborative Ranking: A Case Study on Entity Linking</title>
<author confidence="0.976772">Zheng</author>
<affiliation confidence="0.997044666666667">Computer Science Graduate City University of New</affiliation>
<email confidence="0.994957">zchen1@gc.cuny.edu</email>
<abstract confidence="0.9993538125">In this paper, we present a new ranking collaborative ranking In contrast to traditional non-collaborative ranking scheme which solely relies on the strengths of isolated queries and one stand-alone ranking algorithm, the new scheme integrates the strengths from multiple collaborators of a query and the strengths from multiple ranking algorithms. We elaborate three specific forms of collaborative ranking, namely, micro collaborative ranking (MiCR), macro collaborative ranking (MaCR) and micro-macro collaborative ranking (MiMaCR). Experiments on entity linking task show that our proposed scheme is indeed effective and promising.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>