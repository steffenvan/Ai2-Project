<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000060">
<title confidence="0.9858235">
Learning Sentential Paraphrases from Bilingual Parallel Corpora
for Text-to-Text Generation
</title>
<author confidence="0.990484">
Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme
</author>
<affiliation confidence="0.9701155">
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.978729" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999800368421053">
Previous work has shown that high quality
phrasal paraphrases can be extracted from
bilingual parallel corpora. However, it is not
clear whether bitexts are an appropriate re-
source for extracting more sophisticated sen-
tential paraphrases, which are more obviously
learnable from monolingual parallel corpora.
We extend bilingual paraphrase extraction to
syntactic paraphrases and demonstrate its abil-
ity to learn a variety of general paraphrastic
transformations, including passivization, da-
tive shift, and topicalization. We discuss how
our model can be adapted to many text gener-
ation tasks by augmenting its feature set, de-
velopment data, and parameter estimation rou-
tine. We illustrate this adaptation by using
our paraphrase model for the task of sentence
compression and achieve results competitive
with state-of-the-art compression systems.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99907988">
Paraphrases are alternative ways of expressing the
same information (Culicover, 1968). Automatically
generating and detecting paraphrases is a crucial as-
pect of many NLP tasks. In multi-document sum-
marization, paraphrase detection is used to collapse
redundancies (Barzilay et al., 1999; Barzilay, 2003).
Paraphrase generation can be used for query expan-
sion in information retrieval and question answer-
ing systems (McKeown, 1979; Anick and Tipirneni,
1999; Ravichandran and Hovy, 2002; Riezler et al.,
2007). Paraphrases allow for more flexible matching
of system output against human references for tasks
like machine translation and automatic summariza-
tion (Zhou et al., 2006; Kauchak and Barzilay, 2006;
Madnani et al., 2007; Snover et al., 2010).
Broadly, we can distinguish two forms of para-
phrases: phrasal paraphrases denote a set of surface
text forms with the same meaning:
the committee’s second proposal
the second proposal of the committee
while syntactic paraphrases augment the surface
forms by introducing nonterminals (or slots) that are
annotated with syntactic constraints:
the NP1’s NP2
the NP2 of the NP1
It is evident that the latter have a much higher poten-
tial for generalization and for capturing interesting
paraphrastic transformations.
A variety of different types of corpora (and se-
mantic equivalence cues) have been used to auto-
matically induce paraphrase collections for English
(Madnani and Dorr, 2010). Perhaps the most nat-
ural type of corpus for this task is a monolingual
parallel text, which allows sentential paraphrases to
be extracted since the sentence pairs in such corpora
are perfect paraphrases of each other (Barzilay and
McKeown, 2001; Pang et al., 2003). While rich syn-
tactic paraphrases have been learned from monolin-
gual parallel corpora, they suffer from very limited
data availability and thus have poor coverage.
Other methods obtain paraphrases from raw
monolingual text by relying on distributional simi-
larity (Lin and Pantel, 2001; Bhagat and Ravichan-
dran, 2008). While vast amounts of data are
readily available for these approaches, the distri-
butional similarity signal they use is noisier than
the sentence-level correspondency in parallel cor-
pora and additionally suffers from problems such as
mistaking cousin expressions or antonyms (such as
{boy, girl} or {rise, fall}) for paraphrases.
</bodyText>
<page confidence="0.951532">
1168
</page>
<note confidence="0.957765">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.916809787234043">
Abundantly available bilingual parallel corpora phrase table, English paraphrases are obtained by
have been shown to address both these issues, ob- pivoting through foreign language phrases. Since
taining paraphrases via a pivoting step over foreign many paraphrases can be extracted for a phrase,
language phrases (Bannard and Callison-Burch, Bannard and Callison-Burch rank them using a para-
2005). The coverage of paraphrase lexica extracted phrase probability defined in terms of the translation
from bitexts has been shown to outperform that model probabilities p(f|e) and p(e|f):
obtained from other sources (Zhao et al., 2008a). �p(e2|e1) = p(e2,f|e1) (1)
While there have been efforts pursuing the extrac- f
tion of more powerful paraphrases (Madnani et �= p(e2|f, e1)p(f|e1) (2)
al., 2007; Callison-Burch, 2008; Cohn and Lapata, f
2008; Zhao et al., 2008b), it is not yet clear to what E≈ p(e2|f)p(f|e1). (3)
extent sentential paraphrases can be induced from f
bitexts. In this paper we: Several subsequent efforts extended the bilin-
• Extend the bilingual pivoting approach to para- gual pivoting technique, many of which introduced
phrase induction to produce rich syntactic para- elements of more contemporary syntax-based ap-
phrases. proaches to statistical machine translation. Mad-
• Perform a thorough analysis of the types of nani et al. (2007) extended the technique to hier-
paraphrases we obtain and discuss the para- archical phrase-based machine translation (Chiang,
phrastic transformations we are capable of cap- 2005), which is formally a synchronous context-free
turing. grammar (SCFG) and thus can be thought of as a
• Describe how training paradigms for syntac- paraphrase grammar. The paraphrase grammar can
tic/sentential paraphrase models should be tai- paraphrase (or “decode”) input sentences using an
lored to different text-to-text generation tasks. SCFG decoder, like the Hiero, Joshua or cdec MT
• Demonstrate our framework’s suitability for a systems (Chiang, 2007; Li et al., 2009; Dyer et al.,
variety of text-to-text generation tasks by ob- 2010). Like Hiero, Madnani’s model uses just one
taining state-of-the-art results on the example nonterminal X instead of linguistic nonterminals.
task of sentence compression. Three additional efforts incorporated linguistic
2 Related Work syntax. Callison-Burch (2008) introduced syntac-
Madnani and Dorr (2010) survey a variety of data- tic constraints by labeling all phrases and para-
driven paraphrasing techniques, categorizing them phrases (even non-constituent phrases) with CCG-
based on the type of data that they use. These inspired slash categories (Steedman and Baldridge,
include large monolingual texts (Lin and Pantel, 2011), an approach similar to Zollmann and Venu-
2001; Szpektor et al., 2004; Bhagat and Ravichan- gopal (2006)’s syntax-augmented machine transla-
dran, 2008), comparable corpora (Barzilay and Lee, tion (SAMT). Callison-Burch did not formally de-
2003; Dolan et al., 2004), monolingual parallel cor- fine a synchronous grammar, nor discuss decoding,
pora (Barzilay and McKeown, 2001; Pang et al., since his presentation did not include hierarchical
2003), and bilingual parallel corpora (Bannard and rules. Cohn and Lapata (2008) used the GHKM
Callison-Burch, 2005; Madnani et al., 2007; Zhao et extraction method (Galley et al., 2004), which is
al., 2008b). We focus on the latter type of data. limited to constituent phrases and thus produces
Paraphrase extraction using bilingual parallel cor- a reasonably small set of syntactic rules. Zhao
pora was proposed by Bannard and Callison-Burch et al. (2008b) added slots to bilingually extracted
(2005) who induced paraphrases using techniques paraphrase patterns that were labeled with part-of-
from phrase-based statistical machine translation speech tags, but not larger syntactic constituents.
(Koehn et al., 2003). After extracting a bilingual Before the shift to statistical natural language pro-
1169 cessing, paraphrasing was often treated as syntactic
transformations or by parsing and then generating
sie versuchten das leck mit einer detonation zu schliessen
</bodyText>
<figure confidence="0.992637857142857">
they
PRP VBD
NP
tried closing the
S
VP VP
VBG
DT NN IN DT NN
NP NP
leak with a blast
PP
NP - das leck  |the leak
PP/NN - mit einer  |with a
VP - NP PP/NN detonation zu schliessen  |closing NP PP/NN blast
</figure>
<figureCaption confidence="0.798853666666667">
Figure 1: Synchronous grammar rules for translation are
extracted from sentence pairs in a bixtext which have
been automatically parsed and word-aligned. Extraction
methods vary on whether they extract only minimal rules
for phrases dominated by nodes in the parse tree, or more
complex rules that include non-constituent phrases.
</figureCaption>
<bodyText confidence="0.945931266666667">
where the rule’s left-hand side C ∈ N is a nonter-
minal, γ ∈ (N ∪TS)* and α ∈ (N ∪TT)* are strings
of terminal and nonterminal symbols with an equal
number of nonterminals cNT (γ) = cNT (α) and
∼: {1 ... cNT(γ)} → {1 ... cNT(α)}
constitutes a one-to-one correspondency function
between the nonterminals in γ and α. A non-
negative weight w ≥ 0 is assigned to each rule, re-
flecting the likelihood of the rule.
Rule Extraction Phrase-based approaches to sta-
tistical machine translation (and their successors)
extract pairs of (e, f) phrases from automatically
word-aligned parallel sentences. Och (2003b)
described various heuristics for extracting phrase
alignments from the Viterbi word-level alignments
that are estimated using Brown et al. (1993) word-
alignment models.
These phrase extraction heuristics have been ex-
tended so that they extract synchronous grammar
rules (Galley et al., 2004; Chiang, 2005; Zollmann
and Venugopal, 2006; Liu et al., 2006). Most of
these extraction methods require that one side of the
parallel corpus be parsed. This is typically done au-
tomatically with a statistical parser.
Figure 1 shows examples of rules obtained from
a sentence pair. To extract a rule, we first choose a
source side span f like das leck. Then we use phrase
extraction techniques to find target spans e that are
consistent with the word alignment (in this case the
from a semantic representation (McKeown, 1979;
Muraki, 1982; Meteer and Shaked, 1988; Shem-
tov, 1996; Yamamoto, 2002). Indeed, some work
generated paraphrases using (non-probabilistic) syn-
chronous grammars (Shieber and Schabes, 1990;
Dras, 1997; Dras, 1999; Kozlowski et al., 2003).
After the rise of statistical machine translation, a
number of its techniques were repurposed for para-
phrasing. These include sentence alignment (Gale
and Church, 1993; Barzilay and Elhadad, 2003),
word alignment and noisy channel decoding (Brown
et al., 1990; Quirk et al., 2004), phrase-based models
(Koehn et al., 2003; Bannard and Callison-Burch,
2005), hierarchical phrase-based models (Chiang,
2005; Madnani et al., 2007), log-linear models and
minimum error rate training (Och, 2003a; Madnani
et al., 2007; Zhao et al., 2008a), and here syntax-
based machine translation (Wu, 1997; Yamada and
Knight, 2001; Melamed, 2004; Quirk et al., 2005).
Beyond cementing the ties between paraphrasing
and syntax-based statistical machine translation, the
novel contributions of our paper are (1) an in-depth
analysis of the types of structural and sentential
paraphrases that can be extracted with bilingual piv-
oting, (2) a discussion of how our English–English
paraphrase grammar should be adapted to specific
text-to-text generation tasks (Zhao et al., 2009) with
(3) a concrete example of the adaptation procedure
for the task of paraphrase-based sentence compres-
sion (Knight and Marcu, 2002; Cohn and Lapata,
2008; Cohn and Lapata, 2009).
</bodyText>
<sectionHeader confidence="0.977948" genericHeader="method">
3 SCFGs in Translation
</sectionHeader>
<bodyText confidence="0.999429666666667">
The model we use in our paraphrasing approach is
a syntactically informed synchronous context-free
grammar (SCFG). The SCFG formalism (Aho and
Ullman, 1972) was repopularized for statistical ma-
chine translation by Chiang (2005). Formally, a
probabilistic SCFG G is defined by specifying
</bodyText>
<equation confidence="0.919479">
G = hN, TS, TT, R, 5i,
</equation>
<bodyText confidence="0.998509">
where N is a set of nonterminal symbols, TS and TT
are the source and target language vocabularies, R
is a set of rules and 5 ∈ N is the root symbol. The
rules in R take the form:
</bodyText>
<equation confidence="0.725541">
C → hγ, α,∼,wi,
</equation>
<page confidence="0.882096">
1170
</page>
<bodyText confidence="0.99982808">
leak is consistent with our f). The nonterminal sym-
bol that is the left-hand side of the SCFG rule is then
determined by the syntactic constituent that domi-
nates e (in this case NP). To introduce nonterminals
into the right-hand side of the rule, we can apply
rules extracted over sub-phrases of f, synchronously
substituting the corresponding nonterminal symbol
for the sub-phrases on both sides. The synchronous
substitution applied to f and e then yields the corre-
spondency —.
One significant differentiating factor between the
competing ways of extracting SCFG rules is whether
the extraction method generates rules only for con-
stituent phrases that are dominated by a node in
the parse tree (Galley et al., 2004; Cohn and
Lapata, 2008) or whether they include arbitrary
phrases, including non-constituent phrases (Zoll-
mann and Venugopal, 2006; Callison-Burch, 2008).
We adopt the extraction for all phrases, including
non-constituents, since it allows us to cover a much
greater set of phrases, both in translation and para-
phrasing.
Feature Functions Rather than assigning a single
weight w, we define a set of feature functions ϕ~ =
{ϕ1...ϕNI that are combined in a log-linear model:
</bodyText>
<equation confidence="0.9728625">
N
w = — λi log ϕi. (4)
i=1
~
</equation>
<bodyText confidence="0.999074666666667">
The weights λ of these feature functions are set to
maximize some objective function like BLEU (Pap-
ineni et al., 2002) using a procedure called minimum
error rate training (MERT), owing to Och (2003a).
MERT iteratively adjusts the weights until the de-
coder produces output that best matches reference
translations in a development set, according to the
objective function. We will examine appropriate ob-
jective functions for text-to-text generation tasks in
Section 6.2.
Typical features used in statistical machine trans-
lation include phrase translation probabilities (cal-
culated using maximum likelihood estimation over
all phrase pairs enumerable in the parallel cor-
pus), word-for-word lexical translation probabili-
ties (which help to smooth sparser phrase transla-
tion estimates), a “rule application penalty” (which
governs whether the system prefers fewer longer
</bodyText>
<figure confidence="0.3984536">
NP/NN - dem rest des  |the rest of the
VB+JJ - gefŠhrlich werden  |be dangerous
NP - NP/NN dorfes  |NP/NN village
VP/PP - nicht VB+JJ kšnnen  |can not VB+JJ
S - sie NP VP/PP  |they VP/PP to NP
</figure>
<figureCaption confidence="0.996259">
Figure 2: An example derivation produced by a syntactic
machine translation system. Although the synchronous
trees are unlike the derivations found in the Penn Tree-
bank, their yield is a good translation of the German.
</figureCaption>
<bodyText confidence="0.999486076923077">
phrases or a greater number of shorter phrases), and
a language model probability.
Decoding Given an SCFG and an input source
sentence, the decoder performs a search for the sin-
gle most probable derivation via the CKY algorithm.
In principle the best translation should be the En-
glish sentence e that is the most probable after sum-
ming over all d E D derivations, since many deriva-
tions yield the same e. In practice, we use a Viterbi
approximation and return the translation that is the
yield of the single best derivation:
Derivations are simply successive applications of the
SCFG rules such as those given in Figure 2.
</bodyText>
<sectionHeader confidence="0.963814" genericHeader="method">
4 SCFGs in Paraphrasing
</sectionHeader>
<bodyText confidence="0.999066714285714">
Rule Extraction To create a paraphrase grammar
from a translation grammar, we extend the syntac-
tically informed pivot approach of Callison-Burch
(2008) to the SCFG model. For this purpose, we
assume a grammar that translates from a given for-
eign language to English. For each pair of trans-
lation rules where the left-hand side C and foreign
</bodyText>
<figure confidence="0.924809791666667">
sie
they can not be dangerous to
dem rest des dorfes nicht
NP/NN
NP
S
VP/PP
VB+JJ
S
gefŠhrlich werden
the rest of the village
VP/PP
VB+JJ
NP/NN
NP
kšnnen
�eˆ = arg max p(d, e|f)
e∈Trans(f)
d∈D(e,f)
� yield(arg
d
D( f)p(d,
∈
|f)). (5)
</figure>
<page confidence="0.911833">
1171
</page>
<bodyText confidence="0.895656">
we create a paraphrase rule:
</bodyText>
<equation confidence="0.814653">
C → hα1, α2, ∼, ~ϕi,
</equation>
<bodyText confidence="0.996880333333333">
where the nonterminal correspondency relation ∼
has been set to reflect the combined nonterminal
alignment:
</bodyText>
<equation confidence="0.989889">
−1
1 ◦ ∼2 .
</equation>
<bodyText confidence="0.999404583333333">
Feature Functions In the computation of the fea-
tures ϕ~ from ~ϕ1 and ~ϕ2 we follow the approximation
in Equation 3, which yields lexical and phrasal para-
phrase probability features. Additionally, we add a
boolean indicator for whether the rule is an iden-
tity paraphrase, δidentity. Another indicator feature,
δreorder, fires if the rule swaps the order of two non-
terminals, which enables us to promote more com-
plex paraphrases that require structural reordering.
Decoding With this, paraphrasing becomes an
English-to-English translation problem which can
be formulated similarly to Equation 5 as:
</bodyText>
<equation confidence="0.9699295">
ˆe2 ≈ yield(arg max p(d, e2|e1)).
d∈D(e2,e1)
</equation>
<bodyText confidence="0.99987">
Figure 3 shows an example derivation produced as a
result of applying our paraphrase rules in the decod-
ing process. Another advantage of using the decoder
from statistical machine translation is that n-gram
language models, which have been shown to be use-
ful in natural language generation (Langkilde and
Knight, 1998), are already well integrated (Huang
and Chiang, 2007).
</bodyText>
<sectionHeader confidence="0.974763" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.9994614">
A key motivation for the use of syntactic paraphrases
over their phrasal counterparts is their potential to
capture meaning-preserving linguistic transforma-
tions in a more general fashion. A phrasal system is
limited to memorizing fully lexicalized transforma-
tions in its paraphrase table, resulting in poor gener-
alization capabilities. By contrast, a syntactic para-
phrasing system intuitively should be able to address
this issue and learn well-formed and generic patterns
that can be easily applied to unseen data.
</bodyText>
<equation confidence="0.9553045">
Lexical paraphrase: JJ -&gt; beleidigend  |offensive
JJ - offensive  |insulting JJ -&gt; beleidigend  |insulting
Reduced relative clause: NP -&gt; NP die VP  |NP VP
NP - NP that VP  |NP VP NP -&gt; NP die VP  |NP that VP
Pred. adjective copula deletion: VP - sind JJ fŸr NP  |are JJ to NP
VP - are JJ to NP  |JJ NP VP - sind JJ fŸr NP  |JJ NP
Partitive construction: NP -&gt; CD der NNS  |CD of the NNS
NP - CD of the NNS  |CD NNS NP -&gt; CD der NNS  |CD NNS
</equation>
<figureCaption confidence="0.9531725">
Figure 3: An example of a synchronous paraphrastic
derivation. A few of the rules applied in the parse are
show in the left column, with the pivot phrases that gave
rise to them on the right.
</figureCaption>
<bodyText confidence="0.999947208333333">
To put this expectation to the test, we investigate
how our grammar captures a number of well-known
paraphrastic transformations.1 Table 1 shows the
transformations along with examples of the generic
grammar rules our system learns to represent them.
When given a transformation to extract a syntactic
paraphrase for, we want to find rules that neither
under- nor over-generalize. This means that, while
replacing the maximum number of syntactic argu-
ments with nonterminals, the rules ideally will both
retain enough lexicalization to serve as sufficient ev-
idence for the applicability of the transformation and
impose constraints on the nonterminals to ensure the
arguments’ well-formedness.
The paraphrases implementing the possessive rule
and the dative shift shown in Table 1 are a good
examples of this: the two noun-phrase arguments
to the expressions are abstracted to nonterminals
while each rule’s lexicalization provides an appro-
priate frame of evidence for the transform. This is
important for a good representation of dative shift,
which is a reordering transformation that fully ap-
plies to certain ditransitive verbs while other verbs
are uncommon in one of the forms:
</bodyText>
<footnote confidence="0.95805">
1The data and software used to extract the grammar we draw
these examples from is described in Section 6.5.
</footnote>
<figure confidence="0.609142090909091">
twelve cartoons insulting the prophet mohammad
the prophet mohammad
cartoons that are offensive to
CD NNS JJ DT NNP
NP
DT+NNP
NP
VP
NP
12
of the
Paraphrase Rule Foreign Pivot Phrase
CD NNS JJ DT NNP
NP
NP
VP
DT+NNP
NP
∼ = ∼
string γ match:
C → hγ, α1, ∼1, ~ϕ1i
C → hγ, α2, ∼2, ~ϕ2i,
</figure>
<page confidence="0.629433">
1172
</page>
<table confidence="0.999611733333334">
Possessive rule NP → the NN of the NNP  |the NNP’s NN
NP → the NNS, made by NNS2  |the NNS2’s NNS,
Dative shift VP → give NN to NP  |give NP the NN
VP → provide NP, to NP2  |give NP2 NP,
Adv./adj. phrase move S/VP → ADVP they VBP  |they VPB ADVP
S → it is ADJP VP  |VP is ADJP
Verb particle shift VP → VB NP up  |VB up NP
Reduced relative clause SBAR/S → although PRP VBP that  |although PRP VBP
ADJP → very JJ that S  |JJ S
Partitive constructions NP → CD of the NN  |CD NN
NP → all DT\NP  |all of the DT\NP
Topicalization S → NP, VP .  |VP, NP .
Passivization SBAR → that NP had VBN  |which was VBN by NP
Light verbs VP → take action ADVP  |to act ADVP
VP → TO take a decision PP  |TO decide PP
</table>
<tableCaption confidence="0.9889925">
Table 1: A selection of meaning-preserving transformations and hand-picked examples of syntactic paraphrases that
our system extracts capturing these.
</tableCaption>
<bodyText confidence="0.994010142857143">
give decontamination equipment to Japan
give Japan decontamination equipment
provide decontamination equipment to Japan
? provide Japan decontamination equipment
Note how our system extracts a dative shift rule for
to give and a rule that both shifts and substitutes a
more appropriate verb for to provide.
The use of syntactic nonterminals in our para-
phrase rules to capture complex transforms also
makes it possible to impose constraints on their ap-
plication. For comparison, as Madnani et al. (2007)
do not impose any constraints on how the nontermi-
nal X can be realized, their equivalent of the topi-
calization rule would massively overgeneralize:
</bodyText>
<equation confidence="0.929235">
S → X1, X2 .  |X2, X1 .
</equation>
<bodyText confidence="0.999194516129032">
Additional examples of transforms our use of syn-
tax allows us to capture are the adverbial phrase
shift and the reduction of a relative clause, as well
as other phenomena listed in Table 1.
Unsurprisingly, syntactic information alone is not
sufficient to capture all transformations. For in-
stance it is hard to extract generic paraphrases for all
instances of passivization, since our syntactic model
currently has no means of representing the morpho-
logical changes that the verb undergoes:
the reactor leaks radiation
radiation is leaking from the reactor .
Still, for cases where the verb’s morphology does
not change, we manage to learn a rule:
the radiation that the reactor had leaked
the radiation which leaked from the reactor.
Another example of a deficiency in our synchronous
grammar models are light verb constructs such as:
to take a walk
to walk.
Here, a noun is transformed into the corresponding
verb – something our synchronous syntactic CFGs
are not able to capture except through memorization.
Our survey shows that we are able to extract ap-
propriately generic representations for a wide range
of paraphrastic transformations. This is a surpris-
ing result which shows that bilingual parallel cor-
pora can be used to learn sentential paraphrases, and
that they are a viable alternative to other data sources
like monolingual parallel corpora, which more obvi-
ously contain sentential paraphrases, but are scarce.
</bodyText>
<sectionHeader confidence="0.998577" genericHeader="method">
6 Text-to-Text Applications
</sectionHeader>
<bodyText confidence="0.9999688">
The core of many text-to-text generation tasks is
sentential paraphrasing, augmented with specific
constraints or goals. Since our model borrows much
of its machinery from statistical machine translation
– a sentential rewriting problem itself – it is straight-
forward to use our paraphrase grammars to generate
new sentences using SMT’s decoding and param-
eter optimization techniques. Our framework can
be adapted to many different text-to-text generation
tasks. These could include text simplification, sen-
</bodyText>
<page confidence="0.958209">
1173
</page>
<bodyText confidence="0.998643333333333">
tence compression, poetry generation, query expan-
sion, transforming declarative sentences into ques-
tions, and deriving hypotheses for textual entail-
ment. Each individual text-to-text application re-
quires that our framework be adapted in several
ways, by specifying:
</bodyText>
<listItem confidence="0.984268769230769">
• A mechanism for extracting synchronous
grammar rules (in this paper we argue that
pivot-based paraphrasing is widely applicable).
• An appropriate set of rule-level features that
capture information pertinent to the task (e.g.
whether a rule simplifies a phrase).
• An appropriate “objective function” that scores
the output of the model, i.e. a task-specific
equivalent to the BLEU metric in SMT.
• A development set with examples of the sen-
tential transformations that we are modeling.
• Optionally, a way of injecting task-specific
rules that were not extracted automatically.
</listItem>
<bodyText confidence="0.999941272727273">
In the remainder of this section, we illustrate how
our bilingually extracted paraphrases can be adapted
to perform sentence compression, which is the task
of reducing the length of sentence while preserving
its core meaning. Most previous approaches to sen-
tence compression focused only on the deletion of
a subset of words from the sentence (Knight and
Marcu, 2002). Our approach follows Cohn and La-
pata (2008), who expand the task to include substi-
tutions, insertions and reorderings that are automat-
ically learned from parallel texts.
</bodyText>
<subsectionHeader confidence="0.99582">
6.1 Feature Design
</subsectionHeader>
<bodyText confidence="0.999984666666667">
In Section 4 we discussed phrasal probabilities.
While these help quantify how good a paraphrase
is in general, they do not make any statement on
task-specific things such as the change in language
complexity or text length. To make this information
available to the decoder, we enhance our paraphrases
with four compression-targeted features. We add the
count features csrc and ctgt, indicating the number of
words on either side of the rule as well as two differ-
ence features: cdcount = ctgt − csrc and the anal-
ogously computed difference in the average word
length in characters, cdavg.
</bodyText>
<subsectionHeader confidence="0.998766">
6.2 Objective Function
</subsectionHeader>
<bodyText confidence="0.992451">
Given our paraphrasing system’s connection to
SMT, the naive/obvious choice for parameter op-
timization would be to optimize for BLEU over a
set of paraphrases, for instance parallel English ref-
erence translations for a machine translation task
(Madnani et al., 2007). For a candidate C and a ref-
erence R, (with lengths c and r) BLEU is defined as:
</bodyText>
<equation confidence="0.94661625">
BLEUN(C, R)
J �N
e(1−c/r) · e n=1 log wnpn if c/r G 1
=
</equation>
<bodyText confidence="0.999975153846154">
where pn is the modified n-gram precision of C
against R, with typically N = 4 and wn = 1 N . The
“brevity penalty” term e(1−c/r) is added to prevent
short candidates from achieving perfect scores.
Naively optimizing for BLEU, however, will re-
sult in a trivial paraphrasing system heavily biased
towards producing identity “paraphrases”. This is
obviously not what we are looking for. Moreover,
BLEU does not provide a mechanism for directly
specifying a per-sentence compression rate, which
is desirable for the compression task.
Instead, we propose PR´ECIS, an objective func-
tion tailored to the text compression task:
</bodyText>
<equation confidence="0.9884295">
PR´ECISλ,ϕ(I, C, R)
r eλ(ϕ−c/i) · BLEU (C, R) if c/i ≥ cp
</equation>
<bodyText confidence="0.987285625">
Sl BLEU(C, R) otherwise
For an input sentence I, an output C and ref-
erence compression R (with lengths i, c and r),
PR´ECIS combines the precision estimate of BLEU
with an additional “verbosity penalty” that is ap-
plied to compressions that fail to meet a given target
compression rate cp. We rely on the BLEU brevity
penalty to prevent the system from producing overly
aggressive compressions. The scaling term A deter-
mines how severely we penalize deviations from cp.
In our experiments we use A = 10.
It is straightforward to find similar adaptations for
other tasks. For text simplification, for instance, the
penalty term can include a readability metric. For
poetry generation we can analogously penalize out-
puts that break the meter (Greene et al., 2010).
</bodyText>
<subsectionHeader confidence="0.998248">
6.3 Development Data
</subsectionHeader>
<bodyText confidence="0.997767">
To tune the parameters of our paraphrase system for
sentence compression, we need an appropriate cor-
</bodyText>
<equation confidence="0.7381615">
�N otherwise ,
e n=1 log wnpn
</equation>
<page confidence="0.977334">
1174
</page>
<bodyText confidence="0.999986294117647">
pus of reference compressions. Since our model is
designed to compress by paraphrasing rather than
deletion, the commonly used deletion-based com-
pression data sets like the Ziff-Davis corpus are not
suitable. We have thus created a corpus of com-
pression paraphrases. Beginning with 9570 tuples
of parallel English–English sentences obtained from
multiple reference translations for machine transla-
tion evaluation, we construct a parallel compression
corpus by selecting the longest reference in each tu-
ple as the source sentence and the shortest reference
as the target sentence. We further retain only those
sentence pairs where the compression rate cr falls in
the range 0.5 &lt; cr &lt; 0.8. From these, we randomly
select 936 sentences for the development set, as well
as 560 sentences for a test set that we use to gauge
the performance of our system.
</bodyText>
<subsectionHeader confidence="0.958363">
6.4 Grammar Augmentations
</subsectionHeader>
<bodyText confidence="0.999952692307692">
As we discussed in Section 5, the paraphrase gram-
mar we induce is capable of representing a wide va-
riety of transformations. However, the formalism
and extraction method are not explicitly geared to-
wards a compression application. For instance, the
synchronous nature of our grammar does not allow
us to perform deletions of constituents as done by
Cohn and Lapata (2007)’s tree transducers. One way
to extend the grammar’s capabilities towards the re-
quirements of a given task is by injecting additional
rules designed to capture appropriate operations.
For the compression task, this could include
adding rules to delete target-side nonterminals:
</bodyText>
<equation confidence="0.842417">
JJ -+ JJ  |e
</equation>
<bodyText confidence="0.99977225">
This would render the grammar asynchronous and
require adjustments to the decoding process. Al-
ternatively, we can generate rules that specifically
delete particular adjectives from the corpus:
</bodyText>
<equation confidence="0.521117">
JJ -+ superfluous  |e .
</equation>
<bodyText confidence="0.999838666666667">
In our experiments we evaluate the latter approach
by generating optional deletion rules for all adjec-
tives, adverbs and determiners.
</bodyText>
<subsectionHeader confidence="0.937193">
6.5 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99954325">
We extracted a paraphrase grammar from the
French–English Europarl corpus (v5). The bitext
was aligned using the Berkeley aligner and the En-
glish side was parsed with the Berkeley parser. We
</bodyText>
<table confidence="0.9975542">
Grammar # Rules
total 42,353,318
w/o identity 23,641,016
w/o complex constituents 6,439,923
w/o complex const. &amp; identity 5,097,250
</table>
<tableCaption confidence="0.901626">
Table 2: Number and distribution of rules in our para-
phrase grammar. Note the significant number of identity
paraphrases and rules with complex nonterminal labels.
</tableCaption>
<bodyText confidence="0.998525388888889">
obtained the initial translation grammar using the
SAMT toolkit (Venugopal and Zollmann, 2009).
The grammars we extract tend to be extremely
large. To keep their size manageable, we only con-
sider translation rules that have been seen more than
3 times and whose translation probability exceeds
10−4 for pivot recombination. Additionally, we only
retain the top 25 most likely paraphrases of each
phrase, ranked by a uniformly weighted combina-
tion of phrasal and lexical paraphrase probabilities.
We tuned the model parameters to our PR´ECIS
objective function, implemented in the Z-MERT
toolkit (Zaidan, 2009). For decoding we used the
Joshua decoder (Li et al., 2010). The language
model used in our paraphraser and the Clarke and
Lapata (2008) baseline system is a Kneser-Ney dis-
counted 5-gram model estimated on the Gigaword
corpus using the SRILM toolkit (Stolcke, 2002).
</bodyText>
<subsectionHeader confidence="0.979738">
6.6 Evaluation
</subsectionHeader>
<bodyText confidence="0.999888166666667">
To assess the output quality of the resulting sentence
compression system, we compare it to two state-of-
the-art sentence compression systems. Specifically,
we compare against our implementation of Clarke
and Lapata (2008)’s compression model which uses
a series of constraints in an integer linear program-
ming (ILP) solver, and Cohn and Lapata (2007)’s
tree transducer toolkit (T3) which learns a syn-
chronous tree substitution grammar (STSG) from
paired monolingual sentences. Unlike SCFGs, the
STSG formalism allows changes to the tree topol-
ogy. Cohn and Lapata argue that this is a natural
fit for sentence compression, since deletions intro-
duce structural mismatches. We trained the T3 soft-
ware2 on the 936 (full, compressed) sentence pairs
that comprise our development set. This is equiva-
lent in size to the training corpora that Cohn and La-
pata (2007) used (their training corpora ranged from
</bodyText>
<footnote confidence="0.978403">
2www.dcs.shef.ac.uk/people/T.Cohn/t3/
</footnote>
<page confidence="0.994078">
1175
</page>
<bodyText confidence="0.999937931818182">
882–1020 sentence pairs), and has the advantage of
being in-domain with respect to our test set. Both
these systems reported results outperforming previ-
ous systems such as McDonald (2006). To showcase
the value of the adaptations discussed above, we also
compare variants of our paraphrase-based compres-
sion systems: using Hiero instead of syntax, using
syntax with or without compression features, using
an augmented grammar with optional deletion rules.
We solicit human judgments of the compres-
sions along two five-point scales: grammaticality
and meaning. Judges are instructed to decide how
much the meaning from a reference translation is
retained in the compressed sentence, with a score
of 5 indicating that all of the important information
is present, and 1 being that the compression does
not retain any of the original meaning. Similarly, a
grammar score of 5 indicates perfect grammaticality,
and a grammar score of 1 is assigned to sentences
that are entirely ungrammatical. To ensure fairness,
we perform pairwise system comparisons with com-
pression rates strictly tied on the sentence-level. For
any comparison, a sentence is only included in the
computation of average scores if the difference be-
tween both systems’ compression rates is &lt; 0.05.3
Table 4 shows a set of pairwise comparisons for
compression rates ≈ 0.5. We see that going from
a Hiero-based to a syntactic paraphrase grammar
yields a significant improvement in grammatical-
ity. Adding compression-specific features improves
grammaticality even further. Further augmenting the
grammar with deletion rules significantly helps re-
tain the core meaning at compression rates this high,
however compared to the un-augmented syntactic
system grammaticality scores drop. While our ap-
proach significantly outperforms the T3 system, we
are not able to match ILP’s results in grammaticality.
In Table 3 we compare our system to the ILP ap-
proach at a modest compression rate of ≈ 0.8. Here,
we significantly outperform ILP in meaning reten-
tion while achieving comparable results in gram-
maticality. This improvement is significant at p &lt;
0.0001, using the sign test, while the better gram-
maticality score of the ILP system is not statisti-
</bodyText>
<footnote confidence="0.99793475">
3Because evaluation quality correlates linearly with com-
pression rate, the community-accepted practice of not compar-
ing based on a closely tied compression rate is potentially sub-
ject to erroneous interpretation (Napoles et al., 2011).
</footnote>
<table confidence="0.9978124">
CR Meaning Grammar
Reference 0.73 4.26 4.35
Syntax+Feat. 0.80 3.67 3.38
ILP 0.80 3.50 3.49
Random Deletions 0.50 1.94 1.57
</table>
<tableCaption confidence="0.89023425">
Table 3: Results of the human evaluation on longer com-
pressions: pairwise compression rates (CR), meaning and
grammaticality scores. Bold indicates a statistically sig-
nificance difference at p &lt; 0.05.
</tableCaption>
<table confidence="0.999912545454546">
CR Meaning Grammar
Hiero 0.56 2.57 2.35
Syntax 0.56 2.76 2.67
Syntax 0.53 2.70 2.49
Syntax+Feat. 0.53 2.71 2.54
Syntax+Feat. 0.54 2.79 2.71
Syntax+Aug. 0.54 2.96 2.52
Syntax+Aug. 0.52 2.87 2.40
ILP 0.52 2.83 3.09
Syntax+Aug. 0.50 2.41 2.20
T3 0.50 2.01 1.93
</table>
<tableCaption confidence="0.94130225">
Table 4: Human evaluation for shorter compressions and
for variations of our paraphrase system. +Feat. includes
the compression features from Section 6.1, +Aug. in-
cludes optional deletion rules from Section 6.4.
</tableCaption>
<bodyText confidence="0.999793636363636">
cally significant (p &lt; 0.088). These results indi-
cate that, over a variety of compression rates, our
framework for text-to-text generation is performing
as well as or better than specifically tailored state-
of-the-art methods.
Table 5 shows an example sentence drawn from
our test set and the compressions produced by the
different systems. We see that both the paraphrase
and ILP systems produce good quality results, with
the paraphrase system retaining the meaning of the
source sentence more accurately.
</bodyText>
<sectionHeader confidence="0.988112" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999786444444444">
In this work we introduced a method to learn syntac-
tically informed paraphrases from bilingual parallel
texts. We discussed the expressive power and limita-
tions of our formalism and outlined straightforward
adaptation strategies for applications in text-to-text
generation. We demonstrated when our paraphras-
ing system was adapted to do sentence compression,
it achieved results competitive with state-of-the-art
compression systems with only minimal effort.
</bodyText>
<page confidence="0.865829">
1176
</page>
<bodyText confidence="0.995723652173913">
Source he also expected that he would have a role in the future at the level of the islamic movement
across the palestinian territories , even if he was not lucky enough to win in the elections.
Reference he expects to have a future role in the islamic movement in the palestinian territories if he is
not successful in the elections .
Syntax+Feat. he also expected that he would have a role in the future of the islamic movement in the
palestinian territories , although he was not lucky enough to win elections .
ILP he also expected that he would have a role at the level of the islamic movement , even if he
was not lucky enough to win in the elections .
Source in this war which has carried on for the last 12 days , around 700 palestinians , which include
a large number of women and children, have died.
Reference about 700 palestinians , mostly women and children, have been killed in the israeli offensive
over the last 12 days .
Syntax+Feat. in this war has done for the last 12 days , around 700 palestinians , including women and
children, died.
ILP in this war which has carried for the days palestinians , which include a number of women
and children died.
Source hala speaks arabic most of the time with her son , taking into consideration that he can speak
english with others .
Reference hala speaks to her son mostly in arabic , as he can speak english to others.
Syntax+Feat. hala speaks arabic most of the time with her son, considering that he can speak english with
others .
ILP hala speaks arabic most of the time , taking into consideration that he can speak english with
others .
</bodyText>
<tableCaption confidence="0.988939">
Table 5: Example compressions produced by the two systems in Table 3 for three input sentences from our test data.
</tableCaption>
<sectionHeader confidence="0.997354" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999024">
We would like to thank Trevor Cohn for kindly pro-
viding us with the T3 compression system. This re-
search was supported by the NSF under grant IIS-
0713448. Opinions, interpretations, and conclusions
are the authors’ alone.
</bodyText>
<sectionHeader confidence="0.997803" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998150066666667">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Peter G. Anick and Suresh Tipirneni. 1999. The para-
phrase search assistant: terminological feedback for
iterative information seeking. In Proceedings of SI-
GIR.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings of
ACL.
Regina Barzilay. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Genera-
tion. Ph.D. thesis, Columbia University, New York.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Peter Brown, John Cocke, Stephen Della Pietra, Vincent
Della Pietra, Frederick Jelinek, Robert Mercer, and
</reference>
<page confidence="0.947153">
1177
</page>
<reference confidence="0.999209168224299">
Paul Poossin. 1990. A statistical approach to language
translation. Computational Linguistics, 16(2), June.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311, June.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273–381.
Trevor Cohn and Mirella Lapata. 2007. Large margin
synchronous generation and its application to sentence
compression. In Proceedings of EMNLP-CoLing.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research (JAIR), 34:637–674.
Peter W. Culicover. 1968. Paraphrase generation and
information retrieval from stored text. Mechani-
cal Translation and Computational Linguistics, 11(1-
2):78–88.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the COLING.
Mark Dras. 1997. Representing paraphrases using syn-
chronous tree adjoining grammars. In Proceedings of
ACL.
Mark Dras. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text. Ph.D. thesis, Macquarie
University, Australia.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL.
William Gale and Kenneth Church. 1993. A program
for aligning sentences in bilingual corpora. Compu-
atational Linguistics, 19(1):75–90.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010.
Automatic analysis of rhythmic poetry with applica-
tions to generation and translation. In Proceedings of
EMNLP.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
EMNLP.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91–107.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Raymond Kozlowski, Kathleen McCoy, and K. Vijay-
Shanker. 2003. Generation of single-sentence
paraphrases from predicate/argument structure using
lexico-grammatical resources. In Workshop On Para-
phrasing.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Workshop On
Natural Language Generation, Ontario, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proceedings of WMT09.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of WMT10.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343–360.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–388.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of WMT07.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
</reference>
<page confidence="0.865426">
1178
</page>
<reference confidence="0.999867768421053">
Kathleen R. McKeown. 1979. Paraphrasing using given
and new information in a question-answer system. In
Proceedings ofACL.
Dan Melamed. 2004. Statistical machine translation by
parsing. In Proceedings ofACL.
Marie W. Meteer and Varda Shaked. 1988. Strategies for
effective paraphrasing. In Proceedings of COLING.
Kazunori Muraki. 1982. On a semantic model for multi-
lingual paraphrasing. In Proceedings of COLING.
Courtney Napoles, Chris Callison-Burch, and Ben-
jamin Van Durme. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Work-
shop on Monolingual Text-To-Text Generation.
Franz Josef Och. 2003a. Minimum error rate training for
statistical machine translation. In Proceedings ofACL.
Franz Josef Och. 2003b. Minimum error rate training in
statistical machine translation. In Proceedings ofACL.
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual bitext-derived
paraphrases in automatic MT evaluation. In Proceed-
ings of WMT06.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings ofACL.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings ofACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
sufrace text patterns for a question answering system.
In Proceedings ofACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings ofACL.
Hadar Shemtov. 1996. Generation of paraphrases from
ambiguous logical forms. In Proceedings of COLING.
Stuart Shieber and Yves Schabes. 1990. Generation and
synchronous tree-adjoining grammars. In Workshop
On Natural Language Generation.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117–127.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. In Non-Transformational
Syntax: Formal and Explicit Models of Grammar.
Wiley-Blackwell.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceeding of the International
Conference on Spoken Language Processing.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP, Pro-
ceedings of EMNLP.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. Prague Bul-
letin of Mathematical Linguistics, 91.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings ofACL.
Kazuhide Yamamoto. 2002. Machine translation by in-
teraction between paraphraser and transfer. In Pro-
ceedings of COLING.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008a. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot approach for extracting paraphrase
patterns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings ofACL.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of WMT06.
</reference>
<page confidence="0.996939">
1179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.890638">
<title confidence="0.997">Learning Sentential Paraphrases from Bilingual Parallel for Text-to-Text Generation</title>
<author confidence="0.954572">Chris Callison-Burch Ganitkevitch</author>
<author confidence="0.954572">Courtney Napoles</author>
<author confidence="0.954572">Van</author>
<affiliation confidence="0.953744">Center for Language and Speech Processing, and Johns Hopkins University</affiliation>
<abstract confidence="0.99981545">Previous work has shown that high quality can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate refor extracting more sophisticated senwhich are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling.</booktitle>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="11389" citStr="Aho and Ullman, 1972" startWordPosition="1717" endWordPosition="1720">-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs in Translation The model we use in our paraphrasing approach is a syntactically informed synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by Chiang (2005). Formally, a probabilistic SCFG G is defined by specifying G = hN, TS, TT, R, 5i, where N is a set of nonterminal symbols, TS and TT are the source and target language vocabularies, R is a set of rules and 5 ∈ N is the root symbol. The rules in R take the form: C → hγ, α,∼,wi, 1170 leak is consistent with our f). The nonterminal symbol that is the left-hand side of the SCFG rule is then determined by the syntactic constituent that dominates e (in this case NP). To introduce nonterminals into the right-hand side of the rule</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter G Anick</author>
<author>Suresh Tipirneni</author>
</authors>
<title>The paraphrase search assistant: terminological feedback for iterative information seeking.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1581" citStr="Anick and Tipirneni, 1999" startWordPosition="217" endWordPosition="220"> adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are a</context>
</contexts>
<marker>Anick, Tipirneni, 1999</marker>
<rawString>Peter G. Anick and Suresh Tipirneni. 1999. The paraphrase search assistant: terminological feedback for iterative information seeking. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10329" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1558" endWordPosition="1561">a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English </context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="10169" citStr="Barzilay and Elhadad, 2003" startWordPosition="1534" endWordPosition="1537"> span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2805" citStr="Barzilay and McKeown, 2001" startWordPosition="407" endWordPosition="410"> annotated with syntactic constraints: the NP1’s NP2 the NP2 of the NP1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expression</context>
<context position="6758" citStr="Barzilay and McKeown, 2001" startWordPosition="990" endWordPosition="993"> paradriven paraphrasing techniques, categorizing them phrases (even non-constituent phrases) with CCGbased on the type of data that they use. These inspired slash categories (Steedman and Baldridge, include large monolingual texts (Lin and Pantel, 2011), an approach similar to Zollmann and Venu2001; Szpektor et al., 2004; Bhagat and Ravichan- gopal (2006)’s syntax-augmented machine transladran, 2008), comparable corpora (Barzilay and Lee, tion (SAMT). Callison-Burch did not formally de2003; Dolan et al., 2004), monolingual parallel cor- fine a synchronous grammar, nor discuss decoding, pora (Barzilay and McKeown, 2001; Pang et al., since his presentation did not include hierarchical 2003), and bilingual parallel corpora (Bannard and rules. Cohn and Lapata (2008) used the GHKM Callison-Burch, 2005; Madnani et al., 2007; Zhao et extraction method (Galley et al., 2004), which is al., 2008b). We focus on the latter type of data. limited to constituent phrases and thus produces Paraphrase extraction using bilingual parallel cor- a reasonably small set of syntactic rules. Zhao pora was proposed by Bannard and Callison-Burch et al. (2008b) added slots to bilingually extracted (2005) who induced paraphrases using </context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1411" citStr="Barzilay et al., 1999" startWordPosition="192" endWordPosition="195">scuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: th</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>Regina Barzilay, Kathleen R. McKeown, and Michael Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
</authors>
<title>Information Fusion for Mutlidocument Summarization: Paraphrasing and Generation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University,</institution>
<location>New York.</location>
<contexts>
<context position="1428" citStr="Barzilay, 2003" startWordPosition="196" endWordPosition="197"> be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s sec</context>
</contexts>
<marker>Barzilay, 2003</marker>
<rawString>Regina Barzilay. 2003. Information Fusion for Mutlidocument Summarization: Paraphrasing and Generation. Ph.D. thesis, Columbia University, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Large scale acquisition of paraphrases for learning surface patterns.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="3142" citStr="Bhagat and Ravichandran, 2008" startWordPosition="458" endWordPosition="462">ollections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expressions or antonyms (such as {boy, girl} or {rise, fall}) for paraphrases. 1168 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora phrase table, Engl</context>
</contexts>
<marker>Bhagat, Ravichandran, 2008</marker>
<rawString>Rahul Bhagat and Deepak Ravichandran. 2008. Large scale acquisition of paraphrases for learning surface patterns. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>John Cocke</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Frederick Jelinek</author>
<author>Robert Mercer</author>
<author>Paul Poossin</author>
</authors>
<title>A statistical approach to language translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="10232" citStr="Brown et al., 1990" startWordPosition="1544" endWordPosition="1547">target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraph</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, Poossin, 1990</marker>
<rawString>Peter Brown, John Cocke, Stephen Della Pietra, Vincent Della Pietra, Frederick Jelinek, Robert Mercer, and Paul Poossin. 1990. A statistical approach to language translation. Computational Linguistics, 16(2), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9068" citStr="Brown et al. (1993)" startWordPosition="1361" endWordPosition="1364">symbols with an equal number of nonterminals cNT (γ) = cNT (α) and ∼: {1 ... cNT(γ)} → {1 ... cNT(α)} constitutes a one-to-one correspondency function between the nonterminals in γ and α. A nonnegative weight w ≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word align</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4494" citStr="Callison-Burch, 2008" startWordPosition="656" endWordPosition="657">aining paraphrases via a pivoting step over foreign many paraphrases can be extracted for a phrase, language phrases (Bannard and Callison-Burch, Bannard and Callison-Burch rank them using a para2005). The coverage of paraphrase lexica extracted phrase probability defined in terms of the translation from bitexts has been shown to outperform that model probabilities p(f|e) and p(e|f): obtained from other sources (Zhao et al., 2008a). �p(e2|e1) = p(e2,f|e1) (1) While there have been efforts pursuing the extrac- f tion of more powerful paraphrases (Madnani et �= p(e2|f, e1)p(f|e1) (2) al., 2007; Callison-Burch, 2008; Cohn and Lapata, f 2008; Zhao et al., 2008b), it is not yet clear to what E≈ p(e2|f)p(f|e1). (3) extent sentential paraphrases can be induced from f bitexts. In this paper we: Several subsequent efforts extended the bilin• Extend the bilingual pivoting approach to para- gual pivoting technique, many of which introduced phrase induction to produce rich syntactic para- elements of more contemporary syntax-based apphrases. proaches to statistical machine translation. Mad• Perform a thorough analysis of the types of nani et al. (2007) extended the technique to hierparaphrases we obtain and discu</context>
<context position="6021" citStr="Callison-Burch (2008)" startWordPosition="884" endWordPosition="885">r can tic/sentential paraphrase models should be tai- paraphrase (or “decode”) input sentences using an lored to different text-to-text generation tasks. SCFG decoder, like the Hiero, Joshua or cdec MT • Demonstrate our framework’s suitability for a systems (Chiang, 2007; Li et al., 2009; Dyer et al., variety of text-to-text generation tasks by ob- 2010). Like Hiero, Madnani’s model uses just one taining state-of-the-art results on the example nonterminal X instead of linguistic nonterminals. task of sentence compression. Three additional efforts incorporated linguistic 2 Related Work syntax. Callison-Burch (2008) introduced syntacMadnani and Dorr (2010) survey a variety of data- tic constraints by labeling all phrases and paradriven paraphrasing techniques, categorizing them phrases (even non-constituent phrases) with CCGbased on the type of data that they use. These inspired slash categories (Steedman and Baldridge, include large monolingual texts (Lin and Pantel, 2011), an approach similar to Zollmann and Venu2001; Szpektor et al., 2004; Bhagat and Ravichan- gopal (2006)’s syntax-augmented machine transladran, 2008), comparable corpora (Barzilay and Lee, tion (SAMT). Callison-Burch did not formally </context>
<context position="12614" citStr="Callison-Burch, 2008" startWordPosition="1926" endWordPosition="1927"> can apply rules extracted over sub-phrases of f, synchronously substituting the corresponding nonterminal symbol for the sub-phrases on both sides. The synchronous substitution applied to f and e then yields the correspondency —. One significant differentiating factor between the competing ways of extracting SCFG rules is whether the extraction method generates rules only for constituent phrases that are dominated by a node in the parse tree (Galley et al., 2004; Cohn and Lapata, 2008) or whether they include arbitrary phrases, including non-constituent phrases (Zollmann and Venugopal, 2006; Callison-Burch, 2008). We adopt the extraction for all phrases, including non-constituents, since it allows us to cover a much greater set of phrases, both in translation and paraphrasing. Feature Functions Rather than assigning a single weight w, we define a set of feature functions ϕ~ = {ϕ1...ϕNI that are combined in a log-linear model: N w = — λi log ϕi. (4) i=1 ~ The weights λ of these feature functions are set to maximize some objective function like BLEU (Papineni et al., 2002) using a procedure called minimum error rate training (MERT), owing to Och (2003a). MERT iteratively adjusts the weights until the de</context>
<context position="15036" citStr="Callison-Burch (2008)" startWordPosition="2323" endWordPosition="2324">gle most probable derivation via the CKY algorithm. In principle the best translation should be the English sentence e that is the most probable after summing over all d E D derivations, since many derivations yield the same e. In practice, we use a Viterbi approximation and return the translation that is the yield of the single best derivation: Derivations are simply successive applications of the SCFG rules such as those given in Figure 2. 4 SCFGs in Paraphrasing Rule Extraction To create a paraphrase grammar from a translation grammar, we extend the syntactically informed pivot approach of Callison-Burch (2008) to the SCFG model. For this purpose, we assume a grammar that translates from a given foreign language to English. For each pair of translation rules where the left-hand side C and foreign sie they can not be dangerous to dem rest des dorfes nicht NP/NN NP S VP/PP VB+JJ S gefŠhrlich werden the rest of the village VP/PP VB+JJ NP/NN NP kšnnen �eˆ = arg max p(d, e|f) e∈Trans(f) d∈D(e,f) � yield(arg d D( f)p(d, ∈ |f)). (5) 1171 we create a paraphrase rule: C → hα1, α2, ∼, ~ϕi, where the nonterminal correspondency relation ∼ has been set to reflect the combined nonterminal alignment: −1 1 ◦ ∼2 . F</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9226" citStr="Chiang, 2005" startWordPosition="1387" endWordPosition="1388">nterminals in γ and α. A nonnegative weight w ≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work</context>
<context position="11460" citStr="Chiang (2005)" startWordPosition="1729" endWordPosition="1730">be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs in Translation The model we use in our paraphrasing approach is a syntactically informed synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by Chiang (2005). Formally, a probabilistic SCFG G is defined by specifying G = hN, TS, TT, R, 5i, where N is a set of nonterminal symbols, TS and TT are the source and target language vocabularies, R is a set of rules and 5 ∈ N is the root symbol. The rules in R take the form: C → hγ, α,∼,wi, 1170 leak is consistent with our f). The nonterminal symbol that is the left-hand side of the SCFG rule is then determined by the syntactic constituent that dominates e (in this case NP). To introduce nonterminals into the right-hand side of the rule, we can apply rules extracted over sub-phrases of f, synchronously sub</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5671" citStr="Chiang, 2007" startWordPosition="835" endWordPosition="836">hierparaphrases we obtain and discuss the para- archical phrase-based machine translation (Chiang, phrastic transformations we are capable of cap- 2005), which is formally a synchronous context-free turing. grammar (SCFG) and thus can be thought of as a • Describe how training paradigms for syntac- paraphrase grammar. The paraphrase grammar can tic/sentential paraphrase models should be tai- paraphrase (or “decode”) input sentences using an lored to different text-to-text generation tasks. SCFG decoder, like the Hiero, Joshua or cdec MT • Demonstrate our framework’s suitability for a systems (Chiang, 2007; Li et al., 2009; Dyer et al., variety of text-to-text generation tasks by ob- 2010). Like Hiero, Madnani’s model uses just one taining state-of-the-art results on the example nonterminal X instead of linguistic nonterminals. task of sentence compression. Three additional efforts incorporated linguistic 2 Related Work syntax. Callison-Burch (2008) introduced syntacMadnani and Dorr (2010) survey a variety of data- tic constraints by labeling all phrases and paradriven paraphrasing techniques, categorizing them phrases (even non-constituent phrases) with CCGbased on the type of data that they u</context>
<context position="16650" citStr="Chiang, 2007" startWordPosition="2594" endWordPosition="2595">more complex paraphrases that require structural reordering. Decoding With this, paraphrasing becomes an English-to-English translation problem which can be formulated similarly to Equation 5 as: ˆe2 ≈ yield(arg max p(d, e2|e1)). d∈D(e2,e1) Figure 3 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. Another advantage of using the decoder from statistical machine translation is that n-gram language models, which have been shown to be useful in natural language generation (Langkilde and Knight, 1998), are already well integrated (Huang and Chiang, 2007). 5 Analysis A key motivation for the use of syntactic paraphrases over their phrasal counterparts is their potential to capture meaning-preserving linguistic transformations in a more general fashion. A phrasal system is limited to memorizing fully lexicalized transformations in its paraphrase table, resulting in poor generalization capabilities. By contrast, a syntactic paraphrasing system intuitively should be able to address this issue and learn well-formed and generic patterns that can be easily applied to unseen data. Lexical paraphrase: JJ -&gt; beleidigend |offensive JJ - offensive |insul</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>31--273</pages>
<contexts>
<context position="30017" citStr="Clarke and Lapata (2008)" startWordPosition="4800" endWordPosition="4803">end to be extremely large. To keep their size manageable, we only consider translation rules that have been seen more than 3 times and whose translation probability exceeds 10−4 for pivot recombination. Additionally, we only retain the top 25 most likely paraphrases of each phrase, ranked by a uniformly weighted combination of phrasal and lexical paraphrase probabilities. We tuned the model parameters to our PR´ECIS objective function, implemented in the Z-MERT toolkit (Zaidan, 2009). For decoding we used the Joshua decoder (Li et al., 2010). The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a Kneser-Ney discounted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002). 6.6 Evaluation To assess the output quality of the resulting sentence compression system, we compare it to two state-ofthe-art sentence compression systems. Specifically, we compare against our implementation of Clarke and Lapata (2008)’s compression model which uses a series of constraints in an integer linear programming (ILP) solver, and Cohn and Lapata (2007)’s tree transducer toolkit (T3) which learns a synchronous tree substitution grammar (STSG) from paired </context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal of Artificial Intelligence Research, 31:273–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Large margin synchronous generation and its application to sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoLing.</booktitle>
<contexts>
<context position="28122" citStr="Cohn and Lapata (2007)" startWordPosition="4510" endWordPosition="4513">ere the compression rate cr falls in the range 0.5 &lt; cr &lt; 0.8. From these, we randomly select 936 sentences for the development set, as well as 560 sentences for a test set that we use to gauge the performance of our system. 6.4 Grammar Augmentations As we discussed in Section 5, the paraphrase grammar we induce is capable of representing a wide variety of transformations. However, the formalism and extraction method are not explicitly geared towards a compression application. For instance, the synchronous nature of our grammar does not allow us to perform deletions of constituents as done by Cohn and Lapata (2007)’s tree transducers. One way to extend the grammar’s capabilities towards the requirements of a given task is by injecting additional rules designed to capture appropriate operations. For the compression task, this could include adding rules to delete target-side nonterminals: JJ -+ JJ |e This would render the grammar asynchronous and require adjustments to the decoding process. Alternatively, we can generate rules that specifically delete particular adjectives from the corpus: JJ -+ superfluous |e . In our experiments we evaluate the latter approach by generating optional deletion rules for a</context>
<context position="30513" citStr="Cohn and Lapata (2007)" startWordPosition="4875" endWordPosition="4878">ding we used the Joshua decoder (Li et al., 2010). The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a Kneser-Ney discounted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002). 6.6 Evaluation To assess the output quality of the resulting sentence compression system, we compare it to two state-ofthe-art sentence compression systems. Specifically, we compare against our implementation of Clarke and Lapata (2008)’s compression model which uses a series of constraints in an integer linear programming (ILP) solver, and Cohn and Lapata (2007)’s tree transducer toolkit (T3) which learns a synchronous tree substitution grammar (STSG) from paired monolingual sentences. Unlike SCFGs, the STSG formalism allows changes to the tree topology. Cohn and Lapata argue that this is a natural fit for sentence compression, since deletions introduce structural mismatches. We trained the T3 software2 on the 936 (full, compressed) sentence pairs that comprise our development set. This is equivalent in size to the training corpora that Cohn and Lapata (2007) used (their training corpora ranged from 2www.dcs.shef.ac.uk/people/T.Cohn/t3/ 1175 882–1020</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Large margin synchronous generation and its application to sentence compression. In Proceedings of EMNLP-CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<contexts>
<context position="6905" citStr="Cohn and Lapata (2008)" startWordPosition="1012" endWordPosition="1015">spired slash categories (Steedman and Baldridge, include large monolingual texts (Lin and Pantel, 2011), an approach similar to Zollmann and Venu2001; Szpektor et al., 2004; Bhagat and Ravichan- gopal (2006)’s syntax-augmented machine transladran, 2008), comparable corpora (Barzilay and Lee, tion (SAMT). Callison-Burch did not formally de2003; Dolan et al., 2004), monolingual parallel cor- fine a synchronous grammar, nor discuss decoding, pora (Barzilay and McKeown, 2001; Pang et al., since his presentation did not include hierarchical 2003), and bilingual parallel corpora (Bannard and rules. Cohn and Lapata (2008) used the GHKM Callison-Burch, 2005; Madnani et al., 2007; Zhao et extraction method (Galley et al., 2004), which is al., 2008b). We focus on the latter type of data. limited to constituent phrases and thus produces Paraphrase extraction using bilingual parallel cor- a reasonably small set of syntactic rules. Zhao pora was proposed by Bannard and Callison-Burch et al. (2008b) added slots to bilingually extracted (2005) who induced paraphrases using techniques paraphrase patterns that were labeled with part-offrom phrase-based statistical machine translation speech tags, but not larger syntacti</context>
<context position="11184" citStr="Cohn and Lapata, 2008" startWordPosition="1686" endWordPosition="1689">ada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs in Translation The model we use in our paraphrasing approach is a syntactically informed synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by Chiang (2005). Formally, a probabilistic SCFG G is defined by specifying G = hN, TS, TT, R, 5i, where N is a set of nonterminal symbols, TS and TT are the source and target language vocabularies, R is a set of rules and 5 ∈ N is the root symbol. The rules in R take the form: C → hγ, α,∼,wi, 1170 leak is consistent with our f). The nont</context>
<context position="12484" citStr="Cohn and Lapata, 2008" startWordPosition="1908" endWordPosition="1911">by the syntactic constituent that dominates e (in this case NP). To introduce nonterminals into the right-hand side of the rule, we can apply rules extracted over sub-phrases of f, synchronously substituting the corresponding nonterminal symbol for the sub-phrases on both sides. The synchronous substitution applied to f and e then yields the correspondency —. One significant differentiating factor between the competing ways of extracting SCFG rules is whether the extraction method generates rules only for constituent phrases that are dominated by a node in the parse tree (Galley et al., 2004; Cohn and Lapata, 2008) or whether they include arbitrary phrases, including non-constituent phrases (Zollmann and Venugopal, 2006; Callison-Burch, 2008). We adopt the extraction for all phrases, including non-constituents, since it allows us to cover a much greater set of phrases, both in translation and paraphrasing. Feature Functions Rather than assigning a single weight w, we define a set of feature functions ϕ~ = {ϕ1...ϕNI that are combined in a log-linear model: N w = — λi log ϕi. (4) i=1 ~ The weights λ of these feature functions are set to maximize some objective function like BLEU (Papineni et al., 2002) us</context>
<context position="24114" citStr="Cohn and Lapata (2008)" startWordPosition="3843" endWordPosition="3847"> to the BLEU metric in SMT. • A development set with examples of the sentential transformations that we are modeling. • Optionally, a way of injecting task-specific rules that were not extracted automatically. In the remainder of this section, we illustrate how our bilingually extracted paraphrases can be adapted to perform sentence compression, which is the task of reducing the length of sentence while preserving its core meaning. Most previous approaches to sentence compression focused only on the deletion of a subset of words from the sentence (Knight and Marcu, 2002). Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. 6.1 Feature Design In Section 4 we discussed phrasal probabilities. While these help quantify how good a paraphrase is in general, they do not make any statement on task-specific things such as the change in language complexity or text length. To make this information available to the decoder, we enhance our paraphrases with four compression-targeted features. We add the count features csrc and ctgt, indicating the number of words on either side of the rule as well as </context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>34--637</pages>
<contexts>
<context position="11208" citStr="Cohn and Lapata, 2009" startWordPosition="1690" endWordPosition="1693">elamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs in Translation The model we use in our paraphrasing approach is a syntactically informed synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by Chiang (2005). Formally, a probabilistic SCFG G is defined by specifying G = hN, TS, TT, R, 5i, where N is a set of nonterminal symbols, TS and TT are the source and target language vocabularies, R is a set of rules and 5 ∈ N is the root symbol. The rules in R take the form: C → hγ, α,∼,wi, 1170 leak is consistent with our f). The nonterminal symbol that is t</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence Research (JAIR), 34:637–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Culicover</author>
</authors>
<title>Paraphrase generation and information retrieval from stored text.</title>
<date>1968</date>
<booktitle>Mechanical Translation and Computational Linguistics,</booktitle>
<pages>11--1</pages>
<contexts>
<context position="1210" citStr="Culicover, 1968" startWordPosition="165" endWordPosition="166">aphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauch</context>
</contexts>
<marker>Culicover, 1968</marker>
<rawString>Peter W. Culicover. 1968. Paraphrase generation and information retrieval from stored text. Mechanical Translation and Computational Linguistics, 11(1-2):78–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<contexts>
<context position="6648" citStr="Dolan et al., 2004" startWordPosition="975" endWordPosition="978">ced syntacMadnani and Dorr (2010) survey a variety of data- tic constraints by labeling all phrases and paradriven paraphrasing techniques, categorizing them phrases (even non-constituent phrases) with CCGbased on the type of data that they use. These inspired slash categories (Steedman and Baldridge, include large monolingual texts (Lin and Pantel, 2011), an approach similar to Zollmann and Venu2001; Szpektor et al., 2004; Bhagat and Ravichan- gopal (2006)’s syntax-augmented machine transladran, 2008), comparable corpora (Barzilay and Lee, tion (SAMT). Callison-Burch did not formally de2003; Dolan et al., 2004), monolingual parallel cor- fine a synchronous grammar, nor discuss decoding, pora (Barzilay and McKeown, 2001; Pang et al., since his presentation did not include hierarchical 2003), and bilingual parallel corpora (Bannard and rules. Cohn and Lapata (2008) used the GHKM Callison-Burch, 2005; Madnani et al., 2007; Zhao et extraction method (Galley et al., 2004), which is al., 2008b). We focus on the latter type of data. limited to constituent phrases and thus produces Paraphrase extraction using bilingual parallel cor- a reasonably small set of syntactic rules. Zhao pora was proposed by Bannar</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>Representing paraphrases using synchronous tree adjoining grammars.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9934" citStr="Dras, 1997" startWordPosition="1501" endWordPosition="1502">ne side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machi</context>
</contexts>
<marker>Dras, 1997</marker>
<rawString>Mark Dras. 1997. Representing paraphrases using synchronous tree adjoining grammars. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>Tree Adjoining Grammar and the Reluctant Paraphrasing of Text.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University,</institution>
<contexts>
<context position="9946" citStr="Dras, 1999" startWordPosition="1503" endWordPosition="1504">he parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translati</context>
</contexts>
<marker>Dras, 1999</marker>
<rawString>Mark Dras. 1999. Tree Adjoining Grammar and the Reluctant Paraphrasing of Text. Ph.D. thesis, Macquarie University, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Compuatational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="10140" citStr="Gale and Church, 1993" startWordPosition="1530" endWordPosition="1533">st choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contribution</context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>William Gale and Kenneth Church. 1993. A program for aligning sentences in bilingual corpora. Compuatational Linguistics, 19(1):75–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="7011" citStr="Galley et al., 2004" startWordPosition="1029" endWordPosition="1032">approach similar to Zollmann and Venu2001; Szpektor et al., 2004; Bhagat and Ravichan- gopal (2006)’s syntax-augmented machine transladran, 2008), comparable corpora (Barzilay and Lee, tion (SAMT). Callison-Burch did not formally de2003; Dolan et al., 2004), monolingual parallel cor- fine a synchronous grammar, nor discuss decoding, pora (Barzilay and McKeown, 2001; Pang et al., since his presentation did not include hierarchical 2003), and bilingual parallel corpora (Bannard and rules. Cohn and Lapata (2008) used the GHKM Callison-Burch, 2005; Madnani et al., 2007; Zhao et extraction method (Galley et al., 2004), which is al., 2008b). We focus on the latter type of data. limited to constituent phrases and thus produces Paraphrase extraction using bilingual parallel cor- a reasonably small set of syntactic rules. Zhao pora was proposed by Bannard and Callison-Burch et al. (2008b) added slots to bilingually extracted (2005) who induced paraphrases using techniques paraphrase patterns that were labeled with part-offrom phrase-based statistical machine translation speech tags, but not larger syntactic constituents. (Koehn et al., 2003). After extracting a bilingual Before the shift to statistical natural</context>
<context position="9212" citStr="Galley et al., 2004" startWordPosition="1383" endWordPosition="1386">nction between the nonterminals in γ and α. A nonnegative weight w ≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Ind</context>
<context position="12460" citStr="Galley et al., 2004" startWordPosition="1904" endWordPosition="1907">e is then determined by the syntactic constituent that dominates e (in this case NP). To introduce nonterminals into the right-hand side of the rule, we can apply rules extracted over sub-phrases of f, synchronously substituting the corresponding nonterminal symbol for the sub-phrases on both sides. The synchronous substitution applied to f and e then yields the correspondency —. One significant differentiating factor between the competing ways of extracting SCFG rules is whether the extraction method generates rules only for constituent phrases that are dominated by a node in the parse tree (Galley et al., 2004; Cohn and Lapata, 2008) or whether they include arbitrary phrases, including non-constituent phrases (Zollmann and Venugopal, 2006; Callison-Burch, 2008). We adopt the extraction for all phrases, including non-constituents, since it allows us to cover a much greater set of phrases, both in translation and paraphrasing. Feature Functions Rather than assigning a single weight w, we define a set of feature functions ϕ~ = {ϕ1...ϕNI that are combined in a log-linear model: N w = — λi log ϕi. (4) i=1 ~ The weights λ of these feature functions are set to maximize some objective function like BLEU (P</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erica Greene</author>
<author>Tugba Bodrumlu</author>
<author>Kevin Knight</author>
</authors>
<title>Automatic analysis of rhythmic poetry with applications to generation and translation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="26716" citStr="Greene et al., 2010" startWordPosition="4282" endWordPosition="4285">cision estimate of BLEU with an additional “verbosity penalty” that is applied to compressions that fail to meet a given target compression rate cp. We rely on the BLEU brevity penalty to prevent the system from producing overly aggressive compressions. The scaling term A determines how severely we penalize deviations from cp. In our experiments we use A = 10. It is straightforward to find similar adaptations for other tasks. For text simplification, for instance, the penalty term can include a readability metric. For poetry generation we can analogously penalize outputs that break the meter (Greene et al., 2010). 6.3 Development Data To tune the parameters of our paraphrase system for sentence compression, we need an appropriate cor�N otherwise , e n=1 log wnpn 1174 pus of reference compressions. Since our model is designed to compress by paraphrasing rather than deletion, the commonly used deletion-based compression data sets like the Ziff-Davis corpus are not suitable. We have thus created a corpus of compression paraphrases. Beginning with 9570 tuples of parallel English–English sentences obtained from multiple reference translations for machine translation evaluation, we construct a parallel comp</context>
</contexts>
<marker>Greene, Bodrumlu, Knight, 2010</marker>
<rawString>Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010. Automatic analysis of rhythmic poetry with applications to generation and translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16650" citStr="Huang and Chiang, 2007" startWordPosition="2592" endWordPosition="2595">o promote more complex paraphrases that require structural reordering. Decoding With this, paraphrasing becomes an English-to-English translation problem which can be formulated similarly to Equation 5 as: ˆe2 ≈ yield(arg max p(d, e2|e1)). d∈D(e2,e1) Figure 3 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. Another advantage of using the decoder from statistical machine translation is that n-gram language models, which have been shown to be useful in natural language generation (Langkilde and Knight, 1998), are already well integrated (Huang and Chiang, 2007). 5 Analysis A key motivation for the use of syntactic paraphrases over their phrasal counterparts is their potential to capture meaning-preserving linguistic transformations in a more general fashion. A phrasal system is limited to memorizing fully lexicalized transformations in its paraphrase table, resulting in poor generalization capabilities. By contrast, a syntactic paraphrasing system intuitively should be able to address this issue and learn well-formed and generic patterns that can be easily applied to unseen data. Lexical paraphrase: JJ -&gt; beleidigend |offensive JJ - offensive |insul</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1831" citStr="Kauchak and Barzilay, 2006" startWordPosition="254" endWordPosition="257">1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP1’s NP2 the NP2 of the NP1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (a</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<pages>139--91</pages>
<contexts>
<context position="11161" citStr="Knight and Marcu, 2002" startWordPosition="1682" endWordPosition="1685">anslation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs in Translation The model we use in our paraphrasing approach is a syntactically informed synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by Chiang (2005). Formally, a probabilistic SCFG G is defined by specifying G = hN, TS, TT, R, 5i, where N is a set of nonterminal symbols, TS and TT are the source and target language vocabularies, R is a set of rules and 5 ∈ N is the root symbol. The rules in R take the form: C → hγ, α,∼,wi, 1170 leak is consisten</context>
<context position="24069" citStr="Knight and Marcu, 2002" startWordPosition="3836" endWordPosition="3839"> of the model, i.e. a task-specific equivalent to the BLEU metric in SMT. • A development set with examples of the sentential transformations that we are modeling. • Optionally, a way of injecting task-specific rules that were not extracted automatically. In the remainder of this section, we illustrate how our bilingually extracted paraphrases can be adapted to perform sentence compression, which is the task of reducing the length of sentence while preserving its core meaning. Most previous approaches to sentence compression focused only on the deletion of a subset of words from the sentence (Knight and Marcu, 2002). Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. 6.1 Feature Design In Section 4 we discussed phrasal probabilities. While these help quantify how good a paraphrase is in general, they do not make any statement on task-specific things such as the change in language complexity or text length. To make this information available to the decoder, we enhance our paraphrases with four compression-targeted features. We add the count features csrc and ctgt, indicating the number of</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial Intelligence, 139:91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="7541" citStr="Koehn et al., 2003" startWordPosition="1106" endWordPosition="1109">llison-Burch, 2005; Madnani et al., 2007; Zhao et extraction method (Galley et al., 2004), which is al., 2008b). We focus on the latter type of data. limited to constituent phrases and thus produces Paraphrase extraction using bilingual parallel cor- a reasonably small set of syntactic rules. Zhao pora was proposed by Bannard and Callison-Burch et al. (2008b) added slots to bilingually extracted (2005) who induced paraphrases using techniques paraphrase patterns that were labeled with part-offrom phrase-based statistical machine translation speech tags, but not larger syntactic constituents. (Koehn et al., 2003). After extracting a bilingual Before the shift to statistical natural language pro1169 cessing, paraphrasing was often treated as syntactic transformations or by parsing and then generating sie versuchten das leck mit einer detonation zu schliessen they PRP VBD NP tried closing the S VP VP VBG DT NN IN DT NN NP NP leak with a blast PP NP - das leck |the leak PP/NN - mit einer |with a VP - NP PP/NN detonation zu schliessen |closing NP PP/NN blast Figure 1: Synchronous grammar rules for translation are extracted from sentence pairs in a bixtext which have been automatically parsed and word-alig</context>
<context position="10294" citStr="Koehn et al., 2003" startWordPosition="1554" endWordPosition="1557"> this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a dis</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Kozlowski</author>
<author>Kathleen McCoy</author>
<author>K VijayShanker</author>
</authors>
<title>Generation of single-sentence paraphrases from predicate/argument structure using lexico-grammatical resources.</title>
<date>2003</date>
<booktitle>In Workshop On Paraphrasing.</booktitle>
<contexts>
<context position="9971" citStr="Kozlowski et al., 2003" startWordPosition="1505" endWordPosition="1508">corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and </context>
</contexts>
<marker>Kozlowski, McCoy, VijayShanker, 2003</marker>
<rawString>Raymond Kozlowski, Kathleen McCoy, and K. VijayShanker. 2003. Generation of single-sentence paraphrases from predicate/argument structure using lexico-grammatical resources. In Workshop On Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>The practical value of n-grams in generation.</title>
<date>1998</date>
<booktitle>In Workshop On Natural Language Generation,</booktitle>
<location>Ontario, Canada.</location>
<contexts>
<context position="16596" citStr="Langkilde and Knight, 1998" startWordPosition="2584" endWordPosition="2587">le swaps the order of two nonterminals, which enables us to promote more complex paraphrases that require structural reordering. Decoding With this, paraphrasing becomes an English-to-English translation problem which can be formulated similarly to Equation 5 as: ˆe2 ≈ yield(arg max p(d, e2|e1)). d∈D(e2,e1) Figure 3 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. Another advantage of using the decoder from statistical machine translation is that n-gram language models, which have been shown to be useful in natural language generation (Langkilde and Knight, 1998), are already well integrated (Huang and Chiang, 2007). 5 Analysis A key motivation for the use of syntactic paraphrases over their phrasal counterparts is their potential to capture meaning-preserving linguistic transformations in a more general fashion. A phrasal system is limited to memorizing fully lexicalized transformations in its paraphrase table, resulting in poor generalization capabilities. By contrast, a syntactic paraphrasing system intuitively should be able to address this issue and learn well-formed and generic patterns that can be easily applied to unseen data. Lexical paraphra</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. The practical value of n-grams in generation. In Workshop On Natural Language Generation, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of WMT09.</booktitle>
<contexts>
<context position="5688" citStr="Li et al., 2009" startWordPosition="837" endWordPosition="840">s we obtain and discuss the para- archical phrase-based machine translation (Chiang, phrastic transformations we are capable of cap- 2005), which is formally a synchronous context-free turing. grammar (SCFG) and thus can be thought of as a • Describe how training paradigms for syntac- paraphrase grammar. The paraphrase grammar can tic/sentential paraphrase models should be tai- paraphrase (or “decode”) input sentences using an lored to different text-to-text generation tasks. SCFG decoder, like the Hiero, Joshua or cdec MT • Demonstrate our framework’s suitability for a systems (Chiang, 2007; Li et al., 2009; Dyer et al., variety of text-to-text generation tasks by ob- 2010). Like Hiero, Madnani’s model uses just one taining state-of-the-art results on the example nonterminal X instead of linguistic nonterminals. task of sentence compression. Three additional efforts incorporated linguistic 2 Related Work syntax. Callison-Burch (2008) introduced syntacMadnani and Dorr (2010) survey a variety of data- tic constraints by labeling all phrases and paradriven paraphrasing techniques, categorizing them phrases (even non-constituent phrases) with CCGbased on the type of data that they use. These inspire</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of WMT09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Ann Irvine</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Ziyuan Wang</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua 2.0: A toolkit for parsing-based machine translation with syntax, semirings, discriminative training and other goodies.</title>
<date>2010</date>
<booktitle>In Proceedings of WMT10.</booktitle>
<contexts>
<context position="29940" citStr="Li et al., 2010" startWordPosition="4787" endWordPosition="4790">AMT toolkit (Venugopal and Zollmann, 2009). The grammars we extract tend to be extremely large. To keep their size manageable, we only consider translation rules that have been seen more than 3 times and whose translation probability exceeds 10−4 for pivot recombination. Additionally, we only retain the top 25 most likely paraphrases of each phrase, ranked by a uniformly weighted combination of phrasal and lexical paraphrase probabilities. We tuned the model parameters to our PR´ECIS objective function, implemented in the Z-MERT toolkit (Zaidan, 2009). For decoding we used the Joshua decoder (Li et al., 2010). The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a Kneser-Ney discounted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002). 6.6 Evaluation To assess the output quality of the resulting sentence compression system, we compare it to two state-ofthe-art sentence compression systems. Specifically, we compare against our implementation of Clarke and Lapata (2008)’s compression model which uses a series of constraints in an integer linear programming (ILP) solver, and Cohn and Lapata (2007)’s tree transducer toolkit </context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Irvine, Khudanpur, Schwartz, Thornton, Wang, Weese, Zaidan, 2010</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Ann Irvine, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Ziyuan Wang, Jonathan Weese, and Omar Zaidan. 2010. Joshua 2.0: A toolkit for parsing-based machine translation with syntax, semirings, discriminative training and other goodies. In Proceedings of WMT10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules from text.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="3110" citStr="Lin and Pantel, 2001" startWordPosition="454" endWordPosition="457">ly induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expressions or antonyms (such as {boy, girl} or {rise, fall}) for paraphrases. 1168 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Abundantly available bilingual par</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules from text. Natural Language Engineering, 7(3):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment templates for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/Coling.</booktitle>
<contexts>
<context position="9275" citStr="Liu et al., 2006" startWordPosition="1393" endWordPosition="1396">w ≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment templates for statistical machine translation. In Proceedings of the ACL/Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="2559" citStr="Madnani and Dorr, 2010" startWordPosition="367" endWordPosition="370">al paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP1’s NP2 the NP2 of the NP1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amou</context>
<context position="6062" citStr="Madnani and Dorr (2010)" startWordPosition="887" endWordPosition="891">should be tai- paraphrase (or “decode”) input sentences using an lored to different text-to-text generation tasks. SCFG decoder, like the Hiero, Joshua or cdec MT • Demonstrate our framework’s suitability for a systems (Chiang, 2007; Li et al., 2009; Dyer et al., variety of text-to-text generation tasks by ob- 2010). Like Hiero, Madnani’s model uses just one taining state-of-the-art results on the example nonterminal X instead of linguistic nonterminals. task of sentence compression. Three additional efforts incorporated linguistic 2 Related Work syntax. Callison-Burch (2008) introduced syntacMadnani and Dorr (2010) survey a variety of data- tic constraints by labeling all phrases and paradriven paraphrasing techniques, categorizing them phrases (even non-constituent phrases) with CCGbased on the type of data that they use. These inspired slash categories (Steedman and Baldridge, include large monolingual texts (Lin and Pantel, 2011), an approach similar to Zollmann and Venu2001; Szpektor et al., 2004; Bhagat and Ravichan- gopal (2006)’s syntax-augmented machine transladran, 2008), comparable corpora (Barzilay and Lee, tion (SAMT). Callison-Burch did not formally de2003; Dolan et al., 2004), monolingual </context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Necip Fazil Ayan</author>
<author>Philip Resnik</author>
<author>Bonnie Dorr</author>
</authors>
<title>Using paraphrases for parameter tuning in statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of WMT07.</booktitle>
<contexts>
<context position="1853" citStr="Madnani et al., 2007" startWordPosition="258" endWordPosition="261">ing and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP1’s NP2 the NP2 of the NP1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalenc</context>
<context position="6962" citStr="Madnani et al., 2007" startWordPosition="1021" endWordPosition="1024">rge monolingual texts (Lin and Pantel, 2011), an approach similar to Zollmann and Venu2001; Szpektor et al., 2004; Bhagat and Ravichan- gopal (2006)’s syntax-augmented machine transladran, 2008), comparable corpora (Barzilay and Lee, tion (SAMT). Callison-Burch did not formally de2003; Dolan et al., 2004), monolingual parallel cor- fine a synchronous grammar, nor discuss decoding, pora (Barzilay and McKeown, 2001; Pang et al., since his presentation did not include hierarchical 2003), and bilingual parallel corpora (Bannard and rules. Cohn and Lapata (2008) used the GHKM Callison-Burch, 2005; Madnani et al., 2007; Zhao et extraction method (Galley et al., 2004), which is al., 2008b). We focus on the latter type of data. limited to constituent phrases and thus produces Paraphrase extraction using bilingual parallel cor- a reasonably small set of syntactic rules. Zhao pora was proposed by Bannard and Callison-Burch et al. (2008b) added slots to bilingually extracted (2005) who induced paraphrases using techniques paraphrase patterns that were labeled with part-offrom phrase-based statistical machine translation speech tags, but not larger syntactic constituents. (Koehn et al., 2003). After extracting a </context>
<context position="10400" citStr="Madnani et al., 2007" startWordPosition="1567" endWordPosition="1570">emtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generatio</context>
<context position="20715" citStr="Madnani et al. (2007)" startWordPosition="3313" endWordPosition="3316">preserving transformations and hand-picked examples of syntactic paraphrases that our system extracts capturing these. give decontamination equipment to Japan give Japan decontamination equipment provide decontamination equipment to Japan ? provide Japan decontamination equipment Note how our system extracts a dative shift rule for to give and a rule that both shifts and substitutes a more appropriate verb for to provide. The use of syntactic nonterminals in our paraphrase rules to capture complex transforms also makes it possible to impose constraints on their application. For comparison, as Madnani et al. (2007) do not impose any constraints on how the nonterminal X can be realized, their equivalent of the topicalization rule would massively overgeneralize: S → X1, X2 . |X2, X1 . Additional examples of transforms our use of syntax allows us to capture are the adverbial phrase shift and the reduction of a relative clause, as well as other phenomena listed in Table 1. Unsurprisingly, syntactic information alone is not sufficient to capture all transformations. For instance it is hard to extract generic paraphrases for all instances of passivization, since our syntactic model currently has no means of r</context>
<context position="25139" citStr="Madnani et al., 2007" startWordPosition="4008" endWordPosition="4011">e to the decoder, we enhance our paraphrases with four compression-targeted features. We add the count features csrc and ctgt, indicating the number of words on either side of the rule as well as two difference features: cdcount = ctgt − csrc and the analogously computed difference in the average word length in characters, cdavg. 6.2 Objective Function Given our paraphrasing system’s connection to SMT, the naive/obvious choice for parameter optimization would be to optimize for BLEU over a set of paraphrases, for instance parallel English reference translations for a machine translation task (Madnani et al., 2007). For a candidate C and a reference R, (with lengths c and r) BLEU is defined as: BLEUN(C, R) J �N e(1−c/r) · e n=1 log wnpn if c/r G 1 = where pn is the modified n-gram precision of C against R, with typically N = 4 and wn = 1 N . The “brevity penalty” term e(1−c/r) is added to prevent short candidates from achieving perfect scores. Naively optimizing for BLEU, however, will result in a trivial paraphrasing system heavily biased towards producing identity “paraphrases”. This is obviously not what we are looking for. Moreover, BLEU does not provide a mechanism for directly specifying a per-sen</context>
</contexts>
<marker>Madnani, Ayan, Resnik, Dorr, 2007</marker>
<rawString>Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie Dorr. 2007. Using paraphrases for parameter tuning in statistical machine translation. In Proceedings of WMT07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="31292" citStr="McDonald (2006)" startWordPosition="4996" endWordPosition="4997">ws changes to the tree topology. Cohn and Lapata argue that this is a natural fit for sentence compression, since deletions introduce structural mismatches. We trained the T3 software2 on the 936 (full, compressed) sentence pairs that comprise our development set. This is equivalent in size to the training corpora that Cohn and Lapata (2007) used (their training corpora ranged from 2www.dcs.shef.ac.uk/people/T.Cohn/t3/ 1175 882–1020 sentence pairs), and has the advantage of being in-domain with respect to our test set. Both these systems reported results outperforming previous systems such as McDonald (2006). To showcase the value of the adaptations discussed above, we also compare variants of our paraphrase-based compression systems: using Hiero instead of syntax, using syntax with or without compression features, using an augmented grammar with optional deletion rules. We solicit human judgments of the compressions along two five-point scales: grammaticality and meaning. Judges are instructed to decide how much the meaning from a reference translation is retained in the compressed sentence, with a score of 5 indicating that all of the important information is present, and 1 being that the compr</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Paraphrasing using given and new information in a question-answer system.</title>
<date>1979</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1554" citStr="McKeown, 1979" startWordPosition="215" endWordPosition="216">illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterm</context>
<context position="9736" citStr="McKeown, 1979" startWordPosition="1474" endWordPosition="1475">cs have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hiera</context>
</contexts>
<marker>McKeown, 1979</marker>
<rawString>Kathleen R. McKeown. 1979. Paraphrasing using given and new information in a question-answer system. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10598" citStr="Melamed, 2004" startWordPosition="1600" endWordPosition="1601">se of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lap</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>Dan Melamed. 2004. Statistical machine translation by parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie W Meteer</author>
<author>Varda Shaked</author>
</authors>
<title>Strategies for effective paraphrasing.</title>
<date>1988</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="9775" citStr="Meteer and Shaked, 1988" startWordPosition="1478" endWordPosition="1481"> they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 20</context>
</contexts>
<marker>Meteer, Shaked, 1988</marker>
<rawString>Marie W. Meteer and Varda Shaked. 1988. Strategies for effective paraphrasing. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Muraki</author>
</authors>
<title>On a semantic model for multilingual paraphrasing.</title>
<date>1982</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="9750" citStr="Muraki, 1982" startWordPosition="1476" endWordPosition="1477">tended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase</context>
</contexts>
<marker>Muraki, 1982</marker>
<rawString>Kazunori Muraki. 1982. On a semantic model for multilingual paraphrasing. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Chris Callison-Burch</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Evaluating sentence compression: Pitfalls and suggested remedies.</title>
<date>2011</date>
<booktitle>In Workshop on Monolingual Text-To-Text Generation.</booktitle>
<marker>Napoles, Callison-Burch, Van Durme, 2011</marker>
<rawString>Courtney Napoles, Chris Callison-Burch, and Benjamin Van Durme. 2011. Evaluating sentence compression: Pitfalls and suggested remedies. In Workshop on Monolingual Text-To-Text Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8920" citStr="Och (2003" startWordPosition="1343" endWordPosition="1344">nt phrases. where the rule’s left-hand side C ∈ N is a nonterminal, γ ∈ (N ∪TS)* and α ∈ (N ∪TT)* are strings of terminal and nonterminal symbols with an equal number of nonterminals cNT (γ) = cNT (α) and ∼: {1 ... cNT(γ)} → {1 ... cNT(α)} constitutes a one-to-one correspondency function between the nonterminals in γ and α. A nonnegative weight w ≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we firs</context>
<context position="10462" citStr="Och, 2003" startWordPosition="1578" endWordPosition="1579">g (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the</context>
<context position="13161" citStr="Och (2003" startWordPosition="2023" endWordPosition="2024">t phrases (Zollmann and Venugopal, 2006; Callison-Burch, 2008). We adopt the extraction for all phrases, including non-constituents, since it allows us to cover a much greater set of phrases, both in translation and paraphrasing. Feature Functions Rather than assigning a single weight w, we define a set of feature functions ϕ~ = {ϕ1...ϕNI that are combined in a log-linear model: N w = — λi log ϕi. (4) i=1 ~ The weights λ of these feature functions are set to maximize some objective function like BLEU (Papineni et al., 2002) using a procedure called minimum error rate training (MERT), owing to Och (2003a). MERT iteratively adjusts the weights until the decoder produces output that best matches reference translations in a development set, according to the objective function. We will examine appropriate objective functions for text-to-text generation tasks in Section 6.2. Typical features used in statistical machine translation include phrase translation probabilities (calculated using maximum likelihood estimation over all phrase pairs enumerable in the parallel corpus), word-for-word lexical translation probabilities (which help to smooth sparser phrase translation estimates), a “rule applic</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003a. Minimum error rate training for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8920" citStr="Och (2003" startWordPosition="1343" endWordPosition="1344">nt phrases. where the rule’s left-hand side C ∈ N is a nonterminal, γ ∈ (N ∪TS)* and α ∈ (N ∪TT)* are strings of terminal and nonterminal symbols with an equal number of nonterminals cNT (γ) = cNT (α) and ∼: {1 ... cNT(γ)} → {1 ... cNT(α)} constitutes a one-to-one correspondency function between the nonterminals in γ and α. A nonnegative weight w ≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we firs</context>
<context position="10462" citStr="Och, 2003" startWordPosition="1578" endWordPosition="1579">g (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the</context>
<context position="13161" citStr="Och (2003" startWordPosition="2023" endWordPosition="2024">t phrases (Zollmann and Venugopal, 2006; Callison-Burch, 2008). We adopt the extraction for all phrases, including non-constituents, since it allows us to cover a much greater set of phrases, both in translation and paraphrasing. Feature Functions Rather than assigning a single weight w, we define a set of feature functions ϕ~ = {ϕ1...ϕNI that are combined in a log-linear model: N w = — λi log ϕi. (4) i=1 ~ The weights λ of these feature functions are set to maximize some objective function like BLEU (Papineni et al., 2002) using a procedure called minimum error rate training (MERT), owing to Och (2003a). MERT iteratively adjusts the weights until the decoder produces output that best matches reference translations in a development set, according to the objective function. We will examine appropriate objective functions for text-to-text generation tasks in Section 6.2. Typical features used in statistical machine translation include phrase translation probabilities (calculated using maximum likelihood estimation over all phrase pairs enumerable in the parallel corpus), word-for-word lexical translation probabilities (which help to smooth sparser phrase translation estimates), a “rule applic</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003b. Minimum error rate training in statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Declan Groves</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Contextual bitext-derived paraphrases in automatic MT evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of WMT06.</booktitle>
<marker>Owczarzak, Groves, Van Genabith, Way, 2006</marker>
<rawString>Karolina Owczarzak, Declan Groves, Josef Van Genabith, and Andy Way. 2006. Contextual bitext-derived paraphrases in automatic MT evaluation. In Proceedings of WMT06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="2825" citStr="Pang et al., 2003" startWordPosition="411" endWordPosition="414">nstraints: the NP1’s NP2 the NP2 of the NP1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expressions or antonyms (such </context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="13081" citStr="Papineni et al., 2002" startWordPosition="2007" endWordPosition="2011">4; Cohn and Lapata, 2008) or whether they include arbitrary phrases, including non-constituent phrases (Zollmann and Venugopal, 2006; Callison-Burch, 2008). We adopt the extraction for all phrases, including non-constituents, since it allows us to cover a much greater set of phrases, both in translation and paraphrasing. Feature Functions Rather than assigning a single weight w, we define a set of feature functions ϕ~ = {ϕ1...ϕNI that are combined in a log-linear model: N w = — λi log ϕi. (4) i=1 ~ The weights λ of these feature functions are set to maximize some objective function like BLEU (Papineni et al., 2002) using a procedure called minimum error rate training (MERT), owing to Och (2003a). MERT iteratively adjusts the weights until the decoder produces output that best matches reference translations in a development set, according to the objective function. We will examine appropriate objective functions for text-to-text generation tasks in Section 6.2. Typical features used in statistical machine translation include phrase translation probabilities (calculated using maximum likelihood estimation over all phrase pairs enumerable in the parallel corpus), word-for-word lexical translation probabili</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="10253" citStr="Quirk et al., 2004" startWordPosition="1548" endWordPosition="1551">are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be ext</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10619" citStr="Quirk et al., 2005" startWordPosition="1602" endWordPosition="1605">al machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs i</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning sufrace text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1610" citStr="Ravichandran and Hovy, 2002" startWordPosition="221" endWordPosition="224">raphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic const</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning sufrace text patterns for a question answering system. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1633" citStr="Riezler et al., 2007" startWordPosition="225" endWordPosition="228">f sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP1’s NP2 t</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hadar Shemtov</author>
</authors>
<title>Generation of paraphrases from ambiguous logical forms.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="9790" citStr="Shemtov, 1996" startWordPosition="1482" endWordPosition="1484"> grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et </context>
</contexts>
<marker>Shemtov, 1996</marker>
<rawString>Hadar Shemtov. 1996. Generation of paraphrases from ambiguous logical forms. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Generation and synchronous tree-adjoining grammars.</title>
<date>1990</date>
<booktitle>In Workshop On Natural Language Generation.</booktitle>
<contexts>
<context position="9922" citStr="Shieber and Schabes, 1990" startWordPosition="1497" endWordPosition="1500">tion methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here synta</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Stuart Shieber and Yves Schabes. 1990. Generation and synchronous tree-adjoining grammars. In Workshop On Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="1875" citStr="Snover et al., 2010" startWordPosition="262" endWordPosition="265">phrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP1’s NP2 the NP2 of the NP1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2010</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2010. Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate. Machine Translation, 23(2-3):117–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Jason Baldridge</author>
</authors>
<title>Combinatory categorial grammar. In Non-Transformational Syntax: Formal and Explicit Models of Grammar.</title>
<date>2011</date>
<publisher>Wiley-Blackwell.</publisher>
<marker>Steedman, Baldridge, 2011</marker>
<rawString>Mark Steedman and Jason Baldridge. 2011. Combinatory categorial grammar. In Non-Transformational Syntax: Formal and Explicit Models of Grammar. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceeding of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="30146" citStr="Stolcke, 2002" startWordPosition="4822" endWordPosition="4823">e translation probability exceeds 10−4 for pivot recombination. Additionally, we only retain the top 25 most likely paraphrases of each phrase, ranked by a uniformly weighted combination of phrasal and lexical paraphrase probabilities. We tuned the model parameters to our PR´ECIS objective function, implemented in the Z-MERT toolkit (Zaidan, 2009). For decoding we used the Joshua decoder (Li et al., 2010). The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a Kneser-Ney discounted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002). 6.6 Evaluation To assess the output quality of the resulting sentence compression system, we compare it to two state-ofthe-art sentence compression systems. Specifically, we compare against our implementation of Clarke and Lapata (2008)’s compression model which uses a series of constraints in an integer linear programming (ILP) solver, and Cohn and Lapata (2007)’s tree transducer toolkit (T3) which learns a synchronous tree substitution grammar (STSG) from paired monolingual sentences. Unlike SCFGs, the STSG formalism allows changes to the tree topology. Cohn and Lapata argue that this is a</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceeding of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Hristo Tanev</author>
<author>Ido Dagan</author>
<author>Bonaventura Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP, Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6455" citStr="Szpektor et al., 2004" startWordPosition="948" endWordPosition="951">n the example nonterminal X instead of linguistic nonterminals. task of sentence compression. Three additional efforts incorporated linguistic 2 Related Work syntax. Callison-Burch (2008) introduced syntacMadnani and Dorr (2010) survey a variety of data- tic constraints by labeling all phrases and paradriven paraphrasing techniques, categorizing them phrases (even non-constituent phrases) with CCGbased on the type of data that they use. These inspired slash categories (Steedman and Baldridge, include large monolingual texts (Lin and Pantel, 2011), an approach similar to Zollmann and Venu2001; Szpektor et al., 2004; Bhagat and Ravichan- gopal (2006)’s syntax-augmented machine transladran, 2008), comparable corpora (Barzilay and Lee, tion (SAMT). Callison-Burch did not formally de2003; Dolan et al., 2004), monolingual parallel cor- fine a synchronous grammar, nor discuss decoding, pora (Barzilay and McKeown, 2001; Pang et al., since his presentation did not include hierarchical 2003), and bilingual parallel corpora (Bannard and rules. Cohn and Lapata (2008) used the GHKM Callison-Burch, 2005; Madnani et al., 2007; Zhao et extraction method (Galley et al., 2004), which is al., 2008b). We focus on the latt</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Coppola. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP, Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
</authors>
<title>Grammar based statistical MT on Hadoop: An end-to-end toolkit for large scale PSCFG based MT.</title>
<date>2009</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>91</volume>
<contexts>
<context position="29366" citStr="Venugopal and Zollmann, 2009" startWordPosition="4696" endWordPosition="4699">adverbs and determiners. 6.5 Experimental Setup We extracted a paraphrase grammar from the French–English Europarl corpus (v5). The bitext was aligned using the Berkeley aligner and the English side was parsed with the Berkeley parser. We Grammar # Rules total 42,353,318 w/o identity 23,641,016 w/o complex constituents 6,439,923 w/o complex const. &amp; identity 5,097,250 Table 2: Number and distribution of rules in our paraphrase grammar. Note the significant number of identity paraphrases and rules with complex nonterminal labels. obtained the initial translation grammar using the SAMT toolkit (Venugopal and Zollmann, 2009). The grammars we extract tend to be extremely large. To keep their size manageable, we only consider translation rules that have been seen more than 3 times and whose translation probability exceeds 10−4 for pivot recombination. Additionally, we only retain the top 25 most likely paraphrases of each phrase, ranked by a uniformly weighted combination of phrasal and lexical paraphrase probabilities. We tuned the model parameters to our PR´ECIS objective function, implemented in the Z-MERT toolkit (Zaidan, 2009). For decoding we used the Joshua decoder (Li et al., 2010). The language model used </context>
</contexts>
<marker>Venugopal, Zollmann, 2009</marker>
<rawString>Ashish Venugopal and Andreas Zollmann. 2009. Grammar based statistical MT on Hadoop: An end-to-end toolkit for large scale PSCFG based MT. Prague Bulletin of Mathematical Linguistics, 91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="10558" citStr="Wu, 1997" startWordPosition="1594" endWordPosition="1595">zlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10583" citStr="Yamada and Knight, 2001" startWordPosition="1596" endWordPosition="1599"> al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 200</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhide Yamamoto</author>
</authors>
<title>Machine translation by interaction between paraphraser and transfer.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="9807" citStr="Yamamoto, 2002" startWordPosition="1485" endWordPosition="1486">(Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-l</context>
</contexts>
<marker>Yamamoto, 2002</marker>
<rawString>Kazuhide Yamamoto. 2002. Machine translation by interaction between paraphraser and transfer. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="29881" citStr="Zaidan, 2009" startWordPosition="4778" endWordPosition="4779">ls. obtained the initial translation grammar using the SAMT toolkit (Venugopal and Zollmann, 2009). The grammars we extract tend to be extremely large. To keep their size manageable, we only consider translation rules that have been seen more than 3 times and whose translation probability exceeds 10−4 for pivot recombination. Additionally, we only retain the top 25 most likely paraphrases of each phrase, ranked by a uniformly weighted combination of phrasal and lexical paraphrase probabilities. We tuned the model parameters to our PR´ECIS objective function, implemented in the Z-MERT toolkit (Zaidan, 2009). For decoding we used the Joshua decoder (Li et al., 2010). The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a Kneser-Ney discounted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002). 6.6 Evaluation To assess the output quality of the resulting sentence compression system, we compare it to two state-ofthe-art sentence compression systems. Specifically, we compare against our implementation of Clarke and Lapata (2008)’s compression model which uses a series of constraints in an integer linear programming (ILP) so</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Cheng Niu</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Combining multiple resources to improve SMT-based paraphrasing model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="4307" citStr="Zhao et al., 2008" startWordPosition="626" endWordPosition="629">ilable bilingual parallel corpora phrase table, English paraphrases are obtained by have been shown to address both these issues, ob- pivoting through foreign language phrases. Since taining paraphrases via a pivoting step over foreign many paraphrases can be extracted for a phrase, language phrases (Bannard and Callison-Burch, Bannard and Callison-Burch rank them using a para2005). The coverage of paraphrase lexica extracted phrase probability defined in terms of the translation from bitexts has been shown to outperform that model probabilities p(f|e) and p(e|f): obtained from other sources (Zhao et al., 2008a). �p(e2|e1) = p(e2,f|e1) (1) While there have been efforts pursuing the extrac- f tion of more powerful paraphrases (Madnani et �= p(e2|f, e1)p(f|e1) (2) al., 2007; Callison-Burch, 2008; Cohn and Lapata, f 2008; Zhao et al., 2008b), it is not yet clear to what E≈ p(e2|f)p(f|e1). (3) extent sentential paraphrases can be induced from f bitexts. In this paper we: Several subsequent efforts extended the bilin• Extend the bilingual pivoting approach to para- gual pivoting technique, many of which introduced phrase induction to produce rich syntactic para- elements of more contemporary syntax-base</context>
<context position="10504" citStr="Zhao et al., 2008" startWordPosition="1584" endWordPosition="1587">grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of para</context>
</contexts>
<marker>Zhao, Niu, Zhou, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng Li. 2008a. Combining multiple resources to improve SMT-based paraphrasing model. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Pivot approach for extracting paraphrase patterns from bilingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="4307" citStr="Zhao et al., 2008" startWordPosition="626" endWordPosition="629">ilable bilingual parallel corpora phrase table, English paraphrases are obtained by have been shown to address both these issues, ob- pivoting through foreign language phrases. Since taining paraphrases via a pivoting step over foreign many paraphrases can be extracted for a phrase, language phrases (Bannard and Callison-Burch, Bannard and Callison-Burch rank them using a para2005). The coverage of paraphrase lexica extracted phrase probability defined in terms of the translation from bitexts has been shown to outperform that model probabilities p(f|e) and p(e|f): obtained from other sources (Zhao et al., 2008a). �p(e2|e1) = p(e2,f|e1) (1) While there have been efforts pursuing the extrac- f tion of more powerful paraphrases (Madnani et �= p(e2|f, e1)p(f|e1) (2) al., 2007; Callison-Burch, 2008; Cohn and Lapata, f 2008; Zhao et al., 2008b), it is not yet clear to what E≈ p(e2|f)p(f|e1). (3) extent sentential paraphrases can be induced from f bitexts. In this paper we: Several subsequent efforts extended the bilin• Extend the bilingual pivoting approach to para- gual pivoting technique, many of which introduced phrase induction to produce rich syntactic para- elements of more contemporary syntax-base</context>
<context position="10504" citStr="Zhao et al., 2008" startWordPosition="1584" endWordPosition="1587">grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of para</context>
</contexts>
<marker>Zhao, Wang, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2008b. Pivot approach for extracting paraphrase patterns from bilingual corpora. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Xiang Lan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Application-driven statistical paraphrase generation.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="11027" citStr="Zhao et al., 2009" startWordPosition="1661" endWordPosition="1664">ear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs in Translation The model we use in our paraphrasing approach is a syntactically informed synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by Chiang (2005). Formally, a probabilistic SCFG G is defined by specifying G = hN, TS, TT, R, 5i, where N is a set of nonterminal symbols, TS and TT are the source and target languag</context>
</contexts>
<marker>Zhao, Lan, Liu, Li, 2009</marker>
<rawString>Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009. Application-driven statistical paraphrase generation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin-Yew Lin</author>
<author>Dragos Stefan Munteanu</author>
<author>Eduard Hovy</author>
</authors>
<title>Paraeval: Using paraphrases to evaluate summaries automatically.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="1803" citStr="Zhou et al., 2006" startWordPosition="250" endWordPosition="253">mation (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP1’s NP2 the NP2 of the NP1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of d</context>
</contexts>
<marker>Zhou, Lin, Munteanu, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. Paraeval: Using paraphrases to evaluate summaries automatically. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of WMT06.</booktitle>
<contexts>
<context position="9256" citStr="Zollmann and Venugopal, 2006" startWordPosition="1389" endWordPosition="1392">γ and α. A nonnegative weight w ≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (</context>
<context position="12591" citStr="Zollmann and Venugopal, 2006" startWordPosition="1921" endWordPosition="1925">ight-hand side of the rule, we can apply rules extracted over sub-phrases of f, synchronously substituting the corresponding nonterminal symbol for the sub-phrases on both sides. The synchronous substitution applied to f and e then yields the correspondency —. One significant differentiating factor between the competing ways of extracting SCFG rules is whether the extraction method generates rules only for constituent phrases that are dominated by a node in the parse tree (Galley et al., 2004; Cohn and Lapata, 2008) or whether they include arbitrary phrases, including non-constituent phrases (Zollmann and Venugopal, 2006; Callison-Burch, 2008). We adopt the extraction for all phrases, including non-constituents, since it allows us to cover a much greater set of phrases, both in translation and paraphrasing. Feature Functions Rather than assigning a single weight w, we define a set of feature functions ϕ~ = {ϕ1...ϕNI that are combined in a log-linear model: N w = — λi log ϕi. (4) i=1 ~ The weights λ of these feature functions are set to maximize some objective function like BLEU (Papineni et al., 2002) using a procedure called minimum error rate training (MERT), owing to Och (2003a). MERT iteratively adjusts t</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of WMT06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>