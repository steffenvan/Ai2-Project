<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.548192">
Whitepaper of NEWS 2010 Shared Task on Transliteration Generation*
</title>
<author confidence="0.737842">
Haizhou Li†, A Kumaran$, Min Zhang† and Vladimir Pervouchine†
</author>
<affiliation confidence="0.728081">
†Institute for Infocomm Research, A*STAR, Singapore 138632
</affiliation>
<email confidence="0.815665">
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
</email>
<note confidence="0.638584">
$Multilingual Systems Research, Microsoft Research India
</note>
<email confidence="0.992493">
A.Kumaran@microsoft.com
</email>
<sectionHeader confidence="0.994657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994">
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of
shared task in the NEWS 2010 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
</bodyText>
<sectionHeader confidence="0.96792" genericHeader="method">
1 Task Description
</sectionHeader>
<bodyText confidence="0.99997105">
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be
released, on which the participants are expected
to produce a ranked list of transliteration candi-
dates in another language (i.e. n-best translitera-
tions), and this will be evaluated using common
metrics. For every language pair the participants
must submit at least one run that uses only the
data provided by the NEWS workshop organisers
in a given language pair (designated as “standard”
run, primary submission). Users may submit more
“stanrard” runs. They may also submit several
“non-standard” runs for each language pair that
</bodyText>
<footnote confidence="0.442544">
*http://translit.i2r.a-star.edu.sg/news2010/
</footnote>
<bodyText confidence="0.929470333333333">
use other data than those provided by the NEWS
2010 workshop; such runs would be evaluated and
reported separately.
</bodyText>
<sectionHeader confidence="0.974414" genericHeader="method">
2 Important Dates
</sectionHeader>
<table confidence="0.756857666666667">
Research paper submission deadline 1 May 2010
Shared task
Registration opens 1 Feb 2010
Registration closes 13 Mar 2010
Training/Development data release 19 Feb 2010
Test data release 13 Mar 2010
Results Submission Due 20 Mar 2010
Results Announcement 27 Mar 2010
Task (short) Papers Due 5 Apr 2010
For all submissions
Acceptance Notification 6 May 2010
Workshop Date 16 Jul 2010
</table>
<sectionHeader confidence="0.916624" genericHeader="method">
3 Participation
</sectionHeader>
<listItem confidence="0.994377875">
1. Registration (1 Feb 2010)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training &amp; Development Data (19 Feb 2010)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
(b) All registered participants are required
to participate in the evaluation of at least
one language pair, submit the results and
a short paper and attend the workshop at
ACL 2010.
3. Evaluation Script (19 Feb 2010)
</listItem>
<page confidence="0.924381">
12
</page>
<note confidence="0.586788">
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 12–20,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.931769656565656">
(a) A sample test set and expected user out-
put format are to be released.
(b) An evaluation script, which runs on the
above two, is to be released.
(c) The participants must make sure that
their output is produced in a way that
the evaluation script may run and pro-
duce the expected output.
(d) The same script (with held out test data
and the user outputs) would be used for
final evaluation.
4. Test data (13 Mar 2010)
(a) The test data would be released on 13
March 2010, and the participants have a
maximum of 7 days to submit their re-
sults in the expected format.
(b) One “standard” run must be submit-
ted from every group on a given lan-
guage pair. Additional “standard” runs
may be submitted, up to 4 “standard”
runs in total. However, the partici-
pants must indicate one of the submit-
ted “standard” runs as the “primary sub-
mission”. The primary submission will
be used for the performance summary.
In addition to the “standard” runs, more
“non-standard” runs may be submitted.
In total, maximum 8 runs (up to 4 “stan-
dard” runs plus up to 4 “non-standard”
runs) can be submitted from each group
on a registered language pair. The defi-
nition of “standard” and “non-standard”
runs is in Section 5.
(c) Any runs that are “non-standard” must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
“transliteration generation” task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of “transliteration discov-
ery”, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
5. Results (27 Mar 2010)
(a) On 27 March 2010, the evaluation re-
sults would be announced and will be
made available on the Workshop web-
site.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Furthermore, all participants should
agree not to reveal identities of other
participants in any of their publications
unless you get permission from the other
respective participants. By default, all
participants remain anonymous in pub-
lished results, unless they indicate oth-
erwise at the time of uploading their re-
sults. Note that the results of all systems
will be published, but the identities of
those participants that choose not to dis-
close their identity to other participants
will be masked. As a result, in this case,
your organisation name will still appear
in the web site as one of participants, but
it will not be linked explicitly to your re-
sults.
6. Short Papers on Task (5 Apr 2010)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) The review of the system papers will be
done to improve paper quality and read-
ability and make sure the authors’ ideas
</listItem>
<page confidence="0.996151">
13
</page>
<bodyText confidence="0.9990562">
and methods can be understood by the
workshop participants. We are aiming
at accepting all system papers, and se-
lected ones will be presented orally in
the NEWS 2010 workshop.
</bodyText>
<listItem confidence="0.989335">
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review will be
managed electronically through https://
www.softconf.com/acl2010/NEWS.
</listItem>
<sectionHeader confidence="0.939051" genericHeader="method">
4 Language Pairs
</sectionHeader>
<bodyText confidence="0.977330434782609">
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1. NEWS 2010 Shared Task
offers 12 evaluation subtasks, among them ChEn
and ThEn are the back-transliteration of EnCh and
EnTh tasks respectively. NEWS 2010 releases
training, development and testing data for each of
the language pairs. NEWS 2010 continues some
language pairs that were evaluated in NEWS 2009.
In such cases, the training and development data in
the release of NEWS 2010 may overlap with those
in NEWS 2009. However, the test data in NEWS
2010 are entirely new.
The names given in the training sets for Chi-
nese, Japanese, Korean and Thai languages are
Western names and their respective translitera-
tions; the Japanese Name (in English) → Japanese
Kanji data set consists only of native Japanese
names; the Arabic data set consists only of native
Arabic names. The Indic data set (Hindi, Tamil,
Kannada, Bangla) consists of a mix of Indian and
Western names.
Examples of transliteration:
</bodyText>
<figure confidence="0.95751325">
English → Chinese
Timothy →�#,9
English → Japanese Katakana
Harrington → �9✓r�
English → Korean Hangul
Bennett → 베넷
Japanese name in English → Japanese Kanji
Akihiro → #9AL&apos;
English → Kannada
Arabic → Arabic name in English
�� → Khalid
5 Standard Databases
</figure>
<subsectionHeader confidence="0.443708">
Training Data (Parallel)
</subsectionHeader>
<bodyText confidence="0.73729525">
Paired names between source and target lan-
guages; size 5K – 32K.
Training Data is used for training a basic
transliteration system.
</bodyText>
<subsectionHeader confidence="0.794252">
Development Data (Parallel)
</subsectionHeader>
<bodyText confidence="0.988722166666667">
Paired names between source and target lan-
guages; size 2K – 6K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
</bodyText>
<subsectionHeader confidence="0.795848">
Testing Data
</subsectionHeader>
<bodyText confidence="0.94192325">
Source names only; size 2K – 3K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
</bodyText>
<listItem confidence="0.969897458333333">
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al.,
2004; MSRI, 2010; CJKI, 2010). NEWS
2010 will provide the contact details of each
individual database. The data would be pro-
vided in Unicode UTF-8 encoding, in XML
format; the results are expected to be sub-
mitted in UTF-8 encoding in XML format.
The XML formats details are available in Ap-
pendix A.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
</listItem>
<figure confidence="0.5356855">
English → Hindi
English → Tamil
</figure>
<page confidence="0.92994">
14
</page>
<table confidence="0.999880428571428">
Name origin Source script Target script Data Owner Data Size Task ID
Train Dev Test
Western English Chinese Institute for Infocomm Research 32K 6K 2K EnCh
Western Chinese English Institute for Infocomm Research 25K 5K 2K ChEn
Western English Korean Hangul CJK Institute 5K 2K 2K EnKo
Western English Japanese Katakana CJK Institute 23K 3K 3K EnJa
Japanese English Japanese Kanji CJK Institute 7K 3K 3K JnJk
Arabic Arabic English CJK Institute 25K 2.5K 2.5K ArAe
Mixed English Hindi Microsoft Research India 10K 2K 2K EnHi
Mixed English Tamil Microsoft Research India 8K 2K 2K EnTa
Mixed English Kannada Microsoft Research India 8K 2K 2K EnKa
Mixed English Bangla Microsoft Research India 10K 2K 2K EnBa
Western English Thai NECTEC 26K 2K 2K EnTh
Western Thai English NECTEC 24K 2K 2K ThEn
</table>
<tableCaption confidence="0.999787">
Table 1: Source and target languages for the shared task on transliteration.
</tableCaption>
<bodyText confidence="0.901432555555556">
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
ensures that the evaluation provides
normalisation with respect to data
quality.
</bodyText>
<listItem confidence="0.946526166666667">
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. Standard Runs We expect that the partici-
pants to use only the data (parallel names)
provided by the Shared Task for translitera-
tion task for a “standard” run to ensure a fair
evaluation. One such run (using only the data
provided by the shared task) is mandatory for
all participants for a given language pair that
they participate in.
5. Non-standard Runs If more data (either par-
allel names data or monolingual data) were
used, then all such runs using extra data must
be marked as “non-standard”. For such “non-
standard” runs, it is required to disclose the
size and characteristics of the data used in the
system paper.
6. A participant may submit a maximum of 8
runs for a given language pair (including the
mandatory 1 “standard” run marked as “pri-
mary submission”).
</listItem>
<sectionHeader confidence="0.987381" genericHeader="method">
6 Paper Format
</sectionHeader>
<bodyText confidence="0.999855090909091">
Paper submissions to NEWS 2010 should follow
the ACL 2010 paper submission policy, includ-
ing paper format, blind review policy and title and
author format convention. Full papers (research
paper) are in two-column format without exceed-
ing eight (8) pages of content plus one extra page
for references and short papers (task paper) are
also in two-column format without exceeding four
(4) pages, including references. Submission must
conform to the official ACL 2010 style guidelines.
For details, please refer to the ACL 2010 website2.
</bodyText>
<sectionHeader confidence="0.994279" genericHeader="method">
7 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.999895545454546">
We plan to measure the quality of the translitera-
tion task using the following 4 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
The following notation is further assumed:
</bodyText>
<footnote confidence="0.95">
2http://acl2010.org/authors.html
</footnote>
<page confidence="0.998086">
15
</page>
<bodyText confidence="0.9466215">
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni &gt; 1)
ri,j : j-th reference transliteration for i-th
name in the test set
</bodyText>
<listItem confidence="0.990852047619048">
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 &lt; k &lt; 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
</listItem>
<bodyText confidence="0.683012">
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
</bodyText>
<table confidence="0.980907666666667">
are calculated as = =
Ri LCS(ci,1, ri,m)
Pi
F
z
=
|ri,m|
LCS(ci,1, ri,m)
|ci,1|
Ri x Pi
2
Ri + Pi
</table>
<listItem confidence="0.985601">
• The length is computed in distinct Unicode
characters.
• No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses’ etc.)
3. Mean Reciprocal Rank (MRR) Measures
</listItem>
<bodyText confidence="0.967690166666667">
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
</bodyText>
<equation confidence="0.9840034">
minj if Iri,j,ci,k : ri,j = ci,ki
{ 0 otherwise I
(7)
1
MRR = N
</equation>
<bodyText confidence="0.991973">
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let’s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
</bodyText>
<equation confidence="0.662781833333333">
1 XN 1 if I ri,j : ri,j = ci,1
ACC = N { �
0 otherwise
i=1
(1)
RRi =
XN RRi (8)
i=1
1 XN 1 Xni !num(i, k) (9)
LCS(c, r) = 2 (|c |+ |r |− ED(c, r)) (2) 1 ni k=1
MAPref = N
i
</equation>
<bodyText confidence="0.999301714285714">
where ED is the edit distance and |x |is the length
of x. For example, the longest common subse-
quence between “abcd” and “afcde” is “acd” and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
</bodyText>
<equation confidence="0.4333275">
ri,m = arg min (ED(ci,1, ri,j)) (3)
j
</equation>
<bodyText confidence="0.219614">
then Recall, Precision and F-score for i-th word
</bodyText>
<sectionHeader confidence="0.979515" genericHeader="method">
8 Contact Us
</sectionHeader>
<bodyText confidence="0.964665">
If you have any questions about this share task and
the database, please email to
</bodyText>
<figure confidence="0.34703675">
Dr. Haizhou Li
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
</figure>
<footnote confidence="0.842291">
#08-05 South Tower, Connexis
Singapore 138632
hli@i2r.a-star.edu.sg
</footnote>
<page confidence="0.994919">
16
</page>
<sectionHeader confidence="0.441758" genericHeader="conclusions">
Dr. A. Kumaran
</sectionHeader>
<reference confidence="0.7568063">
Microsoft Research India
Scientia, 196/36, Sadashivnagar 2nd Main
Road
Bangalore 560080 INDIA
a.kumaran@microsoft.com
Mr. Jack Halpern
CEO, The CJK Dictionary Institute, Inc.
Komine Building (3rd &amp; 4th floors)
34-14, 2-chome, Tohoku, Niiza-shi
Saitama 352-0001 JAPAN
</reference>
<email confidence="0.698026">
jack@cjki.org
</email>
<sectionHeader confidence="0.96547" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9976635">
[CJKI2010] CJKI. 2010. CJK Institute.
http://www.cjk.org/.
[Li et al.2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159–166, Barcelona, Spain.
[MSRI2010] MSRI. 2010. Microsoft Research India.
http://research.microsoft.com/india.
</reference>
<page confidence="0.994648">
17
</page>
<figure confidence="0.5389404">
A Training/Development Data
• File Naming Conventions:
NEWS10 train XXYY nnnn.xml
NEWS10 dev XXYY nnnn.xml
NEWS10 test XXYY nnnn.xml
</figure>
<listItem confidence="0.4770508">
– XX: Source Language
– YY: Target Language
– nnnn: size of parallel/monolingual
names (“25K”, “10000”, etc)
• File formats:
</listItem>
<bodyText confidence="0.735076">
All data will be made available in XML for-
mats (Figure 1).
</bodyText>
<listItem confidence="0.968885">
• Data Encoding Formats:
</listItem>
<bodyText confidence="0.996984666666667">
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
</bodyText>
<subsectionHeader confidence="0.7125">
B Submission of Results
</subsectionHeader>
<listItem confidence="0.810127">
• File Naming Conventions:
</listItem>
<bodyText confidence="0.997595333333333">
You can give your files any name you like.
During submission online you will need to
indicate whether this submission belongs to
a “standard” or “non-standard” run, and if it
is a “standard” run, whether it is the primary
submission.
</bodyText>
<listItem confidence="0.946969">
• File formats:
</listItem>
<bodyText confidence="0.8255845">
All data will be made available in XML for-
mats (Figure 2).
</bodyText>
<listItem confidence="0.975993">
• Data Encoding Formats:
</listItem>
<bodyText confidence="0.999571">
The results are expected to be submitted in
UTF-8 encoded files without byte-order mark
only, and in the XML format specified.
</bodyText>
<page confidence="0.990438">
18
</page>
<figure confidence="0.996274384615385">
&lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt;
&lt;TransliterationCorpus
CorpusID = &amp;quot;NEWS2010-Train-EnHi-25K&amp;quot;
SourceLang = &amp;quot;English&amp;quot;
TargetLang = &amp;quot;Hindi&amp;quot;
CorpusType = &amp;quot;Train|Dev&amp;quot;
CorpusSize = &amp;quot;25000&amp;quot;
CorpusFormat = &amp;quot;UTF8&amp;quot;&gt;
&lt;Name ID=”1”&gt;
&lt;SourceName&gt;eeeeee1&lt;/SourceName&gt;
&lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh1_1&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh1_2&lt;/TargetName&gt;
...
&lt;TargetName ID=&amp;quot;n&amp;quot;&gt;hhhhhh1_n&lt;/TargetName&gt;
&lt;/Name&gt;
&lt;Name ID=”2”&gt;
&lt;SourceName&gt;eeeeee2&lt;/SourceName&gt;
&lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh2_1&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh2_2&lt;/TargetName&gt;
...
&lt;TargetName ID=&amp;quot;m&amp;quot;&gt;hhhhhh2_m&lt;/TargetName&gt;
&lt;/Name&gt;
...
&lt;!-- rest of the names to follow --&gt;
...
&lt;/TransliterationCorpus&gt;
</figure>
<figureCaption confidence="0.998767">
Figure 1: File: NEWS2010 Train EnHi 25K.xml
</figureCaption>
<page confidence="0.887832">
19
</page>
<figure confidence="0.99630028125">
&lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt;
&lt;TransliterationTaskResults
SourceLang = &amp;quot;English&amp;quot;
TargetLang = &amp;quot;Hindi&amp;quot;
GroupID = &amp;quot;Trans University&amp;quot;
RunID = &amp;quot;1&amp;quot;
RunType = &amp;quot;Standard&amp;quot;
Comments = &amp;quot;HMM Run with params: alpha=0.8 beta=1.25&amp;quot;&gt;
&lt;Name ID=&amp;quot;1&amp;quot;&gt;
&lt;SourceName&gt;eeeeee1&lt;/SourceName&gt;
&lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh11&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh12&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;3&amp;quot;&gt;hhhhhh13&lt;/TargetName&gt;
...
&lt;TargetName ID=&amp;quot;10&amp;quot;&gt;hhhhhh110&lt;/TargetName&gt;
&lt;!-- Participants to provide their
top 10 candidate transliterations --&gt;
&lt;/Name&gt;
&lt;Name ID=&amp;quot;2&amp;quot;&gt;
&lt;SourceName&gt;eeeeee2&lt;/SourceName&gt;
&lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh21&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh22&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;3&amp;quot;&gt;hhhhhh23&lt;/TargetName&gt;
...
&lt;TargetName ID=&amp;quot;10&amp;quot;&gt;hhhhhh110&lt;/TargetName&gt;
&lt;!-- Participants to provide their
top 10 candidate transliterations --&gt;
&lt;/Name&gt;
...
&lt;!-- All names in test corpus to follow --&gt;
...
&lt;/TransliterationTaskResults&gt;
</figure>
<figureCaption confidence="0.998498">
Figure 2: Example file: NEWS2010 EnHi TUniv 01 StdRunHMMBased.xml
</figureCaption>
<page confidence="0.961858">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999381">of NEWS 2010 Shared Task on Transliteration</title>
<author confidence="0.645451">A Min Vladimir</author>
<affiliation confidence="0.495422">for Infocomm Research, A*STAR, Singapore Systems Research, Microsoft Research</affiliation>
<email confidence="0.997033">A.Kumaran@microsoft.com</email>
<abstract confidence="0.996360475">Transliteration is defined as phonetic translation of names across languages. Transliteration of Named Entities (NEs) is necessary in many applications, such as machine translation, corpus alignment, cross-language IR, information extraction and automatic lexicon acquisition. All such systems call for high-performance transliteration, which is the focus of shared task in the NEWS 2010 workshop. The objective of the shared task is to promote machine transliteration research by providing a common benchmarking platform for the community to evaluate the state-of-the-art technologies. 1 Task Description The task is to develop machine transliteration system in one or more of the specified language pairs being considered for the task. Each language pair consists of a source and a target language. The training and development data sets released for each language pair are to be used for developing a transliteration system in whatever way that the participants find appropriate. At the evaluation time, a test set of source names only would be released, on which the participants are expected to produce a ranked list of transliteration candiin another language (i.e. transliterations), and this will be evaluated using common metrics. For every language pair the participants must submit at least one run that uses only the data provided by the NEWS workshop organisers in a given language pair (designated as “standard” run, primary submission). Users may submit more “stanrard” runs. They may also submit several “non-standard” runs for each language pair that use other data than those provided by the NEWS 2010 workshop; such runs would be evaluated and reported separately.</abstract>
<note confidence="0.975420692307692">2 Important Dates paper submission deadline May 2010 Shared task Registration opens 1 Feb 2010 Registration closes 13 Mar 2010 Training/Development data release 19 Feb 2010 Test data release 13 Mar 2010 Results Submission Due 20 Mar 2010 Results Announcement 27 Mar 2010 Task (short) Papers Due 5 Apr 2010 For all submissions Acceptance Notification 6 May 2010 Workshop Date 16 Jul 2010</note>
<abstract confidence="0.989447257425742">3 Participation 1. Registration (1 Feb 2010) (a) NEWS Shared Task opens for registration. (b) Prospective participants are to register to the NEWS Workshop homepage. 2. Training &amp; Development Data (19 Feb 2010) (a) Registered participants are to obtain training and development data from the Shared Task organiser and/or the designated copyright owners of databases. (b) All registered participants are required to participate in the evaluation of at least one language pair, submit the results and a short paper and attend the workshop at ACL 2010. 3. Evaluation Script (19 Feb 2010) 12 of the 2010 Named Entities Workshop, ACL pages Sweden, 16 July 2010. Association for Computational Linguistics (a) A sample test set and expected user output format are to be released. (b) An evaluation script, which runs on the above two, is to be released. (c) The participants must make sure that their output is produced in a way that the evaluation script may run and produce the expected output. (d) The same script (with held out test data and the user outputs) would be used for final evaluation. 4. Test data (13 Mar 2010) (a) The test data would be released on 13 March 2010, and the participants have a maximum of 7 days to submit their results in the expected format. (b) One “standard” run must be submitted from every group on a given language pair. Additional “standard” runs may be submitted, up to 4 “standard” runs in total. However, the participants must indicate one of the submitted “standard” runs as the “primary submission”. The primary submission will be used for the performance summary. In addition to the “standard” runs, more “non-standard” runs may be submitted. In total, maximum 8 runs (up to 4 “standard” runs plus up to 4 “non-standard” runs) can be submitted from each group on a registered language pair. The definition of “standard” and “non-standard” runs is in Section 5. (c) Any runs that are “non-standard” must be tagged as such. (d) The test set is a list of names in source language only. Every group will produce and submit a ranked list of transliteration candidates in another language for each given name in the test set. Please note that this shared task is a “transliteration generation” task, i.e., given a name in a source language one is supposed to generate one or more transliterations in a target language. It is not the task of “transliteration discovery”, i.e., given a name in the source language and a set of names in the target language evaluate how to find the appropriate names from the target set that are transliterations of the given source name. 5. Results (27 Mar 2010) (a) On 27 March 2010, the evaluation results would be announced and will be made available on the Workshop website. (b) Note that only the scores (in respective metrics) of the participating systems on each language pairs would be published, and no explicit ranking of the participating systems would be published. (c) Note that this is a shared evaluation task and not a competition; the results are meant to be used to evaluate systems on common data set with common metrics, and not to rank the participating systems. While the participants can cite the performance of their systems (scores on metrics) from the workshop report, they should not use any ranking information in their publications. (d) Furthermore, all participants should agree not to reveal identities of other participants in any of their publications unless you get permission from the other respective participants. By default, all participants remain anonymous in published results, unless they indicate otherwise at the time of uploading their results. Note that the results of all systems will be published, but the identities of those participants that choose not to disclose their identity to other participants will be masked. As a result, in this case, your organisation name will still appear in the web site as one of participants, but it will not be linked explicitly to your results. 6. Short Papers on Task (5 Apr 2010) (a) Each submitting site is required to submit a 4-page system paper (short paper) for its submissions, including their approach, data used and the results on eitest set or development set or by fold cross validation on training set. (b) The review of the system papers will be done to improve paper quality and readability and make sure the authors’ ideas 13 and methods can be understood by the workshop participants. We are aiming at accepting all system papers, and selected ones will be presented orally in the NEWS 2010 workshop. (c) All registered participants are required to register and attend the workshop to introduce your work. (d) All paper submission and review will be managed electronically through https:// www.softconf.com/acl2010/NEWS. 4 Language Pairs The tasks are to transliterate personal names or place names from a source to a target language as summarised in Table 1. NEWS 2010 Shared Task offers 12 evaluation subtasks, among them ChEn and ThEn are the back-transliteration of EnCh and EnTh tasks respectively. NEWS 2010 releases training, development and testing data for each of the language pairs. NEWS 2010 continues some language pairs that were evaluated in NEWS 2009. In such cases, the training and development data in the release of NEWS 2010 may overlap with those in NEWS 2009. However, the test data in NEWS 2010 are entirely new. The names given in the training sets for Chinese, Japanese, Korean and Thai languages are Western names and their respective transliterathe Japanese Name (in English) Kanji data set consists only of native Japanese names; the Arabic data set consists only of native Arabic names. The Indic data set (Hindi, Tamil, Kannada, Bangla) consists of a mix of Indian and Western names. Examples of transliteration: Katakana Hangul name in English Kanji name in English 5 Standard Databases Training Data (Parallel) Paired names between source and target languages; size 5K – 32K. Training Data is used for training a basic transliteration system. Development Data (Parallel) Paired names between source and target languages; size 2K – 6K. Development Data is in addition to the Training data, which is used for system fine-tuning of parameters in case of need. Participants are allowed to use it as part of training data. Testing Data Source names only; size 2K – 3K. This is a held-out set, which would be used for evaluating the quality of the transliterations. 1. Participants will need to obtain licenses from the respective copyright owners and/or agree to the terms and conditions of use that are given on the downloading website (Li et al., 2004; MSRI, 2010; CJKI, 2010). NEWS 2010 will provide the contact details of each individual database. The data would be provided in Unicode UTF-8 encoding, in XML format; the results are expected to be submitted in UTF-8 encoding in XML format. The XML formats details are available in Appendix A. 2. The data are provided in 3 sets as described above. 3. Name pairs are distributed as-is, as provided by the respective creators. (a) While the databases are mostly manually checked, there may be still inconsistency (that is, non-standard usage, region-specific usage, errors, etc.) or incompleteness (that is, not all right variations may be covered). (b) The participants may use any method to further clean up the data provided. 14</abstract>
<title confidence="0.959927">Name origin Source script Target script Data Owner Data Size Task ID</title>
<author confidence="0.974638">Train Dev Test</author>
<affiliation confidence="0.683924">Western English Chinese Institute for Infocomm Research 32K 6K 2K EnCh Western Chinese English Institute for Infocomm Research 25K 5K 2K ChEn</affiliation>
<address confidence="0.430569">Western English Korean Hangul CJK Institute 5K 2K 2K EnKo Western English Japanese Katakana CJK Institute 23K 3K 3K EnJa</address>
<note confidence="0.877735625">Japanese English Japanese Kanji CJK Institute 7K 3K 3K JnJk Arabic Arabic English CJK Institute 25K 2.5K 2.5K ArAe Mixed English Hindi Microsoft Research India 10K 2K 2K EnHi Mixed English Tamil Microsoft Research India 8K 2K 2K EnTa Mixed English Kannada Microsoft Research India 8K 2K 2K EnKa Mixed English Bangla Microsoft Research India 10K 2K 2K EnBa Western English Thai NECTEC 26K 2K 2K EnTh Western Thai English NECTEC 24K 2K 2K ThEn</note>
<abstract confidence="0.993643503875969">Table 1: Source and target languages for the shared task on transliteration. i. If they are cleaned up manually, we appeal that such data be provided back to the organisers for redistribution to all the participating groups in that language pair; such sharing benefits all participants, and further ensures that the evaluation provides normalisation with respect to data quality. ii. If automatic cleanup were used, such cleanup would be considered a part of the system fielded, and hence not required to be shared with all participants. Standard Runs expect that the participants to use only the data (parallel names) provided by the Shared Task for transliteration task for a “standard” run to ensure a fair evaluation. One such run (using only the data provided by the shared task) is mandatory for all participants for a given language pair that they participate in. Non-standard Runs more data (either parallel names data or monolingual data) were used, then all such runs using extra data must be marked as “non-standard”. For such “nonstandard” runs, it is required to disclose the size and characteristics of the data used in the system paper. 6. A participant may submit a maximum of 8 runs for a given language pair (including the mandatory 1 “standard” run marked as “primary submission”). 6 Paper Format Paper submissions to NEWS 2010 should follow the ACL 2010 paper submission policy, including paper format, blind review policy and title and author format convention. Full papers (research paper) are in two-column format without exceeding eight (8) pages of content plus one extra page for references and short papers (task paper) are also in two-column format without exceeding four (4) pages, including references. Submission must conform to the official ACL 2010 style guidelines. details, please refer to the ACL 2010 7 Evaluation Metrics We plan to measure the quality of the transliteration task using the following 4 metrics. We accept up to 10 output candidates in a ranked list for each input entry. Since a given source name may have multiple correct target transliterations, all these alternatives are treated equally in the evaluation. That is, any of these alternatives are considered as a correct transliteration, and the first correct transliteration in the ranked list is accepted as a correct hit. The following notation is further assumed: 15 Total number of names (source words) in the test set Number of reference transliterations name in the test set reference transliteration for name in the test set candidate transliteration (system for name in the test set k &lt; Number of candidate transliterations produced by a transliteration system Word Accuracy in Top-1 (ACC) known as Word Error Rate, it measures correctness of the first transliteration candidate in the candidate list produced by a transliteration system. 1 that all top candidates are correct transliterations i.e. they match one of the refand 0 that none of the top candidates are correct. Fuzziness in Top-1 (Mean F-score) mean F-score measures how different, on average, the top transliteration candidate is from its closest reference. F-score for each source word is a function of Precision and Recall and equals 1 when the top candidate matches one of the references, and 0 when there are no common characters between the candidate and any of the references. Precision and Recall are calculated based on the length of the Longest Common Subsequence between a candidate and a reference: are calculated as F = = z = • The length is computed in distinct Unicode characters. • No distinction is made on different character types of a language (e.g., vowel vs. consovs. combining Mean Reciprocal Rank (MRR) MRR any right answer by system, from among the candidates. tells approximately the average rank of the correct transliteration. MRR closer to 1 implies that the correct answer is mostly produced close to the top of the n-best lists. (7) 1 Measures tightly the precision in the candidates for source name, for which reference transliterations are available. If all of the references are produced, then the MAP is 1. Let’s denote the number of correct candidates for source word in list as then given by 1 { 1 1 1 = 2 − i the edit distance and the length For example, the longest common subsequence between “abcd” and “afcde” is “acd” and its length is 3. The best matching reference, that is, the reference for which the edit distance has the minimum, is taken for calculation. If the best matching reference is given by arg min j then Recall, Precision and F-score for i-th word 8 Contact Us If you have any questions about this share task and</abstract>
<note confidence="0.69038325">the database, please email to Dr. Haizhou Li for Infocomm Research A*STAR 1 Fusionopolis Way Singapore 138632 hli@i2r.a-star.edu.sg 16</note>
<author confidence="0.950285">A Kumaran</author>
<affiliation confidence="0.995271">Microsoft Research India</affiliation>
<address confidence="0.927751666666667">Scientia, 196/36, Sadashivnagar 2nd Main Road Bangalore 560080 INDIA</address>
<email confidence="0.999481">a.kumaran@microsoft.com</email>
<author confidence="0.99989">Jack Halpern</author>
<affiliation confidence="0.990612">CEO, The CJK Dictionary Institute, Inc.</affiliation>
<address confidence="0.943183666666667">Komine Building (3rd &amp; 4th floors) 34-14, 2-chome, Tohoku, Niiza-shi Saitama 352-0001 JAPAN</address>
<email confidence="0.987982">jack@cjki.org</email>
<note confidence="0.770330266666667">References [CJKI2010] CJKI. 2010. CJK Institute. http://www.cjk.org/. [Li et al.2004] Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine In 42nd ACL Annual pages 159–166, Barcelona, Spain. [MSRI2010] MSRI. 2010. Microsoft Research India. http://research.microsoft.com/india. 17 A Training/Development Data • File Naming Conventions: NEWS10 train XXYY nnnn.xml NEWS10 dev XXYY nnnn.xml NEWS10 test XXYY nnnn.xml</note>
<title confidence="0.687705">Source Language Target Language</title>
<abstract confidence="0.941308076923077">size of parallel/monolingual names (“25K”, “10000”, etc) • File formats: All data will be made available in XML formats (Figure 1). • Data Encoding Formats: The data will be in Unicode UTF-8 encoding files without byte-order mark, and in the XML format specified. B Submission of Results • File Naming Conventions: You can give your files any name you like. During submission online you will need to indicate whether this submission belongs to a “standard” or “non-standard” run, and if it is a “standard” run, whether it is the primary submission. • File formats: All data will be made available in XML formats (Figure 2). • Data Encoding Formats: The results are expected to be submitted in UTF-8 encoded files without byte-order mark only, and in the XML format specified. 18 &lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt;</abstract>
<title confidence="0.5651298">lt;TransliterationCorpus CorpusID = &amp;quot;NEWS2010-Train-EnHi-25K&amp;quot; SourceLang = &amp;quot;English&amp;quot; TargetLang = &amp;quot;Hindi&amp;quot; CorpusType = &amp;quot;Train|Dev&amp;quot;</title>
<note confidence="0.8639006">CorpusSize = &amp;quot;25000&amp;quot; CorpusFormat = &amp;quot;UTF8&amp;quot;&gt; &lt;SourceName&gt;eeeeee1&lt;/SourceName&gt; &lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh1_1&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh1_2&lt;/TargetName&gt; ... &lt;TargetName ID=&amp;quot;n&amp;quot;&gt;hhhhhh1_n&lt;/TargetName&gt; &lt;/Name&gt; &lt;SourceName&gt;eeeeee2&lt;/SourceName&gt; &lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh2_1&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh2_2&lt;/TargetName&gt; ... &lt;TargetName ID=&amp;quot;m&amp;quot;&gt;hhhhhh2_m&lt;/TargetName&gt; &lt;/Name&gt; ... &lt;!-rest of the names to follow --&gt; ... &lt;/TransliterationCorpus&gt; Figure 1: File: NEWS2010 Train EnHi 25K.xml 19 &lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt; &lt;TransliterationTaskResults SourceLang = &amp;quot;English&amp;quot; TargetLang = &amp;quot;Hindi&amp;quot; GroupID = &amp;quot;Trans University&amp;quot; RunID = &amp;quot;1&amp;quot; RunType = &amp;quot;Standard&amp;quot; Comments = &amp;quot;HMM Run with params: alpha=0.8 beta=1.25&amp;quot;&gt; &lt;Name ID=&amp;quot;1&amp;quot;&gt; &lt;SourceName&gt;eeeeee1&lt;/SourceName&gt; &lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh11&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh12&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;3&amp;quot;&gt;hhhhhh13&lt;/TargetName&gt; ... &lt;TargetName ID=&amp;quot;10&amp;quot;&gt;hhhhhh110&lt;/TargetName&gt; &lt;!-- Participants to provide their top 10 candidate transliterations --&gt; &lt;/Name&gt; &lt;Name ID=&amp;quot;2&amp;quot;&gt; &lt;SourceName&gt;eeeeee2&lt;/SourceName&gt; &lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh21&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh22&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;3&amp;quot;&gt;hhhhhh23&lt;/TargetName&gt; ... &lt;TargetName ID=&amp;quot;10&amp;quot;&gt;hhhhhh110&lt;/TargetName&gt;</note>
<abstract confidence="0.841218166666667">lt;!-- Participants to provide their top 10 candidate transliterations --&gt; &lt;/Name&gt; ... &lt;!-- All names in test corpus to follow --&gt; ...</abstract>
<note confidence="0.491739666666667">lt;/TransliterationTaskResults&gt; Figure 2: Example file: NEWS2010 EnHi TUniv 01 StdRunHMMBased.xml 20</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>0001</date>
<booktitle>Scientia, 196/36, Sadashivnagar 2nd Main Road Bangalore 560080 INDIA a.kumaran@microsoft.com Mr. Jack Halpern CEO, The CJK Dictionary Institute, Inc. Komine Building (3rd &amp; 4th floors) 34-14, 2-chome,</booktitle>
<institution>Microsoft Research India</institution>
<location>Tohoku, Niiza-shi Saitama</location>
<marker>0001</marker>
<rawString> Microsoft Research India Scientia, 196/36, Sadashivnagar 2nd Main Road Bangalore 560080 INDIA a.kumaran@microsoft.com Mr. Jack Halpern CEO, The CJK Dictionary Institute, Inc. Komine Building (3rd &amp; 4th floors) 34-14, 2-chome, Tohoku, Niiza-shi Saitama 352-0001 JAPAN</rawString>
</citation>
<citation valid="true">
<authors>
<author>CJKI</author>
</authors>
<date>2010</date>
<note>CJK Institute. http://www.cjk.org/.</note>
<marker>[CJKI2010]</marker>
<rawString>CJKI. 2010. CJK Institute. http://www.cjk.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proc. 42nd ACL Annual Meeting,</booktitle>
<pages>159--166</pages>
<location>Barcelona,</location>
<marker>[Li et al.2004]</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. In Proc. 42nd ACL Annual Meeting, pages 159–166, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MSRI</author>
</authors>
<date>2010</date>
<institution>Microsoft Research India.</institution>
<note>http://research.microsoft.com/india.</note>
<marker>[MSRI2010]</marker>
<rawString>MSRI. 2010. Microsoft Research India. http://research.microsoft.com/india.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>