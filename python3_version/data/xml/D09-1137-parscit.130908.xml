<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000474">
<title confidence="0.991012">
Human-competitive tagging using automatic keyphrase extraction
</title>
<author confidence="0.998581">
Olena Medelyan, Eibe Frank, Ian H. Witten
</author>
<affiliation confidence="0.997187">
Computer Science Department
University of Waikato
</affiliation>
<email confidence="0.998259">
{olena,eibe,ihw}@cs.waikato.ac.nz
</email>
<sectionHeader confidence="0.993881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999737">
This paper connects two research areas: auto-
matic tagging on the web and statistical key-
phrase extraction. First, we analyze the quality
of tags in a collaboratively created folksonomy
using traditional evaluation techniques. Next,
we demonstrate how documents can be tagged
automatically with a state-of-the-art keyphrase
extraction algorithm, and further improve per-
formance in this new domain using a new al-
gorithm, “Maui”, that utilizes semantic infor-
mation extracted from Wikipedia. Maui out-
performs existing approaches and extracts tags
that are competitive with those assigned by the
best performing human taggers.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999874590909091">
Tagging is the process of labeling web resources
based on their content. Each label, or tag, corre-
sponds to a topic in a given document. Unlike
metadata assigned by authors, or by professional
indexers in libraries, tags are assigned by end-
users for organizing and sharing information that
is of interest to them. The organic system of tags
assigned by all users of a given web platform is
called a folksonomy.
In contrast to traditional taxonomies painstak-
ingly constructed by experts, a user can add any
tags to a folksonomy. This leads to the greatest
downside of tagging, inconsistency, which origi-
nates in the synonymy and polysemy of human
language, as well as in the varying degrees of
specificity used by taggers (Golder and Huber-
man, 2006). In traditional libraries, consistency is
the primary evaluation criterion of indexing
(Rolling, 1981). Much work has been done on
describing the statistical properties of folksono-
mies, such as tag distribution and co-occurrences
(Halpin et al., 2007; Sigurbjšrnsson et al., 2008;
Sood et al., 2007), but to our knowledge there
has been none on assessing the actual quality of
tags. How well do human taggers perform? How
consistent are they with each other?
One potential solution to inconsistency in
folksonomies is to use suggestion tools that
automatically compute tags for new documents
(e.g. Mishne, 2006; Sood et al., 2007; Heymann
et al., 2008). Interestingly, the blooming research
on automatic tagging has so far not been con-
nected to work on keyphrase extraction (e.g.
Frank et al., 1999; Turney, 2003; Hulth, 2004),
which can be used as a tool for the same task
(note: we use tag and keyphrase as synonyms).
Instead of simple heuristics based on term fre-
quencies and co-occurrence of tags, keyphrase
extraction methods apply machine learning to
determine typical distributions of properties
common to manually assigned phrases, and can
include analysis of semantic relations between
candidate tags (Turney, 2003). How well do
state-of-the-art keyphrase extraction systems per-
form compared to simple tagging techniques?
How consistent are they with human taggers?
These are questions we address in this paper.
Until now, keyphrase extraction methods have
primarily been evaluated using a single set of
keyphrases for each document, thereby largely
ignoring the subjective nature of the task.
Collaboratively tagged documents, on the other
hand, offer multiple tag assignments by inde-
pendent users, a unique basis for evaluation that
we capitalize upon in this paper.
The experiments reported in this paper fill
these gaps in the research on automatic tagging
and keyphrase extraction. First, we analyze tag-
ging consistency on the CiteULike.org platform
for organizing academic citations. Methods tradi-
tionally used for the evaluation of professional
indexing will provide insight into the quality of
this folksonomy. Next, we extract a high quality
corpus from CiteULike, containing documents
that have been tagged consistently by the best
human taggers.
</bodyText>
<page confidence="0.941217">
1318
</page>
<note confidence="0.9980645">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1318–1327,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.999744">
Figure 1. Quality control of CiteULike data
</figureCaption>
<bodyText confidence="0.999972045454545">
Following that, our goal is to build a system
that matches the performance of these taggers.
We first apply an existing approach proposed by
Brooks and Montanez (2006) and compare it to
the keyphrase extraction algorithm Kea (Frank et
al., 1999). Next we create a new algorithm,
called Maui, that enhances Kea’s successful ma-
chine learning framework with semantic knowl-
edge retrieved from Wikipedia, new features, and
a new classification model. We evaluate Maui
using tag sets assigned to the same documents by
several users and show that it is as consistent
with CiteULike users as they are with each other.
Most of the computation required for auto-
matic tagging with this method can be performed
offline. In practice, it can be used as a tag sug-
gestion tool that provides users with tags describ-
ing the main topics of newly added documents,
which can then be corrected or enhanced by per-
sonal tags if required. This will improve consis-
tency in the folksonomy without compromising
its flexibility.
</bodyText>
<sectionHeader confidence="0.996731" genericHeader="method">
2 Collaboratively-tagged Data
</sectionHeader>
<bodyText confidence="0.999945285714286">
CiteULike.org is a bookmarking service that re-
sembles the popular del.icio.us, but concentrates
on scholarly papers. Rather than replicating the
full text of tagged papers it simply points to them
on the web (e.g. PubMed, CiteSeer, ScienceDi-
rect, Amazon) or in journals (e.g. HighWire, Na-
ture). This avoids violating copyright but means
that the full text of articles is not necessarily
available. When entering new resources, users
are encouraged to assign tags describing their
content or reflecting their own grouping of the
information. However, the system does not sug-
gest tags. Moreover, users do not see other users’
tags and are thus not biased in their tag choices.
</bodyText>
<subsectionHeader confidence="0.998616">
2.1 Extracting a high quality tagged corpus
</subsectionHeader>
<bodyText confidence="0.999618962962963">
The CiteULike data set is freely available and
contains information about which documents
were tagged with what tags by which users (al-
though identities are not provided).
CiteULike’s 22,300 users have tagged 713,600
documents with 2.4M “tag assignments”— sin-
gle applications of a tag by a user to a document.
The two most popular tags, bibtex-import and
no-tag, indicate an information source and a
missing tag respectively. Most of the remainder
describe particular concepts relevant to the
documents. We exclude non-content tags from
our experiments, e.g. personal tags like to-read
or todo. Note that spam entries have been elimi-
nated from the data set.
Because CiteULike taggers are not profes-
sional indexers, high quality of the assigned top-
ics cannot be guaranteed. In fact, manual as-
sessment of users’ tags by human evaluators
shows precision of 59% (Mishne, 2006) and 49%
(Sood et al., 2006). However, why is the opinion
of human evaluators valued more than the opin-
ion of taggers? We propose an alternative way of
determining ground truth using an automatic ap-
proach to determine reliable tags: We concen-
trate on a subset of CiteULike containing docu-
ments that have been indexed with at least three
tags on which at least two users have agreed.
In order to be able to measure the tagging con-
sistency between the users, and then compare it
to the algorithm’s consistency, we need taggers
who have tagged documents that some others
had tagged. We say that two users are “co-
taggers” if they have both tagged at least one
common document. As well as restricting the
document set, we only include taggers who have
at least two co-taggers.
Figure 1 shows the proportions of CiteULike
documents that are discarded in order to produce
our high quality data set. The final set contains
only 2,100 documents (0.3% of the original).
Unfortunately, many of these are unavailable for
download—for example, books at Amazon.com
and ArXiv.org references cannot be crawled. We
further restrict attention to two sources: High-
Wire and Nature, both of which provide easily-
accessible PDFs of the full text.
The result is a set of 180 documents indexed
by 332 taggers. A total of 4,638 tags were as-
signed by all taggers to documents in this set;
however, the number of tags on which at least
two users agreed is significantly smaller, namely
946. Still, this results in accurate tag sets that
contain an average of five tags per document.
</bodyText>
<page confidence="0.990088">
1319
</page>
<table confidence="0.811287710526316">
tagger co-taggers documents consistency
1 1 5 71.4
2 1 5 71.4
3 6 5 57.9
4 6 6 51.0
5 11 12 50.4
6 2 5 50.1
7 4 6 48.3
8 8 8 47.1
9 13 16 45.4
10 12 8 44.4
11 7 6 43.5
12 7 6 41.7
13 8 5 40.9
14 7 6 39.7
15 9 13 38.8
16 4 5 38.4
17 12 9 37.3
18 4 14 36.1
19 9 8 35.9
20 10 11 33.7
21 7 6 33.1
22 6 5 33.0
23 7 10 32.1
24 11 16 31.7
25 8 13 30.6
26 6 8 30.6
27 9 6 29.8
28 10 12 29.0
29 8 6 28.8
30 9 10 27.9
31 10 8 26.7
32 8 7 26.3
33 10 5 25.6
34 8 7 21.0
35 9 9 18.3
36 3 6 7.9
average 7.5 8.1 37.7
</table>
<tableCaption confidence="0.957358">
Table 1. Consistency of the most prolific and
most consistent taggers
</tableCaption>
<bodyText confidence="0.999900458333333">
Note that traditionally much smaller data sets are
used to assess consistency of human indexers,
because such sets need to be created specifically
for the experiment. Collaborative tagging plat-
forms like CiteULike can be mined for large col-
lections of this kind in natural settings.
Most documents in the extracted set relate to
the area of bioinformatics. To give an example, a
document entitled Initial sequencing and com-
parative analysis of the mouse genome was
tagged by eight users with a total of 22 tags. Four
of them agreed on the tag mouse, but one used
the broader term rodents. Three agreed on the tag
genome, but one added genome paper, and an-
other used the more specific comparative genom-
ics. There are also cases when tags are written
together, e.g. genomepaper, or with a prefix key
genome, or in a different grammatical form: se-
quence vs. sequencing. This example shows that
many inconsistencies in tags are not caused by
personalized tag choices as Chirita et al. (2007)
suggest, but rather stem from the lack of guide-
lines and uniform tag suggestions that a book-
marking service could provide.
</bodyText>
<subsectionHeader confidence="0.999854">
2.2 Measuring tagging consistency
</subsectionHeader>
<bodyText confidence="0.997601852941176">
Traditional indexers aim for consistency, on the
basis that this will enhance document retrieval
(Leonard, 1975). Consistency is measured using
experiments in which several people index the
same documents—usually a small set of a few
dozen documents. It is computed for pairs of in-
dexers, by formulae such as Rolling’s (1981):
,
where C is the number of tags (index terms) in-
dexers I1 and I2 have in common and A and B is
the size of their tag sets respectively.
In our experiments, before computing the
number of terms in common, we stem each tag
with the Porter (1980) stemmer. For example, the
overlap C between the tag sets {complex systems,
network, small world} and {theoretical, small
world, networks, dynamics} consist of the two
tags {network, small world}, and the consistency
is 2×2/(3+4) = 0.57.
To compute the overall consistency of a par-
ticular indexer, this figure is averaged over all
documents and co-indexers. There were no cases
where the same user reassigned tags to the same
articles, so computing intra-tagger consistency,
although interesting, was not impossible.
To our knowledge, traditional indexing consis-
tency metrics have not yet been applied to col-
laboratively tagged data. However, experiments
on determining tagging quality do follow the
same idea. For example, Xu et al. (2006) define
an authority metric that assigns high scores to
those users who match other users’ choices on
the same documents, in order to eliminate
spammers.
</bodyText>
<subsectionHeader confidence="0.999905">
2.3 Consistency of CiteULike taggers
</subsectionHeader>
<bodyText confidence="0.999980272727273">
In the collection of 180 documents tagged by 332
users described in Section 3.1, each tagger has 18
co-taggers on average, ranging from 2 to 129,
and has indexed 1 to 25 documents. For each
user we compute the consistency with all other
users who tagged the same document. Consis-
tency is then averaged across documents. We
found that the distribution of per-user consis-
tency resembles a power law with a few users
achieving high consistency values and a long tail
of inconsistent taggers. The maximum consis-
</bodyText>
<page confidence="0.941428">
1320
</page>
<bodyText confidence="0.9999695">
tency in this group is 92.3% and the average is
18.5%. The average consistency of the most pro-
lific 70 indexers—those who have indexed at
least five documents—is in the same range,
namely 18.4%. The consistency of traditional
approaches to free indexing is reported to be be-
tween 4% and 67%, with an average of 27% de-
pending on what aids are used (Leininger, 2000).
It is instructive to consider the group of best
taggers. We define these as the ones who (a) ex-
hibit greater than average consistency with all
others, and (b) are sufficiently prolific, i.e. have
tagged at least five documents. There are 36 such
taggers; Table 1 lists their consistency within this
group. The average consistency they achieve as a
group is 37.7%, which is the similar to the aver-
age consistency of professionals (Leininger,
2000).
The above consistency analysis provides in-
sight into the tagging quality of the best
CiteULike users, based on HighWire and Nature
articles. For the purposes of this paper, it shows
how the tagging community can be restricted to a
best-performing group of taggers by measuring
their consistency. This is helpful for testing the
performance of automatic tagging (Section 4.4).
</bodyText>
<sectionHeader confidence="0.9486" genericHeader="method">
3 Automatic tagging with Maui
</sectionHeader>
<bodyText confidence="0.999894291666667">
Maui is a general algorithm for automatic topical
indexing based on the Kea system (Frank et al.,
1999).1 It works in two stages: candidate selec-
tion and machine learning based filtering. In this
paper, we apply it to automatic tagging. In the
candidate selection stage, Maui first determines
textual sequences defined by orthographic
boundaries and splits these sequences into to-
kens. Then all n-grams up to a maximum length
of 3 words that do not begin or end with a stop-
word are extracted as candidate tags. To reduce
the number of candidates, all those that appear
only once are discarded. This speeds up the train-
ing and the extraction process without impacting
the results. In the filtering stage several features
are computed for each candidate, which are then
input to a machine learning model to obtain the
probability that the candidate is indeed a tag.
Maui’s architecture resembles that of many
other supervised keyphrase extraction systems
(Turney, 2000; Hulth 2004; Medelyan et al.,
2008). However, this architecture has not previ-
ously been applied to the task of automatic tag-
ging.
</bodyText>
<footnote confidence="0.990588">
1 Maui is open-source and available for download
at http://maui-indexer.googlecode.com
</footnote>
<subsectionHeader confidence="0.98937">
3.1 Features indicating significance
</subsectionHeader>
<bodyText confidence="0.9999558">
We now describe the features used in the classi-
fication model to determine whether a phrase is
likely to be a tag. We begin with three baseline
features used in Kea (Frank et al., 1999), and
extend the set with three features that have been
found useful in previous work. We also add three
new features that have not been evaluated before:
spread, semantic relatedness and inverse
Wikipedia linkage. All Wikipedia-based features
are computed using the WikipediaMiner toolkit.2
</bodyText>
<listItem confidence="0.997839523809524">
1. TF×IDF combines the frequency of a
phrase in a particular document with its inverse
occurrence frequency in general use (Salton and
McGill, 1983). This score is high for rare phrases
that appear frequently in a document and there-
fore are more likely to be significant.
2. Position of the first occurrence is com-
puted as the relative distance of the first occur-
rence of the candidate tag from the beginning of
the document. Candidates with very high or very
low values are likely to be tags, because they
appear either in the opening document parts such
as title, abstract, table of contents, and introduc-
tion, or in the document’s final sections such as
conclusion and reference lists.
3. Keyphraseness quantifies how often a can-
didate phrase appears as a tag in the training cor-
pus. Automatic tagging approaches utilize the
same information: Mishne (2006) and Sood et al.
(2006) automatically suggest tags previously as-
signed to similar documents. However, in Maui
(as in Kea) this feature is just one component of
the overall model. Thus if a candidate never ap-
pears as a keyphrase in the training corpus, it can
still be extracted if its other feature values are
significant enough.
4. Phrase length is measured in words. Gen-
erally speaking, the longer the phrase, the more
specific it is. Training captures and quantifies the
specificity preference in a given training corpus.
5. Node degree quantifies the semantic relat-
edness of a candidate tag to other candidates.
Turney (2003) computes semantic relatedness
using search engine statistics. Instead, following
Medelyan et al. (2008), we utilize Wikipedia
hyperlinks for this task. We first map each can-
didate phrase to its most common Wikipedia
page. For example, the word Jaguar appears as a
link anchor in Wikipedia 927 times. In 466 cases
it links to the article Jaguar cars, thus the com-
monness of this mapping is 0.5. In 203 cases it
links to the animal description, a commonness of
</listItem>
<footnote confidence="0.96058">
2 http://wikipedia-miner.sourceforge.net/
</footnote>
<page confidence="0.988915">
1321
</page>
<bodyText confidence="0.82662447368421">
0.22. We compute the node degree of the corre-
sponding Wikipedia article as the number of
hyperlinks that connect it to other Wikipedia
pages that have been identified for other candi-
date tags from the same document. A document
that describes a particular topic will cover many
related concepts, so high node degree—which
indicates strong connectivity to other phrases in
the same document—means that a candidate is
more likely to be significant.
6. Wikipedia-based keyphraseness is the
likelihood of a phrase being a link in the
Wikipedia corpus. It divides the number of
Wikipedia pages in which the phrase appears in
the anchor text of a link by the total number of
Wikipedia pages containing it. We multiply this
number by the phrase’s document frequency.
The new features proposed in this paper are
the following:
</bodyText>
<listItem confidence="0.975044625">
7. Spread of a phrase is the distance between
its first and last occurrences in a document. Both
values are computed relative to the length of the
document (see feature 2). High values help to
determine phrases that are mentioned both in the
beginning and at the end of a document.
8. Semantic relatedness of a phrase has al-
ready been captured as the node degree (see fea-
</listItem>
<bodyText confidence="0.921392538461539">
ture 5). However, recent research allows us to
compute semantic relatedness with better tech-
niques than mere hyperlink counts. Milne and
Witten (2008) propose an efficient Wikipedia
based approach that is nearly as accurate as hu-
man subjects at quantifying the relationship be-
tween two given concepts. Given a set of candi-
date phrases we determine the most likely
Wikipedia articles that describe them (as ex-
plained in feature 5), and then determine the total
relatedness of a given phrase to all other candi-
dates. The higher the value, the more likely is the
phrase to be a tag.
</bodyText>
<listItem confidence="0.941548857142857">
9. Inverse Wikipedia linkage is another fea-
ture that utilizes Wikipedia as a source of lan-
guage usage statistics. Here, again given the
most likely Wikipedia article for a given phrase,
we count the number of other Wikipedia articles
that link to it and normalize this value as in in-
verse document frequency:
</listItem>
<bodyText confidence="0.999868833333333">
where linksTo(AP) is the number of incoming
links to the article A representing the candidate
phrase P, and N is the total number of links in
our Wikipedia snapshot (52M). This feature
highlights those phrases that refer to concepts
commonly used to describe other concepts.
</bodyText>
<subsectionHeader confidence="0.999033">
3.2 Machine learning in Maui
</subsectionHeader>
<bodyText confidence="0.999991794871795">
In order to build the model, we use the subset of
the CiteULike collection described in Section
3.1. For each document we know a set of tags
that at least two users have agreed on. This is
used as ground truth for building the model. For
each training document, candidate phrases (i.e.
n-grams) are identified and their feature values
are calculated as described above.
Each candidate is then marked as a positive or
negative example, depending on whether users
have assigned it as a tag to the corresponding
document. The machine-learning model is con-
structed automatically from these labeled train-
ing examples using the WEKA machine learning
workbench. Kea (Frank et al., 1999) uses the
Naïve Bayes classifier, which implicitly assumes
that the features are independent of each other
given the classification. However, Kea uses only
two or three features, whereas Maui combines
nine features amongst which there are many ob-
vious relationships, e.g. first occurrence and
spread, or node degree and semantic relatedness.
Consequently, we also consider bagged decision
trees, which can model attribute interactions and
do not require parameter tuning to yield good
results. Bagging learns an ensemble of classifiers
and uses them in combination, thereby often
achieving significantly better results than the in-
dividual classifiers (Breiman, 1996). Different
trees are generated by sampling from the original
dataset with replacement. Like Naïve Bayes,
bagged trees yield probability estimates that can
be used to rank candidates.
To select tags from a new document, Maui de-
termines candidate phrases and their feature val-
ues, and then applies the classifier built during
training. This classifier determines the probabil-
ity that a candidate is a tag based on relative fre-
quencies observed from the training data.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999083">
Here we describe the data used in the experi-
ments and the results obtained, addressing the
following questions:
</bodyText>
<listItem confidence="0.959456125">
1. How does a state-of-the-art keyphrase ex-
traction method perform on collaboratively
tagged data, compared to a baseline auto-
matic tagging method?
2. What is the performance of Maui with old
and new features?
3. How consistent are Maui’s tags compared to
those assigned by human taggers?
</listItem>
<page confidence="0.960732">
1322
</page>
<table confidence="0.9997674">
P R F
1 Top words based on TFxIDF 16.8 17.3 17.0
2 Top phrases based on TFxIDF 14.4 16.0 15.2
3 Kea (TFxIDF, 1st occur) 20.4 22.3 21.3
4 Kea (+keyphraseness) 41.1 43.1 42.1
</table>
<tableCaption confidence="0.99963">
Table 2. Baseline auto-tagging approach vs. Kea
</tableCaption>
<subsectionHeader confidence="0.96889">
4.1 Evaluation method
</subsectionHeader>
<bodyText confidence="0.999993926829268">
The evaluation was performed using a set of 180
documents, described in Section 3.1, each tagged
with at least three tags on which two users have
agreed. In the following, unless explicitly stated
otherwise, these are the only tags we use. We
consider them to be ground truth. There are on
average five such tags per document, and our
goal is to extract tag sets that contain them all.
We regard a predicted tag as “correct” if it
matches one of the ground truth tags after using
the Porter stemmer. We measure performance by
computing Precision (the percentage of correct
extracted tags out of all extracted), Recall (the
percentage of correct extracted tags out of all
correct) and F-Measure (the harmonic mean of
the two). Given the set {yeast (4), network (3),
regulation (2), metabolic (2)} of ground truth
tags, where the numbers in parenthesis show how
many users have assigned each one, and the set
{network, metabolic, regulatory, ChIP-chip,
transcription} of predicted tags, three out of five
predicted terms are correct, yielding a precision
of 60%, and three out of four ground-truth terms
are extracted, a recall of 75%. The F-measure
combining the two values is 67%.
The reported precision and recall values are
averaged over all test documents. We use 10-fold
cross-validation for evaluation, which allows us
to use all 180 documents as test documents with-
out introducing optimistic bias in the perform-
ance measures obtained.
The results obtained in Sections 4.2 and 4.3
using this evaluation provide answers to the first
two questions above. To answer the third we
compare the indexing consistency of Maui to that
of CiteULike users in Section 4.4. Here, we con-
sider the assigned tag sets individually and com-
pute the consistency of Maui with each tagger as
described in Section 3.2. We compare Maui both
to all 332 users who tagged these documents, and
to the 36 best taggers identified in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.977114">
4.2 Keyphrase extraction vs. auto-tagging
</subsectionHeader>
<bodyText confidence="0.99982675">
As noted earlier, Brooks and Montanez (2006)
automatically determine tags by extracting terms
with the highest TFxIDF values for each post
and argue that their quality is perhaps better than
</bodyText>
<table confidence="0.9997108">
P R F
1 TFxIDF 14.4 16 15.2
2 1st occurrence 5.4 5.4 5.4
3 Keyphraseness 25.2 26.3 25.5
4 Length 2.1 2.1 2.1
5 Node degree 8.3 9.0 8.6
6 Wikipedia keyphraseness 16.9 18.3 17.6
7 Spread 12.1 13.0 12.5
8 Semantic relatedness 7.1 7.3 7.2
9 Inverse Wikipedia linkage 7.3 6.8 7.0
</table>
<tableCaption confidence="0.999886">
Table 3. Evaluation of individual features
</tableCaption>
<bodyText confidence="0.999821454545455">
that of manual tags. Note that they only use one-
word tags. We evaluate this approach using our
180 test documents and cross-validation, and
compare the top five extracted tags with those
assigned manually. Comparing the first two rows
of Table 2 shows that using multi-word phrases
as candidate tags (Section 4) is less accurate than
using single words, which gives an overall F-
Measure of 17%. Multi-words have higher
TFxIDF values, but single words are the majority
among the users’ tags. The length feature applied
in the next section helps to capture this character-
istic, without compromising Maui’s ability to
assign correct multi-words tags.
Adding a second feature, the position of the
first occurrence, and using Kea’s Naïve Bayes
model to learn their conditional distribution, im-
proves the results by 5 percentage points (row 3).
Adding the keyphraseness feature (row 4) nearly
doubles the F-Measure, from 21.3 to 42.1%. This
shows that CiteULike users tend to re-assign ex-
isting tags.
</bodyText>
<subsectionHeader confidence="0.999467">
4.3 Maui with additional features
</subsectionHeader>
<bodyText confidence="0.99995325">
To evaluate Maui let us first consider the indi-
vidual performance of old and new features, as
shown in Table 3. Rows 1 to 3 evaluate the stan-
dard features used by Frank et al. (1999); Rows 4
to 6 evaluate features that were previously used
in Kea for controlled indexing (Medelyan et al.,
2006) and which we have adapted in Maui for
free indexing. Rows 7 to 9 evaluate the three new
features of Maui. The values can be compared to
keyphrase extraction by chance (F-Measure =
1%) and to the multi-word TFxIDF baseline in
Table 2, row 2 (F-Measure = 15.2%). The
strength of these features varies from 2.1 to
25.5% (F-Measure). The strongest ones are key-
phraseness, Wikipedia keyphraseness, TFxIDF
and spread.
Table 4 demonstrates Maui’s performance
when the features are combined and shows how
the two different classifiers, Naïve Bayes (left)
and bagged decision trees (right), compare to
</bodyText>
<page confidence="0.907075">
1323
</page>
<table confidence="0.9998225">
Naïve Bayes F Bagged decision trees F
P R P R
1 Features 1 – 3 41.1 43.1 42.1 40.3 42.2 41.2
2 Features 1 – 6 38.9 41.1 40.0 40.3 42.6 41.4
3 Features 1 – 3, 7 – 9 39.3 41.1 40.2 43.7 46.2 44.9
4 Features 1 – 9 37.6 39.6 38.6 45.7 48.6 47.1
</table>
<tableCaption confidence="0.999886">
Table 4. Combining all features in Maui
</tableCaption>
<bodyText confidence="0.999548914285714">
each other. The baseline in row 1 (left) shows
Kea’s performance, using TF×IDF, first occur-
rence, keyphraseness and Naïve Bayes to com-
bine them (same as row 4 in Table 2). Using de-
cision trees with these three features does not
improve the performance (row 1, right). The fol-
lowing row combines the three original features
with length, node degree and Wikipedia-based
keyphraseness. In contrast to previous research
(Medelyan et al., 2008), in this setting we do not
observe an improvement with either Naïve Bayes
or bagged decision trees. In row 3 we combine
the three original features with the three new
ones introduced in this work. While Naïve
Bayes’ values are lower than the baseline, with
bagged decision trees Maui’s F-Measure im-
proves from 41.2 to 44.9%. The best results are
obtained by combining all nine features, again
using bagged decision trees, giving in row 4
(right) a notably improved F-Measure of 47.1%.
The recall of 48.6% shows that we match nearly
half of all tags on which at least two human tag-
gers have agreed.
Given this best combination of features, we
eliminate each feature one by one starting from
the individually weakest feature, in order to de-
termine the contribution of each feature to this
overall result. Table 5 compares the values and
only bagged decision trees are used this time.
The ‘Difference’ column quantifies the differ-
ence between the best F-Measure achieved with
all 9 features and excluding the one that is exam-
ined in that row. Interestingly, one of the strong-
est features, TF×IDF, is the one that contributes
the least when all features are combined, while
</bodyText>
<table confidence="0.992394416666667">
Features F-Measure Difference
All 9 Features 47.1
– Length 45 2.1
– 1st occurrence 45.6 1.5
– Inverse Wikip linkage 45.1 2
– Semantic relatedness 45.4 1.7
– Node degree 46 1.1
– Spread 46.4 0.7
– TFxIDF 46.8 0.3
– Wikip keyphraseness 43.1 4
– Keyphraseness 30.2 16.9
Non-Wikip features 41.7 5.4
</table>
<tableCaption confidence="0.999861">
Table 5. Evaluation using feature elimination
</tableCaption>
<bodyText confidence="0.999946933333333">
the contribution of the strongest feature—
keyphraseness—is, as expected, the highest, add-
ing 16.9 points. The second most important fea-
ture is Wikipedia keyphraseness, contributing 4
percentage points to the overall result.
Since some of the features in the best perform-
ing combination rely on Wikipedia as a knowl-
edge source, it is interesting to determine
Wikipedia’s exact contribution. The last row of
Table 5 combines the following features:
TF×IDF, first occurrence, keyphraseness, length
and spread. The F-Measure is 5.4 points lower
than that of Maui with all 9 features combined.
Therefore, the contribution of Wikipedia-based
features is significant.
</bodyText>
<subsectionHeader confidence="0.999099">
4.4 Maui’s consistency with human taggers
</subsectionHeader>
<bodyText confidence="0.99992535483871">
In Section 2.3 we discussed the indexing consis-
tency of CiteULike users on our data. There are a
total of 332 taggers and their consistency with
each other is 18.5%. Now, we use results ob-
tained with Maui during the cross-validation,
when all 9 features and bagged decision trees are
used (Table 4, row 4, right; see examples in Ta-
ble 5), and compute how consistent Maui is with
each human user, based on whatever document
this user has tagged. Then we average the results
to obtain the overall consistency with all 332
users.
Maui’s consistency with the 332 human tag-
gers ranges from 0 to 80%, with an average of
23.8%. The only cases where very low consis-
tency was achieved are those where the human
has only assigned a few tags per document (one
to three), or has some idiosyncratic tagging be-
havior (for example, one tagger adds the word
key in front of most tags). Still, with an average
of 23.8%, Maui’s performance is over 5 points
higher than that of an average CiteULike tagger
(18.5%)—and note this group only includes tag-
gers who have at least two co-taggers.
In Section 2.3 we were also able to determine
a smaller group of users who perform best and
are most prolific. This group consists of 36 tag-
gers whose consistency exceeds the average of
the original 332 users. These 36 taggers have
tagged a total of 143 documents with an average
consistency of 37.6%. Maui’s consistency with
</bodyText>
<page confidence="0.975267">
1324
</page>
<table confidence="0.999414">
Document 86865. Neural correlates 44. Exploring complex 353537. Computational 101. Network motifs:
of decision variables in networks. Strogatz. Nature roles for dopamine in simple building blocks
parietal cortex. Platt and 410, 8 (2001) behavioural control. Mon- of complex networks.
Glimcher. Nature 400,15 tague et al. Nature 431, Milo et al. Science 298,
(1999) 14 (2004) 824 (2002)
Tags cortex complex networks dopamine complex networks
assigned decision networks learning network
by lip review neuroscience motifs
Maui monkey synchronization review gene
visual graph reward complex
</table>
<tableCaption confidence="0.975442">
Table 6. Tags assigned by CiteULike taggers and Maui to four sample documents
</tableCaption>
<figure confidence="0.9269685">
decision making
decisionmaking
lip
monkey
neurophysiology
reward
</figure>
<figureCaption confidence="0.82882425">
Idiosyncratic: brain,
choice, cortex, decision,
electrophysiology, eye-
movements, limitations,
monkeys, neuroecono-
mics, neurons, neuro-
science, other, ppc, quals,
reinforcementlearning
</figureCaption>
<figure confidence="0.992844888888889">
complex
complexity
complex networks
graph
networks
review
small world
social networks
survey
</figure>
<figureCaption confidence="0.861938">
Idiosyncratic: 2001, adap-
tive systems, bistability,
coupled oscillator, graph
mining, graphs, explorig,
network biological, neu-
rons, strogatz
</figureCaption>
<figure confidence="0.989838">
dopamine
neuroscience
reinforcement learning
review
Idiosyncratic:
</figure>
<figureCaption confidence="0.930972833333333">
action selection, attention,
behavior, behavioral con-
trol, cognitive control,
learning, network, rein-
forcementlearning, re-
ward, td model
</figureCaption>
<figure confidence="0.957221285714286">
applied math
combinatorics
complexity
motifs
network
original
sub graph pattern
</figure>
<figureCaption confidence="0.751869625">
Idiosyncratic: 2002,
datamining, data min-
ing, graphs, link analy-
sis, modularity, net
paper, patterns, protein,
science, sysbio, web
characterization, web
graph
</figureCaption>
<bodyText confidence="0.739271">
Tags
assigned
by
CiteULike
taggers
these taggers ranges from 11.5% to 56%, with an
average of 35%. This places it only 2.6 percent-
age points behind the average performance of the
best CiteULike taggers. In fact, it outperforms 17
of them (cf. Table 1).
</bodyText>
<subsectionHeader confidence="0.880347">
4.5 Examples
</subsectionHeader>
<bodyText confidence="0.999830615384615">
Table 6 compares Maui with some of
CiteULike’s best human taggers on four ran-
domly chosen test documents. Boldface in the
taggers’ row indicates a tag that has been chosen
by at least two other human taggers; the remain-
ing tags have been chosen by just one human.
Boldface in Maui’s row shows tags that match
human tags. For each document Maui extracts
several tags assigned by at least two humans.
The other tags it chooses are generally chosen by
at least one human tagger, and even if not, they
are still related to the main theme of the docu-
ment.
</bodyText>
<sectionHeader confidence="0.994205" genericHeader="evaluation">
5 Discussion and related work
</sectionHeader>
<bodyText confidence="0.999787411764706">
It is possible to indirectly compare the results of
several previously published automatic tagging
approaches with Maui’s. For each paper, we
compute Maui’s results in settings closest to the
reported ones.
Brooks and Montanez (2006) extract terms
with the highest TF×IDF values as tags for posts
on technorati.com. They do not report precision
and recall values for their system, but our re-
implementation resulted in precision of 16.8%
and recall of 17.3% for the top five assigned
tags, compared to those agreed to by at least two
CiteULike users on 180 documents. Adding
eight additional features and combining them
using machine learning gives a clear improve-
ment—Maui achieves 45.7% and 48.7% preci-
sion and recall respectively.
Mishne (2006) uses TF×IDF-weighted terms
as full-text queries to retrieve posts similar to the
one being analyzed. Tags assigned to these posts
are analyzed to retrieve the best ones using clus-
tering and heuristic ranking; tags assigned by the
given user receive extra weight. Mishne per-
forms manual evaluation on 30 short articles and
reports precision and recall for the top ten tags of
38% and 47% respectively. We matched Maui’s
top ten terms to all tags assigned to 180 docu-
ments automatically and obtained precision and
recall of 44% and 29% respectively. (We believe
that manual rather than automatic evaluation
would be likely to give a far more favorable as-
sessment of our system.)
Chirita et al. (2007) aim to extract personal-
ized tags. Given a web page, they first retrieve
</bodyText>
<page confidence="0.971022">
1325
</page>
<bodyText confidence="0.999945833333333">
similar documents stored on the user’s desktop
and then determine keywords for these docu-
ments. They evaluate different term scoring
techniques, such as term and document fre-
quency, lexical dispersion, sentence scoring, and
term co-occurrence. Like the Kea algorithm, the
best formula combines term frequency with the
position of the first occurrence of the term, nor-
malized by page length. It yields a precision of
80% for the top four tags assigned to 30 large
websites (32Kbytes), again evaluated manually.
Our documents are considerably longer
(47Kbytes) and thus more difficult to work with,
nevertheless Maui achieves only slightly lower
values, from 66% to 80%, when evaluating
automatically against user-assigned tags. (The
above caveat regarding automatic and manual
assessment applies here too.)
Budura et al. (2008) develop a scoring for-
mula that combines three features (tag frequency,
tag co-occurrence and document similarity) and
manually evaluate it on ten CiteULike docu-
ments. Their precision for the top three to five
tags ranges from 66% to 77%, slightly worse
than in our paper (66% to 80%).
The only reported automatic evaluation of tags
was found in Sood et al. (2006), where TagAssist
was tested on 1000 blog posts. This algorithm is
similar to Mishne’s (2006), but uses centroid-
based clustering. Exact matching of TagAssist’s
tags against existing ones yielded precision and
recall of 13.1% and 22.8% respectively. This is
substantially lower than Maui’s 45.75% and
48.7% obtained with best settings (Section 4.3).
Note that this indirect comparison does not re-
veal the true ranking of approaches, because their
task definitions and test sets are slightly differ-
ent. It would be interesting to compare other sys-
tems on the multiple tagger set described in this
paper, as we believe this would more objectively
reflect the performance of humans and algo-
rithms.
</bodyText>
<sectionHeader confidence="0.999721" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999967166666667">
This paper has introduced a systematic way of
evaluating automatic tagging techniques without
the need for manual inspection. We have shown
how documents with multiple tag sets can be
used in conjunction with a standard consistency
measure to identify a robust test corpus for these
techniques. Based on the evaluation methodol-
ogy developed, we have shown that machine-
learning-based automatic keyphrase extraction
produces tag sets that exhibit consistency on a
par with that achieved by the best human taggers.
Our results also show a substantial improvement
on an existing automatic tagging approach based
on TF×IDF, and the results compare well to
other systems.
The success of automatic keyphrase extraction
depends primarily on the quality of the features
that are provided to the machine learning algo-
rithm involved. In this paper we have evaluated
nine different features, including two novel
Wikipedia-based semantic features, and found
that their combination used in conjunction with
bagged decision trees produces the best perform-
ance.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999969027027027">
Breiman, L. 1996. Bagging predictors. Machine
Learning 24(2): 123–140.
Brooks, C. H. and N. Montanez. 2006. Improved an-
notation of the blogosphere via autotagging and
hierarchical clustering. In Proc. Int. Conf. on
World Wide Web, Edinburgh, UK. pp. 625–632.
New York, NY, USA. ACM Press.
Budura, A., S. Michel, P. Cudre-Mauroux, and K.
Aberer. 2008. To tag or not to tag - harvesting ad-
jacent metadata in large-scale tagging systems. In
Proc. Int. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, Singapore. pp.
733–734. New York, NY, USA: ACM Press.
Chirita, P. A., S. Costache, W. Nejdl, and S. Hand-
schuh. 2007. P-tag: large scale automatic genera-
tion of personalized annotation tags for the web. In
Proc. Int. Conf. on World Wide Web, Banff, Can-
ada. pp. 845–854. New York, NY, USA: ACM
Press.
Frank, E., G. W. Paynter, I. H. Witten, C. Gutwin, and
C. G. Nevill-Manning. 1999. Domain-specific
keyphrase extraction. In Proc. of the 16th Interna-
tional Joint Conference on Artificial Intelligence,
Stockholm, Sweden. pp. 668–673. San Francisco,
CA: Morgan Kaufmann Publishers.
Golder, S. A. and B. A. Huberman. 2006. Usage pat-
terns of collaborative tagging systems. Journal of
Information Science, 32(2): 198–208.
Halpin, H., V. Robu, and H. Shepherd. 2007. The
complex dynamics of collaborative tagging. In
Proc. Int. Conf. on World Wide Web, pp. 211–220.
New York, NY, USA: ACM Press.
Heymann, P., D. Ramage, and H. Garcia-Molina.
2008. Social tag prediction. In Proc. Int. ACM
SIGIR Conf. on Research and Development in In-
formation Retrieval, Singapore. pp. 531–538. New
York, NY, USA: ACM Press.
</reference>
<page confidence="0.828443">
1326
</page>
<reference confidence="0.999656319148937">
Hulth, A. 2004. Combining machine learning and
natural language processing for automatic key-
word extraction. Ph.D. thesis, Dep. of Computer
and Systems Sciences, Stockholm University.
Leonard, L. E. 1975. Inter-indexer consistency and
retrieval effectiveness: measurement of relation-
ships. Ph.D. thesis, Grad. School of Library Sci-
ence, Univ. of Illinois, Urbana-Champaign, IL.
Leininger, K. 2000. Interindexer consistency in Psyc-
Info. Journal of Librarianship and Information
Science 32(1): 4–8.
Medelyan, O., I. H. Witten and D. Milne. 2008. Topic
indexing with Wikipedia. In Proc. of AAAI’08
Workshop on Wikipedia and Artificial Intelligence:
an Evolving Synergy, Chicago, USA. pp. 19–24.
Mishne, G. 2006. Autotag: a collaborative approach
to automated tag assignment for weblog posts. In
Proc. Int. Conf. on World Wide Web, Edinburgh,
UK. pp. 953–954. New York, NY, USA. ACM
Press
Porter, M. F. 1980. An algorithm for suffix stripping,
Program, 14(3): 130−137.
Rolling, L. 1981. Indexing Consistency, Quality and
Efficiency. Information Processing &amp; Management
17(2): 69–76.
Salton, G. and M. J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill New
York.
Sigurbjšrnsson, B. and R. van Zwol. 2008. Flickr tag
recommendation based on collective knowledge. In
Proc. Int. Conf. on World Wide Web, Beijing,
China. pp. 327–336. New York, NY, USA: ACM
Press.
Sood, S., K. Hammond, S. Owsley, and L. Birnbaum.
2007. TagAssist: Automatic tag suggestion for
blog posts. of Int. Conf. on Weblogs and Social
Media, Boulder, Colorado. Menlo Park, CA.
Turney, P. D. 2003. Coherent keyphrase extraction
via web mining. In Proc. of the 18th Int. Joint
Conf. on Artificial Intelligence, Acapulco, Mexico.
pp. 434–439. San Francisco, CA: Morgan Kauf-
mann Publishers.
Xu, Z., Fu, Y., Mao, J., and D. Su. 2006. Towards the
Semantic Web: Collaborative tag suggestions. In
Proc. Collaborative Web Tagging Workshop at the
Int. Joint Conf. on Artificial Intelligence, Stock-
holm, Sweden.
</reference>
<page confidence="0.993817">
1327
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968296">
<title confidence="0.99974">Human-competitive tagging using automatic keyphrase extraction</title>
<author confidence="0.994577">Olena Medelyan</author>
<author confidence="0.994577">Eibe Frank</author>
<author confidence="0.994577">H Ian</author>
<affiliation confidence="0.99977">Computer Science University of Waikato</affiliation>
<email confidence="0.976661">olena@cs.waikato.ac.nz</email>
<email confidence="0.976661">eibe@cs.waikato.ac.nz</email>
<email confidence="0.976661">ihw@cs.waikato.ac.nz</email>
<abstract confidence="0.999814">This paper connects two research areas: automatic tagging on the web and statistical keyphrase extraction. First, we analyze the quality of tags in a collaboratively created folksonomy using traditional evaluation techniques. Next, we demonstrate how documents can be tagged automatically with a state-of-the-art keyphrase extraction algorithm, and further improve performance in this new domain using a new algorithm, “Maui”, that utilizes semantic information extracted from Wikipedia. Maui outperforms existing approaches and extracts tags that are competitive with those assigned by the best performing human taggers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<journal>Machine Learning</journal>
<volume>24</volume>
<issue>2</issue>
<pages>123--140</pages>
<contexts>
<context position="20604" citStr="Breiman, 1996" startWordPosition="3420" endWordPosition="3421">ssumes that the features are independent of each other given the classification. However, Kea uses only two or three features, whereas Maui combines nine features amongst which there are many obvious relationships, e.g. first occurrence and spread, or node degree and semantic relatedness. Consequently, we also consider bagged decision trees, which can model attribute interactions and do not require parameter tuning to yield good results. Bagging learns an ensemble of classifiers and uses them in combination, thereby often achieving significantly better results than the individual classifiers (Breiman, 1996). Different trees are generated by sampling from the original dataset with replacement. Like Naïve Bayes, bagged trees yield probability estimates that can be used to rank candidates. To select tags from a new document, Maui determines candidate phrases and their feature values, and then applies the classifier built during training. This classifier determines the probability that a candidate is a tag based on relative frequencies observed from the training data. 4 Evaluation Here we describe the data used in the experiments and the results obtained, addressing the following questions: 1. How d</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Breiman, L. 1996. Bagging predictors. Machine Learning 24(2): 123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Brooks</author>
<author>N Montanez</author>
</authors>
<title>Improved annotation of the blogosphere via autotagging and hierarchical clustering.</title>
<date>2006</date>
<booktitle>In Proc. Int. Conf. on World Wide Web,</booktitle>
<pages>625--632</pages>
<publisher>ACM Press.</publisher>
<location>Edinburgh, UK.</location>
<contexts>
<context position="4220" citStr="Brooks and Montanez (2006)" startWordPosition="641" endWordPosition="644">ditionally used for the evaluation of professional indexing will provide insight into the quality of this folksonomy. Next, we extract a high quality corpus from CiteULike, containing documents that have been tagged consistently by the best human taggers. 1318 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1318–1327, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Figure 1. Quality control of CiteULike data Following that, our goal is to build a system that matches the performance of these taggers. We first apply an existing approach proposed by Brooks and Montanez (2006) and compare it to the keyphrase extraction algorithm Kea (Frank et al., 1999). Next we create a new algorithm, called Maui, that enhances Kea’s successful machine learning framework with semantic knowledge retrieved from Wikipedia, new features, and a new classification model. We evaluate Maui using tag sets assigned to the same documents by several users and show that it is as consistent with CiteULike users as they are with each other. Most of the computation required for automatic tagging with this method can be performed offline. In practice, it can be used as a tag suggestion tool that p</context>
<context position="23739" citStr="Brooks and Montanez (2006)" startWordPosition="3939" endWordPosition="3942">ducing optimistic bias in the performance measures obtained. The results obtained in Sections 4.2 and 4.3 using this evaluation provide answers to the first two questions above. To answer the third we compare the indexing consistency of Maui to that of CiteULike users in Section 4.4. Here, we consider the assigned tag sets individually and compute the consistency of Maui with each tagger as described in Section 3.2. We compare Maui both to all 332 users who tagged these documents, and to the 36 best taggers identified in Section 3.3. 4.2 Keyphrase extraction vs. auto-tagging As noted earlier, Brooks and Montanez (2006) automatically determine tags by extracting terms with the highest TFxIDF values for each post and argue that their quality is perhaps better than P R F 1 TFxIDF 14.4 16 15.2 2 1st occurrence 5.4 5.4 5.4 3 Keyphraseness 25.2 26.3 25.5 4 Length 2.1 2.1 2.1 5 Node degree 8.3 9.0 8.6 6 Wikipedia keyphraseness 16.9 18.3 17.6 7 Spread 12.1 13.0 12.5 8 Semantic relatedness 7.1 7.3 7.2 9 Inverse Wikipedia linkage 7.3 6.8 7.0 Table 3. Evaluation of individual features that of manual tags. Note that they only use oneword tags. We evaluate this approach using our 180 test documents and cross-validation,</context>
<context position="33135" citStr="Brooks and Montanez (2006)" startWordPosition="5484" endWordPosition="5487">other human taggers; the remaining tags have been chosen by just one human. Boldface in Maui’s row shows tags that match human tags. For each document Maui extracts several tags assigned by at least two humans. The other tags it chooses are generally chosen by at least one human tagger, and even if not, they are still related to the main theme of the document. 5 Discussion and related work It is possible to indirectly compare the results of several previously published automatic tagging approaches with Maui’s. For each paper, we compute Maui’s results in settings closest to the reported ones. Brooks and Montanez (2006) extract terms with the highest TF×IDF values as tags for posts on technorati.com. They do not report precision and recall values for their system, but our reimplementation resulted in precision of 16.8% and recall of 17.3% for the top five assigned tags, compared to those agreed to by at least two CiteULike users on 180 documents. Adding eight additional features and combining them using machine learning gives a clear improvement—Maui achieves 45.7% and 48.7% precision and recall respectively. Mishne (2006) uses TF×IDF-weighted terms as full-text queries to retrieve posts similar to the one b</context>
</contexts>
<marker>Brooks, Montanez, 2006</marker>
<rawString>Brooks, C. H. and N. Montanez. 2006. Improved annotation of the blogosphere via autotagging and hierarchical clustering. In Proc. Int. Conf. on World Wide Web, Edinburgh, UK. pp. 625–632. New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budura</author>
<author>S Michel</author>
<author>P Cudre-Mauroux</author>
<author>K Aberer</author>
</authors>
<title>To tag or not to tag - harvesting adjacent metadata in large-scale tagging systems.</title>
<date>2008</date>
<booktitle>In Proc. Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, Singapore.</booktitle>
<pages>733--734</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA:</location>
<contexts>
<context position="35251" citStr="Budura et al. (2008)" startWordPosition="5823" endWordPosition="5826">ence scoring, and term co-occurrence. Like the Kea algorithm, the best formula combines term frequency with the position of the first occurrence of the term, normalized by page length. It yields a precision of 80% for the top four tags assigned to 30 large websites (32Kbytes), again evaluated manually. Our documents are considerably longer (47Kbytes) and thus more difficult to work with, nevertheless Maui achieves only slightly lower values, from 66% to 80%, when evaluating automatically against user-assigned tags. (The above caveat regarding automatic and manual assessment applies here too.) Budura et al. (2008) develop a scoring formula that combines three features (tag frequency, tag co-occurrence and document similarity) and manually evaluate it on ten CiteULike documents. Their precision for the top three to five tags ranges from 66% to 77%, slightly worse than in our paper (66% to 80%). The only reported automatic evaluation of tags was found in Sood et al. (2006), where TagAssist was tested on 1000 blog posts. This algorithm is similar to Mishne’s (2006), but uses centroidbased clustering. Exact matching of TagAssist’s tags against existing ones yielded precision and recall of 13.1% and 22.8% r</context>
</contexts>
<marker>Budura, Michel, Cudre-Mauroux, Aberer, 2008</marker>
<rawString>Budura, A., S. Michel, P. Cudre-Mauroux, and K. Aberer. 2008. To tag or not to tag - harvesting adjacent metadata in large-scale tagging systems. In Proc. Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, Singapore. pp. 733–734. New York, NY, USA: ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Chirita</author>
<author>S Costache</author>
<author>W Nejdl</author>
<author>S Handschuh</author>
</authors>
<title>P-tag: large scale automatic generation of personalized annotation tags for the web. In</title>
<date>2007</date>
<booktitle>Proc. Int. Conf. on World Wide Web,</booktitle>
<pages>845--854</pages>
<publisher>ACM Press.</publisher>
<location>Banff,</location>
<contexts>
<context position="9793" citStr="Chirita et al. (2007)" startWordPosition="1639" endWordPosition="1642">n example, a document entitled Initial sequencing and comparative analysis of the mouse genome was tagged by eight users with a total of 22 tags. Four of them agreed on the tag mouse, but one used the broader term rodents. Three agreed on the tag genome, but one added genome paper, and another used the more specific comparative genomics. There are also cases when tags are written together, e.g. genomepaper, or with a prefix key genome, or in a different grammatical form: sequence vs. sequencing. This example shows that many inconsistencies in tags are not caused by personalized tag choices as Chirita et al. (2007) suggest, but rather stem from the lack of guidelines and uniform tag suggestions that a bookmarking service could provide. 2.2 Measuring tagging consistency Traditional indexers aim for consistency, on the basis that this will enhance document retrieval (Leonard, 1975). Consistency is measured using experiments in which several people index the same documents—usually a small set of a few dozen documents. It is computed for pairs of indexers, by formulae such as Rolling’s (1981): , where C is the number of tags (index terms) indexers I1 and I2 have in common and A and B is the size of their ta</context>
<context position="34347" citStr="Chirita et al. (2007)" startWordPosition="5684" endWordPosition="5687">the one being analyzed. Tags assigned to these posts are analyzed to retrieve the best ones using clustering and heuristic ranking; tags assigned by the given user receive extra weight. Mishne performs manual evaluation on 30 short articles and reports precision and recall for the top ten tags of 38% and 47% respectively. We matched Maui’s top ten terms to all tags assigned to 180 documents automatically and obtained precision and recall of 44% and 29% respectively. (We believe that manual rather than automatic evaluation would be likely to give a far more favorable assessment of our system.) Chirita et al. (2007) aim to extract personalized tags. Given a web page, they first retrieve 1325 similar documents stored on the user’s desktop and then determine keywords for these documents. They evaluate different term scoring techniques, such as term and document frequency, lexical dispersion, sentence scoring, and term co-occurrence. Like the Kea algorithm, the best formula combines term frequency with the position of the first occurrence of the term, normalized by page length. It yields a precision of 80% for the top four tags assigned to 30 large websites (32Kbytes), again evaluated manually. Our document</context>
</contexts>
<marker>Chirita, Costache, Nejdl, Handschuh, 2007</marker>
<rawString>Chirita, P. A., S. Costache, W. Nejdl, and S. Handschuh. 2007. P-tag: large scale automatic generation of personalized annotation tags for the web. In Proc. Int. Conf. on World Wide Web, Banff, Canada. pp. 845–854. New York, NY, USA: ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Frank</author>
<author>G W Paynter</author>
<author>I H Witten</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proc. of the 16th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>668--673</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>Stockholm,</location>
<contexts>
<context position="2382" citStr="Frank et al., 1999" startWordPosition="363" endWordPosition="366">mies, such as tag distribution and co-occurrences (Halpin et al., 2007; Sigurbjšrnsson et al., 2008; Sood et al., 2007), but to our knowledge there has been none on assessing the actual quality of tags. How well do human taggers perform? How consistent are they with each other? One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. Mishne, 2006; Sood et al., 2007; Heymann et al., 2008). Interestingly, the blooming research on automatic tagging has so far not been connected to work on keyphrase extraction (e.g. Frank et al., 1999; Turney, 2003; Hulth, 2004), which can be used as a tool for the same task (note: we use tag and keyphrase as synonyms). Instead of simple heuristics based on term frequencies and co-occurrence of tags, keyphrase extraction methods apply machine learning to determine typical distributions of properties common to manually assigned phrases, and can include analysis of semantic relations between candidate tags (Turney, 2003). How well do state-of-the-art keyphrase extraction systems perform compared to simple tagging techniques? How consistent are they with human taggers? These are questions we </context>
<context position="4298" citStr="Frank et al., 1999" startWordPosition="654" endWordPosition="657"> the quality of this folksonomy. Next, we extract a high quality corpus from CiteULike, containing documents that have been tagged consistently by the best human taggers. 1318 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1318–1327, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Figure 1. Quality control of CiteULike data Following that, our goal is to build a system that matches the performance of these taggers. We first apply an existing approach proposed by Brooks and Montanez (2006) and compare it to the keyphrase extraction algorithm Kea (Frank et al., 1999). Next we create a new algorithm, called Maui, that enhances Kea’s successful machine learning framework with semantic knowledge retrieved from Wikipedia, new features, and a new classification model. We evaluate Maui using tag sets assigned to the same documents by several users and show that it is as consistent with CiteULike users as they are with each other. Most of the computation required for automatic tagging with this method can be performed offline. In practice, it can be used as a tag suggestion tool that provides users with tags describing the main topics of newly added documents, w</context>
<context position="13287" citStr="Frank et al., 1999" startWordPosition="2217" endWordPosition="2220">ieve as a group is 37.7%, which is the similar to the average consistency of professionals (Leininger, 2000). The above consistency analysis provides insight into the tagging quality of the best CiteULike users, based on HighWire and Nature articles. For the purposes of this paper, it shows how the tagging community can be restricted to a best-performing group of taggers by measuring their consistency. This is helpful for testing the performance of automatic tagging (Section 4.4). 3 Automatic tagging with Maui Maui is a general algorithm for automatic topical indexing based on the Kea system (Frank et al., 1999).1 It works in two stages: candidate selection and machine learning based filtering. In this paper, we apply it to automatic tagging. In the candidate selection stage, Maui first determines textual sequences defined by orthographic boundaries and splits these sequences into tokens. Then all n-grams up to a maximum length of 3 words that do not begin or end with a stopword are extracted as candidate tags. To reduce the number of candidates, all those that appear only once are discarded. This speeds up the training and the extraction process without impacting the results. In the filtering stage </context>
<context position="14590" citStr="Frank et al., 1999" startWordPosition="2429" endWordPosition="2432">learning model to obtain the probability that the candidate is indeed a tag. Maui’s architecture resembles that of many other supervised keyphrase extraction systems (Turney, 2000; Hulth 2004; Medelyan et al., 2008). However, this architecture has not previously been applied to the task of automatic tagging. 1 Maui is open-source and available for download at http://maui-indexer.googlecode.com 3.1 Features indicating significance We now describe the features used in the classification model to determine whether a phrase is likely to be a tag. We begin with three baseline features used in Kea (Frank et al., 1999), and extend the set with three features that have been found useful in previous work. We also add three new features that have not been evaluated before: spread, semantic relatedness and inverse Wikipedia linkage. All Wikipedia-based features are computed using the WikipediaMiner toolkit.2 1. TF×IDF combines the frequency of a phrase in a particular document with its inverse occurrence frequency in general use (Salton and McGill, 1983). This score is high for rare phrases that appear frequently in a document and therefore are more likely to be significant. 2. Position of the first occurrence </context>
<context position="19938" citStr="Frank et al., 1999" startWordPosition="3321" endWordPosition="3324">ike collection described in Section 3.1. For each document we know a set of tags that at least two users have agreed on. This is used as ground truth for building the model. For each training document, candidate phrases (i.e. n-grams) are identified and their feature values are calculated as described above. Each candidate is then marked as a positive or negative example, depending on whether users have assigned it as a tag to the corresponding document. The machine-learning model is constructed automatically from these labeled training examples using the WEKA machine learning workbench. Kea (Frank et al., 1999) uses the Naïve Bayes classifier, which implicitly assumes that the features are independent of each other given the classification. However, Kea uses only two or three features, whereas Maui combines nine features amongst which there are many obvious relationships, e.g. first occurrence and spread, or node degree and semantic relatedness. Consequently, we also consider bagged decision trees, which can model attribute interactions and do not require parameter tuning to yield good results. Bagging learns an ensemble of classifiers and uses them in combination, thereby often achieving significan</context>
<context position="25414" citStr="Frank et al. (1999)" startWordPosition="4224" endWordPosition="4227">sing Maui’s ability to assign correct multi-words tags. Adding a second feature, the position of the first occurrence, and using Kea’s Naïve Bayes model to learn their conditional distribution, improves the results by 5 percentage points (row 3). Adding the keyphraseness feature (row 4) nearly doubles the F-Measure, from 21.3 to 42.1%. This shows that CiteULike users tend to re-assign existing tags. 4.3 Maui with additional features To evaluate Maui let us first consider the individual performance of old and new features, as shown in Table 3. Rows 1 to 3 evaluate the standard features used by Frank et al. (1999); Rows 4 to 6 evaluate features that were previously used in Kea for controlled indexing (Medelyan et al., 2006) and which we have adapted in Maui for free indexing. Rows 7 to 9 evaluate the three new features of Maui. The values can be compared to keyphrase extraction by chance (F-Measure = 1%) and to the multi-word TFxIDF baseline in Table 2, row 2 (F-Measure = 15.2%). The strength of these features varies from 2.1 to 25.5% (F-Measure). The strongest ones are keyphraseness, Wikipedia keyphraseness, TFxIDF and spread. Table 4 demonstrates Maui’s performance when the features are combined and </context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Frank, E., G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proc. of the 16th International Joint Conference on Artificial Intelligence, Stockholm, Sweden. pp. 668–673. San Francisco, CA: Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Golder</author>
<author>B A Huberman</author>
</authors>
<title>Usage patterns of collaborative tagging systems.</title>
<date>2006</date>
<journal>Journal of Information Science,</journal>
<volume>32</volume>
<issue>2</issue>
<pages>198--208</pages>
<contexts>
<context position="1583" citStr="Golder and Huberman, 2006" startWordPosition="236" endWordPosition="240">iven document. Unlike metadata assigned by authors, or by professional indexers in libraries, tags are assigned by endusers for organizing and sharing information that is of interest to them. The organic system of tags assigned by all users of a given web platform is called a folksonomy. In contrast to traditional taxonomies painstakingly constructed by experts, a user can add any tags to a folksonomy. This leads to the greatest downside of tagging, inconsistency, which originates in the synonymy and polysemy of human language, as well as in the varying degrees of specificity used by taggers (Golder and Huberman, 2006). In traditional libraries, consistency is the primary evaluation criterion of indexing (Rolling, 1981). Much work has been done on describing the statistical properties of folksonomies, such as tag distribution and co-occurrences (Halpin et al., 2007; Sigurbjšrnsson et al., 2008; Sood et al., 2007), but to our knowledge there has been none on assessing the actual quality of tags. How well do human taggers perform? How consistent are they with each other? One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. M</context>
</contexts>
<marker>Golder, Huberman, 2006</marker>
<rawString>Golder, S. A. and B. A. Huberman. 2006. Usage patterns of collaborative tagging systems. Journal of Information Science, 32(2): 198–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Halpin</author>
<author>V Robu</author>
<author>H Shepherd</author>
</authors>
<title>The complex dynamics of collaborative tagging.</title>
<date>2007</date>
<booktitle>In Proc. Int. Conf. on World Wide Web,</booktitle>
<pages>211--220</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA:</location>
<contexts>
<context position="1834" citStr="Halpin et al., 2007" startWordPosition="273" endWordPosition="276">atform is called a folksonomy. In contrast to traditional taxonomies painstakingly constructed by experts, a user can add any tags to a folksonomy. This leads to the greatest downside of tagging, inconsistency, which originates in the synonymy and polysemy of human language, as well as in the varying degrees of specificity used by taggers (Golder and Huberman, 2006). In traditional libraries, consistency is the primary evaluation criterion of indexing (Rolling, 1981). Much work has been done on describing the statistical properties of folksonomies, such as tag distribution and co-occurrences (Halpin et al., 2007; Sigurbjšrnsson et al., 2008; Sood et al., 2007), but to our knowledge there has been none on assessing the actual quality of tags. How well do human taggers perform? How consistent are they with each other? One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. Mishne, 2006; Sood et al., 2007; Heymann et al., 2008). Interestingly, the blooming research on automatic tagging has so far not been connected to work on keyphrase extraction (e.g. Frank et al., 1999; Turney, 2003; Hulth, 2004), which can be used as a</context>
</contexts>
<marker>Halpin, Robu, Shepherd, 2007</marker>
<rawString>Halpin, H., V. Robu, and H. Shepherd. 2007. The complex dynamics of collaborative tagging. In Proc. Int. Conf. on World Wide Web, pp. 211–220. New York, NY, USA: ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heymann</author>
<author>D Ramage</author>
<author>H Garcia-Molina</author>
</authors>
<title>Social tag prediction.</title>
<date>2008</date>
<booktitle>In Proc. Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, Singapore.</booktitle>
<pages>531--538</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA:</location>
<contexts>
<context position="2236" citStr="Heymann et al., 2008" startWordPosition="339" endWordPosition="342">istency is the primary evaluation criterion of indexing (Rolling, 1981). Much work has been done on describing the statistical properties of folksonomies, such as tag distribution and co-occurrences (Halpin et al., 2007; Sigurbjšrnsson et al., 2008; Sood et al., 2007), but to our knowledge there has been none on assessing the actual quality of tags. How well do human taggers perform? How consistent are they with each other? One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. Mishne, 2006; Sood et al., 2007; Heymann et al., 2008). Interestingly, the blooming research on automatic tagging has so far not been connected to work on keyphrase extraction (e.g. Frank et al., 1999; Turney, 2003; Hulth, 2004), which can be used as a tool for the same task (note: we use tag and keyphrase as synonyms). Instead of simple heuristics based on term frequencies and co-occurrence of tags, keyphrase extraction methods apply machine learning to determine typical distributions of properties common to manually assigned phrases, and can include analysis of semantic relations between candidate tags (Turney, 2003). How well do state-of-the-a</context>
</contexts>
<marker>Heymann, Ramage, Garcia-Molina, 2008</marker>
<rawString>Heymann, P., D. Ramage, and H. Garcia-Molina. 2008. Social tag prediction. In Proc. Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, Singapore. pp. 531–538. New York, NY, USA: ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hulth</author>
</authors>
<title>Combining machine learning and natural language processing for automatic keyword extraction.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Dep. of Computer and Systems Sciences, Stockholm University.</institution>
<contexts>
<context position="2410" citStr="Hulth, 2004" startWordPosition="369" endWordPosition="370"> co-occurrences (Halpin et al., 2007; Sigurbjšrnsson et al., 2008; Sood et al., 2007), but to our knowledge there has been none on assessing the actual quality of tags. How well do human taggers perform? How consistent are they with each other? One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. Mishne, 2006; Sood et al., 2007; Heymann et al., 2008). Interestingly, the blooming research on automatic tagging has so far not been connected to work on keyphrase extraction (e.g. Frank et al., 1999; Turney, 2003; Hulth, 2004), which can be used as a tool for the same task (note: we use tag and keyphrase as synonyms). Instead of simple heuristics based on term frequencies and co-occurrence of tags, keyphrase extraction methods apply machine learning to determine typical distributions of properties common to manually assigned phrases, and can include analysis of semantic relations between candidate tags (Turney, 2003). How well do state-of-the-art keyphrase extraction systems perform compared to simple tagging techniques? How consistent are they with human taggers? These are questions we address in this paper. Until</context>
<context position="14162" citStr="Hulth 2004" startWordPosition="2362" endWordPosition="2363">nces into tokens. Then all n-grams up to a maximum length of 3 words that do not begin or end with a stopword are extracted as candidate tags. To reduce the number of candidates, all those that appear only once are discarded. This speeds up the training and the extraction process without impacting the results. In the filtering stage several features are computed for each candidate, which are then input to a machine learning model to obtain the probability that the candidate is indeed a tag. Maui’s architecture resembles that of many other supervised keyphrase extraction systems (Turney, 2000; Hulth 2004; Medelyan et al., 2008). However, this architecture has not previously been applied to the task of automatic tagging. 1 Maui is open-source and available for download at http://maui-indexer.googlecode.com 3.1 Features indicating significance We now describe the features used in the classification model to determine whether a phrase is likely to be a tag. We begin with three baseline features used in Kea (Frank et al., 1999), and extend the set with three features that have been found useful in previous work. We also add three new features that have not been evaluated before: spread, semantic </context>
</contexts>
<marker>Hulth, 2004</marker>
<rawString>Hulth, A. 2004. Combining machine learning and natural language processing for automatic keyword extraction. Ph.D. thesis, Dep. of Computer and Systems Sciences, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Leonard</author>
</authors>
<title>Inter-indexer consistency and retrieval effectiveness: measurement of relationships.</title>
<date>1975</date>
<tech>Ph.D. thesis,</tech>
<institution>Grad. School of Library Science, Univ. of Illinois,</institution>
<location>Urbana-Champaign, IL.</location>
<contexts>
<context position="10063" citStr="Leonard, 1975" startWordPosition="1682" endWordPosition="1683">aper, and another used the more specific comparative genomics. There are also cases when tags are written together, e.g. genomepaper, or with a prefix key genome, or in a different grammatical form: sequence vs. sequencing. This example shows that many inconsistencies in tags are not caused by personalized tag choices as Chirita et al. (2007) suggest, but rather stem from the lack of guidelines and uniform tag suggestions that a bookmarking service could provide. 2.2 Measuring tagging consistency Traditional indexers aim for consistency, on the basis that this will enhance document retrieval (Leonard, 1975). Consistency is measured using experiments in which several people index the same documents—usually a small set of a few dozen documents. It is computed for pairs of indexers, by formulae such as Rolling’s (1981): , where C is the number of tags (index terms) indexers I1 and I2 have in common and A and B is the size of their tag sets respectively. In our experiments, before computing the number of terms in common, we stem each tag with the Porter (1980) stemmer. For example, the overlap C between the tag sets {complex systems, network, small world} and {theoretical, small world, networks, dyn</context>
</contexts>
<marker>Leonard, 1975</marker>
<rawString>Leonard, L. E. 1975. Inter-indexer consistency and retrieval effectiveness: measurement of relationships. Ph.D. thesis, Grad. School of Library Science, Univ. of Illinois, Urbana-Champaign, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Leininger</author>
</authors>
<title>Interindexer consistency in PsycInfo.</title>
<date>2000</date>
<journal>Journal of Librarianship and Information Science</journal>
<volume>32</volume>
<issue>1</issue>
<pages>4--8</pages>
<contexts>
<context position="12328" citStr="Leininger, 2000" startWordPosition="2062" endWordPosition="2063">me document. Consistency is then averaged across documents. We found that the distribution of per-user consistency resembles a power law with a few users achieving high consistency values and a long tail of inconsistent taggers. The maximum consis1320 tency in this group is 92.3% and the average is 18.5%. The average consistency of the most prolific 70 indexers—those who have indexed at least five documents—is in the same range, namely 18.4%. The consistency of traditional approaches to free indexing is reported to be between 4% and 67%, with an average of 27% depending on what aids are used (Leininger, 2000). It is instructive to consider the group of best taggers. We define these as the ones who (a) exhibit greater than average consistency with all others, and (b) are sufficiently prolific, i.e. have tagged at least five documents. There are 36 such taggers; Table 1 lists their consistency within this group. The average consistency they achieve as a group is 37.7%, which is the similar to the average consistency of professionals (Leininger, 2000). The above consistency analysis provides insight into the tagging quality of the best CiteULike users, based on HighWire and Nature articles. For the p</context>
</contexts>
<marker>Leininger, 2000</marker>
<rawString>Leininger, K. 2000. Interindexer consistency in PsycInfo. Journal of Librarianship and Information Science 32(1): 4–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>I H Witten</author>
<author>D Milne</author>
</authors>
<title>Topic indexing with Wikipedia.</title>
<date>2008</date>
<booktitle>In Proc. of AAAI’08 Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy,</booktitle>
<pages>pp.</pages>
<location>Chicago, USA.</location>
<contexts>
<context position="14186" citStr="Medelyan et al., 2008" startWordPosition="2364" endWordPosition="2367">kens. Then all n-grams up to a maximum length of 3 words that do not begin or end with a stopword are extracted as candidate tags. To reduce the number of candidates, all those that appear only once are discarded. This speeds up the training and the extraction process without impacting the results. In the filtering stage several features are computed for each candidate, which are then input to a machine learning model to obtain the probability that the candidate is indeed a tag. Maui’s architecture resembles that of many other supervised keyphrase extraction systems (Turney, 2000; Hulth 2004; Medelyan et al., 2008). However, this architecture has not previously been applied to the task of automatic tagging. 1 Maui is open-source and available for download at http://maui-indexer.googlecode.com 3.1 Features indicating significance We now describe the features used in the classification model to determine whether a phrase is likely to be a tag. We begin with three baseline features used in Kea (Frank et al., 1999), and extend the set with three features that have been found useful in previous work. We also add three new features that have not been evaluated before: spread, semantic relatedness and inverse </context>
<context position="16474" citStr="Medelyan et al. (2008)" startWordPosition="2735" endWordPosition="2738">as in Kea) this feature is just one component of the overall model. Thus if a candidate never appears as a keyphrase in the training corpus, it can still be extracted if its other feature values are significant enough. 4. Phrase length is measured in words. Generally speaking, the longer the phrase, the more specific it is. Training captures and quantifies the specificity preference in a given training corpus. 5. Node degree quantifies the semantic relatedness of a candidate tag to other candidates. Turney (2003) computes semantic relatedness using search engine statistics. Instead, following Medelyan et al. (2008), we utilize Wikipedia hyperlinks for this task. We first map each candidate phrase to its most common Wikipedia page. For example, the word Jaguar appears as a link anchor in Wikipedia 927 times. In 466 cases it links to the article Jaguar cars, thus the commonness of this mapping is 0.5. In 203 cases it links to the animal description, a commonness of 2 http://wikipedia-miner.sourceforge.net/ 1321 0.22. We compute the node degree of the corresponding Wikipedia article as the number of hyperlinks that connect it to other Wikipedia pages that have been identified for other candidate tags from </context>
<context position="26845" citStr="Medelyan et al., 2008" startWordPosition="4478" endWordPosition="4481">tures 1 – 6 38.9 41.1 40.0 40.3 42.6 41.4 3 Features 1 – 3, 7 – 9 39.3 41.1 40.2 43.7 46.2 44.9 4 Features 1 – 9 37.6 39.6 38.6 45.7 48.6 47.1 Table 4. Combining all features in Maui each other. The baseline in row 1 (left) shows Kea’s performance, using TF×IDF, first occurrence, keyphraseness and Naïve Bayes to combine them (same as row 4 in Table 2). Using decision trees with these three features does not improve the performance (row 1, right). The following row combines the three original features with length, node degree and Wikipedia-based keyphraseness. In contrast to previous research (Medelyan et al., 2008), in this setting we do not observe an improvement with either Naïve Bayes or bagged decision trees. In row 3 we combine the three original features with the three new ones introduced in this work. While Naïve Bayes’ values are lower than the baseline, with bagged decision trees Maui’s F-Measure improves from 41.2 to 44.9%. The best results are obtained by combining all nine features, again using bagged decision trees, giving in row 4 (right) a notably improved F-Measure of 47.1%. The recall of 48.6% shows that we match nearly half of all tags on which at least two human taggers have agreed. G</context>
</contexts>
<marker>Medelyan, Witten, Milne, 2008</marker>
<rawString>Medelyan, O., I. H. Witten and D. Milne. 2008. Topic indexing with Wikipedia. In Proc. of AAAI’08 Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy, Chicago, USA. pp. 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mishne</author>
</authors>
<title>Autotag: a collaborative approach to automated tag assignment for weblog posts.</title>
<date>2006</date>
<booktitle>In Proc. Int. Conf. on World Wide Web,</booktitle>
<pages>953--954</pages>
<publisher>ACM Press</publisher>
<location>Edinburgh, UK.</location>
<contexts>
<context position="2194" citStr="Mishne, 2006" startWordPosition="333" endWordPosition="334">). In traditional libraries, consistency is the primary evaluation criterion of indexing (Rolling, 1981). Much work has been done on describing the statistical properties of folksonomies, such as tag distribution and co-occurrences (Halpin et al., 2007; Sigurbjšrnsson et al., 2008; Sood et al., 2007), but to our knowledge there has been none on assessing the actual quality of tags. How well do human taggers perform? How consistent are they with each other? One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. Mishne, 2006; Sood et al., 2007; Heymann et al., 2008). Interestingly, the blooming research on automatic tagging has so far not been connected to work on keyphrase extraction (e.g. Frank et al., 1999; Turney, 2003; Hulth, 2004), which can be used as a tool for the same task (note: we use tag and keyphrase as synonyms). Instead of simple heuristics based on term frequencies and co-occurrence of tags, keyphrase extraction methods apply machine learning to determine typical distributions of properties common to manually assigned phrases, and can include analysis of semantic relations between candidate tags </context>
<context position="6674" citStr="Mishne, 2006" startWordPosition="1040" endWordPosition="1041">gle applications of a tag by a user to a document. The two most popular tags, bibtex-import and no-tag, indicate an information source and a missing tag respectively. Most of the remainder describe particular concepts relevant to the documents. We exclude non-content tags from our experiments, e.g. personal tags like to-read or todo. Note that spam entries have been eliminated from the data set. Because CiteULike taggers are not professional indexers, high quality of the assigned topics cannot be guaranteed. In fact, manual assessment of users’ tags by human evaluators shows precision of 59% (Mishne, 2006) and 49% (Sood et al., 2006). However, why is the opinion of human evaluators valued more than the opinion of taggers? We propose an alternative way of determining ground truth using an automatic approach to determine reliable tags: We concentrate on a subset of CiteULike containing documents that have been indexed with at least three tags on which at least two users have agreed. In order to be able to measure the tagging consistency between the users, and then compare it to the algorithm’s consistency, we need taggers who have tagged documents that some others had tagged. We say that two user</context>
<context position="15741" citStr="Mishne (2006)" startWordPosition="2619" endWordPosition="2620">likely to be significant. 2. Position of the first occurrence is computed as the relative distance of the first occurrence of the candidate tag from the beginning of the document. Candidates with very high or very low values are likely to be tags, because they appear either in the opening document parts such as title, abstract, table of contents, and introduction, or in the document’s final sections such as conclusion and reference lists. 3. Keyphraseness quantifies how often a candidate phrase appears as a tag in the training corpus. Automatic tagging approaches utilize the same information: Mishne (2006) and Sood et al. (2006) automatically suggest tags previously assigned to similar documents. However, in Maui (as in Kea) this feature is just one component of the overall model. Thus if a candidate never appears as a keyphrase in the training corpus, it can still be extracted if its other feature values are significant enough. 4. Phrase length is measured in words. Generally speaking, the longer the phrase, the more specific it is. Training captures and quantifies the specificity preference in a given training corpus. 5. Node degree quantifies the semantic relatedness of a candidate tag to ot</context>
<context position="33648" citStr="Mishne (2006)" startWordPosition="5569" endWordPosition="5570">aper, we compute Maui’s results in settings closest to the reported ones. Brooks and Montanez (2006) extract terms with the highest TF×IDF values as tags for posts on technorati.com. They do not report precision and recall values for their system, but our reimplementation resulted in precision of 16.8% and recall of 17.3% for the top five assigned tags, compared to those agreed to by at least two CiteULike users on 180 documents. Adding eight additional features and combining them using machine learning gives a clear improvement—Maui achieves 45.7% and 48.7% precision and recall respectively. Mishne (2006) uses TF×IDF-weighted terms as full-text queries to retrieve posts similar to the one being analyzed. Tags assigned to these posts are analyzed to retrieve the best ones using clustering and heuristic ranking; tags assigned by the given user receive extra weight. Mishne performs manual evaluation on 30 short articles and reports precision and recall for the top ten tags of 38% and 47% respectively. We matched Maui’s top ten terms to all tags assigned to 180 documents automatically and obtained precision and recall of 44% and 29% respectively. (We believe that manual rather than automatic evalu</context>
</contexts>
<marker>Mishne, 2006</marker>
<rawString>Mishne, G. 2006. Autotag: a collaborative approach to automated tag assignment for weblog posts. In Proc. Int. Conf. on World Wide Web, Edinburgh, UK. pp. 953–954. New York, NY, USA. ACM Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping,</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>130--137</pages>
<contexts>
<context position="10521" citStr="Porter (1980)" startWordPosition="1766" endWordPosition="1767">provide. 2.2 Measuring tagging consistency Traditional indexers aim for consistency, on the basis that this will enhance document retrieval (Leonard, 1975). Consistency is measured using experiments in which several people index the same documents—usually a small set of a few dozen documents. It is computed for pairs of indexers, by formulae such as Rolling’s (1981): , where C is the number of tags (index terms) indexers I1 and I2 have in common and A and B is the size of their tag sets respectively. In our experiments, before computing the number of terms in common, we stem each tag with the Porter (1980) stemmer. For example, the overlap C between the tag sets {complex systems, network, small world} and {theoretical, small world, networks, dynamics} consist of the two tags {network, small world}, and the consistency is 2×2/(3+4) = 0.57. To compute the overall consistency of a particular indexer, this figure is averaged over all documents and co-indexers. There were no cases where the same user reassigned tags to the same articles, so computing intra-tagger consistency, although interesting, was not impossible. To our knowledge, traditional indexing consistency metrics have not yet been applie</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M. F. 1980. An algorithm for suffix stripping, Program, 14(3): 130−137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rolling</author>
</authors>
<title>Indexing Consistency, Quality and Efficiency.</title>
<date>1981</date>
<journal>Information Processing &amp; Management</journal>
<volume>17</volume>
<issue>2</issue>
<pages>69--76</pages>
<contexts>
<context position="1686" citStr="Rolling, 1981" startWordPosition="252" endWordPosition="253">endusers for organizing and sharing information that is of interest to them. The organic system of tags assigned by all users of a given web platform is called a folksonomy. In contrast to traditional taxonomies painstakingly constructed by experts, a user can add any tags to a folksonomy. This leads to the greatest downside of tagging, inconsistency, which originates in the synonymy and polysemy of human language, as well as in the varying degrees of specificity used by taggers (Golder and Huberman, 2006). In traditional libraries, consistency is the primary evaluation criterion of indexing (Rolling, 1981). Much work has been done on describing the statistical properties of folksonomies, such as tag distribution and co-occurrences (Halpin et al., 2007; Sigurbjšrnsson et al., 2008; Sood et al., 2007), but to our knowledge there has been none on assessing the actual quality of tags. How well do human taggers perform? How consistent are they with each other? One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. Mishne, 2006; Sood et al., 2007; Heymann et al., 2008). Interestingly, the blooming research on automati</context>
</contexts>
<marker>Rolling, 1981</marker>
<rawString>Rolling, L. 1981. Indexing Consistency, Quality and Efficiency. Information Processing &amp; Management 17(2): 69–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill</publisher>
<location>New York.</location>
<contexts>
<context position="15030" citStr="Salton and McGill, 1983" startWordPosition="2496" endWordPosition="2499">e now describe the features used in the classification model to determine whether a phrase is likely to be a tag. We begin with three baseline features used in Kea (Frank et al., 1999), and extend the set with three features that have been found useful in previous work. We also add three new features that have not been evaluated before: spread, semantic relatedness and inverse Wikipedia linkage. All Wikipedia-based features are computed using the WikipediaMiner toolkit.2 1. TF×IDF combines the frequency of a phrase in a particular document with its inverse occurrence frequency in general use (Salton and McGill, 1983). This score is high for rare phrases that appear frequently in a document and therefore are more likely to be significant. 2. Position of the first occurrence is computed as the relative distance of the first occurrence of the candidate tag from the beginning of the document. Candidates with very high or very low values are likely to be tags, because they appear either in the opening document parts such as title, abstract, table of contents, and introduction, or in the document’s final sections such as conclusion and reference lists. 3. Keyphraseness quantifies how often a candidate phrase ap</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. and M. J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sigurbjšrnsson</author>
<author>R van Zwol</author>
</authors>
<title>Flickr tag recommendation based on collective knowledge.</title>
<date>2008</date>
<booktitle>In Proc. Int. Conf. on World Wide Web,</booktitle>
<pages>327--336</pages>
<publisher>ACM Press.</publisher>
<location>Beijing,</location>
<marker>Sigurbjšrnsson, van Zwol, 2008</marker>
<rawString>Sigurbjšrnsson, B. and R. van Zwol. 2008. Flickr tag recommendation based on collective knowledge. In Proc. Int. Conf. on World Wide Web, Beijing, China. pp. 327–336. New York, NY, USA: ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sood</author>
<author>K Hammond</author>
<author>S Owsley</author>
<author>L Birnbaum</author>
</authors>
<title>TagAssist: Automatic tag suggestion for blog posts.</title>
<date>2007</date>
<booktitle>of Int. Conf. on Weblogs and Social</booktitle>
<location>Media, Boulder, Colorado. Menlo Park, CA.</location>
<contexts>
<context position="1883" citStr="Sood et al., 2007" startWordPosition="281" endWordPosition="284">itional taxonomies painstakingly constructed by experts, a user can add any tags to a folksonomy. This leads to the greatest downside of tagging, inconsistency, which originates in the synonymy and polysemy of human language, as well as in the varying degrees of specificity used by taggers (Golder and Huberman, 2006). In traditional libraries, consistency is the primary evaluation criterion of indexing (Rolling, 1981). Much work has been done on describing the statistical properties of folksonomies, such as tag distribution and co-occurrences (Halpin et al., 2007; Sigurbjšrnsson et al., 2008; Sood et al., 2007), but to our knowledge there has been none on assessing the actual quality of tags. How well do human taggers perform? How consistent are they with each other? One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. Mishne, 2006; Sood et al., 2007; Heymann et al., 2008). Interestingly, the blooming research on automatic tagging has so far not been connected to work on keyphrase extraction (e.g. Frank et al., 1999; Turney, 2003; Hulth, 2004), which can be used as a tool for the same task (note: we use tag and key</context>
</contexts>
<marker>Sood, Hammond, Owsley, Birnbaum, 2007</marker>
<rawString>Sood, S., K. Hammond, S. Owsley, and L. Birnbaum. 2007. TagAssist: Automatic tag suggestion for blog posts. of Int. Conf. on Weblogs and Social Media, Boulder, Colorado. Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Coherent keyphrase extraction via web mining.</title>
<date>2003</date>
<booktitle>In Proc. of the 18th Int. Joint Conf. on Artificial Intelligence,</booktitle>
<pages>434--439</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>Acapulco,</location>
<contexts>
<context position="2396" citStr="Turney, 2003" startWordPosition="367" endWordPosition="368">stribution and co-occurrences (Halpin et al., 2007; Sigurbjšrnsson et al., 2008; Sood et al., 2007), but to our knowledge there has been none on assessing the actual quality of tags. How well do human taggers perform? How consistent are they with each other? One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. Mishne, 2006; Sood et al., 2007; Heymann et al., 2008). Interestingly, the blooming research on automatic tagging has so far not been connected to work on keyphrase extraction (e.g. Frank et al., 1999; Turney, 2003; Hulth, 2004), which can be used as a tool for the same task (note: we use tag and keyphrase as synonyms). Instead of simple heuristics based on term frequencies and co-occurrence of tags, keyphrase extraction methods apply machine learning to determine typical distributions of properties common to manually assigned phrases, and can include analysis of semantic relations between candidate tags (Turney, 2003). How well do state-of-the-art keyphrase extraction systems perform compared to simple tagging techniques? How consistent are they with human taggers? These are questions we address in thi</context>
<context position="16370" citStr="Turney (2003)" startWordPosition="2724" endWordPosition="2725"> (2006) automatically suggest tags previously assigned to similar documents. However, in Maui (as in Kea) this feature is just one component of the overall model. Thus if a candidate never appears as a keyphrase in the training corpus, it can still be extracted if its other feature values are significant enough. 4. Phrase length is measured in words. Generally speaking, the longer the phrase, the more specific it is. Training captures and quantifies the specificity preference in a given training corpus. 5. Node degree quantifies the semantic relatedness of a candidate tag to other candidates. Turney (2003) computes semantic relatedness using search engine statistics. Instead, following Medelyan et al. (2008), we utilize Wikipedia hyperlinks for this task. We first map each candidate phrase to its most common Wikipedia page. For example, the word Jaguar appears as a link anchor in Wikipedia 927 times. In 466 cases it links to the article Jaguar cars, thus the commonness of this mapping is 0.5. In 203 cases it links to the animal description, a commonness of 2 http://wikipedia-miner.sourceforge.net/ 1321 0.22. We compute the node degree of the corresponding Wikipedia article as the number of hype</context>
</contexts>
<marker>Turney, 2003</marker>
<rawString>Turney, P. D. 2003. Coherent keyphrase extraction via web mining. In Proc. of the 18th Int. Joint Conf. on Artificial Intelligence, Acapulco, Mexico. pp. 434–439. San Francisco, CA: Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Xu</author>
<author>Y Fu</author>
<author>J Mao</author>
<author>D Su</author>
</authors>
<title>Towards the Semantic Web: Collaborative tag suggestions.</title>
<date>2006</date>
<booktitle>In Proc. Collaborative Web Tagging Workshop at the Int. Joint Conf. on Artificial Intelligence,</booktitle>
<location>Stockholm, Sweden.</location>
<contexts>
<context position="11261" citStr="Xu et al. (2006)" startWordPosition="1877" endWordPosition="1880">rld, networks, dynamics} consist of the two tags {network, small world}, and the consistency is 2×2/(3+4) = 0.57. To compute the overall consistency of a particular indexer, this figure is averaged over all documents and co-indexers. There were no cases where the same user reassigned tags to the same articles, so computing intra-tagger consistency, although interesting, was not impossible. To our knowledge, traditional indexing consistency metrics have not yet been applied to collaboratively tagged data. However, experiments on determining tagging quality do follow the same idea. For example, Xu et al. (2006) define an authority metric that assigns high scores to those users who match other users’ choices on the same documents, in order to eliminate spammers. 2.3 Consistency of CiteULike taggers In the collection of 180 documents tagged by 332 users described in Section 3.1, each tagger has 18 co-taggers on average, ranging from 2 to 129, and has indexed 1 to 25 documents. For each user we compute the consistency with all other users who tagged the same document. Consistency is then averaged across documents. We found that the distribution of per-user consistency resembles a power law with a few u</context>
</contexts>
<marker>Xu, Fu, Mao, Su, 2006</marker>
<rawString>Xu, Z., Fu, Y., Mao, J., and D. Su. 2006. Towards the Semantic Web: Collaborative tag suggestions. In Proc. Collaborative Web Tagging Workshop at the Int. Joint Conf. on Artificial Intelligence, Stockholm, Sweden.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>