<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000676">
<title confidence="0.995528">
An Approach to Reducing Annotation Costs for BioNLP
</title>
<author confidence="0.994976">
Michael Bloodgood
</author>
<affiliation confidence="0.9932575">
Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.927228">
Newark, DE 19716
</address>
<email confidence="0.999496">
bloodgoo@cis.udel.edu
</email>
<sectionHeader confidence="0.999824" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999832">
There is a broad range of BioNLP tasks for which
active learning (AL) can significantly reduce anno-
tation costs and a specific AL algorithm we have
developed is particularly effective in reducing an-
notation costs for these tasks. We have previously
developed an AL algorithm called ClosestInitPA
that works best with tasks that have the following
characteristics: redundancy in training material,
burdensome annotation costs, Support Vector Ma-
chines (SVMs) work well for the task, and imbal-
anced datasets (i.e. when set up as a binary
classification problem, one class is substantially
rarer than the other). Many BioNLP tasks have
these characteristics and thus our AL algorithm is a
natural approach to apply to BioNLP tasks.
</bodyText>
<sectionHeader confidence="0.988387" genericHeader="categories and subject descriptors">
2 Active Learning Algorithm
</sectionHeader>
<bodyText confidence="0.999543611111111">
ClosestInitPA uses SVMs as its base learner. This
fits well with many BioNLP tasks where SVMs
deliver high performance (Giuliano et al., 2006;
Lee et al., 2004). ClosestInitPA is based on the
strategy of selecting the points which are closest to
the current model’s hyperplane (Tong and Koller,
2002) for human annotation. ClosestInitPA works
best in situations with imbalanced data, which is
often the case for BioNLP tasks. For example, in
the AIMed dataset annotated with protein-protein
interactions, the percentage of pairs of proteins in
the same sentence that are annotated as interacting
is only 17.6%.
SVMs (Vapnik, 1998) are learning systems that
learn linear functions for classification. A state-
ment of the optimization problem solved by soft-
margin SVMs that enables the use of asymmetric
cost factors is the following:
</bodyText>
<equation confidence="0.9434545">
Minimize: 1  ||w ||2 + +
C ∑ ξi + C− ∑
</equation>
<author confidence="0.272117">
K. Vijay-Shanker
</author>
<affiliation confidence="0.712932">
Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.591799">
Newark, DE 19716
</address>
<email confidence="0.809966">
vijay@cis.udel.edu
</email>
<bodyText confidence="0.7792592">
Subject to: ∀ k : yk [w� ⋅ x�k + b] ≥ 1− ξk (2)
where (w , b)
� represents the hyperplane that is
learned, x�k is the feature vector for example k, yk
in {+1,-1} is the label for example k,
</bodyText>
<equation confidence="0.712052">
ξk = max(0,1− yk[w�⋅ x�k + b]) is the slack vari-
</equation>
<bodyText confidence="0.999854545454545">
able for example k, and C+ and C- are user-defined
cost factors that trade off separating the data with a
large margin and misclassifying training examples.
Let PA=C+/C-. PA stands for “positive amplifi-
cation.” We use this term because as the PA is in-
creased, the importance of positive examples is
amplified. ClosestInitPA is described in Figure 3.
We have previously shown that setting PA based
on a small initial set of data outperforms the more
obvious approach of using the current labeled data
to estimate PA.
</bodyText>
<figureCaption confidence="0.996701">
Figure 3. ClosestInitPA algorithm.
</figureCaption>
<bodyText confidence="0.9995515">
We have previously developed a stopping crite-
rion called staticPredictions that is based on stop-
ping when we detect that the predictions of our
models on some unlabeled data have stabilized. All
of the automatic stopping points in our results are
determined using staticPredictions.
</bodyText>
<figure confidence="0.9374925">
Initialization:
• L = small initial set of labeled data
• U = large pool of unlabeled data
PA = # neg examples in L
# pos examples in L
Loop until stopping criterion is met:
</figure>
<listItem confidence="0.8218094">
1. Train an SVM with parameters C+
and C- set such that C+/C- = PA.
2. batch = select k points from U that
are closest to the hyperplane learned
in step 1.
</listItem>
<equation confidence="0.884702875">
U = U – batch
L = L U batch
2
i : y= +1 j: y = − 1
i j
(1)
j
ξ
</equation>
<page confidence="0.962509">
104
</page>
<note confidence="0.5982485">
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 104–105,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<figure confidence="0.72248325">
3 Experiments
100% 67220 90.28 90.28
AutoStopPoint 8720 85.41 89.24
Protein-Protein Interaction Extraction: We used
</figure>
<bodyText confidence="0.95307125">
the AImed corpus, which was previously used for
training protein interaction extraction systems in
(Giuliano et al., 2006). We cast RE as a binary
classification task as in (Giuliano et al., 2006).
We do 10-fold cross validation and use what is
referred to in (Giuliano et al., 2006) as the KGC
kernel with SVMlight (Joachims, 1999) in our ex-
periments. Table 1 reports the results.
</bodyText>
<table confidence="0.970033428571429">
StoppingPoint Average # Labels F Measure
Random AL
20% 1012 48.33 54.34
30% 1516 49.76 54.52
40% 2022 53.11 56.39
100% 5060 57.54 57.54
AutoStopPoint 1562 51.25 55.34
</table>
<tableCaption confidence="0.931225">
Table 1. AImed Stopping Point Performance. “AutoS-
topPoint” is when the stopping criterion says to stop.
</tableCaption>
<bodyText confidence="0.967996833333333">
Medline Text Classification: We use the Oh-
sumed corpus (Hersh, 1994) and a linear kernel
with SVMlight with binary features for each word
that occurs in the training data at least three times.
Results for the five largest categories for one ver-
sus the rest classification are in Table 2.
</bodyText>
<table confidence="0.990518714285714">
StoppingPoint Average # Labels F Measure
Random AL
20% 1260 49.99 61.49
30% 1880 54.18 62.72
40% 2500 57.46 63.75
100% 6260 65.75 65.75
AutoStopPoint 1204 47.06 60.73
</table>
<tableCaption confidence="0.9458115">
Table 2. Ohsumed stopping point performance. “AutoS-
topPoint” is when the stopping criterion says to stop.
</tableCaption>
<bodyText confidence="0.997587727272727">
GENIA NER: We assume a two-phase model
(Lee et al., 2004) where boundary identification of
named entities is performed in the first phase and
the entities are classified in the second phase. As in
the semantic classification evaluation of (Lee et al.,
2004), we assume that boundary identification has
been performed. We use features based on those
from (Lee et al., 2004), a one versus the rest setup
and 10-fold cross validation. Tables 3-5 show the
results for the three most common types in
GENIA.
</bodyText>
<table confidence="0.995999">
StoppingPoint Average # Labels F Measure
Random AL
20% 13440 86.78 90.16
30% 20120 87.81 90.27
40% 26900 88.55 90.32
</table>
<tableCaption confidence="0.974257">
Table 3. Protein stopping points performance. “AutoS-
topPoint” is when the stopping criterion says to stop.
</tableCaption>
<table confidence="0.996787285714286">
StoppingPoint Average # Labels F Measure
Random AL
20% 13440 79.85 82.06
30% 20120 80.40 81.98
40% 26900 80.85 81.84
100% 67220 81.68 81.68
AutoStopPoint 7060 78.35 82.29
</table>
<tableCaption confidence="0.9834615">
Table 4. DNA stopping points performance. “AutoS-
topPoint” is when the stopping criterion says to stop.
</tableCaption>
<table confidence="0.976201571428571">
StoppingPoint Average # Labels F Measure
Random AL
20% 13440 84.01 86.76
30% 20120 84.62 86.63
40% 26900 85.25 86.45
100% 67220 86.08 86.08
AutoStopPoint 4200 81.32 86.31
</table>
<tableCaption confidence="0.917537">
Table 5. Cell Type stopping points performance. “Au-
toStopPoint” is when the stopping criterion says to stop.
</tableCaption>
<sectionHeader confidence="0.999381" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9997518">
ClosestInitPA is well suited to many BioNLP
tasks. In experiments, the annotation savings are
practically significant for extracting protein-protein
interactions, classifying Medline text, and perform-
ing biomedical named entity recognition.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99974605">
Claudio Giuliano, Alberto Lavelli, and Lorenza Roma-
no. 2006. Exploiting Shallow Linguistic Information
for Relation Extraction from Biomedical Literature.
In Proceedings of the EACL, 401-408.
William Hersh, Buckley, C., Leone, T.J., and Hickman,
D. (1994). Ohsumed: an interactive retrieval evalua-
tion and new large text collection for research. ACM
SIGIR.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning, MIT-Press, 169-184.
Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, and
Hae-Chang Rim. 2004. Biomedical named entity
recognition using two-phase model based on SVMs.
Journal of Biomedical Informatics, Vol 37, 436–447.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text
classification. JMLR 2: 45-66.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley &amp; Sons, New York, NY, USA.
</reference>
<page confidence="0.999016">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902224">
<title confidence="0.999759">An Approach to Reducing Annotation Costs for BioNLP</title>
<author confidence="0.993883">Michael</author>
<affiliation confidence="0.99691">Computer and Information University of</affiliation>
<address confidence="0.932819">Newark, DE 19716</address>
<email confidence="0.975336">bloodgoo@cis.udel.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alberto Lavelli</author>
<author>Lorenza Romano</author>
</authors>
<title>Exploiting Shallow Linguistic Information for Relation Extraction from Biomedical Literature.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>401--408</pages>
<contexts>
<context position="1078" citStr="Giuliano et al., 2006" startWordPosition="161" endWordPosition="164">alled ClosestInitPA that works best with tasks that have the following characteristics: redundancy in training material, burdensome annotation costs, Support Vector Machines (SVMs) work well for the task, and imbalanced datasets (i.e. when set up as a binary classification problem, one class is substantially rarer than the other). Many BioNLP tasks have these characteristics and thus our AL algorithm is a natural approach to apply to BioNLP tasks. 2 Active Learning Algorithm ClosestInitPA uses SVMs as its base learner. This fits well with many BioNLP tasks where SVMs deliver high performance (Giuliano et al., 2006; Lee et al., 2004). ClosestInitPA is based on the strategy of selecting the points which are closest to the current model’s hyperplane (Tong and Koller, 2002) for human annotation. ClosestInitPA works best in situations with imbalanced data, which is often the case for BioNLP tasks. For example, in the AIMed dataset annotated with protein-protein interactions, the percentage of pairs of proteins in the same sentence that are annotated as interacting is only 17.6%. SVMs (Vapnik, 1998) are learning systems that learn linear functions for classification. A statement of the optimization problem s</context>
<context position="3791" citStr="Giuliano et al., 2006" startWordPosition="632" endWordPosition="635">in an SVM with parameters C+ and C- set such that C+/C- = PA. 2. batch = select k points from U that are closest to the hyperplane learned in step 1. U = U – batch L = L U batch 2 i : y= +1 j: y = − 1 i j (1) j ξ 104 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 104–105, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 3 Experiments 100% 67220 90.28 90.28 AutoStopPoint 8720 85.41 89.24 Protein-Protein Interaction Extraction: We used the AImed corpus, which was previously used for training protein interaction extraction systems in (Giuliano et al., 2006). We cast RE as a binary classification task as in (Giuliano et al., 2006). We do 10-fold cross validation and use what is referred to in (Giuliano et al., 2006) as the KGC kernel with SVMlight (Joachims, 1999) in our experiments. Table 1 reports the results. StoppingPoint Average # Labels F Measure Random AL 20% 1012 48.33 54.34 30% 1516 49.76 54.52 40% 2022 53.11 56.39 100% 5060 57.54 57.54 AutoStopPoint 1562 51.25 55.34 Table 1. AImed Stopping Point Performance. “AutoStopPoint” is when the stopping criterion says to stop. Medline Text Classification: We use the Ohsumed corpus (Hersh, 1994) </context>
</contexts>
<marker>Giuliano, Lavelli, Romano, 2006</marker>
<rawString>Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Exploiting Shallow Linguistic Information for Relation Extraction from Biomedical Literature. In Proceedings of the EACL, 401-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Hersh</author>
<author>C Buckley</author>
<author>T J Leone</author>
<author>D Hickman</author>
</authors>
<title>Ohsumed: an interactive retrieval evaluation and new large text collection for research.</title>
<date>1994</date>
<publisher>ACM SIGIR.</publisher>
<marker>Hersh, Buckley, Leone, Hickman, 1994</marker>
<rawString>William Hersh, Buckley, C., Leone, T.J., and Hickman, D. (1994). Ohsumed: an interactive retrieval evaluation and new large text collection for research. ACM SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<date>1999</date>
<booktitle>Making large-Scale SVM Learning Practical. Advances in Kernel Methods -Support Vector Learning, MIT-Press,</booktitle>
<pages>169--184</pages>
<contexts>
<context position="4001" citStr="Joachims, 1999" startWordPosition="672" endWordPosition="673">BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 104–105, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 3 Experiments 100% 67220 90.28 90.28 AutoStopPoint 8720 85.41 89.24 Protein-Protein Interaction Extraction: We used the AImed corpus, which was previously used for training protein interaction extraction systems in (Giuliano et al., 2006). We cast RE as a binary classification task as in (Giuliano et al., 2006). We do 10-fold cross validation and use what is referred to in (Giuliano et al., 2006) as the KGC kernel with SVMlight (Joachims, 1999) in our experiments. Table 1 reports the results. StoppingPoint Average # Labels F Measure Random AL 20% 1012 48.33 54.34 30% 1516 49.76 54.52 40% 2022 53.11 56.39 100% 5060 57.54 57.54 AutoStopPoint 1562 51.25 55.34 Table 1. AImed Stopping Point Performance. “AutoStopPoint” is when the stopping criterion says to stop. Medline Text Classification: We use the Ohsumed corpus (Hersh, 1994) and a linear kernel with SVMlight with binary features for each word that occurs in the training data at least three times. Results for the five largest categories for one versus the rest classification are in </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-Scale SVM Learning Practical. Advances in Kernel Methods -Support Vector Learning, MIT-Press, 169-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ki-Joong Lee</author>
<author>Young-Sook Hwang</author>
<author>Seonho Kim</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Biomedical named entity recognition using two-phase model based on SVMs.</title>
<date>2004</date>
<journal>Journal of Biomedical Informatics, Vol</journal>
<volume>37</volume>
<pages>436--447</pages>
<contexts>
<context position="1097" citStr="Lee et al., 2004" startWordPosition="165" endWordPosition="168">t works best with tasks that have the following characteristics: redundancy in training material, burdensome annotation costs, Support Vector Machines (SVMs) work well for the task, and imbalanced datasets (i.e. when set up as a binary classification problem, one class is substantially rarer than the other). Many BioNLP tasks have these characteristics and thus our AL algorithm is a natural approach to apply to BioNLP tasks. 2 Active Learning Algorithm ClosestInitPA uses SVMs as its base learner. This fits well with many BioNLP tasks where SVMs deliver high performance (Giuliano et al., 2006; Lee et al., 2004). ClosestInitPA is based on the strategy of selecting the points which are closest to the current model’s hyperplane (Tong and Koller, 2002) for human annotation. ClosestInitPA works best in situations with imbalanced data, which is often the case for BioNLP tasks. For example, in the AIMed dataset annotated with protein-protein interactions, the percentage of pairs of proteins in the same sentence that are annotated as interacting is only 17.6%. SVMs (Vapnik, 1998) are learning systems that learn linear functions for classification. A statement of the optimization problem solved by softmargin</context>
<context position="4940" citStr="Lee et al., 2004" startWordPosition="828" endWordPosition="831">edline Text Classification: We use the Ohsumed corpus (Hersh, 1994) and a linear kernel with SVMlight with binary features for each word that occurs in the training data at least three times. Results for the five largest categories for one versus the rest classification are in Table 2. StoppingPoint Average # Labels F Measure Random AL 20% 1260 49.99 61.49 30% 1880 54.18 62.72 40% 2500 57.46 63.75 100% 6260 65.75 65.75 AutoStopPoint 1204 47.06 60.73 Table 2. Ohsumed stopping point performance. “AutoStopPoint” is when the stopping criterion says to stop. GENIA NER: We assume a two-phase model (Lee et al., 2004) where boundary identification of named entities is performed in the first phase and the entities are classified in the second phase. As in the semantic classification evaluation of (Lee et al., 2004), we assume that boundary identification has been performed. We use features based on those from (Lee et al., 2004), a one versus the rest setup and 10-fold cross validation. Tables 3-5 show the results for the three most common types in GENIA. StoppingPoint Average # Labels F Measure Random AL 20% 13440 86.78 90.16 30% 20120 87.81 90.27 40% 26900 88.55 90.32 Table 3. Protein stopping points perfo</context>
</contexts>
<marker>Lee, Hwang, Kim, Rim, 2004</marker>
<rawString>Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, and Hae-Chang Rim. 2004. Biomedical named entity recognition using two-phase model based on SVMs. Journal of Biomedical Informatics, Vol 37, 436–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2002</date>
<journal>JMLR</journal>
<volume>2</volume>
<pages>45--66</pages>
<contexts>
<context position="1237" citStr="Tong and Koller, 2002" startWordPosition="187" endWordPosition="190">Vector Machines (SVMs) work well for the task, and imbalanced datasets (i.e. when set up as a binary classification problem, one class is substantially rarer than the other). Many BioNLP tasks have these characteristics and thus our AL algorithm is a natural approach to apply to BioNLP tasks. 2 Active Learning Algorithm ClosestInitPA uses SVMs as its base learner. This fits well with many BioNLP tasks where SVMs deliver high performance (Giuliano et al., 2006; Lee et al., 2004). ClosestInitPA is based on the strategy of selecting the points which are closest to the current model’s hyperplane (Tong and Koller, 2002) for human annotation. ClosestInitPA works best in situations with imbalanced data, which is often the case for BioNLP tasks. For example, in the AIMed dataset annotated with protein-protein interactions, the percentage of pairs of proteins in the same sentence that are annotated as interacting is only 17.6%. SVMs (Vapnik, 1998) are learning systems that learn linear functions for classification. A statement of the optimization problem solved by softmargin SVMs that enables the use of asymmetric cost factors is the following: Minimize: 1 ||w ||2 + + C ∑ ξi + C− ∑ K. Vijay-Shanker Computer and </context>
</contexts>
<marker>Tong, Koller, 2002</marker>
<rawString>Simon Tong and Daphne Koller. 2002. Support vector machine active learning with applications to text classification. JMLR 2: 45-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1567" citStr="Vapnik, 1998" startWordPosition="239" endWordPosition="240">s SVMs as its base learner. This fits well with many BioNLP tasks where SVMs deliver high performance (Giuliano et al., 2006; Lee et al., 2004). ClosestInitPA is based on the strategy of selecting the points which are closest to the current model’s hyperplane (Tong and Koller, 2002) for human annotation. ClosestInitPA works best in situations with imbalanced data, which is often the case for BioNLP tasks. For example, in the AIMed dataset annotated with protein-protein interactions, the percentage of pairs of proteins in the same sentence that are annotated as interacting is only 17.6%. SVMs (Vapnik, 1998) are learning systems that learn linear functions for classification. A statement of the optimization problem solved by softmargin SVMs that enables the use of asymmetric cost factors is the following: Minimize: 1 ||w ||2 + + C ∑ ξi + C− ∑ K. Vijay-Shanker Computer and Information Sciences University of Delaware Newark, DE 19716 vijay@cis.udel.edu Subject to: ∀ k : yk [w� ⋅ x�k + b] ≥ 1− ξk (2) where (w , b) � represents the hyperplane that is learned, x�k is the feature vector for example k, yk in {+1,-1} is the label for example k, ξk = max(0,1− yk[w�⋅ x�k + b]) is the slack variable for exa</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik. 1998. Statistical Learning Theory. John Wiley &amp; Sons, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>