<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9880265">
A log-linear model with an n-gram reference distribution for accurate HPSG
parsing
</title>
<author confidence="0.990201">
Takashi Ninomiya
</author>
<affiliation confidence="0.994035">
Information Technology Center
University of Tokyo
</affiliation>
<email confidence="0.982299">
ninomi@r.dl.itc.u-tokyo.ac.jp
</email>
<author confidence="0.995908">
Takuya Matsuzaki
</author>
<affiliation confidence="0.998504">
Department of Computer Science
University of Tokyo
</affiliation>
<email confidence="0.989703">
matuzaki@is.s.u-tokyo.ac.jp
</email>
<author confidence="0.995525">
Yusuke Miyao
</author>
<affiliation confidence="0.998498">
Department of Computer Science
University of Tokyo
</affiliation>
<email confidence="0.984171">
yusuke@is.s.u-tokyo.ac.jp
</email>
<author confidence="0.99007">
Jun’ichi Tsujii
</author>
<affiliation confidence="0.9998315">
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
</affiliation>
<address confidence="0.570547">
NaCTeM (National Center for Text Mining)
</address>
<email confidence="0.974916">
tsujii@is.s.u-tokyo.ac.jp
</email>
<note confidence="0.65923">
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
</note>
<sectionHeader confidence="0.930317" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946285714286">
This paper describes a log-linear model with
an n-gram reference distribution for accurate
probabilistic HPSG parsing. In the model,
the n-gram reference distribution is simply
defined as the product of the probabilities
of selecting lexical entries, which are pro-
vided by the discriminative method with ma-
chine learning features of word and POS
n-gram as defined in the CCG/HPSG/CDG
supertagging. Recently, supertagging be-
comes well known to drastically improve
the parsing accuracy and speed, but su-
pertagging techniques were heuristically in-
troduced, and hence the probabilistic mod-
els for parse trees were not well defined.
We introduce the supertagging probabilities
as a reference distribution for the log-linear
model of the probabilistic HPSG. This is the
first model which properly incorporates the
supertagging probabilities into parse tree’s
probabilistic model.
</bodyText>
<sectionHeader confidence="0.995168" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869482758621">
For the last decade, fast, accurate and wide-coverage
parsing for real-world text has been pursued in
sophisticated grammar formalisms, such as head-
driven phrase structure grammar (HPSG) (Pollard
and Sag, 1994), combinatory categorial grammar
(CCG) (Steedman, 2000) and lexical function gram-
mar (LFG) (Bresnan, 1982). They are preferred
because they give precise and in-depth analyses
for explaining linguistic phenomena, such as pas-
sivization, control verbs and relative clauses. The
main difficulty of developing parsers in these for-
malisms was how to model a well-defined proba-
bilistic model for graph structures such as feature
structures. This was overcome by a probabilistic
model which provides probabilities of discriminat-
ing a correct parse tree among candidates of parse
trees in a log-linear model or maximum entropy
model (Berger et al., 1996) with many features for
parse trees (Abney, 1997; Johnson et al., 1999; Rie-
zler et al., 2000; Malouf and van Noord, 2004; Ka-
plan et al., 2004; Miyao and Tsujii, 2005). Follow-
ing this discriminative approach, techniques for effi-
ciency were investigated for estimation (Geman and
Johnson, 2002; Miyao and Tsujii, 2002; Malouf and
van Noord, 2004) and parsing (Clark and Curran,
2004b; Clark and Curran, 2004a; Ninomiya et al.,
2005).
An interesting approach to the problem of parsing
efficiency was using supertagging (Clark and Cur-
</bodyText>
<page confidence="0.443398">
60
</page>
<note confidence="0.9250075">
Proceedings of the 10th Conference on Parsing Technologies, pages 60–68,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
ran, 2004b; Clark and Curran, 2004a; Wang, 2003;
Wang and Harper, 2004; Nasr and Rambow, 2004;
Ninomiya et al., 2006; Foth et al., 2006; Foth and
Menzel, 2006), which was originally developed for
</note>
<bodyText confidence="0.997613483333334">
lexicalized tree adjoining grammars (LTAG) (Ban-
galore and Joshi, 1999). Supertagging is a process
where words in an input sentence are tagged with
‘supertags,’ which are lexical entries in lexicalized
grammars, e.g., elementary trees in LTAG, lexical
categories in CCG, and lexical entries in HPSG. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated in
the case of a CCG parser (Clark and Curran, 2004a)
with the result of a drastic improvement in the pars-
ing speed. Wang and Harper (2004) also demon-
strated the effects of supertagging with a statisti-
cal constraint dependency grammar (CDG) parser
by showing accuracy as high as the state-of-the-art
parsers, and Foth et al. (2006) and Foth and Menzel
(2006) reported that accuracy was significantly im-
proved by incorporating the supertagging probabili-
ties into manually tuned Weighted CDG. Ninomiya
et al. (2006) showed the parsing model using only
supertagging probabilities could achieve accuracy as
high as the probabilistic model for phrase structures.
This means that syntactic structures are almost de-
termined by supertags as is claimed by Bangalore
and Joshi (1999). However, supertaggers themselves
were heuristically used as an external tagger. They
filter out unlikely lexical entries just to help parsing
(Clark and Curran, 2004a), or the probabilistic mod-
els for phrase structures were trained independently
of the supertagger’s probabilistic models (Wang and
Harper, 2004; Ninomiya et al., 2006). In the case of
supertagging of Weighted CDG (Foth et al., 2006),
parameters for Weighted CDG are manually tuned,
i.e., their model is not a well-defined probabilistic
model.
We propose a log-linear model for probabilistic
HPSG parsing in which the supertagging probabil-
ities are introduced as a reference distribution for
the probabilistic HPSG. The reference distribution is
simply defined as the product of the probabilities of
selecting lexical entries, which are provided by the
discriminative method with machine learning fea-
tures of word and part-of-speech (POS) n-gram as
defined in the CCG/HPSG/CDG supertagging. This
is the first model which properly incorporates the su-
pertagging probabilities into parse tree’s probabilis-
tic model. We compared our model with the proba-
bilistic model for phrase structures (Miyao and Tsu-
jii, 2005). This model uses word and POS unigram
for its reference distribution, i.e., the probabilities of
unigram supertagging. Our model can be regarded
as an extension of a unigram reference distribution
to an n-gram reference distribution with features that
are used in supertagging. We also compared with a
probabilistic model in (Ninomiya et al., 2006). The
probabilities of their model are defined as the prod-
uct of probabilities of supertagging and probabilities
of the probabilistic model for phrase structures, but
their model was trained independently of supertag-
ging probabilities, i.e., the supertagging probabili-
ties are not used for reference distributions.
</bodyText>
<sectionHeader confidence="0.503311" genericHeader="introduction">
2 HPSG and probabilistic models
</sectionHeader>
<bodyText confidence="0.99578747826087">
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical entries
express word-specific characteristics. The structures
of sentences are explained using combinations of
schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
An example of HPSG parsing of the sentence
“Spring has come” is shown in Figure 1. First,
each of the lexical entries for “has” and “come”
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly applying
schemata to lexical/phrasal signs. Finally, the parse
result is output as a phrasal sign that dominates the
sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
(L, R), where
</bodyText>
<equation confidence="0.82859875">
L = {l = (w, F)Jw E W, F E FJ is a set of
lexical entries, and
R is a set of schemata; i.e., r E R is a partial
function: F x F — F.
</equation>
<bodyText confidence="0.9987345">
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
</bodyText>
<figure confidence="0.778801">
61
HEAD verb
SUBJ &lt; &gt;
COMPS &lt; &gt;
</figure>
<figureCaption confidence="0.915066">
Figure 1: HPSG parsing.
</figureCaption>
<equation confidence="0.96862925">
SUBJ &lt; &gt;
SUBJ &lt; &gt; SUBJ &lt; &gt;
COMPS &lt; &gt; COMPS &lt; 2
2 SUBJ &lt; &gt;
</equation>
<bodyText confidence="0.999656545454545">
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries deter-
mine the dominant syntactic structures.
Previous studies (Abney, 1997; Johnson et al.,
1999; Riezler et al., 2000; Malouf and van Noord,
2004; Kaplan et al., 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model or
maximum entropy model (Berger et al., 1996). The
probability that a parse result T is assigned to a
given sentence w = (w1, ... , wn) is
</bodyText>
<subsectionHeader confidence="0.739881">
(Probabilistic HPSG)
</subsectionHeader>
<bodyText confidence="0.999991333333334">
to sentence w. Because the number of parse can-
didates is exponentially related to the length of the
sentence, the estimation is intractable for long sen-
tences. To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T |w). Miyao and Tsujii (2005) also introduced a
preliminary probabilistic model p0(T|w) whose es-
timation does not require the parsing of a treebank.
This model is introduced as a reference distribution
(Jelinek, 1998; Johnson and Riezler, 2000) of the
probabilistic HPSG model; i.e., the computation of
parse trees given low probabilities by the model is
omitted in the estimation stage (Miyao and Tsujii,
2005), or a probabilistic model can be augmented
by several distributions estimated from the larger
and simpler corpus (Johnson and Riezler, 2000). In
(Miyao and Tsujii, 2005), p0(T|w) is defined as the
product of probabilities of selecting lexical entries
with word and POS unigram features:
</bodyText>
<equation confidence="0.8274914">
(Miyao and Tsujii (2005)’s model)
ÃX !
puniref(T |w) = p0(T |w) Zw 1 exp λufu(T ) u
p0(T&apos;|w) expXλufu(T )!
Ã u
</equation>
<figure confidence="0.99157111627907">
HEAD verb
SUBJ &lt;
HEAD noun
COMPS &lt; &gt;
&gt;
COMPS &lt; &gt;
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt; &gt;
2
HEAD verb
2 SUBJ &lt; &gt;
1
COMPS &lt; &gt;
come
has
head-comp
Spring has come
subject-head
HEAD verb
1
COMPS &lt; &gt;
head-comp
HEAD verb
1
COMPS &lt; &gt;
1 SUBJ &lt; &gt;
1&gt;
1
HEAD noun
SUBJ &lt; &gt;
COMPS &lt; &gt;
Spring
XZw =
T1
ÃX ! p0(T|w) = Yn p(li|wi),
1 i=1
phpsg(T |w) = exp λufu(T )
Zw
u
exp ÃXλu fu (T&apos;)J
u J
</figure>
<bodyText confidence="0.986751161290323">
where λu is a model parameter, fu is a feature func-
tion that represents a characteristic of parse tree T,
and Z, is the sum over the set of all possible parse
trees for the sentence. Intuitively, the probability
is defined as the normalized product of the weights
exp(λu) when a characteristic corresponding to fu
appears in parse result T. The model parameters, λu,
are estimated using numerical optimization methods
(Malouf, 2002) to maximize the log-likelihood of
the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the compu-
tation of p(T|w) for all parse candidates assigned
HEAD noun
HEAD verb
where li is a lexical entry assigned to word wi in
T and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model with
other two types of probabilistic models using a su-
pertagger (Ninomiya et al., 2006). The first one is
the simplest probabilistic model, which is defined
with only the probabilities of lexical entry selec-
tion. It is defined simply as the product of the prob-
abilities of selecting all lexical entries in the sen-
tence; i.e., the model does not use the probabilities
of phrase structures like the probabilistic models ex-
plained above. Given a set of lexical entries, L, a
sentence, w = (w1,... , wn), and the probabilistic
model of lexical entry selection, p(li E L|w, i), the
first model is formally defined as follows:
</bodyText>
<figure confidence="0.997391108695652">
XZw =
T1
62
HEAD noun
1 SUBJ &lt;&gt;
COMPS &lt;&gt;
froot= &lt;S, has, VBZ,
Spring/NN has/VBZ come/VBN
HEAD verb
SUBJ &lt;&gt;
COMPS &lt;&gt;
subject-head
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt; &gt;
2
flex= &lt;spring, NN,
HEAD verb
SUBJ &lt;NP&gt;
COMPS &lt;VP&gt;
&gt;
fbinary=
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt;&gt;
head-comp
HEAD verb
2 SUBJ &lt; &gt;
1
COMPS &lt;&gt;
head-comp, 1, 0,
1, VP, has, VBZ,
1, VP, come, VBN,
HEAD noun
SUBJ &lt;&gt;
COMPS &lt;&gt;
&gt;
HEAD verb
SUBJ &lt;NP&gt;
,
COMPS &lt;VP&gt;
HEAD verb
SUBJ &lt;NP&gt;
COMPS &lt;&gt;
</figure>
<figureCaption confidence="0.994521">
Figure 2: Example of features.
</figureCaption>
<table confidence="0.627087333333333">
(Ninomiya et al. (2006)’s model 1)
c
* r, d, c, +
</table>
<construct confidence="0.424311">
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
(r, sy, hw, hp, hl)
(sy, hw, hp, hl)
</construct>
<equation confidence="0.955855">
(wi, pi, li)
À
wi−1, wi, wi+1,
pi−2, pi−1, pi, pi+1, pi+2
</equation>
<bodyText confidence="0.840689">
r name of the applied schema
d distance between the head words of the daughters
whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
</bodyText>
<tableCaption confidence="0.966686">
Table 1: Feature templates.
</tableCaption>
<equation confidence="0.940161285714286">
fbinary =
funary =
froot =
flex =
¿fsptag =
pmodel1(T |w) = Yn p(li|w, i),
i=1
</equation>
<bodyText confidence="0.9996324">
where li is a lexical entry assigned to word wi in T
and p(li|w, i) is the probability of selecting lexical
entry li for wi.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
</bodyText>
<note confidence="0.408504">
(Probabilistic model of lexical entry selection)
</note>
<equation confidence="0.9969314">
p(li |w, i) = Z. expÃX λufu (1j,
u(la, w, i)!
u
exp ÃX λufu(li, w, i)I
u /
</equation>
<bodyText confidence="0.999945333333333">
where Z,,, is the sum over all possible lexical entries
for the word wi.
The second model is a hybrid model of supertag-
ging and the probabilistic HPSG. The probabilities
are given as the product of Ninomiya et al. (2006)’s
model 1 and the probabilistic HPSG.
</bodyText>
<equation confidence="0.846212">
(Ninomiya et al. (2006)’s model 3)
pmodel3(T |w) = pmodel1(T |w)phpsg(T |w)
</equation>
<bodyText confidence="0.98697105882353">
In the experiments, we compared our model with
Miyao and Tsujii (2005)’s model and Ninomiya et
al. (2006)’s model 1 and 3. The features used in our
model and their model are combinations of the fea-
ture templates listed in Table 1 and Table 2. The
feature templates fbinary and funary are defined for
constituents at binary and unary branches, froot is a
feature template set for the root nodes of parse trees.
flex is a feature template set for calculating the uni-
gram reference distribution and is used in Miyao and
Tsujii (2005)’s model. fsptag is a feature template
set for calculating the probabilities of selecting lex-
ical entries in Ninomiya et al. (2006)’s model 1 and
3. The feature templates in fsptag are word trigrams
and POS 5-grams. An example of features applied
to the parse tree for the sentence “Spring has come”
is shown in Figure 2.
</bodyText>
<sectionHeader confidence="0.812758" genericHeader="method">
3 Probabilistic HPSG with an n-gram
reference distribution
</sectionHeader>
<bodyText confidence="0.999504666666667">
In this section, we propose a probabilistic model
with an n-gram reference distribution for probabilis-
tic HPSG parsing. This is an extension of Miyao
and Tsujii (2005)’s model by replacing the unigram
reference distribution with an n-gram reference dis-
tribution. Our model is formally defined as follows:
</bodyText>
<figure confidence="0.56293825">
XZ. =
l�
63
combinations of feature templates for fbinary
</figure>
<construct confidence="0.768714545454546">
(r, d, c, hw, hp, hl), (r, d, c, hw, hp), (r, d, c, hw, hl),
(r, d, c, sy, hw), (r, c, sp, hw, hp, hl), (r, c, sp, hw, hp),
(r, c, sp, hw, hl), (r, c, sp, sy, hw), (r, d, c, hp, hl),
(r, d, c, hp), (r, d, c, hl), (r, d, c, sy), (r, c, sp, hp, hl),
(r, c, sp, hp), (r, c, sp, hl), (r, c, sp, sy)
combinations of feature templates for funary
(r, hw, hp, hl), (r, hw, hp), (r, hw, hl), (r, sy, hw),
(r, hp, hl), (r, hp), (r, hl), (r, sy)
combinations of feature templates for froot
(hw, hp, hl), (hw, hp), (hw, hl),
(sy, hw), (hp, hl), (hp), (hl), (sy)
</construct>
<equation confidence="0.8663775625">
combinations of feature templates for flex
(wi, pi, li), (pi, li)
combinations of feature templates for fsptag
(wi−1), (wi), (wi+1),
(pi−2), (pi−1), (pi), (pi+1), (pi+2), (pi+3),
(wi−1, wi), (wi, wi+1),
(pi−1, wi), (pi, wi), (pi+1, wi),
(�P„,i, pi+1, pi+2,pi+3), (pi−2, pi−1,pi),
(�p„,i−1, pi, pi+1�„),, (pi, pi+1, pi+2) �„,
(pd-2, pi−1), (pi−1, pi), (pi,pi+1), (pi+1, pi+2)
Table 2: Combinations of feature templates.
(Probabilistic HPSG with an n-gram reference distribution)
pnref (T |w) =
pmodel 1 (T  |w) exp Au fu (T) ÃX
Znre f
u
</equation>
<bodyText confidence="0.9999683125">
In our model, Ninomiya et al. (2006)’s model 1
is used as a reference distribution. The probabilis-
tic model of lexical entry selection and its feature
templates are the same as defined in Ninomiya et al.
(2006)’s model 1.
The formula of our model is the same as Ni-
nomiya et al. (2006)’s model 3. But, their model
is not a probabilistic model with a reference distri-
bution. Both our model and their model consist of
the probabilities for lexical entries (= pmodel1(T lw))
and the probabilities for phrase structures (= the rest
of each formula). The only difference between our
model and their model is the way of how to train
model parameters for phrase structures. In both our
model and their model, the parameters for lexical en-
tries (= the parameters of pmodel1(T 1w)) are first es-
timated from the word and POS sequences indepen-
dently of the parameters for phrase structures. That
is, the estimated parameters for lexical entries are
the same in both models, and hence the probabilities
of pmodel1(T Iw) of both models are the same. Note
that the parameters for lexical entries will never be
updated after this estimation stage; i.e., the parame-
ters for lexical entries are not estimated in the same
time with the parameters for phrase structures. The
difference of our model and their model is the esti-
mation of parameters for phrase structures. In our
model, given the probabilities for lexical entries, the
parameters for phrase structures are estimated so as
to maximize the entire probabilistic model (= the
product of the probabilities for lexical entries and
the probabilities for phrase structures) in the train-
ing corpus. In their model, the parameters for phrase
structures are trained without using the probabili-
ties for lexical entries, i.e., the parameters for phrase
structures are estimated so as to maximize the prob-
abilities for phrase structures only. That is, the pa-
rameters for lexical entries and the parameters for
phrase structures are trained independently in their
model.
Miyao and Tsujii (2005)’s model also uses a ref-
erence distribution, but with word and POS unigram
features, as is explained in the previous section. The
only difference between our model and Miyao and
Tsujii (2005)’s model is that our model uses se-
quences of word and POS tags as n-gram features
for selecting lexical entries in the same way as su-
pertagging does.
</bodyText>
<sectionHeader confidence="0.999206" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.950972461538461">
We evaluated the speed and accuracy of parsing
by using Enju 2.1, the HPSG grammar for English
(Miyao et al., 2005; Miyao and Tsujii, 2005). The
lexicon of the grammar was extracted from Sec-
tions 02-21 of the Penn Treebank (Marcus et al.,
1994) (39,832 sentences). The grammar consisted
of 3,797 lexical entries for 10,536 words1. The prob-
TI
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by apply-
ing lexical rules to observed lexical entries in the HPSG tree-
bank (Nakanishi et al., 2004). The lexicon, however, included
many lexical entries that do not appear in the HPSG treebank.
</bodyText>
<equation confidence="0.9380498">
Znref =
ÃX !
pmodel1(T �|w) exp �ufu(T �) �
u
X
</equation>
<page confidence="0.704175">
64
</page>
<table confidence="0.996429666666667">
No. of tested sentences Total No. of sentences Avg. length of tested sentences
Section 23 2,299 (100.00%) 2,299 22.2
Section 24 1,245 (99.84%) 1,247 23.0
</table>
<tableCaption confidence="0.991846">
Table 3: Statistics of the Penn Treebank.
</tableCaption>
<table confidence="0.999821941176471">
LP Section 23 (Gold POSs) UR UF Avg. time
(%) LR LF UP (%) (%) (ms)
(%) (%) (%)
Miyao and Tsujii (2005)’s model 87.26 86.50 86.88 90.73 89.93 90.33 604
Ninomiya et al. (2006)’s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129
Ninomiya et al. (2006)’s model 3 89.48 88.58 89.02 92.33 91.40 91.86 152
our model 1 89.78 89.28 89.53 92.58 92.07 92.32 234
our model 2 90.03 89.60 89.82 92.82 92.37 92.60 1379
Section 23 (POS tagger)
LP LR LF UP UR UF Avg. time
(%) (%) (%) (%) (%) (%) (ms)
Miyao and Tsujii (2005)’s model 84.96 84.25 84.60 89.55 88.80 89.17 674
Ninomiya et al. (2006)’s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154
Ninomiya et al. (2006)’s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183
Matsuzaki et al. (2007)’s model 86.93 86.47 86.70 - - - 30
our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260
our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821
</table>
<tableCaption confidence="0.999692">
Table 4: Experimental results for Section 23.
</tableCaption>
<bodyText confidence="0.999446882352941">
abilistic models were trained using the same portion
of the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al., 2005) and quick check
(Malouf et al., 2000).
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tuple
(Q, wh, a, wa), where a is the predicate type (e.g.,
adjective, intransitive verb), wh is the head word of
the predicate, a is the argument label (MODARG,
ARG1, ..., ARG4), and wa is the head word of
the argument. Labeled precision (LP)/labeled re-
call (LR) is the ratio of tuples correctly identified
by the parser2. Unlabeled precision (UP)/unlabeled
recall (UR) is the ratio of tuples without the pred-
icate type and the argument label. This evaluation
scheme was the same as used in previous evaluations
of lexicalized grammars (Hockenmaier, 2003; Clark
The HPSG treebank is used for training the probabilistic model
for lexical entry selection, and hence, those lexical entries that
do not appear in the treebank are rarely selected by the proba-
bilistic model. The ‘effective’ tag set size, therefore, is around
1,361, the number of lexical entries without those never-seen
lexical entries.
2When parsing fails, precision and recall are evaluated, al-
though nothing is output by the parser; i.e., recall decreases
greatly.
and Curran, 2004b; Miyao and Tsujii, 2005). The
experiments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the Tree-
bank was used as the development set, and the per-
formance was evaluated using sentences of &lt; 100
words in Section 23. The performance of each
model was analyzed using the sentences in Section
24 of &lt; 100 words. Table 3 details the numbers
and average lengths of the tested sentences of &lt; 100
words in Sections 23 and 24, and the total numbers
of sentences in Sections 23 and 24.
The parsing performance for Section 23 is shown
in Table 4. The upper half of the table shows the per-
formance using the correct POSs in the Penn Tree-
bank, and the lower half shows the performance us-
ing the POSs given by a POS tagger (Tsuruoka and
Tsujii, 2005). LF and UF in the figure are labeled
F-score and unlabeled F-score. F-score is the har-
monic mean of precision and recall. We evaluated
our model in two settings. One is implemented with
a narrow beam width (‘our model 1’ in the figure),
and the other is implemented with a wider beam
width (‘our model 2’ in the figure)3. ‘our model
</bodyText>
<figure confidence="0.85406144">
3The beam thresholding parameters for ‘our model 1’ are
α0 = 10, Aα = 5, αlast = 30, β0 = 5.0, Aβ = 2.5, βlast =
15.0, δ0 = 10, Aδ = 5, δlast = 30, κ0 = 5.0, Aκ =
2.5, κlast = 15.0, θ0 = 6.0, Aθ = 3.5, and θlast = 20.0.
65
88.00%
87.50%
87.00%
86.50%
86.00%
85.50%
85.00%
84.50%
84.00%
83.50%
83.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
Miyao and Tsujii
(2005)&apos;s model
Ninomiya et al.
(2006)&apos;s model 1
Ninomiya et al.
(2006)&apos;s model 3
our model
</figure>
<figureCaption confidence="0.998791">
Figure 3: F-score versus average parsing time for sentences in Section 24 of G 100 words.
</figureCaption>
<bodyText confidence="0.993840515789474">
1’ was introduced to measure the performance with
balanced F-score and speed, which we think appro-
priate for practical use. ‘our model 2’ was intro-
duced to measure how high the precision and re-
call could reach by sacrificing speed. Our mod-
els increased the parsing accuracy. ‘our model 1’
was around 2.6 times faster and had around 2.65
points higher F-score than Miyao and Tsujii (2005)’s
model. ‘our model 2’ was around 2.3 times slower
but had around 2.9 points higher F-score than Miyao
and Tsujii (2005)’s model. We must admit that the
difference between our models and Ninomiya et al.
(2006)’s model 3 was not as great as the differ-
ence from Miyao and Tsujii (2005)’s model, but ‘our
model 1’ achieved 0.56 points higher F-score, and
‘our model 2’ achieved 0.8 points higher F-score.
When the automatic POS tagger was introduced, F-
score dropped by around 2.4 points for all models.
We also compared our model with Matsuzaki et
al. (2007)’s model. Matsuzaki et al. (2007) pro-
The terms κ and δ are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs in the chart
cell. The terms α and β are the thresholds of the number and
the beam width of lexical entries, and θ is the beam width for
global thresholding (Goodman, 1997). The terms with suffixes
0 are the initial values. The parser iterates parsing until it suc-
ceeds to generate a parse tree. The parameters increase for each
iteration by the terms prefixed by 0, and parsing finishes when
the parameters reach the terms with suffixes last. Details of the
parameters are written in (Ninomiya et al., 2005). The beam
thresholding parameters for ‘our model 2’ are α0 = 18, Dα =
6, αlast = 42, β0 = 9.0, Oβ = 3.0, βlast = 21.0, δ0 =
18, = 6, δlast = 42, κ0 = 9.0, 0κ = 3.0, κlast = 21.0.
In ‘our model 2’, the global thresholding was not used.
posed a technique for efficient HPSG parsing with
supertagging and CFG filtering. Their results with
the same grammar and servers are also listed in the
lower half of Table 4. They achieved drastic im-
provement in efficiency. Their parser ran around 6
times faster than Ninomiya et al. (2006)’s model 3,
9 times faster than ‘our model 1’ and 60 times faster
than ‘our model 2.’ Instead, our models achieved
better accuracy. ‘our model 1’ had around 0.5 higher
F-score, and ‘our model 2’ had around 0.8 points
higher F-score. Their efficiency is mainly due to
elimination of ungrammatical lexical entries by the
CFG filtering. They first parse a sentence with a
CFG grammar compiled from an HPSG grammar,
and then eliminate lexical entries that are not in the
parsed CFG trees. Obviously, this technique can
also be applied to the HPSG parsing of our mod-
els. We think that efficiency of HPSG parsing with
our models will be drastically improved by applying
this technique.
The average parsing time and labeled F-score
curves of each probabilistic model for the sentences
in Section 24 of G 100 words are graphed in Fig-
ure 3. The graph clearly shows the difference of
our model and other models. As seen in the graph,
our model achieved higher F-score than other model
when beam threshold was widen. This implies that
other models were probably difficult to reach the F-
score of ‘our model 1’ and ‘our model 2’ for Section
23 even if we changed the beam thresholding param-
eters. However, F-score of our model dropped eas-
66
ily when we narrow down the beam threshold, com-
pared to other models. We think that this is mainly
due to its bad implementation of parser interface.
The n-gram reference distribution is incorporated
into the kernel of the parser, but the n-gram fea-
tures and a maximum entropy estimator are defined
in other modules; n-gram features are defined in a
grammar module, and a maximum entropy estimator
for the n-gram reference distribution is implemented
with a general-purpose maximum entropy estimator
module. Consequently, strings that represent the n-
gram information are very frequently changed into
feature structures and vice versa when they go in and
out of the kernel of the parser. On the other hand, Ni-
nomiya et al. (2006)’s model 3 uses the supertagger
as an external module. Once the parser acquires the
supertagger’s outputs, the n-gram information never
goes in and out of the kernel. This advantage of Ni-
nomiya et al. (2006)’s model can apparently be im-
plemented in our model, but this requires many parts
of rewriting of the implemented parser. We estimate
that the overhead of the interface is around from 50
to 80 ms/sentence. We think that re-implementation
of the parser will improve the parsing speed as esti-
mated. In Figure 3, the line of our model crosses the
line of Ninomiya et al. (2006)’s model. If the esti-
mation is correct, our model will be faster and more
accurate so that the lines in the figure do not cross.
Speed-up in our model is left as a future work.
</bodyText>
<sectionHeader confidence="0.997701" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999998153846154">
We proposed a probabilistic model in which su-
pertagging is consistently integrated into the prob-
abilistic model for HPSG. In the model, the n-gram
reference distribution is simply defined as the prod-
uct of the probabilities of selecting lexical entries
with machine learning features of word and POS n-
gram as defined in the CCG/HPSG/CDG supertag-
ging. We conducted experiments on the Penn Tree-
bank with a wide-coverage HPSG parser. In the ex-
periments, we compared our model with the prob-
abilistic HPSG with a unigram reference distribu-
tion (Miyao and Tsujii, 2005) and the probabilistic
HPSG with supertagging (Ninomiya et al., 2006).
Though our model was not as fast as Ninomiya
et al. (2006)’s models, it achieved the highest ac-
curacy among them. Our model had around 2.65
points higher F-score than Miyao and Tsujii (2005)’s
model and around 0.56 points higher F-score than
the Ninomiya et al. (2006)’s model 3. When we sac-
rifice parsing speed, our model achieved around 2.9
points higher F-score than Miyao and Tsujii (2005)’s
model and around 0.8 points higher F-score than Ni-
nomiya et al. (2006)’s model 3. Our model achieved
higher F-score because parameters for phrase struc-
tures in our model are trained with the supertagging
probabilities, which are not in other models.
</bodyText>
<sectionHeader confidence="0.990314" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999921742857143">
Steven P. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597–618.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237–265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39–71.
Joan Bresnan. 1982. The Mental Representation of
Grammatical Relations. MIT Press, Cambridge, MA.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG parsing.
In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proc. of
ACL’04, pages 104–111.
Killian Foth and Wolfgang Menzel. 2006. Hybrid pars-
ing: Using probabilistic models as predictors for a
symbolic parser. In Proc. of COLING-ACL 2006.
Killian Foth, Tomas By, and Wolfgang Menzel. 2006.
Guiding a constraint dependency parser with su-
pertags. In Proc. of COLING-ACL 2006.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochas-
tic unification-based grammars. In Proc. of ACL’02,
pages 279–286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11–25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL’03, pages 359–366.
F. Jelinek. 1998. Statistical Methods for Speech Recog-
nition. The MIT Press.
</reference>
<page confidence="0.674674">
67
</page>
<reference confidence="0.999920227848101">
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proc. ofNAACL-2000, pages 154–161.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
“unification-based” grammars. In Proc. of ACL ’99,
pages 535–541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL’04.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In Proc. of IJCNLP-04 Workshop “Beyond
Shallow Analyses”.
Robert Malouf, John Carroll, and Ann Copestake. 2000.
Efficient feature structure operations without compi-
lation. Journal of Natural Language Engineering,
6(1):29–46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49–55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2007. Efficient HPSG parsing with supertagging and
CFG-filtering. In Proc. of IJCAI 2007, pages 1671–
1676.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proc. of HLT
2002, pages 292–297.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proc. ofACL’05, pages 83–90.
Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsu-
jii, 2005. Keh-Yih Su, Jun’ichi Tsujii, Jong-Hyeok
Lee and Oi Yee Kwong (Eds.), Natural Language
Processing - IJCNLP 2004 LNAI 3248, chapter
Corpus-oriented Grammar Development for Acquir-
ing a Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684–693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2004. An empirical investigation of the effect of lexi-
cal rules on parsing with a treebank grammar. In Proc.
of TLT’04, pages 103–114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,
and Jun’ichi Tsujii. 2005. Efficacy of beam threshold-
ing, unification filtering and hybrid parsing in proba-
bilistic HPSG parsing. In Proc. of IWPT 2005, pages
103–114.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Ex-
tremely lexicalized models for accurate and fast HPSG
parsing. In Proc. of EMNLP 2006, pages 155–163.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proc. ofACL’00, pages 480–487.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In Proc. of HLT/EMNLP 2005,
pages 467–474.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proc.
of ACL’04 Incremental Parsing workshop: Bringing
Engineering and Cognition Together, pages 42–49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Grammar.
Ph.D. thesis, Purdue University.
</reference>
<page confidence="0.962933">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.226401">
<title confidence="0.9800225">A log-linear model with an n-gram reference distribution for accurate HPSG parsing</title>
<author confidence="0.966194">Takashi</author>
<affiliation confidence="0.99896">Information Technology University of Tokyo</affiliation>
<email confidence="0.885438">ninomi@r.dl.itc.u-tokyo.ac.jp</email>
<author confidence="0.721308">Takuya</author>
<affiliation confidence="0.999901">Department of Computer University of Tokyo</affiliation>
<email confidence="0.951419">matuzaki@is.s.u-tokyo.ac.jp</email>
<author confidence="0.992503">Yusuke Miyao</author>
<affiliation confidence="0.999855">Department of Computer University of Tokyo</affiliation>
<email confidence="0.953947">yusuke@is.s.u-tokyo.ac.jp</email>
<author confidence="0.784764">Jun’ichi</author>
<affiliation confidence="0.910525333333333">Department of Computer Science, University of School of Informatics, University of NaCTeM (National Center for Text Mining)</affiliation>
<email confidence="0.719839">tsujii@is.s.u-tokyo.ac.jp</email>
<note confidence="0.865883">Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan</note>
<abstract confidence="0.9995785">This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertagging. Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but supertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse tree’s probabilistic model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="2397" citStr="Abney, 1997" startWordPosition="334" endWordPosition="335">ar (LFG) (Bresnan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 </context>
<context position="7863" citStr="Abney, 1997" startWordPosition="1213" endWordPosition="1214">f words and a set F of feature structures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)Jw E W, F E FJ is a set of lexical entries, and R is a set of schemata; i.e., r E R is a partial function: F x F — F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of 61 HEAD verb SUBJ &lt; &gt; COMPS &lt; &gt; Figure 1: HPSG parsing. SUBJ &lt; &gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; COMPS &lt; 2 2 SUBJ &lt; &gt; parsing. Note that HPSG is one of the lexicalized grammar formalisms, in which lexical entries determine the dominant syntactic structures. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Gema</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven P. Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="3305" citStr="Bangalore and Joshi, 1999" startWordPosition="473" endWordPosition="477">n Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accura</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2349" citStr="Berger et al., 1996" startWordPosition="324" endWordPosition="327">grammar (CCG) (Steedman, 2000) and lexical function grammar (LFG) (Bresnan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 6</context>
<context position="8125" citStr="Berger et al., 1996" startWordPosition="1253" endWordPosition="1256"> computes a set of phrasal signs, i.e., feature structures, as a result of 61 HEAD verb SUBJ &lt; &gt; COMPS &lt; &gt; Figure 1: HPSG parsing. SUBJ &lt; &gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; COMPS &lt; 2 2 SUBJ &lt; &gt; parsing. Note that HPSG is one of the lexicalized grammar formalisms, in which lexical entries determine the dominant syntactic structures. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii (2005) also introduced a preliminary probabilistic model p0(T|w) whose estimation does not require the parsing of</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1810" citStr="Bresnan, 1982" startWordPosition="243" endWordPosition="244">r parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse tree’s probabilistic model. 1 Introduction For the last decade, fast, accurate and wide-coverage parsing for real-world text has been pursued in sophisticated grammar formalisms, such as headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994), combinatory categorial grammar (CCG) (Steedman, 2000) and lexical function grammar (LFG) (Bresnan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et </context>
</contexts>
<marker>Bresnan, 1982</marker>
<rawString>Joan Bresnan. 1982. The Mental Representation of Grammatical Relations. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proc. of COLING-04.</booktitle>
<contexts>
<context position="2729" citStr="Clark and Curran, 2004" startWordPosition="387" endWordPosition="390">es such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a proc</context>
<context position="4580" citStr="Clark and Curran, 2004" startWordPosition="672" endWordPosition="675">et al. (2006) and Foth and Menzel (2006) reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG. Ninomiya et al. (2006) showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999). However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a), or the probabilistic models for phrase structures were trained independently of the supertagger’s probabilistic models (Wang and Harper, 2004; Ninomiya et al., 2006). In the case of supertagging of Weighted CDG (Foth et al., 2006), parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model. We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG. The reference distribution is simply defined as the product of the probabilities of</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004a. The importance of supertagging for wide-coverage CCG parsing. In Proc. of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proc. of ACL’04,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2729" citStr="Clark and Curran, 2004" startWordPosition="387" endWordPosition="390">es such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a proc</context>
<context position="4580" citStr="Clark and Curran, 2004" startWordPosition="672" endWordPosition="675">et al. (2006) and Foth and Menzel (2006) reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG. Ninomiya et al. (2006) showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999). However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a), or the probabilistic models for phrase structures were trained independently of the supertagger’s probabilistic models (Wang and Harper, 2004; Ninomiya et al., 2006). In the case of supertagging of Weighted CDG (Foth et al., 2006), parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model. We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG. The reference distribution is simply defined as the product of the probabilities of</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In Proc. of ACL’04, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Killian Foth</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Hybrid parsing: Using probabilistic models as predictors for a symbolic parser.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<contexts>
<context position="3198" citStr="Foth and Menzel, 2006" startWordPosition="459" endWordPosition="462">ciency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated </context>
</contexts>
<marker>Foth, Menzel, 2006</marker>
<rawString>Killian Foth and Wolfgang Menzel. 2006. Hybrid parsing: Using probabilistic models as predictors for a symbolic parser. In Proc. of COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Killian Foth</author>
<author>Tomas By</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Guiding a constraint dependency parser with supertags.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<contexts>
<context position="3174" citStr="Foth et al., 2006" startWordPosition="455" endWordPosition="458">techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (</context>
<context position="4814" citStr="Foth et al., 2006" startWordPosition="708" endWordPosition="711">agging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999). However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a), or the probabilistic models for phrase structures were trained independently of the supertagger’s probabilistic models (Wang and Harper, 2004; Ninomiya et al., 2006). In the case of supertagging of Weighted CDG (Foth et al., 2006), parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model. We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG. The reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and part-of-speech (POS) n-gram as defined in the CCG/HPSG/CDG supertagging. This is the first model which properly inc</context>
</contexts>
<marker>Foth, By, Menzel, 2006</marker>
<rawString>Killian Foth, Tomas By, and Wolfgang Menzel. 2006. Guiding a constraint dependency parser with supertags. In Proc. of COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Mark Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In Proc. of ACL’02,</booktitle>
<pages>279--286</pages>
<contexts>
<context position="2640" citStr="Geman and Johnson, 2002" startWordPosition="372" endWordPosition="375">in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexic</context>
<context position="8483" citStr="Geman and Johnson, 2002" startWordPosition="1316" endWordPosition="1319">1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii (2005) also introduced a preliminary probabilistic model p0(T|w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution (Jelinek, 1998; Johnson and Riezler, 2000) of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage (Miyao and Tsujii, 2005), or a probabilistic model can be augmented by several distributions estimated from</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>Stuart Geman and Mark Johnson. 2002. Dynamic programming for parsing and estimation of stochastic unification-based grammars. In Proc. of ACL’02, pages 279–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple pass parsing.</title>
<date>1997</date>
<booktitle>In Proc. of EMNLP-1997,</booktitle>
<pages>11--25</pages>
<contexts>
<context position="19710" citStr="Goodman, 1997" startWordPosition="3326" endWordPosition="3327">LR LF UP UR UF Avg. time (%) (%) (%) (%) (%) (%) (ms) Miyao and Tsujii (2005)’s model 84.96 84.25 84.60 89.55 88.80 89.17 674 Ninomiya et al. (2006)’s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154 Ninomiya et al. (2006)’s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183 Matsuzaki et al. (2007)’s model 86.93 86.47 86.70 - - - 30 our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260 our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821 Table 4: Experimental results for Section 23. abilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and quick check (Malouf et al., 2000). We measured the accuracy of the predicateargument relations output of the parser. A predicate-argument relation is defined as a tuple (Q, wh, a, wa), where a is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser2. Unlabeled precision (UP)/unlabeled recall (UR) is the ra</context>
<context position="23907" citStr="Goodman, 1997" startWordPosition="4075" endWordPosition="4076"> and Tsujii (2005)’s model, but ‘our model 1’ achieved 0.56 points higher F-score, and ‘our model 2’ achieved 0.8 points higher F-score. When the automatic POS tagger was introduced, Fscore dropped by around 2.4 points for all models. We also compared our model with Matsuzaki et al. (2007)’s model. Matsuzaki et al. (2007) proThe terms κ and δ are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and β are the thresholds of the number and the beam width of lexical entries, and θ is the beam width for global thresholding (Goodman, 1997). The terms with suffixes 0 are the initial values. The parser iterates parsing until it succeeds to generate a parse tree. The parameters increase for each iteration by the terms prefixed by 0, and parsing finishes when the parameters reach the terms with suffixes last. Details of the parameters are written in (Ninomiya et al., 2005). The beam thresholding parameters for ‘our model 2’ are α0 = 18, Dα = 6, αlast = 42, β0 = 9.0, Oβ = 3.0, βlast = 21.0, δ0 = 18, = 6, δlast = 42, κ0 = 9.0, 0κ = 3.0, κlast = 21.0. In ‘our model 2’, the global thresholding was not used. posed a technique for effici</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Global thresholding and multiple pass parsing. In Proc. of EMNLP-1997, pages 11–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Parsing with generative models of predicate-argument structure.</title>
<date>2003</date>
<booktitle>In Proc. of ACL’03,</booktitle>
<pages>359--366</pages>
<contexts>
<context position="20485" citStr="Hockenmaier, 2003" startWordPosition="3452" endWordPosition="3453">t of the parser. A predicate-argument relation is defined as a tuple (Q, wh, a, wa), where a is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser2. Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark The HPSG treebank is used for training the probabilistic model for lexical entry selection, and hence, those lexical entries that do not appear in the treebank are rarely selected by the probabilistic model. The ‘effective’ tag set size, therefore, is around 1,361, the number of lexical entries without those never-seen lexical entries. 2When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly. and Curran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Sec</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Parsing with generative models of predicate-argument structure. In Proc. of ACL’03, pages 359–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8805" citStr="Jelinek, 1998" startWordPosition="1367" endWordPosition="1368">ence w = (w1, ... , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii (2005) also introduced a preliminary probabilistic model p0(T|w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution (Jelinek, 1998; Johnson and Riezler, 2000) of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage (Miyao and Tsujii, 2005), or a probabilistic model can be augmented by several distributions estimated from the larger and simpler corpus (Johnson and Riezler, 2000). In (Miyao and Tsujii, 2005), p0(T|w) is defined as the product of probabilities of selecting lexical entries with word and POS unigram features: (Miyao and Tsujii (2005)’s model) ÃX ! puniref(T |w) = p0(T |w) Zw 1 exp λufu(T ) u p0(T&apos;|w) expXλufu(T )! Ã u HEAD v</context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>F. Jelinek. 1998. Statistical Methods for Speech Recognition. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stefan Riezler</author>
</authors>
<title>Exploiting auxiliary distributions in stochastic unification-based grammars.</title>
<date>2000</date>
<booktitle>In Proc. ofNAACL-2000,</booktitle>
<pages>154--161</pages>
<contexts>
<context position="8833" citStr="Johnson and Riezler, 2000" startWordPosition="1369" endWordPosition="1372">.. , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii (2005) also introduced a preliminary probabilistic model p0(T|w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution (Jelinek, 1998; Johnson and Riezler, 2000) of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage (Miyao and Tsujii, 2005), or a probabilistic model can be augmented by several distributions estimated from the larger and simpler corpus (Johnson and Riezler, 2000). In (Miyao and Tsujii, 2005), p0(T|w) is defined as the product of probabilities of selecting lexical entries with word and POS unigram features: (Miyao and Tsujii (2005)’s model) ÃX ! puniref(T |w) = p0(T |w) Zw 1 exp λufu(T ) u p0(T&apos;|w) expXλufu(T )! Ã u HEAD verb SUBJ &lt; HEAD noun COMPS &lt;</context>
</contexts>
<marker>Johnson, Riezler, 2000</marker>
<rawString>Mark Johnson and Stefan Riezler. 2000. Exploiting auxiliary distributions in stochastic unification-based grammars. In Proc. ofNAACL-2000, pages 154–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proc. of ACL ’99,</booktitle>
<pages>535--541</pages>
<contexts>
<context position="2419" citStr="Johnson et al., 1999" startWordPosition="336" endWordPosition="339">snan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Comput</context>
<context position="7885" citStr="Johnson et al., 1999" startWordPosition="1215" endWordPosition="1218"> set F of feature structures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)Jw E W, F E FJ is a set of lexical entries, and R is a set of schemata; i.e., r E R is a partial function: F x F — F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of 61 HEAD verb SUBJ &lt; &gt; COMPS &lt; &gt; Figure 1: HPSG parsing. SUBJ &lt; &gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; COMPS &lt; 2 2 SUBJ &lt; &gt; parsing. Note that HPSG is one of the lexicalized grammar formalisms, in which lexical entries determine the dominant syntactic structures. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) a</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proc. of ACL ’99, pages 535–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>S Riezler</author>
<author>T H King</author>
<author>J T Maxwell</author>
<author>A Vasserman</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL’04.</booktitle>
<contexts>
<context position="2490" citStr="Kaplan et al., 2004" startWordPosition="350" endWordPosition="354">nalyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wa</context>
<context position="7956" citStr="Kaplan et al., 2004" startWordPosition="1228" endWordPosition="1231">R), where L = {l = (w, F)Jw E W, F E FJ is a set of lexical entries, and R is a set of schemata; i.e., r E R is a partial function: F x F — F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of 61 HEAD verb SUBJ &lt; &gt; COMPS &lt; &gt; Figure 1: HPSG parsing. SUBJ &lt; &gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; COMPS &lt; 2 2 SUBJ &lt; &gt; parsing. Note that HPSG is one of the lexicalized grammar formalisms, in which lexical entries determine the dominant syntactic structures. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic program</context>
</contexts>
<marker>Kaplan, Riezler, King, Maxwell, Vasserman, 2004</marker>
<rawString>R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell III, and A. Vasserman. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Proc. of HLT/NAACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proc. of IJCNLP-04 Workshop “Beyond Shallow Analyses”.</booktitle>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Robert Malouf and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proc. of IJCNLP-04 Workshop “Beyond Shallow Analyses”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>John Carroll</author>
<author>Ann Copestake</author>
</authors>
<title>Efficient feature structure operations without compilation.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="19801" citStr="Malouf et al., 2000" startWordPosition="3338" endWordPosition="3341"> 84.96 84.25 84.60 89.55 88.80 89.17 674 Ninomiya et al. (2006)’s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154 Ninomiya et al. (2006)’s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183 Matsuzaki et al. (2007)’s model 86.93 86.47 86.70 - - - 30 our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260 our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821 Table 4: Experimental results for Section 23. abilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and quick check (Malouf et al., 2000). We measured the accuracy of the predicateargument relations output of the parser. A predicate-argument relation is defined as a tuple (Q, wh, a, wa), where a is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser2. Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was</context>
</contexts>
<marker>Malouf, Carroll, Copestake, 2000</marker>
<rawString>Robert Malouf, John Carroll, and Ann Copestake. 2000. Efficient feature structure operations without compilation. Journal of Natural Language Engineering, 6(1):29–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL-2002,</booktitle>
<pages>49--55</pages>
<contexts>
<context position="10197" citStr="Malouf, 2002" startWordPosition="1630" endWordPosition="1631">head-comp HEAD verb 1 COMPS &lt; &gt; 1 SUBJ &lt; &gt; 1&gt; 1 HEAD noun SUBJ &lt; &gt; COMPS &lt; &gt; Spring XZw = T1 ÃX ! p0(T|w) = Yn p(li|wi), 1 i=1 phpsg(T |w) = exp λufu(T ) Zw u exp ÃXλu fu (T&apos;)J u J where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T, and Z, is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu) when a characteristic corresponding to fu appears in parse result T. The model parameters, λu, are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T|w) for all parse candidates assigned HEAD noun HEAD verb where li is a lexical entry assigned to word wi in T and p(li|wi) is the probability of selecting lexical entry li for wi. In the experiments, we compared our model with other two types of probabilistic models using a supertagger (Ninomiya et al., 2006). The first one is the simplest probabilistic model, which is defined with only the probabilities of lexical entry selection. It </context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proc. of CoNLL-2002, pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="18021" citStr="Marcus et al., 1994" startWordPosition="3024" endWordPosition="3027">yao and Tsujii (2005)’s model also uses a reference distribution, but with word and POS unigram features, as is explained in the previous section. The only difference between our model and Miyao and Tsujii (2005)’s model is that our model uses sequences of word and POS tags as n-gram features for selecting lexical entries in the same way as supertagging does. 4 Experiments We evaluated the speed and accuracy of parsing by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1. The probTI 1An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG treebank. Znref = ÃX ! pmodel1(T �|w) exp �ufu(T �) � u X 64 No. of tested sentences Total No. of sentences Avg. length of tested sentences Section 23 2,299 (100.00%) 2,299 22.2 Section 24 1,245 (99.84%) 1,247 23</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficient HPSG parsing with supertagging and CFG-filtering.</title>
<date>2007</date>
<booktitle>In Proc. of IJCAI</booktitle>
<pages>1671--1676</pages>
<contexts>
<context position="19391" citStr="Matsuzaki et al. (2007)" startWordPosition="3270" endWordPosition="3273">)’s model 87.26 86.50 86.88 90.73 89.93 90.33 604 Ninomiya et al. (2006)’s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129 Ninomiya et al. (2006)’s model 3 89.48 88.58 89.02 92.33 91.40 91.86 152 our model 1 89.78 89.28 89.53 92.58 92.07 92.32 234 our model 2 90.03 89.60 89.82 92.82 92.37 92.60 1379 Section 23 (POS tagger) LP LR LF UP UR UF Avg. time (%) (%) (%) (%) (%) (%) (ms) Miyao and Tsujii (2005)’s model 84.96 84.25 84.60 89.55 88.80 89.17 674 Ninomiya et al. (2006)’s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154 Ninomiya et al. (2006)’s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183 Matsuzaki et al. (2007)’s model 86.93 86.47 86.70 - - - 30 our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260 our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821 Table 4: Experimental results for Section 23. abilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and quick check (Malouf et al., 2000). We measured the accuracy of the predicateargument relations output of the parser. A predicate-argument relation is defined as a tuple (Q, wh, a, wa), where a is the predicate type (e.g., a</context>
<context position="23583" citStr="Matsuzaki et al. (2007)" startWordPosition="4010" endWordPosition="4013"> had around 2.65 points higher F-score than Miyao and Tsujii (2005)’s model. ‘our model 2’ was around 2.3 times slower but had around 2.9 points higher F-score than Miyao and Tsujii (2005)’s model. We must admit that the difference between our models and Ninomiya et al. (2006)’s model 3 was not as great as the difference from Miyao and Tsujii (2005)’s model, but ‘our model 1’ achieved 0.56 points higher F-score, and ‘our model 2’ achieved 0.8 points higher F-score. When the automatic POS tagger was introduced, Fscore dropped by around 2.4 points for all models. We also compared our model with Matsuzaki et al. (2007)’s model. Matsuzaki et al. (2007) proThe terms κ and δ are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and β are the thresholds of the number and the beam width of lexical entries, and θ is the beam width for global thresholding (Goodman, 1997). The terms with suffixes 0 are the initial values. The parser iterates parsing until it succeeds to generate a parse tree. The parameters increase for each iteration by the terms prefixed by 0, and parsing finishes when the parameters reach the terms with suffixes last. Deta</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Efficient HPSG parsing with supertagging and CFG-filtering. In Proc. of IJCAI 2007, pages 1671– 1676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proc. of HLT</booktitle>
<pages>292--297</pages>
<contexts>
<context position="2664" citStr="Miyao and Tsujii, 2002" startWordPosition="376" endWordPosition="379">ow to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining gr</context>
<context position="8529" citStr="Miyao and Tsujii, 2002" startWordPosition="1324" endWordPosition="1327">0; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii (2005) also introduced a preliminary probabilistic model p0(T|w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution (Jelinek, 1998; Johnson and Riezler, 2000) of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage (Miyao and Tsujii, 2005), or a probabilistic model can be augmented by several distributions estimated from the larger and simpler corpus (Johnson and Ri</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proc. of HLT 2002, pages 292–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proc. ofACL’05,</booktitle>
<pages>83--90</pages>
<contexts>
<context position="2515" citStr="Miyao and Tsujii, 2005" startWordPosition="355" endWordPosition="358">g linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr</context>
<context position="5591" citStr="Miyao and Tsujii, 2005" startWordPosition="823" endWordPosition="827">stic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG. The reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and part-of-speech (POS) n-gram as defined in the CCG/HPSG/CDG supertagging. This is the first model which properly incorporates the supertagging probabilities into parse tree’s probabilistic model. We compared our model with the probabilistic model for phrase structures (Miyao and Tsujii, 2005). This model uses word and POS unigram for its reference distribution, i.e., the probabilities of unigram supertagging. Our model can be regarded as an extension of a unigram reference distribution to an n-gram reference distribution with features that are used in supertagging. We also compared with a probabilistic model in (Ninomiya et al., 2006). The probabilities of their model are defined as the product of probabilities of supertagging and probabilities of the probabilistic model for phrase structures, but their model was trained independently of supertagging probabilities, i.e., the super</context>
<context position="7981" citStr="Miyao and Tsujii, 2005" startWordPosition="1232" endWordPosition="1235">, F)Jw E W, F E FJ is a set of lexical entries, and R is a set of schemata; i.e., r E R is a partial function: F x F — F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of 61 HEAD verb SUBJ &lt; &gt; COMPS &lt; &gt; Figure 1: HPSG parsing. SUBJ &lt; &gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; COMPS &lt; 2 2 SUBJ &lt; &gt; parsing. Note that HPSG is one of the lexicalized grammar formalisms, in which lexical entries determine the dominant syntactic structures. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estima</context>
<context position="9312" citStr="Miyao and Tsujii (2005)" startWordPosition="1444" endWordPosition="1447">tion does not require the parsing of a treebank. This model is introduced as a reference distribution (Jelinek, 1998; Johnson and Riezler, 2000) of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage (Miyao and Tsujii, 2005), or a probabilistic model can be augmented by several distributions estimated from the larger and simpler corpus (Johnson and Riezler, 2000). In (Miyao and Tsujii, 2005), p0(T|w) is defined as the product of probabilities of selecting lexical entries with word and POS unigram features: (Miyao and Tsujii (2005)’s model) ÃX ! puniref(T |w) = p0(T |w) Zw 1 exp λufu(T ) u p0(T&apos;|w) expXλufu(T )! Ã u HEAD verb SUBJ &lt; HEAD noun COMPS &lt; &gt; &gt; COMPS &lt; &gt; HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt; &gt; come has head-comp Spring has come subject-head HEAD verb 1 COMPS &lt; &gt; head-comp HEAD verb 1 COMPS &lt; &gt; 1 SUBJ &lt; &gt; 1&gt; 1 HEAD noun SUBJ &lt; &gt; COMPS &lt; &gt; Spring XZw = T1 ÃX ! p0(T|w) = Yn p(li|wi), 1 i=1 phpsg(T |w) = exp λufu(T ) Zw u exp ÃXλu fu (T&apos;)J u J where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T, and Z, is the sum over the set of all pos</context>
<context position="13110" citStr="Miyao and Tsujii (2005)" startWordPosition="2169" endWordPosition="2172"> li for wi. The probabilities of lexical entry selection, p(li|w, i), are defined as follows: (Probabilistic model of lexical entry selection) p(li |w, i) = Z. expÃX λufu (1j, u(la, w, i)! u exp ÃX λufu(li, w, i)I u / where Z,,, is the sum over all possible lexical entries for the word wi. The second model is a hybrid model of supertagging and the probabilistic HPSG. The probabilities are given as the product of Ninomiya et al. (2006)’s model 1 and the probabilistic HPSG. (Ninomiya et al. (2006)’s model 3) pmodel3(T |w) = pmodel1(T |w)phpsg(T |w) In the experiments, we compared our model with Miyao and Tsujii (2005)’s model and Ninomiya et al. (2006)’s model 1 and 3. The features used in our model and their model are combinations of the feature templates listed in Table 1 and Table 2. The feature templates fbinary and funary are defined for constituents at binary and unary branches, froot is a feature template set for the root nodes of parse trees. flex is a feature template set for calculating the unigram reference distribution and is used in Miyao and Tsujii (2005)’s model. fsptag is a feature template set for calculating the probabilities of selecting lexical entries in Ninomiya et al. (2006)’s model </context>
<context position="17422" citStr="Miyao and Tsujii (2005)" startWordPosition="2918" endWordPosition="2921">entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In their model, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only. That is, the parameters for lexical entries and the parameters for phrase structures are trained independently in their model. Miyao and Tsujii (2005)’s model also uses a reference distribution, but with word and POS unigram features, as is explained in the previous section. The only difference between our model and Miyao and Tsujii (2005)’s model is that our model uses sequences of word and POS tags as n-gram features for selecting lexical entries in the same way as supertagging does. 4 Experiments We evaluated the speed and accuracy of parsing by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) </context>
<context position="18769" citStr="Miyao and Tsujii (2005)" startWordPosition="3155" endWordPosition="3158">tically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG treebank. Znref = ÃX ! pmodel1(T �|w) exp �ufu(T �) � u X 64 No. of tested sentences Total No. of sentences Avg. length of tested sentences Section 23 2,299 (100.00%) 2,299 22.2 Section 24 1,245 (99.84%) 1,247 23.0 Table 3: Statistics of the Penn Treebank. LP Section 23 (Gold POSs) UR UF Avg. time (%) LR LF UP (%) (%) (ms) (%) (%) (%) Miyao and Tsujii (2005)’s model 87.26 86.50 86.88 90.73 89.93 90.33 604 Ninomiya et al. (2006)’s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129 Ninomiya et al. (2006)’s model 3 89.48 88.58 89.02 92.33 91.40 91.86 152 our model 1 89.78 89.28 89.53 92.58 92.07 92.32 234 our model 2 90.03 89.60 89.82 92.82 92.37 92.60 1379 Section 23 (POS tagger) LP LR LF UP UR UF Avg. time (%) (%) (%) (%) (%) (%) (ms) Miyao and Tsujii (2005)’s model 84.96 84.25 84.60 89.55 88.80 89.17 674 Ninomiya et al. (2006)’s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154 Ninomiya et al. (2006)’s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183 M</context>
<context position="21004" citStr="Miyao and Tsujii, 2005" startWordPosition="3531" endWordPosition="3534">valuation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark The HPSG treebank is used for training the probabilistic model for lexical entry selection, and hence, those lexical entries that do not appear in the treebank are rarely selected by the probabilistic model. The ‘effective’ tag set size, therefore, is around 1,361, the number of lexical entries without those never-seen lexical entries. 2When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly. and Curran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and the performance was evaluated using sentences of &lt; 100 words in Section 23. The performance of each model was analyzed using the sentences in Section 24 of &lt; 100 words. Table 3 details the numbers and average lengths of the tested sentences of &lt; 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is shown in Table 4. The upper half of the table shows the performance using the c</context>
<context position="22465" citStr="Miyao and Tsujii (2005)" startWordPosition="3815" endWordPosition="3818">f precision and recall. We evaluated our model in two settings. One is implemented with a narrow beam width (‘our model 1’ in the figure), and the other is implemented with a wider beam width (‘our model 2’ in the figure)3. ‘our model 3The beam thresholding parameters for ‘our model 1’ are α0 = 10, Aα = 5, αlast = 30, β0 = 5.0, Aβ = 2.5, βlast = 15.0, δ0 = 10, Aδ = 5, δlast = 30, κ0 = 5.0, Aκ = 2.5, κlast = 15.0, θ0 = 6.0, Aθ = 3.5, and θlast = 20.0. 65 88.00% 87.50% 87.00% 86.50% 86.00% 85.50% 85.00% 84.50% 84.00% 83.50% 83.00% 0 100 200 300 400 500 600 700 800 900 Parsing time (ms/sentence) Miyao and Tsujii (2005)&apos;s model Ninomiya et al. (2006)&apos;s model 1 Ninomiya et al. (2006)&apos;s model 3 our model Figure 3: F-score versus average parsing time for sentences in Section 24 of G 100 words. 1’ was introduced to measure the performance with balanced F-score and speed, which we think appropriate for practical use. ‘our model 2’ was introduced to measure how high the precision and recall could reach by sacrificing speed. Our models increased the parsing accuracy. ‘our model 1’ was around 2.6 times faster and had around 2.65 points higher F-score than Miyao and Tsujii (2005)’s model. ‘our model 2’ was around 2.3</context>
<context position="28060" citStr="Miyao and Tsujii, 2005" startWordPosition="4799" endWordPosition="4802">peed-up in our model is left as a future work. 5 Conclusion We proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for HPSG. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution (Miyao and Tsujii, 2005) and the probabilistic HPSG with supertagging (Ninomiya et al., 2006). Though our model was not as fast as Ninomiya et al. (2006)’s models, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than Miyao and Tsujii (2005)’s model and around 0.56 points higher F-score than the Ninomiya et al. (2006)’s model 3. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than Miyao and Tsujii (2005)’s model and around 0.8 points higher F-score than Ninomiya et al. (2006)’s model 3. Our model achieved higher F-score because parameter</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proc. ofACL’05, pages 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Keh-Yih Su, Jun’ichi Tsujii, Jong-Hyeok Lee and Oi Yee Kwong (Eds.), Natural Language Processing - IJCNLP</title>
<date>2005</date>
<pages>684--693</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="17891" citStr="Miyao et al., 2005" startWordPosition="3001" endWordPosition="3004">hat is, the parameters for lexical entries and the parameters for phrase structures are trained independently in their model. Miyao and Tsujii (2005)’s model also uses a reference distribution, but with word and POS unigram features, as is explained in the previous section. The only difference between our model and Miyao and Tsujii (2005)’s model is that our model uses sequences of word and POS tags as n-gram features for selecting lexical entries in the same way as supertagging does. 4 Experiments We evaluated the speed and accuracy of parsing by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1. The probTI 1An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG treebank. Znref = ÃX ! pmodel1(T �|w) exp �ufu(T �) � u X 64 No. of tested sentenc</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2005</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii, 2005. Keh-Yih Su, Jun’ichi Tsujii, Jong-Hyeok Lee and Oi Yee Kwong (Eds.), Natural Language Processing - IJCNLP 2004 LNAI 3248, chapter Corpus-oriented Grammar Development for Acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank, pages 684–693. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>An empirical investigation of the effect of lexical rules on parsing with a treebank grammar.</title>
<date>2004</date>
<booktitle>In Proc. of TLT’04,</booktitle>
<pages>103--114</pages>
<contexts>
<context position="18324" citStr="Nakanishi et al., 2004" startWordPosition="3072" endWordPosition="3075">lecting lexical entries in the same way as supertagging does. 4 Experiments We evaluated the speed and accuracy of parsing by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1. The probTI 1An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG treebank. Znref = ÃX ! pmodel1(T �|w) exp �ufu(T �) � u X 64 No. of tested sentences Total No. of sentences Avg. length of tested sentences Section 23 2,299 (100.00%) 2,299 22.2 Section 24 1,245 (99.84%) 1,247 23.0 Table 3: Statistics of the Penn Treebank. LP Section 23 (Gold POSs) UR UF Avg. time (%) LR LF UP (%) (%) (ms) (%) (%) (%) Miyao and Tsujii (2005)’s model 87.26 86.50 86.88 90.73 89.93 90.33 604 Ninomiya et al. (2006)’s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129 Ninomiya et al. (2006)’s model 3 </context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2004</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2004. An empirical investigation of the effect of lexical rules on parsing with a treebank grammar. In Proc. of TLT’04, pages 103–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Supertagging and full parsing.</title>
<date>2004</date>
<booktitle>In Proc. of the 7th International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7).</booktitle>
<contexts>
<context position="3132" citStr="Nasr and Rambow, 2004" startWordPosition="447" endWordPosition="450">005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improveme</context>
</contexts>
<marker>Nasr, Rambow, 2004</marker>
<rawString>Alexis Nasr and Owen Rambow. 2004. Supertagging and full parsing. In Proc. of the 7th International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT</booktitle>
<pages>103--114</pages>
<contexts>
<context position="2779" citStr="Ninomiya et al., 2005" startWordPosition="395" endWordPosition="398">y a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged wi</context>
<context position="19763" citStr="Ninomiya et al., 2005" startWordPosition="3331" endWordPosition="3334">(%) (ms) Miyao and Tsujii (2005)’s model 84.96 84.25 84.60 89.55 88.80 89.17 674 Ninomiya et al. (2006)’s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154 Ninomiya et al. (2006)’s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183 Matsuzaki et al. (2007)’s model 86.93 86.47 86.70 - - - 30 our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260 our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821 Table 4: Experimental results for Section 23. abilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and quick check (Malouf et al., 2000). We measured the accuracy of the predicateargument relations output of the parser. A predicate-argument relation is defined as a tuple (Q, wh, a, wa), where a is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser2. Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argu</context>
<context position="24243" citStr="Ninomiya et al., 2005" startWordPosition="4130" endWordPosition="4133">he terms κ and δ are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and β are the thresholds of the number and the beam width of lexical entries, and θ is the beam width for global thresholding (Goodman, 1997). The terms with suffixes 0 are the initial values. The parser iterates parsing until it succeeds to generate a parse tree. The parameters increase for each iteration by the terms prefixed by 0, and parsing finishes when the parameters reach the terms with suffixes last. Details of the parameters are written in (Ninomiya et al., 2005). The beam thresholding parameters for ‘our model 2’ are α0 = 18, Dα = 6, αlast = 42, β0 = 9.0, Oβ = 3.0, βlast = 21.0, δ0 = 18, = 6, δlast = 42, κ0 = 9.0, 0κ = 3.0, κlast = 21.0. In ‘our model 2’, the global thresholding was not used. posed a technique for efficient HPSG parsing with supertagging and CFG filtering. Their results with the same grammar and servers are also listed in the lower half of Table 4. They achieved drastic improvement in efficiency. Their parser ran around 6 times faster than Ninomiya et al. (2006)’s model 3, 9 times faster than ‘our model 1’ and 60 times faster than ‘o</context>
</contexts>
<marker>Ninomiya, Tsuruoka, Miyao, Tsujii, 2005</marker>
<rawString>Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic HPSG parsing. In Proc. of IWPT 2005, pages 103–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Extremely lexicalized models for accurate and fast HPSG parsing.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>155--163</pages>
<contexts>
<context position="3155" citStr="Ninomiya et al., 2006" startWordPosition="451" endWordPosition="454">scriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed</context>
<context position="4749" citStr="Ninomiya et al., 2006" startWordPosition="696" endWordPosition="699">DG. Ninomiya et al. (2006) showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999). However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a), or the probabilistic models for phrase structures were trained independently of the supertagger’s probabilistic models (Wang and Harper, 2004; Ninomiya et al., 2006). In the case of supertagging of Weighted CDG (Foth et al., 2006), parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model. We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG. The reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and part-of-speech (POS) n-gram as defined in the CCG/</context>
<context position="10668" citStr="Ninomiya et al., 2006" startWordPosition="1709" endWordPosition="1712">aracteristic corresponding to fu appears in parse result T. The model parameters, λu, are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T|w) for all parse candidates assigned HEAD noun HEAD verb where li is a lexical entry assigned to word wi in T and p(li|wi) is the probability of selecting lexical entry li for wi. In the experiments, we compared our model with other two types of probabilistic models using a supertagger (Ninomiya et al., 2006). The first one is the simplest probabilistic model, which is defined with only the probabilities of lexical entry selection. It is defined simply as the product of the probabilities of selecting all lexical entries in the sentence; i.e., the model does not use the probabilities of phrase structures like the probabilistic models explained above. Given a set of lexical entries, L, a sentence, w = (w1,... , wn), and the probabilistic model of lexical entry selection, p(li E L|w, i), the first model is formally defined as follows: XZw = T1 62 HEAD noun 1 SUBJ &lt;&gt; COMPS &lt;&gt; froot= &lt;S, has, VBZ, Spri</context>
<context position="12925" citStr="Ninomiya et al. (2006)" startWordPosition="2139" endWordPosition="2142">= funary = froot = flex = ¿fsptag = pmodel1(T |w) = Yn p(li|w, i), i=1 where li is a lexical entry assigned to word wi in T and p(li|w, i) is the probability of selecting lexical entry li for wi. The probabilities of lexical entry selection, p(li|w, i), are defined as follows: (Probabilistic model of lexical entry selection) p(li |w, i) = Z. expÃX λufu (1j, u(la, w, i)! u exp ÃX λufu(li, w, i)I u / where Z,,, is the sum over all possible lexical entries for the word wi. The second model is a hybrid model of supertagging and the probabilistic HPSG. The probabilities are given as the product of Ninomiya et al. (2006)’s model 1 and the probabilistic HPSG. (Ninomiya et al. (2006)’s model 3) pmodel3(T |w) = pmodel1(T |w)phpsg(T |w) In the experiments, we compared our model with Miyao and Tsujii (2005)’s model and Ninomiya et al. (2006)’s model 1 and 3. The features used in our model and their model are combinations of the feature templates listed in Table 1 and Table 2. The feature templates fbinary and funary are defined for constituents at binary and unary branches, froot is a feature template set for the root nodes of parse trees. flex is a feature template set for calculating the unigram reference distri</context>
<context position="15435" citStr="Ninomiya et al. (2006)" startWordPosition="2587" endWordPosition="2590">p), (hw, hl), (sy, hw), (hp, hl), (hp), (hl), (sy) combinations of feature templates for flex (wi, pi, li), (pi, li) combinations of feature templates for fsptag (wi−1), (wi), (wi+1), (pi−2), (pi−1), (pi), (pi+1), (pi+2), (pi+3), (wi−1, wi), (wi, wi+1), (pi−1, wi), (pi, wi), (pi+1, wi), (�P„,i, pi+1, pi+2,pi+3), (pi−2, pi−1,pi), (�p„,i−1, pi, pi+1�„),, (pi, pi+1, pi+2) �„, (pd-2, pi−1), (pi−1, pi), (pi,pi+1), (pi+1, pi+2) Table 2: Combinations of feature templates. (Probabilistic HPSG with an n-gram reference distribution) pnref (T |w) = pmodel 1 (T |w) exp Au fu (T) ÃX Znre f u In our model, Ninomiya et al. (2006)’s model 1 is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in Ninomiya et al. (2006)’s model 1. The formula of our model is the same as Ninomiya et al. (2006)’s model 3. But, their model is not a probabilistic model with a reference distribution. Both our model and their model consist of the probabilities for lexical entries (= pmodel1(T lw)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and their model is the way of how to train model para</context>
<context position="18840" citStr="Ninomiya et al. (2006)" startWordPosition="3167" endWordPosition="3170">erated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG treebank. Znref = ÃX ! pmodel1(T �|w) exp �ufu(T �) � u X 64 No. of tested sentences Total No. of sentences Avg. length of tested sentences Section 23 2,299 (100.00%) 2,299 22.2 Section 24 1,245 (99.84%) 1,247 23.0 Table 3: Statistics of the Penn Treebank. LP Section 23 (Gold POSs) UR UF Avg. time (%) LR LF UP (%) (%) (ms) (%) (%) (%) Miyao and Tsujii (2005)’s model 87.26 86.50 86.88 90.73 89.93 90.33 604 Ninomiya et al. (2006)’s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129 Ninomiya et al. (2006)’s model 3 89.48 88.58 89.02 92.33 91.40 91.86 152 our model 1 89.78 89.28 89.53 92.58 92.07 92.32 234 our model 2 90.03 89.60 89.82 92.82 92.37 92.60 1379 Section 23 (POS tagger) LP LR LF UP UR UF Avg. time (%) (%) (%) (%) (%) (%) (ms) Miyao and Tsujii (2005)’s model 84.96 84.25 84.60 89.55 88.80 89.17 674 Ninomiya et al. (2006)’s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154 Ninomiya et al. (2006)’s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183 Matsuzaki et al. (2007)’s model 86.93 86.47 86.70 - - - 30 our model 1 8</context>
<context position="22496" citStr="Ninomiya et al. (2006)" startWordPosition="3820" endWordPosition="3823">ated our model in two settings. One is implemented with a narrow beam width (‘our model 1’ in the figure), and the other is implemented with a wider beam width (‘our model 2’ in the figure)3. ‘our model 3The beam thresholding parameters for ‘our model 1’ are α0 = 10, Aα = 5, αlast = 30, β0 = 5.0, Aβ = 2.5, βlast = 15.0, δ0 = 10, Aδ = 5, δlast = 30, κ0 = 5.0, Aκ = 2.5, κlast = 15.0, θ0 = 6.0, Aθ = 3.5, and θlast = 20.0. 65 88.00% 87.50% 87.00% 86.50% 86.00% 85.50% 85.00% 84.50% 84.00% 83.50% 83.00% 0 100 200 300 400 500 600 700 800 900 Parsing time (ms/sentence) Miyao and Tsujii (2005)&apos;s model Ninomiya et al. (2006)&apos;s model 1 Ninomiya et al. (2006)&apos;s model 3 our model Figure 3: F-score versus average parsing time for sentences in Section 24 of G 100 words. 1’ was introduced to measure the performance with balanced F-score and speed, which we think appropriate for practical use. ‘our model 2’ was introduced to measure how high the precision and recall could reach by sacrificing speed. Our models increased the parsing accuracy. ‘our model 1’ was around 2.6 times faster and had around 2.65 points higher F-score than Miyao and Tsujii (2005)’s model. ‘our model 2’ was around 2.3 times slower but had around 2.</context>
<context position="24770" citStr="Ninomiya et al. (2006)" startWordPosition="4233" endWordPosition="4236"> the terms with suffixes last. Details of the parameters are written in (Ninomiya et al., 2005). The beam thresholding parameters for ‘our model 2’ are α0 = 18, Dα = 6, αlast = 42, β0 = 9.0, Oβ = 3.0, βlast = 21.0, δ0 = 18, = 6, δlast = 42, κ0 = 9.0, 0κ = 3.0, κlast = 21.0. In ‘our model 2’, the global thresholding was not used. posed a technique for efficient HPSG parsing with supertagging and CFG filtering. Their results with the same grammar and servers are also listed in the lower half of Table 4. They achieved drastic improvement in efficiency. Their parser ran around 6 times faster than Ninomiya et al. (2006)’s model 3, 9 times faster than ‘our model 1’ and 60 times faster than ‘our model 2.’ Instead, our models achieved better accuracy. ‘our model 1’ had around 0.5 higher F-score, and ‘our model 2’ had around 0.8 points higher F-score. Their efficiency is mainly due to elimination of ungrammatical lexical entries by the CFG filtering. They first parse a sentence with a CFG grammar compiled from an HPSG grammar, and then eliminate lexical entries that are not in the parsed CFG trees. Obviously, this technique can also be applied to the HPSG parsing of our models. We think that efficiency of HPSG p</context>
<context position="26726" citStr="Ninomiya et al. (2006)" startWordPosition="4567" endWordPosition="4571">its bad implementation of parser interface. The n-gram reference distribution is incorporated into the kernel of the parser, but the n-gram features and a maximum entropy estimator are defined in other modules; n-gram features are defined in a grammar module, and a maximum entropy estimator for the n-gram reference distribution is implemented with a general-purpose maximum entropy estimator module. Consequently, strings that represent the ngram information are very frequently changed into feature structures and vice versa when they go in and out of the kernel of the parser. On the other hand, Ninomiya et al. (2006)’s model 3 uses the supertagger as an external module. Once the parser acquires the supertagger’s outputs, the n-gram information never goes in and out of the kernel. This advantage of Ninomiya et al. (2006)’s model can apparently be implemented in our model, but this requires many parts of rewriting of the implemented parser. We estimate that the overhead of the interface is around from 50 to 80 ms/sentence. We think that re-implementation of the parser will improve the parsing speed as estimated. In Figure 3, the line of our model crosses the line of Ninomiya et al. (2006)’s model. If the es</context>
<context position="28129" citStr="Ninomiya et al., 2006" startWordPosition="4809" endWordPosition="4812">d a probabilistic model in which supertagging is consistently integrated into the probabilistic model for HPSG. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution (Miyao and Tsujii, 2005) and the probabilistic HPSG with supertagging (Ninomiya et al., 2006). Though our model was not as fast as Ninomiya et al. (2006)’s models, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than Miyao and Tsujii (2005)’s model and around 0.56 points higher F-score than the Ninomiya et al. (2006)’s model 3. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than Miyao and Tsujii (2005)’s model and around 0.8 points higher F-score than Ninomiya et al. (2006)’s model 3. Our model achieved higher F-score because parameters for phrase structures in our model are trained with the supertaggin</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Extremely lexicalized models for accurate and fast HPSG parsing. In Proc. of EMNLP 2006, pages 155–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Jonas Kuhn</author>
<author>Mark Johnson</author>
</authors>
<title>Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training.</title>
<date>2000</date>
<booktitle>In Proc. ofACL’00,</booktitle>
<pages>480--487</pages>
<contexts>
<context position="2441" citStr="Riezler et al., 2000" startWordPosition="340" endWordPosition="344">preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ra</context>
<context position="7907" citStr="Riezler et al., 2000" startWordPosition="1219" endWordPosition="1222">ctures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)Jw E W, F E FJ is a set of lexical entries, and R is a set of schemata; i.e., r E R is a partial function: F x F — F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of 61 HEAD verb SUBJ &lt; &gt; COMPS &lt; &gt; Figure 1: HPSG parsing. SUBJ &lt; &gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; COMPS &lt; 2 2 SUBJ &lt; &gt; parsing. Note that HPSG is one of the lexicalized grammar formalisms, in which lexical entries determine the dominant syntactic structures. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is (Probabilistic HPSG) to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (M</context>
</contexts>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark Johnson. 2000. Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training. In Proc. ofACL’00, pages 480–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1759" citStr="Steedman, 2000" startWordPosition="235" endWordPosition="236">ly introduced, and hence the probabilistic models for parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse tree’s probabilistic model. 1 Introduction For the last decade, fast, accurate and wide-coverage parsing for real-world text has been pursued in sophisticated grammar formalisms, such as headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994), combinatory categorial grammar (CCG) (Steedman, 2000) and lexical function grammar (LFG) (Bresnan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP</booktitle>
<pages>467--474</pages>
<contexts>
<context position="21743" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="3670" endWordPosition="3673"> as the development set, and the performance was evaluated using sentences of &lt; 100 words in Section 23. The performance of each model was analyzed using the sentences in Section 24 of &lt; 100 words. Table 3 details the numbers and average lengths of the tested sentences of &lt; 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is shown in Table 4. The upper half of the table shows the performance using the correct POSs in the Penn Treebank, and the lower half shows the performance using the POSs given by a POS tagger (Tsuruoka and Tsujii, 2005). LF and UF in the figure are labeled F-score and unlabeled F-score. F-score is the harmonic mean of precision and recall. We evaluated our model in two settings. One is implemented with a narrow beam width (‘our model 1’ in the figure), and the other is implemented with a wider beam width (‘our model 2’ in the figure)3. ‘our model 3The beam thresholding parameters for ‘our model 1’ are α0 = 10, Aα = 5, αlast = 30, β0 = 5.0, Aβ = 2.5, βlast = 15.0, δ0 = 10, Aδ = 5, δlast = 30, κ0 = 5.0, Aκ = 2.5, κlast = 15.0, θ0 = 6.0, Aθ = 3.5, and θlast = 20.0. 65 88.00% 87.50% 87.00% 86.50% 86.00% 85.50% 8</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proc. of HLT/EMNLP 2005, pages 467–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary P Harper</author>
</authors>
<title>A statistical constraint dependency grammar (CDG) parser.</title>
<date>2004</date>
<booktitle>In Proc. of ACL’04 Incremental Parsing workshop: Bringing Engineering and Cognition Together,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="3109" citStr="Wang and Harper, 2004" startWordPosition="443" endWordPosition="446">04; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result</context>
<context position="4725" citStr="Wang and Harper, 2004" startWordPosition="692" endWordPosition="695">nually tuned Weighted CDG. Ninomiya et al. (2006) showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999). However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a), or the probabilistic models for phrase structures were trained independently of the supertagger’s probabilistic models (Wang and Harper, 2004; Ninomiya et al., 2006). In the case of supertagging of Weighted CDG (Foth et al., 2006), parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model. We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG. The reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and part-of-speech (POS) n-gra</context>
</contexts>
<marker>Wang, Harper, 2004</marker>
<rawString>Wen Wang and Mary P. Harper. 2004. A statistical constraint dependency grammar (CDG) parser. In Proc. of ACL’04 Incremental Parsing workshop: Bringing Engineering and Cognition Together, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
</authors>
<title>Statistical Parsing and Language Modeling based on Constraint Dependency Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Purdue University.</institution>
<contexts>
<context position="3086" citStr="Wang, 2003" startWordPosition="441" endWordPosition="442">n et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran,</context>
</contexts>
<marker>Wang, 2003</marker>
<rawString>Wen Wang. 2003. Statistical Parsing and Language Modeling based on Constraint Dependency Grammar. Ph.D. thesis, Purdue University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>