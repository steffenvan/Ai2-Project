<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001438">
<title confidence="0.9978735">
Language Independent Authorship Attribution
using Character Level Language Models
</title>
<author confidence="0.995827">
Fuchun Pengt Dale Schuurmanst Vlado Kesel11 Shaojun Wangt
</author>
<affiliation confidence="0.999434">
School of Computer Science, University of Waterloo, Canada
</affiliation>
<email confidence="0.63922">
ff3peng, dale, sjwangl@cs.uwaterloo.ca
</email>
<affiliation confidence="0.86636">
IFaculty of Computing Science, Dalhousie University, Canada
</affiliation>
<email confidence="0.393264">
vlado@cs . dal . ca
</email>
<sectionHeader confidence="0.993127" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988052631579">
We present a method for computer-
assisted authorship attribution based on
character-level n-gram language mod-
els. Our approach is based on sim-
ple information theoretic principles, and
achieves improved performance across
a variety of languages without requir-
ing extensive pre-processing or feature
selection. To demonstrate the effec-
tiveness and language independence of
our approach, we present experimen-
tal results on Greek, English, and Chi-
nese data. We show that our approach
achieves state of the art performance in
each of these cases. In particular, we ob-
tain a 18% accuracy improvement over
the best published results for a Greek
data set, while using a far simpler tech-
nique than previous investigations.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964155555556">
Automated authorship attribution is the problem
of identifying the author of an anonymous text, or
text whose authorship is in doubt (Love, 2002).
A famous example is the Federalist Papers, of
which twelve are claimed to have been written
both by Alexander Hamilton and James Madison
(Holmes and Forsyth, 1995). Recently, vast repos-
itories of electronic text have become available
on the Internet, making the problem of managing
large text collections increasingly important. Au-
tomated text categorization (TC) is a useful way
to organize a large document collection by impos-
ing a desired categorization scheme. For exam-
ple, categorizing documents by their author is an
important case that has become increasingly use-
ful, but also increasingly difficult in the age of
web-documents that can be easily copied, trans-
lated and edited. Author attribution is becoming
an important application in web information man-
agement, and is beginning to play a role in areas
such as information retrieval, information extrac-
tion and question answering.
Many algorithms have been invented for as-
sessing the authorship of given text. These al-
gorithms rely on the fact that authors use lin-
guistic devices at every level—semantic, syntac-
tic, lexicographic, orthographic and morphologi-
cal (Ephratt, 1997)—to produce their text. Typi-
cally, such devices are applied unconsciously by
the author, and thus provide a useful basis for un-
ambiguously determining authorship. The most
common approach to determining authorship is to
use a stylistic analysis that proceeds in two steps:
first, specific style markers are extracted, and sec-
ond, a classification procedure is applied to the
resulting description. These methods are usually
based on calculating lexical measures that repre-
sent the richness of the author&apos;s vocabulary and the
frequency of common word use (Stamatatos et al.,
2001). Style marker extraction is usually accom-
plished by some form of non-trivial NLP analysis,
such as tagging, parsing and morphological analy-
sis. A classifier is then constructed, usually by first
performing a non-trivial feature selection step that
employs mutual information or Chi-square testing
</bodyText>
<page confidence="0.992504">
267
</page>
<bodyText confidence="0.994767231884058">
to determine relevant features.
There are several problems with this standard
approach however. First, techniques used for style
marker extraction are almost always language de-
pendent, and in fact differ dramatically from lan-
guage to language. For example, an English parser
usually cannot be applied to German or Chinese.
Second, feature selection is not a trivial process,
and usually involves setting thresholds to elim-
inate uninformative features (Scott and Matwin,
1999). These decisions can be extremely sub-
tle, because although rare features contribute less
signal than common features, they can still have
an important cumulative effect (Aizawa, 2001).
Third, current authorship attribution systems in-
variably perform their analysis at the word level.
However, although word level analysis seems to
be intuitive, it ignores the fact that morphological
features can also play an important role, and more-
over that many Asian languages such as Chinese
and Japanese do not have word boundaries explic-
itly identified in text. In fact, word segmentation
itself is a difficult problem in Asian languages,
which creates an extra level of difficulty in coping
with the errors this process introduces.
In this paper, we propose a simple method that
avoids each of these problems. Our approach is
based on building a character-level&apos;n-gram model
of an author&apos;s writing, using techniques from sta-
tistical language modeling. Language modeling
is concerned with capturing regularities of natural
language—for example, semantic, syntactic, lex-
icographic and morphological patterns—that can
be used to make predictions. Many of the fea-
tures considered in language modeling coincide
with those used in authorship attribution, and it is
therefore natural to apply language modeling con-
cepts to this problem. To perform authorship at-
tribution, we build a character-level&apos;n-gram lan-
guage model for each author. This approach ex-
ploits morphological features while avoiding the
need for explicitly segmented words. By consid-
ering all possible character n-grams as potential
features, we also avoid the need to run sophisti-
cated NLP tools, such as parsers and taggers, to
produce candidate features. Finally, we avoid fea-
ture selection entirely by including every feature
in the model, but use estimation methods from sta-
tistical language modeling to avoid over-fitting a
sparse set of training data. The result is a surpris-
ingly simple, effective approach to authorship at-
tribution that is completely language independent.
2 n-Gram Language Modeling
The dominant motivation for language modeling
has traditionally come from speech recognition,
but language models have recently become widely
used in many other application areas, such as in-
formation retrieval, machine translation, optical
character recognition, spelling correction, docu-
ment classification, and bio-informatics.
The goal of language modeling is to predict the
probability of naturally occurring word sequences,
s = wiw2...tvN; or more simply, to put high prob-
ability on word sequences that actually occur (and
low probability on word sequences that never oc-
cur). Given a word sequence &apos;iv&apos; w2...wN to be
used as a test corpus, the quality of a language
model can be measured by the empirical perplex-
ity and entropy scores on this corpus
</bodyText>
<equation confidence="0.96390725">
1
Perplexity =
z =1 wi -1)
Entropy = log 2 Perplexity
</equation>
<bodyText confidence="0.9998848">
where the goal is to minimize these measures.
The simplest and most successful approach to
language modeling is still based on the n-gram
model. By the chain rule of probability we can
write the probability of any word sequence as
</bodyText>
<equation confidence="0.989896">
Pr (iv w2... wN) =
i=1
</equation>
<bodyText confidence="0.999896">
An n-gram model approximates this probability
by assuming that the only words relevant to pre-
dicting Pr(wwi ... w1) are the previous n - 1
words; that is, it assumes
A straightforward maximum likelihood estimate
of&apos;n-gram probabilities from a corpus is given by
the observed frequency of each of the patterns
</bodyText>
<equation confidence="0.935393">
#(wi-n+1.-wi)
Pr (wilw+1...wi_i) =
(2)
</equation>
<page confidence="0.985536">
268
</page>
<bodyText confidence="0.999964380952381">
where #(.) denotes the number of occurrences of
a specified gram in the training corpus. Although
one could attempt to use simple n-gram models
to capture long range dependencies in language,
attempting to do so directly immediately creates
sparse data problems: Using grams of length up to
n entails estimating the probability of Wn events,
where W is the size of the word vocabulary. This
quickly overwhelms modern computational and
data resources for even modest choices of n (be-
yond 3 to 6). Also, because of the heavy tailed
nature of language (i.e. Zipf&apos;s law) one is likely
to encounter novel n-grams that were never wit-
nessed during training in any test corpus, and
therefore some mechanism for assigning non-zero
probability to novel n-grams is a central and un-
avoidable issue in statistical language modeling.
One standard approach to smoothing probability
estimates to cope with sparse data problems (and
to cope with potentially missing n-grams) is to use
some sort of back-off estimator.
</bodyText>
<equation confidence="0.960727285714286">
Pr(Wi 1Wz—n+1.—Wi-1)
1 Pr(wilwi-n+1.-wi-i),
if #(wi_n+1...wi) &gt; 0
X Pr(Wi
kri—n+2.••Wi-1),
otherwise
(3)
</equation>
<bodyText confidence="0.999979636363636">
1998). The details of the smoothing techniques
are omitted here for simplicity.
The language models described above use indi-
vidual words as the basic unit, although one can
easily consider models that use individual char-
acters as the basic unit instead. The rest of the
details remain the same in this case. The only dif-
ference is that the character vocabulary is always
much smaller than the word vocabulary, which
means that one can normally use a much larger
context n in a character-level model (although the
text spanned by a character model is still usually
less than that spanned by a word model). The ben-
efits of the character-level model in the context of
author attribution are that it avoids the need for ex-
plicit word segmentation in the case of Asian lan-
guages, it captures important morphological prop-
erties of an author&apos;s writing, it can still discover
useful inter-word and inter-phrase features, and it
greatly reduces the sparse data problems associ-
ated with large vocabulary models. We experiment
with character-level models below.
</bodyText>
<sectionHeader confidence="0.932989" genericHeader="method">
3 Language Models for Author Attribution
</sectionHeader>
<bodyText confidence="0.999022285714286">
Our approach to applying language models to au-
thor attribution is to use Bayesian decision theory.
Assume we wish to classify a text D into an au-
thor category c E C = {ci , }. A natural
choice is to pick the category c that has the largest
posterior probability given the text. That is,
where
</bodyText>
<equation confidence="0.9656839">
Pr(Wi IWz—n+1.—Wz-1) =
#(Wi—n+1.—Wz-1)
c* = arg max{Pr(c}D)}
cEc
discount#(wi_n+1...wi)
(6)
is the discounted probability and
(wi_n+1...wi_i) is a normalization constant
1 —
1 —
</equation>
<bodyText confidence="0.989740125">
The discounted probability (4) can be computed
with different smoothing techniques, including
linear smoothing, absolute smoothing, Good-
Turing smoothing and Witten-Bell smoothing. Al-
though this choice has been found to make a differ-
ence in practice, Good-Turing smoothing is nor-
mally a reasonable choice (Chen and Goodman,
Using Bayes rule, this can be rewritten as
</bodyText>
<equation confidence="0.98789">
c* = arg max{Pr(D1c) Pr(c)}
cEc (7)
= arg max{Pr(D1c)} (8)
cEC
</equation>
<bodyText confidence="0.999899818181818">
where deducing Eq. (8) from Eq. (7) assumes
uniformly weighted categories (since we have no
other prior knowledge in this case). Here, Pr (D c)
is the likelihood of D under category c, which can
be computed by Eq. (1). Therefore, our approach
is to learn a separate language model for each au-
thor, by training on a data set from that author.
Then, to categorize a new text D, we supply D
to each language model, evaluate the likelihood of
D under the model, and pick the winning author
category according to Eq. (8).
</bodyText>
<page confidence="0.99655">
269
</page>
<sectionHeader confidence="0.999272" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999974833333333">
In this section, we present experimental results for
our approach on three different languages. We first
describe the performance measures used in our ex-
periments, and then present results on Greek data,
English data and Chinese data, in Sections 4.2, 4.3
and 4.4 respectively.
</bodyText>
<subsectionHeader confidence="0.981017">
4.1 Performance Measures
</subsectionHeader>
<bodyText confidence="0.987231888888889">
We assume each text document we are classifying
is written by a single author. (Although it is inter-
esting to consider the problem of classifying mul-
tiply authored texts, we do not discuss this case
in this paper.) In the case where each text only
belongs to one author, the performance of an au-
thorship classifier can be naturally measured by its
overall accuracy: the number of correctly classi-
fied texts divided by the number of texts classified
overall. However, overall accuracy does not char-
acterize how the classifier performs on each indi-
vidual category. Therefore, to measure classifier
performance for each category we use the preci-
sion, recall, and macro-average F-measure scores.
For a given category c, precision is the number of
correctly classified texts in c, divided by the num-
ber of all texts classified to be in c. Recall is the
number of correctly classified texts in c, divided
by the number of all texts that truly belong to c. F-
measure is a combination of precision and recall
2 xprectSZOTIx recall
defined as A macro-average F-
preciston+recall
measure is computed by averaging over all indi-
vidual F-measures for each category. We now pro-
ceed to present our results for the three different
languages.
</bodyText>
<subsectionHeader confidence="0.993981">
4.2 Greek data
</subsectionHeader>
<bodyText confidence="0.995399142857143">
We have experimented with two Greek data sets,
A and B, used in the studies of (Stamatatos et
al., 1999) and (Stamatatos et al., 2000). Both
sets were originally downloaded from the web-
site of the Modern Greek Weekly Newspaper TO
BHMA. Each of the two data sets consists of 200
singly-authored documents written by 10 differ-
ent authors, with 20 different documents written
by each author. In our experiments we used 10 of
each authors&apos; documents as training data and 10 as
test instances. The specific authors that appear are
shown in Table 1.
The main difference between the two sets is that
the documents in group A are written by journal-
</bodyText>
<table confidence="0.999857954545455">
Data Author Name Train size
set (characters)
A AO G. Bitros 47868
Al K. Chalbatzakis 71889
A2 G. Lakopoulos 77549
A3 T. Lianos 45766
A4 N. Marakis 59785
AS D. Mitropoulos 70210
A6 D. Nikolakopoulos 75316
A7 N. Nikolaou 51025
A8 D. Psychogios 35886
A9 R. Someritis 50816
B BO S. Alaxiotis 77295
B1 G. Babiniotis 75965
B2 G. Dertilis 66810
B3 C. Kiosse 102204
B4 A. Liakos 89519
B5 D. Maronitis 36665
B6 M. Ploritis 72469
B7 T. Tasios 80267
B8 K. Tsoukalas 104065
B9 G. Vokos 64479
</table>
<tableCaption confidence="0.999938">
Table 1: Authors in two Greek data sets, A and B.
</tableCaption>
<bodyText confidence="0.998672047619048">
ists on a variety of topics, including news reports,
editorials, etc., whereas the documents in group B
are written by scholars on topics in science, his-
tory, culture, etc. The result is that the documents
in group A are more heterogeneous in their style,
whereas the documents in group B are more ho-
mogeneous owing to the more rigid strictures of
academic writing (Stamatatos et al., 2000).
In our experiments, we obtained the best perfor-
mance on both group A and group B by using a 3-
gram model with absolute smoothing. The best ac-
curacy we obtained on test documents from group
A is 74%, and 90% on group B. This compares
favorably to the best accuracy reported in (Sta-
matatos et al., 2000) of 72% on group A and 70%
on group B.1 Thus, our accuracy improvement is
2% on group A and 18% on group B, which is sur-
prising given the relative simplicity of our method.
&apos;Note that Stamatatos et al&apos;s measures of identification
error and average error correspond to our recall and overall
accuracy measures respectively.
</bodyText>
<page confidence="0.990614">
270
</page>
<table confidence="0.999624033333333">
Results on group A
Computer Estimate
True Label AO Al A2 A3 A4 A5 A6 A7 A8 A9 Recall
AO 10 1.00
Al 6 2 2 0.60
A2 9 1 0.90
A3 2 2 5 1 0.50
A4 9 1 0.90
A5 1 9 0.90
A6 3 7 0.70
A7 3 1 6 0.60
A8 2 1 3 4 0.30
A9 10 1.00
Precision 0.83 1.00 0.45 0.71 1.00 0.82 0.88 0.67 1.00 0.67
Overall Accuracy: 0.74 Macro-average F-measure: 0.73
Results on group B
Computer Estimate
True Label BO B1 B2 B3 B4 B5 B6 B7 B8 B9 Recall
BO 8 2 0.8
B1 10 1.0
B2 8 1 1 0.8
B3 10 1.0
B4 10 1.0
B5 2 3 4 1 0.4
B6 10 1.0
B7 10 1.0
B8 10 1.0
B9 10 1.0
Precision 1.00 0.83 1.00 1.00 0.77 1.00 0.91 1.00 0.77 0.91
Overall Accuracy: 0.90 Macro-average F-measure: 0.89
</table>
<tableCaption confidence="0.999441">
Table 2: Experimental results on the Greek data sets.
</tableCaption>
<bodyText confidence="0.999984666666667">
To compare individual author categories, Table 2
gives the confusion matrices obtained on the two
data sets. Comparing Table 2 to (Stamatatos et al.,
2000, Table 6), shows that we obtain better results
in every category of group B, and perform slightly
better on group A authors, except K. Chalbatzakis
(author A1-0.6 versus 0.7 recall).
Note that our performance on group B is much
stronger than group A, whereas similar results
were obtained on both groups in (Stamatatos et
al., 2000). This suggests that our language model-
ing approach is more effective at capturing author-
specific idiosyncrasies in a homogeneous collec-
tion like B, but in collections like A where a given
author may write of many different types of text,
our method is less effective. It appears that some-
times genre might dominate authorship when it
comes to style. By looking at the confusion ma-
trices, one can see that authors Al, A3, AS, A6,
A7 are mistakenly identified as A2, resulting in a
low precision for A2 (0.45). Also, many of the
documents produced by A8 are not correctly iden-
tified, resulting in a low recall for A8 (0.3). We are
investigating whether genre is responsible for this.
</bodyText>
<page confidence="0.992822">
271
</page>
<subsectionHeader confidence="0.989861">
4.3 English data
</subsectionHeader>
<bodyText confidence="0.98744225">
The English data used in our experiments is avail-
able from the Alex Catalogue of Electronic Texts.2
We used the 8 most prolific authors from this col-
lection, shown in Table 3.
</bodyText>
<table confidence="0.9991756">
Author Name Training size
word(character)
E0 Charles Dickens 1614258 (9033267)
El John Keats 49314(335676)
E2 John Milton 146446(868857)
E3 William Shakespeare 645605 (3642829)
E4 Robert L. Stevenson 1036108 (5687003)
E5 Oscar Wilde 102092 (585092)
E6 Ralph W. Emerson 384570 (2201546)
E7 Edgar Allan Poe 307710 (1785067)
</table>
<tableCaption confidence="0.999869">
Table 3: Authors appearing in the English data set.
</tableCaption>
<bodyText confidence="0.999936636363636">
To reduce any sparse data problems we might
face, we first converted the corpus into lowercase
characters and only used 30 most frequent char-
acters in the vocabulary, which comprises over
99% of all character occurrences in the corpus.
The best accuracy we obtained was 98%, which
was achieved by a 6-gram model using absolute
smoothing. This is excellent performance. How-
ever, it is probably due to the distinct idiosyncratic
writing styles of these famous authors. We are in-
vestigating more challenging data sets.
</bodyText>
<subsectionHeader confidence="0.999541">
4.4 Chinese data
</subsectionHeader>
<bodyText confidence="0.999918357142857">
Authorship attribution in Chinese normally re-
quires an initial word segmentation phase, fol-
lowed by a feature extraction process at the word
level, as in English. However, word segmenta-
tion is itself a hard problem in Chinese, and an
improper segmentation may cause insurmountable
problems for later prediction phases. We avoid the
word segmentation problem by simply operating
at the character level.
The Chinese corpus we used in our experiments
was also downloaded from the Internet.3 Eight
of the most popular modern Chinese martial art
novelists were included in this study, shown in Ta-
ble 4. One or two novels was selected from each
</bodyText>
<footnote confidence="0.998543">
2http://www.infomotions.com/alex/downloads/
3 http://chineseculture.about.com/library/chinese/blindex.htm
</footnote>
<bodyText confidence="0.507631">
author to be used as training data, and 20 addi-
tional novels were used as a test set.
</bodyText>
<table confidence="0.999756">
Author Name Training size
(character)
CO Gu Long 1466286
Cl Huang Yi 1860555
C2 Jin Yong 979885
C3 Liang Yusheng 1085179
C4 Wen RuiAn 536986
C5 Xiao Yi 875678
C6 Chen Qinyun 857929
C7 Wo Losheng 1554689
</table>
<tableCaption confidence="0.999756">
Table 4: Authors appearing in the Chinese data.
</tableCaption>
<bodyText confidence="0.9999393125">
Note that a significant difference between Chi-
nese and English or Greek is that the Chinese char-
acter vocabulary is much larger than the English
or Greek character vocabularies. For example, the
most commonly used Chinese character set con-
tains 6763 characters. In our experiments, we en-
countered about 4600 distinct Chinese characters.
Therefore, to reduce the sparse data problems we
might encounter, we first selected the most fre-
quent 2500 characters as our vocabulary, which
comprised about 99% of all character occurrences.
The best overall accuracy we obtained was
94%, with a 3-gram language model using Witten-
Bell smoothing. Once again, this is effective per-
formance, but possibly obtained on an easy data
set. We undertake a more detailed analysis below.
</bodyText>
<sectionHeader confidence="0.974495" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999971">
As discussed in Section 2, the perplexity of an
ri-gram language model depends on several fac-
tors, including the context length n, the smoothing
technique, and the size of training corpus. Differ-
ent choices of these factors will result in different
perplexities in the test corpus, which could influ-
ence the final decision in using Eq. (8). To ascer-
tain the sensitivity of our results to these factors,
we assess their influence in turn.
</bodyText>
<subsectionHeader confidence="0.997397">
5.1 Influence of Context Length
</subsectionHeader>
<bodyText confidence="0.9988788">
The context length n is a key factor in&apos;n-gram lan-
guage modeling. A context n that is too small will
not capture sufficient information to accurately
model character dependencies. On the other hand,
a context n that is too large will create sparse data
</bodyText>
<page confidence="0.991413">
272
</page>
<bodyText confidence="0.999440875">
problems in training Both extremes will result in
a larger perplexity than an optimal length. In a
word level n-gram model, n must normally be set
to 2 or 3 to obtain optimal performance. However,
it would seem intuitive that a longer context would
be more effective at the character level. The influ-
ence of context length n in each of the three lan-
guages is illustrated in Figure 1. Here we see that
</bodyText>
<figure confidence="0.653959">
Context length N
</figure>
<figureCaption confidence="0.999823">
Figure 1: Influence of context length on accuracy.
</figureCaption>
<bodyText confidence="0.999747285714286">
authorship attribution accuracy initially increases
with increasing context, but begins to degrade as
sparse data problems begin to take hold. Inter-
estingly, the optimal context length is different in
each of the three languages. In each case, the re-
sults are not overly sensitive after a minimal con-
text length has been reached.
</bodyText>
<subsectionHeader confidence="0.998241">
5.2 Influence of Smoothing Technique
</subsectionHeader>
<bodyText confidence="0.999973">
Another key factor affecting the performance of a
language model is the smoothing technique used.
It has been found that Good-Turing smoothing
performs effectively in many contexts (Chen and
Goodman, 1998). However, our goal is to make a
final decision based on the ranking of perplexities,
not just their absolute values, which means that
the best smoothing technique for language model-
ing in the sense of perplexity may not be the best
choice for text categorization. Indeed, we find that
the language models using Good-Turing smooth-
ing do not always perform best in our experiments.
The effects of smoothing in each of the three lan-
guages is illustrated in Figure 2. Figure 2 shows
that in most cases the smoothing technique does
not have a significant effect on authorship attribu-
tion accuracy, except in one case (Greek) where
</bodyText>
<figure confidence="0.982431555555556">
2 25 3 3.5 4 4.5 5 55 6
English
r=d
g 0.4
0
5
Cheese
2 2.5 3 3.5 4
Context length N
</figure>
<figureCaption confidence="0.999949">
Figure 2: Influence of smoothing on accuracy.
</figureCaption>
<bodyText confidence="0.999895">
Good-Turing smoothing is not as effective as other
techniques. Nevertheless, for the most part, one
can use any standard smoothing technique in this
problem and obtain comparable performance, be-
cause the rankings they produce are almost always
the same.
</bodyText>
<subsectionHeader confidence="0.992178">
5.3 Influence of Training Size
</subsectionHeader>
<bodyText confidence="0.99987016">
Clearly, the size of the training corpus can affect
the accuracy of a language model. Normally with
a larger training corpus more reliable statistics can
be recovered which leads to better prediction ac-
curacy on test data. To test the effect of training
set size we obtained an additional 10 documents
from each author in the group B Greek data set.
In fact, this same additional data has been used in
(Stamatatos et al., 2001) to improve the accuracy
of their method from 71% to 87%. Here we find
that the extra training data also improves the accu-
racy of our method, although not so dramatically.
Figure 3 shows the improvement obtained for n-
gram language models using absolute smoothing.
Here we can see that indeed the extra train-
ing data uniformly improves attribution accuracy.
On the augmented training data the best model
(3-gram) now obtains a 92% attribution accu-
racy, compared to the 90% we obtained originally.
Moreover, this improves the best result obtained
in (Stamatatos et al., 2001) of 87%. However, our
improvement (90% to 92%) is not nearly as great
as that obtained by (Stamatatos et al., 2001) (71%
to 87%). However, this could be due to the fact
that it is harder to reduce a small prediction error.
</bodyText>
<figure confidence="0.970579666666667">
Greek
1
0.8
</figure>
<page confidence="0.994901">
273
</page>
<bodyText confidence="0.968515461538461">
cantly deeper NLP techniques. The simplicity of
our method, however, makes it immediately ap-
plicable to any natural language and yields effec-
tive baseline performance. We have experimen-
tally analyzed the influence of various factors that
can affect the accuracy of our approach and found
that, for the most part, our results are fairly robust
to perturbations of the method. We are investigat-
ing alternative data sets and additional languages.
8 Acknowledgments
We thank E. Stamatatos for supplying us with the
Greek data. Research supported by Bell Univer-
sity Labs and MITACS.
</bodyText>
<sectionHeader confidence="0.667594" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.98801234375">
A. Aizawa. 2001. Linguistic Techniques to Improve
the Performance of Automatic Text Categorization.
In Proceedings 6th NLP Pac. Rim Symp. NLPRS-01.
C. Apte, F. Damerau and S. Weiss. 1994. Toward
Language Independent Automated Learning of Text
Categorization Models. In Proceedings SIGIR-94.
T. Bell, J. Cleary and I. Witten. 1990. Text Compres-
sion. Prentice Hall.
W. Cavnar and J. Trenkle. 1994. N-Gram-Based Text
Categorization. In Proceedings SDAIR-94.
S. Chen and J. Goodman. 1998. An Empirical Study
of Smoothing Techniques for Language Modeling.
TR-10-98, Harvard.
M. Ephratt. 1997. Authorship Attribution- the Case of
Lexical Innovations. In Proc. ACH-ALLC-97.
D. Holmes and R. Forsyth. 1995. The Federalist Re-
visited: New Directions in Authorship Attribution.
In Literary and Linguistic Computing, 10, 111-127.
H. Love, (2002). Attributing Authorship: An Introduc-
tion. Cambridge University Press.
S. Scott and S. Matwin. 1999. Feature Engineering for
Text Classification, In Proceedings ICML-99.
E. Stamatatos, N. Fakotakis and G. Kokkinakis. 1999.
Automatic Authorship Attribution, In EACL-99
E. Stamatatos, N. Fakotakis and G. Kokkinakis. 2000.
Automatic Text Categorization in Terms of Genre
and Author. Comput. Ling., 26 (4), pp. 471-495.
E. Stamatatos, N. Fakotakis and G. Kokkinakis.
2001. Computer-based Authorship Attribution
without Lexical Measures Computers and the Hu-
inanities, 35, pp. 193-214.
I. Witten, Z. Bray, M. Mahoui and W. Teahan. 1999.
</reference>
<figureCaption confidence="0.7261868">
Text mining: A New Frontier for Lossless Compres-
sion. Proceedings IEEE Data Compression 97
35 4
Context length N
Figure 3: Influence of training size on accuracy.
</figureCaption>
<sectionHeader confidence="0.915689" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.992987242424242">
In principle, any language modeling technique can
be used to perform authorship attribution based on
Eq. (8). However, n-gram models are extremely
simple and have been found to be effective in
many applications. For example, character level
n-gram language models can be easily applied to
any language, and even non-language sequences
such as DNA and music. Character level n-gram
models are widely used in text compression—e.g.,
the PPM model (Bell et al., 1990)—and have re-
cently been found to be effective in text mining
problems as well (Witten et al., 2000). Text cat-
egorization with n-gram models has also been at-
tempted by (Cavnar and Trenkle, 1994). However,
they use&apos;n-grams as features for traditional feature
selection, and then deploy classifiers based on cal-
culating feature-vector similarities. In the domain
of language independent text categorization, Apt
et al.(Apt et al., 1994) have used word-based lan-
guage modeling techniques for both English and
German. However, their techniques do not apply
to Asian languages where word segmentation re-
mains a significant problem.
7 Conclusion
We have presented a novel approach to automated
authorship attribution that is based on character
level n-gram language modeling. We have demon-
strated our approach on three different languages
and obtained effective performance in each case.
In particular, we have obtained a 18% accuracy
improvement for one Greek data set (group B),
over a state of the art system that uses signifi-
274
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649904">
<title confidence="0.9989075">Language Independent Authorship Attribution using Character Level Language Models</title>
<author confidence="0.998841">Pengt Dale Schuurmanst Vlado Shaojun Wangt</author>
<affiliation confidence="0.997824">School of Computer Science, University of Waterloo, Canada</affiliation>
<email confidence="0.852474">ff3peng,dale,sjwangl@cs.uwaterloo.ca</email>
<affiliation confidence="0.966944">IFaculty of Computing Science, Dalhousie University, Canada</affiliation>
<email confidence="0.785643">vlado@cs.dal.ca</email>
<abstract confidence="0.9992903">We present a method for computerassisted authorship attribution based on character-level n-gram language models. Our approach is based on simple information theoretic principles, and achieves improved performance across a variety of languages without requiring extensive pre-processing or feature selection. To demonstrate the effectiveness and language independence of our approach, we present experimental results on Greek, English, and Chinese data. We show that our approach achieves state of the art performance in each of these cases. In particular, we obtain a 18% accuracy improvement over the best published results for a Greek data set, while using a far simpler technique than previous investigations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>