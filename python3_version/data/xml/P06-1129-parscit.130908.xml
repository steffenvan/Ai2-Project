<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999267">
Exploring Distributional Similarity Based Models
for Query Spelling Correction
</title>
<author confidence="0.991146">
Mu Li
</author>
<affiliation confidence="0.977009">
Microsoft Research Asia
</affiliation>
<address confidence="0.955635666666667">
5F Sigma Center
Zhichun Road, Haidian District
Beijing, China, 100080
</address>
<email confidence="0.999329">
muli@microsoft.com
</email>
<author confidence="0.99905">
Yang Zhang
</author>
<affiliation confidence="0.997085333333333">
School of
Computer Science and Technology
Tianjin University
</affiliation>
<address confidence="0.673075">
Tianjin, China, 300072
</address>
<email confidence="0.96577">
yangzhang@tju.edu.cn
</email>
<bodyText confidence="0.981229947368421">
bstract
A query speller is crucial to search en-
gine in improving web search relevance.
This paper describes novel methods for
use of distributional similarity estimated
from query logs in learning improved
query spelling correction models. The
key to our methods is the property of dis-
tributional similarity between two terms:
it is high between a frequently occurring
misspelling and its correction, and low
between two irrelevant terms only with
similar spellings. We present two models
that are able to take advantage of this
property. Experimental results demon-
strate that the distributional similarity
based models can significantly outper-
form their baseline systems in the web
query spelling correction task.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99914825">
Investigations into query log data reveal that
more than 10% of queries sent to search engines
contain misspelled terms (Cucerzan and Brill,
2004). Such statistics indicate that a good query
speller is crucial to search engine in improving
web search relevance, because there is little op-
portunity that a search engine can retrieve many
relevant contents with misspelled terms.
</bodyText>
<subsectionHeader confidence="0.87164075">
Muhua Zhu
School of
Information Science and Engineering
Northeastern University
</subsectionHeader>
<address confidence="0.832162">
Shenyang, Liaoning, China, 110004
</address>
<email confidence="0.992526">
zhumh@ics.neu.edu.cn
</email>
<author confidence="0.546387">
Ming Zhou
</author>
<affiliation confidence="0.292068">
Microsoft Research Asia
</affiliation>
<address confidence="0.269335">
5F Sigma Center
Zhichun Road, Haidian District
Beijing, China, 100080
</address>
<email confidence="0.852608">
mingzhou@microsoft.com
</email>
<bodyText confidence="0.9997439375">
The problem of designing a spelling correction
program for web search queries, however, poses
special technical challenges and cannot be well
solved by general purpose spelling correction
methods. Cucerzan and Brill (2004) discussed in
detail specialties and difficulties of a query spell
checker, and illustrated why the existing methods
could not work for query spelling correction.
They also identified that no single evidence, ei-
ther a conventional spelling lexicon or term fre-
quency in the query logs, can serve as criteria for
validate queries.
To address these challenges, we concentrate
on the problem of learning improved query spell-
ing correction model by integrating distributional
similarity information automatically derived
from query logs. The key contribution of our
work is identifying that we can successfully use
the evidence of distributional similarity to
achieve better spelling correction accuracy. We
present two methods that are able to take advan-
tage of distributional similarity information. The
first method extends a string edit-based error
model with confusion probabilities within a gen-
erative source channel model. The second
method explores the effectiveness of our ap-
proach within a discriminative maximum entropy
model framework by integrating distributional
similarity-based features. Experimental results
demonstrate that both methods can significantly
outperform their baseline systems in the spelling
correction task for web search queries.
</bodyText>
<page confidence="0.937502">
1025
</page>
<note confidence="0.524234">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1025–1032,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999448125">
The rest of the paper is structured as follows:
after a brief overview of the related work in Sec-
tion 2, we discuss the motivations for our ap-
proach, and describe two methods that can make
use of distributional similarity information in
Section 3. Experiments and results are presented
in Section 4. The last section contains summaries
and outlines promising future work.
</bodyText>
<sectionHeader confidence="0.9997" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999909555555556">
The method for web query spelling correction
proposed by Cucerzan and Brill (2004) is
essentially based on a source channel model, but
it requires iterative running to derive suggestions
for very-difficult-to-correct spelling errors. Word
bigram model trained from search query logs is
used as the source model, and the error model is
approximated by inverse weighted edit distance
of a correction candidate from its original term.
The weights of edit operations are interactively
optimized based on statistics from the query logs.
They observed that an edit distance-based error
model only has less impact on the overall
accuracy than the source model. The paper
reports that un-weighted edit distance will cause
the overall accuracy of their speller’s output to
drop by around 2%. The work of Ahmad and
Kondrak (2005) tried to employ an unsupervised
approach to error model estimation. They
designed an EM (Expectation Maximization)
algorithm to optimize the probabilities of edit
operations over a set of search queries from the
query logs, by exploiting the fact that there are
more than 10% misspelled queries scattered
throughout the query logs. Their method is
concerned with single character edit operations,
and evaluation was performed on an isolated
word spelling correction task.
There are two lines of research in conventional
spelling correction, which deal with non-word
errors and real-word errors respectively. Non-
word error spelling correction is concerned with
the task of generating and ranking a list of possi-
ble spelling corrections for each query word not
found in a lexicon. While traditionally candidate
ranking is based on manually tuned scores such
as assigning weights to different edit operations
or leveraging candidate frequencies, some statis-
tical models have been proposed for this ranking
task in recent years. Brill and Moore (2000) pre-
sented an improved error model over the one
proposed by Kernigham et al. (1990) by allowing
generic string-to-string edit operations, which
helps with modeling major cognitive errors such
as the confusion between le and al. Toutanova
and Moore (2002) further explored this via ex-
plicit modeling of phonetic information of Eng-
lish words. Both these two methods require mis-
spelled/correct word pairs for training, and the
latter also needs a pronunciation lexicon. Real-
word spelling correction is also referred to as
context sensitive spelling correction, which tries
to detect incorrect usage of valid words in certain
contexts (Golding and Roth, 1996; Mangu and
Brill, 1997).
Distributional similarity between words has
been investigated and successfully applied in
many natural language tasks such as automatic
semantic knowledge acquisition (Dekang Lin,
1998) and language model smoothing (Essen and
Steinbiss, 1992; Dagan et al., 1997). An investi-
gation on distributional similarity functions can
be found in (Lillian Lee, 1999).
</bodyText>
<sectionHeader confidence="0.9099725" genericHeader="method">
3 Distributional Similarity-Based Mod-
els for Query Spelling Correction
</sectionHeader>
<subsectionHeader confidence="0.999685">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999295290322581">
Most of the previous work on spelling correction
concentrates on the problem of designing better
error models based on properties of character
strings. This direction ever evolves from simple
Damerau-Levenshtein distance (Damerau, 1964;
Levenshtein, 1966) to probabilistic models that
estimate string edit probabilities from corpus
(Church and Gale, 1991; Mayes et al, 1991; Ris-
tad and Yianilos, 1997; Brill and Moore, 2000;
and Ahmad and Kondrak, 2005). In the men-
tioned methods, however, the similarities be-
tween two strings are modeled on the average of
many misspelling-correction pairs, which may
cause many idiosyncratic spelling errors to be
ignored. Some of those are typical word-level
cognitive errors. For instance, given the query
term adventura, a character string-based error
model usually assigns similar similarities to its
two most probable corrections adventure and
aventura. Taking into account that adventure has
a much higher frequency of occurring, it is most
likely that adventure would be generated as a
suggestion. However, our observation into the
query logs reveals that adventura in most cases is
actually a common misspelling of aventura. Two
annotators were asked to judge 36 randomly
sampled queries that contain more than one term,
and they agreed upon that 35 of them should be
aventura.
To solve this problem, we consider alternative
methods to make use of the information beyond a
</bodyText>
<page confidence="0.991083">
1026
</page>
<bodyText confidence="0.999529093023256">
term’s character strings. Distributional similarity
provides such a dimension to view the possibility
that one word can be replaced by another based
on the statistics of words co-occuring with them.
Distributional similarity has been proposed to
perform tasks such as language model smoothing
and word clustering, but to the best of our
knowledge, it has not been explored in estimat-
ing similarities between misspellings and their
corrections. In this section, we will only involve
the consine metric for illustration purpose.
Query logs can serve as an excellent corpus
for distributional similarity estimation. This is
because query logs are not only an up-to-date
term base, but also a comprehensive spelling er-
ror repository (Cucerzan and Brill, 2004; Ahmad
and Kondrak, 2005). Given enough size of query
logs, some misspellings, such as adventura, will
occur so frequently that we can obtain reliable
statistics of their typical usage. Essential to our
method is the observation of high distributional
similarity between frequently occurring spelling
errors and their corrections, but low between ir-
relevant terms. For example, we observe that
adventura occurred more than 3,300 times in a
set of logged queries that spanned three months,
and its context was similar to that of aventura.
Both of them usually appeared after words like
peurto and lyrics, and were followed by mall,
palace and resort. Further computation shows
that, in the tf (term frequency) vector space based
on surrounding words, the cosine value between
them is approximately 0.8, which indicates these
two terms are used in a very similar way among
all the users trying to search aventura. The co-
sine between adventura and adventure is less
than 0.03 and basically we can conclude that
they are two irrelevant terms, although their
spellings are similar.
Distributional similarity is also helpful to ad-
dress another challenge for query spelling correc-
tion: differentiating valid OOV terms from fre-
quently occurring misspellings.
</bodyText>
<table confidence="0.9913182">
InLex Freq Cosine
vaccum No 18,430 0.99
vacuum Yes 158,428
seraphin No 1,718 0.30
seraphim Yes 14,407
</table>
<tableCaption confidence="0.8546778">
Table 1. Statistics of two word pairs
with similar spellings
Table 1 lists detailed statistics of two word
pairs, each of pair of words have similar spelling,
lexicon and frequency properties. But the distri-
</tableCaption>
<bodyText confidence="0.99941175">
butional similarity between each pair of words
provides the necessary information to make cor-
rection classification that vacuum is a spelling
error while seraphin is a valid OOV term.
</bodyText>
<subsectionHeader confidence="0.999885">
3.2 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.99946">
In this work, we view the query spelling correc-
tion task as a statistical sequence inference prob-
lem. Under the probabilistic model framework, it
can be conceptually formulated as follows.
Given a correction candidate set C for a query
string q:
</bodyText>
<equation confidence="0.989203">
C = {c  |EditDist(q, c) &lt; d
</equation>
<bodyText confidence="0.99749275">
in which each correction candidate c satisfies the
constraint that the edit distance between c and q
is less than a given threshold 6, the model is to
find c* in C with the highest probability:
</bodyText>
<equation confidence="0.973336">
c* arg
= max P(c  |q)
cÎC
</equation>
<bodyText confidence="0.999985285714286">
In practice, the correction candidate set C is
not generated from the entire query string di-
rectly. Correction candidates are generated for
each term of a query first, and then C is con-
structed by composing the candidates of individ-
ual terms. The edit distance threshold 6 is set for
each term proportionally to the length of the term.
</bodyText>
<subsectionHeader confidence="0.999672">
3.3 Source Channel Model
</subsectionHeader>
<bodyText confidence="0.999911">
Source channel model has been widely used for
spelling correction (Kernigham et al., 1990;
Mayes, Damerau et al., 1991; Brill and More,
2000; Ahmad and Kondrak, 2005). Instead of
directly optimize (1), source channel model tries
to solve an equivalent problem by applying
Bayes’s rule and dropping the constant denomi-
nator:
</bodyText>
<equation confidence="0.968868">
c* arg
= max P(q  |c)P(c)
cÎC
</equation>
<bodyText confidence="0.999336846153846">
In this approach, two component generative
models are involved: source model P(c) that gen-
erates the user’s intended query c and error
model P(q|c) that generates the real query q
given c. These two component models can be
independently estimated.
In practice, for a multi-term query, the source
model can be approximated with an n-gram sta-
tistical language model, which is estimated with
tokenized query logs. Taking bigram model for
example, c is a correction candidate containing n
terms, c = c1 c2 ... cn , then P(c) can be written as
the product of consecutive bigram probabilities:
</bodyText>
<equation confidence="0.983727">
P(c) = H P(ci  |ci−1)
}
</equation>
<page confidence="0.927872">
1027
</page>
<bodyText confidence="0.998414">
Similarly, the error model probability of a
query is decomposed into generation probabili-
ties of individual terms which are assumed to be
independently generated:
</bodyText>
<equation confidence="0.992034">
P(q  |c) = ∏ P(qi  |ci )
</equation>
<bodyText confidence="0.996242416666667">
Previous proposed methods for error model
estimation are all based on the similarity between
the character strings of qi and ci as described in
3.1. Here we describe a distributional similarity-
based method for this problem. Essentially there
are different ways to estimate distributional simi-
larity between two words (Dagan et al., 1997),
and the one we propose to use is confusion prob-
ability (Essen and Steinbiss, 1992). Formally,
confusion probability Pc estimates the possibil-
ity that one word w1 can be replaced by another
word w2:
</bodyText>
<equation confidence="0.9875609">
P w w
(  |)
1
P w w
(  |)=∑ P w w P w
(  |) ( )
c 2 1 2 2
P w
( )
w
</equation>
<bodyText confidence="0.999914846153846">
where w belongs to the set of words that co-
occur with both w1 and w2.
From the spelling correction point of view,
given w1 to be a valid word and w2 one of its
spelling errors, Pc (w2  |w1) actually estimates
opportunity that w1 is misspelled as w2 in query
logs. Compared to other similarity measures such
as cosine or Euclidean distance, confusion prob-
ability is of interest because it defines a probabil-
istic distribution rather than a generic measure.
This property makes it more theoretically sound
to be used as error model probability in the
Bayesian framework of the source channel model.
Thus it can be applied and evaluated independ-
ently. However, before using confusion probabil-
ity as our error model, we have to solve two
problems: probability renormalization and
smoothing.
Unlike string edit-based error models, which
distribute a major portion of probability over
terms with similar spellings, confusion probabil-
ity distributes probability over the entire vocabu-
lary in the training data. This property may cause
the problem of unfair comparison between dif-
ferent correction candidates if we directly use (3)
as the error model probability. This is because
the synonyms of different candidates may share
different portion of confusion probabilities. This
problem can be solved by re-normalizing the
probabilities only over a term’s possible correc-
tion candidates and itself. To obtain better esti-
mation, here we also require that the frequency
of a correction candidate should be higher than
that of the query term, based on the observation
that correct spellings generally occur more often
in query logs. Formally, given a word w and its
correction candidate set C, the confusion prob-
ability of a word w′ conditioned on w can be
redefined as
</bodyText>
<equation confidence="0.979297428571429">
 P w w
′ ′
c (  |)
w C
′ ∈

P w w
(  |)
′ = ∑ ∈ P c w
′ (4)
c c C c (  |)

0 w C
′∉
</equation>
<bodyText confidence="0.999332363636364">
where Pc ′ (w′  |w) is the original definition of con-
fusion probability.
In addition, we might also have the zero-
probability problem when the query term has not
appeared or there are few context words for it in
the query logs. In such cases there is no distribu-
tional similarity information available to any
known terms. To solve this problem, we define
the final error model probability as the linear
combination of confusion probability and a string
edit-based error model probability Ped (q  |c) :
</bodyText>
<equation confidence="0.810921">
P(q  |c) = λPc (q  |c) + (1−λ)Ped (q  |c) (5)
</equation>
<bodyText confidence="0.99417">
where X is the interpolation parameter between 0
and 1 that can be experimentally optimized on a
development data set.
</bodyText>
<subsectionHeader confidence="0.886614">
3.4 Maximum Entropy Model
</subsectionHeader>
<bodyText confidence="0.984565823529412">
Theoretically we are more interested in building
a unified probabilistic spelling correction model
that is able to leverage all available features,
which could include (but not limited to) tradi-
tional character string-based typographical simi-
larity, phonetic similarity and distributional simi-
larity proposed in this work. The maximum en-
tropy model (Berger et al., 1996) provides us
with a well-founded framework for this purpose,
which has been extensively used in natural lan
guage processing tasks ranging from part-of-
speech tagging to machine translation.
For our task, the maximum entropy model
defines a posterior probabilistic distribution
P(c  |q) over a set of feature functions fi (q, c)
defined on an input query q and its correction
candidate c:
</bodyText>
<equation confidence="0.982712578947368">
(  |)
c q = (6)
N
P
exp
N
∑ λ
i = if i
1
( , )
c q
∑c exp ∑
λ
i = if i
1
( , )
c q
(3)

</equation>
<page confidence="0.968625">
1028
</page>
<bodyText confidence="0.999910333333333">
where As are feature weights, which can be opti-
mized by maximizing the posterior probability
on the training set:
</bodyText>
<equation confidence="0.969217666666667">
max log λ (  |)
E P t q
λ
</equation>
<bodyText confidence="0.999953888888889">
where TD denotes the set of training samples in
the form of query-truth pairs presented to the
training algorithm.
We use the Generalized Iterative Scaling (GIS)
algorithm (Darroch and Ratcliff, 1972) to learn
the model parameter As of the maximum entropy
model. GIS training requires normalization over
all possible prediction classes as shown in the
denominator in equation (6). Since the potential
number of correction candidates may be huge for
multi-term queries, it would not be practical to
perform the normalization over the entire search
space. Instead, we use a method to approximate
the sum over the n-best list (a list of most prob-
able correction candidates). This is similar to
what Och and Ney (2002) used for their maxi-
mum entropy-based statistical machine transla-
tion training.
</bodyText>
<sectionHeader confidence="0.782242" genericHeader="method">
3.4.1 Features
</sectionHeader>
<bodyText confidence="0.99906">
Features used in our maximum entropy model
are classified into two categories I) baseline fea-
tures and II) features supported by distributional
similarity evidence. Below we list the feature
templates.
</bodyText>
<sectionHeader confidence="0.839765" genericHeader="method">
Category I:
</sectionHeader>
<bodyText confidence="0.937140666666667">
1. Language model probability feature. This
is the only real-valued feature with feature value
set to the logarithm of source model probability:
</bodyText>
<subsectionHeader confidence="0.839195">
4.1 Dataset
</subsectionHeader>
<figure confidence="0.376740428571429">
arg
λ* =
( t q TD
, )∈
ing lexicon;
didate is be-
low certain thresholds.
</figure>
<figureCaption confidence="0.7744868">
query term and its correction candidate are above
certain thresholds;
4. Lexicon-based features, which are gener-
ated by checking whether a query term and its
correction candidate are in a conventional spell-
</figureCaption>
<bodyText confidence="0.675128913043478">
5. Phonetic similarity-based features, which
are generated by checking whether the edit dis-
tance between the metaphones (Philips, 1990) of
a query term and its correction can
frequency is higher than certain
thresholds but there are no candidates for it with
higher frequency and high enough distributional
similarity. This is usually an indicator that the
query term is valid and not covered by the spell-
ing lexicon. The frequency thresholds are enu-
merated from 10,000 to 50,000 with the interval
7. Distributional similarity based correction
candidate features, which are generated by
checking whether a correction
fre-
quency is higher than the query term or the cor-
rection candidate is in the lexicon, and at the
same time the distributional similarity is higher
than certain thresholds. This generally gives the
evidence that the query term may be a common
misspelling of the current candidate. The distri-
butional similarity thresholds are enumerated
from 0.6 to 1 with the interval
</bodyText>
<figure confidence="0.91729075">
term’s
candidate’s
0.1.
4 Experimental Results
</figure>
<equation confidence="0.426751">
fprob (q, c) = log P(c )
</equation>
<bodyText confidence="0.910449285714286">
2. Edit distance-based features, which are
generated by checking whether the weighted
Levenshtein edit distance between a query term
and its correction is in certain range;
All the following features, including this one,
are binary features, and have the feature function
of the following form:
</bodyText>
<figure confidence="0.9727126">
fn(q,c)=r0
�
constraint satisfied
otherwise
1
</figure>
<bodyText confidence="0.86038175">
an
d 15.3%, respectively. The average length of
queries on training set is 2.8 terms and on test set
it is 2.6.
equencies of a
in which the feature value is set to 1 when the
constraints described in the template are satisfied;
otherwise the feature value is set to 0.
</bodyText>
<listItem confidence="0.80612375">
3. Frequency-based features, which are gen-
erated by checking whether the fr
Category II:
6. Distributional similarity based term fea-
</listItem>
<bodyText confidence="0.9905672">
tures, which are generated by checking whether a
query
5,000.
We randomly sampled 7,000 queries from daily
query logs of MSN Search and they were manu-
ally labeled by two annotators. For each query
identified to contain spelling errors, corrections
were given by the annotators independently.
From the annotation results that both annotators
agreed upon 3,061 queries were extracted, which
were further divided into a test set containing
1,031 queries and a training set containing 2,030
queries. In the test set there are 171 queries iden-
tified containing spelling errors with an error rate
of 16.6%. The numbers on the training set is 312
</bodyText>
<page confidence="0.948517">
1029
</page>
<bodyText confidence="0.999834631578947">
In our experiments, a term bigram model is
used as the source model. The bigram model is
trained with query log data of MSN Search dur-
ing the period from October 2004 to June 2005.
Correction candidates are generated from a term
base extracted from the same set of query logs.
For each of the experiments, the performance
is evaluated by the following metrics:
Accuracy: The number of correct outputs gen-
erated by the system divided by the total number
of queries in the test set;
Recall: The number of correct suggestions for
misspelled queries generated by the system di-
vided by the total number of misspelled queries
in the test set;
Precision: The number of correct suggestions
for misspelled queries generated by the system
divided by the total number of suggestions made
by the system.
</bodyText>
<subsectionHeader confidence="0.920582">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999019714285714">
We first investigated the impact of the interpola-
tion parameter λ in equation (5) by applying the
confusion probability-based error model on train-
ing set. For the string edit-based error model
probability Ped (q  |c) , we used a heuristic score
computed as the inverse of weighted edit dis-
tance, which is similar to the one used by Cucer-
zan and Brill (2004).
Figure 1 shows the accuracy metric at differ-
ent settings of λ. The accuracy generally gains
improvements before λ reaches 0.9. This shows
that confusion probability plays a more important
role in the combination. As a result, we empiri-
cally set λ= 0.9 in the following experiments.
</bodyText>
<figureCaption confidence="0.996527">
Figure 1. Accuracy with different λs
</figureCaption>
<bodyText confidence="0.998122428571429">
To evaluate whether the distributional similar-
ity can contribute to performance improvements,
we conducted the following experiments. For
source channel model, we compared the confu-
sion probability-based error model (SC-SimCM)
against two baseline error model settings, which
are source model only (SC-NoCM) and the heu-
ristic string edit-based error model (SC-EdCM)
we just described. Two maximum entropy mod-
els were trained with different feature sets. ME-
NoSim is the model trained only with baseline
features. It serves as the baseline for ME-Full,
which is trained with all the features described in
3.4.1. In training ME-Full, cosine distance is
used as the similarity measure examined by fea-
ture functions.
In all the experiments we used the standard
viterbi algorithm to search for the best output of
source channel model. The n-best list for maxi-
mum entropy model training and testing is gen-
erated based on language model scores of cor-
rection candidates, which can be easily obtained
by running the forward-viterbi backward-A* al-
gorithm. On a 3.0GHZ Pentium4 personal com-
puter, the system can process 110 queries per
second for source channel model and 86 queries
per second for maximum entropy model, in
which 20 best correction candidates are used.
</bodyText>
<table confidence="0.999666666666667">
Model Accuracy Recall Precision
SC-NoCM 79.7% 63.3% 40.2%
SC-EdCM 84.1% 62.7% 47.4%
SC-SimCM 88.2% 57.4% 58.8%
ME-NoSim 87.8% 52.0% 60.0%
ME-Full 89.0% 60.4% 62.6%
</table>
<tableCaption confidence="0.99978">
Table 2. Performance results for different models
</tableCaption>
<bodyText confidence="0.99783564">
Table 2 details the performance scores for the
experiments, which shows that both of the two
distributional similarity-based models boost ac-
curacy over their baseline settings. SC-SimCM
achieves 26.3% reduction in error rate over SC-
EdCM, which is significant to the 0.001 level
(paired t-test). ME-Full outperforms ME-NoSim
in all three evaluation measures, with 9.8% re-
duction in error rate and 16.2% improvement in
recall, which is significant to the 0.01 level.
It is interesting to note that the accuracy of
SC-SimCM is slightly better than ME-NoSim,
although ME-NoSim makes use of a rich set of
features. ME-NoSim tends to keep queries with
frequently misspelled terms unchanged (e.g. caf-
fine extractions from soda) to reduce false alarms
(e.g. bicycle suggested for biocycle).
We also investigated the performance of the
models discussed above at different recall. Fig-
ure 2 and Figure 3 show the precision-recall
curves and accuracy-recall curves of different
models. We observed that the performance of
SC-SimCM and ME-NoSim are very close to
each other and ME-Full consistently yields better
performance over the entire P-R curve.
</bodyText>
<figure confidence="0.948916727272727">
accuracy
91%
91%
90%
90%
89%
89%
88%
0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
lambda
1030
</figure>
<figureCaption confidence="0.999989">
Figure 2. Precision-recall curve of different models
Figure 3. Accuracy-recall curve of different models
</figureCaption>
<bodyText confidence="0.995561333333333">
We performed a study on the impact of train-
ing size to ensure all models are trained with
enough data.
</bodyText>
<figureCaption confidence="0.9991175">
Figure 4. Accuracy of maximum entropy models
trained with different number of samples
</figureCaption>
<bodyText confidence="0.989496666666667">
Figure 4 shows the accuracy of the two maxi-
mum entropy models as functions of number of
training samples. From the results we can see
that after the number of training samples reaches
600 there are only subtle changes in accuracy
and recall. Therefore basically it can be con-
cluded that 2,000 samples are sufficient to train a
maximum entropy model with the current feature
sets.
</bodyText>
<sectionHeader confidence="0.998514" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999981285714286">
We have presented novel methods to learn better
statistical models for the query spelling correc-
tion task by exploiting distributional similarity
information. We explained the motivation of our
methods with the statistical evidence distilled
from query log data. To evaluate our proposed
methods, two probabilistic models that can take
advantage of such information are investigated.
Experimental results show that both methods can
achieve significant improvements over their
baseline settings.
A subject of future research is exploring more
effective ways to utilize distributional similarity
even beyond query logs. Currently for low-
frequency terms in query logs there are no reli-
able distribution similarity evidence available for
them. A promising method of dealing with this in
next steps is to explore information in the result-
ing page of a search engine, since the snippets in
the resulting page can provide far greater de-
tailed information about terms in a query.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995801361111111">
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs.
Proceedings of EMNLP 2005, pages 955-962.
Adam L. Beger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tion Linguistics, 22(1):39-72.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction.
Proceedings of 38th annual meeting of the ACL,
pages 286-293.
Kenneth W. Church and William A. Gale. 1991.
Probability scoring for spelling correction. In Sta-
tistics and Computing, volume 1, pages 93-103.
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collec-
tive knowledge of web users. Proceedings of
EMNLP’04, pages 293-300.
Ido Dagan, Lillian Lee and Fernando Pereira. 1997.
Similarity-Based Methods for Word Sense Disam-
biguation. Proceedings of the 35th annual meeting
ofACL, pages 56-63.
Fred Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communica-
tion of the ACM 7(3):659-664.
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for long-linear models. Annals of Ma-
thematical Statistics, 43:1470-1480.
Ute Essen and Volker Steinbiss. 1992. Co-occurrence
smoothing for stochastic language modeling. Pro-
ceedings ofICASSP, volume 1, pages 161-164.
Andrew R. Golding and Dan Roth. 1996. Applying
winnow to context-sensitive spelling correction.
Proceedings ofICML 1996, pages 182-190.
Mark D. Kernighan, Kenneth W. Church and William
A. Gale. 1990. A spelling correction program
</reference>
<figure confidence="0.999420829787234">
precision
85%
80%
65%
60%
55%
50%
45%
40%
75%
70%
35% 40% 45% 50% 55% 60%
recall
ME-Full
ME-NoSim
SC-EdCM
SC-SimCM
SC-NoCM
accurac
91%
90%
89%
88%
87%
86%
85%
84%
83%
82%
35% 40% 45% 50% 55% 60%
recall
ME-Full
ME-NoSim
SC-EdCM
SC-SimCM
SC-NoCM
90%
80%
70%
60%
50%
40%
200 400 600 800 1000 1600 2000
ME-Full Recall
ME-Full Accuracy
ME-NoSim Recall
ME-NoSim Accurac
</figure>
<page confidence="0.940548">
1031
</page>
<reference confidence="0.996251181818182">
based on a noisy channel model. Proceedings of
COLING 1990, pages 205-210.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys.
24(4): 377-439
Lillian Lee. 1999. Measures of distributional similar-
ity. Proceedings of the 37th annual meeting ofACL,
pages 25-32.
V. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet
Physice – Doklady 10: 707-710.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. Proceedings of COLINGACL
1998, pages 768-774.
Lidia Mangu and Eric Brill. 1997. Automatic rule
acquisition for spelling correction. Proceedings of
ICML 1997, pages 734-741.
Eric Mayes, Fred Damerau and Robert Mercer. 1991.
Context based spelling correction. Information
processing and management 27(5): 517-522.
Franz Och and Hermann Ney. 2002. Discriminative
training and maimum entropy models for statistical
machine translation. Proceedings of the 40th an-
nual meeting ofACL, pages 295-302.
Lawrence Philips. 1990. Hanging on the metaphone.
Computer Language Magazine, 7(12): 39.
Eric S. Ristad and Peter N. Yianilos. 1997. Learning
string edit distance. Proceedings of ICML 1997.
pages 287-295
Kristina Toutanova and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction.
Proceedings of the 40th annual meeting of ACL,
pages 144-151.
</reference>
<page confidence="0.994687">
1032
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.309125">
<title confidence="0.9996535">Exploring Distributional Similarity Based Models for Query Spelling Correction</title>
<author confidence="0.999537">Mu Li</author>
<affiliation confidence="0.730707">Microsoft Research Asia 5F Sigma Center Zhichun Road, Haidian District</affiliation>
<address confidence="0.993468">Beijing, China, 100080</address>
<email confidence="0.998915">muli@microsoft.com</email>
<author confidence="0.999916">Yang Zhang</author>
<affiliation confidence="0.999461666666667">School of Computer Science and Technology Tianjin University</affiliation>
<address confidence="0.999871">Tianjin, China, 300072</address>
<email confidence="0.69975">yangzhang@tju.edu.cn</email>
<abstract confidence="0.998773736842105">bstract A query speller is crucial to search engine in improving web search relevance. This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models. The key to our methods is the property of distributional similarity between two terms: it is high between a frequently occurring misspelling and its correction, and low between two irrelevant terms only with similar spellings. We present two models that are able to take advantage of this property. Experimental results demonstrate that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Farooq Ahmad</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Learning a spelling error model from search query logs.</title>
<date>2005</date>
<booktitle>Proceedings of EMNLP</booktitle>
<pages>955--962</pages>
<contexts>
<context position="4574" citStr="Ahmad and Kondrak (2005)" startWordPosition="669" endWordPosition="672">-difficult-to-correct spelling errors. Word bigram model trained from search query logs is used as the source model, and the error model is approximated by inverse weighted edit distance of a correction candidate from its original term. The weights of edit operations are interactively optimized based on statistics from the query logs. They observed that an edit distance-based error model only has less impact on the overall accuracy than the source model. The paper reports that un-weighted edit distance will cause the overall accuracy of their speller’s output to drop by around 2%. The work of Ahmad and Kondrak (2005) tried to employ an unsupervised approach to error model estimation. They designed an EM (Expectation Maximization) algorithm to optimize the probabilities of edit operations over a set of search queries from the query logs, by exploiting the fact that there are more than 10% misspelled queries scattered throughout the query logs. Their method is concerned with single character edit operations, and evaluation was performed on an isolated word spelling correction task. There are two lines of research in conventional spelling correction, which deal with non-word errors and real-word errors respe</context>
<context position="7198" citStr="Ahmad and Kondrak, 2005" startWordPosition="1063" endWordPosition="1066">tigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 1997; Brill and Moore, 2000; and Ahmad and Kondrak, 2005). In the mentioned methods, however, the similarities between two strings are modeled on the average of many misspelling-correction pairs, which may cause many idiosyncratic spelling errors to be ignored. Some of those are typical word-level cognitive errors. For instance, given the query term adventura, a character string-based error model usually assigns similar similarities to its two most probable corrections adventure and aventura. Taking into account that adventure has a much higher frequency of occurring, it is most likely that adventure would be generated as a suggestion. However, our </context>
<context position="8946" citStr="Ahmad and Kondrak, 2005" startWordPosition="1335" endWordPosition="1338">statistics of words co-occuring with them. Distributional similarity has been proposed to perform tasks such as language model smoothing and word clustering, but to the best of our knowledge, it has not been explored in estimating similarities between misspellings and their corrections. In this section, we will only involve the consine metric for illustration purpose. Query logs can serve as an excellent corpus for distributional similarity estimation. This is because query logs are not only an up-to-date term base, but also a comprehensive spelling error repository (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005). Given enough size of query logs, some misspellings, such as adventura, will occur so frequently that we can obtain reliable statistics of their typical usage. Essential to our method is the observation of high distributional similarity between frequently occurring spelling errors and their corrections, but low between irrelevant terms. For example, we observe that adventura occurred more than 3,300 times in a set of logged queries that spanned three months, and its context was similar to that of aventura. Both of them usually appeared after words like peurto and lyrics, and were followed by </context>
<context position="11708" citStr="Ahmad and Kondrak, 2005" startWordPosition="1792" endWordPosition="1795">threshold 6, the model is to find c* in C with the highest probability: c* arg = max P(c |q) cÎC In practice, the correction candidate set C is not generated from the entire query string directly. Correction candidates are generated for each term of a query first, and then C is constructed by composing the candidates of individual terms. The edit distance threshold 6 is set for each term proportionally to the length of the term. 3.3 Source Channel Model Source channel model has been widely used for spelling correction (Kernigham et al., 1990; Mayes, Damerau et al., 1991; Brill and More, 2000; Ahmad and Kondrak, 2005). Instead of directly optimize (1), source channel model tries to solve an equivalent problem by applying Bayes’s rule and dropping the constant denominator: c* arg = max P(q |c)P(c) cÎC In this approach, two component generative models are involved: source model P(c) that generates the user’s intended query c and error model P(q|c) that generates the real query q given c. These two component models can be independently estimated. In practice, for a multi-term query, the source model can be approximated with an n-gram statistical language model, which is estimated with tokenized query logs. Ta</context>
</contexts>
<marker>Ahmad, Kondrak, 2005</marker>
<rawString>Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a spelling error model from search query logs. Proceedings of EMNLP 2005, pages 955-962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Beger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computation Linguistics,</journal>
<pages>22--1</pages>
<marker>Beger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Beger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computation Linguistics, 22(1):39-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>Proceedings of 38th annual meeting of the ACL,</booktitle>
<pages>286--293</pages>
<contexts>
<context position="5622" citStr="Brill and Moore (2000)" startWordPosition="829" endWordPosition="832"> on an isolated word spelling correction task. There are two lines of research in conventional spelling correction, which deal with non-word errors and real-word errors respectively. Nonword error spelling correction is concerned with the task of generating and ranking a list of possible spelling corrections for each query word not found in a lexicon. While traditionally candidate ranking is based on manually tuned scores such as assigning weights to different edit operations or leveraging candidate frequencies, some statistical models have been proposed for this ranking task in recent years. Brill and Moore (2000) presented an improved error model over the one proposed by Kernigham et al. (1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Toutanova and Moore (2002) further explored this via explicit modeling of phonetic information of English words. Both these two methods require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon. Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of v</context>
<context position="7168" citStr="Brill and Moore, 2000" startWordPosition="1058" endWordPosition="1061">gan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 1997; Brill and Moore, 2000; and Ahmad and Kondrak, 2005). In the mentioned methods, however, the similarities between two strings are modeled on the average of many misspelling-correction pairs, which may cause many idiosyncratic spelling errors to be ignored. Some of those are typical word-level cognitive errors. For instance, given the query term adventura, a character string-based error model usually assigns similar similarities to its two most probable corrections adventure and aventura. Taking into account that adventure has a much higher frequency of occurring, it is most likely that adventure would be generated </context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. Proceedings of 38th annual meeting of the ACL, pages 286-293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>Probability scoring for spelling correction.</title>
<date>1991</date>
<booktitle>In Statistics and Computing,</booktitle>
<volume>1</volume>
<pages>93--103</pages>
<contexts>
<context position="7099" citStr="Church and Gale, 1991" startWordPosition="1045" endWordPosition="1048">in, 1998) and language model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 1997; Brill and Moore, 2000; and Ahmad and Kondrak, 2005). In the mentioned methods, however, the similarities between two strings are modeled on the average of many misspelling-correction pairs, which may cause many idiosyncratic spelling errors to be ignored. Some of those are typical word-level cognitive errors. For instance, given the query term adventura, a character string-based error model usually assigns similar similarities to its two most probable corrections adventure and aventura. Taking into account that adventure has a much higher frequen</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Kenneth W. Church and William A. Gale. 1991. Probability scoring for spelling correction. In Statistics and Computing, volume 1, pages 93-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>Eric Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of web users.</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP’04,</booktitle>
<pages>293--300</pages>
<contexts>
<context position="1190" citStr="Cucerzan and Brill, 2004" startWordPosition="169" endWordPosition="172">our methods is the property of distributional similarity between two terms: it is high between a frequently occurring misspelling and its correction, and low between two irrelevant terms only with similar spellings. We present two models that are able to take advantage of this property. Experimental results demonstrate that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task. 1 Introduction Investigations into query log data reveal that more than 10% of queries sent to search engines contain misspelled terms (Cucerzan and Brill, 2004). Such statistics indicate that a good query speller is crucial to search engine in improving web search relevance, because there is little opportunity that a search engine can retrieve many relevant contents with misspelled terms. Muhua Zhu School of Information Science and Engineering Northeastern University Shenyang, Liaoning, China, 110004 zhumh@ics.neu.edu.cn Ming Zhou Microsoft Research Asia 5F Sigma Center Zhichun Road, Haidian District Beijing, China, 100080 mingzhou@microsoft.com The problem of designing a spelling correction program for web search queries, however, poses special tech</context>
<context position="3837" citStr="Cucerzan and Brill (2004)" startWordPosition="553" endWordPosition="556">rence on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1025–1032, Sydney, July 2006. c�2006 Association for Computational Linguistics The rest of the paper is structured as follows: after a brief overview of the related work in Section 2, we discuss the motivations for our approach, and describe two methods that can make use of distributional similarity information in Section 3. Experiments and results are presented in Section 4. The last section contains summaries and outlines promising future work. 2 Related Work The method for web query spelling correction proposed by Cucerzan and Brill (2004) is essentially based on a source channel model, but it requires iterative running to derive suggestions for very-difficult-to-correct spelling errors. Word bigram model trained from search query logs is used as the source model, and the error model is approximated by inverse weighted edit distance of a correction candidate from its original term. The weights of edit operations are interactively optimized based on statistics from the query logs. They observed that an edit distance-based error model only has less impact on the overall accuracy than the source model. The paper reports that un-we</context>
<context position="8920" citStr="Cucerzan and Brill, 2004" startWordPosition="1331" endWordPosition="1334">d by another based on the statistics of words co-occuring with them. Distributional similarity has been proposed to perform tasks such as language model smoothing and word clustering, but to the best of our knowledge, it has not been explored in estimating similarities between misspellings and their corrections. In this section, we will only involve the consine metric for illustration purpose. Query logs can serve as an excellent corpus for distributional similarity estimation. This is because query logs are not only an up-to-date term base, but also a comprehensive spelling error repository (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005). Given enough size of query logs, some misspellings, such as adventura, will occur so frequently that we can obtain reliable statistics of their typical usage. Essential to our method is the observation of high distributional similarity between frequently occurring spelling errors and their corrections, but low between irrelevant terms. For example, we observe that adventura occurred more than 3,300 times in a set of logged queries that spanned three months, and its context was similar to that of aventura. Both of them usually appeared after words like peurto and lyr</context>
<context position="21880" citStr="Cucerzan and Brill (2004)" startWordPosition="3538" endWordPosition="3542">ed queries generated by the system divided by the total number of misspelled queries in the test set; Precision: The number of correct suggestions for misspelled queries generated by the system divided by the total number of suggestions made by the system. 4.2 Results We first investigated the impact of the interpolation parameter λ in equation (5) by applying the confusion probability-based error model on training set. For the string edit-based error model probability Ped (q |c) , we used a heuristic score computed as the inverse of weighted edit distance, which is similar to the one used by Cucerzan and Brill (2004). Figure 1 shows the accuracy metric at different settings of λ. The accuracy generally gains improvements before λ reaches 0.9. This shows that confusion probability plays a more important role in the combination. As a result, we empirically set λ= 0.9 in the following experiments. Figure 1. Accuracy with different λs To evaluate whether the distributional similarity can contribute to performance improvements, we conducted the following experiments. For source channel model, we compared the confusion probability-based error model (SC-SimCM) against two baseline error model settings, which are</context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>Silviu Cucerzan and Eric Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. Proceedings of EMNLP’04, pages 293-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-Based Methods for Word Sense Disambiguation.</title>
<date>1997</date>
<booktitle>Proceedings of the 35th annual meeting ofACL,</booktitle>
<pages>56--63</pages>
<contexts>
<context position="6564" citStr="Dagan et al., 1997" startWordPosition="971" endWordPosition="974">of English words. Both these two methods require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon. Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts (Golding and Roth, 1996; Mangu and Brill, 1997). Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition (Dekang Lin, 1998) and language model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 1997; Brill and Moore, </context>
<context position="13034" citStr="Dagan et al., 1997" startWordPosition="2012" endWordPosition="2015"> can be written as the product of consecutive bigram probabilities: P(c) = H P(ci |ci−1) } 1027 Similarly, the error model probability of a query is decomposed into generation probabilities of individual terms which are assumed to be independently generated: P(q |c) = ∏ P(qi |ci ) Previous proposed methods for error model estimation are all based on the similarity between the character strings of qi and ci as described in 3.1. Here we describe a distributional similaritybased method for this problem. Essentially there are different ways to estimate distributional similarity between two words (Dagan et al., 1997), and the one we propose to use is confusion probability (Essen and Steinbiss, 1992). Formally, confusion probability Pc estimates the possibility that one word w1 can be replaced by another word w2: P w w ( |) 1 P w w ( |)=∑ P w w P w ( |) ( ) c 2 1 2 2 P w ( ) w where w belongs to the set of words that cooccur with both w1 and w2. From the spelling correction point of view, given w1 to be a valid word and w2 one of its spelling errors, Pc (w2 |w1) actually estimates opportunity that w1 is misspelled as w2 in query logs. Compared to other similarity measures such as cosine or Euclidean distan</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1997</marker>
<rawString>Ido Dagan, Lillian Lee and Fernando Pereira. 1997. Similarity-Based Methods for Word Sense Disambiguation. Proceedings of the 35th annual meeting ofACL, pages 56-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communication of the ACM</journal>
<pages>7--3</pages>
<contexts>
<context position="6980" citStr="Damerau, 1964" startWordPosition="1031" endWordPosition="1032"> successfully applied in many natural language tasks such as automatic semantic knowledge acquisition (Dekang Lin, 1998) and language model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 1997; Brill and Moore, 2000; and Ahmad and Kondrak, 2005). In the mentioned methods, however, the similarities between two strings are modeled on the average of many misspelling-correction pairs, which may cause many idiosyncratic spelling errors to be ignored. Some of those are typical word-level cognitive errors. For instance, given the query term adventura, a character string-based error model usually assigns similar similarities to</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Fred Damerau. 1964. A technique for computer detection and correction of spelling errors. Communication of the ACM 7(3):659-664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for long-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="17017" citStr="Darroch and Ratcliff, 1972" startWordPosition="2738" endWordPosition="2741">r our task, the maximum entropy model defines a posterior probabilistic distribution P(c |q) over a set of feature functions fi (q, c) defined on an input query q and its correction candidate c: ( |) c q = (6) N P exp N ∑ λ i = if i 1 ( , ) c q ∑c exp ∑ λ i = if i 1 ( , ) c q (3)  1028 where As are feature weights, which can be optimized by maximizing the posterior probability on the training set: max log λ ( |) E P t q λ where TD denotes the set of training samples in the form of query-truth pairs presented to the training algorithm. We use the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) to learn the model parameter As of the maximum entropy model. GIS training requires normalization over all possible prediction classes as shown in the denominator in equation (6). Since the potential number of correction candidates may be huge for multi-term queries, it would not be practical to perform the normalization over the entire search space. Instead, we use a method to approximate the sum over the n-best list (a list of most probable correction candidates). This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. 3.4.1 </context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for long-linear models. Annals of Mathematical Statistics, 43:1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ute Essen</author>
<author>Volker Steinbiss</author>
</authors>
<title>Co-occurrence smoothing for stochastic language modeling.</title>
<date>1992</date>
<booktitle>Proceedings ofICASSP,</booktitle>
<volume>1</volume>
<pages>161--164</pages>
<contexts>
<context position="6543" citStr="Essen and Steinbiss, 1992" startWordPosition="967" endWordPosition="970">ng of phonetic information of English words. Both these two methods require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon. Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts (Golding and Roth, 1996; Mangu and Brill, 1997). Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition (Dekang Lin, 1998) and language model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 19</context>
<context position="13118" citStr="Essen and Steinbiss, 1992" startWordPosition="2027" endWordPosition="2030">P(ci |ci−1) } 1027 Similarly, the error model probability of a query is decomposed into generation probabilities of individual terms which are assumed to be independently generated: P(q |c) = ∏ P(qi |ci ) Previous proposed methods for error model estimation are all based on the similarity between the character strings of qi and ci as described in 3.1. Here we describe a distributional similaritybased method for this problem. Essentially there are different ways to estimate distributional similarity between two words (Dagan et al., 1997), and the one we propose to use is confusion probability (Essen and Steinbiss, 1992). Formally, confusion probability Pc estimates the possibility that one word w1 can be replaced by another word w2: P w w ( |) 1 P w w ( |)=∑ P w w P w ( |) ( ) c 2 1 2 2 P w ( ) w where w belongs to the set of words that cooccur with both w1 and w2. From the spelling correction point of view, given w1 to be a valid word and w2 one of its spelling errors, Pc (w2 |w1) actually estimates opportunity that w1 is misspelled as w2 in query logs. Compared to other similarity measures such as cosine or Euclidean distance, confusion probability is of interest because it defines a probabilistic distribu</context>
</contexts>
<marker>Essen, Steinbiss, 1992</marker>
<rawString>Ute Essen and Volker Steinbiss. 1992. Co-occurrence smoothing for stochastic language modeling. Proceedings ofICASSP, volume 1, pages 161-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Dan Roth</author>
</authors>
<title>Applying winnow to context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>Proceedings ofICML</booktitle>
<pages>182--190</pages>
<contexts>
<context position="6276" citStr="Golding and Roth, 1996" startWordPosition="931" endWordPosition="934">el over the one proposed by Kernigham et al. (1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Toutanova and Moore (2002) further explored this via explicit modeling of phonetic information of English words. Both these two methods require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon. Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts (Golding and Roth, 1996; Mangu and Brill, 1997). Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition (Dekang Lin, 1998) and language model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties o</context>
</contexts>
<marker>Golding, Roth, 1996</marker>
<rawString>Andrew R. Golding and Dan Roth. 1996. Applying winnow to context-sensitive spelling correction. Proceedings ofICML 1996, pages 182-190.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mark D Kernighan</author>
<author>W Kenneth</author>
</authors>
<title>Church and William</title>
<marker>Kernighan, Kenneth, </marker>
<rawString>Mark D. Kernighan, Kenneth W. Church and William</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gale</author>
</authors>
<title>A spelling correction program based on a noisy channel model.</title>
<date>1990</date>
<booktitle>Proceedings of COLING</booktitle>
<pages>205--210</pages>
<marker>Gale, 1990</marker>
<rawString>A. Gale. 1990. A spelling correction program based on a noisy channel model. Proceedings of COLING 1990, pages 205-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys.</journal>
<volume>24</volume>
<issue>4</issue>
<pages>377--439</pages>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys. 24(4): 377-439</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>Proceedings of the 37th annual meeting ofACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="6657" citStr="Lee, 1999" startWordPosition="987" endWordPosition="988">atter also needs a pronunciation lexicon. Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts (Golding and Roth, 1996; Mangu and Brill, 1997). Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition (Dekang Lin, 1998) and language model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 1997; Brill and Moore, 2000; and Ahmad and Kondrak, 2005). In the mentioned methods, however, the similarities betwe</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. Proceedings of the 37th annual meeting ofACL, pages 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physice – Doklady</journal>
<volume>10</volume>
<pages>707--710</pages>
<contexts>
<context position="7000" citStr="Levenshtein, 1966" startWordPosition="1033" endWordPosition="1034">pplied in many natural language tasks such as automatic semantic knowledge acquisition (Dekang Lin, 1998) and language model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 1997; Brill and Moore, 2000; and Ahmad and Kondrak, 2005). In the mentioned methods, however, the similarities between two strings are modeled on the average of many misspelling-correction pairs, which may cause many idiosyncratic spelling errors to be ignored. Some of those are typical word-level cognitive errors. For instance, given the query term adventura, a character string-based error model usually assigns similar similarities to its two most probab</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physice – Doklady 10: 707-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>Proceedings of COLINGACL</booktitle>
<pages>768--774</pages>
<contexts>
<context position="6487" citStr="Lin, 1998" startWordPosition="961" endWordPosition="962">further explored this via explicit modeling of phonetic information of English words. Both these two methods require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon. Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts (Golding and Roth, 1996; Mangu and Brill, 1997). Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition (Dekang Lin, 1998) and language model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church an</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. Proceedings of COLINGACL 1998, pages 768-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>Proceedings of ICML</booktitle>
<pages>734--741</pages>
<contexts>
<context position="6300" citStr="Mangu and Brill, 1997" startWordPosition="935" endWordPosition="938"> by Kernigham et al. (1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Toutanova and Moore (2002) further explored this via explicit modeling of phonetic information of English words. Both these two methods require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon. Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts (Golding and Roth, 1996; Mangu and Brill, 1997). Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition (Dekang Lin, 1998) and language model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. Thi</context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>Lidia Mangu and Eric Brill. 1997. Automatic rule acquisition for spelling correction. Proceedings of ICML 1997, pages 734-741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Mayes</author>
<author>Fred Damerau</author>
<author>Robert Mercer</author>
</authors>
<title>Context based spelling correction. Information processing and management</title>
<date>1991</date>
<volume>27</volume>
<issue>5</issue>
<pages>517--522</pages>
<contexts>
<context position="7118" citStr="Mayes et al, 1991" startWordPosition="1049" endWordPosition="1052">model smoothing (Essen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 1997; Brill and Moore, 2000; and Ahmad and Kondrak, 2005). In the mentioned methods, however, the similarities between two strings are modeled on the average of many misspelling-correction pairs, which may cause many idiosyncratic spelling errors to be ignored. Some of those are typical word-level cognitive errors. For instance, given the query term adventura, a character string-based error model usually assigns similar similarities to its two most probable corrections adventure and aventura. Taking into account that adventure has a much higher frequency of occurring, it</context>
</contexts>
<marker>Mayes, Damerau, Mercer, 1991</marker>
<rawString>Eric Mayes, Fred Damerau and Robert Mercer. 1991. Context based spelling correction. Information processing and management 27(5): 517-522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maimum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th annual meeting ofACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="17531" citStr="Och and Ney (2002)" startWordPosition="2823" endWordPosition="2826">raining algorithm. We use the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) to learn the model parameter As of the maximum entropy model. GIS training requires normalization over all possible prediction classes as shown in the denominator in equation (6). Since the potential number of correction candidates may be huge for multi-term queries, it would not be practical to perform the normalization over the entire search space. Instead, we use a method to approximate the sum over the n-best list (a list of most probable correction candidates). This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. 3.4.1 Features Features used in our maximum entropy model are classified into two categories I) baseline features and II) features supported by distributional similarity evidence. Below we list the feature templates. Category I: 1. Language model probability feature. This is the only real-valued feature with feature value set to the logarithm of source model probability: 4.1 Dataset arg λ* = ( t q TD , )∈ ing lexicon; didate is below certain thresholds. query term and its correction candidate are above certain thre</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Och and Hermann Ney. 2002. Discriminative training and maimum entropy models for statistical machine translation. Proceedings of the 40th annual meeting ofACL, pages 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Philips</author>
</authors>
<title>Hanging on the metaphone.</title>
<date>1990</date>
<journal>Computer Language Magazine,</journal>
<volume>7</volume>
<issue>12</issue>
<pages>39</pages>
<contexts>
<context position="18410" citStr="Philips, 1990" startWordPosition="2962" endWordPosition="2963"> we list the feature templates. Category I: 1. Language model probability feature. This is the only real-valued feature with feature value set to the logarithm of source model probability: 4.1 Dataset arg λ* = ( t q TD , )∈ ing lexicon; didate is below certain thresholds. query term and its correction candidate are above certain thresholds; 4. Lexicon-based features, which are generated by checking whether a query term and its correction candidate are in a conventional spell5. Phonetic similarity-based features, which are generated by checking whether the edit distance between the metaphones (Philips, 1990) of a query term and its correction can frequency is higher than certain thresholds but there are no candidates for it with higher frequency and high enough distributional similarity. This is usually an indicator that the query term is valid and not covered by the spelling lexicon. The frequency thresholds are enumerated from 10,000 to 50,000 with the interval 7. Distributional similarity based correction candidate features, which are generated by checking whether a correction frequency is higher than the query term or the correction candidate is in the lexicon, and at the same time the distri</context>
</contexts>
<marker>Philips, 1990</marker>
<rawString>Lawrence Philips. 1990. Hanging on the metaphone. Computer Language Magazine, 7(12): 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric S Ristad</author>
<author>Peter N Yianilos</author>
</authors>
<title>Learning string edit distance.</title>
<date>1997</date>
<booktitle>Proceedings of ICML</booktitle>
<pages>287--295</pages>
<contexts>
<context position="7145" citStr="Ristad and Yianilos, 1997" startWordPosition="1053" endWordPosition="1057">sen and Steinbiss, 1992; Dagan et al., 1997). An investigation on distributional similarity functions can be found in (Lillian Lee, 1999). 3 Distributional Similarity-Based Models for Query Spelling Correction 3.1 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings. This direction ever evolves from simple Damerau-Levenshtein distance (Damerau, 1964; Levenshtein, 1966) to probabilistic models that estimate string edit probabilities from corpus (Church and Gale, 1991; Mayes et al, 1991; Ristad and Yianilos, 1997; Brill and Moore, 2000; and Ahmad and Kondrak, 2005). In the mentioned methods, however, the similarities between two strings are modeled on the average of many misspelling-correction pairs, which may cause many idiosyncratic spelling errors to be ignored. Some of those are typical word-level cognitive errors. For instance, given the query term adventura, a character string-based error model usually assigns similar similarities to its two most probable corrections adventure and aventura. Taking into account that adventure has a much higher frequency of occurring, it is most likely that advent</context>
</contexts>
<marker>Ristad, Yianilos, 1997</marker>
<rawString>Eric S. Ristad and Peter N. Yianilos. 1997. Learning string edit distance. Proceedings of ICML 1997. pages 287-295</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Robert Moore</author>
</authors>
<title>Pronunciation modeling for improved spelling correction.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th annual meeting of ACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="5876" citStr="Toutanova and Moore (2002)" startWordPosition="869" endWordPosition="872">erating and ranking a list of possible spelling corrections for each query word not found in a lexicon. While traditionally candidate ranking is based on manually tuned scores such as assigning weights to different edit operations or leveraging candidate frequencies, some statistical models have been proposed for this ranking task in recent years. Brill and Moore (2000) presented an improved error model over the one proposed by Kernigham et al. (1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Toutanova and Moore (2002) further explored this via explicit modeling of phonetic information of English words. Both these two methods require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon. Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts (Golding and Roth, 1996; Mangu and Brill, 1997). Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition (Dekang</context>
</contexts>
<marker>Toutanova, Moore, 2002</marker>
<rawString>Kristina Toutanova and Robert Moore. 2002. Pronunciation modeling for improved spelling correction. Proceedings of the 40th annual meeting of ACL, pages 144-151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>