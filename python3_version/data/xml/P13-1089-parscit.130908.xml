<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.585875">
Margin-based Decomposed Amortized Inference
</title>
<author confidence="0.628935">
Gourab Kundu* and Vivek Srikumar* and Dan Roth
</author>
<affiliation confidence="0.948827">
University of Illinois, Urbana-Champaign
</affiliation>
<address confidence="0.622411">
Urbana, IL. 61801
</address>
<email confidence="0.9142">
{kundu2, vsrikum2, danr}@illinois.edu
</email>
<sectionHeader confidence="0.993229" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988393939394">
Given that structured output prediction is
typically performed over entire datasets,
one natural question is whether it is pos-
sible to re-use computation from earlier
inference instances to speed up inference
for future instances. Amortized inference
has been proposed as a way to accomplish
this. In this paper, first, we introduce a new
amortized inference algorithm called the
Margin-based Amortized Inference, which
uses the notion of structured margin to
identify inference problems for which pre-
vious solutions are provably optimal. Sec-
ond, we introduce decomposed amortized
inference, which is designed to address
very large inference problems, where ear-
lier amortization methods become less ef-
fective. This approach works by decom-
posing the output structure and applying
amortization piece-wise, thus increasing
the chance that we can re-use previous so-
lutions for parts of the output structure.
These parts are then combined to a global
coherent solution using Lagrangian relax-
ation. In our experiments, using the NLP
tasks of semantic role labeling and entity-
relation extraction, we demonstrate that
with the margin-based algorithm, we need
to call the inference engine only for a third
of the test examples. Further, we show that
the decomposed variant of margin-based
amortized inference achieves a greater re-
duction in the number of inference calls.
</bodyText>
<sectionHeader confidence="0.998705" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9706835">
A wide variety of NLP problems can be natu-
rally cast as structured prediction problems. For
</bodyText>
<note confidence="0.654618">
* These authors contributed equally to this work.
</note>
<bodyText confidence="0.999851804878049">
some structures like sequences or parse trees, spe-
cialized and tractable dynamic programming algo-
rithms have proven to be very effective. However,
as the structures under consideration become in-
creasingly complex, the computational problem of
predicting structures can become very expensive,
and in the worst case, intractable.
In this paper, we focus on an inference tech-
nique called amortized inference (Srikumar et al.,
2012), where previous solutions to inference prob-
lems are used to speed up new instances. The
main observation that leads to amortized inference
is that, very often, for different examples of the
same size, the structures that maximize the score
are identical. If we can efficiently identify that two
inference problems have the same solution, then
we can re-use previously computed structures for
newer examples, thus giving us a speedup.
This paper has two contributions. First, we de-
scribe a novel algorithm for amortized inference
called margin-based amortization. This algorithm
is on an examination of the structured margin of
a prediction. For a new inference problem, if this
margin is larger than the sum of the decrease in the
score of the previous prediction and any increase
in the score of the second best one, then the previ-
ous solution will be the highest scoring one for the
new problem. We formalize this intuition to derive
an algorithm that finds provably optimal solutions
and show that this approach is a generalization of
previously identified schemes (based on Theorem
1 of (Srikumar et al., 2012)).
Second, we argue that the idea of amortization
is best exploited at the level of parts of the struc-
tures rather than the entire structure because we
expect a much higher redundancy in the parts.
We introduce the notion of decomposed amor-
tized inference, whereby we can attain a significant
improvement in speedup by considering repeated
sub-structures across the dataset and applying any
amortized inference algorithm for the parts.
</bodyText>
<page confidence="0.973095">
905
</page>
<note confidence="0.9141845">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 905–913,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998969083333333">
We evaluate the two schemes and their combi-
nation on two NLP tasks where the output is en-
coded as a structure: PropBank semantic role la-
beling (Punyakanok et al., 2008) and the problem
of recognizing entities and relations in text (Roth
and Yih, 2007; Kate and Mooney, 2010). In these
problems, the inference problem has been framed
as an integer linear program (ILP). We compare
our methods with previous amortized inference
methods and show that margin-based amortization
combined with decomposition significantly out-
performs existing methods.
</bodyText>
<sectionHeader confidence="0.878835" genericHeader="method">
2 Problem Definition and Notation
</sectionHeader>
<bodyText confidence="0.999921521739131">
Structured output prediction encompasses a wide
variety of NLP problems like part-of-speech tag-
ging, parsing and machine translation. The lan-
guage of 0-1 integer linear programs (ILP) pro-
vides a convenient analytical tool for representing
structured prediction problems. The general set-
ting consists of binary inference variables each of
which is associated with a score. The goal of in-
ference is to find the highest scoring global assign-
ment of the variables from a feasible set of assign-
ments, which is defined by linear inequalities.
While efficient inference algorithms exist for
special families of structures (like linear chains
and trees), in the general case, inference can be
computationally intractable. One approach to deal
with the computational complexity of inference
is to use an off-the-shelf ILP solver for solv-
ing the inference problem. This approach has
seen increasing use in the NLP community over
the last several years (for example, (Roth and
Yih, 2004; Clarke and Lapata, 2006; Riedel and
Clarke, 2006) and many others). Other approaches
for solving inference include the use of cutting
plane inference (Riedel, 2009), dual decomposi-
tion (Koo et al., 2010; Rush et al., 2010) and
the related method of Lagrangian relaxation (Rush
and Collins, 2011; Chang and Collins, 2011).
(Srikumar et al., 2012) introduced the notion of
an amortized inference algorithm, defined as an
inference algorithm that can use previous predic-
tions to speed up inference time, thereby giving an
amortized gain in inference time over the lifetime
of the program.
The motivation for amortized inference comes
from the observation that though the number of
possible structures could be large, in practice, only
a small number of these are ever seen in real
data. Furthermore, among the observed structures,
a small subset typically occurs much more fre-
quently than the others. Figure 1 illustrates this
observation in the context of part-of-speech tag-
ging. If we can efficiently characterize and iden-
tify inference instances that have the same solu-
tion, we can take advantage of previously per-
formed computation without paying the high com-
putational cost of inference.
</bodyText>
<figureCaption confidence="0.9927264">
Figure 1: Comparison of number of instances and the num-
ber of unique observed part-of-speech structures in the Gi-
gaword corpus. Note that the number of observed structures
(blue solid line) is much lower than the number of sentences
(red dotted line) for all sentence lengths, with the difference
being very pronounced for shorter sentences. Embedded in
the graph are three histograms that show the distribution of
observed structures for sentences of length 15, 20 and 30. In
all cases, we see that a small number of tag sequences are
much more frequent than the others.
</figureCaption>
<bodyText confidence="0.9444614">
We denote inference problems by the bold-
faced letters p and q. For a problem p, the goal
of inference is to jointly assign values to the parts
of the structure, which are represented by a col-
lection of inference variables y E {0,1}n. For all
vectors, subscripts represent their ith component.
Each yi is associated with a real valued cp,i E R
which is the score for the variable yi being as-
signed the value 1. We denote the vector com-
prising of all the cp,i as cp. The search space
for assignments is restricted via constraints, which
can be written as a collection of linear inequalities,
MTy G b. For a problem p, we denote this fea-
sible set of structures by Kp.
The inference problem is that of finding the fea-
sible assignment to the structure which maximizes
the dot product cTy. Thus, the prediction problem
can be written as
arg max cTy. (1)
y∈Kp
</bodyText>
<page confidence="0.99378">
906
</page>
<bodyText confidence="0.99995745">
We denote the solution of this maximization prob-
lem as yp.
Let the set P = {p1, p2, · · · } denote previously
solved inference problems, along with their re-
spective solutions {y1p, y2p, · · · }. An equivalence
class of integer linear programs, denoted by [P],
consists of ILPs which have the same number of
inference variables and the same feasible set. Let
K[P] denote the feasible set of an equivalence class
[P]. For a program p, the notation p — [P] indi-
cates that it belongs to the equivalence class [P].
(Srikumar et al., 2012) introduced a set of amor-
tized inference schemes, each of which provides a
condition for a new ILP to have the same solu-
tion as a previously seen problem. We will briefly
review one exact inference scheme introduced in
that work. Suppose q belongs to the same equiv-
alence class of ILPs as p. Then the solution to q
will be the same as that of p if the following con-
dition holds for all inference variables:
</bodyText>
<equation confidence="0.998978">
(gyp,i — 1)(cq,i — cp,i) &gt; 0. (2)
</equation>
<bodyText confidence="0.999893727272727">
This condition, referred to as Theorem 1 in that
work, is the baseline for our experiments.
In general, for any amortization scheme
A, we can define two primitive operators
TESTCONDITIONA and SOLUTIONA. Given
a collection of previously solved prob-
lems P and a new inference problem q,
TESTCONDITIONA(P, q) checks if the solu-
tion of the new problem is the same as that
of some previously solved one and if so,
SOLUTIONA(P, q) returns the solution.
</bodyText>
<sectionHeader confidence="0.996606" genericHeader="method">
3 Margin-based Amortization
</sectionHeader>
<bodyText confidence="0.9972824">
In this section, we will introduce a new method
for amortizing inference costs over time. The key
observation that leads to this theorem stems from
the structured margin S for an inference problem
p — [P], which is defined as follows:
</bodyText>
<equation confidence="0.663693">
S = min cTp(yp — y). (3)
yEK[P],y=,4yp
</equation>
<bodyText confidence="0.999789714285714">
That is, for all feasible y, we have cTpyp &gt; cTpy +
S. The margin S is the upper limit on the change in
objective that is allowed for the constraint set K[P]
for which the solution will not change.
For a new inference problem q — [P], we define
A as the maximum change in objective value that
can be effected by an assignment that is not the
</bodyText>
<figureCaption confidence="0.711025">
Figure 2: An illustration of the margin-based amortization
scheme showing the very simple case with only two compet-
ing assignments A and B. Suppose B is the solution yp for
</figureCaption>
<bodyText confidence="0.988819142857143">
the inference problem p with coefficients cp, denoted by the
red hyperplane, and A is the second-best assignment. For a
new coefficient vector cq, if the margin δ is greater than the
sum of the decrease in the objective value of yp and the max-
imum increase in the objective of another solution (Δ), then
the solution to the new inference problem will still be yp. The
margin-based amortization theorem captures this intuition.
</bodyText>
<equation confidence="0.9938685">
T
yEK[P],y�yp (cq — cp) y (4)
</equation>
<bodyText confidence="0.991558863636364">
Before stating the theorem, we will provide an in-
tuitive explanation for it. Moving from cp to cq,
consider the sum of the decrease in the value of
the objective for the solution yp and A, the maxi-
mum change in objective value for an assignment
that is not the solution. If this sum is less than the
margin S, then no other solution will have an ob-
jective value higher than yp. Figure 2 illustrates
this using a simple example where there are only
two competing solutions.
This intuition is captured by our main theorem
which provides a condition for problems p and q
to have the same solution yp.
Theorem 1 (Margin-based Amortization). Let p
denote an inference problem posed as an inte-
ger linear program belonging to an equivalence
class [P] with optimal solution yp. Let p have
a structured margin S, i.e., for any y, we have
cTpyp &gt; cTpy + S. Let q — [P] be another infer-
ence instance in the same equivalence class and
let A be defined as in Equation 4. Then, yp is the
solution of the problem q if the following holds:
</bodyText>
<figure confidence="0.966529461538462">
—(cq — cp)Typ + A G S (5)
decrease in
value of yp
cp
cpTyp
cq
increasing objective
S
A
A B = yp
Two assignments
solution. That is,
A = max
</figure>
<page confidence="0.940362">
907
</page>
<bodyText confidence="0.938944">
Proof. For some feasible y, we have
</bodyText>
<equation confidence="0.816235">
T TT
</equation>
<bodyText confidence="0.978561954545455">
cqyp − cqy &gt; cqyp − cTpy − A
&gt; cTqyp − cTpyp + S − A
&gt; 0
The first inequality comes from the definition of A
in (4) and the second one follows from the defini-
tion of S. The condition of the theorem in (5) gives
us the final step. For any feasible y, the objective
score assigned to yp is greater than the score as-
signed to y according to problem q. That is, yp is
the solution to the new problem.
The margin-based amortization theorem pro-
vides a general, new amortized inference algo-
rithm. Given a new inference problem, we check
whether the inequality (5) holds for any previously
seen problems in the same equivalence class. If so,
we return the cached solution. If no such problem
exists, then we make a call to an ILP solver.
Even though the theorem provides a condition
for two integer linear programs to have the same
solution, checking the validity of the condition re-
quires the computation of A, which in itself is an-
other integer linear program. To get around this,
we observe that if any constraints in Equation 4
are relaxed, the value of the resulting maximum
can only increase. Even with the increased A, if
the condition of the theorem holds, then the rest
of the proof follows and hence the new problem
will have the same solution. In other words, we
can solve relaxed, tractable variants of the maxi-
mization in Equation 4 and still retain the guaran-
tees provided by the theorem. The tradeoff is that,
by doing so, the condition of the theorem will ap-
ply to fewer examples than theoretically possible.
In our experiments, we will define the relaxation
for each problem individually and even with the
relaxations, the inference algorithm based on the
margin-based amortization theorem outperforms
all previous amortized inference algorithms.
The condition in inequality (5) is, in fact, a strict
generalization of the condition for Theorem 1 in
(Srikumar et al., 2012), stated in (2). If the latter
condition holds, then we can show that A &lt; 0 and
(cq − cp)Typ &gt; 0. Since S is, by definition, non-
negative, the margin-based condition is satisfied.
</bodyText>
<sectionHeader confidence="0.991613" genericHeader="method">
4 Decomposed Amortized Inference
</sectionHeader>
<bodyText confidence="0.999898432432432">
One limitation in previously considered ap-
proaches for amortized inference stems from the
expectation that the same full assignment maxi-
mizes the objective score for different inference
problems, or equivalently, that the entire structure
is repeated multiple times. Even with this assump-
tion, we observe a speedup in prediction.
However, intuitively, even if entire structures
are not repeated, we expect parts of the assign-
ment to be the same across different instances. In
this section, we address the following question:
Can we take advantage of the redundancy in com-
ponents of structures to extend amortization tech-
niques to cases where the full structured output is
not repeated? By doing so, we can store partial
computation for future inference problems.
For example, consider the task of part of speech
tagging. While the likelihood of two long sen-
tences having the same part of speech tag sequence
is not high, it is much more likely that shorter sec-
tions of the sentences will share the same tag se-
quence. We see from Figure 1 that the number of
possible structures for shorter sentences is much
smaller than the number of sentences. This im-
plies that many shorter sentences share the same
structure, thus improving the performance of an
amortized inference scheme for such inputs. The
goal of decomposed amortized inference is to ex-
tend this improvement to larger problems by in-
creasing the size of equivalence classes.
To decompose an inference problem, we use the
approach of Lagrangian Relaxation (Lemar´echal,
2001) that has been used successfully for various
NLP tasks (Chang and Collins, 2011; Rush and
Collins, 2011). We will briefly review the under-
lying idea1. The goal is to solve an integer linear
program q, which is defined as
</bodyText>
<equation confidence="0.5222195">
q :max
MTy&lt;b
</equation>
<bodyText confidence="0.999913125">
We partition the constraints into two sets, say C1
denoting M1Ty &lt; b1 and C2, denoting con-
straints M2Ty &lt; b2. The assumption is that in
the absence the constraints C2, the inference prob-
lem becomes computationally easier to solve. In
other words, we can assume the existence of a sub-
routine that can efficiently compute the solution of
the relaxed problem q&apos;:
</bodyText>
<equation confidence="0.75569">
q� : max
MITy&lt;bl
</equation>
<footnote confidence="0.8004505">
1For simplicity, we only write inequality constraints in
the paper. However, all the results here are easily extensible
to equality constraints by removing the non-negativity con-
straints from the corresponding dual variables.
</footnote>
<figure confidence="0.70124625">
cTy
q
cTy
q
</figure>
<page confidence="0.984822">
908
</page>
<bodyText confidence="0.999983666666667">
We define Lagrange multipliers A &gt; 0, with one
λi for each constraint in C2. For problem q, we
can define the Lagrangian as
</bodyText>
<equation confidence="0.8851315">
)
L(y, A) = cT qy − AT (M2Ty − b2
</equation>
<bodyText confidence="0.9940725">
Here, the domain of y is specified by the constraint
set C1. The dual objective is
</bodyText>
<equation confidence="0.9823956">
)
cT qy − AT (M2Ty − b2
(cq − ATM2)T y + ATb2.
= max
M1T y&lt;b1
</equation>
<bodyText confidence="0.999973">
Note that the maximization in the definition of the
dual objective has the same functional form as q&apos;
and any approach to solve q&apos; can be used here to
find the dual objective L(A). The dual of the prob-
lem q, given by ming&gt;0 L(A), can be solved us-
ing subgradient descent over the dual variables.
Relaxing the constraints C2 to define the prob-
lem q&apos; has several effects. First, it can make the re-
sulting inference problem q&apos; easier to solve. More
importantly, removing constraints can also lead to
the merging of multiple equivalence classes, lead-
ing to fewer, more populous equivalence classes.
Finally, removing constraints can decompose the
inference problem q&apos; into smaller independent
sub-problems {q1, q2, · · · } such that no constraint
that is in C1 has active variables from two different
sets in the partition.
For the sub-problem qi comprising of variables
yi, let the corresponding objective coefficients be
cqi and the corresponding sub-matrix of M2 be
Mi2. Now, we can define the dual-augmented sub-
problem as
</bodyText>
<equation confidence="0.99282075">
T
max (cqi − ATMz) yi (6)
Mi T
y&lt;bi
</equation>
<bodyText confidence="0.976469578947368">
Solving all such sub-problems will give us a com-
plete assignment for all the output variables.
We can now define the decomposed amortized
inference algorithm (Algorithm 1) that performs
sub-gradient descent over the dual variables. The
input to the algorithm is a collection of previ-
ously solved problems with their solutions, a new
inference problem q and an amortized inference
scheme A (such as the margin-based amortization
scheme). In addition, for the task at hand, we first
need to identify the set of constraints C2 that can
be introduced via the Lagrangian.
First, we check if the solution can be obtained
without decomposition (lines 1–2). Otherwise,
Algorithm 1 Decomposed Amortized Inference
Input: A collection of previously solved infer-
ence problems P, a new problem q, an amor-
tized inference algorithm A.
Output: The solution to problem q
</bodyText>
<listItem confidence="0.9768354">
1: if TESTCONDITION(A, q, P) then
2: return SOLUTION(A, q, P)
3: else
4: Initialize λi +— 0 for each constraint in C2.
5: fort = 1 ··· T do
6: Partition the problem q into sub-
problems q1, q2, · · · such that no con-
straint in C1 has active variables from
two partitions.
7: for partition qi do
8: yi +— Solve the maximization prob-
lem for qi (Eq. 6) using the amortized
scheme A.
9: end for
10: Let y +— [y1;y2; ···
</listItem>
<figure confidence="0.781930555555556">
1
11: if M2y G b2 and (b2 − M2y)iλi = 0
then
12: return y
13: else
A +— [A − µt (b2 − M2T y)1
14: +
15: end if
16: end for
</figure>
<listItem confidence="0.890048">
17: return solution of q using a standard infer-
ence algorithm
18: end if
</listItem>
<bodyText confidence="0.99947395">
we initialize the dual variables A and try to ob-
tain the solution iteratively. At the tth itera-
tion, we partition the problem q into sub-problems
{q1, q2, · · · } as described earlier (line 6). Each
partition defines a smaller inference problem with
its own objective coefficients and constraints. We
can apply the amortization scheme A to each sub-
problem to obtain a complete solution for the re-
laxed problem (lines 7–10). If this solution satis-
fies the constraints C2 and complementary slack-
ness conditions, then the solution is provably the
maximum of the problem q. Otherwise, we take a
subgradient step to update the value of A using a
step-size µt, subject to the constraint that all dual
variables must be non-negative (line 14). If we do
not converge to a solution in T iterations, we call
the underlying solver on the full problem.
In line 8 of the algorithm, we make multiple
calls to the underlying amortized inference pro-
cedure to solve each sub-problem. If the sub-
</bodyText>
<equation confidence="0.9891075">
L(A) = max
M1T y&lt;b1
</equation>
<page confidence="0.986249">
909
</page>
<bodyText confidence="0.999925285714286">
problem cannot be solved using the procedure,
then we can either solve the sub-problem using a
different approach (effectively giving us the stan-
dard Lagrangian relaxation algorithm for infer-
ence), or we can treat the full instance as a cache
miss and make a call to an ILP solver. In our ex-
periments, we choose the latter strategy.
</bodyText>
<sectionHeader confidence="0.996108" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9966548">
Our experiments show two results: 1. The margin-
based scheme outperforms the amortized infer-
ence approaches from (Srikumar et al., 2012).
2. Decomposed amortized inference gives further
gains in terms of re-using previous solutions.
</bodyText>
<subsectionHeader confidence="0.946027">
5.1 Tasks
</subsectionHeader>
<bodyText confidence="0.999718162790698">
We report the performance of inference on two
NLP tasks: semantic role labeling and the task of
extracting entities and relations from text. In both
cases, we used an existing formulation for struc-
tured inference and only modified the inference
calls. We will briefly describe the problems and
the implementation and point the reader to the lit-
erature for further details.
Semantic Role Labeling (SRL) Our first task is
that of identifying arguments of verbs in a sen-
tence and annotating them with semantic roles
(Gildea and Jurafsky, 2002; Palmer et al., 2010)
. For example, in the sentence Mrs. Haag plays
Eltiani., the verb plays takes two arguments: Mrs.
Haag, the actor, labeled as A0 and Eltiani, the
role, labeled as A1. It has been shown in prior
work (Punyakanok et al., 2008; Toutanova et al.,
2008) that making a globally coherent prediction
boosts performance of SRL.
In this work, we used the SRL system of (Pun-
yakanok et al., 2008), where one inference prob-
lem is generated for each verb and each infer-
ence variables encodes the decision that a given
constituent in the sentence takes a specific role.
The scores for the inference variables are obtained
from a classifier trained on the PropBank cor-
pus. Constraints encode structural and linguistic
knowledge about the problem. For details about
the formulations of the inference problem, please
see (Punyakanok et al., 2008).
Recall from Section 3 that we need to define a
relaxed version of the inference problem to effi-
ciently compute A for the margin-based approach.
For a problem instance with coefficients cq and
cached coefficients cp, we take the sum of the
highest n values of cq − cp as our A, where n is
the number of argument candidates to be labeled.
To identify constraints that can be relaxed for
the decomposed algorithm, we observe that most
constraints are not predicate specific and apply for
all predicates. The only constraint that is predi-
cate specific requires that each predicate can only
accept roles from a list of roles that is defined for
that predicate. By relaxing this constraint in the
decomposed algorithm, we effectively merge all
the equivalence classes for all predicates with a
specific number of argument candidates.
Entity-Relation extraction Our second task is
that of identifying the types of entities in a sen-
tence and the relations among them, which has
been studied by (Roth and Yih, 2007; Kate and
Mooney, 2010) and others. For the sentence
Oswald killed Kennedy, the words Oswald and
Kennedy will be labeled by the type PERSON, and
the KILL relation exists between them.
We followed the experimental setup as de-
scribed in (Roth and Yih, 2007). We defined one
inference problem for each sentence. For every
entity (which is identified by a constituent in the
sentence), an inference variable is introduced for
each entity type. For each pair of constituents, an
inference variable is introduced for each relation
type. Clearly, the assignment of types to entities
and relations are not independent. For example, an
entity of type ORGANIZATION cannot participate
in a relation of type BORN-IN because this rela-
tion label can connect entities of type PERSON and
LOCATION only. Incorporating these natural con-
straints during inference were shown to improve
performance significantly in (Roth and Yih, 2007).
We trained independent classifiers for entities and
relations and framed the inference problem as in
(Roth and Yih, 2007). For further details, we refer
the reader to that paper.
To compute the value of A for the margin-based
algorithm, for a new instance with coefficients cq
and cached coefficients cp, we define A to be the
sum of all non-negative values of cq − cp.
For the decomposed inference algorithm, if the
number of entities is less than 5, no decomposi-
tion is performed. Otherwise, the entities are par-
titioned into two sets: set A includes the first four
entities and set B includes the rest of the entities.
We relaxed the relation constraints that go across
these two sets of entities to obtain two independent
inference problems.
</bodyText>
<page confidence="0.989263">
910
</page>
<subsectionHeader confidence="0.993418">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999994413793104">
We follow the experimental setup of (Srikumar et
al., 2012) and simulate a long-running NLP pro-
cess by caching problems and solutions from the
Gigaword corpus. We used a database engine to
cache ILP and their solutions along with identi-
fiers for the equivalence class and the value of S.
For the margin-based algorithm and the Theo-
rem 1 from (Srikumar et al., 2012), for a new in-
ference problem p ∼ [P], we retrieve all infer-
ence problems from the database that belong to
the same equivalence class [P] as the test prob-
lem p and find the cached assignment y that has
the highest score according to the coefficients of
p. We only consider cached ILPs whose solution
is y for checking the conditions of the theorem.
This optimization ensures that we only process a
small number of cached coefficient vectors.
In a second efficiency optimization, we pruned
the database to remove redundant inference prob-
lems. A problem is redundant if solution to that
problem can be inferred from the other problems
stored in the database that have the same solution
and belong to the same equivalence class. How-
ever, this pruning can be computationally expen-
sive if the number of problems with the same so-
lution and the same equivalence class is very large.
In that case, we first sampled a 5000 problems ran-
domly and selected the non-redundant problems
from this set to keep in the database.
</bodyText>
<subsectionHeader confidence="0.815533">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999987333333333">
We compare our approach to a state-of-the-art ILP
solver2 and also to Theorem 1 from (Srikumar
et al., 2012). We choose this baseline because
it is shown to give the highest improvement in
wall-clock time and also in terms of the num-
ber of cache hits. However, we note that the re-
sults presented in our work outperform all the pre-
vious amortization algorithms, including the ap-
proximate inference methods.
We report two performance metrics – the per-
centage decrease in the number of ILP calls, and
the percentage decrease in the wall-clock infer-
ence time. These are comparable to the speedup
and clock speedup defined in (Srikumar et al.,
2012). For measuring time, since other aspects
of prediction (like feature extraction) are the same
across all settings, we only measure the time taken
for inference and ignore other aspects. For both
</bodyText>
<footnote confidence="0.705442">
2We used the Gurobi optimizer for our experiments.
</footnote>
<bodyText confidence="0.999972774193548">
tasks, we report the runtime performance on sec-
tion 23 of the Penn Treebank. Note that our amor-
tization schemes guarantee optimal solution. Con-
sequently, using amortization, task accuracy re-
mains the same as using the original solver.
Table 1 shows the percentage reduction in the
number of calls to the ILP solver. Note that for
both the SRL and entity-relation problems, the
margin-based approach, even without using de-
composition (the columns labeled Original), out-
performs the previous work. Applying the de-
composed inference algorithm improves both the
baseline and the margin-based approach. Overall,
however, the fewest number of calls to the solver is
made when combining the decomposed inference
algorithm with the margin-based scheme. For the
semantic role labeling task, we need to call the
solver only for one in six examples while for the
entity-relations task, only one in four examples re-
quire a solver call.
Table 2 shows the corresponding reduction in
the wall-clock time for the various settings. We
see that once again, the margin based approach
outperforms the baseline. While the decomposed
inference algorithm improves running time for
SRL, it leads to a slight increase for the entity-
relation problem. Since this increase occurs in
spite of a reduction in the number of solver calls,
we believe that this aspect can be further improved
with an efficient implementation of the decom-
posed inference algorithm.
</bodyText>
<sectionHeader confidence="0.99974" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.998012111111111">
Lagrangian Relaxation in the literature In the
literature, in applications of the Lagrangian relax-
ation technique (such as (Rush and Collins, 2011;
Chang and Collins, 2011; Reichart and Barzilay,
2012) and others), the relaxed problems are solved
using specialized algorithms. However, in both the
relaxations considered in this paper, even the re-
laxed problems cannot be solved without an ILP
solver, and yet we can see improvements from de-
composition in Table 1.
To study the impact of amortization on running
time, we modified our decomposition based infer-
ence algorithm to solve each sub-problem using
the ILP solver instead of amortization. In these ex-
periments, we ran Lagrangian relaxation for until
convergence or at most T iterations. After T itera-
tions, we call the ILP solver and solve the original
problem. We set T to 100 in one set of exper-
</bodyText>
<page confidence="0.991634">
911
</page>
<table confidence="0.999617833333333">
Method % ILP Solver calls required
Semantic Role Labeling Entity-Relation Extraction
Original + Decomp. Original + Decomp.
ILP Solver 100 – 100 –
(Srikumar et al., 2012) 41 24.4 59.5 57.0
Margin-based 32.7 16.6 28.2 25.4
</table>
<tableCaption confidence="0.999445">
Table 1: Reduction in number of inference calls
</tableCaption>
<table confidence="0.999847166666667">
Method % time required compared to ILP Solver
Semantic Role Labeling Entity-Relation Extraction
Original + Decomp. Original + Decomp.
ILP Solver 100 – 100 –
(Srikumar et al., 2012) 54.8 40.0 81 86
Margin-based 45.9 38.1 58.1 61.3
</table>
<tableCaption confidence="0.999929">
Table 2: Reduction in inference time
</tableCaption>
<bodyText confidence="0.999985629629629">
iments (call it Lag1) and T to 1 (call it Lag2).
In SRL, compared to solving the original problem
with ILP Solver, both Lag1 and Lag2 are roughly
2 times slower. For entity relation task, compared
to ILP Solver, Lag1 is 186 times slower and Lag2
is 1.91 times slower. Since we used the same im-
plementation of the decomposition in all experi-
ments, this shows that the decomposed inference
algorithm crucially benefits from the underlying
amortization scheme.
Decomposed amortized inference The decom-
posed amortized inference algorithm helps im-
prove amortized inference in two ways. First,
since the number of structures is a function of its
size, considering smaller sub-structures will allow
us to cache inference problems that cover a larger
subset of the space of possible sub-structures. We
observed this effect in the problem of extracting
entities and relations in text. Second, removing a
constraint need not always partition the structure
into a set of smaller structures. Instead, by re-
moving the constraint, examples that might have
otherwise been in different equivalence classes be-
come part of a combined, larger equivalence class.
Increasing the size of the equivalence classes in-
creases the probability of a cache-hit. In our ex-
periments, we observed this effect in the SRL task.
</bodyText>
<sectionHeader confidence="0.999244" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999989761904762">
Amortized inference takes advantage of the reg-
ularities in structured output to re-use previous
computation and improve running time over the
lifetime of a structured output predictor. In this pa-
per, we have described two approaches for amor-
tizing inference costs over datasets. The first,
called the margin-based amortized inference, is a
new, provably exact inference algorithm that uses
the notion of a structured margin to identify previ-
ously solved problems whose solutions can be re-
used. The second, called decomposed amortized
inference, is a meta-algorithm over any amortized
inference that takes advantage of previously com-
puted sub-structures to provide further reductions
in the number of inference calls. We show via ex-
periments that these methods individually give a
reduction in the number of calls made to an infer-
ence engine for semantic role labeling and entity-
relation extraction. Furthermore, these approaches
complement each other and, together give an addi-
tional significant improvement.
</bodyText>
<sectionHeader confidence="0.99765" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991343">
The authors thank the members of the Cognitive Computa-
tion Group at the University of Illinois for insightful discus-
sions and the anonymous reviewers for valuable feedback.
This research is sponsored by the Army Research Laboratory
(ARL) under agreement W911NF-09-2-0053. The authors
also gratefully acknowledge the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. This material also
is based on research sponsored by DARPA under agreement
number FA8750-13-2-0008. This work has also been sup-
ported by the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Interior National Business
Center contract number D11PC20155. The U.S. Govern-
ment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright an-
notation thereon. Any opinions, findings, and conclusions
or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the view of ARL,
DARPA, AFRL, IARPA, DoI/NBC or the US government.
</bodyText>
<page confidence="0.996064">
912
</page>
<sectionHeader confidence="0.993862" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999938745454546">
Y-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. EMNLP.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics.
R. Kate and R. Mooney. 2010. Joint entity and relation
extraction using card-pyramid parsing. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 203–212. Asso-
ciation for Computational Linguistics.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and
D. Sontag. 2010. Dual decomposition for parsing
with non-projective head automata. In EMNLP.
C. Lemar´echal. 2001. Lagrangian Relaxation. In
Computational Combinatorial Optimization, pages
112–156.
M. Palmer, D. Gildea, and N. Xue. 2010. Semantic
Role Labeling, volume 3. Morgan &amp; Claypool Pub-
lishers.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics.
R. Reichart and R. Barzilay. 2012. Multi event extrac-
tion guided by global constraints. In NAACL, pages
70–79.
S. Riedel and J. Clarke. 2006. Incremental integer
linear programming for non-projective dependency
parsing. In EMNLP.
S. Riedel. 2009. Cutting plane MAP inference for
Markov logic. Machine Learning.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Hwee Tou Ng and Ellen Riloff, editors,
CoNLL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
A.M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through Lagrangian re-
laxation. In ACL, pages 72–82, Portland, Oregon,
USA, June.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
V. Srikumar, G. Kundu, and D. Roth. 2012. On amor-
tizing inference cost for structured prediction. In
EMNLP.
K. Toutanova, A. Haghighi, and C. D. Manning. 2008.
A global joint model for semantic role labeling.
Computational Linguistics, 34:161–191.
</reference>
<page confidence="0.999087">
913
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.591416">
<title confidence="0.91703">Margin-based Decomposed Amortized Inference</title>
<affiliation confidence="0.999972">University of Illinois,</affiliation>
<address confidence="0.990928">Urbana, IL.</address>
<email confidence="0.822402">vsrikum2,</email>
<abstract confidence="0.99835344117647">Given that structured output prediction is typically performed over entire datasets, one natural question is whether it is possible to re-use computation from earlier inference instances to speed up inference for future instances. Amortized inference has been proposed as a way to accomplish this. In this paper, first, we introduce a new amortized inference algorithm called the Amortized which uses the notion of structured margin to identify inference problems for which previous solutions are provably optimal. Secwe introduce amortized which is designed to address very large inference problems, where earlier amortization methods become less effective. This approach works by decomposing the output structure and applying amortization piece-wise, thus increasing the chance that we can re-use previous sofor the output structure. These parts are then combined to a global coherent solution using Lagrangian relaxation. In our experiments, using the NLP tasks of semantic role labeling and entityrelation extraction, we demonstrate that with the margin-based algorithm, we need to call the inference engine only for a third of the test examples. Further, we show that the decomposed variant of margin-based amortized inference achieves a greater reduction in the number of inference calls.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y-W Chang</author>
<author>M Collins</author>
</authors>
<title>Exact decoding of phrase-based translation models through Lagrangian relaxation.</title>
<date>2011</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="5764" citStr="Chang and Collins, 2011" startWordPosition="889" endWordPosition="892">ence can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real data. Furthermore, among the observed structures, a small subset typically occurs much more frequently than the others. Figure 1 illustrates t</context>
<context position="15670" citStr="Chang and Collins, 2011" startWordPosition="2641" endWordPosition="2644"> share the same tag sequence. We see from Figure 1 that the number of possible structures for shorter sentences is much smaller than the number of sentences. This implies that many shorter sentences share the same structure, thus improving the performance of an amortized inference scheme for such inputs. The goal of decomposed amortized inference is to extend this improvement to larger problems by increasing the size of equivalence classes. To decompose an inference problem, we use the approach of Lagrangian Relaxation (Lemar´echal, 2001) that has been used successfully for various NLP tasks (Chang and Collins, 2011; Rush and Collins, 2011). We will briefly review the underlying idea1. The goal is to solve an integer linear program q, which is defined as q :max MTy&lt;b We partition the constraints into two sets, say C1 denoting M1Ty &lt; b1 and C2, denoting constraints M2Ty &lt; b2. The assumption is that in the absence the constraints C2, the inference problem becomes computationally easier to solve. In other words, we can assume the existence of a subroutine that can efficiently compute the solution of the relaxed problem q&apos;: q� : max MITy&lt;bl 1For simplicity, we only write inequality constraints in the paper. </context>
<context position="28852" citStr="Chang and Collins, 2011" startWordPosition="4905" endWordPosition="4908"> the various settings. We see that once again, the margin based approach outperforms the baseline. While the decomposed inference algorithm improves running time for SRL, it leads to a slight increase for the entityrelation problem. Since this increase occurs in spite of a reduction in the number of solver calls, we believe that this aspect can be further improved with an efficient implementation of the decomposed inference algorithm. 6 Discussion Lagrangian Relaxation in the literature In the literature, in applications of the Lagrangian relaxation technique (such as (Rush and Collins, 2011; Chang and Collins, 2011; Reichart and Barzilay, 2012) and others), the relaxed problems are solved using specialized algorithms. However, in both the relaxations considered in this paper, even the relaxed problems cannot be solved without an ILP solver, and yet we can see improvements from decomposition in Table 1. To study the impact of amortization on running time, we modified our decomposition based inference algorithm to solve each sub-problem using the ILP solver instead of amortization. In these experiments, we ran Lagrangian relaxation for until convergence or at most T iterations. After T iterations, we call</context>
</contexts>
<marker>Chang, Collins, 2011</marker>
<rawString>Y-W. Chang and M. Collins. 2011. Exact decoding of phrase-based translation models through Lagrangian relaxation. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Constraint-based sentence compression: An integer programming approach.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5467" citStr="Clarke and Lapata, 2006" startWordPosition="842" endWordPosition="845">oal of inference is to find the highest scoring global assignment of the variables from a feasible set of assignments, which is defined by linear inequalities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inferenc</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>J. Clarke and M. Lapata. 2006. Constraint-based sentence compression: An integer programming approach. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="21409" citStr="Gildea and Jurafsky, 2002" startWordPosition="3663" endWordPosition="3666">ized inference gives further gains in terms of re-using previous solutions. 5.1 Tasks We report the performance of inference on two NLP tasks: semantic role labeling and the task of extracting entities and relations from text. In both cases, we used an existing formulation for structured inference and only modified the inference calls. We will briefly describe the problems and the implementation and point the reader to the literature for further details. Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sentence and annotating them with semantic roles (Gildea and Jurafsky, 2002; Palmer et al., 2010) . For example, in the sentence Mrs. Haag plays Eltiani., the verb plays takes two arguments: Mrs. Haag, the actor, labeled as A0 and Eltiani, the role, labeled as A1. It has been shown in prior work (Punyakanok et al., 2008; Toutanova et al., 2008) that making a globally coherent prediction boosts performance of SRL. In this work, we used the SRL system of (Punyakanok et al., 2008), where one inference problem is generated for each verb and each inference variables encodes the decision that a given constituent in the sentence takes a specific role. The scores for the inf</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kate</author>
<author>R Mooney</author>
</authors>
<title>Joint entity and relation extraction using card-pyramid parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>203--212</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4161" citStr="Kate and Mooney, 2010" startWordPosition="643" endWordPosition="646">a significant improvement in speedup by considering repeated sub-structures across the dataset and applying any amortized inference algorithm for the parts. 905 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 905–913, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics We evaluate the two schemes and their combination on two NLP tasks where the output is encoded as a structure: PropBank semantic role labeling (Punyakanok et al., 2008) and the problem of recognizing entities and relations in text (Roth and Yih, 2007; Kate and Mooney, 2010). In these problems, the inference problem has been framed as an integer linear program (ILP). We compare our methods with previous amortized inference methods and show that margin-based amortization combined with decomposition significantly outperforms existing methods. 2 Problem Definition and Notation Structured output prediction encompasses a wide variety of NLP problems like part-of-speech tagging, parsing and machine translation. The language of 0-1 integer linear programs (ILP) provides a convenient analytical tool for representing structured prediction problems. The general setting con</context>
<context position="23291" citStr="Kate and Mooney, 2010" startWordPosition="3981" endWordPosition="3984">hm, we observe that most constraints are not predicate specific and apply for all predicates. The only constraint that is predicate specific requires that each predicate can only accept roles from a list of roles that is defined for that predicate. By relaxing this constraint in the decomposed algorithm, we effectively merge all the equivalence classes for all predicates with a specific number of argument candidates. Entity-Relation extraction Our second task is that of identifying the types of entities in a sentence and the relations among them, which has been studied by (Roth and Yih, 2007; Kate and Mooney, 2010) and others. For the sentence Oswald killed Kennedy, the words Oswald and Kennedy will be labeled by the type PERSON, and the KILL relation exists between them. We followed the experimental setup as described in (Roth and Yih, 2007). We defined one inference problem for each sentence. For every entity (which is identified by a constituent in the sentence), an inference variable is introduced for each entity type. For each pair of constituents, an inference variable is introduced for each relation type. Clearly, the assignment of types to entities and relations are not independent. For example,</context>
</contexts>
<marker>Kate, Mooney, 2010</marker>
<rawString>R. Kate and R. Mooney. 2010. Joint entity and relation extraction using card-pyramid parsing. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203–212. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5646" citStr="Koo et al., 2010" startWordPosition="870" endWordPosition="873">algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real data. Furthermore, among</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lemar´echal</author>
</authors>
<title>Lagrangian Relaxation.</title>
<date>2001</date>
<booktitle>In Computational Combinatorial Optimization,</booktitle>
<pages>112--156</pages>
<marker>Lemar´echal, 2001</marker>
<rawString>C. Lemar´echal. 2001. Lagrangian Relaxation. In Computational Combinatorial Optimization, pages 112–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>N Xue</author>
</authors>
<date>2010</date>
<booktitle>Semantic Role Labeling,</booktitle>
<volume>3</volume>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="21431" citStr="Palmer et al., 2010" startWordPosition="3667" endWordPosition="3670">r gains in terms of re-using previous solutions. 5.1 Tasks We report the performance of inference on two NLP tasks: semantic role labeling and the task of extracting entities and relations from text. In both cases, we used an existing formulation for structured inference and only modified the inference calls. We will briefly describe the problems and the implementation and point the reader to the literature for further details. Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sentence and annotating them with semantic roles (Gildea and Jurafsky, 2002; Palmer et al., 2010) . For example, in the sentence Mrs. Haag plays Eltiani., the verb plays takes two arguments: Mrs. Haag, the actor, labeled as A0 and Eltiani, the role, labeled as A1. It has been shown in prior work (Punyakanok et al., 2008; Toutanova et al., 2008) that making a globally coherent prediction boosts performance of SRL. In this work, we used the SRL system of (Punyakanok et al., 2008), where one inference problem is generated for each verb and each inference variables encodes the decision that a given constituent in the sentence takes a specific role. The scores for the inference variables are o</context>
</contexts>
<marker>Palmer, Gildea, Xue, 2010</marker>
<rawString>M. Palmer, D. Gildea, and N. Xue. 2010. Semantic Role Labeling, volume 3. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics.</title>
<date>2008</date>
<contexts>
<context position="4055" citStr="Punyakanok et al., 2008" startWordPosition="625" endWordPosition="628">r redundancy in the parts. We introduce the notion of decomposed amortized inference, whereby we can attain a significant improvement in speedup by considering repeated sub-structures across the dataset and applying any amortized inference algorithm for the parts. 905 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 905–913, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics We evaluate the two schemes and their combination on two NLP tasks where the output is encoded as a structure: PropBank semantic role labeling (Punyakanok et al., 2008) and the problem of recognizing entities and relations in text (Roth and Yih, 2007; Kate and Mooney, 2010). In these problems, the inference problem has been framed as an integer linear program (ILP). We compare our methods with previous amortized inference methods and show that margin-based amortization combined with decomposition significantly outperforms existing methods. 2 Problem Definition and Notation Structured output prediction encompasses a wide variety of NLP problems like part-of-speech tagging, parsing and machine translation. The language of 0-1 integer linear programs (ILP) prov</context>
<context position="21655" citStr="Punyakanok et al., 2008" startWordPosition="3708" endWordPosition="3711">d an existing formulation for structured inference and only modified the inference calls. We will briefly describe the problems and the implementation and point the reader to the literature for further details. Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sentence and annotating them with semantic roles (Gildea and Jurafsky, 2002; Palmer et al., 2010) . For example, in the sentence Mrs. Haag plays Eltiani., the verb plays takes two arguments: Mrs. Haag, the actor, labeled as A0 and Eltiani, the role, labeled as A1. It has been shown in prior work (Punyakanok et al., 2008; Toutanova et al., 2008) that making a globally coherent prediction boosts performance of SRL. In this work, we used the SRL system of (Punyakanok et al., 2008), where one inference problem is generated for each verb and each inference variables encodes the decision that a given constituent in the sentence takes a specific role. The scores for the inference variables are obtained from a classifier trained on the PropBank corpus. Constraints encode structural and linguistic knowledge about the problem. For details about the formulations of the inference problem, please see (Punyakanok et al., </context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichart</author>
<author>R Barzilay</author>
</authors>
<title>Multi event extraction guided by global constraints.</title>
<date>2012</date>
<booktitle>In NAACL,</booktitle>
<pages>70--79</pages>
<contexts>
<context position="28882" citStr="Reichart and Barzilay, 2012" startWordPosition="4909" endWordPosition="4912"> see that once again, the margin based approach outperforms the baseline. While the decomposed inference algorithm improves running time for SRL, it leads to a slight increase for the entityrelation problem. Since this increase occurs in spite of a reduction in the number of solver calls, we believe that this aspect can be further improved with an efficient implementation of the decomposed inference algorithm. 6 Discussion Lagrangian Relaxation in the literature In the literature, in applications of the Lagrangian relaxation technique (such as (Rush and Collins, 2011; Chang and Collins, 2011; Reichart and Barzilay, 2012) and others), the relaxed problems are solved using specialized algorithms. However, in both the relaxations considered in this paper, even the relaxed problems cannot be solved without an ILP solver, and yet we can see improvements from decomposition in Table 1. To study the impact of amortization on running time, we modified our decomposition based inference algorithm to solve each sub-problem using the ILP solver instead of amortization. In these experiments, we ran Lagrangian relaxation for until convergence or at most T iterations. After T iterations, we call the ILP solver and solve the </context>
</contexts>
<marker>Reichart, Barzilay, 2012</marker>
<rawString>R. Reichart and R. Barzilay. 2012. Multi event extraction guided by global constraints. In NAACL, pages 70–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5493" citStr="Riedel and Clarke, 2006" startWordPosition="846" endWordPosition="849">nd the highest scoring global assignment of the variables from a feasible set of assignments, which is defined by linear inequalities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observati</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>S. Riedel and J. Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
</authors>
<title>Cutting plane MAP inference for Markov logic.</title>
<date>2009</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="5608" citStr="Riedel, 2009" startWordPosition="865" endWordPosition="866">alities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever</context>
</contexts>
<marker>Riedel, 2009</marker>
<rawString>S. Riedel. 2009. Cutting plane MAP inference for Markov logic. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Hwee Tou Ng</booktitle>
<editor>and Ellen Riloff, editors, CoNLL.</editor>
<contexts>
<context position="5442" citStr="Roth and Yih, 2004" startWordPosition="838" endWordPosition="841"> with a score. The goal of inference is to find the highest scoring global assignment of the variables from a feasible set of assignments, which is defined by linear inequalities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivati</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Hwee Tou Ng and Ellen Riloff, editors, CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Global inference for entity and relation identification via a linear programming formulation.</title>
<date>2007</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<contexts>
<context position="4137" citStr="Roth and Yih, 2007" startWordPosition="639" endWordPosition="642">ereby we can attain a significant improvement in speedup by considering repeated sub-structures across the dataset and applying any amortized inference algorithm for the parts. 905 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 905–913, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics We evaluate the two schemes and their combination on two NLP tasks where the output is encoded as a structure: PropBank semantic role labeling (Punyakanok et al., 2008) and the problem of recognizing entities and relations in text (Roth and Yih, 2007; Kate and Mooney, 2010). In these problems, the inference problem has been framed as an integer linear program (ILP). We compare our methods with previous amortized inference methods and show that margin-based amortization combined with decomposition significantly outperforms existing methods. 2 Problem Definition and Notation Structured output prediction encompasses a wide variety of NLP problems like part-of-speech tagging, parsing and machine translation. The language of 0-1 integer linear programs (ILP) provides a convenient analytical tool for representing structured prediction problems.</context>
<context position="23267" citStr="Roth and Yih, 2007" startWordPosition="3977" endWordPosition="3980">e decomposed algorithm, we observe that most constraints are not predicate specific and apply for all predicates. The only constraint that is predicate specific requires that each predicate can only accept roles from a list of roles that is defined for that predicate. By relaxing this constraint in the decomposed algorithm, we effectively merge all the equivalence classes for all predicates with a specific number of argument candidates. Entity-Relation extraction Our second task is that of identifying the types of entities in a sentence and the relations among them, which has been studied by (Roth and Yih, 2007; Kate and Mooney, 2010) and others. For the sentence Oswald killed Kennedy, the words Oswald and Kennedy will be labeled by the type PERSON, and the KILL relation exists between them. We followed the experimental setup as described in (Roth and Yih, 2007). We defined one inference problem for each sentence. For every entity (which is identified by a constituent in the sentence), an inference variable is introduced for each entity type. For each pair of constituents, an inference variable is introduced for each relation type. Clearly, the assignment of types to entities and relations are not i</context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>D. Roth and W. Yih. 2007. Global inference for entity and relation identification via a linear programming formulation. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Rush</author>
<author>M Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through Lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>72--82</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="5738" citStr="Rush and Collins, 2011" startWordPosition="885" endWordPosition="888"> the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real data. Furthermore, among the observed structures, a small subset typically occurs much more frequently than the othe</context>
<context position="15695" citStr="Rush and Collins, 2011" startWordPosition="2645" endWordPosition="2648">nce. We see from Figure 1 that the number of possible structures for shorter sentences is much smaller than the number of sentences. This implies that many shorter sentences share the same structure, thus improving the performance of an amortized inference scheme for such inputs. The goal of decomposed amortized inference is to extend this improvement to larger problems by increasing the size of equivalence classes. To decompose an inference problem, we use the approach of Lagrangian Relaxation (Lemar´echal, 2001) that has been used successfully for various NLP tasks (Chang and Collins, 2011; Rush and Collins, 2011). We will briefly review the underlying idea1. The goal is to solve an integer linear program q, which is defined as q :max MTy&lt;b We partition the constraints into two sets, say C1 denoting M1Ty &lt; b1 and C2, denoting constraints M2Ty &lt; b2. The assumption is that in the absence the constraints C2, the inference problem becomes computationally easier to solve. In other words, we can assume the existence of a subroutine that can efficiently compute the solution of the relaxed problem q&apos;: q� : max MITy&lt;bl 1For simplicity, we only write inequality constraints in the paper. However, all the results </context>
<context position="28827" citStr="Rush and Collins, 2011" startWordPosition="4901" endWordPosition="4904"> the wall-clock time for the various settings. We see that once again, the margin based approach outperforms the baseline. While the decomposed inference algorithm improves running time for SRL, it leads to a slight increase for the entityrelation problem. Since this increase occurs in spite of a reduction in the number of solver calls, we believe that this aspect can be further improved with an efficient implementation of the decomposed inference algorithm. 6 Discussion Lagrangian Relaxation in the literature In the literature, in applications of the Lagrangian relaxation technique (such as (Rush and Collins, 2011; Chang and Collins, 2011; Reichart and Barzilay, 2012) and others), the relaxed problems are solved using specialized algorithms. However, in both the relaxations considered in this paper, even the relaxed problems cannot be solved without an ILP solver, and yet we can see improvements from decomposition in Table 1. To study the impact of amortization on running time, we modified our decomposition based inference algorithm to solve each sub-problem using the ILP solver instead of amortization. In these experiments, we ran Lagrangian relaxation for until convergence or at most T iterations. Af</context>
</contexts>
<marker>Rush, Collins, 2011</marker>
<rawString>A.M. Rush and M. Collins. 2011. Exact decoding of syntactic translation models through Lagrangian relaxation. In ACL, pages 72–82, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Rush</author>
<author>D Sontag</author>
<author>M Collins</author>
<author>T Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5666" citStr="Rush et al., 2010" startWordPosition="874" endWordPosition="877">or special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real data. Furthermore, among the observed struct</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Srikumar</author>
<author>G Kundu</author>
<author>D Roth</author>
</authors>
<title>On amortizing inference cost for structured prediction.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2145" citStr="Srikumar et al., 2012" startWordPosition="317" endWordPosition="320">on in the number of inference calls. 1 Introduction A wide variety of NLP problems can be naturally cast as structured prediction problems. For * These authors contributed equally to this work. some structures like sequences or parse trees, specialized and tractable dynamic programming algorithms have proven to be very effective. However, as the structures under consideration become increasingly complex, the computational problem of predicting structures can become very expensive, and in the worst case, intractable. In this paper, we focus on an inference technique called amortized inference (Srikumar et al., 2012), where previous solutions to inference problems are used to speed up new instances. The main observation that leads to amortized inference is that, very often, for different examples of the same size, the structures that maximize the score are identical. If we can efficiently identify that two inference problems have the same solution, then we can re-use previously computed structures for newer examples, thus giving us a speedup. This paper has two contributions. First, we describe a novel algorithm for amortized inference called margin-based amortization. This algorithm is on an examination </context>
<context position="5789" citStr="Srikumar et al., 2012" startWordPosition="893" endWordPosition="896"> intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real data. Furthermore, among the observed structures, a small subset typically occurs much more frequently than the others. Figure 1 illustrates this observation in the co</context>
<context position="8596" citStr="Srikumar et al., 2012" startWordPosition="1381" endWordPosition="1384">ot product cTy. Thus, the prediction problem can be written as arg max cTy. (1) y∈Kp 906 We denote the solution of this maximization problem as yp. Let the set P = {p1, p2, · · · } denote previously solved inference problems, along with their respective solutions {y1p, y2p, · · · }. An equivalence class of integer linear programs, denoted by [P], consists of ILPs which have the same number of inference variables and the same feasible set. Let K[P] denote the feasible set of an equivalence class [P]. For a program p, the notation p — [P] indicates that it belongs to the equivalence class [P]. (Srikumar et al., 2012) introduced a set of amortized inference schemes, each of which provides a condition for a new ILP to have the same solution as a previously seen problem. We will briefly review one exact inference scheme introduced in that work. Suppose q belongs to the same equivalence class of ILPs as p. Then the solution to q will be the same as that of p if the following condition holds for all inference variables: (gyp,i — 1)(cq,i — cp,i) &gt; 0. (2) This condition, referred to as Theorem 1 in that work, is the baseline for our experiments. In general, for any amortization scheme A, we can define two primit</context>
<context position="13848" citStr="Srikumar et al., 2012" startWordPosition="2341" endWordPosition="2344">s, we can solve relaxed, tractable variants of the maximization in Equation 4 and still retain the guarantees provided by the theorem. The tradeoff is that, by doing so, the condition of the theorem will apply to fewer examples than theoretically possible. In our experiments, we will define the relaxation for each problem individually and even with the relaxations, the inference algorithm based on the margin-based amortization theorem outperforms all previous amortized inference algorithms. The condition in inequality (5) is, in fact, a strict generalization of the condition for Theorem 1 in (Srikumar et al., 2012), stated in (2). If the latter condition holds, then we can show that A &lt; 0 and (cq − cp)Typ &gt; 0. Since S is, by definition, nonnegative, the margin-based condition is satisfied. 4 Decomposed Amortized Inference One limitation in previously considered approaches for amortized inference stems from the expectation that the same full assignment maximizes the objective score for different inference problems, or equivalently, that the entire structure is repeated multiple times. Even with this assumption, we observe a speedup in prediction. However, intuitively, even if entire structures are not re</context>
<context position="20763" citStr="Srikumar et al., 2012" startWordPosition="3559" endWordPosition="3562">ake multiple calls to the underlying amortized inference procedure to solve each sub-problem. If the subL(A) = max M1T y&lt;b1 909 problem cannot be solved using the procedure, then we can either solve the sub-problem using a different approach (effectively giving us the standard Lagrangian relaxation algorithm for inference), or we can treat the full instance as a cache miss and make a call to an ILP solver. In our experiments, we choose the latter strategy. 5 Experiments and Results Our experiments show two results: 1. The marginbased scheme outperforms the amortized inference approaches from (Srikumar et al., 2012). 2. Decomposed amortized inference gives further gains in terms of re-using previous solutions. 5.1 Tasks We report the performance of inference on two NLP tasks: semantic role labeling and the task of extracting entities and relations from text. In both cases, we used an existing formulation for structured inference and only modified the inference calls. We will briefly describe the problems and the implementation and point the reader to the literature for further details. Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sentence and annotating them </context>
<context position="25017" citStr="Srikumar et al., 2012" startWordPosition="4265" endWordPosition="4268">he margin-based algorithm, for a new instance with coefficients cq and cached coefficients cp, we define A to be the sum of all non-negative values of cq − cp. For the decomposed inference algorithm, if the number of entities is less than 5, no decomposition is performed. Otherwise, the entities are partitioned into two sets: set A includes the first four entities and set B includes the rest of the entities. We relaxed the relation constraints that go across these two sets of entities to obtain two independent inference problems. 910 5.2 Experimental Setup We follow the experimental setup of (Srikumar et al., 2012) and simulate a long-running NLP process by caching problems and solutions from the Gigaword corpus. We used a database engine to cache ILP and their solutions along with identifiers for the equivalence class and the value of S. For the margin-based algorithm and the Theorem 1 from (Srikumar et al., 2012), for a new inference problem p ∼ [P], we retrieve all inference problems from the database that belong to the same equivalence class [P] as the test problem p and find the cached assignment y that has the highest score according to the coefficients of p. We only consider cached ILPs whose sol</context>
<context position="26454" citStr="Srikumar et al., 2012" startWordPosition="4517" endWordPosition="4520">ve redundant inference problems. A problem is redundant if solution to that problem can be inferred from the other problems stored in the database that have the same solution and belong to the same equivalence class. However, this pruning can be computationally expensive if the number of problems with the same solution and the same equivalence class is very large. In that case, we first sampled a 5000 problems randomly and selected the non-redundant problems from this set to keep in the database. 5.3 Results We compare our approach to a state-of-the-art ILP solver2 and also to Theorem 1 from (Srikumar et al., 2012). We choose this baseline because it is shown to give the highest improvement in wall-clock time and also in terms of the number of cache hits. However, we note that the results presented in our work outperform all the previous amortization algorithms, including the approximate inference methods. We report two performance metrics – the percentage decrease in the number of ILP calls, and the percentage decrease in the wall-clock inference time. These are comparable to the speedup and clock speedup defined in (Srikumar et al., 2012). For measuring time, since other aspects of prediction (like fe</context>
<context position="29708" citStr="Srikumar et al., 2012" startWordPosition="5048" endWordPosition="5051"> yet we can see improvements from decomposition in Table 1. To study the impact of amortization on running time, we modified our decomposition based inference algorithm to solve each sub-problem using the ILP solver instead of amortization. In these experiments, we ran Lagrangian relaxation for until convergence or at most T iterations. After T iterations, we call the ILP solver and solve the original problem. We set T to 100 in one set of exper911 Method % ILP Solver calls required Semantic Role Labeling Entity-Relation Extraction Original + Decomp. Original + Decomp. ILP Solver 100 – 100 – (Srikumar et al., 2012) 41 24.4 59.5 57.0 Margin-based 32.7 16.6 28.2 25.4 Table 1: Reduction in number of inference calls Method % time required compared to ILP Solver Semantic Role Labeling Entity-Relation Extraction Original + Decomp. Original + Decomp. ILP Solver 100 – 100 – (Srikumar et al., 2012) 54.8 40.0 81 86 Margin-based 45.9 38.1 58.1 61.3 Table 2: Reduction in inference time iments (call it Lag1) and T to 1 (call it Lag2). In SRL, compared to solving the original problem with ILP Solver, both Lag1 and Lag2 are roughly 2 times slower. For entity relation task, compared to ILP Solver, Lag1 is 186 times slo</context>
</contexts>
<marker>Srikumar, Kundu, Roth, 2012</marker>
<rawString>V. Srikumar, G. Kundu, and D. Roth. 2012. On amortizing inference cost for structured prediction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>A Haghighi</author>
<author>C D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--161</pages>
<contexts>
<context position="21680" citStr="Toutanova et al., 2008" startWordPosition="3712" endWordPosition="3715"> for structured inference and only modified the inference calls. We will briefly describe the problems and the implementation and point the reader to the literature for further details. Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sentence and annotating them with semantic roles (Gildea and Jurafsky, 2002; Palmer et al., 2010) . For example, in the sentence Mrs. Haag plays Eltiani., the verb plays takes two arguments: Mrs. Haag, the actor, labeled as A0 and Eltiani, the role, labeled as A1. It has been shown in prior work (Punyakanok et al., 2008; Toutanova et al., 2008) that making a globally coherent prediction boosts performance of SRL. In this work, we used the SRL system of (Punyakanok et al., 2008), where one inference problem is generated for each verb and each inference variables encodes the decision that a given constituent in the sentence takes a specific role. The scores for the inference variables are obtained from a classifier trained on the PropBank corpus. Constraints encode structural and linguistic knowledge about the problem. For details about the formulations of the inference problem, please see (Punyakanok et al., 2008). Recall from Sectio</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>K. Toutanova, A. Haghighi, and C. D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34:161–191.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>