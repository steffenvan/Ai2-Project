<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012625">
<title confidence="0.996331">
Exploring Entity Relations for Named Entity Disambiguation
</title>
<author confidence="0.978709">
Danuta Ploch
</author>
<affiliation confidence="0.954926">
DAI-Labor, Technische Universit¨at Berlin
</affiliation>
<address confidence="0.602717">
Berlin, Germany
</address>
<email confidence="0.788872">
danuta.ploch@dai-labor.de
</email>
<sectionHeader confidence="0.992247" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99983652631579">
Named entity disambiguation is the task of
linking an entity mention in a text to the cor-
rect real-world referent predefined in a knowl-
edge base, and is a crucial subtask in many
areas like information retrieval or topic detec-
tion and tracking. Named entity disambigua-
tion is challenging because entity mentions
can be ambiguous and an entity can be refer-
enced by different surface forms. We present
an approach that exploits Wikipedia relations
between entities co-occurring with the am-
biguous form to derive a range of novel fea-
tures for classifying candidate referents. We
find that our features improve disambiguation
results significantly over a strong popularity
baseline, and are especially suitable for recog-
nizing entities not contained in the knowledge
base. Our system achieves state-of-the-art re-
sults on the TAC-KBP 2009 dataset.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990306208333334">
Identifying the correct real-world referents of named
entities (NE) mentioned in text (such as people, or-
ganizations, and geographic locations) plays an im-
portant role in various natural language processing
and information retrieval tasks. The goal of Named
Entity Disambiguation (NED) is to label a surface
form denoting an NE in text with one of multiple
predefined NEs from a knowledge base (KB), or
to detect that the surface form refers to an out-of-
KB entity, which is known as NIL detection. NED
has become a popular research field recently, as
the growth of large-scale publicly available encyclo-
pedic knowledge resources such as Wikipedia has
18
stimulated research on linking NEs in text to their
entries in these KBs (Bunescu and Pasca, 2006; Mc-
Namee and Dang, 2009).
The disambiguation of named entities raises sev-
eral challenges: Surface forms in text can be am-
biguous, and the same entity can be referred to by
different surface forms. For example, the surface
form “George Bush” may denote either of two for-
mer U.S. presidents, and the later president can be
referred to by “George W. Bush” or with his nick-
name “Dubya”. Thus, a many-to-many mapping be-
tween surface forms and entities has to be resolved.
In addition, entity mentions may not have a match-
ing entity in the KB, which is often the case for non-
popular entities.
Typical approaches to NED combine the use of
document context knowledge with entity informa-
tion stored in the KB in order to disambiguate en-
tities. Many systems represent document context
and KB information as word or concept vectors,
and rank entities using vector space similarity met-
rics (Cucerzan, 2007). Other authors employ su-
pervised machine learning algorithms to classify or
rank candidate entities (Bunescu and Pasca, 2006;
Zhang et al., 2010). Common features include pop-
ularity metrics based on Wikipedia’s graph structure
or on name mention frequency (Dredze et al., 2010;
Han and Zhao, 2009), similarity metrics explor-
ing Wikipedia’s concept relations (Han and Zhao,
2009), and string similarity features. Recent work
also addresses the task of NIL detection (Dredze et
al., 2010).
While previous research has largely focused on
disambiguating each entity mention in a document
</bodyText>
<subsectionHeader confidence="0.376641">
Proceedings of the ACL-HLT 2011 Student Session, pages 18–23,
</subsectionHeader>
<bodyText confidence="0.99040404">
Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics
separately (McNamee and Dang, 2009), we explore
an approach that is driven by the observation that
entities normally co-occur in texts. Documents of-
ten discuss several different entities related to each
other, e.g. a news article may report on a meeting
of political leaders from different countries. Analo-
gously, entries in a KB such as Wikipedia are linked
to other, related entries.
Our Contributions In this paper, we evaluate a
range of novel disambiguation features that exploit
the relations between NEs identified in a document
and in the KB. Our goal is to explore the usefulness
of Wikipedia’s link structure as source of relations
between entities. We propose a method for candi-
date selection that is based on an inverted index of
surface forms and entities (Section 3.2). Instead of
a bag-of-words approach we use co-occurring NEs
in text for describing an ambiguous surface form.
We introduce several different disambiguation fea-
tures that exploit the relations between entities de-
rived from the graph structure of Wikipedia (Section
3.3). Finally, we combine our disambiguation fea-
tures and achieve state-of-the-art results with a Sup-
port Vector Machine (SVM) classifier (Section 4).
</bodyText>
<sectionHeader confidence="0.897448" genericHeader="method">
2 Problem statement
</sectionHeader>
<bodyText confidence="0.998988888888889">
The task of NED is to assign a surface form s found
in a document d to a target NE t E E(s), where
E(s) C E is a set of candidate NEs from an entity
KB that is defined by E = {e1, e2,..., e,}, or to
recognize that the found surface form s refers to a
missing target entity t E� E(s). For solving the task,
three main challenges have to be addressed:
Ambiguity Names of NEs may be ambiguous.
Since the same surface form s may refer to more
than one NE e, the correct target entity t has to be
determined from a set of candidates E(s)
Name variants Often, name variants (e.g. abbre-
viations, acronyms or synonyms) are used in texts
to refer to the same NE, which has to be considered
for the determination of candidates E(s) for a given
surface form s.
KB coverage KBs cover only a limited number
of NEs, mostly popular NEs. Another challenge of
</bodyText>
<figureCaption confidence="0.978246">
Figure 1: Ambiguity of Wikipedia surface forms. The
</figureCaption>
<bodyText confidence="0.877381833333333">
distribution follows a power law, as many surface forms
have only a single meaning (i.e. refer to a single
Wikipedia concept), and some surface forms are highly
ambiguous, referring to very many different concepts.
NED is therefore to recognize missing NEs where
t E� E(s), given a surface form s (NIL detection).
</bodyText>
<sectionHeader confidence="0.988475" genericHeader="method">
3 Named Entity Disambiguation
</sectionHeader>
<bodyText confidence="0.999976">
We formulate NED as a supervised binary classifi-
cation problem. In this section we describe the con-
struction and structure of the KB and the candidate
selection scheme, followed by an overview of dis-
ambiguation features and the candidate classifica-
tion algorithm.
</bodyText>
<subsectionHeader confidence="0.999493">
3.1 Knowledge base construction
</subsectionHeader>
<bodyText confidence="0.9999828125">
Our approach disambiguates named entities against
a KB constructed from Wikipedia. To this end, we
process Wikipedia to extract several types of infor-
mation for each Wikipedia article describing a con-
cept (i.e. any article not being a redirect page, a dis-
ambiguation page, or any other kind of meta page).
We collect a set of name variants (surface forms)
for each concept from article titles, redirect pages,
disambiguation pages and the anchor texts of inter-
nal Wikipedia links, following Cucerzan (2007). For
each concept, we also collect its set of incoming and
outgoing links to other Wikipedia pages. Finally, we
extract the article’s full text. We store this informa-
tion in an inverted index, which allows for very ef-
ficient access and search during candidate selection
and feature computation.
</bodyText>
<page confidence="0.99541">
19
</page>
<bodyText confidence="0.999958875">
The distribution of surface forms follows a power
law, where the majority of surface forms is unam-
biguous, but some surface forms are very ambigu-
ous (Figure 1). This suggests that for a given set of
distinct surface forms found in a document, many of
these will unambiguously refer to a single Wikipedia
entity. These entities can then be used to disam-
biguate surface forms referring to multiple entities.
</bodyText>
<subsectionHeader confidence="0.999706">
3.2 Candidate selection
</subsectionHeader>
<bodyText confidence="0.999944833333333">
Given a surface form identified in a document, the
task of the candidate selection component is to re-
trieve a set of candidate entities from the KB. To
this end, we execute a search on index fields storing
article titles, redirect titles, and name variants. We
implement a weighted search to give high weights
to exact title matches, a lesser emphasis on redi-
rect matches, and finally a low weight for all other
name variants. In addition, we implement a fuzzy
search on the title and redirect fields to select KB
entries with approximate string similarity to the sur-
face form.
</bodyText>
<subsectionHeader confidence="0.996594">
3.3 Disambiguation features
</subsectionHeader>
<bodyText confidence="0.999806875">
In this section, we describe the features that we use
in our disambiguation approach.
Entity Context (EC) The EC disambiguation fea-
ture is calculated as the cosine similarity between
the document context d of a surface form s and the
Wikipedia article c of each candidate c E E(s). We
represent both contexts as vectors of URIs. To create
d we extract all NEs from the text using the Stanford
NE Recognizer (Finkel et al., 2005) and represent
each NE by its Wikipedia URI. If a surface form is
ambiguous, we choose the most popular NE with the
popularity metric described below. Analogously, we
represent each c as a vector of the incoming and out-
going URIs found on its Wikipedia page.
Link Context (LC) The link context feature is an
extension of the EC feature. Since our observa-
tions have shown that the entity context can be very
small and consequently the overlap between d and
c may be very low, we extend d by all incoming
(LC-in) or by all incoming and outgoing (LC-all)
Wikipedia URIs of the NEs from the entity context.
We assume that Wikipedia pages that refer to other
Wikipedia pages contain information on the refer-
enced pages or at least are thematically related to
these pages. With the extension of d to d’, we ex-
pect a higher overlap between the context vectors, so
that cos(d’, c) &gt; cos(d, c).
Candidate Rank (CR) The features described so
far disambiguate every surface form s E S from a
document d separately, whereas our Candidate Rank
feature aims to disambiguate all surface forms S
found in a document d at once. We represent d as
a graph D = (E(S), L(E(S))) where the nodes
E(S) = UIESE(s) are all candidates of all surface
forms in the document and L(E(S)) is the set of
links between the candidates, as found in Wikipedia.
Then, we compute the PageRank score (Brin and
Page, 1998) of all c E E(S) and choose for each
s the candidate with the highest PageRank score in
the document graph D.
Standard Features In addition to the previously
described features we also implement a set of com-
monly accepted features. These include a feature
based on the cosine similarity between word vector
representations of the document and the Wikipedia
article of each candidate (BOW) (Bunescu, 2007).
We perform stemming, remove stopwords, and
weight words with tf.idf in both cases. Another stan-
dard feature we use is the popularity of a surface
form (SFP). We calculate how often a surface form s
references a candidate c E E(s) in relation to the to-
tal number of mentions of s in Wikipedia (Han and
Zhao, 2009). Since we use an index for selecting
candidates (Section 3.2), we also exploit the candi-
date selection score (CS) returned for each candidate
as a disambiguation feature.
</bodyText>
<subsectionHeader confidence="0.999637">
3.4 Candidate classifier and NIL detection
</subsectionHeader>
<bodyText confidence="0.999795">
We cast NED as a supervised classification task and
use two binary SVM classifiers (Vapnik, 1995). The
first classifier decides for each candidate c E E(s) if
it corresponds to the target entity. Each candidate is
represented as a vector x(&apos;) of features. For training
the classifier we label as a positive example at most
one x(&apos;) from the set of candidates for a surface form
s, and all others as negative.
In addition, we train a separate classifier to detect
NIL queries, i.e. where all x(&apos;) from E(s) are labeled
as negative examples. This may e.g. be the case
</bodyText>
<page confidence="0.982503">
20
</page>
<table confidence="0.999577375">
All queries KB NIL
Baseline features 0.7797 0.6246 0.8964
All features 0.8391 0.6795 0.9592
Best features 0.8422 0.6825 0.9623
Dredze et al. 0.7941 0.6639 0.8919
Zheng et al. 0.8494 0.7900 0.8941
Best TAC 2009 0.8217 0.7725 0.8919
Median TAC 2009 0.7108 0.6352 0.7891
</table>
<tableCaption confidence="0.9586722">
Table 1: Micro-averaged accuracy for TAC-KBP 2009
data compared for different feature sets. The best feature
set contains all features except for LC-all and CR. Our
system outperforms previously reported results on NIL
queries, and compares favorably on all queries.
</tableCaption>
<bodyText confidence="0.9996932">
if the similarity values of all candidates c E E(s)
are very low. We calculate several different fea-
tures, such as the maximum, mean and minimum,
the difference between maximum and mean, and the
difference between maximum and minimum, of all
atomic features, using the feature vectors of all can-
didates in E(s). Both classifier use a radial basis
function kernel, with parameter settings of C = 32
and -y = 8. We optimized these settings on a sepa-
rate development dataset.
</bodyText>
<sectionHeader confidence="0.998744" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.982272695652174">
We conduct our experiments on the 2009 Knowl-
edge Base Population (KBP) dataset of the Text
Analysis Conference (TAC) (McNamee and Dang,
2009). The dataset consists of a KB derived from a
2008 snapshot of the English Wikipedia, and a col-
lection of newswire, weblog and newsgroup docu-
ments. A set of 3904 surface form-document pairs
(queries) is constructed from these sources, encom-
passing 560 unique entities. The majority of queries
(57%) are NIL queries, of the KB queries, 69%
are for organizations and 15% each for persons and
geopolitical entities. For each query the surface
form appearing in the given document has to be dis-
ambiguated against the KB.
We randomly split the 3904 queries to perform
10-fold cross-validation, and stratify the resulting
folds to ensure a similar distribution of KB and NIL
queries in our training data. After normalizing fea-
ture values to be in [0, 1], we train a candidate and
a NIL classifier on 90% of the queries in each it-
eration, and test using the remaining 10%. Results
reported in this paper are then averaged across the
Baseline features Best features Dredze et al. Zheng et al.
</bodyText>
<figureCaption confidence="0.997808333333333">
Figure 2: The micro-averaged accuracy for all types of
queries on TAC-KBP 2009 data in comparison to other
systems.
</figureCaption>
<bodyText confidence="0.997308903225806">
test folds.
Table 1 compares the micro-averaged accuracy of
our approach on KB and NIL queries for different
feature sets, and lists the results of two other state-
of-the-art systems (Dredze et al., 2010; Zheng et al.,
2010), as well as the best and median reported per-
formance of the 2009 TAC-KBP track (McNamee
et al., 2010). Micro-averaged accuracy is calculated
as the fraction of correct queries, and is the official
TAC-KBP evaluation measure. As a baseline we use
a feature set consisting of the BOW and SFP fea-
tures. The best feature set in our experiments com-
prises all features except for the LC-all and CR fea-
tures.
Our best accuracy of 0.84 compares favorably
with other state-of-the-art systems on this dataset.
Using the best feature set improves the disambigua-
tion accuracy by 6.2% over the baseline feature set,
which is significant at p = 0.05. For KB queries
our system’s accuracy is higher than that of Dredze
et al., but lower than the accuracy reported by Zheng
et al. One striking result is the high accuracy for NIL
queries, where our approach outperforms all previ-
ously reported results (Figure 2).
Figure 3 displays the performance of our ap-
proach when iteratively adding features. We can
see that the novel entity features contribute to a
higher overall accuracy. Including the candidate se-
lection score (CS) improves accuracy by 3.6% over
the baseline. The Wikipedia link-based features pro-
vide additional gains, however differences are quite
</bodyText>
<figure confidence="0.994100230769231">
All queries KB NIL
Micro-averaged accuracy
0,9
0,8
0,7
0,6
0,5
0,4
0,3
0,2
0,1
0,0
1,0
</figure>
<page confidence="0.99701">
21
</page>
<table confidence="0.736827083333333">
0,85 dataset, following the TAC-KBP dataset creation
0,80 guidelines. In addition to Wikipedia, we also intend
0,75 to exploit more dynamical information sources. For
example, when considering news articles, NEs of-
ten occur for a certain period of time in consecutive
news dealing with the same topic. This short-time
context could be a useful source of information for
disambiguating novel entities.
Micro-averaged accuracy
SFP + BOW CS + SFP + CS + SFP + CS + SFP + CS + SFP + All References
BOW BOW + EC BOW + EC + BOW + EC +
LC-in LC-all
</table>
<figureCaption confidence="0.9919045">
Figure 3: Differences in micro-averaged accuracy for
various feature combinations on TAC-KBP 2009 data.
Adding Wikipedia link-based features significantly im-
proves performance over the baseline feature set.
</figureCaption>
<bodyText confidence="0.9999652">
small (1.0 − 1.5%). We find that there is hardly any
difference in performance between using the LC-
all and LC-in features. The Candidate Rank (CR)
feature slightly decreases the overall accuracy. A
manual inspection of the CR feature shows that of-
ten candidates cannot be distinguished by the clas-
sifier because they are assigned the same PageRank
scores. We assume this results from our use of uni-
form priors for the edges and vertices of the docu-
ment graphs.
</bodyText>
<sectionHeader confidence="0.990526" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999933722222222">
We presented a supervised approach for named
entity disambiguation that explores novel features
based on Wikipedia’s link structure. These features
use NEs co-occurring with an ambiguous surface
form in a document and their Wikipedia relations to
score the candidates. Our system achieves state-of-
the-art results on the TAC-KBP 2009 dataset. We
find that our features improve disambiguation results
by 6.2% over the popularity baseline, and are espe-
cially helpful for recognizing entities not contained
in the KB.
In future work we plan to explore multilin-
gual data for NED. Since non-English versions of
Wikipedia often are less extensive than the English
version we find it promising to combine Wikipedia
versions of different languages and to use them as a
source for multilingual NED. For multilingual NED
evaluation we are currently working on a German
</bodyText>
<reference confidence="0.993913268292683">
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
WWW7: Proceedings of the seventh international con-
ference on World Wide Web 7, pages 107–117, Ams-
terdam, The Netherlands. Elsevier Science Publishers
B. V.
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), pages 9–16, Trento, Italy.
Razvan Constantin Bunescu. 2007. Learning for Infor-
mation Extraction: From Named Entity Recognition
and Disambiguation To Relation Extraction. Ph.D.
thesis, University of Texas at Austin, Department of
Computer Sciences.
Silviu Cucerzan. 2007. Large-Scale named entity dis-
ambiguation based on Wikipedia data. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL),
pages 708–716, Prague, Czech Republic. Association
for Computational Linguistics.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Proceedings of
the 23rd International Conference on Computational
Linguistics (Coling 2010), pages 277–285, Beijing,
China. Coling 2010 Organizing Committee.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363–370,
Ann Arbor, Michigan. Association for Computational
Linguistics.
Xianpei Han and Jun Zhao. 2009. Named entity dis-
ambiguation by leveraging wikipedia semantic knowl-
edge. In Proceeding of the 18th ACM conference on
Information and knowledge management, pages 215–
224, Hong Kong, China. ACM.
</reference>
<page confidence="0.972589">
22
</page>
<reference confidence="0.999324038461538">
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the tac 2009 knowledge base population track. In
Text Analysis Conference (TAC).
Paul McNamee, Hoa Trang Dang, Heather Simpson,
Patrick Schone, and Stephanie M. Strassel. 2010. An
evaluation of technologies for knowledge base pop-
ulation. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10). European Language Resources Associa-
tion (ELRA).
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Wei Zhang, Jian Su, Chew Lim Tan, and Wen Ting Wang.
2010. Entity linking leveraging automatically gener-
ated annotation. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 1290–1298, Beijing, China. Coling
2010 Organizing Committee.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
’10, pages 483–491, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.998937">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.391183">
<title confidence="0.998751">Exploring Entity Relations for Named Entity Disambiguation</title>
<author confidence="0.860867">Danuta</author>
<affiliation confidence="0.741417">DAI-Labor, Technische Universit¨at Berlin,</affiliation>
<email confidence="0.997272">danuta.ploch@dai-labor.de</email>
<abstract confidence="0.9942973">Named entity disambiguation is the task of linking an entity mention in a text to the correct real-world referent predefined in a knowledge base, and is a crucial subtask in many areas like information retrieval or topic detection and tracking. Named entity disambiguation is challenging because entity mentions can be ambiguous and an entity can be referenced by different surface forms. We present an approach that exploits Wikipedia relations between entities co-occurring with the ambiguous form to derive a range of novel features for classifying candidate referents. We find that our features improve disambiguation results significantly over a strong popularity baseline, and are especially suitable for recognizing entities not contained in the knowledge base. Our system achieves state-of-the-art results on the TAC-KBP 2009 dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In WWW7: Proceedings of the seventh international conference on World Wide Web 7,</booktitle>
<pages>107--117</pages>
<publisher>Elsevier Science Publishers</publisher>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="9803" citStr="Brin and Page, 1998" startWordPosition="1632" endWordPosition="1635">ages. With the extension of d to d’, we expect a higher overlap between the context vectors, so that cos(d’, c) &gt; cos(d, c). Candidate Rank (CR) The features described so far disambiguate every surface form s E S from a document d separately, whereas our Candidate Rank feature aims to disambiguate all surface forms S found in a document d at once. We represent d as a graph D = (E(S), L(E(S))) where the nodes E(S) = UIESE(s) are all candidates of all surface forms in the document and L(E(S)) is the set of links between the candidates, as found in Wikipedia. Then, we compute the PageRank score (Brin and Page, 1998) of all c E E(S) and choose for each s the candidate with the highest PageRank score in the document graph D. Standard Features In addition to the previously described features we also implement a set of commonly accepted features. These include a feature based on the cosine similarity between word vector representations of the document and the Wikipedia article of each candidate (BOW) (Bunescu, 2007). We perform stemming, remove stopwords, and weight words with tf.idf in both cases. Another standard feature we use is the popularity of a surface form (SFP). We calculate how often a surface for</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. In WWW7: Proceedings of the seventh international conference on World Wide Web 7, pages 107–117, Amsterdam, The Netherlands. Elsevier Science Publishers B. V.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06),</booktitle>
<pages>9--16</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="1774" citStr="Bunescu and Pasca, 2006" startWordPosition="271" endWordPosition="274">phic locations) plays an important role in various natural language processing and information retrieval tasks. The goal of Named Entity Disambiguation (NED) is to label a surface form denoting an NE in text with one of multiple predefined NEs from a knowledge base (KB), or to detect that the surface form refers to an out-ofKB entity, which is known as NIL detection. NED has become a popular research field recently, as the growth of large-scale publicly available encyclopedic knowledge resources such as Wikipedia has 18 stimulated research on linking NEs in text to their entries in these KBs (Bunescu and Pasca, 2006; McNamee and Dang, 2009). The disambiguation of named entities raises several challenges: Surface forms in text can be ambiguous, and the same entity can be referred to by different surface forms. For example, the surface form “George Bush” may denote either of two former U.S. presidents, and the later president can be referred to by “George W. Bush” or with his nickname “Dubya”. Thus, a many-to-many mapping between surface forms and entities has to be resolved. In addition, entity mentions may not have a matching entity in the KB, which is often the case for nonpopular entities. Typical appr</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06), pages 9–16, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Constantin Bunescu</author>
</authors>
<title>Learning for Information Extraction: From Named Entity Recognition and Disambiguation To Relation Extraction.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Texas at Austin, Department of Computer Sciences.</institution>
<contexts>
<context position="10207" citStr="Bunescu, 2007" startWordPosition="1701" endWordPosition="1702">s E(S) = UIESE(s) are all candidates of all surface forms in the document and L(E(S)) is the set of links between the candidates, as found in Wikipedia. Then, we compute the PageRank score (Brin and Page, 1998) of all c E E(S) and choose for each s the candidate with the highest PageRank score in the document graph D. Standard Features In addition to the previously described features we also implement a set of commonly accepted features. These include a feature based on the cosine similarity between word vector representations of the document and the Wikipedia article of each candidate (BOW) (Bunescu, 2007). We perform stemming, remove stopwords, and weight words with tf.idf in both cases. Another standard feature we use is the popularity of a surface form (SFP). We calculate how often a surface form s references a candidate c E E(s) in relation to the total number of mentions of s in Wikipedia (Han and Zhao, 2009). Since we use an index for selecting candidates (Section 3.2), we also exploit the candidate selection score (CS) returned for each candidate as a disambiguation feature. 3.4 Candidate classifier and NIL detection We cast NED as a supervised classification task and use two binary SVM </context>
</contexts>
<marker>Bunescu, 2007</marker>
<rawString>Razvan Constantin Bunescu. 2007. Learning for Information Extraction: From Named Entity Recognition and Disambiguation To Relation Extraction. Ph.D. thesis, University of Texas at Austin, Department of Computer Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-Scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>708--716</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2669" citStr="Cucerzan, 2007" startWordPosition="427" endWordPosition="428">residents, and the later president can be referred to by “George W. Bush” or with his nickname “Dubya”. Thus, a many-to-many mapping between surface forms and entities has to be resolved. In addition, entity mentions may not have a matching entity in the KB, which is often the case for nonpopular entities. Typical approaches to NED combine the use of document context knowledge with entity information stored in the KB in order to disambiguate entities. Many systems represent document context and KB information as word or concept vectors, and rank entities using vector space similarity metrics (Cucerzan, 2007). Other authors employ supervised machine learning algorithms to classify or rank candidate entities (Bunescu and Pasca, 2006; Zhang et al., 2010). Common features include popularity metrics based on Wikipedia’s graph structure or on name mention frequency (Dredze et al., 2010; Han and Zhao, 2009), similarity metrics exploring Wikipedia’s concept relations (Han and Zhao, 2009), and string similarity features. Recent work also addresses the task of NIL detection (Dredze et al., 2010). While previous research has largely focused on disambiguating each entity mention in a document Proceedings of </context>
<context position="6659" citStr="Cucerzan (2007)" startWordPosition="1084" endWordPosition="1085">rview of disambiguation features and the candidate classification algorithm. 3.1 Knowledge base construction Our approach disambiguates named entities against a KB constructed from Wikipedia. To this end, we process Wikipedia to extract several types of information for each Wikipedia article describing a concept (i.e. any article not being a redirect page, a disambiguation page, or any other kind of meta page). We collect a set of name variants (surface forms) for each concept from article titles, redirect pages, disambiguation pages and the anchor texts of internal Wikipedia links, following Cucerzan (2007). For each concept, we also collect its set of incoming and outgoing links to other Wikipedia pages. Finally, we extract the article’s full text. We store this information in an inverted index, which allows for very efficient access and search during candidate selection and feature computation. 19 The distribution of surface forms follows a power law, where the majority of surface forms is unambiguous, but some surface forms are very ambiguous (Figure 1). This suggests that for a given set of distinct surface forms found in a document, many of these will unambiguously refer to a single Wikiped</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-Scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 708–716, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Paul McNamee</author>
<author>Delip Rao</author>
<author>Adam Gerber</author>
<author>Tim Finin</author>
</authors>
<title>Entity disambiguation for knowledge base population.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>277--285</pages>
<location>Beijing, China. Coling</location>
<contexts>
<context position="2946" citStr="Dredze et al., 2010" startWordPosition="467" endWordPosition="470"> the case for nonpopular entities. Typical approaches to NED combine the use of document context knowledge with entity information stored in the KB in order to disambiguate entities. Many systems represent document context and KB information as word or concept vectors, and rank entities using vector space similarity metrics (Cucerzan, 2007). Other authors employ supervised machine learning algorithms to classify or rank candidate entities (Bunescu and Pasca, 2006; Zhang et al., 2010). Common features include popularity metrics based on Wikipedia’s graph structure or on name mention frequency (Dredze et al., 2010; Han and Zhao, 2009), similarity metrics exploring Wikipedia’s concept relations (Han and Zhao, 2009), and string similarity features. Recent work also addresses the task of NIL detection (Dredze et al., 2010). While previous research has largely focused on disambiguating each entity mention in a document Proceedings of the ACL-HLT 2011 Student Session, pages 18–23, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics separately (McNamee and Dang, 2009), we explore an approach that is driven by the observation that entities normally co-occur in texts. Documents </context>
<context position="13770" citStr="Dredze et al., 2010" startWordPosition="2303" endWordPosition="2306">training data. After normalizing feature values to be in [0, 1], we train a candidate and a NIL classifier on 90% of the queries in each iteration, and test using the remaining 10%. Results reported in this paper are then averaged across the Baseline features Best features Dredze et al. Zheng et al. Figure 2: The micro-averaged accuracy for all types of queries on TAC-KBP 2009 data in comparison to other systems. test folds. Table 1 compares the micro-averaged accuracy of our approach on KB and NIL queries for different feature sets, and lists the results of two other stateof-the-art systems (Dredze et al., 2010; Zheng et al., 2010), as well as the best and median reported performance of the 2009 TAC-KBP track (McNamee et al., 2010). Micro-averaged accuracy is calculated as the fraction of correct queries, and is the official TAC-KBP evaluation measure. As a baseline we use a feature set consisting of the BOW and SFP features. The best feature set in our experiments comprises all features except for the LC-all and CR features. Our best accuracy of 0.84 compares favorably with other state-of-the-art systems on this dataset. Using the best feature set improves the disambiguation accuracy by 6.2% over t</context>
</contexts>
<marker>Dredze, McNamee, Rao, Gerber, Finin, 2010</marker>
<rawString>Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin. 2010. Entity disambiguation for knowledge base population. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277–285, Beijing, China. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="8422" citStr="Finkel et al., 2005" startWordPosition="1383" endWordPosition="1386">me variants. In addition, we implement a fuzzy search on the title and redirect fields to select KB entries with approximate string similarity to the surface form. 3.3 Disambiguation features In this section, we describe the features that we use in our disambiguation approach. Entity Context (EC) The EC disambiguation feature is calculated as the cosine similarity between the document context d of a surface form s and the Wikipedia article c of each candidate c E E(s). We represent both contexts as vectors of URIs. To create d we extract all NEs from the text using the Stanford NE Recognizer (Finkel et al., 2005) and represent each NE by its Wikipedia URI. If a surface form is ambiguous, we choose the most popular NE with the popularity metric described below. Analogously, we represent each c as a vector of the incoming and outgoing URIs found on its Wikipedia page. Link Context (LC) The link context feature is an extension of the EC feature. Since our observations have shown that the entity context can be very small and consequently the overlap between d and c may be very low, we extend d by all incoming (LC-in) or by all incoming and outgoing (LC-all) Wikipedia URIs of the NEs from the entity contex</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370, Ann Arbor, Michigan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Jun Zhao</author>
</authors>
<title>Named entity disambiguation by leveraging wikipedia semantic knowledge.</title>
<date>2009</date>
<booktitle>In Proceeding of the 18th ACM conference on Information and knowledge management,</booktitle>
<pages>215--224</pages>
<publisher>ACM.</publisher>
<location>Hong Kong, China.</location>
<contexts>
<context position="2967" citStr="Han and Zhao, 2009" startWordPosition="471" endWordPosition="474">lar entities. Typical approaches to NED combine the use of document context knowledge with entity information stored in the KB in order to disambiguate entities. Many systems represent document context and KB information as word or concept vectors, and rank entities using vector space similarity metrics (Cucerzan, 2007). Other authors employ supervised machine learning algorithms to classify or rank candidate entities (Bunescu and Pasca, 2006; Zhang et al., 2010). Common features include popularity metrics based on Wikipedia’s graph structure or on name mention frequency (Dredze et al., 2010; Han and Zhao, 2009), similarity metrics exploring Wikipedia’s concept relations (Han and Zhao, 2009), and string similarity features. Recent work also addresses the task of NIL detection (Dredze et al., 2010). While previous research has largely focused on disambiguating each entity mention in a document Proceedings of the ACL-HLT 2011 Student Session, pages 18–23, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics separately (McNamee and Dang, 2009), we explore an approach that is driven by the observation that entities normally co-occur in texts. Documents often discuss several</context>
<context position="10521" citStr="Han and Zhao, 2009" startWordPosition="1757" endWordPosition="1760">t graph D. Standard Features In addition to the previously described features we also implement a set of commonly accepted features. These include a feature based on the cosine similarity between word vector representations of the document and the Wikipedia article of each candidate (BOW) (Bunescu, 2007). We perform stemming, remove stopwords, and weight words with tf.idf in both cases. Another standard feature we use is the popularity of a surface form (SFP). We calculate how often a surface form s references a candidate c E E(s) in relation to the total number of mentions of s in Wikipedia (Han and Zhao, 2009). Since we use an index for selecting candidates (Section 3.2), we also exploit the candidate selection score (CS) returned for each candidate as a disambiguation feature. 3.4 Candidate classifier and NIL detection We cast NED as a supervised classification task and use two binary SVM classifiers (Vapnik, 1995). The first classifier decides for each candidate c E E(s) if it corresponds to the target entity. Each candidate is represented as a vector x(&apos;) of features. For training the classifier we label as a positive example at most one x(&apos;) from the set of candidates for a surface form s, and </context>
</contexts>
<marker>Han, Zhao, 2009</marker>
<rawString>Xianpei Han and Jun Zhao. 2009. Named entity disambiguation by leveraging wikipedia semantic knowledge. In Proceeding of the 18th ACM conference on Information and knowledge management, pages 215– 224, Hong Kong, China. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of the tac 2009 knowledge base population track.</title>
<date>2009</date>
<booktitle>In Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="1799" citStr="McNamee and Dang, 2009" startWordPosition="275" endWordPosition="279">important role in various natural language processing and information retrieval tasks. The goal of Named Entity Disambiguation (NED) is to label a surface form denoting an NE in text with one of multiple predefined NEs from a knowledge base (KB), or to detect that the surface form refers to an out-ofKB entity, which is known as NIL detection. NED has become a popular research field recently, as the growth of large-scale publicly available encyclopedic knowledge resources such as Wikipedia has 18 stimulated research on linking NEs in text to their entries in these KBs (Bunescu and Pasca, 2006; McNamee and Dang, 2009). The disambiguation of named entities raises several challenges: Surface forms in text can be ambiguous, and the same entity can be referred to by different surface forms. For example, the surface form “George Bush” may denote either of two former U.S. presidents, and the later president can be referred to by “George W. Bush” or with his nickname “Dubya”. Thus, a many-to-many mapping between surface forms and entities has to be resolved. In addition, entity mentions may not have a matching entity in the KB, which is often the case for nonpopular entities. Typical approaches to NED combine the</context>
<context position="3435" citStr="McNamee and Dang, 2009" startWordPosition="538" endWordPosition="541">10). Common features include popularity metrics based on Wikipedia’s graph structure or on name mention frequency (Dredze et al., 2010; Han and Zhao, 2009), similarity metrics exploring Wikipedia’s concept relations (Han and Zhao, 2009), and string similarity features. Recent work also addresses the task of NIL detection (Dredze et al., 2010). While previous research has largely focused on disambiguating each entity mention in a document Proceedings of the ACL-HLT 2011 Student Session, pages 18–23, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics separately (McNamee and Dang, 2009), we explore an approach that is driven by the observation that entities normally co-occur in texts. Documents often discuss several different entities related to each other, e.g. a news article may report on a meeting of political leaders from different countries. Analogously, entries in a KB such as Wikipedia are linked to other, related entries. Our Contributions In this paper, we evaluate a range of novel disambiguation features that exploit the relations between NEs identified in a document and in the KB. Our goal is to explore the usefulness of Wikipedia’s link structure as source of rel</context>
<context position="12467" citStr="McNamee and Dang, 2009" startWordPosition="2081" endWordPosition="2084">he similarity values of all candidates c E E(s) are very low. We calculate several different features, such as the maximum, mean and minimum, the difference between maximum and mean, and the difference between maximum and minimum, of all atomic features, using the feature vectors of all candidates in E(s). Both classifier use a radial basis function kernel, with parameter settings of C = 32 and -y = 8. We optimized these settings on a separate development dataset. 4 Evaluation We conduct our experiments on the 2009 Knowledge Base Population (KBP) dataset of the Text Analysis Conference (TAC) (McNamee and Dang, 2009). The dataset consists of a KB derived from a 2008 snapshot of the English Wikipedia, and a collection of newswire, weblog and newsgroup documents. A set of 3904 surface form-document pairs (queries) is constructed from these sources, encompassing 560 unique entities. The majority of queries (57%) are NIL queries, of the KB queries, 69% are for organizations and 15% each for persons and geopolitical entities. For each query the surface form appearing in the given document has to be disambiguated against the KB. We randomly split the 3904 queries to perform 10-fold cross-validation, and stratif</context>
</contexts>
<marker>McNamee, Dang, 2009</marker>
<rawString>Paul McNamee and Hoa Trang Dang. 2009. Overview of the tac 2009 knowledge base population track. In Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Trang Dang</author>
<author>Heather Simpson</author>
<author>Patrick Schone</author>
<author>Stephanie M Strassel</author>
</authors>
<title>An evaluation of technologies for knowledge base population.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10). European Language Resources Association (ELRA).</booktitle>
<contexts>
<context position="13893" citStr="McNamee et al., 2010" startWordPosition="2326" endWordPosition="2329">queries in each iteration, and test using the remaining 10%. Results reported in this paper are then averaged across the Baseline features Best features Dredze et al. Zheng et al. Figure 2: The micro-averaged accuracy for all types of queries on TAC-KBP 2009 data in comparison to other systems. test folds. Table 1 compares the micro-averaged accuracy of our approach on KB and NIL queries for different feature sets, and lists the results of two other stateof-the-art systems (Dredze et al., 2010; Zheng et al., 2010), as well as the best and median reported performance of the 2009 TAC-KBP track (McNamee et al., 2010). Micro-averaged accuracy is calculated as the fraction of correct queries, and is the official TAC-KBP evaluation measure. As a baseline we use a feature set consisting of the BOW and SFP features. The best feature set in our experiments comprises all features except for the LC-all and CR features. Our best accuracy of 0.84 compares favorably with other state-of-the-art systems on this dataset. Using the best feature set improves the disambiguation accuracy by 6.2% over the baseline feature set, which is significant at p = 0.05. For KB queries our system’s accuracy is higher than that of Dred</context>
</contexts>
<marker>McNamee, Dang, Simpson, Schone, Strassel, 2010</marker>
<rawString>Paul McNamee, Hoa Trang Dang, Heather Simpson, Patrick Schone, and Stephanie M. Strassel. 2010. An evaluation of technologies for knowledge base population. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10). European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York,</location>
<contexts>
<context position="10833" citStr="Vapnik, 1995" startWordPosition="1808" endWordPosition="1809">stemming, remove stopwords, and weight words with tf.idf in both cases. Another standard feature we use is the popularity of a surface form (SFP). We calculate how often a surface form s references a candidate c E E(s) in relation to the total number of mentions of s in Wikipedia (Han and Zhao, 2009). Since we use an index for selecting candidates (Section 3.2), we also exploit the candidate selection score (CS) returned for each candidate as a disambiguation feature. 3.4 Candidate classifier and NIL detection We cast NED as a supervised classification task and use two binary SVM classifiers (Vapnik, 1995). The first classifier decides for each candidate c E E(s) if it corresponds to the target entity. Each candidate is represented as a vector x(&apos;) of features. For training the classifier we label as a positive example at most one x(&apos;) from the set of candidates for a surface form s, and all others as negative. In addition, we train a separate classifier to detect NIL queries, i.e. where all x(&apos;) from E(s) are labeled as negative examples. This may e.g. be the case 20 All queries KB NIL Baseline features 0.7797 0.6246 0.8964 All features 0.8391 0.6795 0.9592 Best features 0.8422 0.6825 0.9623 D</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Zhang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
<author>Wen Ting Wang</author>
</authors>
<title>Entity linking leveraging automatically generated annotation.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1290--1298</pages>
<location>Beijing, China. Coling</location>
<contexts>
<context position="2815" citStr="Zhang et al., 2010" startWordPosition="447" endWordPosition="450">urface forms and entities has to be resolved. In addition, entity mentions may not have a matching entity in the KB, which is often the case for nonpopular entities. Typical approaches to NED combine the use of document context knowledge with entity information stored in the KB in order to disambiguate entities. Many systems represent document context and KB information as word or concept vectors, and rank entities using vector space similarity metrics (Cucerzan, 2007). Other authors employ supervised machine learning algorithms to classify or rank candidate entities (Bunescu and Pasca, 2006; Zhang et al., 2010). Common features include popularity metrics based on Wikipedia’s graph structure or on name mention frequency (Dredze et al., 2010; Han and Zhao, 2009), similarity metrics exploring Wikipedia’s concept relations (Han and Zhao, 2009), and string similarity features. Recent work also addresses the task of NIL detection (Dredze et al., 2010). While previous research has largely focused on disambiguating each entity mention in a document Proceedings of the ACL-HLT 2011 Student Session, pages 18–23, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics separately (McN</context>
</contexts>
<marker>Zhang, Su, Tan, Wang, 2010</marker>
<rawString>Wei Zhang, Jian Su, Chew Lim Tan, and Wen Ting Wang. 2010. Entity linking leveraging automatically generated annotation. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1290–1298, Beijing, China. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Zheng</author>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Learning to link entities with knowledge base.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>483--491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13791" citStr="Zheng et al., 2010" startWordPosition="2307" endWordPosition="2310">normalizing feature values to be in [0, 1], we train a candidate and a NIL classifier on 90% of the queries in each iteration, and test using the remaining 10%. Results reported in this paper are then averaged across the Baseline features Best features Dredze et al. Zheng et al. Figure 2: The micro-averaged accuracy for all types of queries on TAC-KBP 2009 data in comparison to other systems. test folds. Table 1 compares the micro-averaged accuracy of our approach on KB and NIL queries for different feature sets, and lists the results of two other stateof-the-art systems (Dredze et al., 2010; Zheng et al., 2010), as well as the best and median reported performance of the 2009 TAC-KBP track (McNamee et al., 2010). Micro-averaged accuracy is calculated as the fraction of correct queries, and is the official TAC-KBP evaluation measure. As a baseline we use a feature set consisting of the BOW and SFP features. The best feature set in our experiments comprises all features except for the LC-all and CR features. Our best accuracy of 0.84 compares favorably with other state-of-the-art systems on this dataset. Using the best feature set improves the disambiguation accuracy by 6.2% over the baseline feature s</context>
</contexts>
<marker>Zheng, Li, Huang, Zhu, 2010</marker>
<rawString>Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010. Learning to link entities with knowledge base. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 483–491, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>