<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.935747" genericHeader="abstract">
ABSTRACTS OF CURRENT LITERATURE
</sectionHeader>
<bodyText confidence="0.994012444444444">
In October of 1984, a workshop was held at Duke University to consider problems relating to the design of transport-
able natural language processors. The workshop was funded in part by a grant from the National Science Foundation
and was co-sponsored by Duke and the ACL. A number of papers resulted from the meeting, and several of them
appear in the April 1985 issue of the ACM Transactions on Office Information Systems (volume 3, number 2). Bruce
Ballard guest edited the special issue, and wishes to thank the following persons for their assistance in reviewing papers
from the workshop: Joan Bachenko, Ron Brachman, Fred Damerau, Samuel Epstein, Kurt Godden, Ralph Grishman,
Carole Hafner, Jerry Hobbs, Karen Jensen, Mark Jones, Karen Kukich, Elaine Marsh, Sharon Salveter, Jonathan
Slocum, Norm Sondheimer, Douglas Stumberger, and Craig Thompson. The following are abstracts from that special
issue.
</bodyText>
<figure confidence="0.934683142857143">
Transportable Natural Language Process-
ing through Simplicity — The PRE System
Samuel S. Epstein
Bell Communications Research
435 South Street
Morristown, NJ 07960
Transporting the Linguistic String Project
System from a Medical to a Navy Domain
Elaine Marsh
Navy Center for Applied Research in
Artificial Intelligence
Carol Friedman
Courant Institute of Mathematical
Sciences
Portability of Syntax and Semantics in
Datalog
Carole D. Hafner, Kurt Godden
General Motors Research Laboratories
Problems and Some Solutions in Customi-
zation of Natural Language Database
Front Ends
</figure>
<note confidence="0.5566525">
Fred J. Damerau
IBM T. J. Watson Research Laboratory
</note>
<bodyText confidence="0.997439395348837">
PRE (Purposefully Restricted English) is a restricted English database
query language whose implementation has addressed engineering goals,
namely, habitability, interapplication transportability, performance, and
use with a reliable database management system that supports large
numbers of concurrent users and large databases. Habitability has not
been demonstrated, but initial indications are encouraging. The other
goals have clearly been achieved. The existence of the PRE system demon-
strates that an explicitly &amp;quot;minimalist&amp;quot; approach to natural language proc-
essing can facilitate achievement of transportability.
The Linguistic String Project (LSP) natural language processing system has
been developed as a domain-independent natural language processing
system. Initially utilized for processing sets of medical messages and other
texts in the medical domain, it has been used at the Naval Research Labo-
ratory for processing Navy messages about shipboard equipment failures.
This paper describes the structure of the LSP system and the features that
make it transportable from one domain to another. The processing proce-
dures encourage the isolation of domain-specific information, yet take
advantage of the syntactic and semantic similarities between the medical
and Navy domains From our experience in transporting the LSP system,
we identify the features that are required for transportable natural
language systems.
This paper presents a discussion of the techniques developed and problems
encountered during the design, implementation, and experimental use of a
portable natural language processor. Datalog (for &amp;quot;database dialogue&amp;quot;) is
an experimental natural language query system, which was designed to
achieve a maximum degree of portability and extendibility. Datalog uses a
three-level architecture to provide both portability of syntax to new and
extended tasks and portability of semantics to new database applications.
The implementation of each of the three levels, the structures and
conventions that control the interactions among them, and the way in
which different aspects of the design contribute to portability are
described. Finally, two specific, implemented examples are presented,
showing how it was possible to transport or extend Datalog by changing
only one &amp;quot;layer&amp;quot; of the system&apos;s knowledge and achieve correct processing
of the extended input by the entire system.
This paper is concerned with some of the issues arising in the development
of a domain-independent English interface to IBM SQL-based program
products. The TQA system falls into the class of multilayered natural
language processing systems. As a result, there is a large number of poten-
tial points at which customization to a particular database can be done. Of
these, we discuss procedures that affect the reader, the lexicon, the lowest
level of grammar rules, the semantic interpreter, and the output formatter.
Our tests lead us to believe that the approach we are taking will make it
</bodyText>
<note confidence="0.87857525">
Computational Linguistics, Volume 11, Number 4, October-December 1985 253
The FINITE STRING Abstracts of Current Literature
ASK Is Transportable in Half a Dozen
Ways
</note>
<affiliation confidence="0.810829">
Bozena Henisz Thompson,
Frederick B. Thompson
California Institute of Technology
</affiliation>
<note confidence="0.9079876">
Transportability to Other Languages: The
Natural Language Processing Project in
the AI Program at MCC
Jonathan Slocum, Carol F. Justus
Microelectronics and Computer
</note>
<subsectionHeader confidence="0.568207">
Technology Corporation
</subsectionHeader>
<bodyText confidence="0.961161783783784">
possible for database administrators to generate roliust English interfaces
to particular databases without help from linguistic experts.
This paper is a discussion of the technical issues and solutions encountered
in making the ASK System transportable. A natural language system can be
&amp;quot;transportable&amp;quot; in a number of ways. Although transportability
to a new domain is most prominent, other ways are also important if the
system is to have viability in the commercial marketplace.
On the one hand, transporting a system to a new domain may start with
the system prior to adding any domain of knowledge and extend it to
incorporate the new domain. On the other hand, one may wish to add to
a system that already has knowledge of one domain the knowledge
concerning a second domain, that is, to extend the system to cover this
second domain. In the context of ASK, it has been natural to implement
extending and then achieve transportability as a special case.
In this paper, we consider six ways in which the ASK System can be
extended to include new capabilities:
— to a new domain,
— to a new object type,
— to access data from a foreign database,
— to a new natural language,
— to a new programming language,
— to a new computer family.
Special-purpose applications, such as those to accommodate standard
office tasks, would make use of these various means of extension.
We discuss a recently launched, long-term project in natural language
processing, the primary concern of which is that natural language applica-
tions be transportable among human languages. In particular, we seek to
develop system tools and linguistic processing techniques that are them-
selves language-independent to the maximum extent practical. In this paper
we discuss our project goals and outline our intended approach, address
some cross-linguistic requirements, and then present some new linguistic
data that we feel support our approach.
The following abstracts are from Proceedings of the Conference on Theoretical and Methodological Issues in Machine
Translation of Natural Languages, held at Colgate University, Hamilton, New York, 14-16 August 1985. Copies of the
Proceedings are no longer available; papers should be requested from the author(s).
A MU View of the &lt;C,A&gt;,T Framework
in Eurotra
</bodyText>
<note confidence="0.578657">
Doug Arnold, Lieven Jaspaert, Rod Johnson,
Steven Kraumvr, Mike Rosner, Louis
des Tombe, Nino Varile, Susan Warwick
pp. 1-14
On the Production Environment
Proposed for the Eiwotra Project
</note>
<sectionHeader confidence="0.317298" genericHeader="categories and subject descriptors">
D. Bachut
</sectionHeader>
<bodyText confidence="0.98705795">
Groupe d&apos;Etudes pour la Traduction
The background to this paper is the attempt within EUROTRA to develop a
general framework for research and development work in MT, providing in
particular an environment which facilitates reasoning about the relation-
ships between the representations that are necessary for automatic trans-
lation between natural languages. The more immediate background is the
attempt to apply this framework experimentally on a small scale in devel-
oping a &amp;quot;proto-EUROTRA&amp;quot; (affectionately, &amp;quot;the toy&amp;quot;) over the summer
and autumn of this year. The aim of this paper is to give a reasonably clear
idea about the user language and theories of representation for this exper-
iment (the Mu 1 level in terms of the paper by Louis des Tombe), and to
indicate en route some of the directions for further work. It reports work
in progress, and is thus deliberately speculative, programmatic, and rather
informal.
For concreteness, section 2 gives a brief and rather casual restatement
of some of the key assumptions behind this work.
We present the general architecture of a production environment which is
specific for a M(A)T system, and give some proposals to integrate new
functionalities in this system. A good management of the results of the
translation process may lead to an easier improvement of the linguistic
</bodyText>
<page confidence="0.947181">
254 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<figure confidence="0.9542725">
The FINITE STRING Abstracts of Current Literature
Automatique
Universite Scientifique et Medicale de
Grenoble
BP 68 - 38402 Saint Martin d&apos;Heres
&amp; Institut de Formation et Conseil en
Informatique
27, rue de Turenne - 3800 Grenoble
FRANCE
pp. 15-26
A Case Study in Software Evolution:
from Ariane-78.4 to Ariane-85
Ch. Boitet, P. Guillaume,
M. Quezel-Ambrunaz
Groupe d&apos;Etudes pour la Traduction
Automatique
Universite Scientifique et Mddicale de
Grenoble
BP 68 - 38402 Saint Martin d&apos;Heres,
FRANCE
pp. 27-58
New Approaches to Machine Translation
Jaime G. Carbonell, Mamas Tomita
Carnegie-Mellon University
Pittsburgh, PA 15213
pp. 59-74
Lexicon-Driven Machine Translation
R.E. Cullingford
Georgia Institute of Technology
Atlanta, GA 30332
B.A. Onyshkevyrh
Princeton University
Princeton, NJ 08544
pp. 75-115
On the Design of Expert Systems Grafted
on MT Systems
R. Gerber, Ch. Boitet
Groupe d&apos;Etudes pour la Traduction
Automatique
Universite Scientifique et Medicale de
Grenoble
BP 68 - 38402 Saint Martin d&apos;Heres,
FRANCE
pp. 116-134
</figure>
<bodyText confidence="0.997344317073171">
data.
We describe a possible organisation for the machine environment of
such a system and for the management of the data base of texts. Finally,
we give some general rules for the implementation of a monitor.
No abstract.
The current resurgence of interest in machine translation is partially attrib-
utable to the emergence of a variety of new paradigms, ranging from better
translation aids and improved pre- and post-editing methods, to highly
interactive approaches and fully automated knowledge-based systems.
This paper discusses each basic approach and provides some comparative
analysis. It is argued that both interactive and knowledge-based systems
offer considerable promise to remedy the deficiencies of the earlier, more
ad hoc post-editing approaches.
Machine Translation (MT) systems have historically relied upon explicit
grammars in order to analyze the source text and reproduce it in the target
language. In this paper, we argue for a style of MT in which the focus
of processing is at the level of the lexicon, rather than the grammar. This
approach to translation allows an analyzer to map source sentences into an
interlingual form, which then can be mapped (perhaps after intermediate
inferencing steps) back into target sentence(s) which are paraphrase-equi-
valent to the original. Advantages of the approach include: (1) the possi-
bility for different paraphrases of the original; (2) the capability for multi-
sentence expression of the original when no single word (e.g., a verb)
exists in the target language which spans the same meaning complex as a
word in the source; (3) a uniform approach to word sense disambiguation
and anaphoric reference resolution; and, most important, (4) the possibility
for robust handling of ungrammatical and ellipsed source text.
Our MT systems integrate many advanced concepts from the fields of
computer science, linguistics, and AI: specialized languages for linguistic
programming based on production systems, complete linguistic program-
ming environment, multilevel representations, organization of the lexicons
around &amp;quot;lexical units&amp;quot;, units of translation of the size of several para-
graphs, possibility to use text-driven heuristic strategies.
We are now beginning to integrate new techniques: unified design of an
&amp;quot;integrated&amp;quot; lexical data base containing the lexical in &amp;quot;natural&amp;quot; and
&amp;quot;coded&amp;quot; form, use of the &amp;quot;static grammars&amp;quot; formalism as a specification
language, and design of a kind of structural meta-editor (driven by some
static grammar) allowing the interactive construction of a document in the
same way as syntactic editors are used for developing programs.
This paper centers on our study of possible additions of expert systems
equipped with metalinguistic and extralinguistic knowledge, in order to
</bodyText>
<figure confidence="0.949020357142857">
Computational Linguistics, Volume 11, Number 4, October-December 1985 255
The FINITE STRING Abstracts of Current Literature
Machine Translation in the SDCG
Formalism
Xiuming Huang
Institute of Linguistics
Chinese Academy of Social Sciences
Beijing, China
mail: Computing Research Laboratory
New Mexico State University
Las &apos;Cruces, NM 88003
pp. 135-144
Machine Translation as an Expert Task
Rod Johnson, Pete Whitelock
Centre for Computational Linguistics
University of Manchester Institute of
Science and Technology
P.O. Box 88
Manchester M60 1QD UK
pp. 145-153
The Significance of Sublanguage for
Automatic Translation
Richard I. Kittredge
University of Montreal
pp. 154-166
Integrating Syntax and Semantics
Steven L. Lytinen
Cognitive Systems, Inc.
</figure>
<footnote confidence="0.833775">
234 Church Street
New Haven, CT 06510
pp. 167-178
</footnote>
<bodyText confidence="0.977135072727273">
solve some problems encountered in second-generation MT systems.
Several examples of the possible use of expert-corrector systems in M(A)T
(Machine (Aided) Translation) systems are given.
The paper describes the SDCG (Semantic Definite Clause Grammars), a
formalism for Natural Language Processing (NLP), and the XTRA (English
Chinese Sentence TRAnslator) machine translation (MT) system based on
it. The system translates general domain English sentences into grammat-
ical Chinese sentences in a fully automatic manner. It is written in Prolog
and implemented on the DEC-10, the GEC, and the SUN workstation.
SDCG is an augmentation of the DCG (Definite Clause Grammar;
Pereira et al. 1980) which in turn is based on CFG (Context Free Gram-
mar). Implemented in Prolog, the SDCG is highly suitable for NLP in
general, and MT in particular.
A wide range of linguistic phenomena is covered by the XTRA system,
including multiple word senses, coordinate constructions, and prepositional
phrase attachment, among others.
The case against fully automatic high quality machine translation
(FAHQMT) has been well-canvassed in the literature ever since ALPAC.
Although considerable progress in computational linguistics has been made
since then, many of the major arguments against FAHQMT still hold (a
good resume is given by Martin Kay (1980)).
It is not our intention to reopen the case for FAHQMT here. Rather, we
contend that, accepting that FAHQMT is not possible in the current state of
the art, it is both feasible and desirable to set up R &amp; D programmes in MT
which can both produce results which will satisfy sponsors and provide an
environment to support research directed towards bringing MT closer to
the ultimate goal of FAHQMT.
This paper describes the rationale and organisation behind one such
programme, the UMIST English-Japanese MT project.
This paper address three questions:
— What is sublanguage?
— Why is sublanguage analysis important for automatic translation?
— How can a translation system take advantage of sublanguage properties?
The first of these questions appears to have a simple answer. Natural
languages clearly have specialized varieties which are used in reference to
restricted subject matter. We speak, for example, of the &amp;quot;language of
chemistry&amp;quot; to mean a loosely defined set of sentences or texts dealing with
a particular part of reality.
But when we consider the automatic translation of specialized language,
we are forced to be more precise. We must describe sublanguages as
coherent, rule-based systems. The attempt to write grammars for special-
purpose sublanguages raises a number of theoretical and practical prob-
lems, which are only now being intensively discussed. But since the only
path to hiqh-quality automatic translation seems to lie through sublanguage
(at least during the next decade or two), we have no choice but to solve
these problems. This paper should therefore be considered as a brief
summary and progress report.
Well-known examples such as Bar-Hillel&apos;s (1960) &amp;quot;The box is in the pen&amp;quot;
illustrate that extensive semantic analysis is necessary to resolve ambigui-
ties that must be resolved in machine translation. If one accepts the prem-
ise that semantics should be added to the analysis techniques used in
machine translation, what is the way in which it should be added? This
paper will argue for an integrated approach to semantic processing. That is,
syntactic and semantic processing should take place at the same time, rath-
er than in separate stages. However, although I will argue for the inte-
</bodyText>
<page confidence="0.961398">
256 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<figure confidence="0.851716172413793">
The FINITE STRING Abstracts of Current Literature
LMT: A PROLOG-Based Machine
Translation System
Michael C. McCord
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
pp. 179-182
Recovering the Speaker&apos;s Decisions during
Mechanical Translation
David D. McDonald
University of Massachusetts at Amherst,
01003
pp. 183-199
Structural Transformation in the Gener-
ation Stage of MU Japanese to English
Machine Translation System
Makoto Nagao
Department of Electrical Engineering
Kyoto University
Sakyo, Kyoto, JAPAN
pp. 200-223
Interlingua Design for TRANSLATOR
Sergei Nirenburg, Allen B. Tucker
Department of Computer Science
Colgate University
Victor Raskin
Department of English
Purdue University
pp. 224-244
</figure>
<bodyText confidence="0.999772310344827">
gration of syntactic and semantic analysis processes, I will also argue for the
use of a separate body of syntactic knowledge, and for building a separate
syntactic representation during the parsing process. This is in contrast to
previous integrated parsers, which have relied almost exclusively on seman-
tic representations to guide the parsing process, and which have not used a
separate body of syntactic rules.
The talk will describe a machine translation system, LMT, based in
PROLOG, translating from English to German. The effort on LMT per se
has just begun this year, although the logic programming methodology for
the analysis of the source (English) goes back several years (see, e.g.,
McCord 1982).
When studied as a source of insight into the human language faculty, rath-
er than to construct a commercially useful service, mechanical translation
(MT) is carried out by coupling an otherwise normal natural language pars-
ing system to a normal natural language generation system. In this paper
we propose that a crucial capability has been omitted from the design of
the parsers that have been used to date, namely a facility for recognizing
the information that is implicit in the form of any well written text: matters
of emphasis, whether a fact is new or old, whether a relationship is given
explicitly or left as an obvious inference, signals of intended moves in the
discourse, and other things of this sort. We claim that mechanical trans-
lations are &amp;quot;mechanical&amp;quot; principally because they pay no attention to
information of this sort, and propose that this can be dealt with by incor-
porating into the parser knowledge of the relationship between usage and
form of the sort that is commonplace in any modern language generation
system.
No abstract.
The interlingua approach to machine translation (MT) is characterized by
the following two stages:
</bodyText>
<listItem confidence="0.99477875">
1) translation of the source text into an intermediate representation, an
artificial language (interlingua) which is designed to capture the various
types of meaning of the source text, and
2) translation from the interlingua into the target text.
</listItem>
<bodyText confidence="0.998077733333333">
Over the years a number of MT projects tried to develop interlingua-based
systems. In these projects the amount of linguistic and encyclopaedic
knowledge used to produce intermediate representations was quite limited.
However, even at that level difficulties connected with encoding know-
ledge seemed overwhelming. The TRANSLATOR project at Colgate
University benefits from recent developments in knowledge representation
techniques. The text of its interlingua text reflects syntactic, lexical,
contextual, discourse (including speech situation), and pragmatic meaning
of the input. This paper discusses the lexicon and grammar of the interlin-
gua used in TRANSLATOR, and touches upon the structure of the bilingual
(source language to interlingua) dictionaries. The actual compilation of the
interlingua dictionary and additional knowledge bases is an empirical proc-
ess during which modifications to the original formulations are expected to
occur. At all times in the design process the authors were guided by the
desire to make decisions that are &apos;literate&amp;quot; from the point of view of
</bodyText>
<figure confidence="0.910535541666667">
Computational Linguistics, Volume 11, Number 4, October-December 1985 257
The FINITE STRING Abstracts of Current Literature
The Level Hypothesis in Discourse
Analysis
James Pustejovsky
Department of Computer and Informa-
tion Sciences
University of Massachusetts at Amherst,
01003
pp. 245-267
Linguistics and Natural Language
Processing
Victor Raskin
Purdue University
pp. 268-282
Static Grammars: A Formalism for the
Description of Linguistic Models
Bernard Vauquois, Sylviane Chappuy
Groupe d&apos;Etudes pour la Traduction
Automatique
Universite Scientifique et Medicale de
Grenoble
BP 68 - 38402 Saint Martin d&apos;Heres,
FRANCE
</figure>
<bodyText confidence="0.977307711864407">
&amp; Institut de Formation et Conseil en
Informatique
27, rue de Turenne - 3800 Grenoble
FRANCE
pp. 298-322
linguistic theory and the experience of knowledge representation in artifi-
cial intelligence.
In this paper I would like to explore some difficult questions related to
topics in discourse analysis (henceforth DA) and offer a partial solution to
some of them. In particular, I will address the issue of levels in DA and
how the various approaches taken within the field can be classified accord-
ing to a leveled model. I then want to consider an approach I have been
pursuing for representing the semantics of discourse, and consider how it
fits in to the proposed model for DA.
The paper addresses the issue of cooperation between linguistics and
natural language processing (NLP), in general, and between linguistics and
machine translation (MT), in particular. It focuses on just one direction of
such cooperation, namely applications of linguistics to NLP, virtually ignor-
ing for now any possible applications of NLP to linguistics, which can range
from providing computer-based research tools and aids to linguistics to
implementing formal linguistic theories and verifying linguistic models.
Section 1 deals with the question why linguistics must be applied to NLP
and what the consequences of ignoring it are. Section 2 provides a coun-
terpoint by discussing how linguistics should not be applied to NLP and, by
contrast and inference, how it should be. Section 3 narrows the discussion
down to one promising approach to NLP, the sublanguage deal, and the
interesting ways in which linguistics can be utilized within a limited sublan-
guage. Section 4 is devoted specifically — but necessarily briefly — to the
things linguistics can contribute to MT.
The work described here was the consequence of the idea that we wanted
to make a new, more interesting theoretical start in EUROTRA. It is
preliminary and not fully developed yet; it should be seen as the reflection
of a way of thinking about MT. Currently, we are making it more precise,
and experimenting with it. In this paper, we sketch the general outlines of
the new EUROTRA framework; some more exemplification can be found
in the paper by D.J. Arnold et al. (p. 1 of this volume).
Most existing practical machine translation systems are designed to trans-
late documentation, such as technical papers and manuals. However, there
is a growing need for translating not only large texts but also personal
short texts such as letters and informal messages. The conventional
machine translation systems, which are intended to translate large texts,
are not very suitable for these kinds of small jobs. We need an interactive
system which has a totally different design philosophy. This paper
describes the design philosophy of personal/interactive machine trans-
lation systems, and studies in feasibility.
For a linguistic model it is necessary, first of all, to define the mapping
between the strings of words of a language and their structural organisa-
tion, given that with transducers there are many ways of obtaining the
same result using different strategies.
This mapping, which we will call a &amp;quot;static grammar&amp;quot;, is independent of
the analysis, generation, or whatever strategy adopted. Moreover, the
formalism of a static grammar is not affected by the choice or number of
interpretation levels.
Such a grammar is the &amp;quot;reference&amp;quot; for any dynamic modular rule orga-
nisation, whether analysis or generation.
We present here a &amp;quot;static grammar&amp;quot; formalism recently developed at
Grenoble (G.E.T.A. Groupe d&apos;Etude pour la Traduction Automatique)
under the supervision of Prof. B. Vauquois.
Using this formalism, any given language can be described as a series
</bodyText>
<figure confidence="0.992543954545455">
A Preliminary Linguistic Framework for
Eurotra, June 1985
Louis des Tombe, Doug Arnold, Lieven
Jaspaert, Rod Johnson, Steven Krauurr,
Mike Rosner, Nina Varile, Susan Warwick
pp. 283-288
Feasibility Study of Personal/Interactive
Machine Translation Systems
Masora Tomita
Computer Science Department
Carnegie-Mellon University
Pittsburgh, PA 15213
pp. 289-297
258 Computational Linguistics, Volume 11, Number 4, October-December 1985
The FINITE STRING Abstracts of Current Literature
On Debugging Environment Proposed for
Eurotra
N. Verastegui
Institut de Formation et Conseil en
Informatique
27, rue de Turenne - 3800 Grenoble
FRANCE
pp. 323-334
Knowledge Resource Tools for Accessing
Large Text Files
Donald E. Walker
Artificial Intelligence and Information
Science Research
Bell Communications Research
435 South Street MRE 2A379
Morristown, NJ 07960
pp. 335-347
Reflections on the Knowledge Needed to
Process Ill-Formed Language
Ralph M. Weischedel, Lance A. Ran:show
Bolt Beranek and Newman Inc.
10 Moulton Street
Cambridge, MA 02238
pp. 348-358
Characteristics of the METAL Machine
Translation System at Production Stage
John S. White
Siemens Communication Systems
pp. 359-369
</figure>
<bodyText confidence="0.987927662337662">
of &amp;quot;charts&amp;quot;. Each &amp;quot;chart&amp;quot; describes how a certain group of strings corre-
sponds to the structure associated with this group of strings (this structure
is a valid and complete substructure of the linguistic model). The struc-
tures of all the sentences of a language for a given linguistic model can be
described by means of a series of chart inter-references.
The static grammar is used as a base for writing dynamic analysis and
generation modules, however, the static grammar does not concern itself
with strategic, combinatorial, ambiguity problems or the chart of structures
related to dynamic grammars.
We will present here several examples of charts and discuss the dynamic
uses of these static grammars.
A proposal of external specification of the user environment for the
EUROTRA project is presented. The needs of the users and the functions
which are necessary for an efficient testing environment are analyzed.
This paper provides an overview of a research program just being defined
at Bellcore. The objective is to develop facilities for working with large
document collections that provide more refined access to the information
contained in the &amp;quot;source&amp;quot; materials than is possible through current infor-
mation retrieval procedures. The tools being used for this purpose are
machine-readable dictionaries, encyclopedias, and related &amp;quot;resources&amp;quot; that
provide geographical, biographical, and other kinds of specialized know-
ledge. A major feature of the research program is the exploitation of the
reciprocal relationships between sources and resources. These interactions
between texts and tools are intended to support experts who organize and
use information in a workstation environment. Two systems under devel-
opment will be described to illustrate the approach: one providing capabili-
ties for full-text subject assessment; the other for concept elaboration
while reading text. Progress in the research depends critically on develop-
ments in artificial intelligence, computational linguistics, and information
science to provide a scientific base, and on software engineering, database
management, and distributed systems to provide the technology.
This paper reflects about the kinds of morphological syntactic, semantic,
and pragmatic knowledge needed to process ill-formed input. We conclude
that an excellent start on processing ill-formed input has been exemplified
in a number of concrete implementations, but that a substantial amount of
fundamental work must still be done if our systems are to understand
language robustly to the degree that humans do. Furthermore, we
conclude that studying ill-formed language offers important perspectives
on the knowledge and architecture needed to correctly understand natural
languages.
The METAL machine translation system, a joint project of the Linguistic
Research Center and Siemens, has been released for use as part of marketed
translation systems. The system, which presently translates technical
German into English, is an outgrowth of a traditional, generative approach
to automatic analysis and synthesis of natural language phenomena carried
out at the Linguistics Research Center for many years. In its present
manifestation, it is a modular design consisting of purely monolingual lexi-
cons, transfer lexicons, and an augmented phrase structure grammar. The
grammar is powerful enough to constrain application, to build new nodes
with essential characteristics of their sons and new synthetic information as
well, and to perform transformations to re-order, delete, and create
Computational Linguistics, Volume II, Number 4, October-December 1985 259
The FINITE STRING Abstracts of Current Literature
Relevance, Points of View and Dialogue constituents. The parser is enhanced to allow application of rules in levels,
Modelling and eliminating unlikely paths via preferential weightings calculated from
Yorick Wilks lexical and grammatical data. The METAL system, conceived in recent
Computing Research Laboratory years as destined for implementation, has an orientation to user interface
New Mexico State University which includes sophisticated text stripping, unfound word handling and
Las Cruces, NM 88003 reconstitution, and a convenient means of working with the lexicons inter-
pp. 370-387 actively.
This paper attempts to compare two approaches to the modelling of human
discourse and, more particularly, dialogue. Both place themselves within a
general &amp;quot;information processing paradigm&amp;quot;, and both descend from the
insights of Once (1975) that understanding is a matter of inference from
what is said and what is assumed. So general is that assumption now, and
so widespread are the disciplines that draw upon it — philosophy, psycholo-
gy, linguistics, and artificial intelligence — that it is hard to capture briefly
except in opposition to the transformational-generative paradigm of
language, with its notions of the primacy and autonomy of syntax, and the
theoretical primacy of explications of competence over those of perform-
ance. The Generative Semanticists attempted to merge the two traditions
and their failure has made it easier to separate off and clarify the work
under discussion here.
The following abstracts are from the Proceedings of the Ninth International Joint Conference on Artificial Intelligence
(IICAI85), Los Angeles, California, August 1985. Two volumes; edited by Aravind Joshi. [ISBN 0-934613-02-8,
$40.00 for AAAI members; $55.00 for non-members. Morgan Kaufmann Publishers, Inc.; 95 First Street; Los Altos,
CA 94022 (415 941-4960)1
</bodyText>
<figure confidence="0.994203615384615">
Computational Neurolinguistics — What Is
It All About?
Helen M. Gigley
Department of Computer Science
University of New Hampshire
pp. 260-266
A Short Note on Opportunistic Planning
and Memory in Arguments
Lawrence Birnbaum
Department of Computer Science
Yale University
New Haven, CT 06520
pp. 281-283
</figure>
<bodyText confidence="0.990517566666667">
Computational Neurolinguistics (CN) integrates artificial intelligence (Al)
methods with concepts of neurally motivated processing to develop cogni-
tive models of natural language processing.
HOPE is one example of a model developed to address issues in CN.
The model is parallel, and exemplifies language as the result of time
synchronized processes which are asynchronous in nature. Furthermore,
the model is substantially validated to include normal behavioral evidence
in its design. In addition, it attends to aspects of language breakdown
which are well documented in the literature of neurolinguistics or aphasia.
This paper discusses assumptions which underlie the CN approach to
model development. It will describe the neurally motivated or &amp;quot;natural
computational&amp;quot; processes which produce the model&apos;s observable and veri-
fiable behavioral results. The differences in the CN approach to other
models of parallel memory process and behavior will be presented. Finally,
the contribution of the CN research approach as a tool for investigating the
breakdown of language performance and its potential contribution to
understanding brain function will be discussed.
Engaging in an argument is a complex task of natural language processing
that involves understanding an opponent&apos;s utterances, discovering what his
&amp;quot;point&amp;quot; is, determining whether his claims are believable, and fashioning a
coherent rebuttal. Accomplishing these tasks requires the coordination of
many different abilities and many different kinds of knowledge. Because
arguing, and conversation generally, involve real-time interaction with
another agent, this coordination must be even more flexible than is
required for other natural language processing tasks. An arguer must have
some expectations about what his opponents might say, but must also be
able to respond to the unexpected. He must have some idea of the claims
he wants to make, and plans for putting them forward, but his opponent
may confound these plans. Or, more positively, his opponent may say
something that offers an unforeseen opportunity to make a point. Arguing
</bodyText>
<page confidence="0.589137">
260 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<table confidence="0.942985142857143">
The FINITE STRING Abstracts of Current Literature
A Process Model of Case-Based
Reasoning in Problem Solving
Janet L Kolodner, Robert L. Simpson, Jr.,
Katia Sycara-Cyranski
School of Information and Computer
Science
Georgia Institute of Technology
Atlanta, GA 30332
pp. 289-290
Learning to Understand Contractual
Situations
Seth R. Goldman, Michael G. Dyer,
Margot Flowers
Artificial Intelligence Laboratory
Computer Science Department
University of California
Los Angeles, CA 90024
pp. 291-293
A Representation for Complex Physical
Domains
</table>
<bodyText confidence="0.999895888888889">
thus exemplifies the need for a flexible mix of top-down and bottom-up
processing in both language understanding and production.
This paper is concerned with the roles of memory processing and plan-
ning in the processes of understanding and generating utterances in an
argument or conversation. In particular, I will show that the memory and
inferential processing necessary in order to understand another person&apos;s
utterances can and should perform much of the work required to generate
a response, work that most previous theories of conversation would dele-
gate to explicit, goal-directed planning. The consequences of this, both for
memory processing and for planning, will be briefly described and
analyzed.
Much of the problem solving done by both novices and experts uses &amp;quot;case-
based&amp;quot; reasoning, or reasoning by analogy to previous similar cases. We
explore the ways in which case-based reasoning can help in problem solv-
ing. According to our model, transfer of knowledge between cases is guid-
ed largely by the problem solving process itself. Our model shows the
interactions between problem solving processes and memory for exper-
ience. Our computer program, called the MEDIATOR, illustrates case-
based reasoning in interpreting and resolving common sense disputes.
In the field of law, decisions in previous cases often play a significant role
in the presentation and outcome of new cases. Lawyers are constantly
recalling old cases to aid them in preparing their own briefs. How do
lawyers remember cases? What are the features they use to organize and
retrieve past decisions? How do lawyers learn which features are impor-
tant? To address these questions we are constructing a model of legal
novices (i.e., first year law students) and the processes by which they learn
contract law. Our model is embodied in a computer program called STARE
(from the latin, stare decisis, which refers to the principle of using past
cases to decide current disputes). STARE will read descriptions of contrac-
tual situations and attempt to predict the decision based on its general
commonsense knowledge of agreements, the previous cases stored in an
episodic memory, and knowledge of some basic legal concepts.
This paper presents a framework for a theory of granularity which is seen
as a means of constructing simple theories out of more complex ones. A
transitive indistinguishability relation can be defined by means of a set of
relevant predicates, allowing simplification of a theory of complex
phenomena into computationally tractable local theories, or granularities.
Nontransitive indistinguishability relations can be characterized in terms of
relevant partial predicates, and idealization allows simplification into trac-
table local theories. Various local theories must be linked with each other
by means of articulation axioms, to allow shifts of perspective. Such a
treatment of granularity must be built into the very foundations of the
reasoning processes of intelligent agents in a complex world.
Ways in which physical objects interact are explored, and in particular the
concept of freedom is analyzed. Intuitively, the fit between two shapes in a
given spatial configuration is a statement about how much one shape needs
to be mutilated in order to be made identical to the other. The freedom of
one object with respect to another specifies what motions the first object
can go through without the second one moving. The formulations, termed
naive kinematics, are compared to work that was done in the kinematics of
machinery in the 19th century and that has since been somewhat
neglected.
We are exploring a system, called PROMPT, that will be capable of rea-
soning from first principles and high level knowledge in complex, physical
</bodyText>
<table confidence="0.933067754098361">
Granularity
Jerry R. Hobbs
Artificial Intelligence Center
SRI International
Menlo Park, CA 94025
&amp; Center for the Study of Language
and Information
Stanford University
Stanford, CA 94305
pp. 432-435
Naive Kinematics: One Aspect of Shape
Yoav Shoham
Computer Science Department
Yale University
New Haven, CT 06520
pp. 436-442
Computational Linguistics, Volume 11, Number 4, October-December 1985 261
The FINITE STRING Abstracts of Current Literature
Sanjaya Addanki
IBM T.J. Watson Research Center
Ernest Davis
New York University
pp. 443-446
ONYX: An Architecture for Planning in
Uncertain Environments
Curtis Langlotz, Lawrence Fagan,
Samson Tu, John Williams
Medical Computer Science Group
Branimir Sikic
Department of Medicine
Knowledge Systems Laboratory
Stanford University
Stanford, CA 94305
pp. 447-449
Understanding Behavior Using
Consolidation
Tom By!ander, B. Chandrasekaran
Laboratory for Artificial Intelligence
Research
Department of Computer and Informa-
tion Science
The Ohio State University
Columbus, OH 43210
pp. 450-454
A Decidable First-Order Logic for
Knowledge Representation
Peter F. Patel-Schneider
Schlumberger Palo Alto Research
3340 Hillview Avenue
Palo Alto, CA 94304
pp. 455-458
Two Results on Default Logic
Witold Lukaszewicz
Institute of Informatics
University of Warsaw
P.O. Box 1210
00-901 Warszawa, Poland
pp. 459-461
On the Descriptional Complexity of
Production Systems
Peter Trum
</table>
<tableCaption confidence="0.130142">
Battelle-Institute e.V.
</tableCaption>
<bodyText confidence="0.999758333333333">
domains. Such problem solving calls for a representation that will support
the different analyses techniques required (e.g., differential, asymptotic,
perturbation, etc.). Efficiency considerations requires that the repre-
sentation also support heuristic control of reasoning techniques. This paper
lays the ground work for our effort by briefly describing the ontology and
the representation scheme of PROMPT. Our ontology allows reasoning
about multiple pasts and different happenings in the same space-time. The
ontology provides important distinctions between materials, objects, bulk
and distributed abstractions among physical entities. We organise world
knowledge into &amp;quot;prototypes&amp;quot; that are used to focus the reasoning process.
Problem solving involves reasoning with and modifying prototypes.
The ONYX program is designed to fill the need for planning in application
areas where traditional planning methodology is difficult to apply. While
the program being developed will assist with the planning of cancer thera-
py, its architecture is intended to be of use whenever goals are ill-specified,
plan operators have uncertain effects, or trade-offs and unresolvable
conflicts occur between goals. We describe a planning process which uses
strategic information and a mechanistic model of the domain. The process
consists of three steps: (1) generate a small set of plausible plans based on
current data, (2) simulate those plans to predict their possible conse-
quences, and (3) based on the results of those simulations, rank the plans
according to how well each meets the goals for the situation.
In this paper we wish to make three contributions to Naive Physics in the
context of reasoning about devices. (1) We discuss some limitations of
current qualitative simulation approaches with regard to a number of issues
in understanding device behavior and point to the need for additional proc-
esses, (2) We introduce a new approach to deriving the behavior of
devices called consolidation. In this approach, the behavior of a device is
derived from the behavior of its components by inferring the behavior of
selected substructures of the device. (3) We present an ontology of behav-
ior and structure which is well-suited to the consolidation process. This
ontology makes it possible to state rules of behavior composition, i.e.,
simple patterns of behavior and structure are used to infer additional
behaviors.
Even though logic has played an important role in knowledge represen-
tation (KR) research, there has been little effort expended on devising decid-
able logics for KR. Most modifications to logic suggested for KR are either
extensions to first-order logic (e.g., to handle non-monotonicity) or ad hoc
changes in its inference mechanism. This paper presents a variant of first-
order relevance logic that has a decidable algorithm for determining tau-
tological entailment. Although this logic is considerably weaker than
standard first-order logic, it can be used effectively in a KR system when
semantically correct answers to queries are required within a finite amount
of time.
We focus on default logic, a formalism introduced be Reiter to model
default reasoning. The paper consists of two parts. In the first one a
translation method of non-normal defaults into the normal ones is given.
Although not generally valid, this translation seems to work for a wide
class of defaults. In the second part a semantics for normal default
theories is given and the completeness theorem is proved.
In this paper we formalize different methods for describing control know-
ledge in production systems by the concept of production system schemes.
In this framework these methods are compared from the viewpoint of
descriptional complexity, giving us more insight by which means problems
</bodyText>
<page confidence="0.897352">
262 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<figure confidence="0.84727527027027">
The FINITE STRING Abstracts of Current Literature
Am Roemerhof 35
D-6000 Frankfurt am Main
West Germany
pp. 462-464
Evidential Reasoning in Semantic
Networks: A Formal Theory
Lokendra Shastri, Jerome A. Feldman
Computer Science Department
University of Rochester
Rochester, NY 14627
pp. 465-474
An Endorsement-Based Plan Recognition
Program
Michael Sullivan, Paul R. Cohen
Department of Computer and Informa-
tion Science
University of Massachusetts
Amherst, MA 01003
pp. 475-479
A Guide to the Modal Logics of Know-
ledge and Belief: Preliminary Draft
John Y. Halpern
IBM Research Laboratory
San Jose, CA 95193
Yoram Moses
Computer Science Department
Stanford University
Stanford, CA 94305
&amp; IBM Research Lab, San Jose
pp. 480-490
Belief, Awareness, and Limited Reasoning:
Preliminary Report
Ronald Fagin, Joseph Y. Halpern
IBM Research Laboratory
San Jose CA 95193
pp. 491-501
</figure>
<bodyText confidence="0.998651634615385">
can be described in an easy and succinct way.
This paper presents an evidential approach to knowledge representation
and inference wherein the principle of maximum entropy is applied to deal
with uncertainty and incompleteness. It focuses on a restricted represen-
tation language — similar in expressive power to semantic network formal-
isms, and develops a formal theory of evidential inheritance within this
language. The theory applies to a limited, but we think interesting, class of
inheritance problems including those that involve exceptions and multiple
inheritance hierarchies. The language and the accompanying evidential
inference structure provide a natural treatment of defaults and conflicting
information. The evidence combination rule proposed in this paper is
incremental, commutative and associative and hence shares most of the
attractive features of the Dempster-Shafer evidence combination rule.
Furthermore, it is demonstrably better than the Dempster-Shafer rule in
the context of the problems addressed in this paper. The resulting theory
can be implemented as a highly parallel (connectionist) network made up
of active elements that can solve inheritance problems in time proportional
to the depth of the conceptual hierarchy.
This paper describes a plan-recognition program built to explore the theory
of endorsements (Cohen 1983). The program evaluates alternative inter-
pretations of user actions and reasons about which are the most likely
explanation of the user&apos;s intentions. Uncertainty about the various alterna-
tives was encoded in data structures called endorsements. The paper
describes the workings of this program and the successes and limitations of
the endorsement-based approach.
We review and re-examine possible-words semantics for propositional logic
of knowledge and belief with four particular points of emphasis: (1) we
show how general techniques for finding decision procedures and complete
axiomatizations apply to models for knowledge and belief, (2) we show
how sensitive the difficulty of the decision procedure is to such issues as
the choice of modal operators and the axiom system, (3) we discuss how
notions of common knowledge and implicit knowledge among a group of
agents fit into the possible-worlds framework, and (4) we consider to what
extent the possible-worlds approach is a viable one for modelling know-
ledge and belief. As far as complexity is concerned, we show among other
results that while the problem of deciding satisfiability of an S5 formula
with one knower is NP-complete, the problem for many knowers is
PSPACE-complete. Adding an implicit knowledge operator does not
change the complexity substantially, but once a common knowledge opera-
tor is added to the language, the problem becomes complete for exponen-
tial time.
Several new logics for belief and knowledge are introduced and studied, all
of which have the property that agents are not logically omniscient. In
particular, in these logics, the set of beliefs of an agent does not necessarily
contain all valid formulas. Thus, these logics are more suitable than tradi-
tional logics for modelling beliefs of humans (or machines) with limited
reasoning capabilities. Our first logic is essentially an extension of
Levesque&apos;s logic of implicit and explicit belief, where we intend to allow
multiple agents and higher-level belief (i.e., beliefs about beliefs). Our
second logic deals explicitly with &amp;quot;awareness&amp;quot;, where, roughly speaking, it
is necessary to be aware of a concept before one can have beliefs about it.
Our third logic gives a model of &amp;quot;local reasoning&amp;quot;, where an agent is
</bodyText>
<figure confidence="0.772852333333333">
Computational Linguistics, Volume 11, Number 4, October-December 1985 263
The FINITE STRING Abstracts of Current Literature
Event Calculus
Gary C. Borchardt
Coordinated Science Laboratory
University of Illinois
Urbana, IL 61801
pp. 524-527
A Common-Sense Theory of Time
</figure>
<bodyText confidence="0.99938828">
viewed as a &amp;quot;society of minds&amp;quot;, each with its own cluster of beliefs, which
may contradict each other.
Introspection is a general term covering the ability of an agent to reflect
upon the workings of his own cognitive functions. In this paper we will be
concerned with developing an explanatory theory of a particular type of
introspection: a robot agent&apos;s knowledge of his own beliefs. The develop-
ment is both descriptive, in the sense of being able to capture introspective
behavior as it exists; and prescriptive, in yielding an effective means of
adding introspective reasoning abilities to robot agents.
We present a semantic model for knowledge with the following properties:
(1) Knowledge is necessarily correct, (2) agents are logically omniscient,
i.e., they know all the consequences of their knowledge, and (3) agents are
positively introspective, i.e., they are aware of their knowledge, but not
negatively introspective, i.e., they may not be aware of their ignorance.
We argue that this is the appropriate model for implicit knowledge. We
investigate the properties of the model, and use it to formalize the notion
of circumscribed knowledge.
A representation scheme for arbitrary beliefs and wants of an agent in
respect to a situation, as well as to arbitrary beliefs and wants of other
agents, is presented. The representation makes use of elementary situation
descriptions (which are formulated in KL-ONE and delimited by parti-
tions), and acceptance attitudes in respect to these descriptions, or to the
attitudes thereabout. The scheme forms the representational base of
VIE-DPM, the user modelling component of the German-language dialogue
system VIE-LANG.
Much of our commonsense knowledge about the real world is concerned
with the way things are done. This knowledge is often in the form of
procedures or sequences of actions for achieving particular goals. In this
paper, a formalism is presented for representing such knowledge based on
the action of process. A declarative semantics for the representation is
given, which allows a user to state facts about the effect of doing things in
the problem domain of interest. An operational semantics is also provided,
which shows how this knowledge can be used to achieve given goals or to
form intentions regarding their achievement. The formalism also serves as
an executable program specification language suitable for constructing
complex systems.
This paper presents Event Calculus, a model for representing the identify-
ing characteristics of physical events in terms of changes in a scene and
time-related combinations of other physical events. The model is used to
construct a knowledge-based system for event recognition which forms a
high-level description of changes in a scene, given a low-level description
as input.
Time-varying information is represented in the form of &amp;quot;GRAPHs&amp;quot;, data
structures which plot the elements of various domains against time.
Several varieties of operations are presented which map GRAPHs into
GRAPHs, and representations of physical events are formed as symbolic
expressions involving these operations. The paper concludes with an over-
view of the event recognition system, as implemented in INTERLISP on a
VAX 11/780, and an example of a session with this system.
The literature on the nature and representation of time is full of disputes
</bodyText>
<figure confidence="0.900228948275862">
A Computational Theory of Belief
Introspection
Kurt Konolige
Artificial Intelligence Center
SRI International
Menlo Park, CA 94025
pp. 502-508
A Model-Theoretic Analysis of Monotonic
Knowledge
Moshe Y. Vardi
CSLI, Ventura Hall
Stanford University
Stanford, CA 94305
pp. 509-512
Using Situation Descriptions and Russellian
Attitudes for Representing Beliefs and
Wants
Alfred Kobsa
Austrian Research Institute for Artificial
Intelligence
Schottengasse 3
A-1010 Vienna, Austria
&amp; Sonderforschungsbereich
Department of Computer Science
Universitat des Saarlandes
D-66 Saarbrticken, W. Germany
pp. 513-515
A Procedural Logic
Michael P. Georgeff, Amy L. Lansky,
Pierre Bessiere
Artificial Intelligence Center
SRI International
Menlo Park, CA 94025
pp. 516-523
264 Computational Linguistics, Volume 11, Number 4, October-December 1985
The FINITE STRING Abstracts of Current Literature
James F. Allen, Patrick J. Haws
Departments of Computer Science and
Philosophy
University of Rochester
Rochester, NY 14627
pp. 528-531
An Essential Hybrid Reasoning System:
Knowledge and Symbol Level Ac-
counts of KRYPTON
Ronald J. Brachman
AT&amp;T Bell Laboratories
600 Mountain Avenue
Murray Hill, NJ 07974
Victoria Pigman Gilbert
Schlumberger Palo Alto Research
3340 Hillview Avenue
Palo Alto, CA 94304
Hector J. Levesque
Department of Computer Science
University of Toronto
Toronto, Ont., Canada M5S 1A7
pp. 532-539
</figure>
<subsectionHeader confidence="0.921039">
The Layered Architecture of a System for
Reasoning about Programs
Charles Rich
</subsectionHeader>
<bodyText confidence="0.827518">
The Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139
pp. 540-546
</bodyText>
<subsectionHeader confidence="0.9373175">
The Restricted Language Architecture of a
Hybrid Representation System
</subsectionHeader>
<figure confidence="0.6566728">
Marc Vilain
BBN Laboratories
10 Moulton Street
Cambridge, MA 02238
pp. 547-551
Proportionality Graphs, Units Analysis, and
Domain Constraints: Improving the Power
and Efficiency of the Scientific Discovery
Process
Brian Falkenhainer
</figure>
<affiliation confidence="0.9959885">
Department of Computer Science
University of Illinois
</affiliation>
<page confidence="0.796601">
1304 W. Springfield Avenue
</page>
<note confidence="0.27638">
Urbana, IL 61801
</note>
<footnote confidence="0.355269">
pp. 552-554
</footnote>
<bodyText confidence="0.999609509433963">
and contradictory theories. This is surprising since the nature of time does
not cause any worry for people in their everyday coping with the world.
What this suggests is that there is some form of common sense knowledge
about time that is rich enough to enable people to deal with the world, and
which is universal enough to enable cooperation and communication
between people. In this paper, we propose such a theory and defend it in
two ways. We axiomatize a theory of time in terms of intervals and the
single relation MEET. We then show that this axiomatization subsumes
Allen&apos;s interval-based theory. We then extend the theory by formally
defining the beginnings and endings of intervals and show that these have
the properties we normally would associate with points. We distinguish
between these point-like objects and the concept of moment as hypothe-
sized in discrete time models. Finally, we examine the theory in terms of
each of several different models.
Hybrid inference systems are an important way to address the fact that
intelligent systems have multifaceted representational and reasoning
competence. KRYPTON is an experimental prototype that completely
handles both terminological and assertional knowledge; these two kinds of
information are tightly linked by having sentences in an assertional compo-
nent be formed using structured complex predicates defined in a comple-
mentary terminological component. KRYPTON is unique in that it
combines in a completely integrated fashion a frame-based description
language and a first-order resolution theorem-prover. We give here both a
formal Knowledge Level view of the user interface to KRYPTON and the
technical Symbol Level details of the integration of the two disparate
components, thus providing an essential picture of the abstract function
that KRYPTON computes and the implementation technology needed to
make it work. We also illustrate the kind of complex question the system
can answer.
Cake is a hybrid system which provides reasoning facilities for the
Programmer&apos;s Apprentice. This paper describes the architecture of Cake,
which is divided into eight layers, each with associated representations and
reasoning procedures. The operation of Cake is illustrated by a complete
trace of the solution of an example reasoning problem. We also argue that
a hybrid system in general is characterized by the use of multiple repre-
sentations in the sense of multiple data abstractions, which does not nec-
essarily imply distinct implementation data structures.
Hybrid architectures have been used in several recent knowledge represen-
tation systems. This paper explores some distinctions between various
hybrid representation architectures, focusing in particular on systems built
around restricted representation languages. This restricted language archi-
tecture is illustrated by describing KL-TWO, a hybrid reasoner based on the
restricted representation facility RUP. The bulk of this paper discusses
KL-TWO, its subcomponents, and the techniques used to interface them.
An important subproblem of scientific discovery is quantitative discovery,
finding formulas that relate some set (or subset) of a collection of numer-
ical parameters. Current work in quantitative discovery suffers from a lack
of effiency and generality. This paper discusses methods that are efficient
and yet general for discovery equations which try to avoid exponential
search. Importantly, these methods can derive equations that cover subsets
of the data and derive explicit descriptions of when the equations are
applicable. These methods are fully implemented in a system named
ABACUS, which is described and some of its results are presented.
</bodyText>
<table confidence="0.957381125">
Computational Linguistics, Volume 11, Number 4, October-December 1985 265
The FINITE STRING Abstracts of Current Literature
A New Kind of Finite-State Automaton:
Register Vector Grammar
Glenn David Blank
Lehigh University
CSEE Department
Packard Lab 19
Bethlehem, PA 18015
PP. 749-755
An Efficient Context-Free Parsing
Algorithm for Natural Languages
Masan, Tomita
Computer Science Department
Carnegie-Mellon University
Pittsburgh, PA 15213
pp. 756-764
Unrestricted Gapping Granunars
Fred Popowich
Natural Language Group
Laboratory for Computer and Communi-
cations Research
Computing Science Department
Simon Fraser University
Burnaby, B.C., Canada V5A 1S6
pp. 765-768
Parsing with Assertion Sets and Informa-
tion Monotonicity
G. Edward Barton, Jr., Robert C. Berwick
MIT Artificial Intelligence Laboratory
545 Technology Square
Cambridge, MA 02139
</table>
<footnote confidence="0.64652">
pp. 769-771
</footnote>
<note confidence="0.5617585">
Weighted Interaction of Syntax and
Semantics in Natural Language Analysis
Leonard Lesmo, Pietro Torasso
Dipartimento di Informatica
</note>
<subsectionHeader confidence="0.334074">
Universita di Torino
</subsectionHeader>
<bodyText confidence="0.999561240740741">
Register Vector Grammar is a new kind of finite-state automaton that is
sensitive to context — without, of course, being context-sensitive in the
sense of Chomsky hierarchy. Traditional automata are functionally simple:
symbols match by identity and change by replacement. RVG is func-
tionally complex: ternary feature vectors (e.g., + — + +— + +) match and
change by masking (± matches but does not change any value). Func-
tional complexity — as opposed to the computational complexity of non-
finite memory — is well suited for modelling multiple and discontinuous
constraints. RVG is thus very good at handling the permutations and
dependencies of syntax (wh-questions are explored as an example).
Because center-embedding in natural languages is in fact very shallow and
constrained, context-free power is not needed. RVG can thus be guaran-
teed to run in a small linear time, because it is FS, and yet can capture
generations and constraints that functionally simple FS grammars cannot.
This paper introduces an efficient context-free parsing algorithm and
emphasizes its practical value in natural language processing. The algo-
rithm can be viewed as an extended LR parsing algorithm which embodies
the concept of a &amp;quot;graph-structured stack&amp;quot;. Unlike the standard LR,
the algorithm is capable of handling arbitrary non-cyclic context-free gram-
mars including ambiguous grammars, while most of the LR parsing effi-
ciency is preserved. The algorithm seems more efficient than any existing
algorithms including the Docke-Younger-Kasami algorithm and Earley&apos;s
algorithm, as far as practical natural language is concerned, due to utiliza-
tion of LR parsing tables. The algorithm is an all-path parsing algorithm; it
produces all possible parse trees (a parse forest) in an efficient represen-
tation called a &amp;quot;shared-packed-forest&amp;quot;. This paper also shows that
Earley&apos;s forest representation has a defect and his algorithm cannot be
used in natural language processing as an all-path parsing algorithm.
Since the introduction of metamorphosis grammars (MGs) (Colmerauer,
1978), with their associated type 0-like grammar rules, there has been a
desire to allow more general rule formats in logic grammars. Gaps, which
refer to strings of unspecified symbols, were added to the MG rule, result-
ing in extraposition grammars (XGs) (Pereira 1981) and gapping gram-
mars(GGs) (Dahl and Abramson 1984). Unrestricted gapping grammars,
which provide an even more general rule format, possess rules of the form
&amp;quot;a -* [3&amp;quot; where a and p may contain any number of terminal, non-
terminal, or gap symbols in any order. FIGG, a Flexible Implementation of
Gapping Grammars, is an implementation of a large subset of unrestricted
GGs which allows either bottom-up or top-down parsing of sentences.
This system provides more built-in control facilities than previous logic
grammar implementations, which allows the user to restrict the applicabil-
ity of the rules, and to create grammar rules that will be executed more
efficiently.
We propose a new approach to parsing ambiguity in which a parser always
moves forward with the common elements of competing syntactic analyses.
The approach involves assertion sets constrained so that information is
monotonically preserved throughout a parse. Assertion sets have several
advantages over trees as a parsing representation. They may also lead to
better computational understanding of the attention-shifting mechanism.
The present paper discusses the extensions to the parsing strategies
adopted for FIDO (a Flexible Interface for Database Operations). The
parser is able to deal with ill-formed inputs (syntactically ill-formed
sentences, fragments, conjunctions, etc.) because of the strict cooperation
among syntax and semantics. The syntactic knowledge is represented by
</bodyText>
<page confidence="0.928115">
266 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<table confidence="0.6270260625">
The FINITE STRING Abstracts of Current Literature
Via Valperga Caluso, 37
10125 Torino - Italy
pp. 772-778
Syntax, Preference and Right Attachment
Yorick Willis, Xiuming Huang, Dan Fass
Computing Research Laboratory
New Mexico State University
Las Cruces, NM 88003
pp. 779-784
Controlling Search in Flexible Parsing
Steven Minton, Philip J. Hayes, Jill Fain
Computer Science Department
Carnegie-Mellon University
Pittsburgh, PA 15213
pp. 785-787
Griumnatical Relations as the Basis for
Natural Language Parsing and Text
Understanding
Samual Raw, Leonard Joseph,
Condace Kalish
A045, Mitre Corporation
Bedford, MA 01730
pp. 788-790
The Role of Perspective in Responding to
Property Misconceptions
Kathleen F. McCoy
Department of Computer and Informa-
tion Science
University of Pennsylvania
Philadelphia, PA 19104
pp. 791-793
</table>
<title confidence="0.46627">
Tailoring Explanations for the User
</title>
<author confidence="0.694411">
Kathleen R. McKeown, Kevin Matthews
</author>
<affiliation confidence="0.738385">
Department of Computer Science
</affiliation>
<footnote confidence="0.8372102">
Columbia University
New York, NY 10027
Myron Wish
AT&amp;T Bell Laboratories
600 Mountain Avenue
</footnote>
<bodyText confidence="0.999946769230769">
means of packets of condition-action rules associated with syntactic cate-
gories. The non-determinism is mainly handled by means of rules which
restructure the parse tree (called &amp;quot;natural changes&amp;quot;) so that the use of
backtracking is strongly limited.
In order to deal with difficult cases in which no clear-cut mechanism
exists for excluding an interpretation, a weighting mechanism has been
added to the parser so that it is possible to explore a few different hypoth-
eses in parallel and to choose the best one on the basis of complex inter-
action among syntax and semantics.
The paper claims that the right attachment rules for phrases originally
suggested by Frazier and Fodor are wrong, and that none of the subse-
quent patchings of the rules by syntactic methods have improved the situ-
ation. For each rule there are perfectly straightforward and indefinitely
large classes of simple counterexamples. We then examine suggestions by
Ford et al., Schubert and Hirst which are quasi-semantic in nature and
which we consider ingenious but unsatisfactory. We offer a straightfor-
ward solution within the framework of preference semantics, and argue
that the principal issue is not the type and nature of information required
to get appropriate phrase attachments, but the issue of where to store the
information and with what processes to apply it. We present a Prolog
implementation of a best first algorithm covering the data and contrast it
with closely related ones, all of which are based on the preferences of
nouns and prepositions, as well as verbs.
Most natural language parsers require their input to be grammatical. This
significantly constrains the search space that they must explore during
parsing. Parsers which attempt to recover from extragrammatical input
contend with a search space that is potentially much larger, since they
cannot necessarily prune branches when grammatical expectations are
violated. In this paper we discuss the control structure of the experimental
MULTIPAR parser, which directs its search by exploring potential parses in
order of their degree of grammatical deviation.
The KING KONG parser described by this paper attempts to apply the
principles of relational grammar to the parsing of English in order to over-
come the problems encountered by syntactic and semantic parsers. Specif-
ically, this parser uses relational categories such as subject, direct object,
and instrument to map syntactic constituents onto semantic roles within
CD-like structures. Thus, the parser makes use of both syntactic and
semantic information to guide its parse.
In order to adequately respond to misconceptions involving an object&apos;s
properties, we must have a context-sensitive method for determining object
similarity. Such a method is introduced here. Some of the necessary
contextual information is captured by a new notion of object perspective. It
is shown how object perspective can be used to account for different
responses to a given misconception in different contexts.
In order for an expert system to provide the most effective explanations, it
should be able to tailor its responses to the concerns of the user. One way
in which explanations may be tailored is by point of view. A method is
presented for representing the knowledge to support different points of
view in the current domain. In addition, we present a method for deter-
mining the point of view to take by inferring the user&apos;s goal within a brief
discourse segment. The advising system&apos;s response to the derived goal
depends on the strength of its belief in the inference for which a method of
</bodyText>
<table confidence="0.914224555555555">
Computational Linguistics, Volume 11, Number 4, October-December 1985 267
The FINITE STRING Abstracts of Current Literature
Murray Hill, NJ 07974
pp. 794-798
Description-Directed Natural Language
Generation
David D. McDonald, James D. Pustejovsky
Department of Computer and Informa-
tion Science
University of Massachusetts at Amherst,
01003
pp. 799-805
Tense, Aspect and the Cognitive
Representation of Time
Kenneth Man-kam Yip
Artificial Intelligence Laboratory
Massachusetts Institute of Technology
545 Technology Square
Cambridge, MA 02139
pp. 806-814
Lexical Ambiguity as a Touchstone for
Theories of Language Analysis
Lawrence Birnbaum
Yale University
Department of Computer Science
New Haven, CT 06520
pp. 815-820
</table>
<bodyText confidence="0.999646545454546">
determination is also provided. This information enables the system to
decide what answer to give to a question, which kind of justification is
relevant, and when to provide it. Some details of the current implementa-
tion are included.
We report here on a significant new set of capabilities that we have incor-
porated into our language generation system MUMBLE. Their impact will
be to greatly simplify the work of any text planner that uses MUMBLE as
its linguistics component since MUMBLE can now take on many of the
planner&apos;s text organization and decision-making problems with markedly
less hand-tailoring of algorithms in either component. Briefly these new
capabilities are the following:
</bodyText>
<listItem confidence="0.641346">
(a) ATTACHMENT: A new processing stage within MUMBLE that allows
</listItem>
<bodyText confidence="0.994393886363636">
us to readily implement the conventions that go into defining a text&apos;s
intended prose style, e.g., whether the text should have complex
sentences or simple ones, compounds or embeddings, reduced or full
relative clauses, etc. Stylistic conventions are given as independently
stated rules that can be changed according to the situation.
(b) REALIZATION CLASSES are a mechanism for organizing both the
transformational and lexical choices for linguistically realizing a
conceptual object. The mechanism highlights the intentional criteria
which control selection decisions. These criteria effectively constitute
an &amp;quot;interlingua&amp;quot; between planner and linguistic component, describing
the rhetorical uses to which a text choice can be put while allowing its
linguistic details to be encapsulated.
The first part of our paper (sections 2 and 3) describes our general
approach to generation; the rest illustrates the new capabilities through
examples from the UMass COUNSELOR Project. This project is a large
new effort to develop a natural language discourse system based on the
HYPO system (Rissland &amp; Ashley 1984), which acts as a legal advisor
suggesting relevant dimensions and case references for arguing hypothet-
ical legal cases in trade-secret law. At various relevant points we briefly
contrast our work with that of Appelt, Danlois, Gabriel, Jacobs, Mann and
Mattheissen, and McKeown and Derr.
This paper explores the relationships between a computational theory of
temporal representation (as developed by James Allen) and a formal
linguistic theory of tense (as developed by Norbert Hornstein) and aspect.
It aims to provide explicit answers to four fundamental questions: (1) what
is the computational justification for the primitives of a linguistic theory;
(2) what is the computational explanation of the formal grammatical
constraints; (3) what are the processing constraints imposed on the learna-
bility and markedness of these theoretical constructs; and (4) what are the
constraints that a linguistic theory imposes on representations. We show
that one can effectively exploit the interface between the language faculty
and the cognitive facilities by using linguistic constraints to determine
restrictions on the cognitive representations and vice versa.
This paper assesses several broad approaches to language analysis with
respect to the problem of lexical ambiguity. The impact of the problem on
both syntactic and semantic analysis is discussed, and several common
methods for disambiguation, including the use of selectional restrictions
and scriptal lexicons, are analyzed. Their shortcomings illustrate the need
for complex inference to resolve ambiguity, which forms one of the key
functional arguments in favor of integrating language analysis with memo-
ry and inference. However, it has proven surprisingly difficult to realize
such an integrated approach in practice: An assessment of lexical disam-
biguation within some recent models which attempt to do so reveals that
they rely largely on the traditional techniques of selectional restrictions and
</bodyText>
<page confidence="0.925814">
268 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<note confidence="0.801656">
The FINITE STRING Abstracts of Current Literature
</note>
<bodyText confidence="0.999423166666667">
scriptal lexicons, with all their drawbacks. The difficulty is shown to stem
primarily from the theories of memory and inferential processing utilized.
The implications for recent approaches to language analysis based on
connectionist mechanisms are explored. Finally, the requirements imposed
by lexical disambiguation on theories of memory and inferential processing
are discussed.
</bodyText>
<figure confidence="0.695311071428571">
VOX — An Extensible Natural Language
Processor
Ammon Meyers
Artificial Intelligence Project
Computer Science Department
University of California
Irvine, California
pp. 821-825
Partial Constraints in Chinese Analysis
Yiming Yang, Shuji Doshita,
Toyoaki Nishida
Department of Information Science
Kyoto University
Sakyo-ku, Kyoto 606, Japan
pp. 826-828
Grammatical Functions, Discourse
Referents and Quantification
Uwe Reyle
Department of Linguistics
University of Stuttgart
West Germany
pp. 829-831
Discourse Structure and the Proper
Treatment of Interruptions
Barbara J. Grosz
Al Center, SRI International
Menlo Park, CA 94025
&amp; CSLI, Stanford University
Stanford, CA 94305
Candace L. Sidner
BBN Laboratories
10 Moulton Street
Cambridge, MA 02238
pp. 832-839
Evaluating Importance: A Step Towards
Text Summarization
Danilo Fum, Giovanni Guida, Carlo Tasso
Instituto di Matematica, Informatica e
Sisemistica
Universita di Udine
Udine, Italy
pp. 840-844
</figure>
<subsectionHeader confidence="0.927809">
Understanding Analogies in Editorials
</subsectionHeader>
<bodyText confidence="0.967864127659575">
VOX is a Natural Language Processor whose knowledge can be extended
by interaction with a user.
VOX consists of a text analyzer and an extensibility system that share a
knowledge base. The extensibility system lets the user add vocabulary,
concepts, phrases, events, and scenarios to the knowledge base. The
analyzer uses information obtained in this way to understand previously
unhandled text.
The underlying knowledge representation of VOX, called Conceptual
Grammar, has been developed to meet the severe requirements of extensi-
bility. Conceptual Grammar uniformly represents syntactic and semantic
information, and permits modular addition of knowledge.
In this paper we describe a method using semantic constraints to reduce
the ambiguities and generate case structure from phrase structure in
Chinese sentence analysis.
Semantic constraints written on semantic markers indicate the plausible
case structure. Different sets of semantic markers are chosen according to
the purpose. A priority evaluation scheme steers the analysis towards the
most plausible structure first, without trying all possibilities.
A new algorithm is proposed which transforms f-structures into discourse
representation structures (DRSs). Its primary features are that it works
bottom up, that it is capable of translating f-structures without pre-imposing
any arbitrary order on the attributes occurring in it, and that it handles
indeterminacy of scoping by using sets of translations. The approach sheds
light on how an efficient interaction of different components of a natural
language processing model can be achieved.
This paper reports on the development of a computational theory of
discourse. The theory is based on the thesis that discourse structure is a
composite of three structures: the structure of the sequence of utterances,
the structure of intentions conveyed, and the attentional state. The
distinction among these components is essential to provide adequate expla-
nations of such discourse phenomena as clue words, referring expressions
and interruptions. We illustrate the use of the theory for four types of
interruptions and discuss aspects of interruptions previously overlooked.
The paper deals with the problem of evaluating importance of descriptive
texts and proposes a procedural, rule-based approach which is imple-
mented in a prototype experimental system operating in the specific
domain of text summarization. Importance evaluation is performed
through a set of rules which are used to assign importance values to the
different parts of a text and to resolve or explain conflicting evaluations.
The system utilizes world knowledge on the subject domain contained in
an encyclopedia and takes into account a goal assigned by the user for
specifying the pragmatic aspects of the understanding activity. In the
paper some examples of the system operation are presented by following
the evaluation of a small sample text.
The widespread use of analogy in human communication underscores the
Computational Linguistics, Volume 11, Number 4, October-December 1985 269
The FINITE STRING Abstracts of Current Literature
</bodyText>
<reference confidence="0.859355777777778">
Stephanie E. August, Michael G. Dyer
Artificial Intelligence Laboratory
Computer Science Department, 3531 BH
University of California
Los Angeles, CA 90024
pp. 845-847
Integrating Text Planning and Production
in Generation
Eduard H. Hovy
Yale University
Artificial Intelligence Project
2158 Yale Station
New Haven, CT 06520
pp. 848-851
Be Brief, Be to the Point, ... Be Seated
or Relevant Responses in Man/Machine
Conversation
Anne Vilnat, Gerard Sabah
GR22, Paris VI
4, Place Jussieu
75230 Paris Cedex 05
pp. 852-854
SAPHIR+RESEDA, A New Approach to
Intelligent Data Base Access
Bernard Euzenat, Barnard Normier,
Antoine Ogonowski
ERLI
72 quai des Carrieres
94220 Charenton, France
Gian Piero Zarri
Centre National de la Recherche
Scientique, Paris
&amp; TECSIEL
via Barnaba Oriani 32
00197 Roma, Italy
pp. 855-857
RESEARCHER: An Experimental
Intelligent Information System
Michael Lebowitz
Department of Computer Science
Computer Science Building
Columbia University
New York, NY 10027
pp. 858-862
A Parallel-Process Model of On-Line
Inference Processing
Kurt P. Eisdt
Irvine Computational Intelligence Project
Computer Science Department
University of California
Irvine, CA 92717
pp. 863-869
New Approaches to Parsing Conjunctions
Using Prolog
</reference>
<bodyText confidence="0.999124392156863">
need for a system which can recognize and understand analogies. This
paper presents a theory of analogy recognition and comprehension, using
as a domain letters to the editor of a weekly news magazine. Some of the
issues facing a system which understands analogies in this domain are iden-
tified, initial work on this program is reviewed, and work in progress is
discussed.
While the task of language generation seems to separate quite naturally
into the two aspects of language generation (text planning and text
production), it is necessary to have the planning and the production inter-
act at generator decision points in such a way that the former need not
contain explicit syntactic knowledge, and that the latter need not contain
explicit goal-related information. This paper describes the decision points,
the types of plans that are used in making the decisions, and a process that
performs the task. These ideas are embodied in a program.
In the dialogue part of our system, we have tried to increase the user&apos;s
possibilities to criticize the machine&apos;s results and require explanations of
them. The system must then provide a clear justification: either by
furnishing the chain of reasoning or by asking a &amp;quot;good question&amp;quot; when it
failed. The system must also be able to engage in a real dialogue, with
more than one question/one answer. To do that, the system must build
and use several kinds of representation: the reasoning, the topics and a
model of the user, which is used to tailor the system&apos;s responses.
This paper describes a transportable natural language interface to data-
bases, augmented with a knowledge base and inference techniques. The
inference mechanism, based on a classical expert system&apos;s type of
approach, allows, when needed, to automatically convert an input query
into another one which is &amp;quot;semantically close&amp;quot;. According to RESEDA&apos;s
theory, &amp;quot;semantically close&amp;quot; means that the answer to the transformed
query implies what could have been the answer to the original question.
The presented system integrates natural language processing, expert
system and knowledge representation technology to provide a cooperative
database access.
The development of very powerful intelligent information systems requires
the use of many techniques best derived by studying human understanding
methods. RESEARCHER is a system that reads, remembers, generalizes
from, and answers questions about complex technical texts, patent
abstracts in particular. In this paper we discuss three current areas of
research involving RESEARCHER — the generalization of hierarchically
structured representations; the use of long-term memory in text processing,
specifically in resolving ambiguity; and the tailoring of answers to ques-
tions to the level of expertise of different users.
This paper presents a new model of on-line inference processes during text
understanding. The model, called ATLAST, integrates inference processing
at the lexical, syntactic, and pragmatic levels of understanding, and is
consistent with the results of controlled psychological experiments.
ATLAST interprets input text through the interaction of independent but
communicating inference processes running in parallel. The focus of this
paper is on the initial computer implementation of the ATLAST model, and
some observations and issues which arise from that implementation.
Conjunctions are particularly difficult to parse in traditional, phrase-based
grammars. This paper shows how a different representation, not based on
</bodyText>
<page confidence="0.858012">
270 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<figure confidence="0.887875">
The FINITE STRING Abstracts of Current Literature
Sandiway Fong, Robert C. Berwick
Artificial Intelligence Laboratory
MIT, 545 Technology Square
Cambridge, MA 02139
pp. 870-876
On the Use of a Taxonomy of Tune-
Frequency Morphologies for Automatic
Speech Recognition
Renato Be Mori, Mathew Palakal
Concordia University
Department of Computer Science
1455, de Maisonneuve Blvd.
Montreal, Quebec, Canada H3G 1M8
pp. 877-879
Reversible Automata and Induction of the
English Auxiliary System
Robert C. Berwick, Samuel F. Pilato
MIT Artificial Intelligence Laboratory
545 Technology Square
Cambridge, MA 02139
pp. 880-822
DP-Matching: With or Without
Phonemes?
Shigeyoshi Kitazawa
Shizuoka University
Hamamatsu-shi, 432, Japan
Masa-aki Ishikawa, Shuji Doshita
Kyoto University
Sayo-ku, Kyoto-shi, 606, Japan
pp. 883-885
</figure>
<bodyText confidence="0.998686632653061">
tree structures, markedly improves the parsing problem for conjunctions.
It modifies the union of phrase marker model proposed by Goodall (1984),
where conjunction is considered as the linearization of a three-dimensional
union of a non-tree based phrase marker representation. A PROLOG
grammar for conjunctions using this new approach is given. It is far
simpler and more transparent than a recent phrase-based extraposition
parser for conjunctions by Dahl and McCord (1984). Unlike the Dahl and
McCord or ATN SYSCONJ approach, no special trail machinery is needed
for conjunction, beyond that required for analyzing simple sentences.
While of comparable efficiency, the new approach unifies under a single
analysis a host of related constructions: respectively sentences, right node
raising, or gapping. Another advantage is that it is also completely revers-
ible (without cuts), and therefore can be used to generate sentences.
A computer vision approach based on skeletonization and hierarchical
description of speech patterns is proposed. Learning hierarchical
descriptions of phonetic events is discussed. Experimental results are
reported showing the power of the approach in the recognition of
diphthongs in connected letters and digits.
In this paper we apply some recent work of Angluin (1982) to the induc-
tion of the English auxiliary verb system. In general, the induction of
finite automata is computationally intractable. However, Angluin shows
that restricted finite automata, the k-reversible automata, can be learned
by efficient (polynomial time) algorithms We present an explicit computer
model demonstrating that the English auxiliary verb system can in fact be
learned as a 1-reversible automaton, and hence in a computationally feasi-
ble amount of time. The entire system can be acquired by looking at only
half the possible auxiliary verb sequences, and the pattern of generalization
seems compatible with what is known about human acquisition of auxilia-
ries. We conclude that certain linguistic subsystems may well be learnable
by inductive inference methods of this kind, and suggest and extension to
context-free languages.
Attempts at automatic speech recognition have known several waves.
Early efforts were based on the faith that speech is a string of phonemes
that can be isolated and recognized one by one. This wave broke when it
became clear that the physical realization of a phoneme is smeared in time
and mingled with that of its neighbors, and also context and speaker-
dependent.
Next came the invention of the highly successful time-warping DP-
matching methods, in which whole words are matched by templates.
This wave is still going strong, at least in Japan, but it may have reached
a high mark.
To probe this question, we investigate the case of the &amp;quot;jion&amp;quot;, a subset of
character readings that &amp;quot;generates&amp;quot; a large subset of Japanese. This set
has low redundancy and contains many minimal pairs. Error analysis of
DP-matching shows that most errors occur between pairs that differ only in
their initial consonant, especially if it belongs to groups such as plosives or
nasals.
Combining DP-matching with limited-scope phoneme recognition could
break through present limits.
</bodyText>
<page confidence="0.28946">
Computational Linguistics, Volume 11, Number 4, October-December 1985 271
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.846023">ABSTRACTS OF CURRENT LITERATURE</title>
<abstract confidence="0.892892333333334">In October of 1984, a workshop was held at Duke University to consider problems relating to the design of transportable natural language processors. The workshop was funded in part by a grant from the National Science Foundation and was co-sponsored by Duke and the ACL. A number of papers resulted from the meeting, and several of them in the April 1985 issue of the on Office Information Systems 3, number 2). Bruce Ballard guest edited the special issue, and wishes to thank the following persons for their assistance in reviewing papers from the workshop: Joan Bachenko, Ron Brachman, Fred Damerau, Samuel Epstein, Kurt Godden, Ralph Grishman, Carole Hafner, Jerry Hobbs, Karen Jensen, Mark Jones, Karen Kukich, Elaine Marsh, Sharon Salveter, Jonathan Slocum, Norm Sondheimer, Douglas Stumberger, and Craig Thompson. The following are abstracts from that special issue.</abstract>
<title confidence="0.8797455">Transportable Natural Language Processthrough Simplicity — The</title>
<author confidence="0.999945">Samuel S Epstein</author>
<affiliation confidence="0.999759">Bell Communications Research</affiliation>
<address confidence="0.9988145">435 South Street Morristown, NJ 07960</address>
<title confidence="0.914885">Transporting the Linguistic String Project System from a Medical to a Navy Domain</title>
<author confidence="0.952259">Elaine Marsh</author>
<affiliation confidence="0.8214915">Navy Center for Applied Research in Artificial Intelligence</affiliation>
<author confidence="0.996805">Carol Friedman</author>
<affiliation confidence="0.994638">Courant Institute of Mathematical</affiliation>
<title confidence="0.624326666666667">Sciences Portability of Syntax and Semantics in Datalog</title>
<author confidence="0.991738">Carole D Hafner</author>
<author confidence="0.991738">Kurt Godden</author>
<affiliation confidence="0.622889">General Motors Research Laboratories</affiliation>
<title confidence="0.981504666666667">Problems and Some Solutions in Customization of Natural Language Database Front Ends</title>
<author confidence="0.999995">Fred J Damerau</author>
<affiliation confidence="0.805441">IBM T. J. Watson Research Laboratory PRE (Purposefully Restricted English) is a restricted English database</affiliation>
<abstract confidence="0.981342674418605">query language whose implementation has addressed engineering goals, namely, habitability, interapplication transportability, performance, and use with a reliable database management system that supports large numbers of concurrent users and large databases. Habitability has not been demonstrated, but initial indications are encouraging. The other goals have clearly been achieved. The existence of the PRE system demonstrates that an explicitly &amp;quot;minimalist&amp;quot; approach to natural language processing can facilitate achievement of transportability. The Linguistic String Project (LSP) natural language processing system has been developed as a domain-independent natural language processing system. Initially utilized for processing sets of medical messages and other texts in the medical domain, it has been used at the Naval Research Laboratory for processing Navy messages about shipboard equipment failures. This paper describes the structure of the LSP system and the features that make it transportable from one domain to another. The processing procedures encourage the isolation of domain-specific information, yet take advantage of the syntactic and semantic similarities between the medical and Navy domains From our experience in transporting the LSP system, we identify the features that are required for transportable natural language systems. This paper presents a discussion of the techniques developed and problems encountered during the design, implementation, and experimental use of a portable natural language processor. Datalog (for &amp;quot;database dialogue&amp;quot;) is an experimental natural language query system, which was designed to achieve a maximum degree of portability and extendibility. Datalog uses a three-level architecture to provide both portability of syntax to new and extended tasks and portability of semantics to new database applications. The implementation of each of the three levels, the structures and conventions that control the interactions among them, and the way in which different aspects of the design contribute to portability are described. Finally, two specific, implemented examples are presented, showing how it was possible to transport or extend Datalog by changing only one &amp;quot;layer&amp;quot; of the system&apos;s knowledge and achieve correct processing of the extended input by the entire system. This paper is concerned with some of the issues arising in the development of a domain-independent English interface to IBM SQL-based program products. The TQA system falls into the class of multilayered natural language processing systems. As a result, there is a large number of potential points at which customization to a particular database can be done. Of these, we discuss procedures that affect the reader, the lexicon, the lowest level of grammar rules, the semantic interpreter, and the output formatter. Our tests lead us to believe that the approach we are taking will make it Linguistics, Volume 11, Number 4, October-December 1985</abstract>
<title confidence="0.962030333333333">The FINITE STRING Abstracts of Current Literature Transportable in Half a Dozen Ways</title>
<author confidence="0.908197">Bozena Henisz Thompson</author>
<author confidence="0.908197">Frederick B Thompson</author>
<affiliation confidence="0.999083">California Institute of Technology</affiliation>
<title confidence="0.852017">Transportability to Other Languages: The Natural Language Processing Project in the AI Program at MCC</title>
<author confidence="0.994875">Jonathan Slocum</author>
<author confidence="0.994875">Carol F Justus</author>
<affiliation confidence="0.8374755">Microelectronics and Computer Technology Corporation</affiliation>
<abstract confidence="0.992264571428571">possible for database administrators to generate roliust English interfaces to particular databases without help from linguistic experts. This paper is a discussion of the technical issues and solutions encountered in making the ASK System transportable. A natural language system can be &amp;quot;transportable&amp;quot; in a number of ways. Although transportability to a new domain is most prominent, other ways are also important if the system is to have viability in the commercial marketplace. On the one hand, transporting a system to a new domain may start with the system prior to adding any domain of knowledge and extend it to incorporate the new domain. On the other hand, one may wish to add to a system that already has knowledge of one domain the knowledge concerning a second domain, that is, to extend the system to cover this second domain. In the context of ASK, it has been natural to implement extending and then achieve transportability as a special case. this paper, we consider six ways in which the System be extended to include new capabilities: — to a new domain, — to a new object type, — to access data from a foreign database, — to a new natural language, — to a new programming language, — to a new computer family. Special-purpose applications, such as those to accommodate standard office tasks, would make use of these various means of extension. We discuss a recently launched, long-term project in natural language processing, the primary concern of which is that natural language applications be transportable among human languages. In particular, we seek to develop system tools and linguistic processing techniques that are themselves language-independent to the maximum extent practical. In this paper we discuss our project goals and outline our intended approach, address some cross-linguistic requirements, and then present some new linguistic data that we feel support our approach. following abstracts are from of the Conference on Theoretical and Methodological Issues in Machine of Natural Languages, at Colgate University, Hamilton, New York, 14-16 August 1985. Copies of the no longer available; papers should be requested from the author(s).</abstract>
<title confidence="0.930375">View of the &lt;C,A&gt;,T Framework in Eurotra</title>
<author confidence="0.922874666666667">Doug Arnold</author>
<author confidence="0.922874666666667">Lieven Jaspaert</author>
<author confidence="0.922874666666667">Rod Johnson</author>
<author confidence="0.922874666666667">Steven Kraumvr</author>
<author confidence="0.922874666666667">Mike Rosner</author>
<author confidence="0.922874666666667">Louis des Tombe</author>
<author confidence="0.922874666666667">Nino Varile</author>
<author confidence="0.922874666666667">Susan Warwick</author>
<phone confidence="0.457924">pp. 1-14</phone>
<title confidence="0.993167">On the Production Environment Proposed for the Eiwotra Project</title>
<author confidence="0.91469">D Bachut</author>
<abstract confidence="0.967872333333333">Groupe d&apos;Etudes pour la Traduction background to this paper is the attempt within develop a framework for research and development work in in particular an environment which facilitates reasoning about the relationships between the representations that are necessary for automatic translation between natural languages. The more immediate background is the attempt to apply this framework experimentally on a small scale in devela &amp;quot;the toy&amp;quot;) over the summer and autumn of this year. The aim of this paper is to give a reasonably clear idea about the user language and theories of representation for this experiment (the Mu 1 level in terms of the paper by Louis des Tombe), and to indicate en route some of the directions for further work. It reports work in progress, and is thus deliberately speculative, programmatic, and rather informal. For concreteness, section 2 gives a brief and rather casual restatement of some of the key assumptions behind this work. We present the general architecture of a production environment which is for M(A)T and give some proposals to integrate new functionalities in this system. A good management of the results of the translation process may lead to an easier improvement of the linguistic Linguistics, Volume 11, Number 4, October-December 1985</abstract>
<title confidence="0.478417">FINITE STRING of Current Literature Automatique Universite Scientifique et Medicale de Grenoble</title>
<author confidence="0.510286">Saint Martin d&apos;Heres</author>
<affiliation confidence="0.566408">amp; Institut de Formation et Conseil en Informatique</affiliation>
<address confidence="0.65303">27, rue de Turenne - 3800 Grenoble FRANCE</address>
<title confidence="0.492666">A Case Study in Software Evolution: from Ariane-78.4 to Ariane-85</title>
<author confidence="0.963412">P Guillaume Boitet</author>
<author confidence="0.963412">M Quezel-Ambrunaz</author>
<affiliation confidence="0.594855">Groupe d&apos;Etudes pour la Traduction Automatique Universite Scientifique et Mddicale de</affiliation>
<address confidence="0.544303">Grenoble BP 68 - 38402 Saint Martin d&apos;Heres, FRANCE</address>
<phone confidence="0.516444">pp. 27-58</phone>
<title confidence="0.994712">New Approaches to Machine Translation</title>
<author confidence="0.999868">Jaime G Carbonell</author>
<author confidence="0.999868">Mamas Tomita</author>
<affiliation confidence="0.999995">Carnegie-Mellon University</affiliation>
<address confidence="0.999946">Pittsburgh, PA 15213</address>
<phone confidence="0.514205">pp. 59-74</phone>
<title confidence="0.995599">Lexicon-Driven Machine Translation</title>
<author confidence="0.997778">R E Cullingford</author>
<affiliation confidence="0.99483">Institute of</affiliation>
<address confidence="0.999642">Atlanta, GA 30332</address>
<author confidence="0.880958">B A Onyshkevyrh</author>
<affiliation confidence="0.99999">Princeton University</affiliation>
<address confidence="0.999791">Princeton, NJ 08544</address>
<title confidence="0.9938375">On the Design of Expert Systems Grafted on MT Systems</title>
<author confidence="0.65203">Boitet Groupe d&apos;Etudes pour la Traduction</author>
<affiliation confidence="0.6517225">Automatique Universite Scientifique et Medicale de</affiliation>
<address confidence="0.425890333333333">Grenoble BP 68 - 38402 Saint Martin d&apos;Heres, FRANCE</address>
<abstract confidence="0.977467452380952">data. We describe a possible organisation for the machine environment of such a system and for the management of the data base of texts. Finally, we give some general rules for the implementation of a monitor. No abstract. The current resurgence of interest in machine translation is partially attributable to the emergence of a variety of new paradigms, ranging from better translation aids and improved preand post-editing methods, to highly interactive approaches and fully automated knowledge-based systems. This paper discusses each basic approach and provides some comparative analysis. It is argued that both interactive and knowledge-based systems offer considerable promise to remedy the deficiencies of the earlier, more ad hoc post-editing approaches. Machine Translation (MT) systems have historically relied upon explicit grammars in order to analyze the source text and reproduce it in the target language. In this paper, we argue for a style of MT in which the focus processing is at the level of the than the grammar. This approach to translation allows an analyzer to map source sentences into an interlingual form, which then can be mapped (perhaps after intermediate inferencing steps) back into target sentence(s) which are paraphrase-equivalent to the original. Advantages of the approach include: (1) the possibility for different paraphrases of the original; (2) the capability for multisentence expression of the original when no single word (e.g., a verb) exists in the target language which spans the same meaning complex as a word in the source; (3) a uniform approach to word sense disambiguation and anaphoric reference resolution; and, most important, (4) the possibility for robust handling of ungrammatical and ellipsed source text. integrate many advanced concepts from the fields of science, linguistics, and languages for linguistic programming based on production systems, complete linguistic programming environment, multilevel representations, organization of the lexicons around &amp;quot;lexical units&amp;quot;, units of translation of the size of several paragraphs, possibility to use text-driven heuristic strategies. We are now beginning to integrate new techniques: unified design of an &amp;quot;integrated&amp;quot; lexical data base containing the lexical in &amp;quot;natural&amp;quot; and &amp;quot;coded&amp;quot; form, use of the &amp;quot;static grammars&amp;quot; formalism as a specification language, and design of a kind of structural meta-editor (driven by some static grammar) allowing the interactive construction of a document in the same way as syntactic editors are used for developing programs. This paper centers on our study of possible additions of expert systems equipped with metalinguistic and extralinguistic knowledge, in order to Linguistics, Volume 11, Number 4, October-December 1985</abstract>
<title confidence="0.985600666666667">The FINITE STRING Abstracts of Current Literature Machine Translation in the SDCG Formalism</title>
<author confidence="0.984142">Xiuming Huang</author>
<affiliation confidence="0.989309">Institute of Linguistics Chinese Academy of Social Sciences</affiliation>
<address confidence="0.767937">Beijing, China</address>
<affiliation confidence="0.9970385">mail: Computing Research Laboratory New Mexico State University</affiliation>
<address confidence="0.998225">Las &apos;Cruces, NM 88003</address>
<phone confidence="0.638335">pp. 135-144</phone>
<title confidence="0.996104">Machine Translation as an Expert Task</title>
<author confidence="0.999979">Rod Johnson</author>
<author confidence="0.999979">Pete Whitelock</author>
<affiliation confidence="0.996727333333333">Centre for Computational Linguistics University of Manchester Institute of Science and Technology</affiliation>
<address confidence="0.9805435">88 Manchester M60 1QD UK</address>
<phone confidence="0.498663">pp. 145-153</phone>
<title confidence="0.9961515">The Significance of Sublanguage for Automatic Translation</title>
<author confidence="0.999995">Richard I Kittredge</author>
<affiliation confidence="0.998344">University of Montreal</affiliation>
<phone confidence="0.520475">pp. 154-166</phone>
<title confidence="0.997224">Integrating Syntax and Semantics</title>
<author confidence="0.999949">Steven L Lytinen</author>
<affiliation confidence="0.999652">Cognitive Systems, Inc.</affiliation>
<address confidence="0.9991705">234 Church Street New Haven, CT 06510</address>
<abstract confidence="0.976106857142857">pp. 167-178 solve some problems encountered in second-generation MT systems. Several examples of the possible use of expert-corrector systems in M(A)T (Machine (Aided) Translation) systems are given. The paper describes the SDCG (Semantic Definite Clause Grammars), a formalism for Natural Language Processing (NLP), and the XTRA (English Chinese Sentence TRAnslator) machine translation (MT) system based on it. The system translates general domain English sentences into grammatical Chinese sentences in a fully automatic manner. It is written in Prolog and implemented on the DEC-10, the GEC, and the SUN workstation. SDCG is an augmentation of the DCG (Definite Clause Grammar; Pereira et al. 1980) which in turn is based on CFG (Context Free Grammar). Implemented in Prolog, the SDCG is highly suitable for NLP in and particular. wide range of linguistic phenomena is covered by the including multiple word senses, coordinate constructions, and prepositional phrase attachment, among others. The case against fully automatic high quality machine translation been well-canvassed in the literature ever since ALPAC. Although considerable progress in computational linguistics has been made since then, many of the major arguments against FAHQMT still hold (a good resume is given by Martin Kay (1980)). It is not our intention to reopen the case for FAHQMT here. Rather, we contend that, accepting that FAHQMT is not possible in the current state of the art, it is both feasible and desirable to set up R &amp; D programmes in MT which can both produce results which will satisfy sponsors and provide an environment to support research directed towards bringing MT closer to the ultimate goal of FAHQMT. This paper describes the rationale and organisation behind one such programme, the UMIST English-Japanese MT project. This paper address three questions: — What is sublanguage? — Why is sublanguage analysis important for automatic translation? — How can a translation system take advantage of sublanguage properties? The first of these questions appears to have a simple answer. Natural languages clearly have specialized varieties which are used in reference to restricted subject matter. We speak, for example, of the &amp;quot;language of chemistry&amp;quot; to mean a loosely defined set of sentences or texts dealing with a particular part of reality. But when we consider the automatic translation of specialized language, we are forced to be more precise. We must describe sublanguages as coherent, rule-based systems. The attempt to write grammars for specialpurpose sublanguages raises a number of theoretical and practical probwhich are only now being intensively discussed. the only path to hiqh-quality automatic translation seems to lie through sublanguage (at least during the next decade or two), we have no choice but to solve these problems. This paper should therefore be considered as a brief summary and progress report. Well-known examples such as Bar-Hillel&apos;s (1960) &amp;quot;The box is in the pen&amp;quot; illustrate that extensive semantic analysis is necessary to resolve ambiguities that must be resolved in machine translation. If one accepts the premise that semantics should be added to the analysis techniques used in machine translation, what is the way in which it should be added? This will argue for an to semantic processing. That is, syntactic and semantic processing should take place at the same time, raththan in separate stages. However, although I will argue for the inte-</abstract>
<note confidence="0.345301">Linguistics, Volume 11, Number 4, October-December 1985</note>
<title confidence="0.998005333333333">The FINITE STRING Abstracts of Current Literature LMT: A PROLOG-Based Machine Translation System</title>
<author confidence="0.999984">Michael C McCord</author>
<affiliation confidence="0.997103">T. Watson Research Center</affiliation>
<address confidence="0.994575">Yorktown Heights, NY 10598</address>
<phone confidence="0.776299">pp. 179-182</phone>
<title confidence="0.998305">Recovering the Speaker&apos;s Decisions during Mechanical Translation</title>
<author confidence="0.999994">David D McDonald</author>
<affiliation confidence="0.999987">University of Massachusetts at Amherst,</affiliation>
<address confidence="0.996724">01003</address>
<phone confidence="0.831283">pp. 183-199</phone>
<title confidence="0.989744">Structural Transformation in the Generation Stage of MU Japanese to English Machine Translation System</title>
<author confidence="0.999804">Makoto Nagao</author>
<affiliation confidence="0.999562">Department of Electrical Engineering Kyoto University</affiliation>
<address confidence="0.998247">Sakyo, Kyoto, JAPAN</address>
<phone confidence="0.768833">pp. 200-223</phone>
<title confidence="0.994255">Interlingua Design for TRANSLATOR</title>
<author confidence="0.999944">Sergei Nirenburg</author>
<author confidence="0.999944">Allen B Tucker</author>
<affiliation confidence="0.9992045">Department of Computer Science Colgate University</affiliation>
<author confidence="0.999781">Victor Raskin</author>
<affiliation confidence="0.9998665">Department of English Purdue University</affiliation>
<address confidence="0.463604">pp. 224-244</address>
<abstract confidence="0.983502326530612">of syntactic and semantic analysis also argue for the of a separate body of syntactic for building a separate the parsing process. This is in contrast to previous integrated parsers, which have relied almost exclusively on semantic representations to guide the parsing process, and which have not used a separate body of syntactic rules. The talk will describe a machine translation system, LMT, based in PROLOG, translating from English to German. The effort on LMT per se has just begun this year, although the logic programming methodology for the analysis of the source (English) goes back several years (see, e.g., McCord 1982). When studied as a source of insight into the human language faculty, rather than to construct a commercially useful service, mechanical translation (MT) is carried out by coupling an otherwise normal natural language parsing system to a normal natural language generation system. In this paper we propose that a crucial capability has been omitted from the design of the parsers that have been used to date, namely a facility for recognizing the information that is implicit in the form of any well written text: matters of emphasis, whether a fact is new or old, whether a relationship is given explicitly or left as an obvious inference, signals of intended moves in the discourse, and other things of this sort. We claim that mechanical translations are &amp;quot;mechanical&amp;quot; principally because they pay no attention to information of this sort, and propose that this can be dealt with by incorporating into the parser knowledge of the relationship between usage and form of the sort that is commonplace in any modern language generation system. No abstract. The interlingua approach to machine translation (MT) is characterized by the following two stages: 1) translation of the source text into an intermediate representation, an language is designed to capture the various types of meaning of the source text, and 2) translation from the interlingua into the target text. Over the years a number of MT projects tried to develop interlingua-based systems. In these projects the amount of linguistic and encyclopaedic knowledge used to produce intermediate representations was quite limited. However, even at that level difficulties connected with encoding knowseemed overwhelming. The at Colgate University benefits from recent developments in knowledge representation techniques. The text of its interlingua text reflects syntactic, lexical, contextual, discourse (including speech situation), and pragmatic meaning of the input. This paper discusses the lexicon and grammar of the interlinused in touches upon the structure of the bilingual (source language to interlingua) dictionaries. The actual compilation of the interlingua dictionary and additional knowledge bases is an empirical process during which modifications to the original formulations are expected to occur. At all times in the design process the authors were guided by the desire to make decisions that are &apos;literate&amp;quot; from the point of view of Linguistics, Volume 11, Number 4, October-December 1985</abstract>
<title confidence="0.997970666666667">The FINITE STRING Abstracts of Current Literature The Level Hypothesis in Discourse Analysis</title>
<author confidence="0.999275">James Pustejovsky</author>
<affiliation confidence="0.999058">Department of Computer and Information Sciences University of Massachusetts at Amherst,</affiliation>
<address confidence="0.978701">01003</address>
<title confidence="0.9929225">Linguistics and Natural Language Processing</title>
<author confidence="0.999891">Victor Raskin</author>
<affiliation confidence="0.999691">Purdue University</affiliation>
<title confidence="0.9798455">Static Grammars: A Formalism for the Description of Linguistic Models</title>
<author confidence="0.836905">Bernard Vauquois</author>
<author confidence="0.836905">Sylviane Chappuy</author>
<affiliation confidence="0.507322333333333">Groupe d&apos;Etudes pour la Traduction Automatique Universite Scientifique et Medicale de</affiliation>
<address confidence="0.403399">Grenoble</address>
<abstract confidence="0.938929559322034">BP 68 - 38402 Saint Martin d&apos;Heres, FRANCE &amp; Institut de Formation et Conseil en Informatique 27, rue de Turenne - 3800 Grenoble FRANCE linguistic theory and the experience of knowledge representation in artificial intelligence. In this paper I would like to explore some difficult questions related to topics in discourse analysis (henceforth DA) and offer a partial solution to of them. In particular, I will address the issue of DA and how the various approaches taken within the field can be classified according to a leveled model. I then want to consider an approach I have been pursuing for representing the semantics of discourse, and consider how it fits in to the proposed model for DA. The paper addresses the issue of cooperation between linguistics and natural language processing (NLP), in general, and between linguistics and machine translation (MT), in particular. It focuses on just one direction of such cooperation, namely applications of linguistics to NLP, virtually ignoring for now any possible applications of NLP to linguistics, which can range from providing computer-based research tools and aids to linguistics to implementing formal linguistic theories and verifying linguistic models. Section 1 deals with the question why linguistics must be applied to NLP and what the consequences of ignoring it are. Section 2 provides a counterpoint by discussing how linguistics should not be applied to NLP and, by contrast and inference, how it should be. Section 3 narrows the discussion down to one promising approach to NLP, the sublanguage deal, and the interesting ways in which linguistics can be utilized within a limited sublanguage. Section 4 is devoted specifically — but necessarily briefly — to the things linguistics can contribute to MT. The work described here was the consequence of the idea that we wanted to make a new, more interesting theoretical start in EUROTRA. It is preliminary and not fully developed yet; it should be seen as the reflection of a way of thinking about MT. Currently, we are making it more precise, and experimenting with it. In this paper, we sketch the general outlines of the new EUROTRA framework; some more exemplification can be found the paper by et al. (p. 1 of this volume). Most existing practical machine translation systems are designed to translate documentation, such as technical papers and manuals. However, there is a growing need for translating not only large texts but also personal short texts such as letters and informal messages. The conventional machine translation systems, which are intended to translate large texts, are not very suitable for these kinds of small jobs. We need an interactive system which has a totally different design philosophy. This paper describes the design philosophy of personal/interactive machine translation systems, and studies in feasibility. For a linguistic model it is necessary, first of all, to define the mapping between the strings of words of a language and their structural organisation, given that with transducers there are many ways of obtaining the same result using different strategies. This mapping, which we will call a &amp;quot;static grammar&amp;quot;, is independent of the analysis, generation, or whatever strategy adopted. Moreover, the formalism of a static grammar is not affected by the choice or number of interpretation levels. Such a grammar is the &amp;quot;reference&amp;quot; for any dynamic modular rule organisation, whether analysis or generation. We present here a &amp;quot;static grammar&amp;quot; formalism recently developed at Grenoble (G.E.T.A. Groupe d&apos;Etude pour la Traduction Automatique) the supervision of Prof.</abstract>
<title confidence="0.719850333333333">Using this formalism, any given language can be described as a series A Preliminary Linguistic Framework for Eurotra, June 1985</title>
<author confidence="0.900111666666667">Louis des Tombe</author>
<author confidence="0.900111666666667">Doug Arnold</author>
<author confidence="0.900111666666667">Lieven Jaspaert</author>
<author confidence="0.900111666666667">Rod Johnson</author>
<author confidence="0.900111666666667">Steven Krauurr</author>
<author confidence="0.900111666666667">Mike Rosner</author>
<author confidence="0.900111666666667">Nina Varile</author>
<author confidence="0.900111666666667">Susan Warwick</author>
<title confidence="0.94716">Feasibility Study of Personal/Interactive Machine Translation Systems</title>
<author confidence="0.998796">Masora Tomita</author>
<affiliation confidence="0.999962">Computer Science Department Carnegie-Mellon University</affiliation>
<address confidence="0.999925">Pittsburgh, PA 15213</address>
<date confidence="0.364245">258 Computational Linguistics, Volume 11, Number 4, October-December 1985</date>
<title confidence="0.985449333333333">The FINITE STRING Abstracts of Current Literature On Debugging Environment Proposed for Eurotra</title>
<author confidence="0.999387">N Verastegui</author>
<affiliation confidence="0.978608">Institut de Formation et Conseil en Informatique</affiliation>
<address confidence="0.7388285">27, rue de Turenne - 3800 Grenoble FRANCE</address>
<title confidence="0.9823605">Knowledge Resource Tools for Accessing Large Text Files</title>
<author confidence="0.999858">Donald E Walker</author>
<affiliation confidence="0.998997666666667">Artificial Intelligence and Information Science Research Bell Communications Research</affiliation>
<address confidence="0.997324">435 South Street MRE 2A379 Morristown, NJ 07960</address>
<title confidence="0.907318">Reflections on the Knowledge Needed to Process Ill-Formed Language</title>
<author confidence="0.999996">Ralph M Weischedel</author>
<author confidence="0.999996">Lance A Ran show</author>
<affiliation confidence="0.996351">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.999774">10 Moulton Street Cambridge, MA 02238</address>
<title confidence="0.858569">Characteristics of the METAL Machine Translation System at Production Stage</title>
<author confidence="0.997514">John S White</author>
<affiliation confidence="0.531598">Siemens Communication Systems</affiliation>
<abstract confidence="0.956559418181818">of &amp;quot;charts&amp;quot;. Each &amp;quot;chart&amp;quot; describes how a certain group of strings corresponds to the structure associated with this group of strings (this structure is a valid and complete substructure of the linguistic model). The structures of all the sentences of a language for a given linguistic model can be described by means of a series of chart inter-references. The static grammar is used as a base for writing dynamic analysis and generation modules, however, the static grammar does not concern itself with strategic, combinatorial, ambiguity problems or the chart of structures related to dynamic grammars. We will present here several examples of charts and discuss the dynamic uses of these static grammars. A proposal of external specification of the user environment for the EUROTRA project is presented. The needs of the users and the functions which are necessary for an efficient testing environment are analyzed. This paper provides an overview of a research program just being defined at Bellcore. The objective is to develop facilities for working with large document collections that provide more refined access to the information contained in the &amp;quot;source&amp;quot; materials than is possible through current information retrieval procedures. The tools being used for this purpose are machine-readable dictionaries, encyclopedias, and related &amp;quot;resources&amp;quot; that provide geographical, biographical, and other kinds of specialized knowledge. A major feature of the research program is the exploitation of the relationships between interactions between texts and tools are intended to support experts who organize and use information in a workstation environment. Two systems under development will be described to illustrate the approach: one providing capabilities for full-text subject assessment; the other for concept elaboration while reading text. Progress in the research depends critically on developments in artificial intelligence, computational linguistics, and information science to provide a scientific base, and on software engineering, database management, and distributed systems to provide the technology. This paper reflects about the kinds of morphological syntactic, semantic, and pragmatic knowledge needed to process ill-formed input. We conclude that an excellent start on processing ill-formed input has been exemplified in a number of concrete implementations, but that a substantial amount of fundamental work must still be done if our systems are to understand language robustly to the degree that humans do. Furthermore, we conclude that studying ill-formed language offers important perspectives on the knowledge and architecture needed to correctly understand natural languages. translation system, a joint project of the Linguistic Research Center and Siemens, has been released for use as part of marketed translation systems. The system, which presently translates technical German into English, is an outgrowth of a traditional, generative approach to automatic analysis and synthesis of natural language phenomena carried out at the Linguistics Research Center for many years. In its present manifestation, it is a modular design consisting of purely monolingual lexicons, transfer lexicons, and an augmented phrase structure grammar. The grammar is powerful enough to constrain application, to build new nodes with essential characteristics of their sons and new synthetic information as well, and to perform transformations to re-order, delete, and create Linguistics, Volume II, Number 1985 The FINITE STRING Abstracts of Current Literature Relevance, Points of View and Dialogue constituents. The parser is enhanced to allow application of rules in levels, and eliminating unlikely paths via preferential weightings calculated from and grammatical data. The conceived in recent years as destined for implementation, has an orientation to user interface which includes sophisticated text stripping, unfound word handling and reconstitution, and a convenient means of working with the lexicons inter-actively. Modelling This paper attempts to compare two approaches to the modelling of human discourse and, more particularly, dialogue. Both place themselves within a general &amp;quot;information processing paradigm&amp;quot;, and both descend from the insights of Once (1975) that understanding is a matter of inference from what is said and what is assumed. So general is that assumption now, and so widespread are the disciplines that draw upon it — philosophy, psycholo-gy, linguistics, and artificial intelligence — that it is hard to capture briefly except in opposition to the transformational-generative paradigm of language, with its notions of the primacy and autonomy of syntax, and the theoretical primacy of explications of competence over those of perform-ance. The Generative Semanticists attempted to merge the two traditions and their failure has made it easier to separate off and clarify the work under discussion here.</abstract>
<author confidence="0.761561">Yorick Wilks</author>
<affiliation confidence="0.999722">Computing Research Laboratory New Mexico State University</affiliation>
<address confidence="0.998779">Las Cruces, NM 88003</address>
<phone confidence="0.694199">pp. 370-387</phone>
<affiliation confidence="0.346873">following abstracts are from the of the Ninth International Joint Conference on Artificial Intelligence</affiliation>
<address confidence="0.926725">Angeles, California, August 1985. Two volumes; edited by Aravind Joshi. [ISBN 0-934613-02-8, $40.00 for AAAI members; $55.00 for non-members. Morgan Kaufmann Publishers, Inc.; 95 First Street; Los Altos, CA 94022 (415 941-4960)1</address>
<title confidence="0.746661">Computational Neurolinguistics — What Is It All About?</title>
<author confidence="0.999967">Helen M Gigley</author>
<affiliation confidence="0.9722975">Department of Computer Science University of New Hampshire</affiliation>
<phone confidence="0.610586">pp. 260-266</phone>
<title confidence="0.9955085">A Short Note on Opportunistic Planning and Memory in Arguments</title>
<author confidence="0.999808">Lawrence Birnbaum</author>
<affiliation confidence="0.982377">Department of Computer Science Yale University</affiliation>
<address confidence="0.999619">New Haven, CT 06520</address>
<phone confidence="0.562198">pp. 281-283</phone>
<abstract confidence="0.972268322580645">Computational Neurolinguistics (CN) integrates artificial intelligence (Al) methods with concepts of neurally motivated processing to develop cognitive models of natural language processing. HOPE is one example of a model developed to address issues in CN. model is parallel, and exemplifies language as the result of synchronized processes which are asynchronous in nature. Furthermore, the model is substantially validated to include normal behavioral evidence in its design. In addition, it attends to aspects of language breakdown which are well documented in the literature of neurolinguistics or aphasia. This paper discusses assumptions which underlie the CN approach to model development. It will describe the neurally motivated or &amp;quot;natural computational&amp;quot; processes which produce the model&apos;s observable and veribehavioral results. The differences in the to other models of parallel memory process and behavior will be presented. Finally, the contribution of the CN research approach as a tool for investigating the breakdown of language performance and its potential contribution to understanding brain function will be discussed. Engaging in an argument is a complex task of natural language processing that involves understanding an opponent&apos;s utterances, discovering what his &amp;quot;point&amp;quot; is, determining whether his claims are believable, and fashioning a coherent rebuttal. Accomplishing these tasks requires the coordination of many different abilities and many different kinds of knowledge. Because arguing, and conversation generally, involve real-time interaction with another agent, this coordination must be even more flexible than is required for other natural language processing tasks. An arguer must have some expectations about what his opponents might say, but must also be able to respond to the unexpected. He must have some idea of the claims he wants to make, and plans for putting them forward, but his opponent may confound these plans. Or, more positively, his opponent may say something that offers an unforeseen opportunity to make a point. Arguing 260 Computational Linguistics, Volume 11, Number 4, October-December 1985</abstract>
<title confidence="0.997214666666667">The FINITE STRING Abstracts of Current Literature A Process Model of Case-Based Reasoning in Problem Solving</title>
<author confidence="0.9814615">Janet L Kolodner</author>
<author confidence="0.9814615">Robert L Simpson</author>
<author confidence="0.9814615">Katia Sycara-Cyranski</author>
<affiliation confidence="0.998329666666667">School of Information and Computer Science Georgia Institute of Technology</affiliation>
<address confidence="0.999976">Atlanta, GA 30332</address>
<phone confidence="0.832197">pp. 289-290</phone>
<title confidence="0.9960135">Learning to Understand Contractual Situations</title>
<author confidence="0.998579">Seth R Goldman</author>
<author confidence="0.998579">Michael G Dyer</author>
<author confidence="0.998579">Margot Flowers</author>
<affiliation confidence="0.99999">Artificial Intelligence Laboratory Computer Science Department University of California</affiliation>
<address confidence="0.999963">Los Angeles, CA 90024</address>
<phone confidence="0.846662">pp. 291-293</phone>
<title confidence="0.6957295">A Representation for Complex Physical Domains</title>
<abstract confidence="0.985287781818181">thus exemplifies the need for a flexible mix of top-down and bottom-up processing in both language understanding and production. This paper is concerned with the roles of memory processing and planning in the processes of understanding and generating utterances in an argument or conversation. In particular, I will show that the memory and inferential processing necessary in order to understand another person&apos;s utterances can and should perform much of the work required to generate a response, work that most previous theories of conversation would delegate to explicit, goal-directed planning. The consequences of this, both for memory processing and for planning, will be briefly described and analyzed. Much of the problem solving done by both novices and experts uses &amp;quot;casebased&amp;quot; reasoning, or reasoning by analogy to previous similar cases. We explore the ways in which case-based reasoning can help in problem solving. According to our model, transfer of knowledge between cases is guided largely by the problem solving process itself. Our model shows the interactions between problem solving processes and memory for experience. Our computer program, called the MEDIATOR, illustrates casebased reasoning in interpreting and resolving common sense disputes. In the field of law, decisions in previous cases often play a significant role in the presentation and outcome of new cases. Lawyers are constantly recalling old cases to aid them in preparing their own briefs. How do lawyers remember cases? What are the features they use to organize and retrieve past decisions? How do lawyers learn which features are important? To address these questions we are constructing a model of legal novices (i.e., first year law students) and the processes by which they learn contract law. Our model is embodied in a computer program called STARE the latin, decisis, refers to the principle of using past cases to decide current disputes). STARE will read descriptions of contractual situations and attempt to predict the decision based on its general commonsense knowledge of agreements, the previous cases stored in an episodic memory, and knowledge of some basic legal concepts. This paper presents a framework for a theory of granularity which is seen as a means of constructing simple theories out of more complex ones. A transitive indistinguishability relation can be defined by means of a set of relevant predicates, allowing simplification of a theory of complex phenomena into computationally tractable local theories, or granularities. Nontransitive indistinguishability relations can be characterized in terms of relevant partial predicates, and idealization allows simplification into tractable local theories. Various local theories must be linked with each other by means of articulation axioms, to allow shifts of perspective. Such a treatment of granularity must be built into the very foundations of the reasoning processes of intelligent agents in a complex world. Ways in which physical objects interact are explored, and in particular the of analyzed. Intuitively, the two shapes in a given spatial configuration is a statement about how much one shape needs be mutilated in order to be made identical to the other. The one object with respect to another specifies what motions the first object can go through without the second one moving. The formulations, termed kinematics, compared to work that was done in the kinematics of machinery in the 19th century and that has since been somewhat neglected. We are exploring a system, called PROMPT, that will be capable of reasoning from first principles and high level knowledge in complex, physical Granularity</abstract>
<author confidence="0.999727">Jerry R Hobbs</author>
<affiliation confidence="0.99976">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.999913">Menlo Park, CA 94025</address>
<affiliation confidence="0.961603">amp; Center for the Study of Language and Information Stanford University</affiliation>
<address confidence="0.999937">Stanford, CA 94305</address>
<phone confidence="0.723421">pp. 432-435</phone>
<title confidence="0.769996">Naive Kinematics: One Aspect of Shape</title>
<author confidence="0.671791">Yoav Shoham</author>
<affiliation confidence="0.992364">Computer Science Department Yale University</affiliation>
<address confidence="0.997686">New Haven, CT 06520</address>
<note confidence="0.63558">pp. 436-442 Computational Linguistics, Volume 11, Number 4, October-December 1985 261</note>
<title confidence="0.988944">The FINITE STRING Abstracts of Current Literature</title>
<author confidence="0.96431">Sanjaya Addanki</author>
<affiliation confidence="0.957338">T.J. Research Center</affiliation>
<author confidence="0.5689">Ernest Davis</author>
<affiliation confidence="0.970863">New York University</affiliation>
<address confidence="0.521018">pp. 443-446</address>
<title confidence="0.9925175">ONYX: An Architecture for Planning in Uncertain Environments</title>
<author confidence="0.996695">Curtis Langlotz</author>
<author confidence="0.996695">Lawrence Fagan</author>
<author confidence="0.996695">Samson Tu</author>
<author confidence="0.996695">John Williams</author>
<affiliation confidence="0.9349872">Medical Computer Science Group Branimir Sikic Department of Medicine Knowledge Systems Laboratory Stanford University</affiliation>
<address confidence="0.999946">Stanford, CA 94305</address>
<phone confidence="0.787931">pp. 447-449</phone>
<title confidence="0.9990665">Understanding Behavior Using Consolidation</title>
<author confidence="0.999323">Tom Byander</author>
<author confidence="0.999323">B Chandrasekaran</author>
<affiliation confidence="0.9338508">Laboratory for Artificial Intelligence Research Department of Computer and Information Science The Ohio State University</affiliation>
<address confidence="0.999993">Columbus, OH 43210</address>
<phone confidence="0.928764">pp. 450-454</phone>
<title confidence="0.9988925">A Decidable First-Order Logic for Knowledge Representation</title>
<author confidence="0.999459">Peter F Patel-Schneider</author>
<affiliation confidence="0.805693">Schlumberger Palo Alto Research</affiliation>
<address confidence="0.9993825">3340 Hillview Avenue Palo Alto, CA 94304</address>
<phone confidence="0.82938">pp. 455-458</phone>
<title confidence="0.908426">Two Results on Default Logic</title>
<author confidence="0.950028">Witold Lukaszewicz</author>
<affiliation confidence="0.999147">Institute of Informatics University of Warsaw</affiliation>
<address confidence="0.997811">P.O. Box 1210 00-901 Warszawa, Poland</address>
<phone confidence="0.698109">pp. 459-461</phone>
<title confidence="0.9975285">On the Descriptional Complexity of Production Systems</title>
<author confidence="0.980385">Peter Trum</author>
<abstract confidence="0.986461910714286">Battelle-Institute e.V. domains. Such problem solving calls for a representation that will support the different analyses techniques required (e.g., differential, asymptotic, perturbation, etc.). Efficiency considerations requires that the representation also support heuristic control of reasoning techniques. This paper lays the ground work for our effort by briefly describing the ontology and the representation scheme of PROMPT. Our ontology allows reasoning about multiple pasts and different happenings in the same space-time. The ontology provides important distinctions between materials, objects, bulk and distributed abstractions among physical entities. We organise world knowledge into &amp;quot;prototypes&amp;quot; that are used to focus the reasoning process. Problem solving involves reasoning with and modifying prototypes. The ONYX program is designed to fill the need for planning in application areas where traditional planning methodology is difficult to apply. While the program being developed will assist with the planning of cancer therapy, its architecture is intended to be of use whenever goals are ill-specified, plan operators have uncertain effects, or trade-offs and unresolvable conflicts occur between goals. We describe a planning process which uses strategic information and a mechanistic model of the domain. The process consists of three steps: (1) generate a small set of plausible plans based on current data, (2) simulate those plans to predict their possible consequences, and (3) based on the results of those simulations, rank the plans according to how well each meets the goals for the situation. In this paper we wish to make three contributions to Naive Physics in the context of reasoning about devices. (1) We discuss some limitations of current qualitative simulation approaches with regard to a number of issues in understanding device behavior and point to the need for additional processes, (2) We introduce a new approach to deriving the behavior of devices called consolidation. In this approach, the behavior of a device is derived from the behavior of its components by inferring the behavior of selected substructures of the device. (3) We present an ontology of behavior and structure which is well-suited to the consolidation process. This ontology makes it possible to state rules of behavior composition, i.e., simple patterns of behavior and structure are used to infer additional behaviors. Even though logic has played an important role in knowledge representation (KR) research, there has been little effort expended on devising decidlogics for KR. Most modifications to logic suggested for either extensions to first-order logic (e.g., to handle non-monotonicity) or ad hoc changes in its inference mechanism. This paper presents a variant of firstorder relevance logic that has a decidable algorithm for determining tautological entailment. Although this logic is considerably weaker than standard first-order logic, it can be used effectively in a KR system when semantically correct answers to queries are required within a finite amount of time. We focus on default logic, a formalism introduced be Reiter to model default reasoning. The paper consists of two parts. In the first one a translation method of non-normal defaults into the normal ones is given. Although not generally valid, this translation seems to work for a wide of defaults. second part a semantics for normal default theories is given and the completeness theorem is proved. In this paper we formalize different methods for describing control knowledge in production systems by the concept of production system schemes. In this framework these methods are compared from the viewpoint of descriptional complexity, giving us more insight by which means problems 262 Computational Linguistics, Volume 11, Number 4, October-December 1985</abstract>
<title confidence="0.580811333333333">The FINITE STRING Abstracts of Current Literature Am Roemerhof 35 D-6000 Frankfurt am Main</title>
<author confidence="0.781427">West Germany</author>
<phone confidence="0.685568">pp. 462-464</phone>
<title confidence="0.9926355">Evidential Reasoning in Semantic Networks: A Formal Theory</title>
<author confidence="0.999974">Lokendra Shastri</author>
<author confidence="0.999974">Jerome A Feldman</author>
<affiliation confidence="0.9999355">Computer Science Department University of Rochester</affiliation>
<address confidence="0.99994">Rochester, NY 14627</address>
<phone confidence="0.837384">pp. 465-474</phone>
<title confidence="0.9952125">An Endorsement-Based Plan Recognition Program</title>
<author confidence="0.999982">Michael Sullivan</author>
<author confidence="0.999982">Paul R Cohen</author>
<affiliation confidence="0.994136333333333">of Computer and Information Science University of Massachusetts</affiliation>
<address confidence="0.999973">Amherst, MA 01003</address>
<phone confidence="0.771821">pp. 475-479</phone>
<title confidence="0.8467185">Guide to the Modal Logics of Knowledge and Belief: Preliminary Draft</title>
<author confidence="0.999922">John Y Halpern</author>
<affiliation confidence="0.999999">IBM Research Laboratory</affiliation>
<address confidence="0.997551">San Jose, CA 95193</address>
<author confidence="0.618688">Yoram Moses</author>
<affiliation confidence="0.9993515">Computer Science Department Stanford University</affiliation>
<address confidence="0.999672">Stanford, CA 94305</address>
<note confidence="0.719401">amp; IBM Research Lab, San Jose pp. 480-490</note>
<title confidence="0.9338015">Belief, Awareness, and Limited Reasoning: Preliminary Report</title>
<author confidence="0.999978">Ronald Fagin</author>
<author confidence="0.999978">Joseph Y Halpern</author>
<affiliation confidence="0.999999">IBM Research Laboratory</affiliation>
<address confidence="0.999349">San Jose CA 95193</address>
<abstract confidence="0.973806018518519">pp. 491-501 can be described in an easy and succinct way. This paper presents an evidential approach to knowledge representation and inference wherein the principle of maximum entropy is applied to deal with uncertainty and incompleteness. It focuses on a restricted representation language — similar in expressive power to semantic network formalisms, and develops a formal theory of evidential inheritance within this language. The theory applies to a limited, but we think interesting, class of inheritance problems including those that involve exceptions and multiple inheritance hierarchies. The language and the accompanying evidential inference structure provide a natural treatment of defaults and conflicting information. The evidence combination rule proposed in this paper is incremental, commutative and associative and hence shares most of the attractive features of the Dempster-Shafer evidence combination rule. Furthermore, it is demonstrably better than the Dempster-Shafer rule in the context of the problems addressed in this paper. The resulting theory can be implemented as a highly parallel (connectionist) network made up of active elements that can solve inheritance problems in time proportional to the depth of the conceptual hierarchy. This paper describes a plan-recognition program built to explore the theory of endorsements (Cohen 1983). The program evaluates alternative interpretations of user actions and reasons about which are the most likely explanation of the user&apos;s intentions. Uncertainty about the various alternawas encoded in data structures called paper describes the workings of this program and the successes and limitations of the endorsement-based approach. We review and re-examine possible-words semantics for propositional logic of knowledge and belief with four particular points of emphasis: (1) we show how general techniques for finding decision procedures and complete axiomatizations apply to models for knowledge and belief, (2) we show how sensitive the difficulty of the decision procedure is to such issues as the choice of modal operators and the axiom system, (3) we discuss how notions of common knowledge and implicit knowledge among a group of agents fit into the possible-worlds framework, and (4) we consider to what extent the possible-worlds approach is a viable one for modelling knowledge and belief. As far as complexity is concerned, we show among other that while the problem of deciding satisfiability of an one knower is the problem for many knowers is an implicit knowledge operator does not change the complexity substantially, but once a common knowledge operator is added to the language, the problem becomes complete for exponential time. Several new logics for belief and knowledge are introduced and studied, all of which have the property that agents are not logically omniscient. In particular, in these logics, the set of beliefs of an agent does not necessarily contain all valid formulas. Thus, these logics are more suitable than traditional logics for modelling beliefs of humans (or machines) with limited reasoning capabilities. Our first logic is essentially an extension of Levesque&apos;s logic of implicit and explicit belief, where we intend to allow multiple agents and higher-level belief (i.e., beliefs about beliefs). Our second logic deals explicitly with &amp;quot;awareness&amp;quot;, where, roughly speaking, it is necessary to be aware of a concept before one can have beliefs about it. Our third logic gives a model of &amp;quot;local reasoning&amp;quot;, where an agent is Linguistics, Volume 11, Number 4, October-December 1985</abstract>
<title confidence="0.9931285">The FINITE STRING Abstracts of Current Literature Event Calculus</title>
<author confidence="0.999975">Gary C Borchardt</author>
<affiliation confidence="0.999985">Coordinated Science Laboratory University of Illinois</affiliation>
<address confidence="0.999836">Urbana, IL 61801</address>
<phone confidence="0.868376">pp. 524-527</phone>
<abstract confidence="0.995994857142857">A Common-Sense Theory of Time viewed as a &amp;quot;society of minds&amp;quot;, each with its own cluster of beliefs, which may contradict each other. Introspection is a general term covering the ability of an agent to reflect upon the workings of his own cognitive functions. In this paper we will be concerned with developing an explanatory theory of a particular type of introspection: a robot agent&apos;s knowledge of his own beliefs. The development is both descriptive, in the sense of being able to capture introspective behavior as it exists; and prescriptive, in yielding an effective means of adding introspective reasoning abilities to robot agents. We present a semantic model for knowledge with the following properties: (1) Knowledge is necessarily correct, (2) agents are logically omniscient, i.e., they know all the consequences of their knowledge, and (3) agents are positively introspective, i.e., they are aware of their knowledge, but not negatively introspective, i.e., they may not be aware of their ignorance. We argue that this is the appropriate model for implicit knowledge. We investigate the properties of the model, and use it to formalize the notion of circumscribed knowledge. A representation scheme for arbitrary beliefs and wants of an agent in respect to a situation, as well as to arbitrary beliefs and wants of other agents, is presented. The representation makes use of elementary situation descriptions (which are formulated in KL-ONE and delimited by partitions), and acceptance attitudes in respect to these descriptions, or to the attitudes thereabout. The scheme forms the representational base of VIE-DPM, the user modelling component of the German-language dialogue Much of our commonsense knowledge about the real world is concerned with the way things are done. This knowledge is often in the form of actions for achieving particular goals. In this paper, a formalism is presented for representing such knowledge based on action of declarative semantics for the representation is which allows a user to state the effect of doing things in the problem domain of interest. An operational semantics is also provided, shows knowledge can be used to achieve given goals or to form intentions regarding their achievement. The formalism also serves as an executable program specification language suitable for constructing complex systems. This paper presents Event Calculus, a model for representing the identifying characteristics of physical events in terms of changes in a scene and time-related combinations of other physical events. The model is used to construct a knowledge-based system for event recognition which forms a high-level description of changes in a scene, given a low-level description as input. Time-varying information is represented in the form of &amp;quot;GRAPHs&amp;quot;, data structures which plot the elements of various domains against time. Several varieties of operations are presented which map GRAPHs into GRAPHs, and representations of physical events are formed as symbolic expressions involving these operations. The paper concludes with an overof the event recognition system, as implemented in a 11/780, an example of a session with this system.</abstract>
<title confidence="0.979358333333333">The literature on the nature and representation of time is full of disputes A Computational Theory of Belief Introspection</title>
<author confidence="0.999612">Kurt Konolige</author>
<affiliation confidence="0.9998095">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.999986">Menlo Park, CA 94025</address>
<phone confidence="0.939597">pp. 502-508</phone>
<title confidence="0.9860735">A Model-Theoretic Analysis of Monotonic Knowledge</title>
<author confidence="0.999792">Moshe Y Vardi</author>
<address confidence="0.383182">CSLI, Ventura Hall</address>
<affiliation confidence="0.999959">Stanford University</affiliation>
<address confidence="0.999949">Stanford, CA 94305</address>
<phone confidence="0.796246">pp. 509-512</phone>
<title confidence="0.995796666666667">Using Situation Descriptions and Russellian Attitudes for Representing Beliefs and Wants</title>
<author confidence="0.991957">Alfred Kobsa</author>
<affiliation confidence="0.941696">Austrian Research Institute for Artificial Intelligence</affiliation>
<address confidence="0.989685">Schottengasse 3 A-1010 Vienna, Austria</address>
<author confidence="0.472372">Sonderforschungsbereich</author>
<affiliation confidence="0.9991245">Department of Computer Science Universitat des Saarlandes</affiliation>
<address confidence="0.974138">D-66 Saarbrticken, W. Germany</address>
<phone confidence="0.926928">pp. 513-515</phone>
<title confidence="0.994977">A Procedural Logic</title>
<author confidence="0.9663615">Michael P Georgeff</author>
<author confidence="0.9663615">Amy L Lansky</author>
<author confidence="0.9663615">Pierre Bessiere</author>
<affiliation confidence="0.999433">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.999961">Menlo Park, CA 94025</address>
<phone confidence="0.76402">pp. 516-523</phone>
<title confidence="0.620696">Linguistics, Volume 11, Number 4, October-December 1985 The FINITE STRING Abstracts of Current Literature</title>
<author confidence="0.999986">James F Allen</author>
<author confidence="0.999986">Patrick J Haws</author>
<affiliation confidence="0.931529666666667">Departments of Computer Science and Philosophy University of Rochester</affiliation>
<address confidence="0.999944">Rochester, NY 14627</address>
<phone confidence="0.881221">pp. 528-531</phone>
<title confidence="0.917702666666667">An Essential Hybrid Reasoning System: and Symbol Level Acof</title>
<author confidence="0.999995">Ronald J Brachman</author>
<affiliation confidence="0.999892">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.999813">600 Mountain Avenue Murray Hill, NJ 07974</address>
<author confidence="0.986446">Victoria Pigman Gilbert</author>
<affiliation confidence="0.950928">Schlumberger Palo Alto Research</affiliation>
<address confidence="0.999325">3340 Hillview Avenue Palo Alto, CA 94304</address>
<author confidence="0.999944">Hector J Levesque</author>
<affiliation confidence="0.999983">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.996419">Toronto, Ont., Canada M5S 1A7</address>
<title confidence="0.9977135">The Layered Architecture of a System for Reasoning about Programs</title>
<author confidence="0.999916">Charles Rich</author>
<affiliation confidence="0.9999755">The Artificial Intelligence Laboratory Massachusetts Institute of Technology</affiliation>
<address confidence="0.999977">Cambridge, MA 02139</address>
<title confidence="0.9814055">The Restricted Language Architecture of a Hybrid Representation System</title>
<author confidence="0.999997">Marc Vilain</author>
<affiliation confidence="0.997481">BBN Laboratories</affiliation>
<address confidence="0.9993025">10 Moulton Street Cambridge, MA 02238</address>
<title confidence="0.88960575">Proportionality Graphs, Units Analysis, and Domain Constraints: Improving the Power and Efficiency of the Scientific Discovery Process</title>
<author confidence="0.999965">Brian Falkenhainer</author>
<affiliation confidence="0.9999435">Department of Computer Science University of Illinois</affiliation>
<address confidence="0.9988535">1304 W. Springfield Avenue Urbana, IL 61801</address>
<abstract confidence="0.998989283018868">and contradictory theories. This is surprising since the nature of time does not cause any worry for people in their everyday coping with the world. What this suggests is that there is some form of common sense knowledge about time that is rich enough to enable people to deal with the world, and which is universal enough to enable cooperation and communication between people. In this paper, we propose such a theory and defend it in two ways. We axiomatize a theory of time in terms of intervals and the single relation MEET. We then show that this axiomatization subsumes Allen&apos;s interval-based theory. We then extend the theory by formally defining the beginnings and endings of intervals and show that these have the properties we normally would associate with points. We distinguish these point-like objects and the concept of hypothesized in discrete time models. Finally, we examine the theory in terms of each of several different models. Hybrid inference systems are an important way to address the fact that intelligent systems have multifaceted representational and reasoning competence. KRYPTON is an experimental prototype that completely handles both terminological and assertional knowledge; these two kinds of information are tightly linked by having sentences in an assertional component be formed using structured complex predicates defined in a complementary terminological component. KRYPTON is unique in that it combines in a completely integrated fashion a frame-based description language and a first-order resolution theorem-prover. We give here both a formal Knowledge Level view of the user interface to KRYPTON and the technical Symbol Level details of the integration of the two disparate components, thus providing an essential picture of the abstract function that KRYPTON computes and the implementation technology needed to make it work. We also illustrate the kind of complex question the system can answer. Cake is a hybrid system which provides reasoning facilities for the Programmer&apos;s Apprentice. This paper describes the architecture of Cake, which is divided into eight layers, each with associated representations and reasoning procedures. The operation of Cake is illustrated by a complete trace of the solution of an example reasoning problem. We also argue that a hybrid system in general is characterized by the use of multiple representations in the sense of multiple data abstractions, which does not necessarily imply distinct implementation data structures. Hybrid architectures have been used in several recent knowledge representation systems. This paper explores some distinctions between various hybrid representation architectures, focusing in particular on systems built around restricted representation languages. This restricted language architecture is illustrated by describing KL-TWO, a hybrid reasoner based on the restricted representation facility RUP. The bulk of this paper discusses KL-TWO, its subcomponents, and the techniques used to interface them. important subproblem of scientific discovery is discovery, finding formulas that relate some set (or subset) of a collection of numerical parameters. Current work in quantitative discovery suffers from a lack of effiency and generality. This paper discusses methods that are efficient and yet general for discovery equations which try to avoid exponential search. Importantly, these methods can derive equations that cover subsets of the data and derive explicit descriptions of when the equations are applicable. These methods are fully implemented in a system named ABACUS, which is described and some of its results are presented.</abstract>
<note confidence="0.394776">Linguistics, Volume 11, Number 4, October-December 1985</note>
<title confidence="0.991645666666667">The FINITE STRING Abstracts of Current Literature A New Kind of Finite-State Automaton: Register Vector Grammar</title>
<author confidence="0.999975">Glenn David Blank</author>
<affiliation confidence="0.874049333333333">Lehigh University CSEE Department Packard Lab 19</affiliation>
<address confidence="0.996256">Bethlehem, PA 18015</address>
<phone confidence="0.751229">PP. 749-755</phone>
<title confidence="0.996204">An Efficient Context-Free Parsing Algorithm for Natural Languages</title>
<author confidence="0.963998">Tomita Masan</author>
<affiliation confidence="0.9999485">Computer Science Department Carnegie-Mellon University</affiliation>
<address confidence="0.999945">Pittsburgh, PA 15213</address>
<phone confidence="0.497929">pp. 756-764</phone>
<title confidence="0.986938">Unrestricted Gapping Granunars</title>
<author confidence="0.984591">Fred Popowich</author>
<affiliation confidence="0.9852374">Natural Language Group for Computer and Communications Research Computing Science Department Simon Fraser University</affiliation>
<address confidence="0.969536">V5A 1S6</address>
<phone confidence="0.883786">pp. 765-768</phone>
<title confidence="0.9906825">with Assertion Sets and Information Monotonicity</title>
<author confidence="0.999849">G Edward Barton</author>
<author confidence="0.999849">Robert C Berwick</author>
<affiliation confidence="0.999979">MIT Artificial Intelligence Laboratory</affiliation>
<address confidence="0.9978085">545 Technology Square Cambridge, MA 02139</address>
<phone confidence="0.92334">pp. 769-771</phone>
<title confidence="0.9627295">Weighted Interaction of Syntax and Semantics in Natural Language Analysis</title>
<author confidence="0.998108">Leonard Lesmo</author>
<author confidence="0.998108">Pietro Torasso</author>
<affiliation confidence="0.998294">Dipartimento di Informatica Universita di Torino</affiliation>
<abstract confidence="0.985032785714286">Register Vector Grammar is a new kind of finite-state automaton that is sensitive to context — without, of course, being context-sensitive in the sense of Chomsky hierarchy. Traditional automata are functionally simple: match by identity and change by replacement. functionally complex: ternary feature vectors (e.g., + — + +— + +) match and change by masking (± matches but does not change any value). Functional complexity — as opposed to the computational complexity of nonfinite memory — is well suited for modelling multiple and discontinuous constraints. RVG is thus very good at handling the permutations and dependencies of syntax (wh-questions are explored as an example). Because center-embedding in natural languages is in fact very shallow and constrained, context-free power is not needed. RVG can thus be guaranteed to run in a small linear time, because it is FS, and yet can capture generations and constraints that functionally simple FS grammars cannot. This paper introduces an efficient context-free parsing algorithm and emphasizes its practical value in natural language processing. The algorithm can be viewed as an extended LR parsing algorithm which embodies the concept of a &amp;quot;graph-structured stack&amp;quot;. Unlike the standard LR, the algorithm is capable of handling arbitrary non-cyclic context-free grammars including ambiguous grammars, while most of the LR parsing efficiency is preserved. The algorithm seems more efficient than any existing algorithms including the Docke-Younger-Kasami algorithm and Earley&apos;s algorithm, as far as practical natural language is concerned, due to utilization of LR parsing tables. The algorithm is an all-path parsing algorithm; it produces all possible parse trees (a parse forest) in an efficient representation called a &amp;quot;shared-packed-forest&amp;quot;. This paper also shows that Earley&apos;s forest representation has a defect and his algorithm cannot be used in natural language processing as an all-path parsing algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Stephanie E August</author>
<author>G Michael</author>
</authors>
<institution>Dyer Artificial Intelligence Laboratory Computer Science Department, 3531 BH University of California</institution>
<marker>August, Michael, </marker>
<rawString>Stephanie E. August, Michael G. Dyer Artificial Intelligence Laboratory Computer Science Department, 3531 BH University of California</rawString>
</citation>
<citation valid="false">
<authors>
<author>Los Angeles</author>
</authors>
<booktitle>Integrating Text Planning and Production in Generation Eduard H. Hovy Yale University Artificial Intelligence Project 2158 Yale Station</booktitle>
<volume>90024</volume>
<pages>845--847</pages>
<marker>Angeles, </marker>
<rawString>Los Angeles, CA 90024 pp. 845-847 Integrating Text Planning and Production in Generation Eduard H. Hovy Yale University Artificial Intelligence Project 2158 Yale Station</rawString>
</citation>
<citation valid="false">
<authors>
<author>New Haven</author>
</authors>
<volume>06520</volume>
<pages>848--851</pages>
<marker>Haven, </marker>
<rawString>New Haven, CT 06520 pp. 848-851</rawString>
</citation>
<citation valid="false">
<authors>
<author>Be Brief</author>
</authors>
<title>Be to the Point, ... Be Seated or Relevant Responses in Man/Machine Conversation</title>
<marker>Brief, </marker>
<rawString>Be Brief, Be to the Point, ... Be Seated or Relevant Responses in Man/Machine Conversation</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anne Vilnat</author>
</authors>
<title>Gerard Sabah GR22,</title>
<booktitle>VI 4, Place Jussieu 75230 Paris Cedex 05</booktitle>
<pages>852--854</pages>
<location>Paris</location>
<marker>Vilnat, </marker>
<rawString>Anne Vilnat, Gerard Sabah GR22, Paris VI 4, Place Jussieu 75230 Paris Cedex 05 pp. 852-854</rawString>
</citation>
<citation valid="false">
<authors>
<author>SAPHIRRESEDA</author>
</authors>
<title>A New Approach to Intelligent Data Base Access Bernard Euzenat, Barnard Normier,</title>
<booktitle>Antoine Ogonowski ERLI 72 quai des Carrieres 94220 Charenton,</booktitle>
<location>France</location>
<marker>SAPHIRRESEDA, </marker>
<rawString>SAPHIR+RESEDA, A New Approach to Intelligent Data Base Access Bernard Euzenat, Barnard Normier, Antoine Ogonowski ERLI 72 quai des Carrieres 94220 Charenton, France</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gian Piero</author>
</authors>
<title>Zarri Centre National de la Recherche Scientique,</title>
<publisher>TECSIEL</publisher>
<location>Paris &amp;</location>
<marker>Piero, </marker>
<rawString>Gian Piero Zarri Centre National de la Recherche Scientique, Paris &amp; TECSIEL</rawString>
</citation>
<citation valid="true">
<authors>
<author>via Barnaba</author>
</authors>
<title>Oriani</title>
<date></date>
<volume>32</volume>
<pages>00197</pages>
<location>Roma,</location>
<marker>Barnaba, </marker>
<rawString>via Barnaba Oriani 32 00197 Roma, Italy pp. 855-857</rawString>
</citation>
<citation valid="false">
<title>RESEARCHER: An Experimental Intelligent Information System</title>
<institution>Michael Lebowitz Department of Computer Science Computer Science Building Columbia University</institution>
<marker></marker>
<rawString>RESEARCHER: An Experimental Intelligent Information System Michael Lebowitz Department of Computer Science Computer Science Building Columbia University</rawString>
</citation>
<citation valid="false">
<pages>10027--858</pages>
<location>New York, NY</location>
<marker></marker>
<rawString>New York, NY 10027 pp. 858-862</rawString>
</citation>
<citation valid="false">
<title>A Parallel-Process Model of On-Line Inference Processing</title>
<marker></marker>
<rawString>A Parallel-Process Model of On-Line Inference Processing</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kurt</author>
</authors>
<date></date>
<pages>863--869</pages>
<institution>Eisdt Irvine Computational Intelligence Project Computer Science Department University of California</institution>
<location>Irvine, CA</location>
<marker>Kurt, </marker>
<rawString>Kurt P. Eisdt Irvine Computational Intelligence Project Computer Science Department University of California Irvine, CA 92717 pp. 863-869</rawString>
</citation>
<citation valid="false">
<title>New Approaches to Parsing Conjunctions Using Prolog</title>
<marker></marker>
<rawString>New Approaches to Parsing Conjunctions Using Prolog</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>