<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006040">
<title confidence="0.999406">
Unsupervised Parsing for Generating Surface-Based
Relation Extraction Patterns
</title>
<author confidence="0.99757">
Jens Illig
</author>
<affiliation confidence="0.998308">
University of Kassel
</affiliation>
<address confidence="0.694469">
Wilhelmsh¨oher Allee 73
D-34121 Kassel, Germany
</address>
<email confidence="0.993011">
illig@cs.uni-kassel.de
</email>
<author confidence="0.979698">
Benjamin Roth and Dietrich Klakow
</author>
<affiliation confidence="0.981396">
Saarland University
</affiliation>
<address confidence="0.742431">
D-66123 Saarbr¨ucken, Germany
</address>
<email confidence="0.763253">
{benjamin.roth, dietrich.klakow}
@lsv.uni-saarland.de
</email>
<sectionHeader confidence="0.997286" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943466666667">
Finding the right features and patterns for
identifying relations in natural language is
one of the most pressing research ques-
tions for relation extraction. In this pa-
per, we compare patterns based on super-
vised and unsupervised syntactic parsing
and present a simple method for extract-
ing surface patterns from a parsed training
set. Results show that the use of surface-
based patterns not only increases extrac-
tion speed, but also improves the quality
of the extracted relations. We find that, in
this setting, unsupervised parsing, besides
requiring less resources, compares favor-
ably in terms of extraction quality.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859984126985">
Relation extraction is the task of automatically de-
tecting occurrences of expressed relations between
entities in a text and structuring the detected in-
formation in a tabularized form. In natural lan-
guage, there are infinitely many ways to creatively
express a set of semantic relations in accordance to
the syntax of the language. Languages vary across
domains and change over time. It is therefore im-
possible to statically capture all ways of express-
ing a relation.
Most relation extraction systems (Bunescu and
Mooney, 2005; Snow et al., 2005; Zhang et al.,
2006; Mintz et al., 2009; Alfonseca et al., 2012;
Min et al., 2012) generalize semantic relations
by taking into account statistics about the syntac-
tic construction of sentences. Usually supervised
parsers are applied for parsing sentences.
Statistics are then utilized to machine-learn how
textual mentions of relations can be identified.
Many researchers avoid the need for expensive
corpora with manually labeled relations by apply-
ing a scheme called distant supervision (Mintz et
al., 2009; Roth et al., 2013) which hypothesizes
that all text fragments containing argument co-
occurrences of known semantic relation facts in-
deed express these relations. Still, systems rely-
ing on supervised parsers require training from an-
notated treebanks, which are expensive to create,
and highly domain- and language dependent when
available.
An alternative is unsupervised parsing, which
automatically induces grammars by structurally
analyzing unlabeled corpora. Applying unsuper-
vised parsing thus avoids the limitation to lan-
guages and domains for which annotated data is
available. However, induced grammars do not
match traditional linguistic grammars. In most of
the research on parsing, unsupervised parsers are
still evaluated based on their level of correspon-
dence to treebanks. This is known to be prob-
lematic because there are several different ways of
linguistically analyzing text, and treebank anno-
tations also contain questionable analyses (Klein,
2005). Moreover, it is not guaranteed that the syn-
tactic analysis which is most conforming to a gen-
eral linguistic theory is also best suited in an ex-
trinsic evaluation, such as for relation extraction.
In this work, we apply a supervised and an un-
supervised parser to the relation extraction task by
extracting statistically counted patterns from the
resulting parses. By utilizing the performance of
the overall relation extraction system as an indirect
measure of a parser’s practical qualities, we get a
task-driven evaluation comparing supervised and
unsupervised parsers. To the best of our knowl-
edge, this is the first work to compare general-
purpose unsupervised and supervised parsing on
the application of relation extraction. Moreover,
we introduce a simple method to obtain shallow
patterns from syntactic analyses and show that, be-
sides eliminating the need to parse text during sys-
tem application, such patterns also increase extrac-
tion quality. We discover that, for this method, un-
</bodyText>
<page confidence="0.902953">
100
</page>
<note confidence="0.685379">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 100–105,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997845">
supervised parsing achieves better extraction qual-
ity than the more expensive supervised parsing.
</bodyText>
<subsectionHeader confidence="0.834845">
1.1 Related Work
</subsectionHeader>
<bodyText confidence="0.999949842105263">
Unsupervised and weakly supervised training
methods have been applied to relation extraction
(Mintz et al., 2009; Banko et al., 2007; Yates
and Etzioni, 2009) and similar applications such
as semantic parsing (Poon and Domingos, 2009)
and paraphrase acquisition (Lin and Pantel, 2001).
However, in such systems, parsing is commonly
applied as a separately trained subtask1 for which
supervision is used.
H¨anig and Schierle (2009) have applied unsu-
pervised parsing to a relation extraction task but
their task-specific data prohibits supervised pars-
ing for comparison.
Unsupervised parsing is traditionally only eval-
uated intrinsically by comparison to gold-standard
parses. In contrast, Reichart and Rappoport (2009)
count POS token sequences inside sub-phrases for
measuring parsing consistency. But this count is
not clearly related to application qualities.
</bodyText>
<sectionHeader confidence="0.995758" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999972904761905">
A complete relation extraction system consists of
multiple components. Our system follows the ar-
chitecture described by Roth et al. (2012). In
short, the system retrieves queries in the form
of entity names for which all relations captured
by the system are to be returned. The en-
tity names are expanded by alias-names extracted
from Wikipedia link anchor texts. An information
retrieval component retrieves documents contain-
ing either the name or one of the aliases. Further
filtering retains only sentences where a named en-
tity tagger labeled an occurrence of the queried
entity as being of a suitable type and furthermore
found a possible entity for the relation’s second ar-
gument. For each candidate sentence, a classifier
component then identifies whether one of the cap-
tured relation types is expressed and, if so, which
one it is. Postprocessing then outputs the classi-
fied relation according to task-specific format re-
quirements. Here, we focus on the relation type
classifier.
</bodyText>
<footnote confidence="0.552578">
1An exception is the joint syntactic and semantic (super-
vised) parsing model inference by Henderson et al. (2013)
</footnote>
<subsectionHeader confidence="0.994027">
2.1 Pattern Extraction
</subsectionHeader>
<bodyText confidence="0.999985700000001">
For our relation extraction system, we use a simple
pattern matching framework. Whenever at least
one candidate sentence containing two entities A
and B matches one of the patterns extracted for a
certain relation type R, the classifier states that R
holds between A and B.
We experimented with two types of patterns.
First, we simply parsed the training set and ex-
tracted shortest dependency path patterns. These
patterns search for matches on the parse tree.
Following Lin and Pantel (2001), the shortest
path connecting two arguments in a dependency
graph has been widely used as a representation
of relation instance mentions. The general idea
is that shortest paths skip over irrelevant op-
tional parts of a sentence such as in $1, who
... founded $2 where the shortest path pattern
$1←founded→$2 matches although an irrel-
evant relative clause appears between the argu-
ments $1 and $2. Similar representations have
been used by Mintz et al. (2009), Alfonseca et al.
(2012) and Snow et al. (2005).
In a second set of experiments, we used the
shortest dependency paths in parsed training sen-
tences to generate surface-based patterns. These
patterns search for matches directly on plain text
and therefore do no longer rely on parsing at appli-
cation time. The patterns are obtained by turning
the shortest paths between relational arguments in
the parsed training data into token sequences with
gaps. The token sequences consist of all words
in the sentence that appear on the shortest depen-
dency path. Argument positions in the surface pat-
terns are specified by special tokens $1 and $2.
At all places, where there are one or more tokens
which are not on the shortest dependency path but
which are surrounded either by tokens on the de-
pendency path or by arguments, an asterisk repre-
sents up to four unspecified tokens. For the short-
est path $1←,←who→$2 connecting Friedman
and economist in the DMV parse depicted in Fig-
ure 1, this method generates the pattern $1, *
$2 who. As can be seen, such patterns can cap-
ture a conjunction of token presence conditions to
the left, between, and to the right of the arguments.
In cases where argument entities are not parsed as
a single complete phrase, we generate patterns for
each possible combination of outgoing edges from
the two arguments. We dismiss patterns generated
for less than four distinct argument entity pairs of
</bodyText>
<page confidence="0.983409">
101
</page>
<figure confidence="0.999726130434783">
DMV root
Milton Friedman , a conservative economist who died in 2006 at age 94 , received the Nobel Prize for economics in 1976 .
pobj num
nn
appos
punct
amod
det
nsubj
rcmod
prep pobj
prep
dobj
prep
nn
det
pobj
pobj
MALT root
prep
punct
punct
nsubj
</figure>
<figureCaption confidence="0.999996">
Figure 1: Comparison of a DMV (above text) and a MALT parse (below text) of the same sentence.
</figureCaption>
<bodyText confidence="0.998861333333333">
the same relation type. For each pattern, we cal-
culate the precision on the training set and retain
only patterns above a certain precision threshold.
</bodyText>
<subsectionHeader confidence="0.98984">
2.2 Supervised and Unsupervised Parsing
</subsectionHeader>
<bodyText confidence="0.999990212121212">
Typical applications which require syntactic anal-
yses make use of a parser that has been trained un-
der supervision of a labeled corpus conforming to
a linguistically engineered grammar. In contrast,
unsupervised parsing induces a grammar from fre-
quency structures in plain text.
Various algorithms for unsupervised parsing
have been developed in the past decades. Head-
den (2012) gives a rather recent and extensive
overview of unsupervised parsing models. For our
work, we use the Dependency Model with Valence
(DMV) by Klein and Manning (2004). Most of
the more recent unsupervised dependency pars-
ing research is based on this model. DMV is a
generative head-outward parsing model which is
trained by expectation maximization on part-of-
speech (POS) sequences of the input sentences.
Starting from a single root token, head tokens gen-
erate dependants by a probability conditioned on
the direction (left/right) from the head and the
head’s token type. Each head node generates to-
kens until a stop event is generated with a prob-
ability dependent on the same criteria plus a flag
whether some dependant token has already been
generated in the same direction.
For comparison of unsupervised and supervised
parsing, we apply the (Nivre, 2003) determinis-
tic incremental parsing algorithm Nivre arc-eager,
the default algorithm of the MALT framework2
(Nivre et al., 2007). In this model, for each word
token, an SVM classifier decides for a parser state
transition, which, in conjunction with other deci-
sions, determines where phrases begin and end.
</bodyText>
<footnote confidence="0.968531">
2http://www.maltparser.org as of Nov. 2013
</footnote>
<sectionHeader confidence="0.999415" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9999836875">
We used the plain text documents of the English
Newswire and Web Text Documents provided for
TAC KBP challenge 2011 (Ji et al., 2011). We
automatically annotated relation type mentions in
these documents by distant supervision using the
online database Freebase3, i.e. for all relation
types of TAC KBP 2011, we took relation triples
from Freebase and, applying preprocessing as de-
scribed in Section 2, we retrieved sentences men-
tioning both arguments of some Freebase relation
with matching predicted entity types. We hypothe-
size that all sentences express the respective Free-
base relation. This way we retrieved a distantly
supervised training set of 480 622 English sen-
tences containing 92468 distinct relation instances
instantiating 41 TAC KBP relation types.
</bodyText>
<subsectionHeader confidence="0.998143">
3.1 Training and Evaluation
</subsectionHeader>
<bodyText confidence="0.999789285714286">
From our retrieved set of sentences, we took those
with a maximum length of 10 tokens and trans-
formed them to POS sequences. We trained DMV
only on this dataset of short POS sequences, which
we expect to form mentions of a modeled relation.
Therefore, we suspect that DMV training assigns
an increased amount of probability mass to depen-
dency paths along structures which are truly re-
lated to these relations. We used the DMV imple-
mentation from Cohen and Smith (2009) 4.
For the supervised Nivre arc-eager parser we
used MALT (Nivre et al., 2007) with a pre-trained
Penn Treebank (Marcus et al., 1993) model5. As
a baseline, we tested left branching parses i.e.
</bodyText>
<footnote confidence="0.9860385">
3http://www.freebase.com as of Nov. 2013
4publicly available at http://www.ark.cs.cmu.
edu/DAGEEM/ as of Nov. 2013 (parser version 1.0).
5http://www.maltparser.org/mco/
english_parser/engmalt.linear-1.7.mco
as of Nov. 2013
</footnote>
<page confidence="0.995618">
102
</page>
<figure confidence="0.999675515151515">
micro-average KBP F1
0.5
0.4
0.3
precision
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
threshold on pattern-precision
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
recall
0.18
0.16
0.14
0.12
0.08
0.06
0.04
0.02
0.2
0.1
0
lbranch
dmv surface
dmv dep-graph
malt surface
malt dep-graph
lbranch
dmv surface
dmv dep-graph
malt surface
malt dep-graph
</figure>
<figureCaption confidence="0.999746">
Figure 2: micro-averaged F1 and precision&amp;recall results for varied training precision thresholds
</figureCaption>
<table confidence="0.997530636363637">
pattern set (+additional DMV pattern) precision recall Fr
MALT generated patterns only .1769 .2010 .1882
+p:title $1 * $2 of +0.73% +8.40% +4.14%
+p:title $1 , * $2 of +0.90% +4.22% +2.39%
+o:state of hqs $1 * in * , $2 +1.35% +1.59% +1.43%
+p:title $1 , * $2 who +0.90% +1.35% +1.22%
+o:parents $1 , * by $2 +0.62% +1.35% +1.06%
+o:city of hqs $1 , * in $2 , +1.01% +1.04% +1.00%
+p:origin $2 ’s $1 won the +0.84% +1.04% +0.95%
+p:employee of $1 * $2 ’s chief +0.28% +1.04% +0.79%
+o:website $1 : $2 +0.28% +1.04% +0.79%
</table>
<tableCaption confidence="0.787215">
Table 1: DMV patterns improving MALT results
the most, when added to the MALT patternset
</tableCaption>
<bodyText confidence="0.9976887">
dependency trees solely consisting of head-to-
dependent edges from the right to the left6.
All the extracted sentences were parsed and pat-
terns were extracted from the parses. The patterns
were then applied to the corpus and their precision
was determined according to Freebase. With dif-
ferent cut-off values on training precision, the full
relation extraction pipeline described in Section 2
was evaluated with respect to the Slot Filling test
queries of TAC KBP 2011.
</bodyText>
<subsectionHeader confidence="0.979231">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.996865363636364">
Figure 2 (left) depicts F1-measured testset results
for pattern sets with varying training precision
thresholds. Figure 2 (right) shows a precision re-
call plot of the same data points.
As can be seen in Figure 2 (left), flattening
graph patterns to surface-based patterns increased
the overall F1 score. The curve for MALT gen-
erated surface patterns in Figure 2 (right) shows
no increase in precision towards low recall levels
where only the highest-training-precision patterns
are retained. This indicates a lack of precision
</bodyText>
<footnote confidence="0.488854">
6Since for such parses the shortest path is the complete
observed word sequence between the two relation arguments,
surface and parse-tree patterns become equal.
</footnote>
<bodyText confidence="0.999759571428571">
in MALT-based surface patterns. In contrast, the
corresponding DMV-based graph increases mono-
tonically towards lower recall levels, which is re-
flected by the highest F1 score (Figure 2, left).
Table 1 shows the increases in evaluation score
of those DMV-generated patterns which help most
to more precisely identify relations when added to
the set of all MALT-generated patterns (sorted by
F1 score). Figure 1 compares the syntactic analy-
ses of MALT and DMV for an example sentence
where DMV generates one of the listed patterns.
The numbers of Table 1 indicate that such patterns
are missing without alternatives in the pattern set
gained from supervised parsing.
</bodyText>
<sectionHeader confidence="0.997438" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999725">
We have presented a simple method for generat-
ing surface-based patterns from parse trees which,
besides avoiding the need for parsing test data,
also increases extraction quality. By comparing
supervised and unsupervised parsing, we further-
more found that unsupervised parsing not only
eliminates the dependency on expensive domain-
specific training data, but also produce surface-
based extraction patterns of increased quality. Our
results emphasize the need for task-driven evalu-
ation of unsupervised parsing methods and show
that there exist indicative structures for relation ex-
traction beyond widely agreed-on linguistic syntax
analyses.
</bodyText>
<sectionHeader confidence="0.992286" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.581841">
Benjamin Roth is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by this Google
Fellowship.
</bodyText>
<page confidence="0.999251">
103
</page>
<sectionHeader confidence="0.996306" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999779834862385">
Enrique Alfonseca, Katja Filippova, Jean-Yves Delort,
and Guillermo Garrido. 2012. Pattern learning for
relation extraction with a hierarchical topic model.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Short Pa-
pers - Volume 2, ACL ’12, pages 54–59, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the 20th International Joint Conference
on Artifical Intelligence, IJCAI’07, pages 2670–
2676, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ’05, pages
724–731, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, NAACL
’09, pages 74–82, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Christian H¨anig and Martin Schierle. 2009. Rela-
tion extraction based on unsupervised syntactic pars-
ing. In Gerhard Heyer, editor, Text Mining Ser-
vices, Leipziger Beitr¨age zur Informatik, pages 65–
70, Leipzig, Germany. Leipzig University.
William Headden. 2012. Unsupervised Bayesian Lexi-
calized Dependency Grammar Induction. Ph.D. the-
sis, Brown University.
James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multi-lingual joint parsing
of syntactic and semantic dependencies with a latent
variable model. Computational Linguistics, 39(4).
Heng Ji, Ralph Grishman, and Hoa Dang. 2011.
Overview of the TAC2011 knowledge base popula-
tion track. In TAC 2011 Proceedings Papers.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In ACL, ACL
’04, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Dan Klein. 2005. The Unsupervised Learning of Natu-
ral Language Structure. Ph.D. thesis, Stanford Uni-
versity.
Dekang Lin and Patrick Pantel. 2001. DIRT: Discov-
ery of Inference Rules from Text. In Proceedings
of the Seventh ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining
(KDD’01), pages 323–328, New York, NY, USA.
ACM Press.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Comput.
Linguist., 19(2):313–330, June.
Bonan Min, Xiang Li, Ralph Grishman, and Sun Ang.
2012. New york university 2012 system for kbp
slot filling. In Proceedings of the Fifth Text Analysis
Conference (TAC 2012). National Institute of Stan-
dards and Technology (NIST), November.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 1003–1011. Association for Computational
Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149–160.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP
’09, pages 1–10, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Roi Reichart and Ari Rappoport. 2009. Automatic se-
lection of high quality parses created by a fully un-
supervised parser. In Proceedings of the Thirteenth
Conference on Computational Natural Language
Learning, CoNLL ’09, pages 156–164, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Benjamin Roth, Grzegorz Chrupala, Michael Wiegand,
Singh Mittul, and Klakow Dietrich. 2012. General-
izing from freebase and patterns using cluster-based
distant supervision for tac kbp slotfilling 2012. In
Proceedings of the Fifth Text Analysis Conference
(TAC 2012), Gaithersburg, Maryland, USA, Novem-
ber. National Institute of Standards and Technology
(NIST).
Benjamin Roth, Tassilo Barth, Michael Wiegand, and
Dietrich Klakow. 2013. A survey of noise reduction
</reference>
<page confidence="0.984101">
104
</page>
<reference confidence="0.997228904761905">
methods for distant supervision. In Proceedings of
the 2013 workshop on Automated knowledge base
construction, pages 73–78. ACM.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L´eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297–1304. MIT
Press, Cambridge, MA.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. J. Artif. Int. Res., 34(1):255–
296, March.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 825–832, Stroudsburg, PA,
USA. Association for Computational Linguistics.
</reference>
<page confidence="0.999021">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418587">
<title confidence="0.999722">Unsupervised Parsing for Generating Relation Extraction Patterns</title>
<author confidence="0.990789">Jens</author>
<affiliation confidence="0.9591515">University of Wilhelmsh¨oher Allee</affiliation>
<address confidence="0.985438">D-34121 Kassel,</address>
<email confidence="0.997034">illig@cs.uni-kassel.de</email>
<author confidence="0.554619">Roth</author>
<affiliation confidence="0.74981">Saarland</affiliation>
<address confidence="0.901916">D-66123 Saarbr¨ucken,</address>
<email confidence="0.995885">@lsv.uni-saarland.de</email>
<abstract confidence="0.9985271875">Finding the right features and patterns for identifying relations in natural language is one of the most pressing research questions for relation extraction. In this paper, we compare patterns based on supervised and unsupervised syntactic parsing and present a simple method for extracting surface patterns from a parsed training set. Results show that the use of surfacebased patterns not only increases extraction speed, but also improves the quality of the extracted relations. We find that, in this setting, unsupervised parsing, besides requiring less resources, compares favorably in terms of extraction quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Katja Filippova</author>
<author>Jean-Yves Delort</author>
<author>Guillermo Garrido</author>
</authors>
<title>Pattern learning for relation extraction with a hierarchical topic model.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>54--59</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1573" citStr="Alfonseca et al., 2012" startWordPosition="230" endWordPosition="233">Introduction Relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form. In natural language, there are infinitely many ways to creatively express a set of semantic relations in accordance to the syntax of the language. Languages vary across domains and change over time. It is therefore impossible to statically capture all ways of expressing a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et al., 2009; Roth et al., 2013) which hypothesizes that all text fragments containing argument cooccurrences of known semantic relation facts indeed express these rel</context>
<context position="7278" citStr="Alfonseca et al. (2012)" startWordPosition="1101" endWordPosition="1104">ng set and extracted shortest dependency path patterns. These patterns search for matches on the parse tree. Following Lin and Pantel (2001), the shortest path connecting two arguments in a dependency graph has been widely used as a representation of relation instance mentions. The general idea is that shortest paths skip over irrelevant optional parts of a sentence such as in $1, who ... founded $2 where the shortest path pattern $1←founded→$2 matches although an irrelevant relative clause appears between the arguments $1 and $2. Similar representations have been used by Mintz et al. (2009), Alfonseca et al. (2012) and Snow et al. (2005). In a second set of experiments, we used the shortest dependency paths in parsed training sentences to generate surface-based patterns. These patterns search for matches directly on plain text and therefore do no longer rely on parsing at application time. The patterns are obtained by turning the shortest paths between relational arguments in the parsed training data into token sequences with gaps. The token sequences consist of all words in the sentence that appear on the shortest dependency path. Argument positions in the surface patterns are specified by special toke</context>
</contexts>
<marker>Alfonseca, Filippova, Delort, Garrido, 2012</marker>
<rawString>Enrique Alfonseca, Katja Filippova, Jean-Yves Delort, and Guillermo Garrido. 2012. Pattern learning for relation extraction with a hierarchical topic model. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, pages 54–59, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI’07,</booktitle>
<pages>2670--2676</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4436" citStr="Banko et al., 2007" startWordPosition="656" endWordPosition="659"> besides eliminating the need to parse text during system application, such patterns also increase extraction quality. We discover that, for this method, un100 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 100–105, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics supervised parsing achieves better extraction quality than the more expensive supervised parsing. 1.1 Related Work Unsupervised and weakly supervised training methods have been applied to relation extraction (Mintz et al., 2009; Banko et al., 2007; Yates and Etzioni, 2009) and similar applications such as semantic parsing (Poon and Domingos, 2009) and paraphrase acquisition (Lin and Pantel, 2001). However, in such systems, parsing is commonly applied as a separately trained subtask1 for which supervision is used. H¨anig and Schierle (2009) have applied unsupervised parsing to a relation extraction task but their task-specific data prohibits supervised parsing for comparison. Unsupervised parsing is traditionally only evaluated intrinsically by comparison to gold-standard parses. In contrast, Reichart and Rappoport (2009) count POS toke</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI’07, pages 2670– 2676, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>724--731</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1490" citStr="Bunescu and Mooney, 2005" startWordPosition="214" endWordPosition="217">sides requiring less resources, compares favorably in terms of extraction quality. 1 Introduction Relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form. In natural language, there are infinitely many ways to creatively express a set of semantic relations in accordance to the syntax of the language. Languages vary across domains and change over time. It is therefore impossible to statically capture all ways of expressing a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et al., 2009; Roth et al., 2013) which hypothesizes that all text fragments containi</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 724–731, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>74--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12075" citStr="Cohen and Smith (2009)" startWordPosition="1893" endWordPosition="1896">aining set of 480 622 English sentences containing 92468 distinct relation instances instantiating 41 TAC KBP relation types. 3.1 Training and Evaluation From our retrieved set of sentences, we took those with a maximum length of 10 tokens and transformed them to POS sequences. We trained DMV only on this dataset of short POS sequences, which we expect to form mentions of a modeled relation. Therefore, we suspect that DMV training assigns an increased amount of probability mass to dependency paths along structures which are truly related to these relations. We used the DMV implementation from Cohen and Smith (2009) 4. For the supervised Nivre arc-eager parser we used MALT (Nivre et al., 2007) with a pre-trained Penn Treebank (Marcus et al., 1993) model5. As a baseline, we tested left branching parses i.e. 3http://www.freebase.com as of Nov. 2013 4publicly available at http://www.ark.cs.cmu. edu/DAGEEM/ as of Nov. 2013 (parser version 1.0). 5http://www.maltparser.org/mco/ english_parser/engmalt.linear-1.7.mco as of Nov. 2013 102 micro-average KBP F1 0.5 0.4 0.3 precision 0.2 0.1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 threshold on pattern-precision 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 recall 0.18 0.16</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 74–82, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian H¨anig</author>
<author>Martin Schierle</author>
</authors>
<title>Relation extraction based on unsupervised syntactic parsing.</title>
<date>2009</date>
<booktitle>Text Mining Services, Leipziger Beitr¨age zur Informatik,</booktitle>
<pages>65--70</pages>
<editor>In Gerhard Heyer, editor,</editor>
<location>Leipzig, Germany. Leipzig University.</location>
<marker>H¨anig, Schierle, 2009</marker>
<rawString>Christian H¨anig and Martin Schierle. 2009. Relation extraction based on unsupervised syntactic parsing. In Gerhard Heyer, editor, Text Mining Services, Leipziger Beitr¨age zur Informatik, pages 65– 70, Leipzig, Germany. Leipzig University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Headden</author>
</authors>
<title>Unsupervised Bayesian Lexicalized Dependency Grammar Induction.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="9592" citStr="Headden (2012)" startWordPosition="1496" endWordPosition="1498">ALT parse (below text) of the same sentence. the same relation type. For each pattern, we calculate the precision on the training set and retain only patterns above a certain precision threshold. 2.2 Supervised and Unsupervised Parsing Typical applications which require syntactic analyses make use of a parser that has been trained under supervision of a labeled corpus conforming to a linguistically engineered grammar. In contrast, unsupervised parsing induces a grammar from frequency structures in plain text. Various algorithms for unsupervised parsing have been developed in the past decades. Headden (2012) gives a rather recent and extensive overview of unsupervised parsing models. For our work, we use the Dependency Model with Valence (DMV) by Klein and Manning (2004). Most of the more recent unsupervised dependency parsing research is based on this model. DMV is a generative head-outward parsing model which is trained by expectation maximization on part-ofspeech (POS) sequences of the input sentences. Starting from a single root token, head tokens generate dependants by a probability conditioned on the direction (left/right) from the head and the head’s token type. Each head node generates to</context>
</contexts>
<marker>Headden, 2012</marker>
<rawString>William Headden. 2012. Unsupervised Bayesian Lexicalized Dependency Grammar Induction. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Ivan Titov</author>
<author>Gabriele Musillo</author>
</authors>
<title>Multi-lingual joint parsing of syntactic and semantic dependencies with a latent variable model.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="6279" citStr="Henderson et al. (2013)" startWordPosition="937" endWordPosition="940">filtering retains only sentences where a named entity tagger labeled an occurrence of the queried entity as being of a suitable type and furthermore found a possible entity for the relation’s second argument. For each candidate sentence, a classifier component then identifies whether one of the captured relation types is expressed and, if so, which one it is. Postprocessing then outputs the classified relation according to task-specific format requirements. Here, we focus on the relation type classifier. 1An exception is the joint syntactic and semantic (supervised) parsing model inference by Henderson et al. (2013) 2.1 Pattern Extraction For our relation extraction system, we use a simple pattern matching framework. Whenever at least one candidate sentence containing two entities A and B matches one of the patterns extracted for a certain relation type R, the classifier states that R holds between A and B. We experimented with two types of patterns. First, we simply parsed the training set and extracted shortest dependency path patterns. These patterns search for matches on the parse tree. Following Lin and Pantel (2001), the shortest path connecting two arguments in a dependency graph has been widely u</context>
</contexts>
<marker>Henderson, Merlo, Titov, Musillo, 2013</marker>
<rawString>James Henderson, Paola Merlo, Ivan Titov, and Gabriele Musillo. 2013. Multi-lingual joint parsing of syntactic and semantic dependencies with a latent variable model. Computational Linguistics, 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Hoa Dang</author>
</authors>
<title>Overview of the TAC2011 knowledge base population track.</title>
<date>2011</date>
<booktitle>In TAC 2011 Proceedings Papers.</booktitle>
<contexts>
<context position="10947" citStr="Ji et al., 2011" startWordPosition="1712" endWordPosition="1715"> been generated in the same direction. For comparison of unsupervised and supervised parsing, we apply the (Nivre, 2003) deterministic incremental parsing algorithm Nivre arc-eager, the default algorithm of the MALT framework2 (Nivre et al., 2007). In this model, for each word token, an SVM classifier decides for a parser state transition, which, in conjunction with other decisions, determines where phrases begin and end. 2http://www.maltparser.org as of Nov. 2013 3 Experiments We used the plain text documents of the English Newswire and Web Text Documents provided for TAC KBP challenge 2011 (Ji et al., 2011). We automatically annotated relation type mentions in these documents by distant supervision using the online database Freebase3, i.e. for all relation types of TAC KBP 2011, we took relation triples from Freebase and, applying preprocessing as described in Section 2, we retrieved sentences mentioning both arguments of some Freebase relation with matching predicted entity types. We hypothesize that all sentences express the respective Freebase relation. This way we retrieved a distantly supervised training set of 480 622 English sentences containing 92468 distinct relation instances instantia</context>
</contexts>
<marker>Ji, Grishman, Dang, 2011</marker>
<rawString>Heng Ji, Ralph Grishman, and Hoa Dang. 2011. Overview of the TAC2011 knowledge base population track. In TAC 2011 Proceedings Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9758" citStr="Klein and Manning (2004)" startWordPosition="1522" endWordPosition="1525">ns above a certain precision threshold. 2.2 Supervised and Unsupervised Parsing Typical applications which require syntactic analyses make use of a parser that has been trained under supervision of a labeled corpus conforming to a linguistically engineered grammar. In contrast, unsupervised parsing induces a grammar from frequency structures in plain text. Various algorithms for unsupervised parsing have been developed in the past decades. Headden (2012) gives a rather recent and extensive overview of unsupervised parsing models. For our work, we use the Dependency Model with Valence (DMV) by Klein and Manning (2004). Most of the more recent unsupervised dependency parsing research is based on this model. DMV is a generative head-outward parsing model which is trained by expectation maximization on part-ofspeech (POS) sequences of the input sentences. Starting from a single root token, head tokens generate dependants by a probability conditioned on the direction (left/right) from the head and the head’s token type. Each head node generates tokens until a stop event is generated with a probability dependent on the same criteria plus a flag whether some dependant token has already been generated in the same</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpus-based induction of syntactic structure: models of dependency and constituency. In ACL, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>The Unsupervised Learning of Natural Language Structure.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="2983" citStr="Klein, 2005" startWordPosition="437" endWordPosition="438">e is unsupervised parsing, which automatically induces grammars by structurally analyzing unlabeled corpora. Applying unsupervised parsing thus avoids the limitation to languages and domains for which annotated data is available. However, induced grammars do not match traditional linguistic grammars. In most of the research on parsing, unsupervised parsers are still evaluated based on their level of correspondence to treebanks. This is known to be problematic because there are several different ways of linguistically analyzing text, and treebank annotations also contain questionable analyses (Klein, 2005). Moreover, it is not guaranteed that the syntactic analysis which is most conforming to a general linguistic theory is also best suited in an extrinsic evaluation, such as for relation extraction. In this work, we apply a supervised and an unsupervised parser to the relation extraction task by extracting statistically counted patterns from the resulting parses. By utilizing the performance of the overall relation extraction system as an indirect measure of a parser’s practical qualities, we get a task-driven evaluation comparing supervised and unsupervised parsers. To the best of our knowledg</context>
</contexts>
<marker>Klein, 2005</marker>
<rawString>Dan Klein. 2005. The Unsupervised Learning of Natural Language Structure. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT: Discovery of Inference Rules from Text.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’01),</booktitle>
<pages>323--328</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4588" citStr="Lin and Pantel, 2001" startWordPosition="678" endWordPosition="681">hod, un100 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 100–105, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics supervised parsing achieves better extraction quality than the more expensive supervised parsing. 1.1 Related Work Unsupervised and weakly supervised training methods have been applied to relation extraction (Mintz et al., 2009; Banko et al., 2007; Yates and Etzioni, 2009) and similar applications such as semantic parsing (Poon and Domingos, 2009) and paraphrase acquisition (Lin and Pantel, 2001). However, in such systems, parsing is commonly applied as a separately trained subtask1 for which supervision is used. H¨anig and Schierle (2009) have applied unsupervised parsing to a relation extraction task but their task-specific data prohibits supervised parsing for comparison. Unsupervised parsing is traditionally only evaluated intrinsically by comparison to gold-standard parses. In contrast, Reichart and Rappoport (2009) count POS token sequences inside sub-phrases for measuring parsing consistency. But this count is not clearly related to application qualities. 2 Methodology A comple</context>
<context position="6795" citStr="Lin and Pantel (2001)" startWordPosition="1021" endWordPosition="1024">ption is the joint syntactic and semantic (supervised) parsing model inference by Henderson et al. (2013) 2.1 Pattern Extraction For our relation extraction system, we use a simple pattern matching framework. Whenever at least one candidate sentence containing two entities A and B matches one of the patterns extracted for a certain relation type R, the classifier states that R holds between A and B. We experimented with two types of patterns. First, we simply parsed the training set and extracted shortest dependency path patterns. These patterns search for matches on the parse tree. Following Lin and Pantel (2001), the shortest path connecting two arguments in a dependency graph has been widely used as a representation of relation instance mentions. The general idea is that shortest paths skip over irrelevant optional parts of a sentence such as in $1, who ... founded $2 where the shortest path pattern $1←founded→$2 matches although an irrelevant relative clause appears between the arguments $1 and $2. Similar representations have been used by Mintz et al. (2009), Alfonseca et al. (2012) and Snow et al. (2005). In a second set of experiments, we used the shortest dependency paths in parsed training sen</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT: Discovery of Inference Rules from Text. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’01), pages 323–328, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="12209" citStr="Marcus et al., 1993" startWordPosition="1916" endWordPosition="1919">ng and Evaluation From our retrieved set of sentences, we took those with a maximum length of 10 tokens and transformed them to POS sequences. We trained DMV only on this dataset of short POS sequences, which we expect to form mentions of a modeled relation. Therefore, we suspect that DMV training assigns an increased amount of probability mass to dependency paths along structures which are truly related to these relations. We used the DMV implementation from Cohen and Smith (2009) 4. For the supervised Nivre arc-eager parser we used MALT (Nivre et al., 2007) with a pre-trained Penn Treebank (Marcus et al., 1993) model5. As a baseline, we tested left branching parses i.e. 3http://www.freebase.com as of Nov. 2013 4publicly available at http://www.ark.cs.cmu. edu/DAGEEM/ as of Nov. 2013 (parser version 1.0). 5http://www.maltparser.org/mco/ english_parser/engmalt.linear-1.7.mco as of Nov. 2013 102 micro-average KBP F1 0.5 0.4 0.3 precision 0.2 0.1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 threshold on pattern-precision 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 recall 0.18 0.16 0.14 0.12 0.08 0.06 0.04 0.02 0.2 0.1 0 lbranch dmv surface dmv dep-graph malt surface malt dep-graph lbranch dmv surface dmv dep-gra</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Comput. Linguist., 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Xiang Li</author>
<author>Ralph Grishman</author>
<author>Sun Ang</author>
</authors>
<title>New york university 2012 system for kbp slot filling.</title>
<date>2012</date>
<booktitle>In Proceedings of the Fifth Text Analysis Conference (TAC 2012). National Institute of Standards and Technology (NIST),</booktitle>
<contexts>
<context position="1592" citStr="Min et al., 2012" startWordPosition="234" endWordPosition="237">traction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form. In natural language, there are infinitely many ways to creatively express a set of semantic relations in accordance to the syntax of the language. Languages vary across domains and change over time. It is therefore impossible to statically capture all ways of expressing a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et al., 2009; Roth et al., 2013) which hypothesizes that all text fragments containing argument cooccurrences of known semantic relation facts indeed express these relations. Still, syst</context>
</contexts>
<marker>Min, Li, Grishman, Ang, 2012</marker>
<rawString>Bonan Min, Xiang Li, Ralph Grishman, and Sun Ang. 2012. New york university 2012 system for kbp slot filling. In Proceedings of the Fifth Text Analysis Conference (TAC 2012). National Institute of Standards and Technology (NIST), November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mintz</author>
<author>S Bills</author>
<author>R Snow</author>
<author>D Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1549" citStr="Mintz et al., 2009" startWordPosition="226" endWordPosition="229">traction quality. 1 Introduction Relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form. In natural language, there are infinitely many ways to creatively express a set of semantic relations in accordance to the syntax of the language. Languages vary across domains and change over time. It is therefore impossible to statically capture all ways of expressing a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et al., 2009; Roth et al., 2013) which hypothesizes that all text fragments containing argument cooccurrences of known semantic relation facts </context>
<context position="4416" citStr="Mintz et al., 2009" startWordPosition="652" endWordPosition="655">lyses and show that, besides eliminating the need to parse text during system application, such patterns also increase extraction quality. We discover that, for this method, un100 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 100–105, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics supervised parsing achieves better extraction quality than the more expensive supervised parsing. 1.1 Related Work Unsupervised and weakly supervised training methods have been applied to relation extraction (Mintz et al., 2009; Banko et al., 2007; Yates and Etzioni, 2009) and similar applications such as semantic parsing (Poon and Domingos, 2009) and paraphrase acquisition (Lin and Pantel, 2001). However, in such systems, parsing is commonly applied as a separately trained subtask1 for which supervision is used. H¨anig and Schierle (2009) have applied unsupervised parsing to a relation extraction task but their task-specific data prohibits supervised parsing for comparison. Unsupervised parsing is traditionally only evaluated intrinsically by comparison to gold-standard parses. In contrast, Reichart and Rappoport (</context>
<context position="7253" citStr="Mintz et al. (2009)" startWordPosition="1097" endWordPosition="1100">ply parsed the training set and extracted shortest dependency path patterns. These patterns search for matches on the parse tree. Following Lin and Pantel (2001), the shortest path connecting two arguments in a dependency graph has been widely used as a representation of relation instance mentions. The general idea is that shortest paths skip over irrelevant optional parts of a sentence such as in $1, who ... founded $2 where the shortest path pattern $1←founded→$2 matches although an irrelevant relative clause appears between the arguments $1 and $2. Similar representations have been used by Mintz et al. (2009), Alfonseca et al. (2012) and Snow et al. (2005). In a second set of experiments, we used the shortest dependency paths in parsed training sentences to generate surface-based patterns. These patterns search for matches directly on plain text and therefore do no longer rely on parsing at application time. The patterns are obtained by turning the shortest paths between relational arguments in the parsed training data into token sequences with gaps. The token sequences consist of all words in the sentence that appear on the shortest dependency path. Argument positions in the surface patterns are </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="10578" citStr="Nivre et al., 2007" startWordPosition="1652" endWordPosition="1655">POS) sequences of the input sentences. Starting from a single root token, head tokens generate dependants by a probability conditioned on the direction (left/right) from the head and the head’s token type. Each head node generates tokens until a stop event is generated with a probability dependent on the same criteria plus a flag whether some dependant token has already been generated in the same direction. For comparison of unsupervised and supervised parsing, we apply the (Nivre, 2003) deterministic incremental parsing algorithm Nivre arc-eager, the default algorithm of the MALT framework2 (Nivre et al., 2007). In this model, for each word token, an SVM classifier decides for a parser state transition, which, in conjunction with other decisions, determines where phrases begin and end. 2http://www.maltparser.org as of Nov. 2013 3 Experiments We used the plain text documents of the English Newswire and Web Text Documents provided for TAC KBP challenge 2011 (Ji et al., 2011). We automatically annotated relation type mentions in these documents by distant supervision using the online database Freebase3, i.e. for all relation types of TAC KBP 2011, we took relation triples from Freebase and, applying pr</context>
<context position="12154" citStr="Nivre et al., 2007" startWordPosition="1907" endWordPosition="1910">es instantiating 41 TAC KBP relation types. 3.1 Training and Evaluation From our retrieved set of sentences, we took those with a maximum length of 10 tokens and transformed them to POS sequences. We trained DMV only on this dataset of short POS sequences, which we expect to form mentions of a modeled relation. Therefore, we suspect that DMV training assigns an increased amount of probability mass to dependency paths along structures which are truly related to these relations. We used the DMV implementation from Cohen and Smith (2009) 4. For the supervised Nivre arc-eager parser we used MALT (Nivre et al., 2007) with a pre-trained Penn Treebank (Marcus et al., 1993) model5. As a baseline, we tested left branching parses i.e. 3http://www.freebase.com as of Nov. 2013 4publicly available at http://www.ark.cs.cmu. edu/DAGEEM/ as of Nov. 2013 (parser version 1.0). 5http://www.maltparser.org/mco/ english_parser/engmalt.linear-1.7.mco as of Nov. 2013 102 micro-average KBP F1 0.5 0.4 0.3 precision 0.2 0.1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 threshold on pattern-precision 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 recall 0.18 0.16 0.14 0.12 0.08 0.06 0.04 0.02 0.2 0.1 0 lbranch dmv surface dmv dep-graph malt</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<contexts>
<context position="10451" citStr="Nivre, 2003" startWordPosition="1636" endWordPosition="1637">is model. DMV is a generative head-outward parsing model which is trained by expectation maximization on part-ofspeech (POS) sequences of the input sentences. Starting from a single root token, head tokens generate dependants by a probability conditioned on the direction (left/right) from the head and the head’s token type. Each head node generates tokens until a stop event is generated with a probability dependent on the same criteria plus a flag whether some dependant token has already been generated in the same direction. For comparison of unsupervised and supervised parsing, we apply the (Nivre, 2003) deterministic incremental parsing algorithm Nivre arc-eager, the default algorithm of the MALT framework2 (Nivre et al., 2007). In this model, for each word token, an SVM classifier decides for a parser state transition, which, in conjunction with other decisions, determines where phrases begin and end. 2http://www.maltparser.org as of Nov. 2013 3 Experiments We used the plain text documents of the English Newswire and Web Text Documents provided for TAC KBP challenge 2011 (Ji et al., 2011). We automatically annotated relation type mentions in these documents by distant supervision using the </context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4538" citStr="Poon and Domingos, 2009" startWordPosition="671" endWordPosition="674">se extraction quality. We discover that, for this method, un100 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 100–105, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics supervised parsing achieves better extraction quality than the more expensive supervised parsing. 1.1 Related Work Unsupervised and weakly supervised training methods have been applied to relation extraction (Mintz et al., 2009; Banko et al., 2007; Yates and Etzioni, 2009) and similar applications such as semantic parsing (Poon and Domingos, 2009) and paraphrase acquisition (Lin and Pantel, 2001). However, in such systems, parsing is commonly applied as a separately trained subtask1 for which supervision is used. H¨anig and Schierle (2009) have applied unsupervised parsing to a relation extraction task but their task-specific data prohibits supervised parsing for comparison. Unsupervised parsing is traditionally only evaluated intrinsically by comparison to gold-standard parses. In contrast, Reichart and Rappoport (2009) count POS token sequences inside sub-phrases for measuring parsing consistency. But this count is not clearly relate</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 1–10, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Automatic selection of high quality parses created by a fully unsupervised parser.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL ’09,</booktitle>
<pages>156--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5021" citStr="Reichart and Rappoport (2009)" startWordPosition="739" endWordPosition="742">tion (Mintz et al., 2009; Banko et al., 2007; Yates and Etzioni, 2009) and similar applications such as semantic parsing (Poon and Domingos, 2009) and paraphrase acquisition (Lin and Pantel, 2001). However, in such systems, parsing is commonly applied as a separately trained subtask1 for which supervision is used. H¨anig and Schierle (2009) have applied unsupervised parsing to a relation extraction task but their task-specific data prohibits supervised parsing for comparison. Unsupervised parsing is traditionally only evaluated intrinsically by comparison to gold-standard parses. In contrast, Reichart and Rappoport (2009) count POS token sequences inside sub-phrases for measuring parsing consistency. But this count is not clearly related to application qualities. 2 Methodology A complete relation extraction system consists of multiple components. Our system follows the architecture described by Roth et al. (2012). In short, the system retrieves queries in the form of entity names for which all relations captured by the system are to be returned. The entity names are expanded by alias-names extracted from Wikipedia link anchor texts. An information retrieval component retrieves documents containing either the n</context>
</contexts>
<marker>Reichart, Rappoport, 2009</marker>
<rawString>Roi Reichart and Ari Rappoport. 2009. Automatic selection of high quality parses created by a fully unsupervised parser. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL ’09, pages 156–164, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Roth</author>
<author>Grzegorz Chrupala</author>
<author>Michael Wiegand</author>
<author>Singh Mittul</author>
<author>Klakow Dietrich</author>
</authors>
<title>Generalizing from freebase and patterns using cluster-based distant supervision for tac kbp slotfilling</title>
<date>2012</date>
<booktitle>In Proceedings of the Fifth Text Analysis Conference (TAC 2012),</booktitle>
<location>Gaithersburg, Maryland, USA,</location>
<contexts>
<context position="5318" citStr="Roth et al. (2012)" startWordPosition="782" endWordPosition="785">s used. H¨anig and Schierle (2009) have applied unsupervised parsing to a relation extraction task but their task-specific data prohibits supervised parsing for comparison. Unsupervised parsing is traditionally only evaluated intrinsically by comparison to gold-standard parses. In contrast, Reichart and Rappoport (2009) count POS token sequences inside sub-phrases for measuring parsing consistency. But this count is not clearly related to application qualities. 2 Methodology A complete relation extraction system consists of multiple components. Our system follows the architecture described by Roth et al. (2012). In short, the system retrieves queries in the form of entity names for which all relations captured by the system are to be returned. The entity names are expanded by alias-names extracted from Wikipedia link anchor texts. An information retrieval component retrieves documents containing either the name or one of the aliases. Further filtering retains only sentences where a named entity tagger labeled an occurrence of the queried entity as being of a suitable type and furthermore found a possible entity for the relation’s second argument. For each candidate sentence, a classifier component t</context>
</contexts>
<marker>Roth, Chrupala, Wiegand, Mittul, Dietrich, 2012</marker>
<rawString>Benjamin Roth, Grzegorz Chrupala, Michael Wiegand, Singh Mittul, and Klakow Dietrich. 2012. Generalizing from freebase and patterns using cluster-based distant supervision for tac kbp slotfilling 2012. In Proceedings of the Fifth Text Analysis Conference (TAC 2012), Gaithersburg, Maryland, USA, November. National Institute of Standards and Technology (NIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Roth</author>
<author>Tassilo Barth</author>
<author>Michael Wiegand</author>
<author>Dietrich Klakow</author>
</authors>
<title>A survey of noise reduction methods for distant supervision.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 workshop on Automated knowledge base construction,</booktitle>
<pages>73--78</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2038" citStr="Roth et al., 2013" startWordPosition="299" endWordPosition="302">a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et al., 2009; Roth et al., 2013) which hypothesizes that all text fragments containing argument cooccurrences of known semantic relation facts indeed express these relations. Still, systems relying on supervised parsers require training from annotated treebanks, which are expensive to create, and highly domain- and language dependent when available. An alternative is unsupervised parsing, which automatically induces grammars by structurally analyzing unlabeled corpora. Applying unsupervised parsing thus avoids the limitation to languages and domains for which annotated data is available. However, induced grammars do not matc</context>
</contexts>
<marker>Roth, Barth, Wiegand, Klakow, 2013</marker>
<rawString>Benjamin Roth, Tassilo Barth, Michael Wiegand, and Dietrich Klakow. 2013. A survey of noise reduction methods for distant supervision. In Proceedings of the 2013 workshop on Automated knowledge base construction, pages 73–78. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery. In</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 17,</booktitle>
<pages>1297--1304</pages>
<editor>Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1509" citStr="Snow et al., 2005" startWordPosition="218" endWordPosition="221">rces, compares favorably in terms of extraction quality. 1 Introduction Relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form. In natural language, there are infinitely many ways to creatively express a set of semantic relations in accordance to the syntax of the language. Languages vary across domains and change over time. It is therefore impossible to statically capture all ways of expressing a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et al., 2009; Roth et al., 2013) which hypothesizes that all text fragments containing argument cooccur</context>
<context position="7301" citStr="Snow et al. (2005)" startWordPosition="1106" endWordPosition="1109">t dependency path patterns. These patterns search for matches on the parse tree. Following Lin and Pantel (2001), the shortest path connecting two arguments in a dependency graph has been widely used as a representation of relation instance mentions. The general idea is that shortest paths skip over irrelevant optional parts of a sentence such as in $1, who ... founded $2 where the shortest path pattern $1←founded→$2 matches although an irrelevant relative clause appears between the arguments $1 and $2. Similar representations have been used by Mintz et al. (2009), Alfonseca et al. (2012) and Snow et al. (2005). In a second set of experiments, we used the shortest dependency paths in parsed training sentences to generate surface-based patterns. These patterns search for matches directly on plain text and therefore do no longer rely on parsing at application time. The patterns are obtained by turning the shortest paths between relational arguments in the parsed training data into token sequences with gaps. The token sequences consist of all words in the sentence that appear on the shortest dependency path. Argument positions in the surface patterns are specified by special tokens $1 and $2. At all pl</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1297–1304. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>296</pages>
<contexts>
<context position="4462" citStr="Yates and Etzioni, 2009" startWordPosition="660" endWordPosition="663"> the need to parse text during system application, such patterns also increase extraction quality. We discover that, for this method, un100 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 100–105, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics supervised parsing achieves better extraction quality than the more expensive supervised parsing. 1.1 Related Work Unsupervised and weakly supervised training methods have been applied to relation extraction (Mintz et al., 2009; Banko et al., 2007; Yates and Etzioni, 2009) and similar applications such as semantic parsing (Poon and Domingos, 2009) and paraphrase acquisition (Lin and Pantel, 2001). However, in such systems, parsing is commonly applied as a separately trained subtask1 for which supervision is used. H¨anig and Schierle (2009) have applied unsupervised parsing to a relation extraction task but their task-specific data prohibits supervised parsing for comparison. Unsupervised parsing is traditionally only evaluated intrinsically by comparison to gold-standard parses. In contrast, Reichart and Rappoport (2009) count POS token sequences inside sub-phr</context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. J. Artif. Int. Res., 34(1):255– 296, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A composite kernel to extract relations between entities with both flat and structured features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>825--832</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1529" citStr="Zhang et al., 2006" startWordPosition="222" endWordPosition="225">rably in terms of extraction quality. 1 Introduction Relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form. In natural language, there are infinitely many ways to creatively express a set of semantic relations in accordance to the syntax of the language. Languages vary across domains and change over time. It is therefore impossible to statically capture all ways of expressing a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et al., 2009; Roth et al., 2013) which hypothesizes that all text fragments containing argument cooccurrences of known sema</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 825–832, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>