<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015966">
<title confidence="0.996599">
Speech Retrieval in Unknown Languages: a Pilot Study∗
</title>
<author confidence="0.998051">
Xiaodan Zhuang# Jui Ting Huang# Mark Hasegawa-Johnson
</author>
<affiliation confidence="0.998336">
Beckman Institute, Department of Electrical and Computer Engineering
University of Illinois at Urbana-Champaign, U.S.A.
</affiliation>
<email confidence="0.99922">
{xzhuang2,jhuang29,jhasegaw}@uiuc.edu
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999292695652174">
Most cross-lingual speech retrieval assumes
intensive knowledge about all involved lan-
guages. However, such resource may not ex-
ist for some less popular languages. Some
applications call for speech retrieval in un-
known languages. In this work, we lever-
age on a quasi-language-independent subword
recognizer trained on multiple languages, to
obtain an abstracted representation of speech
data in an unknown language. Language-
independent query expansion is achieved ei-
ther by allowing a wide lattice output for an
audio query, or by taking advantage of dis-
tinctive features in speech articulation to pro-
pose subwords most similar to the given sub-
words in a query. We propose using a re-
trieval model based on finite state machines
for fuzzy matching of speech sound patterns,
and further for speech retrieval. A pilot study
of speech retrieval in unknown languages is
presented, using English, Spanish and Russian
as training languages, and Croatian as the un-
known target language.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999310666666667">
Dramatic increase in recorded speech media calls for
efficient retrieval of audio files. Accessing speech
media of a foreign language is a particularly impor-
tant and challenging task, often referred to as cross-
lingual speech retrieval or cross-lingual spoken doc-
ument retrieval.
</bodyText>
<footnote confidence="0.768687">
∗This research is funded by NSF grants 0534106 and
0703624. The authors would like to thank Su-Youn Yoon for
inspiring discussion. *The student authors contribute equally.
</footnote>
<page confidence="0.976886">
3
</page>
<bodyText confidence="0.999940588235295">
Previous work on cross-lingual speech retrieval
mostly leverages on intensive knowledge about all
the languages involved. Most reported work inves-
tigates retrieval in a target language, in response to
audio or text queries given in a different source lan-
guage (Meng et al., 2000; Virga and Khudanpur,
2003). Usually, the speech media in the target lan-
guage, and the audio queries in the source language,
are converted to speech recognition transcripts us-
ing large-vocabulary automatic speech recognizers
(LVASR) trained for the target language and the
source language respectively. The text queries, or
transcribed audio queries, are translated to the tar-
get language. Text retrieval techniques are applied
to retrieve speech, by retrieving the correspond-
ing LVASR transcription in the target language. In
such systems, a large-vocabulary speech recognizer
trained on the target language is essential, which
requires the existence of a dictionary and labeled
acoustic training data in that language.
LVASR currently do not exist for most of the 6000
languages on Earth. In some situations, knowledge
about the target language is limited, and definitely
not sufficient to enable training LVASR. Imagine
an audio database in a target language unknown to
a user, who needs to retrieve spoken content rel-
evant to some audible query in this unknown lan-
guage. For example, the user knows how the name
“Obama” is pronounced in the target language, and
wants to retrieve all spoken documents that contain
the query word, from a database in this unknown
language. A linguist might find himself/herself in
this scenario when he or she tries to collect a large
number of utterances containing some particular
</bodyText>
<note confidence="0.9512435">
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 3–11,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999750770833333">
phrases in an unknown language. Similarly, an in-
formation analyst might wish to leverage on speech
retrieval in unknown languages to organize critical
information before engaging linguistic experts for
finer analysis. We refer to such retrieval tasks as
speech retrieval in unknown languages, in which lit-
tle knowledge about the target language is assumed.
A human linguist attempting to manually per-
form speech retrieval in an unknown language
would necessarily map the perceived speech (both
database and query) into some cognitive abstraction
or schema, representing, perhaps, the phonetic dis-
tinctions that he or she has been trained to hear.
Matching and retrieval of speech would then be per-
formed based on such an abstraction. Two cog-
nitive processes, assimilation and accommodation,
take place when human brains are to process new
information (Bernstein et al., 2007), such as speech
in an unknown language. In accommodation, the in-
ternal stored knowledge adapts to new information
with which it is confronted. In assimilation, the new
information, e.g., speech in an unknown language, is
mapped to previously stored information, e.g., sub-
words (phones) as defined by knowledge about the
languages known to the listener.
This paper models speech retrieval in unknown
languages using a machine learning model of pho-
netic assimilation. A quasi-language-independent
subword recognizer is trained to capture salient sub-
words and their acoustic distribution in multiple
languages. This recognizer is applied on an un-
known language, therefore mapping segments of the
unknown speech to subwords in the known lan-
guages. Through this machine cognitive process,
the database and queries in the unknown language
are represented as sequences of quasi-language-
independent subwords. Speech retrieval is per-
formed based on such representation. Figure 1 illus-
trates that speech retrieval in an unknown language
can be modeled as a special case of assimilation.
This task differs from the more widely studied
known-language speech retrieval task, in that no lin-
guistic knowledge of the target language is assumed.
We can only leverage on knowledge that can be
applied by assimilation to the multiple known lan-
guages. Therefore, this task is more like a cross-
lingual sound pattern retrieval task, leveraged on
quasi-language-independent subwords, rather than
</bodyText>
<figureCaption confidence="0.994071">
Figure 1: Automatic speech retrieval in an unknown lan-
guage (below) is modeled as a special case of the cogni-
tive process called assimilation (above).
</figureCaption>
<bodyText confidence="0.999963032258065">
a translated spoken word/phrase retrieval task us-
ing target language LVASR transcripts, as in most
cross-lingual speech retrieval systems. The quasi-
language-independent subword recognizer is trained
on speech data other than the target language, and
therefore generates much noisier recognition results,
owing to potential mismatch between acoustic distri-
butions, lack of dictionary and lack of a word-level
language model.
To manage the extra difficulty, we adopt a sub-
word lattice representation to encode a wide hypoth-
esis space of recognized speech in the target lan-
guage. Language-independent query expansion is
achieved either by allowing a wide lattice output
for an audio query, or by taking advantage of dis-
tinctive features in speech articulation to propose
quasi-language-independent subwords most similar
to the given subwords in a query. Finite state ma-
chines (FSM) constructed from the speech lattices
are used to allow for fuzzy matching of speech
sound patterns, and further for retrieval in unknown
languages.
We carry out a pilot study of speech retrieval
in unknown languages, using English, Spanish and
Russian as training languages, and Croatian as the
unknown target language. To explain the effect of
additional knowledge about the target language, we
demonstrate the improvements in retrieval perfor-
mance that result by incrementally making available
subword sequence models and acoustic models for
the target language.
</bodyText>
<sectionHeader confidence="0.995003" genericHeader="method">
2 Quasi-Language-Independent subword
Models
</sectionHeader>
<subsectionHeader confidence="0.99952">
2.1 Deriving a subword set
</subsectionHeader>
<bodyText confidence="0.990384">
Based on the assumption that an audible phrase in an
unknown language can be represented as a sequence
</bodyText>
<page confidence="0.994648">
4
</page>
<bodyText confidence="0.999650380952381">
of subwords, the question is to find an appropriate
set of subword symbols. Schultz and Waibel (2001)
reported that a global unit set for the source
languages based on International Phonetic Alpha-
bet (IPA) symbols outperforms language-dependent
phonetic units in cross-lingual word recognition
tasks, whereas language-dependent phonetic units
are better models for multilingual word recognition
(in which the target language is also one of the
source languages). A multilingual task might ben-
efit from partitioning the feature space according to
language identity, i.e., to have different subsets of
models aiming at different languages. By contrast,
a cross-lingual task calls for one consistent set of
models with language-independent properties in or-
der to maximize portability into the new language.
To capture the necessary distinctions between dif-
ferent phones across languages, we first pool to-
gether individual phone inventories for source lan-
guages, each of which has its phones tagged with
a language identity, and then performed bottom-up
clustering on the phone pool based on pairwise sim-
ilarity between their acoustic models. Each cluster
represents one distinct language-independent sub-
word symbol. Since this set is still derived from
multiple languages, we refer to these subword units
as quasi-language-independent subwords. A quasi-
language-independent subword set is derived by the
following steps:
First, we encode all speech in the known lan-
guages using a language-dependent phone set. Each
symbol in this set is defined by the phone iden-
tity and the language identity. One single-Gaussian
three-state left-to-right HMM is trained for each of
these subword units.
Second, similarity between the language-
dependent phones is estimated by the approximated
KL divergence between corresponding acoustic
models. As shown in (Vihola et al., 2002), KL
divergence between single-Gaussian left-to-right
HMMs can be approximated in closed form by
Equation 1,
</bodyText>
<equation confidence="0.976181">
aU ij log (aUij/aV)(1)
ij
riI (bUi : bV ) , (2)
i
</equation>
<bodyText confidence="0.999943076923077">
where aij is the transition probability to hidden state
j, and bi and ri are the observation distribution
and steady-state probability for hidden state i. For
single-Gaussian distribution, I (bUi : by) can be ap-
proximated by,
Third, we use the Affinity Propagation algorithm
(Frey and Dueck, 2007) to conduct pairwise cluster-
ing of phones based on the approximated KL diver-
gence between acoustic models. The tendency for a
data point (a phone) to be an exemplar of a cluster
is controlled by the preference value assigned to that
phone. The preference of a phone i is set as follows
to favor frequent phones to be cluster centers:
</bodyText>
<equation confidence="0.996717">
p(i) = k log(Ci), (3)
</equation>
<bodyText confidence="0.999977923076923">
where Ci is the count of the phone i, and k is a nor-
malization term to control the total number of clus-
ters. To discourage subwords from the same lan-
guage to join a same cluster, pairwise distance be-
tween them are offset by an additional amount, com-
parable to the maximum pairwise distance between
the models.
The resultant subword set is supposed to cap-
ture quasi-language-independent phonetic informa-
tion, and each subword unit has relatively distinctive
acoustic distribution. These subwords are encoded
using the corresponding cluster exemplars as surro-
gates.
</bodyText>
<subsectionHeader confidence="0.999799">
2.2 Recognizing subwords
</subsectionHeader>
<bodyText confidence="0.9999337">
An automatic speech recognition (ASR) system
(Jelinek, 1998) serves to recognize both queries
and speech database, with acoustic models for the
language-independent subwords derived from the
known languages as described in section 2.1. The
front-end features extracted from the speech data
are 39-dimensional features including 12 Perceptual
Linear Prediction (PLP) coefficients and their en-
ergy, as well as the first-order and second order re-
gression coefficients.
</bodyText>
<equation confidence="0.934681714285714">
KLD(U, V ) =
ri
�S
i=1
S
j=1
+ �S
i=1
f
I (bUi : bVi) = 2 L log I v EU
r E
+ tr CEi C(EiV)−1 − (EU)−1ll i
+ tr ((Ez )−1 (µU − µZ) (µU − µZ )T l J
.
</equation>
<page confidence="0.922978">
5
</page>
<bodyText confidence="0.999997322580645">
We create context-dependent models for each
subword, using the same strategy for build-
ing context-dependent triphone models in LVASR
(Woodland et al., 1994). A “triphone” is a subword
with its context defined as its immediate preceding
and following subwords. Each triphone is repre-
sented by a continuous three-state left-to-right Hid-
den Markov Model (HMM). Additionally, there is a
one-state HMM for silence, two three-state HMMs
for noise and unknown sound respectively. The
number of Gaussian mixtures (9 to 21 Gaussians) is
optimized according to a development set consisting
of speech in the known languages. A standard tree-
based state tying technique is adopted for parameter
sharing between subwords with similar contexts.
The “language model” (LM), or more precisely
subword sequence model, should generalize from
the known languages to the unknown language. Our
trial experiments showed that unigram statistics of
subwords and their triphones is more transferable
across languages than N-gram statistics. We also as-
sume that infrequent triphones are less likely to be
salient units that would carry the properties of the
unknown language. Thus, we select the top frequent
triphones and map the rest of the triphones to their
center phones, forming a mixed vocabulary of fre-
quent triphones and context-independent subwords.
The frequencies of these vocabulary entries are used
to estimate an unigram LM in the ASR system. Tri-
phones in the ASR output are mapped back to its
center subwords before the retrieval stage.
</bodyText>
<sectionHeader confidence="0.983396" genericHeader="method">
3 Speech Retrieval through Subword
Indexing
</sectionHeader>
<bodyText confidence="0.999861357142857">
In many cross-lingual speech retrieval systems, the
speech media are processed by a large-vocabulary
automatic speech recognizer (LVASR), which has
access to vocabulary, dictionary, word language
model and acoustic models for the target lan-
guage. With all these resources, state-of-the-art
speech recognition could give reasonable hypoth-
esized word transcript, enabling direct application
of text retrieval techniques. However, this is not
the case in speech retrieval in unknown languages.
Moreover, without the higher level linguistic knowl-
edge, such as a word dictionary, this task aims to
find speech patterns that sound similar, as approxi-
mated by sequences of quasi-language-independent
subwords. Therefore, the sequential information in
the hypothesized subwords is critical.
To deal with the significant noise in the subword
recognition output, and to emphasize the sequential
information, we use the recognizer to obtain sub-
word lattices instead of one-best hypotheses. These
lattices can be represented as weighted automata,
which are compact representations of a large num-
ber of alternative subword sequences, each asso-
ciated with a weight indicating the uncertainty of
the data. Therefore, indexing speech in unknown
language can be achieved by indexing the corre-
sponding weighted automata with quasi-language-
independent subwords associated with the state tran-
sitions.
We adopt the weighted automata indexation algo-
rithm reported in (Allauzen et al., 2004), which is
optimal for searching subword sequences, as it takes
time linear in the sum of the query size and the num-
ber of speech media entries where it appears. The
automata indexation algorithm also preserves the se-
quential information, which is crutial for this task.
We leverage on two kinds of knowledge for query
expansion, namely empirical phone confusion and
knowledge-based phone confusion. An illustration
of our speech retrieval system is presented in Fig-
ure 2. We detail the indexing approaching as well as
query expansion and retrieval in this section.
</bodyText>
<figureCaption confidence="0.9506975">
Figure 2: Framework of speech retrieval through subword
indexing
</figureCaption>
<page confidence="0.986239">
6
</page>
<subsectionHeader confidence="0.943767">
3.1 Subword Finite State Machines as Speech
Indices
</subsectionHeader>
<bodyText confidence="0.9999717">
We construct a full index that can be used to search
for a query within all the speech utterances ui, i E
1, ..., n. In particular, this is achieved by construct-
ing a weighted finite-state transducer T, mapping
each query x to the set of speech utterances where
it appears. Each returned speech utterance u is as-
signed a score, which is the negative log of the ex-
pected count of the query x in utterance u.
The subword lattice for speech utterance ui can be
represented as a weighted finite state automata Ai,
whose path weights correspond to the joint proba-
bility of the observed speech and the hypothesized
subword sequence. To get an automata whose path
weights correspond to desired negative log of poste-
rior probabilities, we simply need to apply a general
weight-pushing algorithm to Ai in the log semiring,
resulting in an automata Bi. In this automata Bi,
the probability of a given string x is the sum of the
probability of all paths that contains x.
The key point of constructing the index transducer
Ti for uttereance ui is to introduce new paths that
enable matching between a query and any portions
of the original paths, while properly normalizing the
path weights. This is achieved by factor selection in
(Allauzen et al., 2004). First, null output is intro-
duced to each transition in the automata, converting
the automata into a transducer. Second, a new tran-
sition is introduced from a new unique initial state to
each existing state, with null input and output. The
weight associated with this transition is the negative
log of the forward probability. Similarly, a new tran-
sition is created from each state to a new unique final
state, with null input and output as the label i of the
current utterance ui. The assicated weight is the neg-
ative log of the backward probability. General finite
state machine optimization operations (Allauzen et
al., 2007) of weighted E-removal, determinization
and minimization over the log semiring can be ap-
plied to the resulting transducer. As shown in (Al-
lauzen et al., 2004), the path with input of string x
and output of label i has a weight corresponding to
the negative log of the expected count of x in utter-
ance ui.
To optimize the retrieval time, we divide all ut-
terances into a few groups. Within each group, the
utterance index transducers are unioned and deter-
minized to get one single index transducer for the
group. It is then feasible to expedite retrieval by
processing each group index transducer in a paral-
lel fashion.
</bodyText>
<subsectionHeader confidence="0.998537">
3.2 Query Expansion
</subsectionHeader>
<bodyText confidence="0.999671161290323">
While sequential information is important, ex-
act string match is very unplausible in this chal-
lenging task, even when subword lattices encode
many alternative recognition hypotheses. Language-
independent query expansion is therefore critical for
success in retrieval. We carry out query expansion
either by allowing a wide lattice output for an audio
query, or by taking advantage of distinctive features
in speech articulation to propose quasi-language-
independent subwords most similar to the given sub-
words in a query.
In particular, for a spoken query, ASR will gen-
erate a subword lattice instead of a one-best sub-
word sequence hypothesis. With the lattice, the au-
dio query is encoded by the best hypothesis from
ASR and its empirical phone confusion. The lattice
can then be represented as a finite-state automata.
However, when the query is given as a target
language subword sequence, we can no longer use
the recognizer to obtain an expanded query. Fur-
thermore, some target language subwords may not
even exist in the quasi-language-independent sub-
word set in the recognizer. In this case, knowledge-
based phone confusion is engaged via the use of a
set of distinctive features Fj, j E 1, ..., M for hu-
man speech (Chomsky and Halle, 1968), including
labial, alveolar, post-alveolar, retroflex, voiced, as-
pirated, front, back, etc.
We estimate similarity from phone a to phone b,
or more precisely, substitution tendency as in Equa-
tion 4,
</bodyText>
<equation confidence="0.965114666666667">
Nab
DFsim(a, b) = log (4)
Na
</equation>
<bodyText confidence="0.796839">
where
</bodyText>
<equation confidence="0.976269">
M
Nab = (Fja x Fjb = 1),
j=1
M
Na = (Fja =� 0).
j=1
</equation>
<page confidence="0.987967">
7
</page>
<bodyText confidence="0.9999475">
The target subword sequence is first mapped to
the derived subword set, by locating the identical
or nearest member phone in the clustering and then
adopting the surrogate for that cluster. This con-
verted sequence of derived subwords is further ex-
panded by adding the most likely alternative quasi-
language-independent subwords, parallel to each
original subword. Transitions to these alternative
subwords are associated with the corresponding sub-
stitution tendency based on distinctive features.
</bodyText>
<subsectionHeader confidence="0.998543">
3.3 Search
</subsectionHeader>
<bodyText confidence="0.999993047619048">
An expanded query, either obtained from an audio
query or a subword sequence query, is represented
as a weighted finite state automata. Searching this
query in the utterances is achieved by composing the
query automata with the index transducer. This re-
sults in another finite state transducer, which is fur-
ther processed by projection on output, removal of
ǫ arcs and determinization. The output is a list of
retrieved speech utterances, each with the expected
count of the query.
Apparently, the precision and recall of the re-
trieval results vary with the width of the subword
lattices used for indexing as well as how much the
query is expanded. We control the width of the sub-
word lattices via the number of tokens and the max-
imum probability decrease allowed for each step in
the Viterbi decoding. The extend to which a sub-
word sequence query is expanded is determined by
the lowest allowed similarity between the original
phone and an alternative phone. These parameters
are set empirically.
</bodyText>
<sectionHeader confidence="0.999608" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.898706">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999532846153846">
The known language pool should cover as many lan-
guage families as possible so that the derived sub-
words could better approximate language indepen-
dence. However, as a pilot study, this paper reports
experiments using only languages within the Indo-
European family. Table 1 summarizes the size of
speech data from each language. Croatian is used
as the unknown target language, and the other three
languages are the known languages used for de-
riving and training the quasi-language-independent
subword models. We extracted 80% of all speakers
per language for training, and 10% as a development
set.
</bodyText>
<table confidence="0.9991372">
Language ID Hours Spks Style
Croatian hrv 21.3 201 Read+answers
English hub 13.6 406 Broadcast
Spanish spa 14.6 120 Read+answers
Russian rus 2.5 63 Read+answers
</table>
<tableCaption confidence="0.9575175">
Table 1: Summary for data: language ID, total length,
number of speakers and speaking style for each language.
</tableCaption>
<subsectionHeader confidence="0.993021">
4.2 Settings
</subsectionHeader>
<bodyText confidence="0.999801371428571">
The speech retrieval task aims to find speech utter-
ances that contain a particular query. We use two
kinds of queries: 1) subword sequence queries, tran-
scribed as a sequence of phonetic symbols in the tar-
get language; 2) audio queries, each being an audio
segment of the speech query in the target language.
Since we aim to match speech patterns that sound
like each other, the queries used in this experiment
are relatively short, about 3 to 5 syllables. This adds
to the challenge in that very limited redundant in-
formation is available for query-utterance matching.
There are totally 40 subword sequences and 40 audio
queries, each occurs in between 18 and 38 utterances
out of a set of 576 utterances.
In addition to a cross-lingual retrieval system built
using only the known languages, we incrementally
augment resource on the target language to build
more knowledgeable systems.
AM0LM0: Both the acoustic model (AM)
and the language model (LM) are quasi-language-
independent, trained using data in multiple known
languages. This happens when no transcribed
speech data or a defined phone set exist for the tar-
get language. Essentially the system has no direct
knowledge about the target language.
AM0LMt: This setting examines the perfor-
mance gap due to the acoustic model mismatch
by using a quasi-language-independent AM, but a
target language LM. Suppose that a word dictio-
nary with phonetic transcription and possibly some
text data from the target language are available,
for training a target language subword LM. To find
the mapping between target triphones and language-
independent source AMs, linguistic knowledge and
phonetic symbol notation are the only information
</bodyText>
<page confidence="0.995264">
8
</page>
<bodyText confidence="0.997827583333333">
we can use. First, we map each of target mono-
phones to source phone symbols: Any source cluster
that contains a phonetic symbol with the same nota-
tion as the target phonetic symbol becomes a surro-
gate symbol for that target phone. If a target phone
is unseen to the known languages, the most similar
phone will be chosen first. The similarity is based on
the distinctive features, as discussed in Section 3.2.
Second, the target triphones are converted to possi-
ble source triphones for which acoustic models ex-
ist. Each target triphone not modeled in the source
language AM is replaced with the corresponding di-
phone (subword pair) if it exists, otherwise the cen-
ter phone.
AMtLM0: This setting examines the perfor-
mance gap due to the language model mismatch by
using a quasi-language-independent source LM, but
a target language AM. For the source triphones and
monophones that do not exist in the target AM, they
are mapped to target AMs in a way similar as de-
scribed above.
AMtLMt: Both AM and LM are trained for the
target language. This setting provides an upper
bound of the performance for different settings.
</bodyText>
<subsectionHeader confidence="0.998696">
4.3 Metrics
</subsectionHeader>
<bodyText confidence="0.998627965517241">
We evaluate the performance for both subword
recognition and speech retrieval, measured as fol-
lows.
Recognition Accuracy: The ground truth is en-
coded using subwords in the target language while
the recognition output is encoded using quasi-
language-independent subwords in Section 2. To
measure the recognition accuracy, we label each
quasi-language-independent subword cluster using
the most frequent target language subword that ap-
pears in that cluster. The hypothesis subword se-
quence is then compared against the groundtruth us-
ing a dynamic-programming-based string alignment
procedure. The recognition accuracy is defined as
REC − ACC = H−I
N x 100%, where H, I, and
N are the numbers of correct labels, insertion errors
and groundtruth labels respectively.
Retrieval Precision: The retrieval performance
is measured using Mean Average Precision (IR −
MAP), defined as the mean of the Average Preci-
sion (AP) for a set of different queries x. Mean
Average Precision (IR − MAP) can be defined in
Equation 5. n is the number of ordered retrieved
utterances and R is the total number of relevant ut-
terances. fi is an indicator function whether the ith
retrieved utterance does contain the query. Precision
pm for top m retrieved utterances can be calculated
as pm = m Em, f (k).
</bodyText>
<equation confidence="0.9957746">
1
IR − MAP = Q
n(x)
1
AP(x) = R(x) E fi(x)pi(x). (5)
</equation>
<bodyText confidence="0.998602333333333">
We use IR − MAPA and IR − MAPS to denote
the retrieval MAP for audio queries and subword se-
quence queries respectively.
</bodyText>
<sectionHeader confidence="0.722479" genericHeader="evaluation">
4.4 Results
</sectionHeader>
<bodyText confidence="0.999730909090909">
Table 2 presents a few examples of the derived
quasi-language-independent subwords. As dis-
cussed in Section 2, these subwords are obtained by
bottom-up clustering of all the language-dependent
IPA phones in the multiple known languages. The
same IPA symbol across languages may lie in the
same cluster, e.g., /z/ in Cluster 1, or different clus-
ters, e.g., /j/ in Cluster 3 and 4. Although symbols
within the same language are discouraged to be in
one cluster, it still desirably happens for highly sim-
ilar pairs, e.g., /1/rus and /j/rus in Cluster 4.
</bodyText>
<table confidence="0.9980564">
Cluster ID Surrogate Other phone members
1 /z/hub /z/spa, /z/rus, /zj/rus
2 /tSj/rus /tS/hub, /tS/spa
3 /j/hub /j/spa
4 /i:/hub /1/rus, /j/rus
</table>
<tableCaption confidence="0.91307">
Table 2: Examples of quasi-language-independent sub-
words, as clusters of source language IPAs.
</tableCaption>
<bodyText confidence="0.867569">
Table 3 compares the subword recognition
and retrieval performance for the quasi-language-
independent subwords and IPA phones. We can
</bodyText>
<table confidence="0.949738333333333">
Setting REC − ACC IR − MAPA IR − MAPS
IPA 37.18% 17.90% 31.40%
AM0LM0 42.52% 23.24% 32.62%
</table>
<tableCaption confidence="0.9957095">
Table 3: Performance of quasi-languange-independent
subword and IPA.
</tableCaption>
<figure confidence="0.806059666666667">
Q
L AP(x),
x=1
</figure>
<page confidence="0.821469">
9
</page>
<table confidence="0.99891425">
Setting AMtLMt AMtLM0 AM0LMt AM0LM0
REC − ACC 73.45% 67.29% 49.88% 42.52%
IR − MAPA 58.82% 52.38% 28.32% 23.24%
IR − MAPS 76.96% 51.86% 34.95% 32.62%
</table>
<tableCaption confidence="0.9777415">
Table 4: Performance of subword recognition and speech
retrieval.
</tableCaption>
<bodyText confidence="0.99943775">
see that on the unknown language Croatian, the de-
rived quasi-language-independent subwords outper-
form the IPA symbol set in both phone recognition
and retrieval using two kinds of queries.
</bodyText>
<figure confidence="0.665162">
Query Expansion
</figure>
<figureCaption confidence="0.822577">
Figure 3: Speech retrieval performance for subword se-
quence queries
</figureCaption>
<figure confidence="0.851361">
Query Expansion
</figure>
<figureCaption confidence="0.998523">
Figure 4: Speech retrieval performance for audio queries
</figureCaption>
<bodyText confidence="0.9999596">
Table 4 presents the subword recognition accu-
racy and retrieval performance with optimal query
width. Figure 3 and Figure 4 presents speech
retrieval performance at varying query widths for
subword sequence queries and audio queries re-
spectively. It is shown that speech retrieval in
completely unknown language achieves MAP of
23.24% and 32.62% while the system trained using
the most available knowledge about the target lan-
guage reaches MAP of 58.82% and 76.96%, for au-
dio queries and subword sequence queries respec-
tively. We also demonstrate access to phone fre-
quency (AM0LMt) and acoustic data (AMtLM0)
both boosts retrieval performance, and the effect is
roughly additive (AMtLMt).
</bodyText>
<sectionHeader confidence="0.996043" genericHeader="conclusions">
5 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999899615384615">
In this work, we present a speech retrieval approach
in unknown languages. This approach leverages
on speech recognition based on quasi-language-
independent subword models derived from multi-
ple known languages, and finite state machine based
fuzzy speech pattern matching and retrieval. Our
experiments use Croatian as the unknown language
and English, Russian and Spanish as the known lan-
guages. Results show that the derived subwords out-
perform the IPA symbols, and access to the subword
language model and acoustic models in the unknown
language explains the gap between this challenging
task and retrieval with knowledge about the target
language.
The proposed retrieval approach on unknown lan-
guages can be viewed as a machine learning model
of phonetic assimiliation, in which the segments
in an unknown language are mapped to language-
independent subwords learned from the multiple
known languages. However, another important cog-
nitive process, i.e., accomodation, is not yet mod-
eled. We believe the capability to create new sub-
words unseen in the known languages would lead
to improved performance. In particular, speech seg-
ments that are hypothesized by the quasi-language-
independent subword recognizer with very low con-
fidence scores can be clustered to form these new
subwords, accomodating to the unknown language.
The approach in this work can be readily scaled
up to much larger speech corpora. In particular,
larger corpora would make it more practical to im-
plement the accomodation process discussed above.
Besides, that would also enable online adaptation
of the model parameters of the quasi-language-
independent subword recognizer. Both are believed
to promise reduced gap between retrieval perfor-
mance in a known language and an unknown lan-
guage, and are potential future work beyond this pa-
per.
</bodyText>
<figure confidence="0.997868379310345">
80
AMtLMt
AMtLM0
AM0LMt
AM0LM0
50
40
30
20
10
narrow wide
IR−MAP(%)
70
60
narrow wide
IR−MAP(%) 60
55
50
45
40
35
30
25
20
15
AMtLMt
AMtLM0
AM0LMt
AM0LM0
</figure>
<page confidence="0.950594">
10
</page>
<sectionHeader confidence="0.99557" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999637594594595">
C. Allauzen, M. Mohri, and M. Saraclar. 2004. Gen-
eral indexation of weighted automata – application to
spoken utterance retrieval. In Proc. HLT-NAACL.
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and
M.Mohri. 2007. Openfst: A general and effi-
cient weighted finite-state transducer library. In Proc.
CIAA.
Bernstein, Penner, Clarke-Stewart, and Roy. 2007. Psy-
chology. Houghton Mifflin Company.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. New York: Harper and Row.
Brendan J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science,
315:972–976.
Frederick Jelinek. 1998. Statistical Methods for Speech
Recognition. The MIT Press.
Helen Meng, Berlin Chen, Erika Grams, Sanjeev Khu-
danpur, Wai-Kit Lo, Gina-Anne Levow, Douglas Oard,
Patrick Schone, Karen Tang, Hsin-Min Wang, and
Jian Qiang Wang. 2000. Mandarin-english informa-
tion (MEI): Investigating translingual speech retrieval.
http://www.clsp.jhu.edu/ws2000/final reports/mei/ws00mei.pdf.
Tanja Schultz and Alex Waibel. 2001. Language inde-
pendent and language adaptive acoustic modeling for
speech recognition. Speech Communication, 35:31–
51.
M. Vihola, M. Harju, P. Salmela, J. Suontausta, and
J. Savela. 2002. Two dissimilarity measures for hmms
and their application in phoneme model clustering. In
Proc. ICASSP, volume 1, pages I–933 – I–936.
Paola Virga and Sanjeev Khudanpur. 2003. Transliter-
ation of proper names in crosslingual information re-
trieval. In Proc. ACL 2003 workshop MLNER.
P.C. Woodland, J.J. Odell, V. Valtchev, and S.J. Young.
1994. Large vocabulary continuous speech recogni-
tion using HTK. In Proc. ICASSP, volume 2, pages
II/125–II/128.
</reference>
<page confidence="0.999456">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493855">
<title confidence="0.8581425">Retrieval in Unknown Languages: a Pilot Ting</title>
<affiliation confidence="0.872922">Beckman Institute, Department of Electrical and Computer University of Illinois at Urbana-Champaign,</affiliation>
<abstract confidence="0.998964416666667">Most cross-lingual speech retrieval assumes intensive knowledge about all involved languages. However, such resource may not exist for some less popular languages. Some applications call for speech retrieval in unknown languages. In this work, we leverage on a quasi-language-independent subword recognizer trained on multiple languages, to obtain an abstracted representation of speech data in an unknown language. Languageindependent query expansion is achieved either by allowing a wide lattice output for an audio query, or by taking advantage of distinctive features in speech articulation to propose subwords most similar to the given subwords in a query. We propose using a retrieval model based on finite state machines for fuzzy matching of speech sound patterns, and further for speech retrieval. A pilot study of speech retrieval in unknown languages is presented, using English, Spanish and Russian as training languages, and Croatian as the unknown target language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Allauzen</author>
<author>M Mohri</author>
<author>M Saraclar</author>
</authors>
<title>General indexation of weighted automata – application to spoken utterance retrieval.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="14631" citStr="Allauzen et al., 2004" startWordPosition="2251" endWordPosition="2254">utput, and to emphasize the sequential information, we use the recognizer to obtain subword lattices instead of one-best hypotheses. These lattices can be represented as weighted automata, which are compact representations of a large number of alternative subword sequences, each associated with a weight indicating the uncertainty of the data. Therefore, indexing speech in unknown language can be achieved by indexing the corresponding weighted automata with quasi-languageindependent subwords associated with the state transitions. We adopt the weighted automata indexation algorithm reported in (Allauzen et al., 2004), which is optimal for searching subword sequences, as it takes time linear in the sum of the query size and the number of speech media entries where it appears. The automata indexation algorithm also preserves the sequential information, which is crutial for this task. We leverage on two kinds of knowledge for query expansion, namely empirical phone confusion and knowledge-based phone confusion. An illustration of our speech retrieval system is presented in Figure 2. We detail the indexing approaching as well as query expansion and retrieval in this section. Figure 2: Framework of speech retr</context>
<context position="16559" citStr="Allauzen et al., 2004" startWordPosition="2580" endWordPosition="2583">ence. To get an automata whose path weights correspond to desired negative log of posterior probabilities, we simply need to apply a general weight-pushing algorithm to Ai in the log semiring, resulting in an automata Bi. In this automata Bi, the probability of a given string x is the sum of the probability of all paths that contains x. The key point of constructing the index transducer Ti for uttereance ui is to introduce new paths that enable matching between a query and any portions of the original paths, while properly normalizing the path weights. This is achieved by factor selection in (Allauzen et al., 2004). First, null output is introduced to each transition in the automata, converting the automata into a transducer. Second, a new transition is introduced from a new unique initial state to each existing state, with null input and output. The weight associated with this transition is the negative log of the forward probability. Similarly, a new transition is created from each state to a new unique final state, with null input and output as the label i of the current utterance ui. The assicated weight is the negative log of the backward probability. General finite state machine optimization opera</context>
</contexts>
<marker>Allauzen, Mohri, Saraclar, 2004</marker>
<rawString>C. Allauzen, M. Mohri, and M. Saraclar. 2004. General indexation of weighted automata – application to spoken utterance retrieval. In Proc. HLT-NAACL. C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M.Mohri. 2007. Openfst: A general and efficient weighted finite-state transducer library. In Proc. CIAA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penner Bernstein</author>
<author>Clarke-Stewart</author>
<author>Roy</author>
</authors>
<date>2007</date>
<publisher>Psychology. Houghton Mifflin Company.</publisher>
<contexts>
<context position="4473" citStr="Bernstein et al., 2007" startWordPosition="676" endWordPosition="679">s speech retrieval in unknown languages, in which little knowledge about the target language is assumed. A human linguist attempting to manually perform speech retrieval in an unknown language would necessarily map the perceived speech (both database and query) into some cognitive abstraction or schema, representing, perhaps, the phonetic distinctions that he or she has been trained to hear. Matching and retrieval of speech would then be performed based on such an abstraction. Two cognitive processes, assimilation and accommodation, take place when human brains are to process new information (Bernstein et al., 2007), such as speech in an unknown language. In accommodation, the internal stored knowledge adapts to new information with which it is confronted. In assimilation, the new information, e.g., speech in an unknown language, is mapped to previously stored information, e.g., subwords (phones) as defined by knowledge about the languages known to the listener. This paper models speech retrieval in unknown languages using a machine learning model of phonetic assimilation. A quasi-language-independent subword recognizer is trained to capture salient subwords and their acoustic distribution in multiple la</context>
</contexts>
<marker>Bernstein, Clarke-Stewart, Roy, 2007</marker>
<rawString>Bernstein, Penner, Clarke-Stewart, and Roy. 2007. Psychology. Houghton Mifflin Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Morris Halle</author>
</authors>
<title>The Sound Pattern of English.</title>
<date>1968</date>
<location>New York: Harper and Row.</location>
<contexts>
<context position="19069" citStr="Chomsky and Halle, 1968" startWordPosition="3002" endWordPosition="3005">d sequence hypothesis. With the lattice, the audio query is encoded by the best hypothesis from ASR and its empirical phone confusion. The lattice can then be represented as a finite-state automata. However, when the query is given as a target language subword sequence, we can no longer use the recognizer to obtain an expanded query. Furthermore, some target language subwords may not even exist in the quasi-language-independent subword set in the recognizer. In this case, knowledgebased phone confusion is engaged via the use of a set of distinctive features Fj, j E 1, ..., M for human speech (Chomsky and Halle, 1968), including labial, alveolar, post-alveolar, retroflex, voiced, aspirated, front, back, etc. We estimate similarity from phone a to phone b, or more precisely, substitution tendency as in Equation 4, Nab DFsim(a, b) = log (4) Na where M Nab = (Fja x Fjb = 1), j=1 M Na = (Fja =� 0). j=1 7 The target subword sequence is first mapped to the derived subword set, by locating the identical or nearest member phone in the clustering and then adopting the surrogate for that cluster. This converted sequence of derived subwords is further expanded by adding the most likely alternative quasilanguage-indep</context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Noam Chomsky and Morris Halle. 1968. The Sound Pattern of English. New York: Harper and Row.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan J Frey</author>
<author>Delbert Dueck</author>
</authors>
<title>Clustering by passing messages between data points.</title>
<date>2007</date>
<journal>Science,</journal>
<pages>315--972</pages>
<contexts>
<context position="10033" citStr="Frey and Dueck, 2007" startWordPosition="1522" endWordPosition="1525">milarity between the languagedependent phones is estimated by the approximated KL divergence between corresponding acoustic models. As shown in (Vihola et al., 2002), KL divergence between single-Gaussian left-to-right HMMs can be approximated in closed form by Equation 1, aU ij log (aUij/aV)(1) ij riI (bUi : bV ) , (2) i where aij is the transition probability to hidden state j, and bi and ri are the observation distribution and steady-state probability for hidden state i. For single-Gaussian distribution, I (bUi : by) can be approximated by, Third, we use the Affinity Propagation algorithm (Frey and Dueck, 2007) to conduct pairwise clustering of phones based on the approximated KL divergence between acoustic models. The tendency for a data point (a phone) to be an exemplar of a cluster is controlled by the preference value assigned to that phone. The preference of a phone i is set as follows to favor frequent phones to be cluster centers: p(i) = k log(Ci), (3) where Ci is the count of the phone i, and k is a normalization term to control the total number of clusters. To discourage subwords from the same language to join a same cluster, pairwise distance between them are offset by an additional amount</context>
</contexts>
<marker>Frey, Dueck, 2007</marker>
<rawString>Brendan J. Frey and Delbert Dueck. 2007. Clustering by passing messages between data points. Science, 315:972–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="11038" citStr="Jelinek, 1998" startWordPosition="1692" endWordPosition="1693">i, and k is a normalization term to control the total number of clusters. To discourage subwords from the same language to join a same cluster, pairwise distance between them are offset by an additional amount, comparable to the maximum pairwise distance between the models. The resultant subword set is supposed to capture quasi-language-independent phonetic information, and each subword unit has relatively distinctive acoustic distribution. These subwords are encoded using the corresponding cluster exemplars as surrogates. 2.2 Recognizing subwords An automatic speech recognition (ASR) system (Jelinek, 1998) serves to recognize both queries and speech database, with acoustic models for the language-independent subwords derived from the known languages as described in section 2.1. The front-end features extracted from the speech data are 39-dimensional features including 12 Perceptual Linear Prediction (PLP) coefficients and their energy, as well as the first-order and second order regression coefficients. KLD(U, V ) = ri �S i=1 S j=1 + �S i=1 f I (bUi : bVi) = 2 L log I v EU r E + tr CEi C(EiV)−1 − (EU)−1ll i + tr ((Ez )−1 (µU − µZ) (µU − µZ )T l J . 5 We create context-dependent models for each </context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>Frederick Jelinek. 1998. Statistical Methods for Speech Recognition. The MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Helen Meng</author>
<author>Berlin Chen</author>
</authors>
<title>Erika Grams, Sanjeev Khudanpur, Wai-Kit Lo, Gina-Anne Levow,</title>
<location>Douglas Oard, Patrick Schone, Karen Tang, Hsin-Min Wang, and</location>
<marker>Meng, Chen, </marker>
<rawString>Helen Meng, Berlin Chen, Erika Grams, Sanjeev Khudanpur, Wai-Kit Lo, Gina-Anne Levow, Douglas Oard, Patrick Schone, Karen Tang, Hsin-Min Wang, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Qiang Wang</author>
</authors>
<title>Mandarin-english information (MEI): Investigating translingual speech retrieval.</title>
<date>2000</date>
<note>http://www.clsp.jhu.edu/ws2000/final reports/mei/ws00mei.pdf.</note>
<marker>Wang, 2000</marker>
<rawString>Jian Qiang Wang. 2000. Mandarin-english information (MEI): Investigating translingual speech retrieval. http://www.clsp.jhu.edu/ws2000/final reports/mei/ws00mei.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanja Schultz</author>
<author>Alex Waibel</author>
</authors>
<title>Language independent and language adaptive acoustic modeling for speech recognition.</title>
<date>2001</date>
<journal>Speech Communication,</journal>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>35</volume>
<pages>933--936</pages>
<contexts>
<context position="7819" citStr="Schultz and Waibel (2001)" startWordPosition="1186" endWordPosition="1189">using English, Spanish and Russian as training languages, and Croatian as the unknown target language. To explain the effect of additional knowledge about the target language, we demonstrate the improvements in retrieval performance that result by incrementally making available subword sequence models and acoustic models for the target language. 2 Quasi-Language-Independent subword Models 2.1 Deriving a subword set Based on the assumption that an audible phrase in an unknown language can be represented as a sequence 4 of subwords, the question is to find an appropriate set of subword symbols. Schultz and Waibel (2001) reported that a global unit set for the source languages based on International Phonetic Alphabet (IPA) symbols outperforms language-dependent phonetic units in cross-lingual word recognition tasks, whereas language-dependent phonetic units are better models for multilingual word recognition (in which the target language is also one of the source languages). A multilingual task might benefit from partitioning the feature space according to language identity, i.e., to have different subsets of models aiming at different languages. By contrast, a cross-lingual task calls for one consistent set </context>
</contexts>
<marker>Schultz, Waibel, 2001</marker>
<rawString>Tanja Schultz and Alex Waibel. 2001. Language independent and language adaptive acoustic modeling for speech recognition. Speech Communication, 35:31– 51. M. Vihola, M. Harju, P. Salmela, J. Suontausta, and J. Savela. 2002. Two dissimilarity measures for hmms and their application in phoneme model clustering. In Proc. ICASSP, volume 1, pages I–933 – I–936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Virga</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Transliteration of proper names in crosslingual information retrieval.</title>
<date>2003</date>
<booktitle>In Proc. ACL 2003 workshop MLNER.</booktitle>
<contexts>
<context position="2028" citStr="Virga and Khudanpur, 2003" startWordPosition="301" endWordPosition="304"> is a particularly important and challenging task, often referred to as crosslingual speech retrieval or cross-lingual spoken document retrieval. ∗This research is funded by NSF grants 0534106 and 0703624. The authors would like to thank Su-Youn Yoon for inspiring discussion. *The student authors contribute equally. 3 Previous work on cross-lingual speech retrieval mostly leverages on intensive knowledge about all the languages involved. Most reported work investigates retrieval in a target language, in response to audio or text queries given in a different source language (Meng et al., 2000; Virga and Khudanpur, 2003). Usually, the speech media in the target language, and the audio queries in the source language, are converted to speech recognition transcripts using large-vocabulary automatic speech recognizers (LVASR) trained for the target language and the source language respectively. The text queries, or transcribed audio queries, are translated to the target language. Text retrieval techniques are applied to retrieve speech, by retrieving the corresponding LVASR transcription in the target language. In such systems, a large-vocabulary speech recognizer trained on the target language is essential, whic</context>
</contexts>
<marker>Virga, Khudanpur, 2003</marker>
<rawString>Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of proper names in crosslingual information retrieval. In Proc. ACL 2003 workshop MLNER.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C Woodland</author>
<author>J J Odell</author>
<author>V Valtchev</author>
<author>S J Young</author>
</authors>
<title>Large vocabulary continuous speech recognition using HTK.</title>
<date>1994</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>2</volume>
<pages>125--128</pages>
<contexts>
<context position="11750" citStr="Woodland et al., 1994" startWordPosition="1819" endWordPosition="1822">ge-independent subwords derived from the known languages as described in section 2.1. The front-end features extracted from the speech data are 39-dimensional features including 12 Perceptual Linear Prediction (PLP) coefficients and their energy, as well as the first-order and second order regression coefficients. KLD(U, V ) = ri �S i=1 S j=1 + �S i=1 f I (bUi : bVi) = 2 L log I v EU r E + tr CEi C(EiV)−1 − (EU)−1ll i + tr ((Ez )−1 (µU − µZ) (µU − µZ )T l J . 5 We create context-dependent models for each subword, using the same strategy for building context-dependent triphone models in LVASR (Woodland et al., 1994). A “triphone” is a subword with its context defined as its immediate preceding and following subwords. Each triphone is represented by a continuous three-state left-to-right Hidden Markov Model (HMM). Additionally, there is a one-state HMM for silence, two three-state HMMs for noise and unknown sound respectively. The number of Gaussian mixtures (9 to 21 Gaussians) is optimized according to a development set consisting of speech in the known languages. A standard treebased state tying technique is adopted for parameter sharing between subwords with similar contexts. The “language model” (LM),</context>
</contexts>
<marker>Woodland, Odell, Valtchev, Young, 1994</marker>
<rawString>P.C. Woodland, J.J. Odell, V. Valtchev, and S.J. Young. 1994. Large vocabulary continuous speech recognition using HTK. In Proc. ICASSP, volume 2, pages II/125–II/128.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>