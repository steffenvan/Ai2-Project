<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001014">
<title confidence="0.990358">
Inducing Combinatory Categorial Grammars with Genetic Algorithms
</title>
<author confidence="0.997331">
Elias Ponvert
</author>
<affiliation confidence="0.99272">
Department of Linguistics
University of Texas at Austin
1 University Station B5100
</affiliation>
<address confidence="0.79564">
Austin, TX 78712-0198 USA
</address>
<email confidence="0.997926">
ponvert@mail.utexas.edu
</email>
<sectionHeader confidence="0.993878" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999891388888889">
This paper proposes a novel approach to the
induction of Combinatory Categorial Gram-
mars (CCGs) by their potential affinity with
the Genetic Algorithms (GAs). Specifically,
CCGs utilize a rich yet compact notation for
lexical categories, which combine with rela-
tively few grammatical rules, presumed uni-
versal. Thus, the search for a CCG consists
in large part in a search for the appropri-
ate categories for the data-set’s lexical items.
We present and evaluates a system utilizing
a simple GA to successively search and im-
prove on such assignments. The fitness of
categorial-assignments is approximated by
the coverage of the resulting grammar on the
data-set itself, and candidate solutions are
updated via the standard GA techniques of
reproduction, crossover and mutation.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991372">
The discovery of grammars from unannotated ma-
terial is an important problem which has received
much recent research. We propose a novel approach
to this effort by leveraging the theoretical insights of
Combinatory Categorial Grammars (CCG) (Steed-
man, 2000), and their potential affinity with Ge-
netic Algorithms (GA) (Goldberg, 1989). Specifi-
cally, CCGs utilize an extremely small set of gram-
matical rules, presumed near-universal, which op-
erate over a rich set of grammatical categories,
which are themselves simple and straightforward
data structures. A search for a CCG grammar for
a language can be construed as a search for ac-
curate category assignments to the words of that
language, albeit over a large landscape of poten-
tial solutions. GAs are biologically-inspired general
purpose search/optimization methods that have suc-
ceeded in these kinds of environments: wherein so-
lutions are straightforwardly coded, yet nevertheless
the solution space is complex and difficult.
We evaluate a system that uses a GA to suc-
cessively refine a population of categorial lexicons
given a collection of unannotated training material.
This is an important problem for several reasons.
First of all, the development of annotated training
material is expensive and difficult, and so schemes
to discover linguistic patterns from unannotated text
may help cut down the cost of corpora development.
Also, this project is closely related to the problem of
resolving lexical gaps in parsing, which is a dogged
problem for statistical parsing systems in CCG, even
trained in a supervised manner. Carrying over tech-
niques from this project to that could help solve a
major problem in CCG parsing technology.
Statistical parsing with CCGs is an active area
of research. The development of CCGbank (Hock-
enmaier and Steedman, 2005) based on the Penn
Treebank has allowed for the development of wide-
coverage statistical parsers. In particular, Hock-
enmaier and Steedman (2001) report a generative
model for CCG parsing roughly akin to the Collins
parser (Collins, 1997) specific to CCG. Whereas
Hockenmaier’s parser is trained on (normal-form)
CCG derivations, Clark and Curran (2003) present
a CCG parser trained on the dependency structures
within parsed sentences, as well as the possible
derivations for them, using a log-linear (Maximum-
Entropy) model. This is one of the most accurate
parsers for producing deep dependencies currently
available. Both systems, however, suffer from gaps
</bodyText>
<page confidence="0.988646">
7
</page>
<bodyText confidence="0.928034555555556">
Proceedings of the ACL 2007 Student Research Workshop, pages 7–12,
Prague, June 2007. c�2007 Association for Computational Linguistics
in lexical coverage.
The system proposed here was evaluated against
a small corpus of unannotated English with the goal
of inducing a categorial lexicon for the fragment.
The system is not ultimately successful and fails to
achieve the baseline category assignment accuracy,
however it does suggest directions for improvement.
</bodyText>
<sectionHeader confidence="0.994695" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.994891">
2.1 Genetic Algorithms
</subsectionHeader>
<bodyText confidence="0.999963210526316">
The basic insight of a GA is that, given a problem
domain for which solutions can be straightforwardly
encoded as chromosomes, and for which candidate
solutions can be evaluated using a faithful fitness
function, then the biologically inspired operations of
reproduction, crossover and mutation can in certain
cases be applied to multisets or populations of can-
didate solutions toward the discovery of true or ap-
proximate solutions.
Among the applications of GA to computational
linguistics, (Smith and Witten, 1995) and (Korkmaz
and ¨Uc¸oluk, 2001) each present GAs for the induc-
tion of phrase structure grammars, applied success-
fully over small data-sets. Similarly, (Losee, 2000)
presents a system that uses a GA to learn part-of-
speech tagging and syntax rules from a collection of
documents. Other proposals related specifically to
the acquisition of categorial grammars are cited in
§2.3.
</bodyText>
<subsectionHeader confidence="0.99552">
2.2 Combinatory Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.9999538125">
CCG is a mildly context sensitive grammatical for-
malism. The principal design features of CCG is that
it posits a small set of grammatical rules that oper-
ate over rich grammatical categories. The categories
are, in the simplest case, formed by the atomic cate-
gories s (for sentence), np (noun phrase), n (com-
mon noun), etc., closed under the slash operators
/, \ . There is not a substantive distinction between
lexical and phrasal categories. The intuitive inter-
pretation of non-atomic categories is as follows: a
word for phrase of type A/B is looking for an item
of type B on the right, to form an item of type A.
Likewise, an item of type A\B is looking for an item
of type B on the left. type A. For example, in the
derivation in Figure 1, “scores” combines with the
np “another goal” to form the verb phrase “scores
</bodyText>
<figure confidence="0.6807366">
another
&gt;
Ronaldinho (s\np)/np np
s\np &lt;
s
</figure>
<figureCaption confidence="0.999337">
Figure 1: Example CCG derivation
</figureCaption>
<table confidence="0.932125333333333">
Application
A/B B ⇒&gt; A B A\B ⇒&lt; A
Composition ⇒&gt;B A/C B\C A\B ⇒&lt;B A\C
A/B B/C
Crossed-Composition
A/B B\C ⇒&gt;B× A\C B/C A\B ⇒&lt;B× A/C
</table>
<figureCaption confidence="0.995007">
Figure 2: CCG Rules
</figureCaption>
<bodyText confidence="0.964420333333333">
another goal”. This, in turn, combines with the np
“Ronaldinho” to form a sentence.
The example illustrates the rule of Application,
denoted with &lt; and &gt; in derivations. The schemata
for this rule, along with the Composition rule (B)
and the Crossed-Composition rule (B×), are given in
</bodyText>
<figureCaption confidence="0.603662">
Figure 2. The rules of CCG are taken as universals,
thus the acquisition of a CCG grammar can be seen
as the acquisition of a categorial lexicon.
</figureCaption>
<subsectionHeader confidence="0.683784">
2.3 Related Work
</subsectionHeader>
<bodyText confidence="0.999940368421053">
In addition to the supervised grammar systems out-
lined in §1, the following proposals have been put
forward toward the induction of categorial gram-
mars.
Watkinson and Mandahar (2000) report a Catego-
rial Grammar induction system related to that pro-
posed here. They generate a Categorial Grammar
using a fixed and limited set of categories and, uti-
lizing an unannotated corpus, successively refine the
lexicon by testing it against the corpus sentences one
at a time. Using a constructed corpus, their strategy
worked extremely well: 100% accuracy on lexical
category selection as well as 100% parsing accuracy
with the resulting statistical CG parser. With natu-
rally occurring text, however, their system does not
perform as well: approximately 77% lexical accu-
racy and 37% parsing accuracy.
One fundamental difference between the strategy
proposed here and that of Watkinson and Manda-
</bodyText>
<figure confidence="0.968134">
scores np/n
goal
n
&gt;
np
</figure>
<page confidence="0.984413">
8
</page>
<bodyText confidence="0.999984770833333">
har is that we propose to successively generate and
evaluate populations of candidate solutions, rather
than refining a single solution. Also, while Watkin-
son and Mandahar use logical methods to construct
a probabilistic parser, the present system uses ap-
proximate methods and yet derives symbolic parsing
systems. Finally, Watkinson and Mandahar utilize
an extremely small set of known categories, smaller
than the set used here.
Clark (1996) outlines a strategy for the acquisi-
tion of Tree-Adjoining Grammars (Joshi, 1985) sim-
ilar to the one proposed here: specifically, he out-
lines a learning model based on the co-evolution of a
parser, which builds parse trees given an input string
and a set of category-assignments, and a shred-
der, which chooses/discovers category-assignments
from parse-trees. The proposed strategy is not im-
plemented and tested, however.
Briscoe (2000) models the acquisition of catego-
rial grammars using evolutionary techniques from a
different perspective. In his experiments, language
agents induced parameters for languages from other
language agents generating training material. The
acquisition of languages is not induced using GA per
se, but the evolutionary development of languages is
modeled using GA techniques.
Also closely related to the present proposal is the
work of Villavicencio (2002). Villavicencio presents
a system that learns a unification-based categorial
grammar from a semantically-annotated corpus of
child-directed speech. The learning algorithm is
based on a Principles-and-Parameters language ac-
quisition scheme, making use of logical forms and
word order to induce possible categories within a
typed feature-structure hierarchy. Her system has
the advantage of not having to pre-compile a list of
known categories, as did Watkinson and Mandahar
as well as the present proposal. However, Villav-
icencio does make extensive use of the semantics
of the corpus examples, which the current proposal
does not. This is related to the divergent motivations
of two proposals: Villavicencio aims to present a
psychologically realistic language learner and takes
it as psychologically plausible that logical forms are
accessible to the language learner; the current pro-
posal is preoccupied with grammar induction from
unannotated text, and assumes (sentence-level) log-
ical forms to be inaccessible.
</bodyText>
<figure confidence="0.935665260869565">
n is the size of the population
A are candidate category assignments
F are fitness scores
E are example sentences
m is the likelihood of mutation
Initialize:
for i &lt;-- 1 to n :
A[i] &lt;-- RANDOMASSIGNMENT()
Loop:
for i &lt;-- 1 to length[A] :
F[i] &lt;-- 0
P &lt;-- NEWPARSER(A[i])
for j &lt;-- 1 to length[E] :
F[i] &lt;-- F[i]+SCORE(P�PARSE(E[i]))
A &lt;-- REPRODUCE(A,F)
� Crossover:
for i &lt;-- 1 to n− 1 :
CROSSOVER(A[i],A[i+ 1])
� Mutate:
for i &lt;-- 1 to n :
if RANDOM() &lt; m :
MUTATE(A[i])
Until: End conditions are met
</figure>
<figureCaption confidence="0.999399">
Figure 3: Pseudo-code for CCG induction GA.
</figureCaption>
<sectionHeader confidence="0.995075" genericHeader="method">
3 System
</sectionHeader>
<bodyText confidence="0.985380615384616">
As stated, the task is to choose the correct CCG cat-
egories for a set of lexical items given a collection of
unannotated or minimally annotated strings. A can-
didate solution genotype is an assignment of CCG
categories to the lexical items (types rather than to-
kens) contained in the textual material. A candi-
date phenotype is a CCG parser initialized with these
category assignments. The fitness of each candi-
date solution is evaluated by how well its phenotype
(parser) parses the strings of the training material.
Pseudo-code for the algorithm is given in Fig. 3.
For the most part, very simple GA techniques were
used; specifically:
</bodyText>
<listItem confidence="0.9691778">
• REPRODUCE The reproduction scheme utilizes
roulette wheel technique: initialize a weighted
roulette wheel, where the sections of the wheel
correspond to the candidates and the weights
of the sections correspond to the fitness of the
candidate. The likelihood that a candidate is
selected in a roulette wheel spin is directly pro-
portionate to the fitness of the candidate.
• CROSSOVER The crossover strategy is a simple
partition scheme. Given two candidates C and
</listItem>
<page confidence="0.976063">
9
</page>
<bodyText confidence="0.996741666666667">
D, choose a center point 0 &lt; i &lt; n where n the
number of genes (category-assignments), swap
C[0, i] +— D[0, i] and D[i, n] +— C[i, n].
</bodyText>
<listItem confidence="0.664736142857143">
• MUTATE The mutation strategy simply swaps
a certain number of individual assignments in
a candidate solution with others. For the ex-
periments reported here, if a given candidate
is chosen to be mutated, 25% of its genes are
modified. The probability a candidate was se-
lected is 10%.
</listItem>
<bodyText confidence="0.930678">
In the implementation of this strategy, the follow-
ing simplifying assumptions were made:
</bodyText>
<listItem confidence="0.998852">
• A given candidate solution only posits a single
CCG category for each lexical item.
• The CCG categories to assign to the lexical
items are known a priori.
• The parser only used a subset of CCG – pure
CCG (Eisner, 1996) – consisting of the Appli-
cation and Composition rules.
</listItem>
<subsectionHeader confidence="0.99941">
3.1 Chromosome Encodings
</subsectionHeader>
<bodyText confidence="0.999982">
A candidate solution is a simplified assignment of
categories to lexical items, in the following manner.
The system creates a candidate solution by assigning
lexical items a random category selection, as in:
</bodyText>
<subsectionHeader confidence="0.997738">
3.2 Fitness
</subsectionHeader>
<bodyText confidence="0.999991666666667">
The parser used is straightforward implementation
of the normal-form CCG parser presented by Eis-
ner (1996). The fitness of the parser is evaluated on
its parsing coverage on the individual strings, which
is a score based on the chart output. Several chart
fitness scores were evaluated, including:
</bodyText>
<listItem confidence="0.996582">
• SPANS The number of spans parsed
• RELATIVE The number of spans the string
parsed divided by the string length
• WEIGHTED The sum of the lengths of the spans
parsed
</listItem>
<bodyText confidence="0.951039333333333">
See §5.1 for a comparison of these fitness metrics.
Additionally, the following also factored into
scoring parses:
</bodyText>
<listItem confidence="0.962320333333333">
• S-BONUS Add an additional bonus to candi-
dates for each sentence they parse completely.
• PSEUDO-SMOOTHING Assign all parses at
least a small score, to help avoid premature
convergence. The metrics that count singleton
spans do this informally.
</listItem>
<figure confidence="0.9381835">
Ronaldinho (s\np)/np 4 Evaluation
Barcelona pp
kicks (s\np)/(s\np)
...
</figure>
<bodyText confidence="0.956467620689655">
Given the fixed vocabulary, and the fixed category
list, the representation can be simplified to lists of
indices to categories, indexed to the full vocabulary
list:
...
Then the category assignment can be construed as
a finite function from word-indices to category-
indices 10 H 15,1 H 42,2 H 37,...1 or simply the
vector (15,42,37,...). The chromosome encodings
for the GA scheme described here are just this: vec-
tors of integer category indices.
The system was evaluated on a small data-set of ex-
amples taken from the World Cup test-bed included
with the OpenCCG grammar development system1
and simplified considerably. This included 19 ex-
ample sentences with a total of 105 word-types and
613 tokens from (Baldridge, 2002).
In spite of the simplifying assumption that an in-
dividual candidate only assigns a single category to
a lexical item, one can derive a multi-assignment of
categories to lexemes from the population by choos-
ing the top category elected by the candidates. It
is on the basis of these derived assignments that the
system was evaluated. The examples chosen require
only 1-to-1 category assignment, hence the relevant
category from the test-bed constitutes the gold stan-
dard (minus Baldridge (2002)’s modalities). The
baseline for this dataset, assigning np to all lexical
items, was 28.6%. The hypothesis is that optimizing
</bodyText>
<footnote confidence="0.688905">
1http://openccg.sf.net
</footnote>
<figure confidence="0.9951633">
Ronaldinho
Barcelona
kicks
...
...
15 (s\np)/np
...
37 (s\np)/(s\np)
0
1
</figure>
<page confidence="0.8748205">
2
10
</page>
<table confidence="0.9189585">
Fitness Metric Accuracy
COUNT 18.5
RELATIVE 22.0
WEIGHTED 20.4
</table>
<tableCaption confidence="0.999692">
Table 1: Final accuracy of the metrics
</tableCaption>
<bodyText confidence="0.999840833333333">
parsing coverage with a GA scheme would correlate
with improved category-accuracy.
The end-conditions apply if the parsing coverage
for the derived grammar exceeds 90%. Such end-
conditions generally were not met; otherwise, ex-
periments ran for 100 generations, with a popula-
tion of 50 candidates. Because of the heavy reliance
of GAs on pseudo-random number generation, indi-
vidual experiments can show idiosyncratic success
or failure. To control for this, the experiments were
replicated 100 times each. The results presented
here are averages over the runs.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999497">
5.1 Fitness Metrics
</subsectionHeader>
<bodyText confidence="0.999937625">
The various fitness metrics were each evaluated, and
their final accuracies are reported in Table 1. The re-
sults were negative, as category accuracy did not ap-
proach the baseline. Examining the average system
accuracy over time helps illustrate some of the issues
involved. Figure 4 shows the growth of category ac-
curacy for each of the metrics. Pathologically, the
random assignments at the start of each experiment
have better accuracy than after the application of GA
techniques.
Figure 5 compares the accuracy of the category
assignments to the GA’s internal measure of its fit-
ness, using the Count Spans metric as a point of ref-
erence. (The fitness metric is scaled for compari-
son with the accuracy.) While fitness, in the average
case, steadily increases, accuracy does not increase
with such steadiness and degrades significantly in
the early generations.
The intuitive reason for this is that, initially,
the random assignment of categories succeeds by
chance in many cases, however the likelihood of ac-
curate or even compatible assignments to words that
occur adjacent in the examples is fairly low. The
GA promotes these assignments over others, appar-
</bodyText>
<figure confidence="0.96943">
0 10 20 30 40 50 60 70 80 90 100
Generations
</figure>
<figureCaption confidence="0.99953">
Figure 4: Comparison of fitness metrics
</figureCaption>
<figure confidence="0.994257">
0 10 20 30 40 50 60 70 80 90 100
Generations
</figure>
<figureCaption confidence="0.999958">
Figure 5: Fitness and accuracy: COUNT
</figureCaption>
<bodyText confidence="0.99997175">
ently committing the candidates to incorrect assign-
ments early on and not recovering from these com-
mitments. The WEIGHTED and RELATIVE metrics
are designed to try to overcome these effects by pro-
moting grammars that parse longer spans, but they
do not succeed. Perhaps exponential rather than lin-
ear bonus for parsing spans of length greater than
two would be effective.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999688785714286">
This project attempts to induce a grammar from
unannotated material, which is an extremely diffi-
cult problem for computational linguistics. Without
access to training material, logical forms, or other
relevant features to aid in the induction, the system
attempts to learn from string patterns alone. Using
GAs may aid in this process, but, in general, in-
duction from string patterns alone takes much larger
data-sets than the one discussed here.
The GA presented here takes a global perspective
on the progress of the candidates, in that the indi-
vidual categories assigned to the individual words
are not evaluated directly, but rather as members of
candidates that are scored. For a system such as
</bodyText>
<figure confidence="0.988868470588235">
30
25
20
15
10
Count
Relative
Weighted
Baseline
30
25
20
15
10
Accuracy
Fitness
Baseline
</figure>
<page confidence="0.995806">
11
</page>
<bodyText confidence="0.9999322">
this to take advantage of the patterns that arise out
of the text itself, a much more fine-grained perspec-
tive is necessary, since the performance of individ-
ual category-assignments to words being the focus
of the task.
</bodyText>
<sectionHeader confidence="0.998182" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99699225">
I would like to thank Jason Baldridge, Greg Kobele,
Mark Steedman, and the anonymous reviewers for
the ACL Student Research Workshop for valuable
feedback and discussion.
</bodyText>
<sectionHeader confidence="0.999182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999942839285714">
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
Ted Briscoe. 2000. Grammatical acquisition: Inductive
bias and coevolution of language and the language ac-
quisition device. Language, 76:245–296.
Stephen Clark and James R Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of EMNLP-03, pages 97–105, Sapporo, Japan.
Robin Clark. 1996. Complexity and the induction of
Tree Adjoining Grammars. Unpublished manuscript,
University of Pennsylvania.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL-
97, pages 16–23, Madrid, Spain.
Jason Eisner. 1996. Efficient normal-form parsing for
Combinatory Categorial Grammar. In Proceedings of
ACL-96, pages 79–86, Santa Cruz, USA.
David E. Goldberg. 1989. Genetic Algorithms in Search,
Optimization and Machine Learning. Addison-
Wesley.
Julia Hockenmaier and Mark Steedman. 2001. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proceedings of ACL, pages
335–342, Philadelphia, USA.
Julia Hockenmaier and Mark Steedman. 2005. CCG-
bank: User’s manual. Technical Report MC-SIC-05-
09, Department of Computer and Information Science,
University of Pennsylvania.
Aravind Joshi. 1985. An introduction to Tree Adjoining
Grammars. In A. Manaster-Ramer, editor, Mathemat-
ics of Language. John Benjamins.
Emin Erkan Korkmaz and G¨okt¨urk ¨Uc¸oluk. 2001. Ge-
netic programming for grammar induction. In 2001
Genetic and Evolutionary Computation Conference:
Late Breaking Papers, pages 245–251, San Francisco,
USA.
Rober M. Losee. 2000. Learning syntactic rules and tags
with genetic algorithms for information retrieval and
filtering: An empirical basis for grammatical rules. In-
formation Processing and Management, 32:185–197.
Tony C. Smith and Ian H. Witten. 1995. A genetic algo-
rithm for the induction of natural language grammars.
In Proc. of IJCAI-95 Workshop on New Approaches to
Learning for Natural Language Processing, pages 17–
24, Montreal, Canada.
Mark Steedman. 2000. The Syntactic Process. MIT,
Cambridge, Mass.
Aline Villavicencio. 2002. The Acquisition of a
Unification-Based Generalised Categorial Grammar.
Ph.D. thesis, University of Cambridge.
Stephen Watkinson and Suresh Manandhar. 2000. Un-
supervised lexical learning with categorial grammars
using the LLL corpus. In James Cussens and Sa&amp;quot;so
Dz&amp;quot;eroski, editors, Language Learning in Logic, pages
16–27, Berlin. Springer.
</reference>
<page confidence="0.998462">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.578551">
<title confidence="0.999983">Inducing Combinatory Categorial Grammars with Genetic Algorithms</title>
<author confidence="0.999824">Elias Ponvert</author>
<affiliation confidence="0.8737">Department of Linguistics University of Texas at Austin 1 University Station B5100</affiliation>
<address confidence="0.956299">Austin, TX 78712-0198 USA</address>
<email confidence="0.997685">ponvert@mail.utexas.edu</email>
<abstract confidence="0.998883315789474">This paper proposes a novel approach to the induction of Combinatory Categorial Grammars (CCGs) by their potential affinity with the Genetic Algorithms (GAs). Specifically, CCGs utilize a rich yet compact notation for lexical categories, which combine with relatively few grammatical rules, presumed universal. Thus, the search for a CCG consists in large part in a search for the appropriate categories for the data-set’s lexical items. We present and evaluates a system utilizing a simple GA to successively search and improve on such assignments. The fitness of categorial-assignments is approximated by the coverage of the resulting grammar on the data-set itself, and candidate solutions are updated via the standard GA techniques of reproduction, crossover and mutation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
</authors>
<title>Lexically Specified Derivational Control in Combinatory Categorial Grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="13978" citStr="Baldridge, 2002" startWordPosition="2235" endWordPosition="2236">ices to categories, indexed to the full vocabulary list: ... Then the category assignment can be construed as a finite function from word-indices to categoryindices 10 H 15,1 H 42,2 H 37,...1 or simply the vector (15,42,37,...). The chromosome encodings for the GA scheme described here are just this: vectors of integer category indices. The system was evaluated on a small data-set of examples taken from the World Cup test-bed included with the OpenCCG grammar development system1 and simplified considerably. This included 19 example sentences with a total of 105 word-types and 613 tokens from (Baldridge, 2002). In spite of the simplifying assumption that an individual candidate only assigns a single category to a lexical item, one can derive a multi-assignment of categories to lexemes from the population by choosing the top category elected by the candidates. It is on the basis of these derived assignments that the system was evaluated. The examples chosen require only 1-to-1 category assignment, hence the relevant category from the test-bed constitutes the gold standard (minus Baldridge (2002)’s modalities). The baseline for this dataset, assigning np to all lexical items, was 28.6%. The hypothesi</context>
</contexts>
<marker>Baldridge, 2002</marker>
<rawString>Jason Baldridge. 2002. Lexically Specified Derivational Control in Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
</authors>
<title>Grammatical acquisition: Inductive bias and coevolution of language and the language acquisition device.</title>
<date>2000</date>
<journal>Language,</journal>
<pages>76--245</pages>
<contexts>
<context position="8197" citStr="Briscoe (2000)" startWordPosition="1294" endWordPosition="1295"> methods and yet derives symbolic parsing systems. Finally, Watkinson and Mandahar utilize an extremely small set of known categories, smaller than the set used here. Clark (1996) outlines a strategy for the acquisition of Tree-Adjoining Grammars (Joshi, 1985) similar to the one proposed here: specifically, he outlines a learning model based on the co-evolution of a parser, which builds parse trees given an input string and a set of category-assignments, and a shredder, which chooses/discovers category-assignments from parse-trees. The proposed strategy is not implemented and tested, however. Briscoe (2000) models the acquisition of categorial grammars using evolutionary techniques from a different perspective. In his experiments, language agents induced parameters for languages from other language agents generating training material. The acquisition of languages is not induced using GA per se, but the evolutionary development of languages is modeled using GA techniques. Also closely related to the present proposal is the work of Villavicencio (2002). Villavicencio presents a system that learns a unification-based categorial grammar from a semantically-annotated corpus of child-directed speech. </context>
</contexts>
<marker>Briscoe, 2000</marker>
<rawString>Ted Briscoe. 2000. Grammatical acquisition: Inductive bias and coevolution of language and the language acquisition device. Language, 76:245–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Log-linear models for wide-coverage CCG parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-03,</booktitle>
<pages>97--105</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3174" citStr="Clark and Curran (2003)" startWordPosition="480" endWordPosition="483">CG, even trained in a supervised manner. Carrying over techniques from this project to that could help solve a major problem in CCG parsing technology. Statistical parsing with CCGs is an active area of research. The development of CCGbank (Hockenmaier and Steedman, 2005) based on the Penn Treebank has allowed for the development of widecoverage statistical parsers. In particular, Hockenmaier and Steedman (2001) report a generative model for CCG parsing roughly akin to the Collins parser (Collins, 1997) specific to CCG. Whereas Hockenmaier’s parser is trained on (normal-form) CCG derivations, Clark and Curran (2003) present a CCG parser trained on the dependency structures within parsed sentences, as well as the possible derivations for them, using a log-linear (MaximumEntropy) model. This is one of the most accurate parsers for producing deep dependencies currently available. Both systems, however, suffer from gaps 7 Proceedings of the ACL 2007 Student Research Workshop, pages 7–12, Prague, June 2007. c�2007 Association for Computational Linguistics in lexical coverage. The system proposed here was evaluated against a small corpus of unannotated English with the goal of inducing a categorial lexicon for</context>
</contexts>
<marker>Clark, Curran, 2003</marker>
<rawString>Stephen Clark and James R Curran. 2003. Log-linear models for wide-coverage CCG parsing. In Proceedings of EMNLP-03, pages 97–105, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Clark</author>
</authors>
<title>Complexity and the induction of Tree Adjoining Grammars.</title>
<date>1996</date>
<institution>University of Pennsylvania.</institution>
<note>Unpublished manuscript,</note>
<contexts>
<context position="7762" citStr="Clark (1996)" startWordPosition="1227" endWordPosition="1228">ly 77% lexical accuracy and 37% parsing accuracy. One fundamental difference between the strategy proposed here and that of Watkinson and Mandascores np/n goal n &gt; np 8 har is that we propose to successively generate and evaluate populations of candidate solutions, rather than refining a single solution. Also, while Watkinson and Mandahar use logical methods to construct a probabilistic parser, the present system uses approximate methods and yet derives symbolic parsing systems. Finally, Watkinson and Mandahar utilize an extremely small set of known categories, smaller than the set used here. Clark (1996) outlines a strategy for the acquisition of Tree-Adjoining Grammars (Joshi, 1985) similar to the one proposed here: specifically, he outlines a learning model based on the co-evolution of a parser, which builds parse trees given an input string and a set of category-assignments, and a shredder, which chooses/discovers category-assignments from parse-trees. The proposed strategy is not implemented and tested, however. Briscoe (2000) models the acquisition of categorial grammars using evolutionary techniques from a different perspective. In his experiments, language agents induced parameters for</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Robin Clark. 1996. Complexity and the induction of Tree Adjoining Grammars. Unpublished manuscript, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL97,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="3059" citStr="Collins, 1997" startWordPosition="466" endWordPosition="467">oblem of resolving lexical gaps in parsing, which is a dogged problem for statistical parsing systems in CCG, even trained in a supervised manner. Carrying over techniques from this project to that could help solve a major problem in CCG parsing technology. Statistical parsing with CCGs is an active area of research. The development of CCGbank (Hockenmaier and Steedman, 2005) based on the Penn Treebank has allowed for the development of widecoverage statistical parsers. In particular, Hockenmaier and Steedman (2001) report a generative model for CCG parsing roughly akin to the Collins parser (Collins, 1997) specific to CCG. Whereas Hockenmaier’s parser is trained on (normal-form) CCG derivations, Clark and Curran (2003) present a CCG parser trained on the dependency structures within parsed sentences, as well as the possible derivations for them, using a log-linear (MaximumEntropy) model. This is one of the most accurate parsers for producing deep dependencies currently available. Both systems, however, suffer from gaps 7 Proceedings of the ACL 2007 Student Research Workshop, pages 7–12, Prague, June 2007. c�2007 Association for Computational Linguistics in lexical coverage. The system proposed </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of ACL97, pages 16–23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Efficient normal-form parsing for Combinatory Categorial Grammar.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL-96,</booktitle>
<pages>79--86</pages>
<location>Santa Cruz, USA.</location>
<contexts>
<context position="12055" citStr="Eisner, 1996" startWordPosition="1927" endWordPosition="1928">, n] +— C[i, n]. • MUTATE The mutation strategy simply swaps a certain number of individual assignments in a candidate solution with others. For the experiments reported here, if a given candidate is chosen to be mutated, 25% of its genes are modified. The probability a candidate was selected is 10%. In the implementation of this strategy, the following simplifying assumptions were made: • A given candidate solution only posits a single CCG category for each lexical item. • The CCG categories to assign to the lexical items are known a priori. • The parser only used a subset of CCG – pure CCG (Eisner, 1996) – consisting of the Application and Composition rules. 3.1 Chromosome Encodings A candidate solution is a simplified assignment of categories to lexical items, in the following manner. The system creates a candidate solution by assigning lexical items a random category selection, as in: 3.2 Fitness The parser used is straightforward implementation of the normal-form CCG parser presented by Eisner (1996). The fitness of the parser is evaluated on its parsing coverage on the individual strings, which is a score based on the chart output. Several chart fitness scores were evaluated, including: •</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Efficient normal-form parsing for Combinatory Categorial Grammar. In Proceedings of ACL-96, pages 79–86, Santa Cruz, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Goldberg</author>
</authors>
<date>1989</date>
<booktitle>Genetic Algorithms in Search, Optimization and Machine Learning.</booktitle>
<publisher>AddisonWesley.</publisher>
<contexts>
<context position="1345" citStr="Goldberg, 1989" startWordPosition="197" endWordPosition="198"> search and improve on such assignments. The fitness of categorial-assignments is approximated by the coverage of the resulting grammar on the data-set itself, and candidate solutions are updated via the standard GA techniques of reproduction, crossover and mutation. 1 Introduction The discovery of grammars from unannotated material is an important problem which has received much recent research. We propose a novel approach to this effort by leveraging the theoretical insights of Combinatory Categorial Grammars (CCG) (Steedman, 2000), and their potential affinity with Genetic Algorithms (GA) (Goldberg, 1989). Specifically, CCGs utilize an extremely small set of grammatical rules, presumed near-universal, which operate over a rich set of grammatical categories, which are themselves simple and straightforward data structures. A search for a CCG grammar for a language can be construed as a search for accurate category assignments to the words of that language, albeit over a large landscape of potential solutions. GAs are biologically-inspired general purpose search/optimization methods that have succeeded in these kinds of environments: wherein solutions are straightforwardly coded, yet nevertheless</context>
</contexts>
<marker>Goldberg, 1989</marker>
<rawString>David E. Goldberg. 1989. Genetic Algorithms in Search, Optimization and Machine Learning. AddisonWesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="2966" citStr="Hockenmaier and Steedman (2001)" startWordPosition="448" endWordPosition="452">otated text may help cut down the cost of corpora development. Also, this project is closely related to the problem of resolving lexical gaps in parsing, which is a dogged problem for statistical parsing systems in CCG, even trained in a supervised manner. Carrying over techniques from this project to that could help solve a major problem in CCG parsing technology. Statistical parsing with CCGs is an active area of research. The development of CCGbank (Hockenmaier and Steedman, 2005) based on the Penn Treebank has allowed for the development of widecoverage statistical parsers. In particular, Hockenmaier and Steedman (2001) report a generative model for CCG parsing roughly akin to the Collins parser (Collins, 1997) specific to CCG. Whereas Hockenmaier’s parser is trained on (normal-form) CCG derivations, Clark and Curran (2003) present a CCG parser trained on the dependency structures within parsed sentences, as well as the possible derivations for them, using a log-linear (MaximumEntropy) model. This is one of the most accurate parsers for producing deep dependencies currently available. Both systems, however, suffer from gaps 7 Proceedings of the ACL 2007 Student Research Workshop, pages 7–12, Prague, June 200</context>
</contexts>
<marker>Hockenmaier, Steedman, 2001</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2001. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of ACL, pages 335–342, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: User’s manual.</title>
<date>2005</date>
<tech>Technical Report MC-SIC-05-09,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="2823" citStr="Hockenmaier and Steedman, 2005" startWordPosition="426" endWordPosition="430">st of all, the development of annotated training material is expensive and difficult, and so schemes to discover linguistic patterns from unannotated text may help cut down the cost of corpora development. Also, this project is closely related to the problem of resolving lexical gaps in parsing, which is a dogged problem for statistical parsing systems in CCG, even trained in a supervised manner. Carrying over techniques from this project to that could help solve a major problem in CCG parsing technology. Statistical parsing with CCGs is an active area of research. The development of CCGbank (Hockenmaier and Steedman, 2005) based on the Penn Treebank has allowed for the development of widecoverage statistical parsers. In particular, Hockenmaier and Steedman (2001) report a generative model for CCG parsing roughly akin to the Collins parser (Collins, 1997) specific to CCG. Whereas Hockenmaier’s parser is trained on (normal-form) CCG derivations, Clark and Curran (2003) present a CCG parser trained on the dependency structures within parsed sentences, as well as the possible derivations for them, using a log-linear (MaximumEntropy) model. This is one of the most accurate parsers for producing deep dependencies cur</context>
</contexts>
<marker>Hockenmaier, Steedman, 2005</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2005. CCGbank: User’s manual. Technical Report MC-SIC-05-09, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>An introduction to Tree Adjoining Grammars. In</title>
<date>1985</date>
<booktitle>Mathematics of Language.</booktitle>
<editor>A. Manaster-Ramer, editor,</editor>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="7843" citStr="Joshi, 1985" startWordPosition="1239" endWordPosition="1240">een the strategy proposed here and that of Watkinson and Mandascores np/n goal n &gt; np 8 har is that we propose to successively generate and evaluate populations of candidate solutions, rather than refining a single solution. Also, while Watkinson and Mandahar use logical methods to construct a probabilistic parser, the present system uses approximate methods and yet derives symbolic parsing systems. Finally, Watkinson and Mandahar utilize an extremely small set of known categories, smaller than the set used here. Clark (1996) outlines a strategy for the acquisition of Tree-Adjoining Grammars (Joshi, 1985) similar to the one proposed here: specifically, he outlines a learning model based on the co-evolution of a parser, which builds parse trees given an input string and a set of category-assignments, and a shredder, which chooses/discovers category-assignments from parse-trees. The proposed strategy is not implemented and tested, however. Briscoe (2000) models the acquisition of categorial grammars using evolutionary techniques from a different perspective. In his experiments, language agents induced parameters for languages from other language agents generating training material. The acquisiti</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Aravind Joshi. 1985. An introduction to Tree Adjoining Grammars. In A. Manaster-Ramer, editor, Mathematics of Language. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emin Erkan Korkmaz</author>
<author>G¨okt¨urk ¨Uc¸oluk</author>
</authors>
<title>Genetic programming for grammar induction.</title>
<date>2001</date>
<booktitle>In 2001 Genetic and Evolutionary Computation Conference: Late Breaking Papers,</booktitle>
<pages>245--251</pages>
<location>San Francisco, USA.</location>
<marker>Korkmaz, ¨Uc¸oluk, 2001</marker>
<rawString>Emin Erkan Korkmaz and G¨okt¨urk ¨Uc¸oluk. 2001. Genetic programming for grammar induction. In 2001 Genetic and Evolutionary Computation Conference: Late Breaking Papers, pages 245–251, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rober M Losee</author>
</authors>
<title>Learning syntactic rules and tags with genetic algorithms for information retrieval and filtering: An empirical basis for grammatical rules.</title>
<date>2000</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>32--185</pages>
<contexts>
<context position="4663" citStr="Losee, 2000" startWordPosition="705" endWordPosition="706">lutions can be straightforwardly encoded as chromosomes, and for which candidate solutions can be evaluated using a faithful fitness function, then the biologically inspired operations of reproduction, crossover and mutation can in certain cases be applied to multisets or populations of candidate solutions toward the discovery of true or approximate solutions. Among the applications of GA to computational linguistics, (Smith and Witten, 1995) and (Korkmaz and ¨Uc¸oluk, 2001) each present GAs for the induction of phrase structure grammars, applied successfully over small data-sets. Similarly, (Losee, 2000) presents a system that uses a GA to learn part-ofspeech tagging and syntax rules from a collection of documents. Other proposals related specifically to the acquisition of categorial grammars are cited in §2.3. 2.2 Combinatory Categorial Grammar CCG is a mildly context sensitive grammatical formalism. The principal design features of CCG is that it posits a small set of grammatical rules that operate over rich grammatical categories. The categories are, in the simplest case, formed by the atomic categories s (for sentence), np (noun phrase), n (common noun), etc., closed under the slash opera</context>
</contexts>
<marker>Losee, 2000</marker>
<rawString>Rober M. Losee. 2000. Learning syntactic rules and tags with genetic algorithms for information retrieval and filtering: An empirical basis for grammatical rules. Information Processing and Management, 32:185–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony C Smith</author>
<author>Ian H Witten</author>
</authors>
<title>A genetic algorithm for the induction of natural language grammars.</title>
<date>1995</date>
<booktitle>In Proc. of IJCAI-95 Workshop on New Approaches to Learning for Natural Language Processing,</booktitle>
<pages>17--24</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4497" citStr="Smith and Witten, 1995" startWordPosition="678" endWordPosition="681">signment accuracy, however it does suggest directions for improvement. 2 Background 2.1 Genetic Algorithms The basic insight of a GA is that, given a problem domain for which solutions can be straightforwardly encoded as chromosomes, and for which candidate solutions can be evaluated using a faithful fitness function, then the biologically inspired operations of reproduction, crossover and mutation can in certain cases be applied to multisets or populations of candidate solutions toward the discovery of true or approximate solutions. Among the applications of GA to computational linguistics, (Smith and Witten, 1995) and (Korkmaz and ¨Uc¸oluk, 2001) each present GAs for the induction of phrase structure grammars, applied successfully over small data-sets. Similarly, (Losee, 2000) presents a system that uses a GA to learn part-ofspeech tagging and syntax rules from a collection of documents. Other proposals related specifically to the acquisition of categorial grammars are cited in §2.3. 2.2 Combinatory Categorial Grammar CCG is a mildly context sensitive grammatical formalism. The principal design features of CCG is that it posits a small set of grammatical rules that operate over rich grammatical categor</context>
</contexts>
<marker>Smith, Witten, 1995</marker>
<rawString>Tony C. Smith and Ian H. Witten. 1995. A genetic algorithm for the induction of natural language grammars. In Proc. of IJCAI-95 Workshop on New Approaches to Learning for Natural Language Processing, pages 17– 24, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="1269" citStr="Steedman, 2000" startWordPosition="185" endWordPosition="187">ems. We present and evaluates a system utilizing a simple GA to successively search and improve on such assignments. The fitness of categorial-assignments is approximated by the coverage of the resulting grammar on the data-set itself, and candidate solutions are updated via the standard GA techniques of reproduction, crossover and mutation. 1 Introduction The discovery of grammars from unannotated material is an important problem which has received much recent research. We propose a novel approach to this effort by leveraging the theoretical insights of Combinatory Categorial Grammars (CCG) (Steedman, 2000), and their potential affinity with Genetic Algorithms (GA) (Goldberg, 1989). Specifically, CCGs utilize an extremely small set of grammatical rules, presumed near-universal, which operate over a rich set of grammatical categories, which are themselves simple and straightforward data structures. A search for a CCG grammar for a language can be construed as a search for accurate category assignments to the words of that language, albeit over a large landscape of potential solutions. GAs are biologically-inspired general purpose search/optimization methods that have succeeded in these kinds of e</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>The Acquisition of a Unification-Based Generalised Categorial Grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="8649" citStr="Villavicencio (2002)" startWordPosition="1359" endWordPosition="1360">assignments, and a shredder, which chooses/discovers category-assignments from parse-trees. The proposed strategy is not implemented and tested, however. Briscoe (2000) models the acquisition of categorial grammars using evolutionary techniques from a different perspective. In his experiments, language agents induced parameters for languages from other language agents generating training material. The acquisition of languages is not induced using GA per se, but the evolutionary development of languages is modeled using GA techniques. Also closely related to the present proposal is the work of Villavicencio (2002). Villavicencio presents a system that learns a unification-based categorial grammar from a semantically-annotated corpus of child-directed speech. The learning algorithm is based on a Principles-and-Parameters language acquisition scheme, making use of logical forms and word order to induce possible categories within a typed feature-structure hierarchy. Her system has the advantage of not having to pre-compile a list of known categories, as did Watkinson and Mandahar as well as the present proposal. However, Villavicencio does make extensive use of the semantics of the corpus examples, which </context>
</contexts>
<marker>Villavicencio, 2002</marker>
<rawString>Aline Villavicencio. 2002. The Acquisition of a Unification-Based Generalised Categorial Grammar. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Watkinson</author>
<author>Suresh Manandhar</author>
</authors>
<title>Unsupervised lexical learning with categorial grammars using the LLL corpus.</title>
<date>2000</date>
<booktitle>In James Cussens and Sa&amp;quot;so Dz&amp;quot;eroski, editors, Language Learning in Logic,</booktitle>
<pages>16--27</pages>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<marker>Watkinson, Manandhar, 2000</marker>
<rawString>Stephen Watkinson and Suresh Manandhar. 2000. Unsupervised lexical learning with categorial grammars using the LLL corpus. In James Cussens and Sa&amp;quot;so Dz&amp;quot;eroski, editors, Language Learning in Logic, pages 16–27, Berlin. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>