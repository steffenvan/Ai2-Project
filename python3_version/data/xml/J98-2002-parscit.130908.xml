<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9988515">
Generalizing Case Frames Using a
Thesaurus and the MDL Principle
</title>
<author confidence="0.999142">
Hang Li* Naoki Abe*
</author>
<affiliation confidence="0.777847">
NEC Corporation NEC Corporation
</affiliation>
<bodyText confidence="0.997619727272727">
A new method for automatically acquiring case frame patterns from large corpora is proposed.
In particular, the problem of generalizing values of a case frame slot for a verb is viewed as
that of estimating a conditional probability distribution over a partition of words, and a new
generalization method based on the Minimum Description Length (MDL) principle is proposed.
In order to assist with efficiency, the proposed method makes use of an existing thesaurus and
restricts its attention to those partitions that are present as &amp;quot;cuts&amp;quot; in the thesaurus tree, thus
reducing the generalization problem to that of estimating a &amp;quot;tree cut model&amp;quot; of the thesaurus tree.
An efficient algorithm is given, which provably obtains the optimal tree cut model for the given
frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method
were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed
method improves upon or is at least comparable with existing methods.
</bodyText>
<sectionHeader confidence="0.989956" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999965190476191">
We address the problem of automatically acquiring case frame patterns (selectional
patterns, subcategorization patterns) from large corpora. A satisfactory solution to this
problem would have a great impact on various tasks in natural language processing,
including the structural disambiguation problem in parsing. The acquired knowledge
would also be helpful for building a lexicon, as it would provide lexicographers with
word usage descriptions.
In our view, the problem of acquiring case frame patterns involves the following
two issues: (a) acquiring patterns of individual case frame slots; and (b) learning
dependencies that may exist between different slots. In this paper, we confine ourselves
to the former issue, and refer the interested reader to Li and Abe (1996), which deals
with the latter issue.
The case frame (case slot) pattern acquisition process consists of two phases: extrac-
tion of case frame instances from corpus data, and generalization of those instances to
case frame patterns. The generalization step is needed in order to represent the input
case frame instances more compactly as well as to judge the (degree of) acceptability
of unseen case frame instances. For the extraction problem, there have been various
methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grish-
man and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent
1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization
problem, in contrast, is a more challenging one and has not been solved completely. A
number of methods for generalizing values of a case frame slot for a verb have been
</bodyText>
<note confidence="0.753409">
* CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan.
</note>
<email confidence="0.734927">
E-mail:{lihang,abe}@ccm.cl.nec.co.jp
</email>
<note confidence="0.9014125">
© 1998 Association for Computational Linguistics
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.999676875">
proposed. Some of these methods make use of prior knowledge in the form of an
existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka
1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowl-
edge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this
paper, we propose a new generalization method, belonging to the first of these two
categories, which is both theoretically well-motivated and computationally efficient.
Specifically, we formalize the problem of generalizing values of a case frame slot
for a given verb as that of estimating a conditional probability distribution over a
partition of words, and propose a new generalization method based on the Minimum
Description Length principle (MDL): a principle of data compression and statistical
estimation from information theory.&apos; In order to assist with efficiency, our method
makes use of an existing thesaurus and restricts its attention on those partitions that
are present as &amp;quot;cuts&amp;quot; in the thesaurus tree, thus reducing the generalization problem
to that of estimating a &amp;quot;tree cut model&amp;quot; of the thesaurus tree. We then give an efficient
algorithm that provably obtains the optimal tree cut model for the given frequency data
of a case slot, in the sense of MDL. In order to test the effectiveness of our method, we
conducted PP-attachment disambiguation experiments using the case frame patterns
obtained by our method. Our experimental results indicate that the proposed method
improves upon or is at least comparable to existing methods.
The remainder of this paper is organized as follows: In Section 2, we formalize the
problem of generalizing values of a case frame slot as that of estimating a conditional
distribution. In Section 3, we describe our MDL-based generalization method. In Sec-
tion 4, we present our experimental results. We then give some concluding remarks
in Section 5.
</bodyText>
<sectionHeader confidence="0.998828" genericHeader="method">
2. The Problem
</sectionHeader>
<subsectionHeader confidence="0.996086">
2.1 The Data Sparseness Problem
</subsectionHeader>
<bodyText confidence="0.998638727272727">
Suppose that the data available to us are of the type shown in Table 1, which are slot
values for a given verb (verb,slot_name,slot_value triples) automatically extracted from
a corpus using existing techniques. By counting the frequency of occurrence of each
noun at a given slot of a verb, the frequency data shown in Figure 1 can be obtained.
We will refer to this type of data as co-occurrence data. The problem of generalizing
values of a case frame slot for a verb (or, in general, a head) can be viewed as the
problem of learning the underlying conditional probability distribution that gives
rise to such co-occurrence data. Such a conditional distribution can be represented by
a probability model that specifies the conditional probability P(n I v, r) for each n in
the set of nouns Ar = {ni, n2, . , nN} , v in the set of verbs V = {vi, 02, , vv}, and r
in the set of slot names R. = r2, ...,rRI, satisfying:
</bodyText>
<equation confidence="0.8913355">
E P(n I v,r) = 1. (1)
nEAr
</equation>
<bodyText confidence="0.9995245">
This type of probability model is often referred to as a word-based model. Since the
number of probability parameters in word-based models is large (0(N •V .R)), accurate
</bodyText>
<note confidence="0.5200416">
1 Recently, MDL and related techniques have become popular in corpus-based natural language
processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and
Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright
1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern
acquisition.
</note>
<page confidence="0.995388">
218
</page>
<note confidence="0.996264">
Li and Abe Generalizing Case Frames
</note>
<tableCaption confidence="0.991845">
Table 1
</tableCaption>
<figure confidence="0.957986076923077">
Example (verb, slot_name,
slot_value) triple data.
verb slot_name slot_value
fly argl bee
fly argl bird
fly argl bird
fly argl crow
fly argl bird
fly argl eagle
fly argl bee
fly argl eagle
fly argl bird
fly argl crow
</figure>
<figureCaption confidence="0.996693">
Figure 1
</figureCaption>
<bodyText confidence="0.997078">
Frequency data for the subject slot of verb fly.
estimation of a word-based model is difficult with the data size that is available in
practice—a problem usually referred to as the data sparseness problem. For example,
suppose that we employ the maximum-likelihood estimation (or MLE for short) to
estimate the probability parameters of a conditional probability distribution, as de-
scribed above, given the co-occurrence data in Figure 1. In this case, MLE amounts
to estimating the parameters by simply normalizing the frequencies so that they sum
to one, giving, for example, the estimated probabilities of 0, 0.2, and 0.4 for swallow,
eagle, and bird, respectively (see Figure 2). Since in general the number of parameters
exceeds the size of data that is typically available, MLE will result in estimating most
of the probability parameters to be zero.
To address this problem, Grishman and Sterling (1994) proposed a method of
smoothing conditional probabilities using the probability values of similar words,
where the similarity between words is judged based on co-occurrence data (see also
Dagan, Marcus, and Makovitch [1992] and Dagan, Pereira, and Lee [19941). More
specifically, conditional probabilities of words are smoothed by taking the weighted
average of those of similar words using the similarity measure as the weights. The
advantage of this approach is that it does not rely on any prior knowledge, but it
appears difficult to find a smoothing method that is both efficient and theoretically
sound. As an alternative, a number of authors have proposed the use of class-based
</bodyText>
<figure confidence="0.949798166666667">
crew eagle
swallow
bird
&amp;quot;Freq. —
bee
beg
</figure>
<page confidence="0.81697">
219
</page>
<note confidence="0.501647">
Computational Linguistics Volume 24, Number 2
</note>
<figureCaption confidence="0.9205655">
Figure 2
Word-based distribution estimated using MLE.
</figureCaption>
<bodyText confidence="0.994343">
models, which assign (conditional) probability values to (existing) classes of words,
rather than individual words.
</bodyText>
<subsectionHeader confidence="0.891344">
2.2 Class-based Models
</subsectionHeader>
<bodyText confidence="0.99990825">
An example of the class-based approach is Resnik&apos;s method of generalizing values of
a case frame slot using a thesaurus and the so-called selectional association measure
(Resnik 1993a, 1993b). The selectional association, denoted A(C I v, r), is defined as
follows:
</bodyText>
<equation confidence="0.979636">
A(C v,r) = P(C I v,r) x log (C)
P(C I v,r)
(2)
P
</equation>
<bodyText confidence="0.9993637">
where C is a class of nouns present in a given thesaurus, v is a verb and r is a slot name,
as described earlier. In generalizing a given noun n to a noun class, this method selects
the noun class C having the maximum A(C I v, r), among all super classes of n in a
given thesaurus. This method is based on an interesting intuition, but its interpretation
as a method of estimation is not clear. We propose a class-based generalization method
whose performance as a method of estimation is guaranteed to be near optimal.
We define the class-based model as a model that consists of a partition of the set
fsf of nouns, and a parameter associated with each member of the partition. Here, a
partition F of .AT is any collection of mutually disjoint subsets of .N that exhaustively
cover N&apos;. The parameters specify the conditional probability P(C I v, r) for each class
</bodyText>
<equation confidence="0.920760666666667">
(subset) C in that partition, such that
E P(C I v,r) = 1. (3)
cEr
</equation>
<bodyText confidence="0.9851365">
Within a given class C, it is assumed that each noun is generated with equal probability,
namely
</bodyText>
<equation confidence="0.8028445">
Vn E C: P(n I v,r) = —1 x P(C I v, r). (4)
I CI
</equation>
<bodyText confidence="0.999960166666667">
Here, we assume that a word belongs to a single class. In practice, however,
many words have sense ambiguity and a word can belong to several different classes,
e.g., bird is a member of both BIRD and MEAT. Thorough treatment of this problem
is beyond the scope of the present paper; we simply note that one can employ an
existing word-sense disambiguation technique (e.g.,Yarowsky 1992, 1994) in prepro-
cessing, and use the disambiguated word senses as virtual words in the following
</bodyText>
<figure confidence="0.9949086">
&amp;quot;Prob.&amp;quot; —
0.45
0.4
0.35 -
0.3
0.25 -
0.2
0.15 -
0.1 -
0.05 -
swallow
crow
eagle bird
bee insect
0
</figure>
<page confidence="0.93641">
220
</page>
<note confidence="0.911096">
Li and Abe Generalizing Case Frames
</note>
<figure confidence="0.588865333333333">
ANIMAL
BIRD INSECT
swallow crow eagle bird bug bee insect
</figure>
<figureCaption confidence="0.699573">
Figure 3
</figureCaption>
<subsectionHeader confidence="0.76867">
An example thesaurus.
</subsectionHeader>
<bodyText confidence="0.999688666666667">
case-pattern acquisition process. It is also possible to extend our model so that each
word probabilistically belongs to several different classes, which would allow us to
resolve both structural and word-sense ambiguities at the time of disambiguation.2
Employing probabilistic membership, however, would make the estimation process
significantly more computationally demanding. We therefore leave this issue as a fu-
ture topic, and employ a simple heuristic of equally distributing each word occurrence
in the data to all of its potential word senses in our experiments. Since our learning
method based on MDL is robust against noise, this should not significantly degrade
performance.
</bodyText>
<subsectionHeader confidence="0.703252">
2.3 The Tree Cut Model
</subsectionHeader>
<bodyText confidence="0.999868">
Since the number of partitions for a given set of nouns is extremely large, the problem
of selecting the best model from among all possible class-based models is most likely
intractable. In this paper, we reduce the number of possible partitions to consider by
using a thesaurus as prior knowledge, following a basic idea of Resnik&apos;s (1992).
In particular, we restrict our attention to those partitions that exist within the
thesaurus in the form of a cut. By thesaurus, we mean a tree in which each leaf node
stands for a noun, while each internal node represents a noun class, and domination
stands for set inclusion (see Figure 3). A cut in a tree is any set of nodes in the tree
that defines a partition of the leaf nodes, viewing each node as representing the set
of all leaf nodes it dominates. For example, in the thesaurus of Figure 3, there are
five cuts: [ANIMAL], [BIRD, INSECT], [BIRD, bug, bee, insect], [swallow, crow, eagle,
bird, INSECT], and [swallow, crow, eagle, bird, bug, bee, insect]. The class of tree cut
models of a fixed thesaurus tree is then obtained by restricting the partition F in the
definition of a class-based model to be those partitions that are present as a cut in that
thesaurus tree.
Formally, a tree cut model M can be represented by a pair consisting of a tree cut
F and a probability parameter vector 0 of the same length, that is:
</bodyText>
<equation confidence="0.990441">
M= (F,0) (5)
</equation>
<bodyText confidence="0.890118">
where F and 9 are:
</bodyText>
<equation confidence="0.999756">
F = c2, • • • , ck+1], 0 = [P(co, P(c2), • . • , P(ck+i)] (6)
</equation>
<bodyText confidence="0.9975055">
where C1, C2, , Ck+i is a cut in the thesaurus tree and Ek,4-11P(C,) = 1 is satisfied.
For simplicity we sometimes write P(C,), i = 1, , (k + 1) for P(C, I v, r).
If we use MLE for the parameter estimation, we can obtain five tree cut models
from the co-occurrence data in Figure 1; Figures 4-6 show three of these. For example,
</bodyText>
<page confidence="0.698594">
2 The model used by Pereira, Tishby, and Lee (1993) is indeed along this direction.
221
</page>
<figure confidence="0.99796375">
Computational Linguistics Volume 24, Number 2
&amp;quot;Prob.&amp;quot; —
0.45 -
0.4 -
0.35 -
0.3
0.25
0.2
0.15
0.1
0.05
swallow crow eagle bird bug bee insect
</figure>
<figureCaption confidence="0.593961">
Figure 4
</figureCaption>
<figure confidence="0.939329">
A tree cut model with [swallow, crow, eagle, bird, bug, bee, insect].
&amp;quot;Prob.&apos;&apos;—
swallow crow eagle bird bug bee insect
</figure>
<figureCaption confidence="0.965499">
Figure 5
</figureCaption>
<bodyText confidence="0.996190357142857">
A tree cut model with [BIRD, bug, bee, insect].
J14= ([BIRD, bug, bee, insect], [0.8, 0, 0.2, 0]) shown in Figure 5 is one such tree cut
model. Recall that A/I defines a conditional probability distribution I) ki(n I v, r) as
follows: For any noun that is in the tree cut, such as bee, the probability is given as
explicitly specified by the model, i.e., P( bee I fly, arg1) = 0.2. For any class in the tree
cut, the probability is distributed uniformly to all nouns dominated by it. For example,
since there are four nouns that fall under the class BIRD, and swallow is one of them,
the probability of swallow is thus given by PA:4(swallow I fly, arg1) = 0.8/4 -= 0.2. Note
that the probabilities assigned to the nouns under BIRD are smoothed, even if the
nouns have different observed frequencies.
We have thus formalized the problem of generalizing values of a case frame slot as
that of estimating a model from the class of tree cut models for some fixed thesaurus
tree; namely, selecting a model that best explains the data from among the class of
tree cut models.
</bodyText>
<sectionHeader confidence="0.864325" genericHeader="method">
3. Generalization Method Based On MDL
</sectionHeader>
<bodyText confidence="0.996215">
The question now becomes what strategy (criterion) we should employ to select the best
tree-cut model. We adopt the Minimum Description Length principle (Rissanen 1978,
</bodyText>
<figure confidence="0.954778869565217">
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
222
Li and Abe Generalizing Case Frames
&apos;Prob. —
swallow Cr010/ eagle bbd bug bee insect
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
</figure>
<figureCaption confidence="0.806778">
Figure 6
</figureCaption>
<tableCaption confidence="0.70748275">
A tree cut model with [BIRD, INSECT].
Table 2
Number of parameters and KL distance from the empirical distribution for the five
tree cut models.
</tableCaption>
<bodyText confidence="0.998346826086956">
Number of Parameters KL Distance
[ANIMAL] 0 0.89
[BIRD, INSECT] 1 0.72
[BIRD, bug, bee, insect] 3 0.4
[swallow, crow, eagle, bird, INSECT] 4 0.32
[swallow, crow, eagle, bird, bug, bee, insect] 6 0
1983, 1984, 1986, 1989), which has various desirable properties, as will be described
later.3
MDL is a principle of data compression and statistical estimation from informa-
tion theory, which states that the best probability model for given data is that which
requires the least code length in bits for the encoding of the model itself and the given
data observed through it.&apos; The former is the model description length and the latter
the data description length.
In our current problem, it tends to be the case, in general, that a model nearer the
root of the thesaurus tree, such as that in Figure 6, is simpler (in terms of the number
of parameters), but tends to have a poorer fit to the data. In contrast, a model nearer
the leaves of the thesaurus tree, such as that in Figure 4, is more complex, but tends
to have a better fit to the data. Table 2 shows the number of free parameters and
the KL distance from the empirical distribution of the data (namely, the word-based
distribution estimated by MLE) shown in Figure 2 for each of the five tree cut models.3
In the table, one can see that there is a trade-off between the simplicity of a model
and the goodness of fit to the data.
In the MDL framework, the model description length is an indicator of model
</bodyText>
<footnote confidence="0.975714571428571">
3 Estimation strategies related to MDL have been independently proposed and studied by various
authors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992).
4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction to the MDL principle.
5 The KL distance (also known as KL-divergence or relative entropy), which is widely used in
information theory and statistics, is a measure of distance between two distributions (e.g., Cover and
Thomas 1991). It is always nonnegative and is zero if and only if the two distributions are identical,
but is asymmetric and hence not a metric (the usual notion of distance).
</footnote>
<page confidence="0.992859">
223
</page>
<note confidence="0.437268">
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.999587166666667">
complexity, while the data description length indicates goodness of fit to the data. The
MDL principle stipulates that the model that minimizes the sum total of the description
lengths should be the best model (both for data compression and statistical estimation).
In the remainder of this section, we will describe how we apply MDL to our
current problem. We will then discuss the rationale behind using MDL in our present
context.
</bodyText>
<subsectionHeader confidence="0.999673">
3.1 Calculating Description Length
</subsectionHeader>
<bodyText confidence="0.999974333333333">
We first show how the description length for a model is calculated. We use S to
denote a sample (or set of data), which is a multiset of examples, each of which is an
occurrence of a noun at a given slot r of a given verb v (i.e., duplication is allowed).
We let I SI denote the size of S as a multiset, and n E S indicate the inclusion of n
in S as a multiset. For example, the column labeled slot_value in Table 1 represents a
sample S for the subject slot of fly, and in this case I SI= 10.
Given a sample S and a tree cut F, we employ MLE to estimate the parame-
ters of the corresponding tree cut model 14 = (F, Ô), where Ô denotes the estimated
parameters.
The total description length L(M, S) of the tree cut model kl and the sample S
observed through M is computed as the sum of the model description length L(F),
parameter description length L(e I r), and data description length L(S I
</bodyText>
<equation confidence="0.826089">
L(M, S) = e), s) = L(n+ L(O r) + r.,(s r,O). (7)
</equation>
<bodyText confidence="0.9781165">
Note that we sometimes refer to L(F) + L(e I n as the model description length.
The model description length L(F) is a subjective quantity, which depends on the
coding scheme employed. Here, we choose to assign the same code length to each cut
and let:
</bodyText>
<equation confidence="0.982888">
L(F) = log II (8)
</equation>
<bodyText confidence="0.953155083333333">
where G denotes the set of all cuts in the thesaurus tree T.&apos; This corresponds to assum-
ing that each tree cut model is equally likely a priori, in the Bayesian interpretation of
MDL. (See Section 3.4.)
The parameter description length L(O I F) is calculated by:
L(e I F) = x log ISI (9)
where I SI denotes the sample size and k denotes the number of free parameters in the
tree cut model, i.e., k equals the number of nodes in F minus one. It is known to be
best to use this number of bits to describe probability parameters in order to minimize
the expected total description length (Rissanen 1984, 1986). An intuitive explanation
of this is that the standard deviation of the maximum-likelihood estimator of each
parameter is of the orderV,-11s1, and hence describing each parameter using more than
– log =1 log IS I bits would be wasteful for the estimation accuracy possible with
</bodyText>
<page confidence="0.489722">
2
</page>
<bodyText confidence="0.9581025">
the given sample size.
Finally, the data description length L(S I F, e) is calculated by:
</bodyText>
<equation confidence="0.543752">
L(S I F, O) = – E log P(n) (10)
nES
</equation>
<page confidence="0.603710333333333">
6 Here and throughout, log denotes the logarithm to the base 2. For reasons why Equation 8 holds, see,
for example, Quinlan and Rivest (1989).
224
</page>
<note confidence="0.931636">
Li and Abe generalizing Case Frames
</note>
<tableCaption confidence="0.992291">
Table 3
</tableCaption>
<bodyText confidence="0.388357285714286">
Calculating the description length for the
model of Figure 5.
BIRD bug bee insect
f(C) 8 0 2 0
Cl 4 1 1 1
P(C) 0.8 0.0 0.2 0.0
P(n) 0.2 0.0 0.2 0.0
</bodyText>
<equation confidence="0.90894">
[BIRD, bug, bee, insect]
L(0 I r) (4-21) X log 10 = 4.98
L(S I r, 0) -(2 + 4 + 2 + 2) x log0.2 = 23.22
</equation>
<bodyText confidence="0.9999655">
where for simplicity we write P(n) for Pm(n I v, r). Recall that P(n) is obtained by
MLE, namely, by normalizing the frequencies:
</bodyText>
<equation confidence="0.9809444">
- 1 -
P(n) = x P(C)
for each C E F and each n E C, where for each C e F:
P(c)= f(c) (12)
Is&apos;
</equation>
<bodyText confidence="0.999940533333333">
where f(C) denotes the total frequency of nouns in class C in the sample S. and F is a
tree cut. We note that, in fact, the maximum-likelihood estimate is one that minimizes
the data description length L(S I F, 0).
With description length defined in the above manner, we wish to select a model
with the minimum description length and output it as the result of generalization.
Since we assume here that every tree cut has an equal L(F), technically we need only
calculate and compare L&apos;(M, S) = L(O F) L(S F, e) as the description length. For
simplicity, we will sometimes write just L&apos;(F) for L&apos;(A.4,S), where r is the tree cut of
when /C4 and S are clear from context.
The description lengths for the data in Figure 1 using various tree cut models
of the thesaurus tree in Figure 3 are shown in Table 4. (Table 3 shows how the de-
scription length is calculated for the model of tree cut [BIRD, bug, bee, insect].) These
figures indicate that the model in Figure 6 is the best model, according to MDL. Thus,
given the data in Table 1 as input, the generalization result shown in Table 5 is ob-
tained.
</bodyText>
<subsectionHeader confidence="0.99999">
3.2 An Efficient Algorithm
</subsectionHeader>
<bodyText confidence="0.999882333333333">
In generalizing values of a case frame slot using MDL, we could, in principle, calculate
the description length of every possible tree cut model and output a model with the
minimum description length as the generalization result, if computation time were of
no concern. But since the number of cuts in a thesaurus tree is exponential in the size
of the tree (for example, it is easy to verify that for a complete b-ary tree of depth d it is
of the order O(2bd 1)), it is impractical to do so. Nonetheless, we were able to devise a
</bodyText>
<page confidence="0.995731">
225
</page>
<note confidence="0.409555">
Computational Linguistics Volume 24, Number 2
</note>
<tableCaption confidence="0.992996">
Table 4
</tableCaption>
<bodyText confidence="0.900665333333333">
Description length of the five tree cut models.
L(0 I r) L(s I r,O) v(r)
[ANIMAL] 0 28.07 28.07
[BIRD, INSECT] 1.66 26.39 28.05
[BIRD, bug, bee, insect] 4.98 23.22 28.20
[swallow, crow, eagle, bird, INSECT] 6.64 22.39 29.03
[swallow, crow, eagle, bird, bug, bee, insect] 9.97 19.22 29.19
Table 5
Generalization result.
verb slot _name slot _value probability
fly arg1 BIRD 0.8
fly argl INSECT 0.2
Here we let t denote a thesaurus (sub)tree, root(t) the root of the tree t.
Initially t is set to the entire tree.
Also input to the algorithm is a co-occurrence data.
</bodyText>
<listItem confidence="0.860535142857143">
algorithm Find-MDL(t) := cut
1. if
2. t is a leaf node
3. then
4. return(N)
5. else
6. For each child tree t, of t c, :=Find-MDL(t1)
7. c:= append(c)
8. if
9. L&apos;([root(t)]) &lt; L&apos; (c)
10. then
11. return([root(t)])
12. else
13. return(c)
</listItem>
<figureCaption confidence="0.998173">
Figure 7
</figureCaption>
<bodyText confidence="0.944769545454545">
The algorithm: Find-MDL.
simple and efficient algorithm based on dynamic programming, which is guaranteed
to find a model with the minimum description length.
Our algorithm, which we call Find-MDL, recursively finds the optimal MDL model
for each child subtree of a given tree and appends all the optimal models of these sub-
trees and returns the appended models, unless collapsing all the lower-level optimal
models into a model consisting of a single node (the root node of the given tree) re-
duces the total description length, in which case it does so. The details of the algorithm
are given in Figure 7. Note that for simplicity we describe Find-MDL as outputting a
tree cut, rather than a complete tree cut model.
Note in the above algorithm that the parameter description length is calculated as
</bodyText>
<page confidence="0.993264">
226
</page>
<note confidence="0.729606">
Li and Abe Generalizing Case Frames
</note>
<figure confidence="0.560796">
f(swallow)=4,f(crow)=4,f(eagle)=4,f(bird)=6,f(bee)=8,f(car)=1,f(jet)=4,f(airplane)=4
</figure>
<figureCaption confidence="0.763503">
Figure 8
</figureCaption>
<bodyText confidence="0.566382">
An example application of Find-MDL.
</bodyText>
<equation confidence="0.8314295">
L±1
log
I SI, where k +1 is the number of nodes in the current cut, both when t is the
2
</equation>
<bodyText confidence="0.999344083333333">
entire tree and when it is a proper subtree. This contrasts with the fact that the number
of free parameters is k for the former, while it is k + 1 for the latter. For the purpose
of finding a tree cut with the minimum description length, however, this distinction
can be ignored (see Appendix A).
Figure 8 illustrates how the algorithm works (on the co-occurrence data shown
at the bottom): In the recursive application of Find-MDL on the subtree rooted at
AIRPLANE, the if-clause on line 9 evaluates to true since L&apos; ([AIRPLANE]) = 32.27,
L&apos;( [jet, helicopter, airplane]) = 32.72, and hence [AIRPLANE] is returned. Then in the
call to Find-MDL on the subtree rooted at ARTIFACT, the same if-clause evaluates
to false since L&apos;( [VEHICLE, AIRPLANE]) = 40.97, L&apos; ([ARTIFACT]) = 41.09, and hence
[VEHICLE, AIRPLANE] is returned.
Concerning the above algorithm, we show that the following proposition holds:
</bodyText>
<subsectionHeader confidence="0.879983">
Proposition 1
</subsectionHeader>
<bodyText confidence="0.999986285714286">
The algorithm Find-MDL terminates in time 0(N x Si),I where N denotes the number
of leaf nodes in the input thesaurus tree T and IS I denotes the input sample size, and
outputs a tree cut model of T with the minimum description length (with respect to
the encoding scheme described in Section 3.1).
Here we will give an intuitive explanation of why the proposition holds, and give
the formal proof in Appendix A. The MLE of each node (class) is obtained simply by
dividing the frequency of nouns within that class by the total sample size. Thus, the
parameter estimation for each subtree can be done independently from the estimation
of the parameters outside the subtree. The data description length for a subtree thus
depends solely on the tree cut within that subtree, and its calculation can be performed
independently for each subtree. As for the parameter description length for a subtree,
it depends only on the number of classes in the tree cut within that subtree, and hence
can be computed independently as well. The formal proof proceeds by mathematical
induction, which verifies that the optimal model in any (sub)tree is either the model
</bodyText>
<figure confidence="0.957124">
• 0.03
VEHICLE AIRPLANE
0.23
•
BIRD 0.51
INSECT
ENTITY
ANIMAL
LVARTIFACTI)=41.09
LVVEHICLE,AIRPLANED=40.97
LVAIRPLANED=32.27
L&apos;Wethelicopter,airplane])=32.72
ARTIFAS,I
■■•■•■•=0=
swallow crow eagle bird bug e insect car bike jet helicopter airplane
227
Computational Linguistics Volume 24, Number 2
</figure>
<bodyText confidence="0.9880295">
consisting of the root of the tree or the model obtained by appending the optimal
submodels for its child subtrees.7
</bodyText>
<subsectionHeader confidence="0.999533">
3.3 Estimation, Generalization, and MDL
</subsectionHeader>
<bodyText confidence="0.997294625">
When a discrete model (a partition F of the set of nouns AT in our present context) is
fixed, and the estimation problem involves only the estimation of probability parame-
ters, the classic maximum-likelihood estimation (MLE) is known to be satisfactory. In
particular, the estimation of a word-based model is one such problem, since the parti-
tion is fixed and the size of the partition equals 1./V-1. Furthermore, for a fixed discrete
model, it is known that MLE coincides with MDL: Given data S {x1 : i = 1,. . . , ml,
MLE estimates parameter P. which maximizes the likelihood with respect to the data;
that is:
</bodyText>
<equation confidence="0.999735">
P = arg max HP(xj). (13)
i=1
</equation>
<bodyText confidence="0.9217186875">
It is easy to see that P also satisfies:
= arg E _ log P( (14)
This is nothing but the MDL estimate in this case, since - log P(xi) is the data
description length.
When the estimation problem involves model selection, i.e., the choice of a tree cut
in the present context, MDL&apos;s behavior significantly deviates from that of MLE. This
is because MDL insists on minimizing the sum total of the data description length
and the model description length, while MLE is still equivalent to minimizing the data
description length only. So, for our problem of estimating a tree cut model, MDL tends
to select a model that is reasonably simple yet fits the data quite well, whereas the
model selected by MLE will be a word-based model (or a tree cut model equivalent
to the word-based moder), as it will always manage to fit the data.
In statistical terms, the superiority of MDL as an estimation method is related to
the fact we noted earlier that even though MLE can provide the best fit to the given
data, the estimation accuracy of the parameters is poor, when applied on a sample of
modest size, as there are too many parameters to estimate. MLE is likely to estimate
most parameters to be zero, and thus suffers from the data sparseness problem. Note
in Table 4, that MDL avoids this problem by taking into account the model complexity
as well as the fit to the data.
MDL stipulates that the model with the minimum description length should be
selected both for data compression and estimation. This intimate connection between
estimation and data compression can also be thought of as that between estimation and
generalization, since in order to compress information, generalization is necessary. In
our current problem, this corresponds to the generalization of individual nouns present
in case frame instances in the data as classes of nouns present in a given thesaurus. For
example, given the thesaurus in Figure 3 and frequency data in Figure 1, we would
7 The process of finding the MDL model tends to be computationally demanding and is often
intractable. When the model class under consideration is restricted to tree structures, however, dynamic
programming is often applicable and the MDL model can be efficiently found. For example, Rissanen
(1995) has devised an algorithm for learning decision trees.
8 Consider, for example, the case when the co-occurrence data is given as
f(swallow) = 2f (crow) -= 2,f(eagle) = 2,f(bird) = 2 for the problem in Section 2.
</bodyText>
<page confidence="0.984829">
228
</page>
<note confidence="0.848196">
Li and Abe Generalizing Case Frames
</note>
<bodyText confidence="0.999814555555556">
like our system to judge that the class BIRD and the noun bee can be the subject slot of
the verb fly. The problem of deciding whether to stop generalizing at BIRD and bee, or
generalizing further to ANIMAL has been addressed by a number of authors (Webster
and Marcus 1989; Velardi, Pazienza, and Fasolo 1991; Nomiyama 1992). Minimization
of the total description length provides a disciplined criterion to do this.
A remarkable fact about MDL is that theoretical findings have indeed verified that
MDL, as an estimation strategy, is near optimal in terms of the rate of convergence of
its estimated models to the true model as data size increases. When the true model
is included in the class of models considered, the models selected by MDL converge
</bodyText>
<subsectionHeader confidence="0.506239">
.log SI
</subsectionHeader>
<bodyText confidence="0.997558428571429">
to the true model at the rate of 0(k* 2.1s,I ), where k* is the number of parameters in
the true model, and IS I the data size, which is near optimal (Barron and Cover 1991;
Yamanishi 1992).
Thus, in the current problem, MDL provides (a) a way of smoothing probability
parameters to solve the data sparseness problem, and at the same time, (b) a way
of generalizing nouns in the data to noun classes of an appropriate level, both as a
corollary to the near optimal estimation of the distribution of the given data.
</bodyText>
<subsectionHeader confidence="0.985275">
3.4 The Bayesian Interpretation of MDL and the Choice of Encoding Scheme
</subsectionHeader>
<bodyText confidence="0.99954475">
There is a Bayesian interpretation of MDL: MDL is essentially equivalent to the &amp;quot;pos-
terior mode&amp;quot; in the Bayesian terminology (Rissanen 1989). Given data S and a number
of models, the Bayesian estimator (posterior mode) selects a model /c/I that maximizes
the posterior probability:
</bodyText>
<equation confidence="0.530799">
= arg max (P(M) • P(S I M)) (15)
</equation>
<bodyText confidence="0.999985">
where P(M) denotes the prior probability of the model M and P(S I M) the probability
of observing the data S given M. Equivalently, M satisfies
</bodyText>
<equation confidence="0.664481">
JI = arg miin(— log P(M) — log P(S I M)). (16)
</equation>
<bodyText confidence="0.99992095">
This is equivalent to the MDL estimate, if we take — log P(M) to be the model descrip-
tion length. Interpreting — log P(M) as the model description length translates, in the
Bayesian estimation, to assigning larger prior probabilities on simpler models, since it
is equivalent to assuming that P(M) = ( )1(M), where l(M) is the description length of
M. (Note that if we assign uniform prior probability P(M) to all models M, then (15)
becomes equivalent to (13), giving the maximum-likelihood estimate.)
Recall, that in our definition of parameter description length, we assign a shorter
parameter description length to a model with a smaller number of parameters k,
which admits the above interpretation. As for the model description length (for tree
cuts) we assigned an equal code length to each tree cut, which translates to placing
no bias on any cut. We could have employed a different coding scheme assigning
shorter code lengths to cuts nearer the root. We chose not to do so partly because, for
sufficiently large sample sizes, the parameter description length starts dominating the
model description length anyway.
Another important property of the definition of description length is that it af-
fects not only the effective prior probabilities on the models, but also the procedure
for computing the model minimizing the measure. Indeed, our definition of model
description length was chosen to be compatible with the dynamic programming tech-
nique, namely, its calculation is performable locally for each subtree. For a different
choice of coding scheme, it is possible that a simple and efficient MDL algorithm like
</bodyText>
<page confidence="0.990226">
229
</page>
<note confidence="0.619814">
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.987485">
Find-MDL may not exist. We believe that our choice of model description length is
derived from a natural encoding scheme with reasonable interpretation as Bayesian
prior, and at the same time allows an efficient algorithm for finding a model with the
minimum description length.
</bodyText>
<subsectionHeader confidence="0.993199">
3.5 The Uniform Distribution Assumption and the Level of Generalization
</subsectionHeader>
<bodyText confidence="0.999989548387097">
The uniform distribution assumption made in (4), namely that all nouns belonging
to a class contained in the tree cut model are assigned the same probability, seems
to be rather stringent. If one were to insist that the model be exactly accurate, then
it would seem that the true model would be the word-based model resulting from
no generalization at all. If we allow approximations, however, it is likely that some
reasonable tree cut model with the uniform probability assumption will be a good
approximation of the true distribution; in fact, a best model for a given data size. As
we remarked earlier, as MDL balances between the fit to the data and the simplicity
of the model, one can expect that the model selected by MDL will be a reasonable
compromise.
Nonetheless, it is still a shortcoming of our model that it contains an oversimplified
assumption, and the problem is especially pressing when rare words are involved. Rare
words may not be observed at a slot of interest in the data simply because they are
rare, and not because they are unfit for that particular slot.9 To see how rare is too
rare for our method, consider the following example.
Suppose that the class BIRD contains 10 words, bird, swallow, crow, eagle, parrot,
waxwing, etc. Consider co-occurrence data having 8 occurrences of bird, 2 occurrences
of swallow, 1 occurrence of crow, 1 occurrence of eagle, and 0 occurrence of all other
words, as part of, say, 100 data obtained for the subject slot of verb fly. For this data
set, our method would select the model that generalizes bird, swallow, etc. to the class
BIRD, since the sum of the data and parameter description lengths for the BIRD subtree
is 76.57 + 3.32 = 79.89 if generalized, and 53.73 + 33.22 = 86.95 if not generalized. For
comparison, consider the data with 10 occurrences of bird, 3 occurrences of swallow
and 1 occurrence of crow, and 0 occurrence of all other words, also as part of 100
data for the subject slot of fly. In this case, our method would select the model that
stops generalizing at bird, swallow, eagle, etc., because the description length for the
same subtree now is 86.22 + 3.32 = 89.54 if generalized, and 55.04 + 33.22 = 88.26 if
not generalized. These examples seem to indicate that our MDL-based method would
choose to generalize, even when there are relatively large differences in frequencies of
words within a class, but knows enough to stop generalizing when the discrepancy
in frequencies is especially noticeable (relative to the given sample size).
</bodyText>
<sectionHeader confidence="0.992447" genericHeader="method">
4. Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999732">
4.1 Experiment 1: A Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999107">
We applied our generalization method to large corpora and inspected the obtained
tree cut models to see if they agreed with human intuition. In our experiments, we
extracted verbs and their case frame slots (verb, slot _name , slot _value triples) from the
tagged texts of the Wall Street Journal corpus (ACL/DCI CD-ROM1) consisting of
126,084 sentences, using existing techniques (specifically, those in Smadja [19931), then
</bodyText>
<footnote confidence="0.41111475">
9 There are several possible measures that one could take to address this issue, including the
incorporation of absolute frequencies of the words (inside and outside the particular slot in question).
This is outside the scope of the present paper, and we simply refer the interested reader to one possible
approach (Abe and Li 1996).
</footnote>
<page confidence="0.991717">
230
</page>
<note confidence="0.987064">
Li and Abe Generalizing Case Frames
</note>
<tableCaption confidence="0.989193">
Table 6
</tableCaption>
<bodyText confidence="0.969197113636364">
Example input data (for the direct object slot of eat).
eat arg2 food 3 eat arg2 lobster 1 eat arg2 seed 1
eat arg2 heart 2 eat arg2 liver 1 eat arg2 plant 1
eat arg2 sandwich 2 eat arg2 crab 1 eat arg2 elephant 1
eat arg2 meal 2 eat arg2 rope 1 eat arg2 seafood 1
eat arg2 amount 2 eat arg2 horse 1 eat arg2 mushroom 1
eat arg2 night 2 eat arg2 bug 1 eat arg2 ketchup 1
eat arg2 lunch 2 eat arg2 bowl 1 eat arg2 sawdust 1
eat arg2 snack 2 eat arg2 month 1 eat arg2 egg 1
eat arg2 jam 2 eat arg2 effect 1 eat arg2 sprout 1
eat arg2 diet 1 eat arg2 debt 1 eat arg2 nail 1
eat arg2 pizza 1 eat arg2 oyster 1
applied our method to generalize the slot_values. Table 6 shows some example triple
data for the direct object slot of the verb eat.
There were some extraction errors present in the data, but we chose not to remove
them, because in general there will always be extraction errors and realistic evaluation
should leave them in.
When generalizing, we used the noun taxonomy of WordNet (version 1.4) (Miller
1995) as our thesaurus. The noun taxonomy of WordNet has a structure of directed
acyclic graph (DAG), and its nodes stand for a word sense (a concept) and often
contain several words having the same word sense. WordNet thus deviates from our
notion of thesaurus—a tree in which each leaf node stands for a noun, each internal
node stands for the class of nouns below it, and a noun is uniquely represented by a
leaf node—so we took a few measures to deal with this.
First, we modified our algorithm Find-MDL so that it can be applied to a DAG;
now, Find-MDL effectively copies each subgraph having multiple parents (and its
associated data) so that the DAG is transformed to a tree structure. Note that with
this modification it is no longer guaranteed that the output model is optimal. Next,
we dealt heuristically with the issue of word-sense ambiguity by equally dividing the
observed frequency of a noun between all the nodes containing that noun. Finally,
when an internal node contained nouns actually occurring in the data, we assigned
the .frequencies of all the nodes below it to that internal node, and excised the whole
subtree (subgraph) below it. The last of these measures, in effect, defines the &amp;quot;starting
cut&amp;quot; of the thesaurus from which to begin generalizing. Since (word senses of) nouns
that occur in natural language tend to concentrate in the middle of a taxonomy, the
starting cut given by this method usually falls around the middle of the thesaurus.1°
Figure 9 shows the starting cut and the resulting cut in WordNet for the direct
object slot of eat with respect to the data in Table 6, where (...) denotes a node in
WordNet. The starting cut consists of nodes (plant ...),(food),etc, which are the high-
est nodes containing values of the direct object slot of eat. Since (food) has significantly
higher frequencies than its neighbors (solid) and (fluid), the generalization stops there
according to MDL. In contrast, the nodes under (life_form ...) have relatively small dif-
ferences in their frequencies, and thus they are generalized to the node (life_form ...).
The same is true of the nodes under (artifact). Since (... amount ...) has a much
</bodyText>
<footnote confidence="0.844276666666667">
10 Cognitive scientists have observed that concepts in the middle of a taxonomy tend to be more
important with respect to learning, recognition, and memory, and their linguistic expressions occur
more frequently in natural language—a phenomenon known as basic level primacy. See Lakoff (1987).
</footnote>
<page confidence="0.9861">
231
</page>
<figure confidence="0.969644428571429">
Computational Linguistics Volume 24, Number 2
TOP
&lt;entity&gt; &lt;abstraction&gt;
0 10
-
&lt;measure,gparrtity,amount...&gt; &lt;space&gt; &lt;ti▪ m• e&gt;
0.08
</figure>
<table confidence="0.861969217391304">
.. — resulting cut
&lt;plant.&gt; s &lt;animal...&gt; &lt;substance&gt; &lt;artifact... - - - starting cut
1 . I % t,
I I A ,
, i , , 0 39 I
1 I I &apos; t
t i , -•
- - .
I i I &apos; , asolid&gt; &lt;fluid&gt; &lt;food&gt;.
I I I Ax 1
St
I / I X / x •
1 x I
I % /
/ I I X II
.
I \ 1
P 1 , r
. x t
41 , • ...
I &apos;■
&lt;mushroom&gt; &lt;lobster&gt; &lt;horse&gt; &lt;lobster&gt; &lt;pizza&gt; &lt;rope&gt;
&lt;life_form...&gt; 0.11 &lt;object&gt;
</table>
<figureCaption confidence="0.92219">
Figure 9
</figureCaption>
<bodyText confidence="0.994351222222222">
An example generalization result (for the direct object slot of eat).
higher frequency than its neighbors (time) and (space), the generalization does not
go up higher. All of these results seem to agree with human intuition, indicating that
our method results in an appropriate level of generalization.
Table 7 shows generalization results for the direct object slot of eat and some
other arbitrarily selected verbs, where classes are sorted in descending order of their
probability values. (Classes with probabilities less than 0.05 are discarded due to space
limitations.)
Table 8 shows the computation time required (on a SPARC &amp;quot;Ultra 1&amp;quot; work station)
to obtain the results shown in Table 7. (The computation time for loading the WordNet
was excluded since it need be done only once.) Even though the noun taxonomy of
WordNet is a large thesaurus containing approximately 50,000 nodes, our method
still manages to efficiently generalize case slots using it. The table also shows the
average number of levels generalized for each slot, namely, the average number of
links between a node in the starting cut and its ancestor node in the resulting cut.
(For example, the number of levels generalized for (plant...) is one in Figure 9.) One
can see that a significant amount of generalization is performed by our method—the
resulting tree cut is about 5 levels higher than the starting cut, on the average.
</bodyText>
<subsectionHeader confidence="0.997812">
4.2 Experiment 2: PP-Attachment Disambiguation
</subsectionHeader>
<bodyText confidence="0.994671833333333">
Case frame patterns obtained by our method can be used in various tasks in natu-
ral language processing. In this paper, we test its effectiveness in a structural (PP-
attachment) disambiguation experiment.
Disambiguation Methods. It has been empirically verified that the use of lexical semantic
knowledge is effective in structural disambiguation, such as the PP-attachment prob-
lem (Hobbs and Bear 1990; Whittemore, Ferrara, and Brunner 1990). There have been
</bodyText>
<page confidence="0.992047">
232
</page>
<note confidence="0.98322">
Li and Abe Generalizing Case Frames
</note>
<tableCaption confidence="0.7864405">
Table 7
Examples of generalization results.
</tableCaption>
<figure confidence="0.947094625">
Class Probability Example Words
Direct Object of eat
(food,nutrient) 0.39 pizza, egg
(life_form,organism,being,living_thing) 0.11 lobster, horse
(measure,quantity,amount,quantum) 0.10 amount of
(artifactarticle,artefact) 0.08 as if eat rope
Direct Object of buy
(objectinanimate_object,physical_object) 0.30 computer, painting
(asset) 0.10 stock, share
(group,grouping) 0.07 company, bank
(legal_documentlegalinstrument,official_document,... ) 0.05 security, ticket
Direct Object of fly
(entity) 0.35 airplane, flag, executive
(linear_measure,long_measure) 0.28 mile
(group,grouping) 0.08 delegation
Direct Object of operate
</figure>
<equation confidence="0.5609142">
(group,grouping) 0.13 company, fleet
(act,human_action,human_activity) 0.13 flight, operation
(structure,construction) 0.12 center
(abstraction) 0.11 service, unit
(possession) 0.06 profit, earnings
</equation>
<tableCaption confidence="0.967957">
Table 8
</tableCaption>
<table confidence="0.851868571428572">
Required computation time and number of generalized levels.
Verb CPU Time (second) Average Number of Generalized Levels
eat 1.00 5.2
buy 0.66 4.6
fly 1.11 6.0
operate 0.90 5.0
Average 0.92 5.2
</table>
<bodyText confidence="0.997797333333334">
many probabilistic methods proposed in the literature to address the PP-attachment
problem using lexical semantic knowledge which, in our view, can be classified into
three types.
The first approach (Hindle and Rooth 1991, 1993) takes doubles of the form
(verb, prep) and (nouni, prep), like those in Table 9, as training data to acquire semantic
knowledge and judges the attachment sites of the prepositional phrases in quadru-
ples of the form (verb, nouni, prep, noun2)—e.g., (see, girl, with, telescope)—based on
the acquired knowledge. Hindle and Rooth (1991) proposed the use of the lexical
association measure calculated based on such doubles. More specifically, they esti-
mate P(prep I verb) and P(prep nouni), and calculate the so-called t-score, which is
a measure of the statistical significance of the difference between P(prep I verb) and
P(prep I nouni). If the t-score indicates that the former probability is significantly larger,
</bodyText>
<page confidence="0.997822">
233
</page>
<note confidence="0.484177">
Computational Linguistics Volume 24, Number 2
</note>
<tableCaption confidence="0.886070428571428">
Table 9
Example input data as doubles.
see in
see with
girl with
man with
Table 10
</tableCaption>
<figureCaption confidence="0.8245825">
Example input data as triples.
see in park
see with telescope
girl with scarf
see with friend
man with hat
</figureCaption>
<tableCaption confidence="0.971233">
Table 11
</tableCaption>
<bodyText confidence="0.983918129032258">
Example input data as quadruples and
labels.
see girl in park ADV
see man with telescope ADV
see girl with scarf ADN
then the prepositional phrase is attached to verb, if the latter probability is significantly
larger, it is attached to nouni, and otherwise no decision is made.
The second approach (Sekine et al. 1992; Chang, Luo, and Su 1992; Resnik 1993a;
Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2)
and (nouni , prep, noun2), like those in Table 10, as training data for acquiring semantic
knowledge and performs PP-attachment disambiguation on quadruples. For example,
Resnik (1993a) proposes the use of the selectional association measure calculated based
on such triples, as described in Section 2. More specifically, his method compares
maxaass19noun2 A(Class, I verb, prep) and maxClass13n0un2 A(Class, I nouni, prep) to make
disambiguation decisions.
The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994;
Collins and Brooks 1995) receives quadruples (verb, nouni, prep, noun2) and labels indi-
cating which way the PP-attachment goes, like those in Table 11, and learns a disam-
biguation rule for resolving PP-attachment ambiguities. For example, Brill and Resnik,
(1994) propose a method they call transformation-based error-driven learning (see also
Brill [1995]). Their method first learns IF-THEN type rules, where the IF parts repre-
sent conditions like (prep is with) and (verb is see), and the THEN parts represent
transformations from (attach to verb) to (attach to nouni), or vice versa. The first rule
is always a default decision, and all the other rules indicate transformations (changes
of attachment sites) subject to various IF conditions.
We note that, for the disambiguation problem, the first two approaches are basi-
cally unsupervised learning methods, in the sense that the training data are merely
positive examples for both types of attachments, which could in principle be extracted
from pure corpus data with no human intervention. (For example, one could just
use unambiguous sentences.) The third approach, on the other hand, is a supervised
learning method, which requires labeled data prepared by a human being.
</bodyText>
<page confidence="0.998359">
234
</page>
<note confidence="0.989555">
Li and Abe Generalizing Case Frames
</note>
<tableCaption confidence="0.998156">
Table 12
</tableCaption>
<table confidence="0.993678714285714">
Number of different types of data.
Training Data
Average number of doubles per data set 91218.1
Average number of triples per data set 91218.1
Average number of quadruples per data set 21656.6
Test Data
Average number of quadruples per data set 820.4
</table>
<bodyText confidence="0.970227411764706">
The generalization method we propose falls into the second category, although it
can also be used as a component in a combined scheme with many of the above meth-
ods (see Brill and Resnik [1994], Alshawi and Carter [1994]). We estimate P(noun2 I
verb, prep) and P(noun2 I nouni, prep) from training data consisting of triples, and com-
pare them: If the former exceeds the latter (by a certain margin) we attach it to verb,
else if the latter exceeds the former (by the same margin) we attach it to nouni.
In our experiments, described below, we compare the performance of our proposed
method, which we refer to as MDL, against the methods proposed by Hindle and
Rooth (1991), Resnik (1993b), and Brill and Resnik (1994), referred to respectively as
LA, SA, and TEL.
Data Set. We used the bracketed corpus of the Penn Treebank (Wall Street Journal cor-
pus) (Marcus, Santorini, and Marcinkiewicz 1993) as our data. First we randomly
selected one of the 26 directories of the WSJ files as the test data and what remains as
the training data. We repeated this process 10 times and obtained 10 sets of data con-
sisting of different training data and test data. We used these 10 data sets to conduct
cross-validation as described below.
From the test data in each data set, we extracted (verb, nouni, prep, noun2) quadru-
ples using the extraction tool provided by the Penn Treebank called &amp;quot;tgrep.&amp;quot; At the
same time, we obtained the answer for the PP-attachment site for each quadruple.
We did not double-check if the answers provided in the Penn Treebank were actually
correct or not. Then from the training data of each data set, we extracted (verb, prep)
and (noun, prep) doubles, and (verb, prep, noun2) and (nounl, prep, noun2) triples using
tools we developed ourselves. We also extracted quadruples from the training data as
before. We then applied 12 heuristic rules to further preprocess the data, which include
(1) changing the inflected form of a word to its stem form, (2) replacing numerals with
the word number, (3) replacing integers between 1,900 and 2,999 with the word year, (4)
replacing co., ltd., etc. with the words company, limited, etc.11 After preprocessing there
still remained some minor errors, which we did not remove further, due to the lack
of a good method for doing so automatically. Table 12 shows the number of different
types of data obtained by the above process.
Experimental Procedure. We first compared the accuracy and coverage for each of the
three disambiguation methods based on unsupervised learning: MDL, SA, and LA.
11 The experimental results obtained here are better than those obtained in our preliminary experiment
(Li and Abe 1995), in part because we only adopted rule (1) in the past.
</bodyText>
<page confidence="0.990429">
235
</page>
<figure confidence="0.999142352941176">
Computational Linguistics Volume 24, Number 2
&amp;quot;MDL&amp;quot; -e—
&amp;quot;SA&amp;quot; -+--
0.98 -
&amp;quot;LA.t&amp;quot; x
0.96
0.94 -
0.92
0.9
rri
0.88
0.86 -
0.84 -
0.82 -
0.8
0 0.2 0.4 0.6 0.8
coverage
</figure>
<figureCaption confidence="0.880023">
Figure 10
</figureCaption>
<bodyText confidence="0.985673666666667">
Accuracy-coverage curves for MDL, SA, and LA.
For MDL, we generalized noun2 given (verb, prep, noun2) and (nounl, prep, noun2)
triples as training data for each data set, using WordNet as the thesaurus in the same
manner as in experiment 1. When disambiguating, we actually compared P(Classi I
verb, prep) and P(Class2 I noun, prep), where Classi and Class2 are classes in the out-
put tree cut models dominating noun2 in place of P(noun2 I verb, prep) and P(noun2
noun1,prep).12 We found that doing so gives a slightly better result. For SA, we em-
ployed a somewhat simplified version in which noun2 is generalized given (verb, prep,
noun2) and (nouni,prep,noun2) triples using WordNet, and maxaass,3n0un2 A(Class,
verb, prep) and maxClass,3noun2 A(Class j nouni,prep) are compared for disambiguation:
If the former exceeds the latter then the prepositional phrase is attached to verb, and
otherwise to nouni. For LA, we estimated P(prep I verb) and P(prep I nouni) from the
training data of each data set and compared them for disambiguation. We then eval-
uated the results achieved by the three methods in terms of accuracy and coverage.
Here, coverage refers to the proportion as a percentage, of the test quadruples on
which the disambiguation method could make a decision, and accuracy refers to the
proportion of correct decisions among them.
In Figure 10, we plot the accuracy-coverage curves for the three methods. In plot-
ting these curves, the attachment site is determined by simply seeing if the difference
between the appropriate measures for the two alternatives, be it probabilities or selec-
tional association values, exceeds a threshold. For each method, the threshold was set
successively to 0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, and 0.75. When the difference between
the two measures is less than a threshold, we rule that no decision can be made. These
curves were obtained by averaging over the 10 data sets.
</bodyText>
<footnote confidence="0.971284333333333">
12 Recall that a node in WordNet represents a word sense and not a word; noun2 can belong to several
classes in the thesaurus. We thus use MaXaaSS, noun2 (P(Classi I verb, prep)) and
maxaass,3„,,,n2(P(ClassiI nouni, Prep)) in place of P(Classi I verb, prep) and P(Class2 I nounl, preP)•
</footnote>
<page confidence="0.993159">
236
</page>
<note confidence="0.992684">
Li and Abe Generalizing Case Frames
</note>
<tableCaption confidence="0.997155">
Table 13
</tableCaption>
<table confidence="0.994543625">
Results of PP-attachment disambiguation.
Coverage(%) Accuracy(%)
Default 100 56.2
MDL + Default 100 82.2
SA + Default 100 76.7
LA + Default 100 80.7
LA.t + Default 100 78.1
TEL 100 82.4
</table>
<bodyText confidence="0.99992509375">
We also implemented the exact method proposed by Hindle and Rooth (1991),
which makes disambiguation judgement using the t-score. Figure 10 shows the re-
sult as LA.t, where the threshold for t-score is set to 1.28 (significance level of 90
percent.) From Figure 10 we see that with respect to accuracy-coverage curves, MDL
outperforms both SA and LA throughout, while SA is better than LA.
Next, we tested the method of applying a default rule after applying each method.
That is, attaching (prep, noun2) to verb for the part of the test data for which no deci-
sion was made by the method in question.&apos; We refer to these combined methods as
MDL+Default, SA+Default, LA+Default, and LA.t+Default. Table 13 shows the results,
again averaged over the 10 data sets.
Finally, we used the transformation-based error-driven learning (TEL) to acquire
transformation rules for each data set and applied the obtained rules to disambiguate
the test data. The average number of obtained rules for a data set was 2,752.3. Table 13
shows the disambiguation result averaged over the 10 data sets. From Table 13, we
see that TEL performs the best, edging over the second place MDL+Default by a small
margin, and then followed by LA+Default, and SA+Default. Below we discuss further
observations concerning these results.
MDL and SA. According to our experimental results, the accuracy and coverage of
MDL appear to be somewhat better than those of SA. As Resnik (1993b) pointed
out, the use of selectional association log P(pc(1c7) seems to be appropriate for cognitive
modeling. Our experiments show, however, that the generalization method currently
employed by Resnik has a tendency to overfit the data. Table 14 shows example gener-
alization results for MDL (with classes with probability less than 0.05 discarded) and
SA. Note that MDL tends to select a tree cut closer to the root of the thesaurus tree.
This is probably the key reason why MDL has a wider coverage than SA for the same
degree of accuracy. One may be concerned that MDL is &amp;quot;overgeneralizing&amp;quot; here,14 but
as shown in Figure 10, its disambiguation accuracy does not seem to be degraded.
Another problem that must be dealt with concerning SA is how to remove noise
(resulting, for example, from erroneous extraction) from the generalization results.
Since SA estimates the ratio between two probability values, namely P(pcA&apos;r), the gen-
eralization result may be lead astray if one of the estimates of P(C I v, r) and P(C) is
unreliable. For instance, a high estimated value for (drop, bead, pearl) at protect against
</bodyText>
<footnote confidence="0.98367175">
13 Interestingly, for the entire data set it is more favorable to attach (prep, noun2) to nouni, but for what
remains after applying LA and MDL, it turns out to be more favorable to attach (prep, noun2) to verb.
14 Note that in Experiment 1, there were more data available, and thus the data were more appropriately
generalized.
</footnote>
<page confidence="0.98416">
237
</page>
<note confidence="0.584145">
Computational Linguistics Volume 24, Number 2
</note>
<tableCaption confidence="0.966698">
Table 14
</tableCaption>
<table confidence="0.997263657142857">
Example generalization results for SA and MDL.
Input
Verb Preposition Noun Frequency
protect against accusation 1
protect against damage 1
protect against decline 1
protect against drop 1
protect against loss 1
protect against resistance 1
protect against squall 1
protect against vagary 1
Generalization Result of MDL
Verb Preposition Noun Class Probability
protect against (act,human_action,human_activity) 0.212
protect against (phenomenon) 0.170
protect against (psychological_feature) 0.099
protect against (event) 0.097
protect against (abstraction) 0.093
Generalization Result of SA
Verb Preposition Noun Class SA
protect against (caprice,impulse,vagary,whim) 1.528
protect against (phenomenon) 0.899
protect against (happening,occurrence,natural_event) 0.339
protect against (deterioration,worsening,decline,declination) 0.285
protect against (act,human_action,human_activity) 0.260
protect against (drop,bead,pearl) 0.202
protect against (drop) 0.202
protect against (descent,declivity,fall,decline,downslope) 0.188
prot6ct against (resistor,resistance) 0.130
protect against (underground,resistance) 0.130
protect against (immunity,resistance) 0.124
protect against (resistance,opposition) 0.111
protect against (loss,deprivation) 0.105
protect against (loss) 0.096
protect against (cost,price,terms,darriage) 0.052
</table>
<bodyText confidence="0.9986246">
shown in Table 14 is rather odd, and is because the estimate of P(C) is unreliable (too
small). This problem apparently costs SA a nonnegligible drop in disambiguation ac-
curacy. In contrast, MDL does not suffer from this problem since a high estimated
probability value is only possible with high frequency, which cannot result just from
extraction errors. Consider, for example, the occurrence of car in the data shown in
Figure 8, which has supposedly resulted from an erroneous extraction. The effect of
this datum gets washed away, as the estimated probability for VEHICLE, to which car
has been generalized, is negligible.
On the other hand, SA has a merit not shared by MDL, namely its use of the
association ratio factors out the effect of absolute frequencies of words, and focuses
</bodyText>
<page confidence="0.997319">
238
</page>
<note confidence="0.99263">
Li and Abe Generalizing Case Frames
</note>
<tableCaption confidence="0.99621">
Table 15
</tableCaption>
<table confidence="0.9709399">
Some hard examples for LA.
Attached to verb Attached to nouni
acquire interest in year
buy stock in trade
ease restriction on export
forecast sale for year
make payment on million
meet standard for resistance
reach agreement in august
show interest in session
win verdict in winter
acquire interest in firm
buy stock in index
ease restriction on type
forecast sale for venture
make payment on debt
meet standard for car
reach agreement in principle
show interest in stock
win verdict in case
</table>
<bodyText confidence="0.999454409090909">
on their co-occurrence relation. Since both MDL and SA have pros and cons, it would
be desirable to develop a methodology that combines the merits of the two methods
(cf. Abe and Li [1996]).
MDL and LA. LA makes its disambiguation decision completely ignoring noun2. As
Resnik (1993b) pointed out, if we hope to improve disambiguation performance by
increasing training data, we need a richer model such as those used in MDL and SA.
We found that 8.8% of the quadruples in our entire test data were such that they shared
the same verb, prep, nouni but had different noun2, and their PP-attachment sites go both
ways in the same data, i.e., both to verb and to nouni. Clearly, for these examples, the
PP-attachment site cannot be reliably determined without knowing noun2. Table 15
shows some of these examples. (We adopted the attachment sites given in the Penn
Tree Bank, without correcting apparently wrong judgements.)
MDL and TEL. We chose TEL as an example of the quadruple approach. This method
was designed specifically for the purpose of resolving PP-attachment ambiguities, and
seems to perform slightly better than ours.
As we remarked earlier, however, the input data required by our method (triples)
could be generated automatically from unparsed corpora making use of existing
heuristic rules (Brent 1993; Smadja 1993), although for the experiments we report
here we used a parsed corpus. Thus it would seem to be easier to obtain more data
in the future for MDL and other methods based on unsupervised learning. Also note
that our method of generalizing values of a case slot can be used for purposes other
than disambiguation.
</bodyText>
<sectionHeader confidence="0.994648" genericHeader="method">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.999898">
We proposed a new method of generalizing case frames. Our approach of applying
MDL to estimate a tree cut model in an existing thesaurus is not limited to just the
problem of generalizing values of a case frame slot. It is potentially useful in other
natural language processing tasks, such as the problem of estimating n-gram models
(Brown et al. 1992) or the problem of semantic tagging (Cucchiarelli and Velardi 1997).
We believe that our method has the following merits: (1) it is theoretically sound; (2) it
is computationally efficient; (3) it is robust against noise. Our experimental results
indicate that the performance of our method is better than, or at least comparable
to, existing methods. One of the disadvantages of our method is that its performance
</bodyText>
<page confidence="0.994583">
239
</page>
<note confidence="0.728974">
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.995604666666667">
depends on the structure of the particular thesaurus used. This, however, is a prob-
lem commonly shared by any generalization method that uses a thesaurus as prior
knowledge.
</bodyText>
<sectionHeader confidence="0.966697" genericHeader="method">
Appendix A: Proof of Proposition 1
</sectionHeader>
<subsectionHeader confidence="0.684058">
Proof
</subsectionHeader>
<bodyText confidence="0.981301">
For an arbitrary subtree T&apos; of a thesaurus tree T and an arbitrary tree cut model
M = 0) of T, let MT, = (UT&apos;, ) denote the submodel of M that is contained in
T&apos;. Also for any sample S and any subtree T&apos; of T, let ST, denote the subsample of S
contained in T&apos;. (Note that MT = M, ST = S.) Then define, in general for any submodel
MT&apos; and subsample ST&apos;, UST&apos; I , Or) to be the data description length of subsample
ST, using submodel L(Or UT&apos;) to be the parameter description length for the
submodel Mr, and L&apos;(MT, ) to be L(ST, I UT&apos;, GT&apos;) + L(OT, I Up). (Note that, when
calculating the parameter description length for a submodel, the sample size of the
entire sample ISI is used.)
First note that for any (sub)tree T, (sub)model MT = (FT, UT) contained in T, and
(sub)sample ST contained in T, and T&apos;s child subtrees T, : i = 1, . ,k, we have:
</bodyText>
<equation confidence="0.999491">
L(sT I rT,OT) = L(ST, I UT, OT,) (17)
i=1
</equation>
<bodyText confidence="0.998218666666667">
provided that FT is not a single node (root node of T). This follows from the mutual
disjointness of the T,, and the independence of the parameters in the Ti.
We also have, when T is a proper subtree of the thesaurus tree:
</bodyText>
<equation confidence="0.986026">
L(OT I UT) = L(OT, I UT). (18)
i=1
</equation>
<bodyText confidence="0.9985575">
Since the number of free parameters of a model in the entire thesaurus tree equals
the number of nodes in the model minus one due to the stochastic condition (that the
probability parameters must sum to one), when T equals the entire thesaurus tree,
theoretically the parameter description length for a tree cut model of T should be:
</bodyText>
<equation confidence="0.987283333333333">
L(OT I UT) = r)
= EL0T, UT)log ISIi 2 (19)
i=1
</equation>
<bodyText confidence="0.999541222222222">
where IS I is the size of the entire sample. Since the second term log21s1 in (19) is
constant once the input sample S is fixed, for the purpose of finding a model with the
minimum description length, it is irrelevant. We will thus use the identity (18) both
when T is the entire tree and when it is a proper subtree. (This allows us to use the
same recursive algorithm, Find-MDL, in all cases.)
It follows from (17) and (18) that the minimization of description length can be
done essentially independently for each subtree. Namely, if we let L&apos;,,,„(A4T, ST) denote
the minimum description length (as defined by [17] and [181) achievable for (sub)model
MT on (sub)sample ST contained in (sub)tree T, Ps(n) the MLE estimate for node n
</bodyText>
<page confidence="0.984966">
240
</page>
<note confidence="0.742793">
Li and Abe Generalizing Case Frames
</note>
<bodyText confidence="0.9711566875">
using the entire sample S. and root(T) the root node of tree T, then we have:
(MT, ST) min { E ST,),
L&apos; ( ( [root( T)l, [Ps (Toot( TM ), ST) } (20)
The rest of the proof proceeds by induction. First, when T is of a single leaf
node, the submodel consisting solely of the node and the MLE of the generation
probability for the class represented by T is returned, which is clearly a submodel
with minimum description length in the subtree T. Next, inductively assume that
Find-MDL(T&apos;) correctly outputs a (sub)model with the minimum description length
for any tree T&apos; of size less than n. Then, given a tree T of size n whose root node has at
least two children, say T : i = 1,. . . ,k, for each T,, Find-MDL(T1) returns a (sub)model
with the minimum description length by the inductive hypothesis. Then, since (20)
holds, whichever way the if-clause on lines 8, 9 of Find-MDL evaluates to, what is
returned on line 11 or line 13 will still be a (sub)model with the minimum description
length, completing the inductive step.
It is easy to see that the running time of the algorithm is linear in both the number
of leaf nodes of the input thesaurus tree and the input sample size. •
</bodyText>
<sectionHeader confidence="0.912769" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.963970588235294">
We are grateful to K. Nakamura and
T. Fujita of NEC C&amp;C Res. Labs. for their
constant encouragement. We thank
K. Yaminishi and J. Takeuchi of C&amp;C Res.
Labs, for their suggestions and comments.
We thank T. Futagami of NIS for his
programming efforts. We also express our
special appreciation to the two anonymous
reviewers who have provided many
valuable comments. We acknowledge the
ACL for providing the ACL/DCI CD-ROM,
LDC of the University of Pennsylvania for
providing the Penn Treebank corpus data,
and Princeton University for providing
WordNet, and E. Brill and P. Resnik for
providing their PP-attachment
disambiguation program.
</bodyText>
<sectionHeader confidence="0.944346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993117465116279">
Abe, Naoki and Hang Li. 1996. Learning
word association norms using tree cut
pair models. Proceedings of the Thirteenth
International Conference on Machine
Learning, pages 3-11.
Almuallim, Hussein, Yasuhiro Akiba,
Takefumi Yamazaki, Akio Yokoo, and
Shigeo Kaneda. 1994. Two methods for
ALT-J/E translation rules from examples
and a semantic hierarchy. Proceedings of the
Fifteenth International Conference on
Computational Linguistics, pages 57-63.
Alshawi, Hiyan and David Carter. 1994.
Training and scaling preference functions
for disambiguation. Computational
Linguistics, 20(4):635-648.
Barron, Andrew R. and Thomas M. Cover.
1991. Minimum complexity density
estimation. IEEE Transaction on Information
Theory, 37(4):1034-1054.
Brent, Michael R. 1993. From grammar to
lexicon: Unsupervised learning of lexical
syntax. Computational Linguistics,
19(2):243-262.
Brent, Michael R. and Timothy A.
Cartwright. 1996. Distributional regularity
and phonotactic constraints are useful for
segmentation. Cognition, 61:93-125.
Brent, Michael R., Sreerama K. Murthy, and
Andrew Lundberg. 1995. Discovering
morphemic suffixes: A case study in
minimum description length induction.
Proceedings of the Fifth International
Workshop on Artificial Intelligence and
Statistics.
Brill, Eric. 1995. Transformation-based
error-driven learning and natural
language processing: A case study in
part-of-speech tagging. Computational
Linguistics, 21(4):543-565.
Brill, Eric and Philip Resnik. 1994. A
rule-based approach to prepositional
phrase attachment disambiguation.
</reference>
<page confidence="0.99239">
241
</page>
<note confidence="0.760394">
Computational Linguistics Volume 24, Number 2
</note>
<reference confidence="0.991251909090909">
Proceedings of the Fifteenth International
Conference on Computational Linguistics,
pages 1198-1204.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. Proceedings of the Fifth
Conference on Applied Natural Language
Processing.
Brown, Peter F., Vincent J. Della Pietra,
Peter V. deSouza, Jenifer C. Lai, and
Robert L. Mercer, 1992. Class-based
n-gram models of natural language.
Computational Linguistics, 18(4):283-298.
Cartwright, Timothy A. and Michael R.
Brent. 1994. Segmenting speech without a
lexicon: The roles of phonotactics and
speech source. Proceedings of the First
Meeting of the ACL Special Interest Group in
Computational Phonology, pages 83-90.
Chang, Jing-Shin, Yih-Fen Luo, and Keh-Yih
Su. 1992. GPSM: A generalized
probabilistic semantic model for
ambiguity resolution. Proceedings of the
30th Annual Meeting, pages 177-184.
Association for Computational
Linguistics.
Collins, Michael and James Brooks. 1995.
Prepositional phrase attachment through
a backed-off model. Proceedings of the Third
Workshop on Very Large Corpora.
Cover, Thomas M. and Joy A. Thomas.
1991. Elements of Information Theory. John
Wiley &amp; Sons Inc., New York.
Cucchiarelli, Alessandro and Paola Velardi.
1997. Automatic selection of class labels
from a thesaurus for an effective semantic
tagging of corpora. Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 380-387.
Dagan, Ido, Shaul Marcus, and Shaul
Makovitch. 1992. Contextual word
similarity and estimation from sparse
data. Proceedings of the 30th Annual
Meeting, pages 164-171. Association for
Computational Linguistics.
Dagan, Ido, Fernando Pereira, and Lillian
Lee. 1994. Similarity-based estimation of
word cooccurrence probabilities.
Proceedings of the 32nd Annual Meeting,
pages 272-278. Association for
Computational Linguistics.
Ellison, T. Mark. 1991. Discovering planar
segregations. Proceedings of AAAI Spring
Symposium on Machine Learning of Natural
Language and Ontology, pages 42-47.
Ellison, T. Mark. 1992. Discovering vowel
harmony. In Walter Daelmans and David
Powers, editors, Background and
Experiments in Machine Learning of Natural
Language, pages 205-207.
Framis, Francesc Ribas. 1994. An
experiment on learning appropriate
selectional restrictions from a parsed
corpus. Proceedings of the Fifteenth
International Conference on Computational
Linguistics, pages 769-774.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Boston.
Grishman, Ralph and John Sterling. 1992.
Acquisition of selectional patterns.
Proceedings of the Fourteenth International
Conference on Computational Linguistics,
pages 658-664.
Grishman, Ralph and John Sterling. 1994.
Generalizing automatically generated
selectional patterns. Proceedings of the
Fifteenth International Conference on
Computational Linguistics, pages 742-747.
Grunwald, Peter. 1996. A minimum
description length approach to grammar
inference. In S. Wemter, E. Riloff, and
G. Scheler, editors, Symbolic, Connectionist
and Statistical Approaches to Learning for
Natural Language Processing, Lecture Note in
Al. Springer Verlag, pages 203-216.
Hindle, Donald and Mats Rooth. 1991.
Structural ambiguity and lexical relations.
Proceedings of the 29th Annual Meeting,
pages 229-236. Association for
Computational Linguistics.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103-120.
Hobbs, Jerry R. and John Bear. 1990. Two
principles of parse preference. Proceedings
of the Thirteenth International Conference on
Computational Linguistics, pages 162-167.
Lakoff, George. 1987. Women, Fire, and
Dangerous Things: What Categories Reveal
about the Mind. The University of Chicago
Press.
Li, Hang and Naoki Abe. 1995. Generalizing
case frames using a thesaurus and the
MDL principle. Proceedings of Recent
Advances in Natural Language Processing,
pages 239-248.
Li, Hang and Naoki Abe. 1996. Learning
dependencies between case frame slots.
Proceedings of the Sixteenth International
Conference on Computational Linguistics,
pages 10-15.
Manning, Christopher D. 1992. Automatic
acquisition of a large subcategorization
dictionary from corpora. Proceedings of the
30th Annual Meeting, pages 235-242.
Association for Computational
Linguistics.
Marcus, Mitchell R, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
</reference>
<page confidence="0.995821">
242
</page>
<note confidence="0.967419">
Li and Abe Generalizing Case Frames
</note>
<reference confidence="0.999839032786885">
Penn Treebank. Computational Linguistics,
19(1):313-330.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of
the ACM, pages 39-41.
Nomiyama, Hiroshi. 1992. Machine
translation by case generalization.
Proceedings of the Fourteenth International
Conference on Computational Linguistics,
pages 714-720.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. Proceedings of the 31st
Annual Meeting, pages 183-190.
Association for Computational
Linguistics.
Quinlan, J. Ross and Ronald L. Rivest. 1989.
Inferring decision trees using the
minimum description length principle.
Information and Computation, 80:227-248.
Ratnaparkhi, Adwait, Jeff Reynar, and Salim
Roukos. 1994. A maximum entropy model
for prepositional phrase attachment.
Proceedings of ARPA Workshop on Human
Language Technology, pages 250-255.
Resnik, Philip. 1992. WordNet and
distributional analysis: A class-based
approach to lexical discovery. Proceedings
of AAAI Workshop on Statistically-based NLP
Techniques.
Resnik, Philip. 1993a. Selection and
Information: A Class-based Approach to
Lexical Relationships. Ph.D. Thesis, Univ. of
Pennsylvania.
Resnik, Philip. 1993b. Semantic classes and
syntactic ambiguity. Proceedings of ARPA
Workshop on Human Language Technology.
Rissanen, Jorma. 1978. Modeling by shortest
data description. Automatic, 14:37-38.
Rissanen, Jorma. 1983. A universal prior for
integers and estimation by minimum
description length. The Annals of Statistics,
11(2):416-431.
Rissanen, Jorma. 1984. Universal coding,
information, predication and estimation.
IEEE Transaction on Information Theory,
30(4):629-636.
Rissanen, Jorma. 1986. Stochastic complexity
and modeling. The Annals of Statistics,
14(3):1080-1100.
Rissanen, Jorma. 1989. Stochastic Complexity
in Statistical Inquiry. World Scientific
Publishing Co., Singapore.
Rissanen, Jorma. 1995. Stochastic complexity
in learning. Proceedings of the Second
European Conference on Computational
Learning Theory (Euro Colt&apos;95), pages
196-210.
Ristad, Eric Sven and Robert G. Thomas.
1995. New techniques for context
modeling. Proceedings of the 33rd Annual
Meeting. Association for Computational
Linguistics.
Schwarz, G. 1978. Estimation of the
dimension of a model. Annals of Statistics,
6:416 446.
Sekine, Satoshi, Jeremy J. Carroll, Sofia
Ananiadou, and Jun&apos;ichi Tsujii. 1992.
Automatic learning for semantic
collocation. Proceedings of the Third
Conference on Applied Natural Language
Processing, pages 104-110.
Smadja, Frank. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19(1):143-177.
Solomonoff, R.J. 1964. A formal theory of
inductive inference 1 and 2. Information
and Control, 7:1-22;224-254.
Stolcke, Andreas and Stephen Omohundro.
1994. Inducing probabilistic grammars by
bayesian model merging. In Rafael C.
Carrasco and Jose Oncina, editors,
Grammatical Inference and Applications.
Springer Verlag, pages 106-118.
Tanaka, Hideki. 1994. Verbal case frame
acquisition from a bilingual corpus:
Gradual knowledge acquisition.
Proceedings of the Fifteenth International
Conference on Computational Linguistics,
pages 727-731.
Tanaka, Hideki. 1996. Decision tree learning
algorithm with structured attributes:
Application to verbal case frame
acquisition. Proceedings of the Sixteenth
International Conference on Computational
Linguistics, pages 943-948.
Utsuro, Takehito and Yuji Matsumoto. 1997.
Learning probabilistic subcategorization
preference by identifying case
dependencies and optimal noun class
generalization level. Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 364-371.
Utsuro, Takehito, Yuji Matsumoto, and
Makoto Nagao. 1992. Lexical knowledge
acquisition from bilingual corpora.
Proceedings of the Fourteenth International
Conference on Computational Linguistics,
pages 581-587.
Velardi, Paola, Maria Teresa Pazienza, and
Michela Fasolo. 1991. How to encode
semantic knowledge: A method for
meaning representation and
computer-aided acquisition. Computational
Linguistics, 17(2):153-170.
Wallace, C. and D. M. Boulton. 1968. An
information measure for classification.
Computer Journal, 11:185-195.
Wallace, C. and P. Freeman. 1992.
Single-factor analysis by minimum
message length estimation. Journal of Royal
Statistical Society, B, 54:195-209.
</reference>
<page confidence="0.96457">
243
</page>
<note confidence="0.35426">
Computational Linguistics Volume 24, Number 2
</note>
<reference confidence="0.999343357142857">
Webster, Mort and Mitch Marcus. 1989.
Automatic acquisition of the lexical
semantics of verbs from sentence frames.
Proceedings of the 27th Annual Meeting,
pages 177-184. Association for
Computational Linguistics.
Whittemore, Greg, Kathleen Ferrara, and
Hans Brunner. 1990. Empirical study of
predictive powers of simple attachment
schemes for post-modifier prepositional
phrases. Proceedings of the 28th Annual
Meeting, pages 23-30. Association for
Computational Linguistics.
Yamanishi, Kenji. 1992. A learning criterion
for stochastic rules. Machine Learning,
9:165-203.
Yarowsky, David. 1992. Word-sense
disambiguation using statistical models of
Roget&apos;s categories trained on large
corpora. Proceedings of the fourteenth
International Conference on Computational
Linguistics, pages 454-460.
Yarowsky, David. 1994. Decision lists for
lexical ambiguity resolution: Application
to accent restoration in Spanish and
French. Proceedings of the 32nd Annual
Meeting, pages 88-95. Association for
Computational Linguistics.
</reference>
<page confidence="0.998534">
244
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950799">
<title confidence="0.9967345">Generalizing Case Frames Using a Thesaurus and the MDL Principle</title>
<author confidence="0.990034">Hang Li Naoki Abe</author>
<affiliation confidence="0.99514">NEC Corporation NEC Corporation</affiliation>
<abstract confidence="0.994828363636364">A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as &amp;quot;cuts&amp;quot; in the thesaurus tree, thus reducing the generalization problem to that of estimating a &amp;quot;tree cut model&amp;quot; of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Naoki Abe</author>
<author>Hang Li</author>
</authors>
<title>Learning word association norms using tree cut pair models.</title>
<date>1996</date>
<booktitle>Proceedings of the Thirteenth International Conference on Machine Learning,</booktitle>
<pages>3--11</pages>
<contexts>
<context position="37289" citStr="Abe and Li 1996" startWordPosition="6402" endWordPosition="6405">n our experiments, we extracted verbs and their case frame slots (verb, slot _name , slot _value triples) from the tagged texts of the Wall Street Journal corpus (ACL/DCI CD-ROM1) consisting of 126,084 sentences, using existing techniques (specifically, those in Smadja [19931), then 9 There are several possible measures that one could take to address this issue, including the incorporation of absolute frequencies of the words (inside and outside the particular slot in question). This is outside the scope of the present paper, and we simply refer the interested reader to one possible approach (Abe and Li 1996). 230 Li and Abe Generalizing Case Frames Table 6 Example input data (for the direct object slot of eat). eat arg2 food 3 eat arg2 lobster 1 eat arg2 seed 1 eat arg2 heart 2 eat arg2 liver 1 eat arg2 plant 1 eat arg2 sandwich 2 eat arg2 crab 1 eat arg2 elephant 1 eat arg2 meal 2 eat arg2 rope 1 eat arg2 seafood 1 eat arg2 amount 2 eat arg2 horse 1 eat arg2 mushroom 1 eat arg2 night 2 eat arg2 bug 1 eat arg2 ketchup 1 eat arg2 lunch 2 eat arg2 bowl 1 eat arg2 sawdust 1 eat arg2 snack 2 eat arg2 month 1 eat arg2 egg 1 eat arg2 jam 2 eat arg2 effect 1 eat arg2 sprout 1 eat arg2 diet 1 eat arg2 de</context>
</contexts>
<marker>Abe, Li, 1996</marker>
<rawString>Abe, Naoki and Hang Li. 1996. Learning word association norms using tree cut pair models. Proceedings of the Thirteenth International Conference on Machine Learning, pages 3-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hussein Almuallim</author>
</authors>
<title>Yasuhiro Akiba, Takefumi Yamazaki, Akio Yokoo, and Shigeo Kaneda.</title>
<date>1994</date>
<booktitle>Proceedings of the Fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>57--63</pages>
<marker>Almuallim, 1994</marker>
<rawString>Almuallim, Hussein, Yasuhiro Akiba, Takefumi Yamazaki, Akio Yokoo, and Shigeo Kaneda. 1994. Two methods for ALT-J/E translation rules from examples and a semantic hierarchy. Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 57-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>David Carter</author>
</authors>
<title>Training and scaling preference functions for disambiguation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<contexts>
<context position="45936" citStr="Alshawi and Carter 1994" startWordPosition="7847" endWordPosition="7850"> 24, Number 2 Table 9 Example input data as doubles. see in see with girl with man with Table 10 Example input data as triples. see in park see with telescope girl with scarf see with friend man with hat Table 11 Example input data as quadruples and labels. see girl in park ADV see man with telescope ADV see girl with scarf ADN then the prepositional phrase is attached to verb, if the latter probability is significantly larger, it is attached to nouni, and otherwise no decision is made. The second approach (Sekine et al. 1992; Chang, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nouni , prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxaass19noun2 A(Class, I verb, prep) and maxClass13n0un2 A(Class, I nouni, prep) to make disambiguation decisions. The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995) r</context>
</contexts>
<marker>Alshawi, Carter, 1994</marker>
<rawString>Alshawi, Hiyan and David Carter. 1994. Training and scaling preference functions for disambiguation. Computational Linguistics, 20(4):635-648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Barron</author>
<author>Thomas M Cover</author>
</authors>
<title>Minimum complexity density estimation.</title>
<date>1991</date>
<journal>IEEE Transaction on Information Theory,</journal>
<pages>37--4</pages>
<contexts>
<context position="30986" citStr="Barron and Cover 1991" startWordPosition="5351" endWordPosition="5354">omiyama 1992). Minimization of the total description length provides a disciplined criterion to do this. A remarkable fact about MDL is that theoretical findings have indeed verified that MDL, as an estimation strategy, is near optimal in terms of the rate of convergence of its estimated models to the true model as data size increases. When the true model is included in the class of models considered, the models selected by MDL converge .log SI to the true model at the rate of 0(k* 2.1s,I ), where k* is the number of parameters in the true model, and IS I the data size, which is near optimal (Barron and Cover 1991; Yamanishi 1992). Thus, in the current problem, MDL provides (a) a way of smoothing probability parameters to solve the data sparseness problem, and at the same time, (b) a way of generalizing nouns in the data to noun classes of an appropriate level, both as a corollary to the near optimal estimation of the distribution of the given data. 3.4 The Bayesian Interpretation of MDL and the Choice of Encoding Scheme There is a Bayesian interpretation of MDL: MDL is essentially equivalent to the &amp;quot;posterior mode&amp;quot; in the Bayesian terminology (Rissanen 1989). Given data S and a number of models, the B</context>
</contexts>
<marker>Barron, Cover, 1991</marker>
<rawString>Barron, Andrew R. and Thomas M. Cover. 1991. Minimum complexity density estimation. IEEE Transaction on Information Theory, 37(4):1034-1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>From grammar to lexicon: Unsupervised learning of lexical syntax.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="2563" citStr="Brent 1993" startWordPosition="397" endWordPosition="398">the latter issue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1</context>
<context position="60318" citStr="Brent 1993" startWordPosition="10153" endWordPosition="10154">chment site cannot be reliably determined without knowing noun2. Table 15 shows some of these examples. (We adopted the attachment sites given in the Penn Tree Bank, without correcting apparently wrong judgements.) MDL and TEL. We chose TEL as an example of the quadruple approach. This method was designed specifically for the purpose of resolving PP-attachment ambiguities, and seems to perform slightly better than ours. As we remarked earlier, however, the input data required by our method (triples) could be generated automatically from unparsed corpora making use of existing heuristic rules (Brent 1993; Smadja 1993), although for the experiments we report here we used a parsed corpus. Thus it would seem to be easier to obtain more data in the future for MDL and other methods based on unsupervised learning. Also note that our method of generalizing values of a case slot can be used for purposes other than disambiguation. 5. Conclusions We proposed a new method of generalizing case frames. Our approach of applying MDL to estimate a tree cut model in an existing thesaurus is not limited to just the problem of generalizing values of a case frame slot. It is potentially useful in other natural l</context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Brent, Michael R. 1993. From grammar to lexicon: Unsupervised learning of lexical syntax. Computational Linguistics, 19(2):243-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Timothy A Cartwright</author>
</authors>
<title>Distributional regularity and phonotactic constraints are useful for segmentation.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--93</pages>
<contexts>
<context position="6418" citStr="Brent and Cartwright 1996" startWordPosition="1024" endWordPosition="1027">set of nouns Ar = {ni, n2, . , nN} , v in the set of verbs V = {vi, 02, , vv}, and r in the set of slot names R. = r2, ...,rRI, satisfying: E P(n I v,r) = 1. (1) nEAr This type of probability model is often referred to as a word-based model. Since the number of probability parameters in word-based models is large (0(N •V .R)), accurate 1 Recently, MDL and related techniques have become popular in corpus-based natural language processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright 1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern acquisition. 218 Li and Abe Generalizing Case Frames Table 1 Example (verb, slot_name, slot_value) triple data. verb slot_name slot_value fly argl bee fly argl bird fly argl bird fly argl crow fly argl bird fly argl eagle fly argl bee fly argl eagle fly argl bird fly argl crow Figure 1 Frequency data for the subject slot of verb fly. estimation of a word-based model is difficult with the data size that is available in practice—a problem usually referred to as the data sparseness problem. For example, suppo</context>
</contexts>
<marker>Brent, Cartwright, 1996</marker>
<rawString>Brent, Michael R. and Timothy A. Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. Cognition, 61:93-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Sreerama K Murthy</author>
<author>Andrew Lundberg</author>
</authors>
<title>Discovering morphemic suffixes: A case study in minimum description length induction.</title>
<date>1995</date>
<booktitle>Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics.</booktitle>
<marker>Brent, Murthy, Lundberg, 1995</marker>
<rawString>Brent, Michael R., Sreerama K. Murthy, and Andrew Lundberg. 1995. Discovering morphemic suffixes: A case study in minimum description length induction. Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<marker>Brill, 1995</marker>
<rawString>Brill, Eric. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Philip Resnik</author>
</authors>
<title>A rule-based approach to prepositional phrase attachment disambiguation.</title>
<date>1994</date>
<booktitle>Proceedings of the Fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>1198--1204</pages>
<contexts>
<context position="46470" citStr="Brill and Resnik 1994" startWordPosition="7924" endWordPosition="7927">g, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nouni , prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxaass19noun2 A(Class, I verb, prep) and maxClass13n0un2 A(Class, I nouni, prep) to make disambiguation decisions. The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995) receives quadruples (verb, nouni, prep, noun2) and labels indicating which way the PP-attachment goes, like those in Table 11, and learns a disambiguation rule for resolving PP-attachment ambiguities. For example, Brill and Resnik, (1994) propose a method they call transformation-based error-driven learning (see also Brill [1995]). Their method first learns IF-THEN type rules, where the IF parts represent conditions like (prep is with) and (verb is see), and the THEN parts represent transformations from (attach to verb) to (attac</context>
<context position="48757" citStr="Brill and Resnik (1994)" startWordPosition="8297" endWordPosition="8300">sed as a component in a combined scheme with many of the above methods (see Brill and Resnik [1994], Alshawi and Carter [1994]). We estimate P(noun2 I verb, prep) and P(noun2 I nouni, prep) from training data consisting of triples, and compare them: If the former exceeds the latter (by a certain margin) we attach it to verb, else if the latter exceeds the former (by the same margin) we attach it to nouni. In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by Hindle and Rooth (1991), Resnik (1993b), and Brill and Resnik (1994), referred to respectively as LA, SA, and TEL. Data Set. We used the bracketed corpus of the Penn Treebank (Wall Street Journal corpus) (Marcus, Santorini, and Marcinkiewicz 1993) as our data. First we randomly selected one of the 26 directories of the WSJ files as the test data and what remains as the training data. We repeated this process 10 times and obtained 10 sets of data consisting of different training data and test data. We used these 10 data sets to conduct cross-validation as described below. From the test data in each data set, we extracted (verb, nouni, prep, noun2) quadruples us</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>Brill, Eric and Philip Resnik. 1994. A rule-based approach to prepositional phrase attachment disambiguation. Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 1198-1204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>Proceedings of the Fifth Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="2622" citStr="Briscoe and Carroll 1997" startWordPosition="403" endWordPosition="406"> pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumo</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Briscoe, Ted and John Carroll. 1997. Automatic extraction of subcategorization from corpora. Proceedings of the Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="61011" citStr="Brown et al. 1992" startWordPosition="10271" endWordPosition="10274">d corpus. Thus it would seem to be easier to obtain more data in the future for MDL and other methods based on unsupervised learning. Also note that our method of generalizing values of a case slot can be used for purposes other than disambiguation. 5. Conclusions We proposed a new method of generalizing case frames. Our approach of applying MDL to estimate a tree cut model in an existing thesaurus is not limited to just the problem of generalizing values of a case frame slot. It is potentially useful in other natural language processing tasks, such as the problem of estimating n-gram models (Brown et al. 1992) or the problem of semantic tagging (Cucchiarelli and Velardi 1997). We believe that our method has the following merits: (1) it is theoretically sound; (2) it is computationally efficient; (3) it is robust against noise. Our experimental results indicate that the performance of our method is better than, or at least comparable to, existing methods. One of the disadvantages of our method is that its performance 239 Computational Linguistics Volume 24, Number 2 depends on the structure of the particular thesaurus used. This, however, is a problem commonly shared by any generalization method tha</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer, 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):283-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy A Cartwright</author>
<author>Michael R Brent</author>
</authors>
<title>Segmenting speech without a lexicon: The roles of phonotactics and speech source.</title>
<date>1994</date>
<booktitle>Proceedings of the First Meeting of the ACL Special Interest Group in Computational Phonology,</booktitle>
<pages>83--90</pages>
<contexts>
<context position="6305" citStr="Cartwright and Brent 1994" startWordPosition="1007" endWordPosition="1010">n be represented by a probability model that specifies the conditional probability P(n I v, r) for each n in the set of nouns Ar = {ni, n2, . , nN} , v in the set of verbs V = {vi, 02, , vv}, and r in the set of slot names R. = r2, ...,rRI, satisfying: E P(n I v,r) = 1. (1) nEAr This type of probability model is often referred to as a word-based model. Since the number of probability parameters in word-based models is large (0(N •V .R)), accurate 1 Recently, MDL and related techniques have become popular in corpus-based natural language processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright 1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern acquisition. 218 Li and Abe Generalizing Case Frames Table 1 Example (verb, slot_name, slot_value) triple data. verb slot_name slot_value fly argl bee fly argl bird fly argl bird fly argl crow fly argl bird fly argl eagle fly argl bee fly argl eagle fly argl bird fly argl crow Figure 1 Frequency data for the subject slot of verb fly. estimation of a word-based model is difficult with the data si</context>
</contexts>
<marker>Cartwright, Brent, 1994</marker>
<rawString>Cartwright, Timothy A. and Michael R. Brent. 1994. Segmenting speech without a lexicon: The roles of phonotactics and speech source. Proceedings of the First Meeting of the ACL Special Interest Group in Computational Phonology, pages 83-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing-Shin Chang</author>
<author>Yih-Fen Luo</author>
<author>Keh-Yih Su</author>
</authors>
<title>GPSM: A generalized probabilistic semantic model for ambiguity resolution.</title>
<date>1992</date>
<booktitle>Proceedings of the 30th Annual Meeting,</booktitle>
<pages>177--184</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Chang, Luo, Su, 1992</marker>
<rawString>Chang, Jing-Shin, Yih-Fen Luo, and Keh-Yih Su. 1992. GPSM: A generalized probabilistic semantic model for ambiguity resolution. Proceedings of the 30th Annual Meeting, pages 177-184. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional phrase attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>Proceedings of the Third Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="46534" citStr="Collins and Brooks 1995" startWordPosition="7933" endWordPosition="7936">; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nouni , prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxaass19noun2 A(Class, I verb, prep) and maxClass13n0un2 A(Class, I nouni, prep) to make disambiguation decisions. The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995) receives quadruples (verb, nouni, prep, noun2) and labels indicating which way the PP-attachment goes, like those in Table 11, and learns a disambiguation rule for resolving PP-attachment ambiguities. For example, Brill and Resnik, (1994) propose a method they call transformation-based error-driven learning (see also Brill [1995]). Their method first learns IF-THEN type rules, where the IF parts represent conditions like (prep is with) and (verb is see), and the THEN parts represent transformations from (attach to verb) to (attach to nouni), or vice versa. The first rule is always a default d</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Collins, Michael and James Brooks. 1995. Prepositional phrase attachment through a backed-off model. Proceedings of the Third Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="17217" citStr="Cover and Thomas 1991" startWordPosition="2908" endWordPosition="2911"> a model and the goodness of fit to the data. In the MDL framework, the model description length is an indicator of model 3 Estimation strategies related to MDL have been independently proposed and studied by various authors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992). 4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction to the MDL principle. 5 The KL distance (also known as KL-divergence or relative entropy), which is widely used in information theory and statistics, is a measure of distance between two distributions (e.g., Cover and Thomas 1991). It is always nonnegative and is zero if and only if the two distributions are identical, but is asymmetric and hence not a metric (the usual notion of distance). 223 Computational Linguistics Volume 24, Number 2 complexity, while the data description length indicates goodness of fit to the data. The MDL principle stipulates that the model that minimizes the sum total of the description lengths should be the best model (both for data compression and statistical estimation). In the remainder of this section, we will describe how we apply MDL to our current problem. We will then discuss the rat</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, Thomas M. and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley &amp; Sons Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Cucchiarelli</author>
<author>Paola Velardi</author>
</authors>
<title>Automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora.</title>
<date>1997</date>
<booktitle>Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>380--387</pages>
<contexts>
<context position="61078" citStr="Cucchiarelli and Velardi 1997" startWordPosition="10281" endWordPosition="10284">e data in the future for MDL and other methods based on unsupervised learning. Also note that our method of generalizing values of a case slot can be used for purposes other than disambiguation. 5. Conclusions We proposed a new method of generalizing case frames. Our approach of applying MDL to estimate a tree cut model in an existing thesaurus is not limited to just the problem of generalizing values of a case frame slot. It is potentially useful in other natural language processing tasks, such as the problem of estimating n-gram models (Brown et al. 1992) or the problem of semantic tagging (Cucchiarelli and Velardi 1997). We believe that our method has the following merits: (1) it is theoretically sound; (2) it is computationally efficient; (3) it is robust against noise. Our experimental results indicate that the performance of our method is better than, or at least comparable to, existing methods. One of the disadvantages of our method is that its performance 239 Computational Linguistics Volume 24, Number 2 depends on the structure of the particular thesaurus used. This, however, is a problem commonly shared by any generalization method that uses a thesaurus as prior knowledge. Appendix A: Proof of Proposi</context>
</contexts>
<marker>Cucchiarelli, Velardi, 1997</marker>
<rawString>Cucchiarelli, Alessandro and Paola Velardi. 1997. Automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora. Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 380-387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Shaul Marcus</author>
<author>Shaul Makovitch</author>
</authors>
<title>Contextual word similarity and estimation from sparse data.</title>
<date>1992</date>
<booktitle>Proceedings of the 30th Annual Meeting,</booktitle>
<pages>164--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Dagan, Marcus, Makovitch, 1992</marker>
<rawString>Dagan, Ido, Shaul Marcus, and Shaul Makovitch. 1992. Contextual word similarity and estimation from sparse data. Proceedings of the 30th Annual Meeting, pages 164-171. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Fernando Pereira</author>
<author>Lillian Lee</author>
</authors>
<title>Similarity-based estimation of word cooccurrence probabilities.</title>
<date>1994</date>
<booktitle>Proceedings of the 32nd Annual Meeting,</booktitle>
<pages>272--278</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Dagan, Pereira, Lee, 1994</marker>
<rawString>Dagan, Ido, Fernando Pereira, and Lillian Lee. 1994. Similarity-based estimation of word cooccurrence probabilities. Proceedings of the 32nd Annual Meeting, pages 272-278. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mark Ellison</author>
</authors>
<title>Discovering planar segregations.</title>
<date>1991</date>
<booktitle>Proceedings of AAAI Spring Symposium on Machine Learning of Natural Language and Ontology,</booktitle>
<pages>42--47</pages>
<contexts>
<context position="6272" citStr="Ellison 1991" startWordPosition="1004" endWordPosition="1005">onal distribution can be represented by a probability model that specifies the conditional probability P(n I v, r) for each n in the set of nouns Ar = {ni, n2, . , nN} , v in the set of verbs V = {vi, 02, , vv}, and r in the set of slot names R. = r2, ...,rRI, satisfying: E P(n I v,r) = 1. (1) nEAr This type of probability model is often referred to as a word-based model. Since the number of probability parameters in word-based models is large (0(N •V .R)), accurate 1 Recently, MDL and related techniques have become popular in corpus-based natural language processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright 1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern acquisition. 218 Li and Abe Generalizing Case Frames Table 1 Example (verb, slot_name, slot_value) triple data. verb slot_name slot_value fly argl bee fly argl bird fly argl bird fly argl crow fly argl bird fly argl eagle fly argl bee fly argl eagle fly argl bird fly argl crow Figure 1 Frequency data for the subject slot of verb fly. estimation of a word-based mo</context>
</contexts>
<marker>Ellison, 1991</marker>
<rawString>Ellison, T. Mark. 1991. Discovering planar segregations. Proceedings of AAAI Spring Symposium on Machine Learning of Natural Language and Ontology, pages 42-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mark Ellison</author>
</authors>
<title>Discovering vowel harmony.</title>
<date>1992</date>
<booktitle>In Walter Daelmans and</booktitle>
<pages>205--207</pages>
<editor>David Powers, editors,</editor>
<marker>Ellison, 1992</marker>
<rawString>Ellison, T. Mark. 1992. Discovering vowel harmony. In Walter Daelmans and David Powers, editors, Background and Experiments in Machine Learning of Natural Language, pages 205-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesc Ribas Framis</author>
</authors>
<title>An experiment on learning appropriate selectional restrictions from a parsed corpus.</title>
<date>1994</date>
<booktitle>Proceedings of the Fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>769--774</pages>
<contexts>
<context position="3166" citStr="Framis 1994" startWordPosition="486" endWordPosition="487">ent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based o</context>
</contexts>
<marker>Framis, 1994</marker>
<rawString>Framis, Francesc Ribas. 1994. An experiment on learning appropriate selectional restrictions from a parsed corpus. Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 769-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<contexts>
<context position="2595" citStr="Grefenstette 1994" startWordPosition="401" endWordPosition="402">e frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tana</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, Gregory. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>Acquisition of selectional patterns.</title>
<date>1992</date>
<booktitle>Proceedings of the Fourteenth International Conference on Computational Linguistics,</booktitle>
<pages>658--664</pages>
<contexts>
<context position="2502" citStr="Grishman and Sterling 1992" startWordPosition="385" endWordPosition="389">ssue, and refer the interested reader to Li and Abe (1996), which deals with the latter issue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the</context>
</contexts>
<marker>Grishman, Sterling, 1992</marker>
<rawString>Grishman, Ralph and John Sterling. 1992. Acquisition of selectional patterns. Proceedings of the Fourteenth International Conference on Computational Linguistics, pages 658-664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>Generalizing automatically generated selectional patterns.</title>
<date>1994</date>
<booktitle>Proceedings of the Fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>742--747</pages>
<contexts>
<context position="3338" citStr="Grishman and Sterling 1994" startWordPosition="513" endWordPosition="516">ed completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length principle (MDL): a principle of data compression and statistical estimation from information theory.&apos; In order to assist with efficiency, o</context>
<context position="7704" citStr="Grishman and Sterling (1994)" startWordPosition="1235" endWordPosition="1238">E for short) to estimate the probability parameters of a conditional probability distribution, as described above, given the co-occurrence data in Figure 1. In this case, MLE amounts to estimating the parameters by simply normalizing the frequencies so that they sum to one, giving, for example, the estimated probabilities of 0, 0.2, and 0.4 for swallow, eagle, and bird, respectively (see Figure 2). Since in general the number of parameters exceeds the size of data that is typically available, MLE will result in estimating most of the probability parameters to be zero. To address this problem, Grishman and Sterling (1994) proposed a method of smoothing conditional probabilities using the probability values of similar words, where the similarity between words is judged based on co-occurrence data (see also Dagan, Marcus, and Makovitch [1992] and Dagan, Pereira, and Lee [19941). More specifically, conditional probabilities of words are smoothed by taking the weighted average of those of similar words using the similarity measure as the weights. The advantage of this approach is that it does not rely on any prior knowledge, but it appears difficult to find a smoothing method that is both efficient and theoretical</context>
<context position="45910" citStr="Grishman and Sterling 1994" startWordPosition="7843" endWordPosition="7846">utational Linguistics Volume 24, Number 2 Table 9 Example input data as doubles. see in see with girl with man with Table 10 Example input data as triples. see in park see with telescope girl with scarf see with friend man with hat Table 11 Example input data as quadruples and labels. see girl in park ADV see man with telescope ADV see girl with scarf ADN then the prepositional phrase is attached to verb, if the latter probability is significantly larger, it is attached to nouni, and otherwise no decision is made. The second approach (Sekine et al. 1992; Chang, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nouni , prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxaass19noun2 A(Class, I verb, prep) and maxClass13n0un2 A(Class, I nouni, prep) to make disambiguation decisions. The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994; </context>
</contexts>
<marker>Grishman, Sterling, 1994</marker>
<rawString>Grishman, Ralph and John Sterling. 1994. Generalizing automatically generated selectional patterns. Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 742-747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Grunwald</author>
</authors>
<title>A minimum description length approach to grammar inference.</title>
<date>1996</date>
<booktitle>Symbolic, Connectionist and Statistical Approaches to Learning for Natural Language Processing, Lecture Note in Al.</booktitle>
<pages>203--216</pages>
<editor>In S. Wemter, E. Riloff, and G. Scheler, editors,</editor>
<publisher>Springer Verlag,</publisher>
<contexts>
<context position="6434" citStr="Grunwald 1996" startWordPosition="1028" endWordPosition="1029">. , nN} , v in the set of verbs V = {vi, 02, , vv}, and r in the set of slot names R. = r2, ...,rRI, satisfying: E P(n I v,r) = 1. (1) nEAr This type of probability model is often referred to as a word-based model. Since the number of probability parameters in word-based models is large (0(N •V .R)), accurate 1 Recently, MDL and related techniques have become popular in corpus-based natural language processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright 1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern acquisition. 218 Li and Abe Generalizing Case Frames Table 1 Example (verb, slot_name, slot_value) triple data. verb slot_name slot_value fly argl bee fly argl bird fly argl bird fly argl crow fly argl bird fly argl eagle fly argl bee fly argl eagle fly argl bird fly argl crow Figure 1 Frequency data for the subject slot of verb fly. estimation of a word-based model is difficult with the data size that is available in practice—a problem usually referred to as the data sparseness problem. For example, suppose that we emplo</context>
</contexts>
<marker>Grunwald, 1996</marker>
<rawString>Grunwald, Peter. 1996. A minimum description length approach to grammar inference. In S. Wemter, E. Riloff, and G. Scheler, editors, Symbolic, Connectionist and Statistical Approaches to Learning for Natural Language Processing, Lecture Note in Al. Springer Verlag, pages 203-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1991</date>
<booktitle>Proceedings of the 29th Annual Meeting,</booktitle>
<pages>229--236</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2474" citStr="Hindle and Rooth 1991" startWordPosition="381" endWordPosition="384">rselves to the former issue, and refer the interested reader to Li and Abe (1996), which deals with the latter issue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make u</context>
<context position="44553" citStr="Hindle and Rooth 1991" startWordPosition="7618" endWordPosition="7621">(group,grouping) 0.13 company, fleet (act,human_action,human_activity) 0.13 flight, operation (structure,construction) 0.12 center (abstraction) 0.11 service, unit (possession) 0.06 profit, earnings Table 8 Required computation time and number of generalized levels. Verb CPU Time (second) Average Number of Generalized Levels eat 1.00 5.2 buy 0.66 4.6 fly 1.11 6.0 operate 0.90 5.0 Average 0.92 5.2 many probabilistic methods proposed in the literature to address the PP-attachment problem using lexical semantic knowledge which, in our view, can be classified into three types. The first approach (Hindle and Rooth 1991, 1993) takes doubles of the form (verb, prep) and (nouni, prep), like those in Table 9, as training data to acquire semantic knowledge and judges the attachment sites of the prepositional phrases in quadruples of the form (verb, nouni, prep, noun2)—e.g., (see, girl, with, telescope)—based on the acquired knowledge. Hindle and Rooth (1991) proposed the use of the lexical association measure calculated based on such doubles. More specifically, they estimate P(prep I verb) and P(prep nouni), and calculate the so-called t-score, which is a measure of the statistical significance of the difference</context>
<context position="48712" citStr="Hindle and Rooth (1991)" startWordPosition="8290" endWordPosition="8293">he second category, although it can also be used as a component in a combined scheme with many of the above methods (see Brill and Resnik [1994], Alshawi and Carter [1994]). We estimate P(noun2 I verb, prep) and P(noun2 I nouni, prep) from training data consisting of triples, and compare them: If the former exceeds the latter (by a certain margin) we attach it to verb, else if the latter exceeds the former (by the same margin) we attach it to nouni. In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by Hindle and Rooth (1991), Resnik (1993b), and Brill and Resnik (1994), referred to respectively as LA, SA, and TEL. Data Set. We used the bracketed corpus of the Penn Treebank (Wall Street Journal corpus) (Marcus, Santorini, and Marcinkiewicz 1993) as our data. First we randomly selected one of the 26 directories of the WSJ files as the test data and what remains as the training data. We repeated this process 10 times and obtained 10 sets of data consisting of different training data and test data. We used these 10 data sets to conduct cross-validation as described below. From the test data in each data set, we extra</context>
<context position="53462" citStr="Hindle and Rooth (1991)" startWordPosition="9089" endWordPosition="9092">aging over the 10 data sets. 12 Recall that a node in WordNet represents a word sense and not a word; noun2 can belong to several classes in the thesaurus. We thus use MaXaaSS, noun2 (P(Classi I verb, prep)) and maxaass,3„,,,n2(P(ClassiI nouni, Prep)) in place of P(Classi I verb, prep) and P(Class2 I nounl, preP)• 236 Li and Abe Generalizing Case Frames Table 13 Results of PP-attachment disambiguation. Coverage(%) Accuracy(%) Default 100 56.2 MDL + Default 100 82.2 SA + Default 100 76.7 LA + Default 100 80.7 LA.t + Default 100 78.1 TEL 100 82.4 We also implemented the exact method proposed by Hindle and Rooth (1991), which makes disambiguation judgement using the t-score. Figure 10 shows the result as LA.t, where the threshold for t-score is set to 1.28 (significance level of 90 percent.) From Figure 10 we see that with respect to accuracy-coverage curves, MDL outperforms both SA and LA throughout, while SA is better than LA. Next, we tested the method of applying a default rule after applying each method. That is, attaching (prep, noun2) to verb for the part of the test data for which no decision was made by the method in question.&apos; We refer to these combined methods as MDL+Default, SA+Default, LA+Defau</context>
</contexts>
<marker>Hindle, Rooth, 1991</marker>
<rawString>Hindle, Donald and Mats Rooth. 1991. Structural ambiguity and lexical relations. Proceedings of the 29th Annual Meeting, pages 229-236. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, Donald and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>John Bear</author>
</authors>
<title>Two principles of parse preference.</title>
<date>1990</date>
<booktitle>Proceedings of the Thirteenth International Conference on Computational Linguistics,</booktitle>
<pages>162--167</pages>
<contexts>
<context position="43165" citStr="Hobbs and Bear 1990" startWordPosition="7445" endWordPosition="7448">) One can see that a significant amount of generalization is performed by our method—the resulting tree cut is about 5 levels higher than the starting cut, on the average. 4.2 Experiment 2: PP-Attachment Disambiguation Case frame patterns obtained by our method can be used in various tasks in natural language processing. In this paper, we test its effectiveness in a structural (PPattachment) disambiguation experiment. Disambiguation Methods. It has been empirically verified that the use of lexical semantic knowledge is effective in structural disambiguation, such as the PP-attachment problem (Hobbs and Bear 1990; Whittemore, Ferrara, and Brunner 1990). There have been 232 Li and Abe Generalizing Case Frames Table 7 Examples of generalization results. Class Probability Example Words Direct Object of eat (food,nutrient) 0.39 pizza, egg (life_form,organism,being,living_thing) 0.11 lobster, horse (measure,quantity,amount,quantum) 0.10 amount of (artifactarticle,artefact) 0.08 as if eat rope Direct Object of buy (objectinanimate_object,physical_object) 0.30 computer, painting (asset) 0.10 stock, share (group,grouping) 0.07 company, bank (legal_documentlegalinstrument,official_document,... ) 0.05 security,</context>
</contexts>
<marker>Hobbs, Bear, 1990</marker>
<rawString>Hobbs, Jerry R. and John Bear. 1990. Two principles of parse preference. Proceedings of the Thirteenth International Conference on Computational Linguistics, pages 162-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
</authors>
<title>Women, Fire, and Dangerous Things: What Categories Reveal about the Mind.</title>
<date>1987</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="40812" citStr="Lakoff (1987)" startWordPosition="7031" endWordPosition="7032">eighbors (solid) and (fluid), the generalization stops there according to MDL. In contrast, the nodes under (life_form ...) have relatively small differences in their frequencies, and thus they are generalized to the node (life_form ...). The same is true of the nodes under (artifact). Since (... amount ...) has a much 10 Cognitive scientists have observed that concepts in the middle of a taxonomy tend to be more important with respect to learning, recognition, and memory, and their linguistic expressions occur more frequently in natural language—a phenomenon known as basic level primacy. See Lakoff (1987). 231 Computational Linguistics Volume 24, Number 2 TOP &lt;entity&gt; &lt;abstraction&gt; 0 10 - &lt;measure,gparrtity,amount...&gt; &lt;space&gt; &lt;ti▪ m• e&gt; 0.08 .. — resulting cut &lt;plant.&gt; s &lt;animal...&gt; &lt;substance&gt; &lt;artifact... - - - starting cut 1 . I % t, I I A , , i , , 0 39 I 1 I I &apos; t t i , -• - - . I i I &apos; , asolid&gt; &lt;fluid&gt; &lt;food&gt;. I I I Ax 1 St I / I X / x • 1 x I I % / / I I X II . I \ 1 P 1 , r . x t 41 , • ... I &apos;■ &lt;mushroom&gt; &lt;lobster&gt; &lt;horse&gt; &lt;lobster&gt; &lt;pizza&gt; &lt;rope&gt; &lt;life_form...&gt; 0.11 &lt;object&gt; Figure 9 An example generalization result (for the direct object slot of eat). higher frequency than its neig</context>
</contexts>
<marker>Lakoff, 1987</marker>
<rawString>Lakoff, George. 1987. Women, Fire, and Dangerous Things: What Categories Reveal about the Mind. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1995</date>
<booktitle>Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<pages>239--248</pages>
<contexts>
<context position="50711" citStr="Li and Abe 1995" startWordPosition="8624" endWordPosition="8627">2,999 with the word year, (4) replacing co., ltd., etc. with the words company, limited, etc.11 After preprocessing there still remained some minor errors, which we did not remove further, due to the lack of a good method for doing so automatically. Table 12 shows the number of different types of data obtained by the above process. Experimental Procedure. We first compared the accuracy and coverage for each of the three disambiguation methods based on unsupervised learning: MDL, SA, and LA. 11 The experimental results obtained here are better than those obtained in our preliminary experiment (Li and Abe 1995), in part because we only adopted rule (1) in the past. 235 Computational Linguistics Volume 24, Number 2 &amp;quot;MDL&amp;quot; -e— &amp;quot;SA&amp;quot; -+-- 0.98 - &amp;quot;LA.t&amp;quot; x 0.96 0.94 - 0.92 0.9 rri 0.88 0.86 - 0.84 - 0.82 - 0.8 0 0.2 0.4 0.6 0.8 coverage Figure 10 Accuracy-coverage curves for MDL, SA, and LA. For MDL, we generalized noun2 given (verb, prep, noun2) and (nounl, prep, noun2) triples as training data for each data set, using WordNet as the thesaurus in the same manner as in experiment 1. When disambiguating, we actually compared P(Classi I verb, prep) and P(Class2 I noun, prep), where Classi and Class2 are clas</context>
</contexts>
<marker>Li, Abe, 1995</marker>
<rawString>Li, Hang and Naoki Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. Proceedings of Recent Advances in Natural Language Processing, pages 239-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Learning dependencies between case frame slots.</title>
<date>1996</date>
<booktitle>Proceedings of the Sixteenth International Conference on Computational Linguistics,</booktitle>
<pages>10--15</pages>
<contexts>
<context position="1934" citStr="Li and Abe (1996)" startWordPosition="295" endWordPosition="298">tion to this problem would have a great impact on various tasks in natural language processing, including the structural disambiguation problem in parsing. The acquired knowledge would also be helpful for building a lexicon, as it would provide lexicographers with word usage descriptions. In our view, the problem of acquiring case frame patterns involves the following two issues: (a) acquiring patterns of individual case frame slots; and (b) learning dependencies that may exist between different slots. In this paper, we confine ourselves to the former issue, and refer the interested reader to Li and Abe (1996), which deals with the latter issue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumot</context>
</contexts>
<marker>Li, Abe, 1996</marker>
<rawString>Li, Hang and Naoki Abe. 1996. Learning dependencies between case frame slots. Proceedings of the Sixteenth International Conference on Computational Linguistics, pages 10-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1992</date>
<booktitle>Proceedings of the 30th Annual Meeting,</booktitle>
<pages>235--242</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2516" citStr="Manning 1992" startWordPosition="390" endWordPosition="391">ed reader to Li and Abe (1996), which deals with the latter issue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an ex</context>
</contexts>
<marker>Manning, 1992</marker>
<rawString>Manning, Christopher D. 1992. Automatic acquisition of a large subcategorization dictionary from corpora. Proceedings of the 30th Annual Meeting, pages 235-242. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell R Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English:</title>
<date>1993</date>
<publisher>The</publisher>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell R, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The</rawString>
</citation>
<citation valid="false">
<authors>
<author>Penn Treebank</author>
</authors>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<marker>Treebank, </marker>
<rawString>Penn Treebank. Computational Linguistics, 19(1):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>39--41</pages>
<contexts>
<context position="38354" citStr="Miller 1995" startWordPosition="6619" endWordPosition="6620"> eat arg2 sawdust 1 eat arg2 snack 2 eat arg2 month 1 eat arg2 egg 1 eat arg2 jam 2 eat arg2 effect 1 eat arg2 sprout 1 eat arg2 diet 1 eat arg2 debt 1 eat arg2 nail 1 eat arg2 pizza 1 eat arg2 oyster 1 applied our method to generalize the slot_values. Table 6 shows some example triple data for the direct object slot of the verb eat. There were some extraction errors present in the data, but we chose not to remove them, because in general there will always be extraction errors and realistic evaluation should leave them in. When generalizing, we used the noun taxonomy of WordNet (version 1.4) (Miller 1995) as our thesaurus. The noun taxonomy of WordNet has a structure of directed acyclic graph (DAG), and its nodes stand for a word sense (a concept) and often contain several words having the same word sense. WordNet thus deviates from our notion of thesaurus—a tree in which each leaf node stands for a noun, each internal node stands for the class of nouns below it, and a noun is uniquely represented by a leaf node—so we took a few measures to deal with this. First, we modified our algorithm Find-MDL so that it can be applied to a DAG; now, Find-MDL effectively copies each subgraph having multipl</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, George A. 1995. WordNet: A lexical database for English. Communications of the ACM, pages 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Nomiyama</author>
</authors>
<title>Machine translation by case generalization.</title>
<date>1992</date>
<booktitle>Proceedings of the Fourteenth International Conference on Computational Linguistics,</booktitle>
<pages>714--720</pages>
<contexts>
<context position="30378" citStr="Nomiyama 1992" startWordPosition="5244" endWordPosition="5245">y found. For example, Rissanen (1995) has devised an algorithm for learning decision trees. 8 Consider, for example, the case when the co-occurrence data is given as f(swallow) = 2f (crow) -= 2,f(eagle) = 2,f(bird) = 2 for the problem in Section 2. 228 Li and Abe Generalizing Case Frames like our system to judge that the class BIRD and the noun bee can be the subject slot of the verb fly. The problem of deciding whether to stop generalizing at BIRD and bee, or generalizing further to ANIMAL has been addressed by a number of authors (Webster and Marcus 1989; Velardi, Pazienza, and Fasolo 1991; Nomiyama 1992). Minimization of the total description length provides a disciplined criterion to do this. A remarkable fact about MDL is that theoretical findings have indeed verified that MDL, as an estimation strategy, is near optimal in terms of the rate of convergence of its estimated models to the true model as data size increases. When the true model is included in the class of models considered, the models selected by MDL converge .log SI to the true model at the rate of 0(k* 2.1s,I ), where k* is the number of parameters in the true model, and IS I the data size, which is near optimal (Barron and Co</context>
</contexts>
<marker>Nomiyama, 1992</marker>
<rawString>Nomiyama, Hiroshi. 1992. Machine translation by case generalization. Proceedings of the Fourteenth International Conference on Computational Linguistics, pages 714-720.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting,</booktitle>
<pages>183--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. Proceedings of the 31st Annual Meeting, pages 183-190. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
<author>Ronald L Rivest</author>
</authors>
<title>Inferring decision trees using the minimum description length principle.</title>
<date>1989</date>
<journal>Information and Computation,</journal>
<pages>80--227</pages>
<contexts>
<context position="16966" citStr="Quinlan and Rivest (1989)" startWordPosition="2868" endWordPosition="2871"> and the KL distance from the empirical distribution of the data (namely, the word-based distribution estimated by MLE) shown in Figure 2 for each of the five tree cut models.3 In the table, one can see that there is a trade-off between the simplicity of a model and the goodness of fit to the data. In the MDL framework, the model description length is an indicator of model 3 Estimation strategies related to MDL have been independently proposed and studied by various authors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992). 4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction to the MDL principle. 5 The KL distance (also known as KL-divergence or relative entropy), which is widely used in information theory and statistics, is a measure of distance between two distributions (e.g., Cover and Thomas 1991). It is always nonnegative and is zero if and only if the two distributions are identical, but is asymmetric and hence not a metric (the usual notion of distance). 223 Computational Linguistics Volume 24, Number 2 complexity, while the data description length indicates goodness of fit to the data. The MDL principle stipulates that the model that m</context>
<context position="20266" citStr="Quinlan and Rivest (1989)" startWordPosition="3474" endWordPosition="3477">order to minimize the expected total description length (Rissanen 1984, 1986). An intuitive explanation of this is that the standard deviation of the maximum-likelihood estimator of each parameter is of the orderV,-11s1, and hence describing each parameter using more than – log =1 log IS I bits would be wasteful for the estimation accuracy possible with 2 the given sample size. Finally, the data description length L(S I F, e) is calculated by: L(S I F, O) = – E log P(n) (10) nES 6 Here and throughout, log denotes the logarithm to the base 2. For reasons why Equation 8 holds, see, for example, Quinlan and Rivest (1989). 224 Li and Abe generalizing Case Frames Table 3 Calculating the description length for the model of Figure 5. BIRD bug bee insect f(C) 8 0 2 0 Cl 4 1 1 1 P(C) 0.8 0.0 0.2 0.0 P(n) 0.2 0.0 0.2 0.0 [BIRD, bug, bee, insect] L(0 I r) (4-21) X log 10 = 4.98 L(S I r, 0) -(2 + 4 + 2 + 2) x log0.2 = 23.22 where for simplicity we write P(n) for Pm(n I v, r). Recall that P(n) is obtained by MLE, namely, by normalizing the frequencies: - 1 - P(n) = x P(C) for each C E F and each n E C, where for each C e F: P(c)= f(c) (12) Is&apos; where f(C) denotes the total frequency of nouns in class C in the sample S. </context>
</contexts>
<marker>Quinlan, Rivest, 1989</marker>
<rawString>Quinlan, J. Ross and Ronald L. Rivest. 1989. Inferring decision trees using the minimum description length principle. Information and Computation, 80:227-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Jeff Reynar</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>Proceedings of ARPA Workshop on Human Language Technology,</booktitle>
<pages>250--255</pages>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Ratnaparkhi, Adwait, Jeff Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase attachment. Proceedings of ARPA Workshop on Human Language Technology, pages 250-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>WordNet and distributional analysis: A class-based approach to lexical discovery.</title>
<date>1992</date>
<booktitle>Proceedings of AAAI Workshop on Statistically-based NLP Techniques.</booktitle>
<marker>Resnik, 1992</marker>
<rawString>Resnik, Philip. 1992. WordNet and distributional analysis: A class-based approach to lexical discovery. Proceedings of AAAI Workshop on Statistically-based NLP Techniques.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A Class-based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. Thesis,</tech>
<institution>Univ. of Pennsylvania.</institution>
<contexts>
<context position="3145" citStr="Resnik 1993" startWordPosition="483" endWordPosition="484">o, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generali</context>
<context position="8856" citStr="Resnik 1993" startWordPosition="1412" endWordPosition="1413">d a smoothing method that is both efficient and theoretically sound. As an alternative, a number of authors have proposed the use of class-based crew eagle swallow bird &amp;quot;Freq. — bee beg 219 Computational Linguistics Volume 24, Number 2 Figure 2 Word-based distribution estimated using MLE. models, which assign (conditional) probability values to (existing) classes of words, rather than individual words. 2.2 Class-based Models An example of the class-based approach is Resnik&apos;s method of generalizing values of a case frame slot using a thesaurus and the so-called selectional association measure (Resnik 1993a, 1993b). The selectional association, denoted A(C I v, r), is defined as follows: A(C v,r) = P(C I v,r) x log (C) P(C I v,r) (2) P where C is a class of nouns present in a given thesaurus, v is a verb and r is a slot name, as described earlier. In generalizing a given noun n to a noun class, this method selects the noun class C having the maximum A(C I v, r), among all super classes of n in a given thesaurus. This method is based on an interesting intuition, but its interpretation as a method of estimation is not clear. We propose a class-based generalization method whose performance as a me</context>
<context position="45881" citStr="Resnik 1993" startWordPosition="7841" endWordPosition="7842">rger, 233 Computational Linguistics Volume 24, Number 2 Table 9 Example input data as doubles. see in see with girl with man with Table 10 Example input data as triples. see in park see with telescope girl with scarf see with friend man with hat Table 11 Example input data as quadruples and labels. see girl in park ADV see man with telescope ADV see girl with scarf ADN then the prepositional phrase is attached to verb, if the latter probability is significantly larger, it is attached to nouni, and otherwise no decision is made. The second approach (Sekine et al. 1992; Chang, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nouni , prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxaass19noun2 A(Class, I verb, prep) and maxClass13n0un2 A(Class, I nouni, prep) to make disambiguation decisions. The third approach (Brill and Resnik 1994; Ratnapark</context>
<context position="48726" citStr="Resnik (1993" startWordPosition="8294" endWordPosition="8295">ugh it can also be used as a component in a combined scheme with many of the above methods (see Brill and Resnik [1994], Alshawi and Carter [1994]). We estimate P(noun2 I verb, prep) and P(noun2 I nouni, prep) from training data consisting of triples, and compare them: If the former exceeds the latter (by a certain margin) we attach it to verb, else if the latter exceeds the former (by the same margin) we attach it to nouni. In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by Hindle and Rooth (1991), Resnik (1993b), and Brill and Resnik (1994), referred to respectively as LA, SA, and TEL. Data Set. We used the bracketed corpus of the Penn Treebank (Wall Street Journal corpus) (Marcus, Santorini, and Marcinkiewicz 1993) as our data. First we randomly selected one of the 26 directories of the WSJ files as the test data and what remains as the training data. We repeated this process 10 times and obtained 10 sets of data consisting of different training data and test data. We used these 10 data sets to conduct cross-validation as described below. From the test data in each data set, we extracted (verb, no</context>
<context position="54839" citStr="Resnik (1993" startWordPosition="9319" endWordPosition="9320">ire transformation rules for each data set and applied the obtained rules to disambiguate the test data. The average number of obtained rules for a data set was 2,752.3. Table 13 shows the disambiguation result averaged over the 10 data sets. From Table 13, we see that TEL performs the best, edging over the second place MDL+Default by a small margin, and then followed by LA+Default, and SA+Default. Below we discuss further observations concerning these results. MDL and SA. According to our experimental results, the accuracy and coverage of MDL appear to be somewhat better than those of SA. As Resnik (1993b) pointed out, the use of selectional association log P(pc(1c7) seems to be appropriate for cognitive modeling. Our experiments show, however, that the generalization method currently employed by Resnik has a tendency to overfit the data. Table 14 shows example generalization results for MDL (with classes with probability less than 0.05 discarded) and SA. Note that MDL tends to select a tree cut closer to the root of the thesaurus tree. This is probably the key reason why MDL has a wider coverage than SA for the same degree of accuracy. One may be concerned that MDL is &amp;quot;overgeneralizing&amp;quot; here</context>
<context position="59283" citStr="Resnik (1993" startWordPosition="9985" endWordPosition="9986">ayment on million meet standard for resistance reach agreement in august show interest in session win verdict in winter acquire interest in firm buy stock in index ease restriction on type forecast sale for venture make payment on debt meet standard for car reach agreement in principle show interest in stock win verdict in case on their co-occurrence relation. Since both MDL and SA have pros and cons, it would be desirable to develop a methodology that combines the merits of the two methods (cf. Abe and Li [1996]). MDL and LA. LA makes its disambiguation decision completely ignoring noun2. As Resnik (1993b) pointed out, if we hope to improve disambiguation performance by increasing training data, we need a richer model such as those used in MDL and SA. We found that 8.8% of the quadruples in our entire test data were such that they shared the same verb, prep, nouni but had different noun2, and their PP-attachment sites go both ways in the same data, i.e., both to verb and to nouni. Clearly, for these examples, the PP-attachment site cannot be reliably determined without knowing noun2. Table 15 shows some of these examples. (We adopted the attachment sites given in the Penn Tree Bank, without c</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Resnik, Philip. 1993a. Selection and Information: A Class-based Approach to Lexical Relationships. Ph.D. Thesis, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic classes and syntactic ambiguity.</title>
<date>1993</date>
<booktitle>Proceedings of ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="3145" citStr="Resnik 1993" startWordPosition="483" endWordPosition="484">o, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generali</context>
<context position="8856" citStr="Resnik 1993" startWordPosition="1412" endWordPosition="1413">d a smoothing method that is both efficient and theoretically sound. As an alternative, a number of authors have proposed the use of class-based crew eagle swallow bird &amp;quot;Freq. — bee beg 219 Computational Linguistics Volume 24, Number 2 Figure 2 Word-based distribution estimated using MLE. models, which assign (conditional) probability values to (existing) classes of words, rather than individual words. 2.2 Class-based Models An example of the class-based approach is Resnik&apos;s method of generalizing values of a case frame slot using a thesaurus and the so-called selectional association measure (Resnik 1993a, 1993b). The selectional association, denoted A(C I v, r), is defined as follows: A(C v,r) = P(C I v,r) x log (C) P(C I v,r) (2) P where C is a class of nouns present in a given thesaurus, v is a verb and r is a slot name, as described earlier. In generalizing a given noun n to a noun class, this method selects the noun class C having the maximum A(C I v, r), among all super classes of n in a given thesaurus. This method is based on an interesting intuition, but its interpretation as a method of estimation is not clear. We propose a class-based generalization method whose performance as a me</context>
<context position="45881" citStr="Resnik 1993" startWordPosition="7841" endWordPosition="7842">rger, 233 Computational Linguistics Volume 24, Number 2 Table 9 Example input data as doubles. see in see with girl with man with Table 10 Example input data as triples. see in park see with telescope girl with scarf see with friend man with hat Table 11 Example input data as quadruples and labels. see girl in park ADV see man with telescope ADV see girl with scarf ADN then the prepositional phrase is attached to verb, if the latter probability is significantly larger, it is attached to nouni, and otherwise no decision is made. The second approach (Sekine et al. 1992; Chang, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nouni , prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxaass19noun2 A(Class, I verb, prep) and maxClass13n0un2 A(Class, I nouni, prep) to make disambiguation decisions. The third approach (Brill and Resnik 1994; Ratnapark</context>
<context position="48726" citStr="Resnik (1993" startWordPosition="8294" endWordPosition="8295">ugh it can also be used as a component in a combined scheme with many of the above methods (see Brill and Resnik [1994], Alshawi and Carter [1994]). We estimate P(noun2 I verb, prep) and P(noun2 I nouni, prep) from training data consisting of triples, and compare them: If the former exceeds the latter (by a certain margin) we attach it to verb, else if the latter exceeds the former (by the same margin) we attach it to nouni. In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by Hindle and Rooth (1991), Resnik (1993b), and Brill and Resnik (1994), referred to respectively as LA, SA, and TEL. Data Set. We used the bracketed corpus of the Penn Treebank (Wall Street Journal corpus) (Marcus, Santorini, and Marcinkiewicz 1993) as our data. First we randomly selected one of the 26 directories of the WSJ files as the test data and what remains as the training data. We repeated this process 10 times and obtained 10 sets of data consisting of different training data and test data. We used these 10 data sets to conduct cross-validation as described below. From the test data in each data set, we extracted (verb, no</context>
<context position="54839" citStr="Resnik (1993" startWordPosition="9319" endWordPosition="9320">ire transformation rules for each data set and applied the obtained rules to disambiguate the test data. The average number of obtained rules for a data set was 2,752.3. Table 13 shows the disambiguation result averaged over the 10 data sets. From Table 13, we see that TEL performs the best, edging over the second place MDL+Default by a small margin, and then followed by LA+Default, and SA+Default. Below we discuss further observations concerning these results. MDL and SA. According to our experimental results, the accuracy and coverage of MDL appear to be somewhat better than those of SA. As Resnik (1993b) pointed out, the use of selectional association log P(pc(1c7) seems to be appropriate for cognitive modeling. Our experiments show, however, that the generalization method currently employed by Resnik has a tendency to overfit the data. Table 14 shows example generalization results for MDL (with classes with probability less than 0.05 discarded) and SA. Note that MDL tends to select a tree cut closer to the root of the thesaurus tree. This is probably the key reason why MDL has a wider coverage than SA for the same degree of accuracy. One may be concerned that MDL is &amp;quot;overgeneralizing&amp;quot; here</context>
<context position="59283" citStr="Resnik (1993" startWordPosition="9985" endWordPosition="9986">ayment on million meet standard for resistance reach agreement in august show interest in session win verdict in winter acquire interest in firm buy stock in index ease restriction on type forecast sale for venture make payment on debt meet standard for car reach agreement in principle show interest in stock win verdict in case on their co-occurrence relation. Since both MDL and SA have pros and cons, it would be desirable to develop a methodology that combines the merits of the two methods (cf. Abe and Li [1996]). MDL and LA. LA makes its disambiguation decision completely ignoring noun2. As Resnik (1993b) pointed out, if we hope to improve disambiguation performance by increasing training data, we need a richer model such as those used in MDL and SA. We found that 8.8% of the quadruples in our entire test data were such that they shared the same verb, prep, nouni but had different noun2, and their PP-attachment sites go both ways in the same data, i.e., both to verb and to nouni. Clearly, for these examples, the PP-attachment site cannot be reliably determined without knowing noun2. Table 15 shows some of these examples. (We adopted the attachment sites given in the Penn Tree Bank, without c</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Resnik, Philip. 1993b. Semantic classes and syntactic ambiguity. Proceedings of ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatic,</journal>
<pages>14--37</pages>
<contexts>
<context position="14926" citStr="Rissanen 1978" startWordPosition="2507" endWordPosition="2508">-= 0.2. Note that the probabilities assigned to the nouns under BIRD are smoothed, even if the nouns have different observed frequencies. We have thus formalized the problem of generalizing values of a case frame slot as that of estimating a model from the class of tree cut models for some fixed thesaurus tree; namely, selecting a model that best explains the data from among the class of tree cut models. 3. Generalization Method Based On MDL The question now becomes what strategy (criterion) we should employ to select the best tree-cut model. We adopt the Minimum Description Length principle (Rissanen 1978, 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 222 Li and Abe Generalizing Case Frames &apos;Prob. — swallow Cr010/ eagle bbd bug bee insect 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 Figure 6 A tree cut model with [BIRD, INSECT]. Table 2 Number of parameters and KL distance from the empirical distribution for the five tree cut models. Number of Parameters KL Distance [ANIMAL] 0 0.89 [BIRD, INSECT] 1 0.72 [BIRD, bug, bee, insect] 3 0.4 [swallow, crow, eagle, bird, INSECT] 4 0.32 [swallow, crow, eagle, bird, bug, bee, insect] 6 0 1983, 1984, 1986, 1989), which has various desirable properties, as will b</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Rissanen, Jorma. 1978. Modeling by shortest data description. Automatic, 14:37-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>A universal prior for integers and estimation by minimum description length. The Annals of Statistics,</title>
<date>1983</date>
<pages>11--2</pages>
<marker>Rissanen, 1983</marker>
<rawString>Rissanen, Jorma. 1983. A universal prior for integers and estimation by minimum description length. The Annals of Statistics, 11(2):416-431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Universal coding, information, predication and estimation.</title>
<date>1984</date>
<journal>IEEE Transaction on Information Theory,</journal>
<pages>30--4</pages>
<contexts>
<context position="19711" citStr="Rissanen 1984" startWordPosition="3377" endWordPosition="3378">) = log II (8) where G denotes the set of all cuts in the thesaurus tree T.&apos; This corresponds to assuming that each tree cut model is equally likely a priori, in the Bayesian interpretation of MDL. (See Section 3.4.) The parameter description length L(O I F) is calculated by: L(e I F) = x log ISI (9) where I SI denotes the sample size and k denotes the number of free parameters in the tree cut model, i.e., k equals the number of nodes in F minus one. It is known to be best to use this number of bits to describe probability parameters in order to minimize the expected total description length (Rissanen 1984, 1986). An intuitive explanation of this is that the standard deviation of the maximum-likelihood estimator of each parameter is of the orderV,-11s1, and hence describing each parameter using more than – log =1 log IS I bits would be wasteful for the estimation accuracy possible with 2 the given sample size. Finally, the data description length L(S I F, e) is calculated by: L(S I F, O) = – E log P(n) (10) nES 6 Here and throughout, log denotes the logarithm to the base 2. For reasons why Equation 8 holds, see, for example, Quinlan and Rivest (1989). 224 Li and Abe generalizing Case Frames Tab</context>
</contexts>
<marker>Rissanen, 1984</marker>
<rawString>Rissanen, Jorma. 1984. Universal coding, information, predication and estimation. IEEE Transaction on Information Theory, 30(4):629-636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Stochastic complexity and modeling.</title>
<date>1986</date>
<journal>The Annals of Statistics,</journal>
<pages>14--3</pages>
<marker>Rissanen, 1986</marker>
<rawString>Rissanen, Jorma. 1986. Stochastic complexity and modeling. The Annals of Statistics, 14(3):1080-1100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Stochastic Complexity in Statistical Inquiry.</title>
<date>1989</date>
<publisher>World Scientific Publishing Co.,</publisher>
<contexts>
<context position="31542" citStr="Rissanen 1989" startWordPosition="5446" endWordPosition="5447">he data size, which is near optimal (Barron and Cover 1991; Yamanishi 1992). Thus, in the current problem, MDL provides (a) a way of smoothing probability parameters to solve the data sparseness problem, and at the same time, (b) a way of generalizing nouns in the data to noun classes of an appropriate level, both as a corollary to the near optimal estimation of the distribution of the given data. 3.4 The Bayesian Interpretation of MDL and the Choice of Encoding Scheme There is a Bayesian interpretation of MDL: MDL is essentially equivalent to the &amp;quot;posterior mode&amp;quot; in the Bayesian terminology (Rissanen 1989). Given data S and a number of models, the Bayesian estimator (posterior mode) selects a model /c/I that maximizes the posterior probability: = arg max (P(M) • P(S I M)) (15) where P(M) denotes the prior probability of the model M and P(S I M) the probability of observing the data S given M. Equivalently, M satisfies JI = arg miin(— log P(M) — log P(S I M)). (16) This is equivalent to the MDL estimate, if we take — log P(M) to be the model description length. Interpreting — log P(M) as the model description length translates, in the Bayesian estimation, to assigning larger prior probabilities </context>
</contexts>
<marker>Rissanen, 1989</marker>
<rawString>Rissanen, Jorma. 1989. Stochastic Complexity in Statistical Inquiry. World Scientific Publishing Co., Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Stochastic complexity in learning.</title>
<date>1995</date>
<booktitle>Proceedings of the Second European Conference on Computational Learning Theory (Euro Colt&apos;95),</booktitle>
<pages>196--210</pages>
<contexts>
<context position="29801" citStr="Rissanen (1995)" startWordPosition="5142" endWordPosition="5143">o compress information, generalization is necessary. In our current problem, this corresponds to the generalization of individual nouns present in case frame instances in the data as classes of nouns present in a given thesaurus. For example, given the thesaurus in Figure 3 and frequency data in Figure 1, we would 7 The process of finding the MDL model tends to be computationally demanding and is often intractable. When the model class under consideration is restricted to tree structures, however, dynamic programming is often applicable and the MDL model can be efficiently found. For example, Rissanen (1995) has devised an algorithm for learning decision trees. 8 Consider, for example, the case when the co-occurrence data is given as f(swallow) = 2f (crow) -= 2,f(eagle) = 2,f(bird) = 2 for the problem in Section 2. 228 Li and Abe Generalizing Case Frames like our system to judge that the class BIRD and the noun bee can be the subject slot of the verb fly. The problem of deciding whether to stop generalizing at BIRD and bee, or generalizing further to ANIMAL has been addressed by a number of authors (Webster and Marcus 1989; Velardi, Pazienza, and Fasolo 1991; Nomiyama 1992). Minimization of the t</context>
</contexts>
<marker>Rissanen, 1995</marker>
<rawString>Rissanen, Jorma. 1995. Stochastic complexity in learning. Proceedings of the Second European Conference on Computational Learning Theory (Euro Colt&apos;95), pages 196-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Robert G Thomas</author>
</authors>
<title>New techniques for context modeling.</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6391" citStr="Ristad and Thomas 1995" startWordPosition="1020" endWordPosition="1023">v, r) for each n in the set of nouns Ar = {ni, n2, . , nN} , v in the set of verbs V = {vi, 02, , vv}, and r in the set of slot names R. = r2, ...,rRI, satisfying: E P(n I v,r) = 1. (1) nEAr This type of probability model is often referred to as a word-based model. Since the number of probability parameters in word-based models is large (0(N •V .R)), accurate 1 Recently, MDL and related techniques have become popular in corpus-based natural language processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright 1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern acquisition. 218 Li and Abe Generalizing Case Frames Table 1 Example (verb, slot_name, slot_value) triple data. verb slot_name slot_value fly argl bee fly argl bird fly argl bird fly argl crow fly argl bird fly argl eagle fly argl bee fly argl eagle fly argl bird fly argl crow Figure 1 Frequency data for the subject slot of verb fly. estimation of a word-based model is difficult with the data size that is available in practice—a problem usually referred to as the data sparseness </context>
</contexts>
<marker>Ristad, Thomas, 1995</marker>
<rawString>Ristad, Eric Sven and Robert G. Thomas. 1995. New techniques for context modeling. Proceedings of the 33rd Annual Meeting. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Schwarz</author>
</authors>
<title>Estimation of the dimension of a model.</title>
<date>1978</date>
<journal>Annals of Statistics,</journal>
<volume>6</volume>
<pages>446</pages>
<contexts>
<context position="16876" citStr="Schwarz 1978" startWordPosition="2855" endWordPosition="2856">to have a better fit to the data. Table 2 shows the number of free parameters and the KL distance from the empirical distribution of the data (namely, the word-based distribution estimated by MLE) shown in Figure 2 for each of the five tree cut models.3 In the table, one can see that there is a trade-off between the simplicity of a model and the goodness of fit to the data. In the MDL framework, the model description length is an indicator of model 3 Estimation strategies related to MDL have been independently proposed and studied by various authors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992). 4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction to the MDL principle. 5 The KL distance (also known as KL-divergence or relative entropy), which is widely used in information theory and statistics, is a measure of distance between two distributions (e.g., Cover and Thomas 1991). It is always nonnegative and is zero if and only if the two distributions are identical, but is asymmetric and hence not a metric (the usual notion of distance). 223 Computational Linguistics Volume 24, Number 2 complexity, while the data description length</context>
</contexts>
<marker>Schwarz, 1978</marker>
<rawString>Schwarz, G. 1978. Estimation of the dimension of a model. Annals of Statistics, 6:416 446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Jeremy J Carroll</author>
<author>Sofia Ananiadou</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Automatic learning for semantic collocation.</title>
<date>1992</date>
<booktitle>Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>104--110</pages>
<contexts>
<context position="45843" citStr="Sekine et al. 1992" startWordPosition="7832" endWordPosition="7835">at the former probability is significantly larger, 233 Computational Linguistics Volume 24, Number 2 Table 9 Example input data as doubles. see in see with girl with man with Table 10 Example input data as triples. see in park see with telescope girl with scarf see with friend man with hat Table 11 Example input data as quadruples and labels. see girl in park ADV see man with telescope ADV see girl with scarf ADN then the prepositional phrase is attached to verb, if the latter probability is significantly larger, it is attached to nouni, and otherwise no decision is made. The second approach (Sekine et al. 1992; Chang, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nouni , prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxaass19noun2 A(Class, I verb, prep) and maxClass13n0un2 A(Class, I nouni, prep) to make disambiguation decisions. The third appr</context>
</contexts>
<marker>Sekine, Carroll, Ananiadou, Tsujii, 1992</marker>
<rawString>Sekine, Satoshi, Jeremy J. Carroll, Sofia Ananiadou, and Jun&apos;ichi Tsujii. 1992. Automatic learning for semantic collocation. Proceedings of the Third Conference on Applied Natural Language Processing, pages 104-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<booktitle>Xtract. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="2576" citStr="Smadja 1993" startWordPosition="399" endWordPosition="400">ssue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almualli</context>
<context position="60332" citStr="Smadja 1993" startWordPosition="10155" endWordPosition="10156">cannot be reliably determined without knowing noun2. Table 15 shows some of these examples. (We adopted the attachment sites given in the Penn Tree Bank, without correcting apparently wrong judgements.) MDL and TEL. We chose TEL as an example of the quadruple approach. This method was designed specifically for the purpose of resolving PP-attachment ambiguities, and seems to perform slightly better than ours. As we remarked earlier, however, the input data required by our method (triples) could be generated automatically from unparsed corpora making use of existing heuristic rules (Brent 1993; Smadja 1993), although for the experiments we report here we used a parsed corpus. Thus it would seem to be easier to obtain more data in the future for MDL and other methods based on unsupervised learning. Also note that our method of generalizing values of a case slot can be used for purposes other than disambiguation. 5. Conclusions We proposed a new method of generalizing case frames. Our approach of applying MDL to estimate a tree cut model in an existing thesaurus is not limited to just the problem of generalizing values of a case frame slot. It is potentially useful in other natural language proces</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Smadja, Frank. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Solomonoff</author>
</authors>
<title>A formal theory of inductive inference 1 and 2. Information and Control,</title>
<date>1964</date>
<pages>7--1</pages>
<contexts>
<context position="16836" citStr="Solomonoff 1964" startWordPosition="2849" endWordPosition="2850">at in Figure 4, is more complex, but tends to have a better fit to the data. Table 2 shows the number of free parameters and the KL distance from the empirical distribution of the data (namely, the word-based distribution estimated by MLE) shown in Figure 2 for each of the five tree cut models.3 In the table, one can see that there is a trade-off between the simplicity of a model and the goodness of fit to the data. In the MDL framework, the model description length is an indicator of model 3 Estimation strategies related to MDL have been independently proposed and studied by various authors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992). 4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction to the MDL principle. 5 The KL distance (also known as KL-divergence or relative entropy), which is widely used in information theory and statistics, is a measure of distance between two distributions (e.g., Cover and Thomas 1991). It is always nonnegative and is zero if and only if the two distributions are identical, but is asymmetric and hence not a metric (the usual notion of distance). 223 Computational Linguistics Volume 24, Number 2 compl</context>
</contexts>
<marker>Solomonoff, 1964</marker>
<rawString>Solomonoff, R.J. 1964. A formal theory of inductive inference 1 and 2. Information and Control, 7:1-22;224-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Stephen Omohundro</author>
</authors>
<title>Inducing probabilistic grammars by bayesian model merging. In</title>
<date>1994</date>
<booktitle>Grammatical Inference and Applications.</booktitle>
<pages>106--118</pages>
<editor>Rafael C. Carrasco and Jose Oncina, editors,</editor>
<publisher>Springer Verlag,</publisher>
<contexts>
<context position="6333" citStr="Stolcke and Omohundro 1994" startWordPosition="1011" endWordPosition="1014">bility model that specifies the conditional probability P(n I v, r) for each n in the set of nouns Ar = {ni, n2, . , nN} , v in the set of verbs V = {vi, 02, , vv}, and r in the set of slot names R. = r2, ...,rRI, satisfying: E P(n I v,r) = 1. (1) nEAr This type of probability model is often referred to as a word-based model. Since the number of probability parameters in word-based models is large (0(N •V .R)), accurate 1 Recently, MDL and related techniques have become popular in corpus-based natural language processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright 1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern acquisition. 218 Li and Abe Generalizing Case Frames Table 1 Example (verb, slot_name, slot_value) triple data. verb slot_name slot_value fly argl bee fly argl bird fly argl bird fly argl crow fly argl bird fly argl eagle fly argl bee fly argl eagle fly argl bird fly argl crow Figure 1 Frequency data for the subject slot of verb fly. estimation of a word-based model is difficult with the data size that is available in prac</context>
</contexts>
<marker>Stolcke, Omohundro, 1994</marker>
<rawString>Stolcke, Andreas and Stephen Omohundro. 1994. Inducing probabilistic grammars by bayesian model merging. In Rafael C. Carrasco and Jose Oncina, editors, Grammatical Inference and Applications. Springer Verlag, pages 106-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Tanaka</author>
</authors>
<title>Verbal case frame acquisition from a bilingual corpus: Gradual knowledge acquisition.</title>
<date>1994</date>
<booktitle>Proceedings of the Fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>727--731</pages>
<contexts>
<context position="3352" citStr="Tanaka 1994" startWordPosition="517" endWordPosition="518">ethods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length principle (MDL): a principle of data compression and statistical estimation from information theory.&apos; In order to assist with efficiency, our method make</context>
</contexts>
<marker>Tanaka, 1994</marker>
<rawString>Tanaka, Hideki. 1994. Verbal case frame acquisition from a bilingual corpus: Gradual knowledge acquisition. Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 727-731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Tanaka</author>
</authors>
<title>Decision tree learning algorithm with structured attributes: Application to verbal case frame acquisition.</title>
<date>1996</date>
<booktitle>Proceedings of the Sixteenth International Conference on Computational Linguistics,</booktitle>
<pages>943--948</pages>
<contexts>
<context position="3202" citStr="Tanaka 1996" startWordPosition="492" endWordPosition="493">1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length pri</context>
</contexts>
<marker>Tanaka, 1996</marker>
<rawString>Tanaka, Hideki. 1996. Decision tree learning algorithm with structured attributes: Application to verbal case frame acquisition. Proceedings of the Sixteenth International Conference on Computational Linguistics, pages 943-948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takehito Utsuro</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Learning probabilistic subcategorization preference by identifying case dependencies and optimal noun class generalization level.</title>
<date>1997</date>
<booktitle>Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>364--371</pages>
<contexts>
<context position="3230" citStr="Utsuro and Matsumoto 1997" startWordPosition="494" endWordPosition="497"> and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * CR,rC Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{lihang,abe}@ccm.cl.nec.co.jp © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length principle (MDL): a principle of</context>
</contexts>
<marker>Utsuro, Matsumoto, 1997</marker>
<rawString>Utsuro, Takehito and Yuji Matsumoto. 1997. Learning probabilistic subcategorization preference by identifying case dependencies and optimal noun class generalization level. Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 364-371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takehito Utsuro</author>
<author>Yuji Matsumoto</author>
<author>Makoto Nagao</author>
</authors>
<title>Lexical knowledge acquisition from bilingual corpora.</title>
<date>1992</date>
<booktitle>Proceedings of the Fourteenth International Conference on Computational Linguistics,</booktitle>
<pages>581--587</pages>
<marker>Utsuro, Matsumoto, Nagao, 1992</marker>
<rawString>Utsuro, Takehito, Yuji Matsumoto, and Makoto Nagao. 1992. Lexical knowledge acquisition from bilingual corpora. Proceedings of the Fourteenth International Conference on Computational Linguistics, pages 581-587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Velardi</author>
<author>Maria Teresa Pazienza</author>
<author>Michela Fasolo</author>
</authors>
<title>How to encode semantic knowledge: A method for meaning representation and computer-aided acquisition.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--2</pages>
<marker>Velardi, Pazienza, Fasolo, 1991</marker>
<rawString>Velardi, Paola, Maria Teresa Pazienza, and Michela Fasolo. 1991. How to encode semantic knowledge: A method for meaning representation and computer-aided acquisition. Computational Linguistics, 17(2):153-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wallace</author>
<author>D M Boulton</author>
</authors>
<title>An information measure for classification.</title>
<date>1968</date>
<journal>Computer Journal,</journal>
<pages>11--185</pages>
<contexts>
<context position="16862" citStr="Wallace and Boulton 1968" startWordPosition="2851" endWordPosition="2854">s more complex, but tends to have a better fit to the data. Table 2 shows the number of free parameters and the KL distance from the empirical distribution of the data (namely, the word-based distribution estimated by MLE) shown in Figure 2 for each of the five tree cut models.3 In the table, one can see that there is a trade-off between the simplicity of a model and the goodness of fit to the data. In the MDL framework, the model description length is an indicator of model 3 Estimation strategies related to MDL have been independently proposed and studied by various authors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992). 4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction to the MDL principle. 5 The KL distance (also known as KL-divergence or relative entropy), which is widely used in information theory and statistics, is a measure of distance between two distributions (e.g., Cover and Thomas 1991). It is always nonnegative and is zero if and only if the two distributions are identical, but is asymmetric and hence not a metric (the usual notion of distance). 223 Computational Linguistics Volume 24, Number 2 complexity, while the data desc</context>
</contexts>
<marker>Wallace, Boulton, 1968</marker>
<rawString>Wallace, C. and D. M. Boulton. 1968. An information measure for classification. Computer Journal, 11:185-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wallace</author>
<author>P Freeman</author>
</authors>
<title>Single-factor analysis by minimum message length estimation.</title>
<date>1992</date>
<journal>Journal of Royal Statistical Society, B,</journal>
<pages>54--195</pages>
<contexts>
<context position="16903" citStr="Wallace and Freeman 1992" startWordPosition="2857" endWordPosition="2860">er fit to the data. Table 2 shows the number of free parameters and the KL distance from the empirical distribution of the data (namely, the word-based distribution estimated by MLE) shown in Figure 2 for each of the five tree cut models.3 In the table, one can see that there is a trade-off between the simplicity of a model and the goodness of fit to the data. In the MDL framework, the model description length is an indicator of model 3 Estimation strategies related to MDL have been independently proposed and studied by various authors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992). 4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction to the MDL principle. 5 The KL distance (also known as KL-divergence or relative entropy), which is widely used in information theory and statistics, is a measure of distance between two distributions (e.g., Cover and Thomas 1991). It is always nonnegative and is zero if and only if the two distributions are identical, but is asymmetric and hence not a metric (the usual notion of distance). 223 Computational Linguistics Volume 24, Number 2 complexity, while the data description length indicates goodness of fit </context>
</contexts>
<marker>Wallace, Freeman, 1992</marker>
<rawString>Wallace, C. and P. Freeman. 1992. Single-factor analysis by minimum message length estimation. Journal of Royal Statistical Society, B, 54:195-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mort Webster</author>
<author>Mitch Marcus</author>
</authors>
<title>Automatic acquisition of the lexical semantics of verbs from sentence frames.</title>
<date>1989</date>
<booktitle>Proceedings of the 27th Annual Meeting,</booktitle>
<pages>177--184</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="30326" citStr="Webster and Marcus 1989" startWordPosition="5235" endWordPosition="5238">mming is often applicable and the MDL model can be efficiently found. For example, Rissanen (1995) has devised an algorithm for learning decision trees. 8 Consider, for example, the case when the co-occurrence data is given as f(swallow) = 2f (crow) -= 2,f(eagle) = 2,f(bird) = 2 for the problem in Section 2. 228 Li and Abe Generalizing Case Frames like our system to judge that the class BIRD and the noun bee can be the subject slot of the verb fly. The problem of deciding whether to stop generalizing at BIRD and bee, or generalizing further to ANIMAL has been addressed by a number of authors (Webster and Marcus 1989; Velardi, Pazienza, and Fasolo 1991; Nomiyama 1992). Minimization of the total description length provides a disciplined criterion to do this. A remarkable fact about MDL is that theoretical findings have indeed verified that MDL, as an estimation strategy, is near optimal in terms of the rate of convergence of its estimated models to the true model as data size increases. When the true model is included in the class of models considered, the models selected by MDL converge .log SI to the true model at the rate of 0(k* 2.1s,I ), where k* is the number of parameters in the true model, and IS I</context>
</contexts>
<marker>Webster, Marcus, 1989</marker>
<rawString>Webster, Mort and Mitch Marcus. 1989. Automatic acquisition of the lexical semantics of verbs from sentence frames. Proceedings of the 27th Annual Meeting, pages 177-184. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Whittemore</author>
<author>Kathleen Ferrara</author>
<author>Hans Brunner</author>
</authors>
<title>Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases.</title>
<date>1990</date>
<booktitle>Proceedings of the 28th Annual Meeting,</booktitle>
<pages>23--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Whittemore, Ferrara, Brunner, 1990</marker>
<rawString>Whittemore, Greg, Kathleen Ferrara, and Hans Brunner. 1990. Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases. Proceedings of the 28th Annual Meeting, pages 23-30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamanishi</author>
</authors>
<title>A learning criterion for stochastic rules.</title>
<date>1992</date>
<booktitle>Machine Learning,</booktitle>
<pages>9--165</pages>
<contexts>
<context position="31003" citStr="Yamanishi 1992" startWordPosition="5355" endWordPosition="5356">tion of the total description length provides a disciplined criterion to do this. A remarkable fact about MDL is that theoretical findings have indeed verified that MDL, as an estimation strategy, is near optimal in terms of the rate of convergence of its estimated models to the true model as data size increases. When the true model is included in the class of models considered, the models selected by MDL converge .log SI to the true model at the rate of 0(k* 2.1s,I ), where k* is the number of parameters in the true model, and IS I the data size, which is near optimal (Barron and Cover 1991; Yamanishi 1992). Thus, in the current problem, MDL provides (a) a way of smoothing probability parameters to solve the data sparseness problem, and at the same time, (b) a way of generalizing nouns in the data to noun classes of an appropriate level, both as a corollary to the near optimal estimation of the distribution of the given data. 3.4 The Bayesian Interpretation of MDL and the Choice of Encoding Scheme There is a Bayesian interpretation of MDL: MDL is essentially equivalent to the &amp;quot;posterior mode&amp;quot; in the Bayesian terminology (Rissanen 1989). Given data S and a number of models, the Bayesian estimator</context>
</contexts>
<marker>Yamanishi, 1992</marker>
<rawString>Yamanishi, Kenji. 1992. A learning criterion for stochastic rules. Machine Learning, 9:165-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>Proceedings of the fourteenth International Conference on Computational Linguistics,</booktitle>
<pages>454--460</pages>
<contexts>
<context position="10451" citStr="Yarowsky 1992" startWordPosition="1710" endWordPosition="1711"> for each class (subset) C in that partition, such that E P(C I v,r) = 1. (3) cEr Within a given class C, it is assumed that each noun is generated with equal probability, namely Vn E C: P(n I v,r) = —1 x P(C I v, r). (4) I CI Here, we assume that a word belongs to a single class. In practice, however, many words have sense ambiguity and a word can belong to several different classes, e.g., bird is a member of both BIRD and MEAT. Thorough treatment of this problem is beyond the scope of the present paper; we simply note that one can employ an existing word-sense disambiguation technique (e.g.,Yarowsky 1992, 1994) in preprocessing, and use the disambiguated word senses as virtual words in the following &amp;quot;Prob.&amp;quot; — 0.45 0.4 0.35 - 0.3 0.25 - 0.2 0.15 - 0.1 - 0.05 - swallow crow eagle bird bee insect 0 220 Li and Abe Generalizing Case Frames ANIMAL BIRD INSECT swallow crow eagle bird bug bee insect Figure 3 An example thesaurus. case-pattern acquisition process. It is also possible to extend our model so that each word probabilistically belongs to several different classes, which would allow us to resolve both structural and word-sense ambiguities at the time of disambiguation.2 Employing probabilis</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, David. 1992. Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. Proceedings of the fourteenth International Conference on Computational Linguistics, pages 454-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>Proceedings of the 32nd Annual Meeting,</booktitle>
<pages>88--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yarowsky, 1994</marker>
<rawString>Yarowsky, David. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. Proceedings of the 32nd Annual Meeting, pages 88-95. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>