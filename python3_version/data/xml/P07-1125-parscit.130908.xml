<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.961182">
Weakly Supervised Learning for Hedge Classification in Scientific Literature
</title>
<author confidence="0.99752">
Ben Medlock
</author>
<affiliation confidence="0.9929205">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.943786">
Cambridge, C133 OFD
</address>
<email confidence="0.995369">
benmedlock@cantab.net
</email>
<author confidence="0.991039">
Ted Briscoe
</author>
<affiliation confidence="0.9913485">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.957793">
Cambridge, C133 OFD
</address>
<email confidence="0.998748">
ejb@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.996659" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965666666667">
We investigate automatic classification
of speculative language (‘hedging’), in
biomedical text using weakly supervised
machine learning. Our contributions include
a precise description of the task with anno-
tation guidelines, analysis and discussion,
a probabilistic weakly supervised learning
model, and experimental evaluation of the
methods presented. We show that hedge
classification is feasible using weakly
supervised ML, and point toward avenues
for future research.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999663">
The automatic processing of scientific papers using
NLP and machine learning (ML) techniques is an
increasingly important aspect of technical informat-
ics. In the quest for a deeper machine-driven ‘under-
standing’ of the mass of scientific literature, a fre-
quently occuring linguistic phenomenon that must
be accounted for is the use of hedging to denote
propositions of a speculative nature. Consider the
following:
</bodyText>
<listItem confidence="0.9384505">
1. Our results prove that XfK89 inhibits Felin-9.
2. Our results suggest that XfK89 might inhibit Felin-9.
</listItem>
<bodyText confidence="0.987672863636364">
The second example contains a hedge, signaled
by the use of suggest and might, which renders
the proposition inhibit(XfK89—*Felin-9) speculative.
Such analysis would be useful in various applica-
tions; for instance, consider a system designed to
identify and extract interactions between genetic en-
tities in the biomedical domain. Case 1 above pro-
vides clear textual evidence of such an interaction
992
and justifies extraction of inhibit(XfK89—*Felin-9),
whereas case 2 provides only weak evidence for
such an interaction.
Hedging occurs across the entire spectrum of sci-
entific literature, though it is particularly common in
the experimental natural sciences. In this study we
consider the problem of learning to automatically
classify sentences containing instances of hedging,
given only a very limited amount of annotator-
labelled ‘seed’ data. This falls within the weakly su-
pervised ML framework, for which a range of tech-
niques have been previously explored. The contri-
butions of our work are as follows:
</bodyText>
<listItem confidence="0.991380357142857">
1. We provide a clear description of the prob-
lem of hedge classification and offer an im-
proved and expanded set of annotation guide-
lines, which as we demonstrate experimentally
are sufficient to induce a high level of agree-
ment between independent annotators.
2. We discuss the specificities of hedge classifica-
tion as a weakly supervised ML task.
3. We derive a probabilistic weakly supervised
learning model and use it to motivate our ap-
proach.
4. We analyze our learning model experimentally
and report promising results for the task on a
new publicly-available dataset.1
</listItem>
<sectionHeader confidence="0.999673" genericHeader="general terms">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.989303">
2.1 Hedge Classification
</subsectionHeader>
<bodyText confidence="0.8188385">
While there is a certain amount of literature within
the linguistics community on the use of hedging in
</bodyText>
<footnote confidence="0.957118">
1available from www.cl.cam.ac.uk/∼bwm23/
</footnote>
<note confidence="0.9451945">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.98281887755102">
scientific text, eg. (Hyland, 1994), there is little of for which the learner is most confident. Early work
direct relevance to the task of classifying speculative by Yarowsky (1995) falls within this framework.
language from an NLP/ML perspective. Banko and Brill (2001) use ‘bagging’ and agree-
The most clearly relevant study is Light et al. ment to measure confidence on unlabelled samples,
(2004) where the focus is on introducing the prob- and more recently McClosky et al. (2006) use self-
lem, exploring annotation issues and outlining po- training for improving parse reranking.
tential applications rather than on the specificities Other relevant recent work includes (Zhang,
of the ML approach, though they do present some 2004), in which random feature projection and a
results using a manually crafted substring match- committee of SVM classifiers is used in a hybrid
ing classifier and a supervised SVM on a collection co/self-training strategy for weakly supervised re-
of Medline abstracts. We will draw on this work lation classification and (Chen et al., 2006) where
throughout our presentation of the task. a graph based algorithm called label propagation is
Hedging is sometimes classed under the umbrella employed to perform weakly supervised relation ex-
concept of subjectivity, which covers a variety of lin- traction.
guistic phenomena used to express differing forms 3 The Hedge Classification Task
of authorial opinion (Wiebe et al., 2004). Riloff et al. Given a collection of sentences, S, the task is to
(2003) explore bootstrapping techniques to identify label each sentence as either speculative or non-
subjective nouns and subsequently classify subjec- speculative (spec or nspec henceforth). Specifically,
tive vs. objective sentences in newswire text. Their S is to be partitioned into two disjoint sets, one rep-
work bears some relation to ours; however, our do- resenting sentences that contain some form of hedg-
mains of interest differ (newswire vs. scientific text) ing, and the other representing those that do not.
and they do not address the problem of hedge clas- To further elucidate the nature of the task and im-
sification directly. prove annotation consistency, we have developed a
2.2 Weakly Supervised Learning new set of guidelines, building on the work of Light
Recent years have witnessed a significant growth et al. (2004). As noted by Light et al., speculative
of research into weakly supervised ML techniques assertions are to be identified on the basis of judge-
for NLP applications. Different approaches are of- ments about the author’s intended meaning, rather
ten characterised as either multi- or single-view, than on the presence of certain designated hedge
where the former generate multiple redundant (or terms.
semi-redundant) ‘views’ of a data sample and per- We begin with the hedge definition given by
form mutual bootstrapping. This idea was for- Light et al. (item 1) and introduce a set of further
malised by Blum and Mitchell (1998) in their guidelines to help elucidate various ‘grey areas’ and
presentation of co-training. Co-training has also tighten the task specification. These were developed
been used for named entity recognition (NER) after initial annotation by the authors, and through
(Collins and Singer, 1999), coreference resolution discussion with colleagues. Further examples are
(Ng and Cardie, 2003), text categorization (Nigam given in online Appendix A2.
and Ghani, 2000) and improving gene name data The following are considered hedge instances:
(Wellner, 2005). 1. An assertion relating to a result that does not
Conversely, single-view learning models operate necessarily follow from work presented, but
without an explicit partition of the feature space. could be extrapolated from it (Light et al.).
Perhaps the most well known of such approaches 2. Relay of hedge made in previous work.
is expectation maximization (EM), used by Nigam Dl and Ser have been proposed to act redundantly in the
et al. (2000) for text categorization and by Ng and sensory bristle lineage.
Cardie (2003) in combination with a meta-level fea- 3. Statement of knowledge paucity.
ture selection procedure. Self-training is an alterna-
tive single-view algorithm in which a labelled pool
is incrementally enlarged with unlabelled samples
993
2available from www.cl.cam.ac.uk/∼bwm23/
</bodyText>
<note confidence="0.6486755">
How endocytosis of Dl leads to the activation of N re-
mains to be elucidated.
</note>
<figure confidence="0.450727285714286">
4. Speculative question.
A second important question is whether the roX genes
have the same, overlapping or complementing functions.
5. Statement of speculative hypothesis.
To test whether the reported sea urchin sequences repre-
sent a true RAG1-like match, we repeated the BLASTP
search against all GenBank proteins.
</figure>
<footnote confidence="0.335719">
6. Anaphoric hedge reference.
This hypothesis is supported by our finding that both pu-
pariation rate and survival are affected by EL9.
</footnote>
<bodyText confidence="0.904548">
The following are not considered hedge instances:
</bodyText>
<listItem confidence="0.895517">
1. Indication of experimentally observed non-
universal behaviour.
proteins with single BIR domains can also have functions
in cell cycle regulation and cytokinesis.
2. Confident assertion based on external work.
Two distinct E3 ubiquitin ligases have been shown to reg-
ulate Dl signaling in Drosophila melanogaster.
3. Statement of existence of proposed alterna-
tives.
</listItem>
<construct confidence="0.480975">
Different models have been proposed to explain how en-
docytosis of the ligand, which removes the ligandfrom the
cell surface, results in N receptor activation.
</construct>
<listItem confidence="0.8801235">
4. Experimentally-supported confirmation of pre-
vious speculation.
</listItem>
<bodyText confidence="0.966935666666667">
Here we show that the hemocytes are the main regulator
of adenosine in the Drosophila larva, as was speculated
previously for mammals.
</bodyText>
<listItem confidence="0.667429">
5. Negation of previous hedge.
</listItem>
<bodyText confidence="0.97267825">
Although the adgf-a mutation leads to larval or pupal
death, we have shown that this is not due to the adenosine
or deoxyadenosine simply blocking cellular proliferation
or survival, as the experiments in vitro would suggest.
</bodyText>
<sectionHeader confidence="0.995434" genericHeader="keywords">
4 Data
</sectionHeader>
<bodyText confidence="0.9999628">
We used an archive of 5579 full-text papers from the
functional genomics literature relating to Drosophila
melanogaster (the fruit fly). The papers were con-
verted to XML and linguistically processed using
the RASP toolkit3. We annotated six of the pa-
pers to form a test set with a total of 380 spec sen-
tences and 1157 nspec sentences, and randomly se-
lected 300,000 sentences from the remaining papers
as training data for the weakly supervised learner. To
ensure selection of complete sentences rather than
</bodyText>
<footnote confidence="0.908347">
3www.informatics.susx.ac.uk/research/nlp/rasp
</footnote>
<table confidence="0.99163425">
Frel
1 n
Original 0.8293 0.9336
Corrected 0.9652 0.9848
</table>
<tableCaption confidence="0.998961">
Table 1: Agreement Scores
</tableCaption>
<bodyText confidence="0.996737666666667">
headings, captions etc., unlabelled samples were
chosen under the constraints that they must be at
least 10 words in length and contain a main verb.
</bodyText>
<sectionHeader confidence="0.963325" genericHeader="introduction">
5 Annotation and Agreement
</sectionHeader>
<bodyText confidence="0.939033583333333">
Two separate annotators were commissioned to la-
bel the sentences in the test set, firstly one of the
authors and secondly a domain expert with no prior
input into the guideline development process. The
two annotators labelled the data independently us-
ing the guidelines outlined in section 3. Relative
F1 (Frel
1 ) and Cohen’s Kappa (n) were then used to
quantify the level of agreement. For brevity we refer
the reader to (Artstein and Poesio, 2005) and (Hripc-
sak and Rothschild, 2004) for formulation and dis-
cussion of n and Frel
</bodyText>
<equation confidence="0.380401">
1 respectively.
</equation>
<bodyText confidence="0.993677">
The two metrics are based on different assump-
tions about the nature of the annotation task. Frel
</bodyText>
<equation confidence="0.733358">
1
</equation>
<bodyText confidence="0.99976475">
is founded on the premise that the task is to recog-
nise and label spec sentences from within a back-
ground population, and does not explicitly model
agreement on nspec instances. It ranges from 0 (no
agreement) to 1 (no disagreement). Conversely, n
gives explicit credit for agreement on both spec and
nspec instances. The observed agreement is then
corrected for ‘chance agreement’, yielding a metric
that ranges between −1 and 1. Given our defini-
tion of hedge classification and assessing the manner
in which the annotation was carried out, we suggest
that the founding assumption of Frel
1 fits the nature
of the task better than that of n.
Following initial agreement calculation, the in-
stances of disagreement were examined. It turned
out that the large majority of cases of disagreement
were due to negligence on behalf of one or other of
the annotators (i.e. cases of clear hedging that were
missed), and that the cases of genuine disagreement
were actually quite rare. New labelings were then
created with the negligent disagreements corrected,
resulting in significantly higher agreement scores.
Values for the original and negligence-corrected la-
</bodyText>
<page confidence="0.992003">
994
</page>
<bodyText confidence="0.997471642857143">
belings are reported in Table 1.
Annotator conferral violates the fundamental as-
sumption of annotator independence, and so the lat-
ter agreement scores do not represent the true level
of agreement; however, it is reasonable to conclude
that the actual agreement is approximately lower
bounded by the initial values and upper bounded by
the latter values. In fact even the lower bound is
well within the range usually accepted as represent-
ing ‘good’ agreement, and thus we are confident in
accepting human labeling as a gold-standard for the
hedge classification task. For our experiments, we
use the labeling of the genetics expert, corrected for
negligent instances.
</bodyText>
<sectionHeader confidence="0.999226" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999948974358974">
In this study we use single terms as features, based
on the intuition that many hedge cues are single
terms (suggest, likely etc.) and due to the success
of ‘bag of words’ representations in many classifica-
tion tasks to date. Investigating more complex sam-
ple representation strategies is an avenue for future
research.
There are a number of factors that make our for-
mulation of hedge classification both interesting and
challenging from a weakly supervised learning per-
spective. Firstly, due to the relative sparsity of hedge
cues, most samples contain large numbers of irrele-
vant features. This is in contrast to much previous
work on weakly supervised learning, where for in-
stance in the case of text categorization (Blum and
Mitchell, 1998; Nigam et al., 2000) almost all con-
tent terms are to some degree relevant, and irrel-
evant terms can often be filtered out (e.g. stop-
word removal). In the same vein, for the case of
entity/relation extraction and classification (Collins
and Singer, 1999; Zhang, 2004; Chen et al., 2006)
the context of the entity or entities in consideration
provides a highly relevant feature space.
Another interesting factor in our formulation of
hedge classification is that the nspec class is defined
on the basis of the absence of hedge cues, render-
ing it hard to model directly. This characteristic
is also problematic in terms of selecting a reliable
set of nspec seed sentences, as by definition at the
beginning of the learning cycle the learner has lit-
tle knowledge about what a hedge looks like. This
problem is addressed in section 10.3.
In this study we develop a learning model based
around the concept of iteratively predicting labels
for unlabelled training samples, the basic paradigm
for both co-training and self-training. However we
generalise by framing the task in terms of the acqui-
sition of labelled training data, from which a super-
vised classifier can subsequently be learned.
</bodyText>
<sectionHeader confidence="0.9985375" genericHeader="method">
7 A Probabilistic Model for Training Data
Acquisition
</sectionHeader>
<bodyText confidence="0.9424516">
In this section, we derive a simple probabilistic
model for acquiring training data for a given learn-
ing task, and use it to motivate our approach to
weakly supervised hedge classification.
Given:
</bodyText>
<listItem confidence="0.997891333333333">
• sample space X
• set of target concept classes Y = {y1 ... yN}
• target function Y : X —* Y
• set of seed samples for each class S1 ... SN
where Si C X and bx E Si[Y (x)=yi]
• set of unlabelled samples U = {x1 ... xK}
</listItem>
<bodyText confidence="0.926595625">
Aim: Infer a set of training samples T for each con-
cept class yi such that bx E 7[Y (x) = yi]
Now, it follows that bxET[Y (x)=yi] is satisfied
in the case that bxET[P(yi|x)=1], which leads to
a model in which T is initialised to Si and then iter-
atively augmented with the unlabelled sample(s) for
which the posterior probability of class membership
is maximal. Formally:
</bodyText>
<equation confidence="0.950349777777778">
At each iteration:
7 +_ xj(E U)
where j = arg max[P(yi|xj)] (1)
j
Expansion with Bayes’ Rule yields:
arg max [P(yi|xj)]
j
�P(xj|yi) - P(yi) (2)
P(xj)
</equation>
<bodyText confidence="0.952314666666667">
An interesting observation is the importance of
the sample prior P(xj) in the denominator, of-
ten ignored for classification purposes because of
its invariance to class. We can expand further by
= arg max
j
</bodyText>
<page confidence="0.991716">
995
</page>
<bodyText confidence="0.770256">
marginalising over the classes in the denominator in
expression 2, yielding:
</bodyText>
<equation confidence="0.9935915">
&amp;quot; #
P(xj|yi) · P (yi)
arg max PN (3)
j n=1 P (yn)P (xj|yn)
</equation>
<bodyText confidence="0.999884166666667">
so we are left with the class priors and class-
conditional likelihoods, which can usually be esti-
mated directly from the data, at least under limited
dependence assumptions. The class priors can be
estimated based on the relative distribution sizes de-
rived from the current training sets:
</bodyText>
<equation confidence="0.998821666666667">
P(yi) = |Ti|
P (4)
k |Tk|
</equation>
<bodyText confidence="0.9860162">
where |S |is the number of samples in training set S.
If we assume feature independence, which as we
will see for our task is not as gross an approximation
as it may at first seem, we can simplify the class-
conditional likelihood in the well known manner:
</bodyText>
<equation confidence="0.9984445">
P (xj|yi) = Y P(xjk|yi) (5)
k
</equation>
<bodyText confidence="0.909647">
and then (estimate the likelihood for each feature:
</bodyText>
<equation confidence="0.9998885">
P(xk  |yi) = aP(yi) + f (xk, Ti) (6)
aP(yi) + |Ti|
</equation>
<bodyText confidence="0.999846181818182">
where f(x, S) is the number of samples in training
set S in which feature x is present, and a is a uni-
versal smoothing constant, scaled by the class prior.
This scaling is motivated by the principle that with-
out knowledge of the true distribution of a partic-
ular feature it makes sense to include knowledge
of the class distribution in the smoothing mecha-
nism. Smoothing is particularly important in the
early stages of the learning process when the amount
of training data is severely limited resulting in unre-
liable frequency estimates.
</bodyText>
<sectionHeader confidence="0.972937" genericHeader="method">
8 Hedge Classification
</sectionHeader>
<bodyText confidence="0.999944285714286">
We will now consider how to apply this learning
model to the hedge classification task. As discussed
earlier, the speculative/non-speculative distinction
hinges on the presence or absence of a few hedge
cues within the sentence. Working on this premise,
all features are ranked according to their probability
of ‘hedge cue-ness’:
</bodyText>
<equation confidence="0.99744875">
P(xk|spec) · P(spec)
P(spec|xk) = (7)
PN
n=1 P(yn)P(xk|yn)
</equation>
<bodyText confidence="0.999039555555556">
which can be computed directly using (4) and (6).
The m most probable features are then selected from
each sentence to compute (5) and the rest are ig-
nored. This has the dual benefit of removing irrele-
vant features and also reducing dependence between
features, as the selected features will often be non-
local and thus not too tightly correlated.
Note that this idea differs from traditional feature
selection in two important ways:
</bodyText>
<listItem confidence="0.971747">
1. Only features indicative of the spec class are
retained, or to put it another way, nspec class
membership is inferred from the absence of
strong spec features.
2. Feature selection in this context is not a prepro-
cessing step; i.e. there is no re-estimation after
selection. This has the potentially detrimental
</listItem>
<bodyText confidence="0.68462125">
side effect of skewing the posterior estimates
in favour of the spec class, but is admissible
for the purposes of ranking and classification
by posterior thresholding (see next section).
</bodyText>
<sectionHeader confidence="0.94866" genericHeader="method">
9 Classification
</sectionHeader>
<bodyText confidence="0.9999635">
The weakly supervised learner returns a labelled
data set for each class, from which a classifier can
be trained. We can easily derive a classifier using
the estimates from our learning model by:
</bodyText>
<equation confidence="0.493433">
xj → spec if P(spec|xj) &gt; a (8)
</equation>
<bodyText confidence="0.999360666666667">
where a is an arbitrary threshold used to control the
precision/recall balance. For comparison purposes,
we also use Joachims’ SVMlight (Joachims, 1999).
</bodyText>
<sectionHeader confidence="0.97348" genericHeader="method">
10 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.529606">
10.1 Method
</subsectionHeader>
<bodyText confidence="0.999845666666667">
To examine the practical efficacy of the learning and
classification models we have presented, we use the
following experimental method:
</bodyText>
<listItem confidence="0.999943142857143">
1. Generate seed training data: Sspec and Snspec
2. Initialise: Tspec←Sspec and Tnspec←Snspec
3. Iterate:
• Order U by P(spec|xj) (expression 3)
• Tspec ← most probable batch
• Tnspec ← least probable batch
• Train classifier using Tspec and Tnspec
</listItem>
<page confidence="0.993424">
996
</page>
<table confidence="0.894559">
Rank α = 0 α = 1 α = 5 α = 100 α = 500
1 interactswith suggest suggest suggest suggest
2 TAFb likely likely likely likely
3 sexta may may may may
4 CRYs might might These These
5 DsRed seems seems results results
6 Cell-Nonautonomous suggests Taken might that
7 arva probably suggests observations be
8 inter-homologue suggesting probably Taken data
9 Mohanty possibly Together findings it
10 meld suggested suggesting Our Our
11 aDNA Taken possibly seems observations
12 Deer unlikely suggested together role
13 Borel Together findings Together most
14 substripe physiology observations role these
15 Failing modulated Given that together
</table>
<tableCaption confidence="0.977177">
Table 2: Features ranked by P(spec|xk) for varying α
</tableCaption>
<listItem confidence="0.989718">
• Compute spec recall/precision BEP
(break-even point) on the test data
</listItem>
<bodyText confidence="0.999973923076923">
The batch size for each iteration is set to 0.001 * |U|.
After each learning iteration, we compute the preci-
sion/recall BEP for the spec class using both clas-
sifiers trained on the current labelled data. We use
BEP because it helps to mitigate against misleading
results due to discrepancies in classification thresh-
old placement. Disadvantageously, BEP does not
measure a classifier’s performance across the whole
of the recall/precision spectrum (as can be obtained,
for instance, from receiver-operating characteristic
(ROC) curves), but for our purposes it provides a
clear, abstracted overview of a classifier’s accuracy
given a particular training set.
</bodyText>
<subsectionHeader confidence="0.675521">
10.2 Parameter Setting
</subsectionHeader>
<bodyText confidence="0.999960904761905">
The training and classification models we have pre-
sented require the setting of two parameters: the
smoothing parameter α and the number of features
per sample m. Analysis of the effect of varying α
on feature ranking reveals that when α = 0, low fre-
quency terms with spurious class correlation dom-
inate and as α increases, high frequency terms be-
come increasingly dominant, eventually smoothing
away genuine low-to-mid frequency correlations.
This effect is illustrated in Table 2, and from this
analysis we chose α = 5 as an appropriate level of
smoothing. We use m=5 based on the intuition that
five is a rough upper bound on the number of hedge
cue features likely to occur in any one sentence.
We use the linear kernel for SVMlight with the
default setting for the regularization parameter C.
We construct binary valued, L2-normalised (unit
length) input vectors to represent each sentence,
as this resulted in better performance than using
frequency-based weights and concords with our
presence/absence feature estimates.
</bodyText>
<subsectionHeader confidence="0.822694">
10.3 Seed Generation
</subsectionHeader>
<bodyText confidence="0.9999244">
The learning model we have presented requires a
set of seeds for each class. To generate seeds for
the spec class, we extracted all sentences from U
containing either (or both) of the terms suggest or
likely, as these are very good (though not perfect)
hedge cues, yielding 6423 spec seeds. Generating
seeds for nspec is much more difficult, as integrity
requires the absence of hedge cues, and this cannot
be done automatically. Thus, we used the following
procedure to obtain a set of nspec seeds:
</bodyText>
<listItem confidence="0.994413333333333">
1. Create initial Snspec by sampling randomly
from U.
2. Manually remove more ‘obvious’ speculative
sentences using pattern matching
3. Iterate:
• Order Snspec by P(spec|xj) using esti-
mates from Sspec and current Snspec
• Examine most probable sentences and re-
move speculative instances
</listItem>
<bodyText confidence="0.998491666666667">
We started with 8830 sentences and after a couple of
hours work reduced this down to a (still potentially
noisy) nspec seed set of 7541 sentences.
</bodyText>
<page confidence="0.991674">
997
</page>
<table confidence="0.941288833333333">
BEP
Prob (Prob) denotes our probabilistic learning model and classifier (§9)
Prob (SVM) denotes probabilistic learning model with SVM classifier
SVM (Prob) denotes committee-based model (§10.4) with probabilistic classifier
SVM (SVM) denotes committee-based model with SVM classifier
Baseline denotes substring matching classifier of (Light et al., 2004)
</table>
<figureCaption confidence="0.987275">
Figure 1: Learning curves
</figureCaption>
<sectionHeader confidence="0.589562" genericHeader="method">
10.4 Baselines
</sectionHeader>
<bodyText confidence="0.9999615">
As a baseline classifier we use the substring match-
ing technique of (Light et al., 2004), which labels
a sentence as spec if it contains one or more of the
following: suggest, potential, likely, may, at least,
in part, possibl, further investigation, unlikely, pu-
tative, insights, point toward, promise and propose.
To provide a comparison for our learning model,
we implement a more traditional self-training pro-
cedure in which at each iteration a committee of five
SVMs is trained on randomly generated overlapping
subsets of the training data and their cumulative con-
fidence is used to select items for augmenting the
labelled training data. For similar work see (Banko
and Brill, 2001; Zhang, 2004).
</bodyText>
<sectionHeader confidence="0.985062" genericHeader="method">
10.5 Results
</sectionHeader>
<bodyText confidence="0.997996321428572">
Figure 1 plots accuracy as a function of the train-
ing iteration. After 150 iterations, all of the weakly
supervised learning models are significantly more
accurate than the baseline according to a binomial
sign test (p &lt; 0.01), though there is clearly still
much room for improvement. The baseline classi-
fier achieves a BEP of 0.60 while both classifiers
using our learning model reach approximately 0.76
BEP with little to tell between them. Interestingly,
the combination of the SVM committee-based learn-
ing model with our classifier (denoted by ‘SVM
(Prob)’), performs competitively with both of the ap-
proaches that use our probabilistic learning model
and significantly better than the SVM committee-
based learning model with an SVM classifier, ‘SVM
(SVM)’, according to a binomial sign test (p&lt;0.01)
after 150 iterations. These results suggest that per-
formance may be enhanced when the learning and
classification tasks are carried out by different mod-
els. This is an interesting possibility, which we in-
tend to explore further.
An important issue in incremental learning sce-
narios is identification of the optimum stopping
point. Various methods have been investigated to ad-
dress this problem, such as ‘counter-training’ (Yan-
garber, 2003) and committee agreement (Zhang,
2004); how such ideas can be adapted for this task is
one of many avenues for future research.
</bodyText>
<sectionHeader confidence="0.975075" genericHeader="method">
10.6 Error Analysis
</sectionHeader>
<bodyText confidence="0.999884821428571">
Some errors are due to the variety of hedge forms.
For example, the learning models were unsuccess-
ful in identifying assertive statements of knowledge
paucity, eg: There is no clear evidence for cy-
tochrome c release during apoptosis in C elegans
or Drosophila. Whether it is possible to learn such
examples without additional seed information is an
open question. This example also highlights the po-
tential benefit of an enriched sample representation,
in this case one which accounts for the negation of
the phrase ‘clear evidence’ which otherwise might
suggest a strongly non-speculative assertion.
In many cases hedge classification is challenging
even for a human annotator. For instance, distin-
guishing between a speculative assertion and one
relating to a pattern of observed non-universal be-
haviour is often difficult. The following example
was chosen by the learner as a spec sentence on the
150th training iteration: Each component consists of
a set of subcomponents that can be localized within
a larger distributed neural system. The sentence
does not, in fact, contain a hedge but rather a state-
ment of observed non-universal behaviour. How-
ever, an almost identical variant with ‘could’ instead
of ‘can’ would be a strong speculative candidate.
This highlights the similarity between many hedge
and non-hedge instances, which makes such cases
hard to learn in a weakly supervised manner.
</bodyText>
<figure confidence="0.999006736842105">
0.8
0.78
0.76
0.74
0.72
0.7
0.68
0.66
0.64
0.62
0.6
0.58
0 20 40 60 80 100 120 140
Iteration
Prob (Prob)
Prob (SVM)
SVM (Prob)
SVM (SVM)
Baseline
</figure>
<page confidence="0.986252">
998
</page>
<sectionHeader confidence="0.936376" genericHeader="conclusions">
11 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999985944444444">
We have shown that weakly supervised ML is ap-
plicable to the problem of hedge classification and
that a reasonable level of accuracy can be achieved.
The work presented here has application in the wider
academic community; in fact a key motivation in
this study is to incorporate hedge classification into
an interactive system for aiding curators in the con-
struction and population of gene databases. We have
presented our initial results on the task using a sim-
ple probabilistic model in the hope that this will
encourage others to investigate alternative learning
models and pursue new techniques for improving ac-
curacy. Our next aim is to explore possibilities of
introducing linguistically-motivated knowledge into
the sample representation to help the learner identify
key hedge-related sentential components, and also to
consider hedge classification at the granularity of as-
sertions rather than text sentences.
</bodyText>
<sectionHeader confidence="0.996537" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999403333333333">
This work was partially supported by the FlySlip
project, BBSRC Grant BBS/B/16291, and we thank
Nikiforos Karamanis and Ruth Seal for thorough an-
notation and helpful discussion. The first author is
supported by an University of Cambridge Millen-
nium Scholarship.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979144">
Ron Artstein and Massimo Poesio. 2005. Kappa3 = al-
pha (or beta). Technical report, University of Essex
Department of Computer Science.
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Meeting of the Association for Computational Linguis-
tics, pages 26–33.
Avrim Blum and Tom Mitchell. 1998. Combining la-
belled and unlabelled data with co-training. In Pro-
ceedings of COLT’ 98, pages 92–100, New York, NY,
USA. ACM Press.
Jinxiu Chen, Donghong Ji, Chew L. Tan, and Zhengyu
Niu. 2006. Relation extraction using label propaga-
tion based semi-supervised learning. In Proceedings
of ACL’06, pages 129–136.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceedings of
the Joint SIGDAT Conference on Empirical Methods
in NLP and Very Large Corpora.
George Hripcsak and Adam Rothschild. 2004. Agree-
ment, the f-measure, and reliability in information re-
trieval. JAm Med Inform Assoc., 12(3):296–298.
K. Hyland. 1994. Hedging in academic writing and eap
textbooks. English for Specific Purposes, 13:239–256.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In A. Smola
B. Sch¨olkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. MIT Press, Cam-
bridge, MA.
M. Light, X.Y. Qiu, and P. Srinivasan. 2004. The lan-
guage of bioscience: Facts, speculations, and state-
ments in between. In Proceedings of BioLink 2004
Workshop on Linking Biological Literature, Ontolo-
gies and Databases: Tools for Users, Boston, May
2004.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In HLT-
NAACL.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
Proceedings of NAACL ’03, pages 94–101, Morris-
town, NJ, USA.
K. Nigam and R. Ghani. 2000. Understanding the be-
havior of co-training. In Proceedings of KDD-2000
Workshop on Text Mining.
Kamal Nigam, Andrew K. McCallum, Sebastian Thrun,
and Tom M. Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM. Machine
Learning, 39(2/3):103–134.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Seventh Conference on Natural Lan-
guage Learning (CoNLL-03). ACL SIGNLL., pages
25–32.
Ben Wellner. 2005. Weakly supervised learning meth-
ods for improving the quality of gene name normal-
ization data. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontologies and
Databases, pages 1–8, Detroit, June. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Comput. Linguist., 30(3):277–308.
Roman Yangarber. 2003. Counter-training in discovery
of semantic patterns. In Proceedings ofACL’03, pages
343–350, Morristown, NJ, USA.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of ACL’95, pages 189–196, Morristown, NJ,
USA. ACL.
Zhu Zhang. 2004. Weakly-supervised relation clas-
sification for information extraction. In CIKM ’04:
Proceedings of the thirteenth ACM international con-
ference on Information and knowledge management,
pages 581–588, New York, NY, USA. ACM Press.
</reference>
<page confidence="0.998726">
999
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925390">
<title confidence="0.998002">Weakly Supervised Learning for Hedge Classification in Scientific Literature</title>
<author confidence="0.999989">Ben Medlock</author>
<affiliation confidence="0.999988">Computer Laboratory University of Cambridge</affiliation>
<address confidence="0.9992">Cambridge, C133 OFD</address>
<email confidence="0.958393">benmedlock@cantab.net</email>
<author confidence="0.999497">Ted Briscoe</author>
<affiliation confidence="0.9999865">Computer Laboratory University of Cambridge</affiliation>
<address confidence="0.999103">Cambridge, C133 OFD</address>
<email confidence="0.997973">ejb@cl.cam.ac.uk</email>
<abstract confidence="0.997617230769231">We investigate automatic classification of speculative language (‘hedging’), in biomedical text using weakly supervised machine learning. Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented. We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Kappa3 = alpha (or beta).</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>University of Essex Department of Computer Science.</institution>
<contexts>
<context position="10417" citStr="Artstein and Poesio, 2005" startWordPosition="1591" endWordPosition="1594">headings, captions etc., unlabelled samples were chosen under the constraints that they must be at least 10 words in length and contain a main verb. 5 Annotation and Agreement Two separate annotators were commissioned to label the sentences in the test set, firstly one of the authors and secondly a domain expert with no prior input into the guideline development process. The two annotators labelled the data independently using the guidelines outlined in section 3. Relative F1 (Frel 1 ) and Cohen’s Kappa (n) were then used to quantify the level of agreement. For brevity we refer the reader to (Artstein and Poesio, 2005) and (Hripcsak and Rothschild, 2004) for formulation and discussion of n and Frel 1 respectively. The two metrics are based on different assumptions about the nature of the annotation task. Frel 1 is founded on the premise that the task is to recognise and label spec sentences from within a background population, and does not explicitly model agreement on nspec instances. It ranges from 0 (no agreement) to 1 (no disagreement). Conversely, n gives explicit credit for agreement on both spec and nspec instances. The observed agreement is then corrected for ‘chance agreement’, yielding a metric th</context>
</contexts>
<marker>Artstein, Poesio, 2005</marker>
<rawString>Ron Artstein and Massimo Poesio. 2005. Kappa3 = alpha (or beta). Technical report, University of Essex Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="3518" citStr="Banko and Brill (2001)" startWordPosition="514" endWordPosition="517">dge Classification While there is a certain amount of literature within the linguistics community on the use of hedging in 1available from www.cl.cam.ac.uk/∼bwm23/ Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics scientific text, eg. (Hyland, 1994), there is little of for which the learner is most confident. Early work direct relevance to the task of classifying speculative by Yarowsky (1995) falls within this framework. language from an NLP/ML perspective. Banko and Brill (2001) use ‘bagging’ and agreeThe most clearly relevant study is Light et al. ment to measure confidence on unlabelled samples, (2004) where the focus is on introducing the prob- and more recently McClosky et al. (2006) use selflem, exploring annotation issues and outlining po- training for improving parse reranking. tential applications rather than on the specificities Other relevant recent work includes (Zhang, of the ML approach, though they do present some 2004), in which random feature projection and a results using a manually crafted substring match- committee of SVM classifiers is used in a h</context>
<context position="23653" citStr="Banko and Brill, 2001" startWordPosition="3781" endWordPosition="3784"> (Light et al., 2004), which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part, possibl, further investigation, unlikely, putative, insights, point toward, promise and propose. To provide a comparison for our learning model, we implement a more traditional self-training procedure in which at each iteration a committee of five SVMs is trained on randomly generated overlapping subsets of the training data and their cumulative confidence is used to select items for augmenting the labelled training data. For similar work see (Banko and Brill, 2001; Zhang, 2004). 10.5 Results Figure 1 plots accuracy as a function of the training iteration. After 150 iterations, all of the weakly supervised learning models are significantly more accurate than the baseline according to a binomial sign test (p &lt; 0.01), though there is clearly still much room for improvement. The baseline classifier achieves a BEP of 0.60 while both classifiers using our learning model reach approximately 0.76 BEP with little to tell between them. Interestingly, the combination of the SVM committee-based learning model with our classifier (denoted by ‘SVM (Prob)’), performs</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Meeting of the Association for Computational Linguistics, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labelled and unlabelled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of COLT’ 98,</booktitle>
<pages>92--100</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6237" citStr="Blum and Mitchell (1998)" startWordPosition="945" endWordPosition="948">s noted by Light et al., speculative of research into weakly supervised ML techniques assertions are to be identified on the basis of judgefor NLP applications. Different approaches are of- ments about the author’s intended meaning, rather ten characterised as either multi- or single-view, than on the presence of certain designated hedge where the former generate multiple redundant (or terms. semi-redundant) ‘views’ of a data sample and per- We begin with the hedge definition given by form mutual bootstrapping. This idea was for- Light et al. (item 1) and introduce a set of further malised by Blum and Mitchell (1998) in their guidelines to help elucidate various ‘grey areas’ and presentation of co-training. Co-training has also tighten the task specification. These were developed been used for named entity recognition (NER) after initial annotation by the authors, and through (Collins and Singer, 1999), coreference resolution discussion with colleagues. Further examples are (Ng and Cardie, 2003), text categorization (Nigam given in online Appendix A2. and Ghani, 2000) and improving gene name data The following are considered hedge instances: (Wellner, 2005). 1. An assertion relating to a result that does </context>
<context position="13196" citStr="Blum and Mitchell, 1998" startWordPosition="2044" endWordPosition="2047">kely etc.) and due to the success of ‘bag of words’ representations in many classification tasks to date. Investigating more complex sample representation strategies is an avenue for future research. There are a number of factors that make our formulation of hedge classification both interesting and challenging from a weakly supervised learning perspective. Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features. This is in contrast to much previous work on weakly supervised learning, where for instance in the case of text categorization (Blum and Mitchell, 1998; Nigam et al., 2000) almost all content terms are to some degree relevant, and irrelevant terms can often be filtered out (e.g. stopword removal). In the same vein, for the case of entity/relation extraction and classification (Collins and Singer, 1999; Zhang, 2004; Chen et al., 2006) the context of the entity or entities in consideration provides a highly relevant feature space. Another interesting factor in our formulation of hedge classification is that the nspec class is defined on the basis of the absence of hedge cues, rendering it hard to model directly. This characteristic is also pro</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labelled and unlabelled data with co-training. In Proceedings of COLT’ 98, pages 92–100, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxiu Chen</author>
<author>Donghong Ji</author>
<author>Chew L Tan</author>
<author>Zhengyu Niu</author>
</authors>
<title>Relation extraction using label propagation based semi-supervised learning.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL’06,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="4319" citStr="Chen et al., 2006" startWordPosition="641" endWordPosition="644">ecently McClosky et al. (2006) use selflem, exploring annotation issues and outlining po- training for improving parse reranking. tential applications rather than on the specificities Other relevant recent work includes (Zhang, of the ML approach, though they do present some 2004), in which random feature projection and a results using a manually crafted substring match- committee of SVM classifiers is used in a hybrid ing classifier and a supervised SVM on a collection co/self-training strategy for weakly supervised reof Medline abstracts. We will draw on this work lation classification and (Chen et al., 2006) where throughout our presentation of the task. a graph based algorithm called label propagation is Hedging is sometimes classed under the umbrella employed to perform weakly supervised relation exconcept of subjectivity, which covers a variety of lin- traction. guistic phenomena used to express differing forms 3 The Hedge Classification Task of authorial opinion (Wiebe et al., 2004). Riloff et al. Given a collection of sentences, S, the task is to (2003) explore bootstrapping techniques to identify label each sentence as either speculative or nonsubjective nouns and subsequently classify subj</context>
<context position="13482" citStr="Chen et al., 2006" startWordPosition="2093" endWordPosition="2096">sting and challenging from a weakly supervised learning perspective. Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features. This is in contrast to much previous work on weakly supervised learning, where for instance in the case of text categorization (Blum and Mitchell, 1998; Nigam et al., 2000) almost all content terms are to some degree relevant, and irrelevant terms can often be filtered out (e.g. stopword removal). In the same vein, for the case of entity/relation extraction and classification (Collins and Singer, 1999; Zhang, 2004; Chen et al., 2006) the context of the entity or entities in consideration provides a highly relevant feature space. Another interesting factor in our formulation of hedge classification is that the nspec class is defined on the basis of the absence of hedge cues, rendering it hard to model directly. This characteristic is also problematic in terms of selecting a reliable set of nspec seed sentences, as by definition at the beginning of the learning cycle the learner has little knowledge about what a hedge looks like. This problem is addressed in section 10.3. In this study we develop a learning model based arou</context>
</contexts>
<marker>Chen, Ji, Tan, Niu, 2006</marker>
<rawString>Jinxiu Chen, Donghong Ji, Chew L. Tan, and Zhengyu Niu. 2006. Relation extraction using label propagation based semi-supervised learning. In Proceedings of ACL’06, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.</booktitle>
<contexts>
<context position="6528" citStr="Collins and Singer, 1999" startWordPosition="987" endWordPosition="990"> than on the presence of certain designated hedge where the former generate multiple redundant (or terms. semi-redundant) ‘views’ of a data sample and per- We begin with the hedge definition given by form mutual bootstrapping. This idea was for- Light et al. (item 1) and introduce a set of further malised by Blum and Mitchell (1998) in their guidelines to help elucidate various ‘grey areas’ and presentation of co-training. Co-training has also tighten the task specification. These were developed been used for named entity recognition (NER) after initial annotation by the authors, and through (Collins and Singer, 1999), coreference resolution discussion with colleagues. Further examples are (Ng and Cardie, 2003), text categorization (Nigam given in online Appendix A2. and Ghani, 2000) and improving gene name data The following are considered hedge instances: (Wellner, 2005). 1. An assertion relating to a result that does not Conversely, single-view learning models operate necessarily follow from work presented, but without an explicit partition of the feature space. could be extrapolated from it (Light et al.). Perhaps the most well known of such approaches 2. Relay of hedge made in previous work. is expect</context>
<context position="13449" citStr="Collins and Singer, 1999" startWordPosition="2087" endWordPosition="2090">ion of hedge classification both interesting and challenging from a weakly supervised learning perspective. Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features. This is in contrast to much previous work on weakly supervised learning, where for instance in the case of text categorization (Blum and Mitchell, 1998; Nigam et al., 2000) almost all content terms are to some degree relevant, and irrelevant terms can often be filtered out (e.g. stopword removal). In the same vein, for the case of entity/relation extraction and classification (Collins and Singer, 1999; Zhang, 2004; Chen et al., 2006) the context of the entity or entities in consideration provides a highly relevant feature space. Another interesting factor in our formulation of hedge classification is that the nspec class is defined on the basis of the absence of hedge cues, rendering it hard to model directly. This characteristic is also problematic in terms of selecting a reliable set of nspec seed sentences, as by definition at the beginning of the learning cycle the learner has little knowledge about what a hedge looks like. This problem is addressed in section 10.3. In this study we de</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Hripcsak</author>
<author>Adam Rothschild</author>
</authors>
<title>Agreement, the f-measure, and reliability in information retrieval.</title>
<date>2004</date>
<journal>JAm Med Inform Assoc.,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="10453" citStr="Hripcsak and Rothschild, 2004" startWordPosition="1596" endWordPosition="1600">led samples were chosen under the constraints that they must be at least 10 words in length and contain a main verb. 5 Annotation and Agreement Two separate annotators were commissioned to label the sentences in the test set, firstly one of the authors and secondly a domain expert with no prior input into the guideline development process. The two annotators labelled the data independently using the guidelines outlined in section 3. Relative F1 (Frel 1 ) and Cohen’s Kappa (n) were then used to quantify the level of agreement. For brevity we refer the reader to (Artstein and Poesio, 2005) and (Hripcsak and Rothschild, 2004) for formulation and discussion of n and Frel 1 respectively. The two metrics are based on different assumptions about the nature of the annotation task. Frel 1 is founded on the premise that the task is to recognise and label spec sentences from within a background population, and does not explicitly model agreement on nspec instances. It ranges from 0 (no agreement) to 1 (no disagreement). Conversely, n gives explicit credit for agreement on both spec and nspec instances. The observed agreement is then corrected for ‘chance agreement’, yielding a metric that ranges between −1 and 1. Given ou</context>
</contexts>
<marker>Hripcsak, Rothschild, 2004</marker>
<rawString>George Hripcsak and Adam Rothschild. 2004. Agreement, the f-measure, and reliability in information retrieval. JAm Med Inform Assoc., 12(3):296–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hyland</author>
</authors>
<title>Hedging in academic writing and eap textbooks. English for Specific Purposes,</title>
<date>1994</date>
<pages>13--239</pages>
<contexts>
<context position="3282" citStr="Hyland, 1994" startWordPosition="479" endWordPosition="480">ilistic weakly supervised learning model and use it to motivate our approach. 4. We analyze our learning model experimentally and report promising results for the task on a new publicly-available dataset.1 2 Related Work 2.1 Hedge Classification While there is a certain amount of literature within the linguistics community on the use of hedging in 1available from www.cl.cam.ac.uk/∼bwm23/ Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics scientific text, eg. (Hyland, 1994), there is little of for which the learner is most confident. Early work direct relevance to the task of classifying speculative by Yarowsky (1995) falls within this framework. language from an NLP/ML perspective. Banko and Brill (2001) use ‘bagging’ and agreeThe most clearly relevant study is Light et al. ment to measure confidence on unlabelled samples, (2004) where the focus is on introducing the prob- and more recently McClosky et al. (2006) use selflem, exploring annotation issues and outlining po- training for improving parse reranking. tential applications rather than on the specificiti</context>
</contexts>
<marker>Hyland, 1994</marker>
<rawString>K. Hyland. 1994. Hedging in academic writing and eap textbooks. English for Specific Purposes, 13:239–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods: Support Vector Machines.</booktitle>
<editor>In A. Smola B. Sch¨olkopf, C. Burges, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="18704" citStr="Joachims, 1999" startWordPosition="2995" endWordPosition="2996">as the potentially detrimental side effect of skewing the posterior estimates in favour of the spec class, but is admissible for the purposes of ranking and classification by posterior thresholding (see next section). 9 Classification The weakly supervised learner returns a labelled data set for each class, from which a classifier can be trained. We can easily derive a classifier using the estimates from our learning model by: xj → spec if P(spec|xj) &gt; a (8) where a is an arbitrary threshold used to control the precision/recall balance. For comparison purposes, we also use Joachims’ SVMlight (Joachims, 1999). 10 Experimental Evaluation 10.1 Method To examine the practical efficacy of the learning and classification models we have presented, we use the following experimental method: 1. Generate seed training data: Sspec and Snspec 2. Initialise: Tspec←Sspec and Tnspec←Snspec 3. Iterate: • Order U by P(spec|xj) (expression 3) • Tspec ← most probable batch • Tnspec ← least probable batch • Train classifier using Tspec and Tnspec 996 Rank α = 0 α = 1 α = 5 α = 100 α = 500 1 interactswith suggest suggest suggest suggest 2 TAFb likely likely likely likely 3 sexta may may may may 4 CRYs might might Thes</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. In A. Smola B. Sch¨olkopf, C. Burges, editor, Advances in Kernel Methods: Support Vector Machines. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Light</author>
<author>X Y Qiu</author>
<author>P Srinivasan</author>
</authors>
<title>The language of bioscience: Facts, speculations, and statements in between.</title>
<date>2004</date>
<booktitle>In Proceedings of BioLink 2004 Workshop on Linking Biological Literature, Ontologies and Databases: Tools for Users,</booktitle>
<location>Boston,</location>
<contexts>
<context position="22923" citStr="Light et al., 2004" startWordPosition="3663" endWordPosition="3666"> using estimates from Sspec and current Snspec • Examine most probable sentences and remove speculative instances We started with 8830 sentences and after a couple of hours work reduced this down to a (still potentially noisy) nspec seed set of 7541 sentences. 997 BEP Prob (Prob) denotes our probabilistic learning model and classifier (§9) Prob (SVM) denotes probabilistic learning model with SVM classifier SVM (Prob) denotes committee-based model (§10.4) with probabilistic classifier SVM (SVM) denotes committee-based model with SVM classifier Baseline denotes substring matching classifier of (Light et al., 2004) Figure 1: Learning curves 10.4 Baselines As a baseline classifier we use the substring matching technique of (Light et al., 2004), which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part, possibl, further investigation, unlikely, putative, insights, point toward, promise and propose. To provide a comparison for our learning model, we implement a more traditional self-training procedure in which at each iteration a committee of five SVMs is trained on randomly generated overlapping subsets of the training data and their cu</context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>M. Light, X.Y. Qiu, and P. Srinivasan. 2004. The language of bioscience: Facts, speculations, and statements in between. In Proceedings of BioLink 2004 Workshop on Linking Biological Literature, Ontologies and Databases: Tools for Users, Boston, May 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="3731" citStr="McClosky et al. (2006)" startWordPosition="550" endWordPosition="553">ociation of Computational Linguistics, pages 992–999, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics scientific text, eg. (Hyland, 1994), there is little of for which the learner is most confident. Early work direct relevance to the task of classifying speculative by Yarowsky (1995) falls within this framework. language from an NLP/ML perspective. Banko and Brill (2001) use ‘bagging’ and agreeThe most clearly relevant study is Light et al. ment to measure confidence on unlabelled samples, (2004) where the focus is on introducing the prob- and more recently McClosky et al. (2006) use selflem, exploring annotation issues and outlining po- training for improving parse reranking. tential applications rather than on the specificities Other relevant recent work includes (Zhang, of the ML approach, though they do present some 2004), in which random feature projection and a results using a manually crafted substring match- committee of SVM classifiers is used in a hybrid ing classifier and a supervised SVM on a collection co/self-training strategy for weakly supervised reof Medline abstracts. We will draw on this work lation classification and (Chen et al., 2006) where throu</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL ’03,</booktitle>
<pages>94--101</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6623" citStr="Ng and Cardie, 2003" startWordPosition="999" endWordPosition="1002">erms. semi-redundant) ‘views’ of a data sample and per- We begin with the hedge definition given by form mutual bootstrapping. This idea was for- Light et al. (item 1) and introduce a set of further malised by Blum and Mitchell (1998) in their guidelines to help elucidate various ‘grey areas’ and presentation of co-training. Co-training has also tighten the task specification. These were developed been used for named entity recognition (NER) after initial annotation by the authors, and through (Collins and Singer, 1999), coreference resolution discussion with colleagues. Further examples are (Ng and Cardie, 2003), text categorization (Nigam given in online Appendix A2. and Ghani, 2000) and improving gene name data The following are considered hedge instances: (Wellner, 2005). 1. An assertion relating to a result that does not Conversely, single-view learning models operate necessarily follow from work presented, but without an explicit partition of the feature space. could be extrapolated from it (Light et al.). Perhaps the most well known of such approaches 2. Relay of hedge made in previous work. is expectation maximization (EM), used by Nigam Dl and Ser have been proposed to act redundantly in the </context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>Vincent Ng and Claire Cardie. 2003. Weakly supervised natural language learning without redundant views. In Proceedings of NAACL ’03, pages 94–101, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>R Ghani</author>
</authors>
<title>Understanding the behavior of co-training.</title>
<date>2000</date>
<booktitle>In Proceedings of KDD-2000 Workshop on Text Mining.</booktitle>
<marker>Nigam, Ghani, 2000</marker>
<rawString>K. Nigam and R. Ghani. 2000. Understanding the behavior of co-training. In Proceedings of KDD-2000 Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew K McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom M Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="13217" citStr="Nigam et al., 2000" startWordPosition="2048" endWordPosition="2051"> success of ‘bag of words’ representations in many classification tasks to date. Investigating more complex sample representation strategies is an avenue for future research. There are a number of factors that make our formulation of hedge classification both interesting and challenging from a weakly supervised learning perspective. Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features. This is in contrast to much previous work on weakly supervised learning, where for instance in the case of text categorization (Blum and Mitchell, 1998; Nigam et al., 2000) almost all content terms are to some degree relevant, and irrelevant terms can often be filtered out (e.g. stopword removal). In the same vein, for the case of entity/relation extraction and classification (Collins and Singer, 1999; Zhang, 2004; Chen et al., 2006) the context of the entity or entities in consideration provides a highly relevant feature space. Another interesting factor in our formulation of hedge classification is that the nspec class is defined on the basis of the absence of hedge cues, rendering it hard to model directly. This characteristic is also problematic in terms of </context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew K. McCallum, Sebastian Thrun, and Tom M. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Seventh Conference on Natural Language Learning (CoNLL-03). ACL SIGNLL.,</booktitle>
<pages>25--32</pages>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Seventh Conference on Natural Language Learning (CoNLL-03). ACL SIGNLL., pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Wellner</author>
</authors>
<title>Weakly supervised learning methods for improving the quality of gene name normalization data.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Detroit,</location>
<contexts>
<context position="6788" citStr="Wellner, 2005" startWordPosition="1025" endWordPosition="1026"> introduce a set of further malised by Blum and Mitchell (1998) in their guidelines to help elucidate various ‘grey areas’ and presentation of co-training. Co-training has also tighten the task specification. These were developed been used for named entity recognition (NER) after initial annotation by the authors, and through (Collins and Singer, 1999), coreference resolution discussion with colleagues. Further examples are (Ng and Cardie, 2003), text categorization (Nigam given in online Appendix A2. and Ghani, 2000) and improving gene name data The following are considered hedge instances: (Wellner, 2005). 1. An assertion relating to a result that does not Conversely, single-view learning models operate necessarily follow from work presented, but without an explicit partition of the feature space. could be extrapolated from it (Light et al.). Perhaps the most well known of such approaches 2. Relay of hedge made in previous work. is expectation maximization (EM), used by Nigam Dl and Ser have been proposed to act redundantly in the et al. (2000) for text categorization and by Ng and sensory bristle lineage. Cardie (2003) in combination with a meta-level fea- 3. Statement of knowledge paucity. t</context>
</contexts>
<marker>Wellner, 2005</marker>
<rawString>Ben Wellner. 2005. Weakly supervised learning methods for improving the quality of gene name normalization data. In Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases, pages 1–8, Detroit, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="4705" citStr="Wiebe et al., 2004" startWordPosition="699" endWordPosition="702">f SVM classifiers is used in a hybrid ing classifier and a supervised SVM on a collection co/self-training strategy for weakly supervised reof Medline abstracts. We will draw on this work lation classification and (Chen et al., 2006) where throughout our presentation of the task. a graph based algorithm called label propagation is Hedging is sometimes classed under the umbrella employed to perform weakly supervised relation exconcept of subjectivity, which covers a variety of lin- traction. guistic phenomena used to express differing forms 3 The Hedge Classification Task of authorial opinion (Wiebe et al., 2004). Riloff et al. Given a collection of sentences, S, the task is to (2003) explore bootstrapping techniques to identify label each sentence as either speculative or nonsubjective nouns and subsequently classify subjec- speculative (spec or nspec henceforth). Specifically, tive vs. objective sentences in newswire text. Their S is to be partitioned into two disjoint sets, one repwork bears some relation to ours; however, our do- resenting sentences that contain some form of hedgmains of interest differ (newswire vs. scientific text) ing, and the other representing those that do not. and they do n</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Comput. Linguist., 30(3):277–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Counter-training in discovery of semantic patterns.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL’03,</booktitle>
<pages>343--350</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="24925" citStr="Yangarber, 2003" startWordPosition="3981" endWordPosition="3983">probabilistic learning model and significantly better than the SVM committeebased learning model with an SVM classifier, ‘SVM (SVM)’, according to a binomial sign test (p&lt;0.01) after 150 iterations. These results suggest that performance may be enhanced when the learning and classification tasks are carried out by different models. This is an interesting possibility, which we intend to explore further. An important issue in incremental learning scenarios is identification of the optimum stopping point. Various methods have been investigated to address this problem, such as ‘counter-training’ (Yangarber, 2003) and committee agreement (Zhang, 2004); how such ideas can be adapted for this task is one of many avenues for future research. 10.6 Error Analysis Some errors are due to the variety of hedge forms. For example, the learning models were unsuccessful in identifying assertive statements of knowledge paucity, eg: There is no clear evidence for cytochrome c release during apoptosis in C elegans or Drosophila. Whether it is possible to learn such examples without additional seed information is an open question. This example also highlights the potential benefit of an enriched sample representation,</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>Roman Yangarber. 2003. Counter-training in discovery of semantic patterns. In Proceedings ofACL’03, pages 343–350, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL’95,</booktitle>
<pages>189--196</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3429" citStr="Yarowsky (1995)" startWordPosition="503" endWordPosition="504">g results for the task on a new publicly-available dataset.1 2 Related Work 2.1 Hedge Classification While there is a certain amount of literature within the linguistics community on the use of hedging in 1available from www.cl.cam.ac.uk/∼bwm23/ Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics scientific text, eg. (Hyland, 1994), there is little of for which the learner is most confident. Early work direct relevance to the task of classifying speculative by Yarowsky (1995) falls within this framework. language from an NLP/ML perspective. Banko and Brill (2001) use ‘bagging’ and agreeThe most clearly relevant study is Light et al. ment to measure confidence on unlabelled samples, (2004) where the focus is on introducing the prob- and more recently McClosky et al. (2006) use selflem, exploring annotation issues and outlining po- training for improving parse reranking. tential applications rather than on the specificities Other relevant recent work includes (Zhang, of the ML approach, though they do present some 2004), in which random feature projection and a resu</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of ACL’95, pages 189–196, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhu Zhang</author>
</authors>
<title>Weakly-supervised relation classification for information extraction.</title>
<date>2004</date>
<booktitle>In CIKM ’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>581--588</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13462" citStr="Zhang, 2004" startWordPosition="2091" endWordPosition="2092">n both interesting and challenging from a weakly supervised learning perspective. Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features. This is in contrast to much previous work on weakly supervised learning, where for instance in the case of text categorization (Blum and Mitchell, 1998; Nigam et al., 2000) almost all content terms are to some degree relevant, and irrelevant terms can often be filtered out (e.g. stopword removal). In the same vein, for the case of entity/relation extraction and classification (Collins and Singer, 1999; Zhang, 2004; Chen et al., 2006) the context of the entity or entities in consideration provides a highly relevant feature space. Another interesting factor in our formulation of hedge classification is that the nspec class is defined on the basis of the absence of hedge cues, rendering it hard to model directly. This characteristic is also problematic in terms of selecting a reliable set of nspec seed sentences, as by definition at the beginning of the learning cycle the learner has little knowledge about what a hedge looks like. This problem is addressed in section 10.3. In this study we develop a learn</context>
<context position="23667" citStr="Zhang, 2004" startWordPosition="3785" endWordPosition="3786">which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part, possibl, further investigation, unlikely, putative, insights, point toward, promise and propose. To provide a comparison for our learning model, we implement a more traditional self-training procedure in which at each iteration a committee of five SVMs is trained on randomly generated overlapping subsets of the training data and their cumulative confidence is used to select items for augmenting the labelled training data. For similar work see (Banko and Brill, 2001; Zhang, 2004). 10.5 Results Figure 1 plots accuracy as a function of the training iteration. After 150 iterations, all of the weakly supervised learning models are significantly more accurate than the baseline according to a binomial sign test (p &lt; 0.01), though there is clearly still much room for improvement. The baseline classifier achieves a BEP of 0.60 while both classifiers using our learning model reach approximately 0.76 BEP with little to tell between them. Interestingly, the combination of the SVM committee-based learning model with our classifier (denoted by ‘SVM (Prob)’), performs competitively</context>
<context position="24963" citStr="Zhang, 2004" startWordPosition="3987" endWordPosition="3988">ntly better than the SVM committeebased learning model with an SVM classifier, ‘SVM (SVM)’, according to a binomial sign test (p&lt;0.01) after 150 iterations. These results suggest that performance may be enhanced when the learning and classification tasks are carried out by different models. This is an interesting possibility, which we intend to explore further. An important issue in incremental learning scenarios is identification of the optimum stopping point. Various methods have been investigated to address this problem, such as ‘counter-training’ (Yangarber, 2003) and committee agreement (Zhang, 2004); how such ideas can be adapted for this task is one of many avenues for future research. 10.6 Error Analysis Some errors are due to the variety of hedge forms. For example, the learning models were unsuccessful in identifying assertive statements of knowledge paucity, eg: There is no clear evidence for cytochrome c release during apoptosis in C elegans or Drosophila. Whether it is possible to learn such examples without additional seed information is an open question. This example also highlights the potential benefit of an enriched sample representation, in this case one which accounts for t</context>
</contexts>
<marker>Zhang, 2004</marker>
<rawString>Zhu Zhang. 2004. Weakly-supervised relation classification for information extraction. In CIKM ’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 581–588, New York, NY, USA. ACM Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>