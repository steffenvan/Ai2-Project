<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002085">
<title confidence="0.990185">
Data point selection for cross-language adaptation of dependency parsers
</title>
<author confidence="0.994674">
Anders Søgaard
</author>
<affiliation confidence="0.9970245">
Center for Language Technology
University of Copenhagen
</affiliation>
<address confidence="0.856864">
Njalsgade 142, DK-2300 Copenhagen S
</address>
<email confidence="0.998061">
soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999539588235294">
We consider a very simple, yet effective, ap-
proach to cross language adaptation of depen-
dency parsers. We first remove lexical items
from the treebanks and map part-of-speech
tags into a common tagset. We then train a
language model on tag sequences in otherwise
unlabeled target data and rank labeled source
data by perplexity per word of tag sequences
from less similar to most similar to the target.
We then train our target language parser on
the most similar data points in the source la-
beled data. The strategy achieves much better
results than a non-adapted baseline and state-
of-the-art unsupervised dependency parsing,
and results are comparable to more complex
projection-based cross language adaptation al-
gorithms.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996652173913">
While unsupervised dependency parsing has seen
rapid progress in recent years, results are still far
from the results that can be achieved with supervised
parsers and not yet good enough to solve real-world
problems. In this paper, we will be interested in an
alternative strategy, namely cross-language adapta-
tion of dependency parsers. The idea is, briefly put,
to learn how to parse Arabic, for example, from, say,
a Danish treebank, comparing unlabeled data from
both languages. This is similar to, but more diffi-
cult than most domain adaptation or transfer learn-
ing scenarios, where differences between source and
target distributions are smaller.
Most previous work in cross-language adapta-
tion has used parallel corpora to project dependency
structures across translations using word alignments
(Smith and Eisner, 2009; Spreyer and Kuhn, 2009;
Ganchev et al., 2009), but in this paper we show
that similar results can be achieved by much simpler
means. Specifically, we build on the cross-language
adaptation algorithm for closely related languages
developed by Zeman and Resnik (2008) and extend
it to much less related languages.
</bodyText>
<sectionHeader confidence="0.635753" genericHeader="introduction">
1.1 Related work
</sectionHeader>
<bodyText confidence="0.999953166666667">
Zeman and Resnik (2008) simply mapped part-of-
speech tags of source and target language treebanks
into a common tagset, delexicalized them (removed
all words), trained a parser on the source language
treebank and applied it to the target language. The
intuition is that, at least for relatively similar lan-
guages, features based on part-of-speech tags are
enough to do reasonably well, and languages are rel-
atively similar at this level of abstraction. Of course
annotations differ, but nouns are likely to be depen-
dents of verbs, prepositions are likely to be depen-
dents of nouns, and so on.
Specifically, Zeman and Resnik (2008) trained a
constituent-based parser on the training section of
the Danish treebank and evaluated it on sentences
of up to 40 words in the test section of the Swedish
treebank and obtained an Fl-score of 66.40%. Dan-
ish and Swedish are of course very similar languages
with almost identical syntax, so in a way this result is
not very surprising. In this paper, we present similar
results (50-75%) on full length sentences for very
different languages from different language fami-
lies. Since less related languages differ more in their
syntax, we use data point selection to find syntactic
</bodyText>
<page confidence="0.977933">
682
</page>
<note confidence="0.5848985">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 682–686,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999911210526316">
constructions in the source language that are likely
to be similar to constructions in the target language.
Smith and Eisner (2009) think of cross-language
adaptation as unsupervised projection using word
aligned parallel text to construct training material for
the target language. They show that hard projection
of dependencies using word alignments performs
better than the unsupervised dependency parsing
approach described in Klein and Manning (2004),
based on EM with clever initialization, and that
a quasi-synchronous model using word alignments
to reestimate parameters in EM performs even bet-
ter. The authors report good results (65%-70%) for
somewhat related languages, training on English and
testing on German and Spanish, but they modified
the annotation in the German data making the treat-
ment of certain syntactic constructions more similar
to the English annotations.
Spreyer and Kuhn (2009) use a similar approach
to parse Dutch using labeled data from German and
obtain good results, but again these are very simi-
lar languages. They later extended their results to
English and Italian (Spreyer et al., 2010), but also
modified annotation considerably in order to do so.
Finally, Ganchev et al. (2009) report results of a
similar approach for Bulgarian and Spanish; they re-
port results with and without hand-written language-
specific rules that complete the projected partial de-
pendency trees.
We will compare our results to the plain approach
of Zeman and Resnik (2008), Ganchev et al. (2009)
without hand-written rules and two recent contribu-
tions to unsupervised dependency parsing, Gillen-
water et al. (2010) and Naseem et al. (2010). Gillen-
water et al. (2010) is a fully unsupervised exten-
sion of the approach described in Klein and Man-
ning (2004), whereas Naseem et al. (2010) rely on
hand-written cross-lingual rules.
</bodyText>
<sectionHeader confidence="0.983608" genericHeader="method">
2 Data
</sectionHeader>
<bodyText confidence="0.99995225">
We use four treebanks from the CoNLL 2006 Shared
Task with standard splits. We use the tagset map-
pings also used by Zeman and Resnik (2008) to ob-
tain a common tagset.12 They define tagset map-
</bodyText>
<footnote confidence="0.564673">
1https://wiki.ufal.ms.mff.cuni.cz/user:zeman:interset
</footnote>
<bodyText confidence="0.988252555555556">
2We use the first letter in the common tag as coarse-grained
part-of-speech, and the first three as fine-grained part-of-speech.
pings for Arabic, Bulgarian, Czech, Danish, Por-
tuguese and Swedish. We only use four of these tree-
banks, since Bulgarian and Czech as well as Danish
and Swedish are very similar languages.
The four treebanks used in our experiments are
thus those for Arabic, Bulgarian, Danish and Por-
tuguese. Arabic is a Semitic VSO language with
relatively free word order and rich morphology. Bul-
garian is a Slavic language with relatively free word
order and rich morphology. Danish is a Germanic
V2 language with relatively poor morphology. Fi-
nally, Portuguese is a Roman language with rela-
tively free word order and rich morphology. In sum,
we consider four languages that are less related than
the language pairs studied in earlier papers on cross-
language adaptation of dependency parsers.
</bodyText>
<sectionHeader confidence="0.999885" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.9999">
3.1 Data point selection
</subsectionHeader>
<bodyText confidence="0.999022178571428">
The key idea in our experiments is that we can use a
simple form of instance weighting, similar to what is
often used for correcting sample selection bias or for
domain adaptation, to improve the approach in Ze-
man and Resnik (2008) by selecting only sentences
in the source data that are similar to our target do-
main or language, considering their perplexity per
word in a language model trained on target data. The
idea is that we order the labeled source data from
most similar to least similar to our target data, using
perplexity per word as metric, and use only a portion
of the source data that is similar to our target data.
In cross-language adaptation, the sample selec-
tion bias is primarily a bias in marginal distribu-
tion P(x). This is the covariate shift assumption
(Shimodaira, 2000). Consequently, each sentence
should be weighted by �����
����� where Pt is the target
distribution, and P3 the source distribution.
To see this let x E X in lowercase denote a spe-
cific value of the input variable, an unlabeled exam-
ple. y E Y in lowercase denotes a class value, and
(x, y) is a labeled example. P((x, y)) is the joint
probability of the labeled example, and P�((x, y)) its
empirical distribution.
In supervised learning with N labeled data points,
we minimize the empirical risk to find a good model
0 for a loss function l : X x Y x O:
</bodyText>
<page confidence="0.969784">
683
</page>
<bodyText confidence="0.537876">
�
</bodyText>
<equation confidence="0.896530230769231">
θˆ = arg min
0∈®
(x,y)∈X ×Y
�
θˆ = arg min
0∈®
(x,y)∈X ×Y
Pt(hxs i ,ys i i) i , θ)
Ps(hxs i ,ys i i)l(xs i,ys
Under the covariate shift assumption ���������
��������� for a
pair hx, yi can be replaced with �����
�����. We simplify
</equation>
<bodyText confidence="0.991405814814815">
this function further assuming that
�
� 0 if Pt(x) is low
= 1 if Pt(x) is high
We use perplexity per word of the source lan-
guage POS sequences relative to a model trained
on target language POS sequences to guess whether
Pt(x) is high or low.
The treebanks are first delexicalized and all fea-
tures except part-of-speech tags removed. The
part-of-speech tags are mapped into a common
tagset using the technique described in Zeman and
Resnik (2008). For our main results, which are pre-
sented in Figure 1, we use the remaining three tree-
banks as training material for each language. The
test section of the language in question is used for
testing, while the POS sequences in the target train-
ing section is used for training the unsmoothed lan-
guage model. We use an unsmoothed trigram lan-
guage model rather than a smoothed language model
since modified Knesser-Ney smoothing is not de-
fined for sequences of part-of-speech tags.3
In our experiments we use a graph-based second-
order non-projective dependency parser that induces
models using MIRA (McDonald et al., 2005).4 We
do not optimize parameters on the different lan-
guages, but use default parameters across the board.
</bodyText>
<footnote confidence="0.9995315">
3http://www-speech.sri.com/projects/srilm/
4http://sourceforge.net/projects/mstparser/
</footnote>
<bodyText confidence="0.999909057142857">
We present two results and a baseline for each lan-
guage in Figure 1. Our baseline is the accuracy of
our dependency parser trained on three languages
and evaluated on the fourth language, where tree-
banks have been delexicalized, and part-of-speech
tags mapped into a common format. This is the pro-
posal by Zeman and Resnik (2008). We then present
results using the 90% most similar data points and
results where the amount of labeled data used is se-
lected using 100 sentences sampled from the train-
ing data as held-out data. It can be seen that using
90% of the labeled data seems to be a good strat-
egy if using held-out data is not an option. Since we
consider the unsupervised scenario where no labeled
data is available for the target language, we consider
the results obtained using the 90% most similar sen-
tences in the labeled data as our primary results.
That we obtain good results training on all the
three remaining treebanks for each language illus-
trates the robustness of our approach. However, it
may in some cases be better to train on data from
a single resource only. The results presented in
Figure 2 are the best results obtained with varying
amounts of source language data (10%, 20%, ... , or
100%). The results are only explorative. In all cases,
we obtain slightly results with training material from
only one language that are better than or as good as
our main results, but differences are marginal. We
obtain the best results for Arabic training using la-
beled data from the Bulgarian treebank, and the best
results for Bulgarian training on Portuguese only.
The best results for Danish were, somewhat surpris-
ingly, obtained using the Arabic treebank,5 and the
best results for Portuguese were obtained training
only on Bulgarian data.
</bodyText>
<sectionHeader confidence="0.988302" genericHeader="method">
4 Error analysis
</sectionHeader>
<bodyText confidence="0.996307">
Consider our analysis of the Arabic sentence in Fig-
ure 3, using the three remaining treebanks as source
data. First note that our dependency labels are all
wrong; we did not map the dependency labels of
the source and target treebanks into a common set
of labels. Otherwise we only make mistakes about
punctuation. Our labels seem meaningful, but come
</bodyText>
<footnote confidence="0.759795666666667">
5Arabic and Danish have in common that definiteness is ex-
pressed by inflectional morphology, though, and both languages
frequently use VSO constructions.
</footnote>
<table confidence="0.757101428571429">
Pˆ(hx, yi)l(x, y, θ)
= arg min N l(xi, yi, θ)
0∈® i=1
In domain adaptation, we can rewrite this as:
Pt(hx, yi) ˆPs(hx, yi)l(x, y, θ)
Ps(hx,yi)
� Ns
</table>
<equation confidence="0.8065846">
i=1
= arg min
0∈®
Pt(x)
Ps(x)
</equation>
<page confidence="0.995268">
684
</page>
<table confidence="0.999015125">
Arabic Bulgarian Danish Portuguese
&lt; 10 00 &lt; 10 00 &lt; 10 00 &lt; 10 00
Ganchev et al. (2009) - - 67.8 - - - - -
Gillenwater et al. (2010) - - 54.3 - 47.2 - 59.8 -
Naseem et al. (2010) - - - - 51.9 - 71.5 -
100% (baseline) - 45.5 - 44.5 - 51.7 - 37.1
90% 48.3 48.4 77.1 70.2 59.4 51.9 83.1 75.1
Held-out % - 49.2 - 70.3 - 52.8 - 75.1
</table>
<figureCaption confidence="0.537075">
Figure 1: Main results.
</figureCaption>
<table confidence="0.99892">
source/target Arabic Bulgarian Danish Portuguese
Arabic - 45.8 56.5 37.8
Bulgarian 50.2 - 50.8 76.9
Danish 46.9 60.4 - 63.5
Portuguese 50.1 70.3 52.2 -
</table>
<figureCaption confidence="0.980681333333333">
Figure 2: Best results obtained with different combinations of source and target languages.
Figure 3: A predicted analysis for an Arabic sentence and
its correct analysis.
</figureCaption>
<bodyText confidence="0.999367769230769">
from different treebanks, e.g. ’pnct’ from the Danish
treebank and ’PUNC’ from the Portuguese one.
If we consider the case where we train on all re-
maining treebanks and use the 90% data points most
similar to the target language, and compare it to our
100% baseline, our error reductions are distributed
as follows, relative to dependency length: For Ara-
bic, the error reduction in F1 scores decreases with
dependency length, and more errors are made at-
taching to the root, but for Portuguese, where the
improvements are more dramatic, we see the biggest
improvements with attachments to the roots and
long dependencies:
</bodyText>
<table confidence="0.766628833333333">
Portuguese bl (Fl) 90% (Fl) err.red
root 0.627 0.913 76.7%
1 0.720 0.894 62.1%
2 0.292 0.768 67.2%
3–6 0.328 0.570 36.0%
7– 0.240 0.561 42.3%
</table>
<bodyText confidence="0.997966">
For Danish, we see a similar pattern, but for Bul-
garian, error reductions are equally distributed.
Generally, it is interesting that cross-language
adaptation and data point selection were less effec-
tive for Danish. One explantation may be differ-
ences in annotation, however. The Danish depen-
dency treebank is annotated very differently from
most other dependency treebanks; for example, the
treebank adopts a DP-analysis of noun phrases.
Finally, we note that all languages benefit from re-
moving the least similar 10% of the labeled source
data, but results are less influenced by how much of
the remaining data we use. For example, for Bulgar-
ian our baseline result using 100% of the source data
is 44.5%, and the result obtained using 90% of the
source data is 70.2%. Using held-out data, we only
use 80% of the source data, which is slightly better
(70.3%), but even if we only use 10% of the source
data, our accuracy is still significantly better than the
baseline (66.9%).
</bodyText>
<sectionHeader confidence="0.999683" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999952555555556">
This paper presented a simple data point selection
strategy for semi-supervised cross language adapta-
tion where no labeled target data is available. This
problem is difficult, but we have presented very pos-
itive results. Since our strategy is a parameter-free
wrapper method it can easily be applied to other
dependency parsers and other problems in natural
language processing, incl. part-of-speech tagging,
named entity recognition, and machine translation.
</bodyText>
<page confidence="0.998783">
685
</page>
<sectionHeader confidence="0.995858" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997185387096774">
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In ACL.
Jennifer Gillenwater, Kuzman Ganchev, Joao Graca, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity in de-
pendency grammar induction. In ACL.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowledge
to guide grammar induction. In EMNLP.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227–244.
David Smith and Jason Eisner. 2009. Parser adaptation
and projection with quasi-synchronous grammar fea-
tures. In EMNLP.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven de-
pendency parsing of new languages using incomplete
and noisy training data. In CoNLL.
Kathrin Spreyer, Lilja Øvrelid, and Jonas Kuhn. 2010.
Training parsers on partial trees: a cross-language
comparison. In LREC.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In IJC-
NLP.
</reference>
<page confidence="0.998719">
686
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.552897">
<title confidence="0.993672">Data point selection for cross-language adaptation of dependency parsers</title>
<author confidence="0.95123">Anders</author>
<affiliation confidence="0.9986335">Center for Language University of</affiliation>
<address confidence="0.923917">Njalsgade 142, DK-2300 Copenhagen</address>
<email confidence="0.990653">soegaard@hum.ku.dk</email>
<abstract confidence="0.979355333333333">consider a yet effective, approach to cross language adaptation of dependency parsers. We first remove lexical items from the treebanks and map part-of-speech tags into a common tagset. We then train a language model on tag sequences in otherwise unlabeled target data and rank labeled source data by perplexity per word of tag sequences from less similar to most similar to the target. We then train our target language parser on the most similar data points in the source labeled data. The strategy achieves much better results than a non-adapted baseline and stateof-the-art unsupervised dependency parsing, and results are comparable to more complex projection-based cross language adaptation algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1820" citStr="Ganchev et al., 2009" startWordPosition="273" endWordPosition="276">ed in an alternative strategy, namely cross-language adaptation of dependency parsers. The idea is, briefly put, to learn how to parse Arabic, for example, from, say, a Danish treebank, comparing unlabeled data from both languages. This is similar to, but more difficult than most domain adaptation or transfer learning scenarios, where differences between source and target distributions are smaller. Most previous work in cross-language adaptation has used parallel corpora to project dependency structures across translations using word alignments (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Ganchev et al., 2009), but in this paper we show that similar results can be achieved by much simpler means. Specifically, we build on the cross-language adaptation algorithm for closely related languages developed by Zeman and Resnik (2008) and extend it to much less related languages. 1.1 Related work Zeman and Resnik (2008) simply mapped part-ofspeech tags of source and target language treebanks into a common tagset, delexicalized them (removed all words), trained a parser on the source language treebank and applied it to the target language. The intuition is that, at least for relatively similar languages, fea</context>
<context position="4745" citStr="Ganchev et al. (2009)" startWordPosition="730" endWordPosition="733"> better. The authors report good results (65%-70%) for somewhat related languages, training on English and testing on German and Spanish, but they modified the annotation in the German data making the treatment of certain syntactic constructions more similar to the English annotations. Spreyer and Kuhn (2009) use a similar approach to parse Dutch using labeled data from German and obtain good results, but again these are very similar languages. They later extended their results to English and Italian (Spreyer et al., 2010), but also modified annotation considerably in order to do so. Finally, Ganchev et al. (2009) report results of a similar approach for Bulgarian and Spanish; they report results with and without hand-written languagespecific rules that complete the projected partial dependency trees. We will compare our results to the plain approach of Zeman and Resnik (2008), Ganchev et al. (2009) without hand-written rules and two recent contributions to unsupervised dependency parsing, Gillenwater et al. (2010) and Naseem et al. (2010). Gillenwater et al. (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al. (2010) rely on hand-written</context>
<context position="11965" citStr="Ganchev et al. (2009)" startWordPosition="1965" endWordPosition="1968"> wrong; we did not map the dependency labels of the source and target treebanks into a common set of labels. Otherwise we only make mistakes about punctuation. Our labels seem meaningful, but come 5Arabic and Danish have in common that definiteness is expressed by inflectional morphology, though, and both languages frequently use VSO constructions. Pˆ(hx, yi)l(x, y, θ) = arg min N l(xi, yi, θ) 0∈® i=1 In domain adaptation, we can rewrite this as: Pt(hx, yi) ˆPs(hx, yi)l(x, y, θ) Ps(hx,yi) � Ns i=1 = arg min 0∈® Pt(x) Ps(x) 684 Arabic Bulgarian Danish Portuguese &lt; 10 00 &lt; 10 00 &lt; 10 00 &lt; 10 00 Ganchev et al. (2009) - - 67.8 - - - - - Gillenwater et al. (2010) - - 54.3 - 47.2 - 59.8 - Naseem et al. (2010) - - - - 51.9 - 71.5 - 100% (baseline) - 45.5 - 44.5 - 51.7 - 37.1 90% 48.3 48.4 77.1 70.2 59.4 51.9 83.1 75.1 Held-out % - 49.2 - 70.3 - 52.8 - 75.1 Figure 1: Main results. source/target Arabic Bulgarian Danish Portuguese Arabic - 45.8 56.5 37.8 Bulgarian 50.2 - 50.8 76.9 Danish 46.9 60.4 - 63.5 Portuguese 50.1 70.3 52.2 - Figure 2: Best results obtained with different combinations of source and target languages. Figure 3: A predicted analysis for an Arabic sentence and its correct analysis. from differ</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Gillenwater</author>
<author>Kuzman Ganchev</author>
<author>Joao Graca</author>
<author>Fernando Pereira</author>
<author>Ben Taskar</author>
</authors>
<title>Sparsity in dependency grammar induction.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5154" citStr="Gillenwater et al. (2010)" startWordPosition="793" endWordPosition="797">but again these are very similar languages. They later extended their results to English and Italian (Spreyer et al., 2010), but also modified annotation considerably in order to do so. Finally, Ganchev et al. (2009) report results of a similar approach for Bulgarian and Spanish; they report results with and without hand-written languagespecific rules that complete the projected partial dependency trees. We will compare our results to the plain approach of Zeman and Resnik (2008), Ganchev et al. (2009) without hand-written rules and two recent contributions to unsupervised dependency parsing, Gillenwater et al. (2010) and Naseem et al. (2010). Gillenwater et al. (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al. (2010) rely on hand-written cross-lingual rules. 2 Data We use four treebanks from the CoNLL 2006 Shared Task with standard splits. We use the tagset mappings also used by Zeman and Resnik (2008) to obtain a common tagset.12 They define tagset map1https://wiki.ufal.ms.mff.cuni.cz/user:zeman:interset 2We use the first letter in the common tag as coarse-grained part-of-speech, and the first three as fine-grained part-of-speech. pings </context>
<context position="12010" citStr="Gillenwater et al. (2010)" startWordPosition="1977" endWordPosition="1980">els of the source and target treebanks into a common set of labels. Otherwise we only make mistakes about punctuation. Our labels seem meaningful, but come 5Arabic and Danish have in common that definiteness is expressed by inflectional morphology, though, and both languages frequently use VSO constructions. Pˆ(hx, yi)l(x, y, θ) = arg min N l(xi, yi, θ) 0∈® i=1 In domain adaptation, we can rewrite this as: Pt(hx, yi) ˆPs(hx, yi)l(x, y, θ) Ps(hx,yi) � Ns i=1 = arg min 0∈® Pt(x) Ps(x) 684 Arabic Bulgarian Danish Portuguese &lt; 10 00 &lt; 10 00 &lt; 10 00 &lt; 10 00 Ganchev et al. (2009) - - 67.8 - - - - - Gillenwater et al. (2010) - - 54.3 - 47.2 - 59.8 - Naseem et al. (2010) - - - - 51.9 - 71.5 - 100% (baseline) - 45.5 - 44.5 - 51.7 - 37.1 90% 48.3 48.4 77.1 70.2 59.4 51.9 83.1 75.1 Held-out % - 49.2 - 70.3 - 52.8 - 75.1 Figure 1: Main results. source/target Arabic Bulgarian Danish Portuguese Arabic - 45.8 56.5 37.8 Bulgarian 50.2 - 50.8 76.9 Danish 46.9 60.4 - 63.5 Portuguese 50.1 70.3 52.2 - Figure 2: Best results obtained with different combinations of source and target languages. Figure 3: A predicted analysis for an Arabic sentence and its correct analysis. from different treebanks, e.g. ’pnct’ from the Danish tr</context>
</contexts>
<marker>Gillenwater, Ganchev, Graca, Pereira, Taskar, 2010</marker>
<rawString>Jennifer Gillenwater, Kuzman Ganchev, Joao Graca, Fernando Pereira, and Ben Taskar. 2010. Sparsity in dependency grammar induction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3981" citStr="Klein and Manning (2004)" startWordPosition="609" endWordPosition="612">eeting of the Association for Computational Linguistics:shortpapers, pages 682–686, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics constructions in the source language that are likely to be similar to constructions in the target language. Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. They show that hard projection of dependencies using word alignments performs better than the unsupervised dependency parsing approach described in Klein and Manning (2004), based on EM with clever initialization, and that a quasi-synchronous model using word alignments to reestimate parameters in EM performs even better. The authors report good results (65%-70%) for somewhat related languages, training on English and testing on German and Spanish, but they modified the annotation in the German data making the treatment of certain syntactic constructions more similar to the English annotations. Spreyer and Kuhn (2009) use a similar approach to parse Dutch using labeled data from German and obtain good results, but again these are very similar languages. They lat</context>
<context position="5294" citStr="Klein and Manning (2004)" startWordPosition="819" endWordPosition="823">d annotation considerably in order to do so. Finally, Ganchev et al. (2009) report results of a similar approach for Bulgarian and Spanish; they report results with and without hand-written languagespecific rules that complete the projected partial dependency trees. We will compare our results to the plain approach of Zeman and Resnik (2008), Ganchev et al. (2009) without hand-written rules and two recent contributions to unsupervised dependency parsing, Gillenwater et al. (2010) and Naseem et al. (2010). Gillenwater et al. (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al. (2010) rely on hand-written cross-lingual rules. 2 Data We use four treebanks from the CoNLL 2006 Shared Task with standard splits. We use the tagset mappings also used by Zeman and Resnik (2008) to obtain a common tagset.12 They define tagset map1https://wiki.ufal.ms.mff.cuni.cz/user:zeman:interset 2We use the first letter in the common tag as coarse-grained part-of-speech, and the first three as fine-grained part-of-speech. pings for Arabic, Bulgarian, Czech, Danish, Portuguese and Swedish. We only use four of these treebanks, since Bulgarian and Czech as well as Dani</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher Manning. 2004. Corpusbased induction of syntactic structure: models of dependency and constituency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9219" citStr="McDonald et al., 2005" startWordPosition="1499" endWordPosition="1502">08). For our main results, which are presented in Figure 1, we use the remaining three treebanks as training material for each language. The test section of the language in question is used for testing, while the POS sequences in the target training section is used for training the unsmoothed language model. We use an unsmoothed trigram language model rather than a smoothed language model since modified Knesser-Ney smoothing is not defined for sequences of part-of-speech tags.3 In our experiments we use a graph-based secondorder non-projective dependency parser that induces models using MIRA (McDonald et al., 2005).4 We do not optimize parameters on the different languages, but use default parameters across the board. 3http://www-speech.sri.com/projects/srilm/ 4http://sourceforge.net/projects/mstparser/ We present two results and a baseline for each language in Figure 1. Our baseline is the accuracy of our dependency parser trained on three languages and evaluated on the fourth language, where treebanks have been delexicalized, and part-of-speech tags mapped into a common format. This is the proposal by Zeman and Resnik (2008). We then present results using the 90% most similar data points and results w</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5179" citStr="Naseem et al. (2010)" startWordPosition="799" endWordPosition="802">ar languages. They later extended their results to English and Italian (Spreyer et al., 2010), but also modified annotation considerably in order to do so. Finally, Ganchev et al. (2009) report results of a similar approach for Bulgarian and Spanish; they report results with and without hand-written languagespecific rules that complete the projected partial dependency trees. We will compare our results to the plain approach of Zeman and Resnik (2008), Ganchev et al. (2009) without hand-written rules and two recent contributions to unsupervised dependency parsing, Gillenwater et al. (2010) and Naseem et al. (2010). Gillenwater et al. (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al. (2010) rely on hand-written cross-lingual rules. 2 Data We use four treebanks from the CoNLL 2006 Shared Task with standard splits. We use the tagset mappings also used by Zeman and Resnik (2008) to obtain a common tagset.12 They define tagset map1https://wiki.ufal.ms.mff.cuni.cz/user:zeman:interset 2We use the first letter in the common tag as coarse-grained part-of-speech, and the first three as fine-grained part-of-speech. pings for Arabic, Bulgarian, Cz</context>
<context position="12056" citStr="Naseem et al. (2010)" startWordPosition="1989" endWordPosition="1992">n set of labels. Otherwise we only make mistakes about punctuation. Our labels seem meaningful, but come 5Arabic and Danish have in common that definiteness is expressed by inflectional morphology, though, and both languages frequently use VSO constructions. Pˆ(hx, yi)l(x, y, θ) = arg min N l(xi, yi, θ) 0∈® i=1 In domain adaptation, we can rewrite this as: Pt(hx, yi) ˆPs(hx, yi)l(x, y, θ) Ps(hx,yi) � Ns i=1 = arg min 0∈® Pt(x) Ps(x) 684 Arabic Bulgarian Danish Portuguese &lt; 10 00 &lt; 10 00 &lt; 10 00 &lt; 10 00 Ganchev et al. (2009) - - 67.8 - - - - - Gillenwater et al. (2010) - - 54.3 - 47.2 - 59.8 - Naseem et al. (2010) - - - - 51.9 - 71.5 - 100% (baseline) - 45.5 - 44.5 - 51.7 - 37.1 90% 48.3 48.4 77.1 70.2 59.4 51.9 83.1 75.1 Held-out % - 49.2 - 70.3 - 52.8 - 75.1 Figure 1: Main results. source/target Arabic Bulgarian Danish Portuguese Arabic - 45.8 56.5 37.8 Bulgarian 50.2 - 50.8 76.9 Danish 46.9 60.4 - 63.5 Portuguese 50.1 70.3 52.2 - Figure 2: Best results obtained with different combinations of source and target languages. Figure 3: A predicted analysis for an Arabic sentence and its correct analysis. from different treebanks, e.g. ’pnct’ from the Danish treebank and ’PUNC’ from the Portuguese one. If </context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetoshi Shimodaira</author>
</authors>
<title>Improving predictive inference under covariate shift by weighting the loglikelihood function.</title>
<date>2000</date>
<journal>Journal of Statistical Planning and Inference,</journal>
<pages>90--227</pages>
<contexts>
<context position="7363" citStr="Shimodaira, 2000" startWordPosition="1164" endWordPosition="1165">, to improve the approach in Zeman and Resnik (2008) by selecting only sentences in the source data that are similar to our target domain or language, considering their perplexity per word in a language model trained on target data. The idea is that we order the labeled source data from most similar to least similar to our target data, using perplexity per word as metric, and use only a portion of the source data that is similar to our target data. In cross-language adaptation, the sample selection bias is primarily a bias in marginal distribution P(x). This is the covariate shift assumption (Shimodaira, 2000). Consequently, each sentence should be weighted by ����� ����� where Pt is the target distribution, and P3 the source distribution. To see this let x E X in lowercase denote a specific value of the input variable, an unlabeled example. y E Y in lowercase denotes a class value, and (x, y) is a labeled example. P((x, y)) is the joint probability of the labeled example, and P�((x, y)) its empirical distribution. In supervised learning with N labeled data points, we minimize the empirical risk to find a good model 0 for a loss function l : X x Y x O: 683 � θˆ = arg min 0∈® (x,y)∈X ×Y � θˆ = arg m</context>
</contexts>
<marker>Shimodaira, 2000</marker>
<rawString>Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90:227–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1773" citStr="Smith and Eisner, 2009" startWordPosition="265" endWordPosition="268">rld problems. In this paper, we will be interested in an alternative strategy, namely cross-language adaptation of dependency parsers. The idea is, briefly put, to learn how to parse Arabic, for example, from, say, a Danish treebank, comparing unlabeled data from both languages. This is similar to, but more difficult than most domain adaptation or transfer learning scenarios, where differences between source and target distributions are smaller. Most previous work in cross-language adaptation has used parallel corpora to project dependency structures across translations using word alignments (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Ganchev et al., 2009), but in this paper we show that similar results can be achieved by much simpler means. Specifically, we build on the cross-language adaptation algorithm for closely related languages developed by Zeman and Resnik (2008) and extend it to much less related languages. 1.1 Related work Zeman and Resnik (2008) simply mapped part-ofspeech tags of source and target language treebanks into a common tagset, delexicalized them (removed all words), trained a parser on the source language treebank and applied it to the target language. The intuition is that,</context>
<context position="3657" citStr="Smith and Eisner (2009)" startWordPosition="565" endWordPosition="568">n a way this result is not very surprising. In this paper, we present similar results (50-75%) on full length sentences for very different languages from different language families. Since less related languages differ more in their syntax, we use data point selection to find syntactic 682 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 682–686, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics constructions in the source language that are likely to be similar to constructions in the target language. Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. They show that hard projection of dependencies using word alignments performs better than the unsupervised dependency parsing approach described in Klein and Manning (2004), based on EM with clever initialization, and that a quasi-synchronous model using word alignments to reestimate parameters in EM performs even better. The authors report good results (65%-70%) for somewhat related languages, training on English and testing on German and Span</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathrin Spreyer</author>
<author>Jonas Kuhn</author>
</authors>
<title>Data-driven dependency parsing of new languages using incomplete and noisy training data. In CoNLL.</title>
<date>2009</date>
<contexts>
<context position="1797" citStr="Spreyer and Kuhn, 2009" startWordPosition="269" endWordPosition="272">per, we will be interested in an alternative strategy, namely cross-language adaptation of dependency parsers. The idea is, briefly put, to learn how to parse Arabic, for example, from, say, a Danish treebank, comparing unlabeled data from both languages. This is similar to, but more difficult than most domain adaptation or transfer learning scenarios, where differences between source and target distributions are smaller. Most previous work in cross-language adaptation has used parallel corpora to project dependency structures across translations using word alignments (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Ganchev et al., 2009), but in this paper we show that similar results can be achieved by much simpler means. Specifically, we build on the cross-language adaptation algorithm for closely related languages developed by Zeman and Resnik (2008) and extend it to much less related languages. 1.1 Related work Zeman and Resnik (2008) simply mapped part-ofspeech tags of source and target language treebanks into a common tagset, delexicalized them (removed all words), trained a parser on the source language treebank and applied it to the target language. The intuition is that, at least for relatively</context>
<context position="4434" citStr="Spreyer and Kuhn (2009)" startWordPosition="678" endWordPosition="681">ey show that hard projection of dependencies using word alignments performs better than the unsupervised dependency parsing approach described in Klein and Manning (2004), based on EM with clever initialization, and that a quasi-synchronous model using word alignments to reestimate parameters in EM performs even better. The authors report good results (65%-70%) for somewhat related languages, training on English and testing on German and Spanish, but they modified the annotation in the German data making the treatment of certain syntactic constructions more similar to the English annotations. Spreyer and Kuhn (2009) use a similar approach to parse Dutch using labeled data from German and obtain good results, but again these are very similar languages. They later extended their results to English and Italian (Spreyer et al., 2010), but also modified annotation considerably in order to do so. Finally, Ganchev et al. (2009) report results of a similar approach for Bulgarian and Spanish; they report results with and without hand-written languagespecific rules that complete the projected partial dependency trees. We will compare our results to the plain approach of Zeman and Resnik (2008), Ganchev et al. (200</context>
</contexts>
<marker>Spreyer, Kuhn, 2009</marker>
<rawString>Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven dependency parsing of new languages using incomplete and noisy training data. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathrin Spreyer</author>
<author>Lilja Øvrelid</author>
<author>Jonas Kuhn</author>
</authors>
<title>Training parsers on partial trees: a cross-language comparison.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="4652" citStr="Spreyer et al., 2010" startWordPosition="715" endWordPosition="718"> a quasi-synchronous model using word alignments to reestimate parameters in EM performs even better. The authors report good results (65%-70%) for somewhat related languages, training on English and testing on German and Spanish, but they modified the annotation in the German data making the treatment of certain syntactic constructions more similar to the English annotations. Spreyer and Kuhn (2009) use a similar approach to parse Dutch using labeled data from German and obtain good results, but again these are very similar languages. They later extended their results to English and Italian (Spreyer et al., 2010), but also modified annotation considerably in order to do so. Finally, Ganchev et al. (2009) report results of a similar approach for Bulgarian and Spanish; they report results with and without hand-written languagespecific rules that complete the projected partial dependency trees. We will compare our results to the plain approach of Zeman and Resnik (2008), Ganchev et al. (2009) without hand-written rules and two recent contributions to unsupervised dependency parsing, Gillenwater et al. (2010) and Naseem et al. (2010). Gillenwater et al. (2010) is a fully unsupervised extension of the appr</context>
</contexts>
<marker>Spreyer, Øvrelid, Kuhn, 2010</marker>
<rawString>Kathrin Spreyer, Lilja Øvrelid, and Jonas Kuhn. 2010. Training parsers on partial trees: a cross-language comparison. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Cross-language parser adaptation between related languages.</title>
<date>2008</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="2040" citStr="Zeman and Resnik (2008)" startWordPosition="307" endWordPosition="310">th languages. This is similar to, but more difficult than most domain adaptation or transfer learning scenarios, where differences between source and target distributions are smaller. Most previous work in cross-language adaptation has used parallel corpora to project dependency structures across translations using word alignments (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Ganchev et al., 2009), but in this paper we show that similar results can be achieved by much simpler means. Specifically, we build on the cross-language adaptation algorithm for closely related languages developed by Zeman and Resnik (2008) and extend it to much less related languages. 1.1 Related work Zeman and Resnik (2008) simply mapped part-ofspeech tags of source and target language treebanks into a common tagset, delexicalized them (removed all words), trained a parser on the source language treebank and applied it to the target language. The intuition is that, at least for relatively similar languages, features based on part-of-speech tags are enough to do reasonably well, and languages are relatively similar at this level of abstraction. Of course annotations differ, but nouns are likely to be dependents of verbs, prepos</context>
<context position="5013" citStr="Zeman and Resnik (2008)" startWordPosition="773" endWordPosition="776">English annotations. Spreyer and Kuhn (2009) use a similar approach to parse Dutch using labeled data from German and obtain good results, but again these are very similar languages. They later extended their results to English and Italian (Spreyer et al., 2010), but also modified annotation considerably in order to do so. Finally, Ganchev et al. (2009) report results of a similar approach for Bulgarian and Spanish; they report results with and without hand-written languagespecific rules that complete the projected partial dependency trees. We will compare our results to the plain approach of Zeman and Resnik (2008), Ganchev et al. (2009) without hand-written rules and two recent contributions to unsupervised dependency parsing, Gillenwater et al. (2010) and Naseem et al. (2010). Gillenwater et al. (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al. (2010) rely on hand-written cross-lingual rules. 2 Data We use four treebanks from the CoNLL 2006 Shared Task with standard splits. We use the tagset mappings also used by Zeman and Resnik (2008) to obtain a common tagset.12 They define tagset map1https://wiki.ufal.ms.mff.cuni.cz/user:zeman:int</context>
<context position="6798" citStr="Zeman and Resnik (2008)" startWordPosition="1063" endWordPosition="1067">ree word order and rich morphology. Danish is a Germanic V2 language with relatively poor morphology. Finally, Portuguese is a Roman language with relatively free word order and rich morphology. In sum, we consider four languages that are less related than the language pairs studied in earlier papers on crosslanguage adaptation of dependency parsers. 3 Experiments 3.1 Data point selection The key idea in our experiments is that we can use a simple form of instance weighting, similar to what is often used for correcting sample selection bias or for domain adaptation, to improve the approach in Zeman and Resnik (2008) by selecting only sentences in the source data that are similar to our target domain or language, considering their perplexity per word in a language model trained on target data. The idea is that we order the labeled source data from most similar to least similar to our target data, using perplexity per word as metric, and use only a portion of the source data that is similar to our target data. In cross-language adaptation, the sample selection bias is primarily a bias in marginal distribution P(x). This is the covariate shift assumption (Shimodaira, 2000). Consequently, each sentence shoul</context>
<context position="8600" citStr="Zeman and Resnik (2008)" startWordPosition="1396" endWordPosition="1399"> ×Y Pt(hxs i ,ys i i) i , θ) Ps(hxs i ,ys i i)l(xs i,ys Under the covariate shift assumption ��������� ��������� for a pair hx, yi can be replaced with ����� �����. We simplify this function further assuming that � � 0 if Pt(x) is low = 1 if Pt(x) is high We use perplexity per word of the source language POS sequences relative to a model trained on target language POS sequences to guess whether Pt(x) is high or low. The treebanks are first delexicalized and all features except part-of-speech tags removed. The part-of-speech tags are mapped into a common tagset using the technique described in Zeman and Resnik (2008). For our main results, which are presented in Figure 1, we use the remaining three treebanks as training material for each language. The test section of the language in question is used for testing, while the POS sequences in the target training section is used for training the unsmoothed language model. We use an unsmoothed trigram language model rather than a smoothed language model since modified Knesser-Ney smoothing is not defined for sequences of part-of-speech tags.3 In our experiments we use a graph-based secondorder non-projective dependency parser that induces models using MIRA (McD</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>