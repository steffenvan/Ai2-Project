<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000063">
<title confidence="0.960454">
A Gibbs Sampler for Phrasal Synchronous Grammar Induction
</title>
<author confidence="0.875129">
Phil Blunsom*
</author>
<email confidence="0.775617">
pblunsom@inf.ed.ac.uk
</email>
<author confidence="0.583644">
Chris Dyer†
</author>
<affiliation confidence="0.801271666666667">
redpony@umd.edu
*Department of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.842576">
Edinburgh, EH8 9AB, UK
</address>
<sectionHeader confidence="0.948212" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983055555556">
We present a phrasal synchronous gram-
mar model of translational equivalence.
Unlike previous approaches, we do not
resort to heuristics or constraints from
a word-alignment model, but instead
directly induce a synchronous grammar
from parallel sentence-aligned corpora.
We use a hierarchical Bayesian prior
to bias towards compact grammars with
small translation units. Inference is per-
formed using a novel Gibbs sampler
over synchronous derivations. This sam-
pler side-steps the intractability issues of
previous models which required inference
over derivation forests. Instead each sam-
pling iteration is highly efficient, allowing
the model to be applied to larger transla-
tion corpora than previous approaches.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999597333333333">
The field of machine translation has seen many
advances in recent years, most notably the shift
from word-based (Brown et al., 1993) to phrase-
based models which use token n-grams as trans-
lation units (Koehn et al., 2003). Although very
few researchers use word-based models for trans-
lation per se, such models are still widely used in
the training of phrase-based models. These word-
based models are used to find the latent word-
alignments between bilingual sentence pairs, from
which a weighted string transducer can be induced
(either finite state (Koehn et al., 2003) or syn-
chronous context free grammar (Chiang, 2007)).
Although wide-spread, the disconnect between the
translation model and the alignment model is arti-
ficial and clearly undesirable. Word-based mod-
els are incapable of learning translational equiv-
alences between non-compositional phrasal units,
while the algorithms used for inducing weighted
transducers from word-alignments are based on
heuristics with little theoretical justification. A
</bodyText>
<figure confidence="0.33787075">
Trevor Cohn*
tcohn@inf.ed.ac.uk
Miles Osborne*
miles@inf.ed.ac.uk
</figure>
<affiliation confidence="0.95136">
†Department of Linguistics
University of Maryland
</affiliation>
<address confidence="0.606591">
College Park, MD 20742, USA
</address>
<bodyText confidence="0.999761536585366">
model which can fulfil both roles would address
both the practical and theoretical short-comings of
the machine translation pipeline.
The machine translation literature is littered
with various attempts to learn a phrase-based
string transducer directly from aligned sentence
pairs, doing away with the separate word align-
ment step (Marcu and Wong, 2002; Cherry and
Lin, 2007; Zhang et al., 2008b; Blunsom et al.,
2008). Unfortunately none of these approaches
resulted in an unqualified success, due largely
to intractable estimation. Large training sets with
hundreds of thousands of sentence pairs are com-
mon in machine translation, leading to a parameter
space of billions or even trillions of possible bilin-
gual phrase-pairs. Moreover, the inference proce-
dure for each sentence pair is non-trivial, prov-
ing NP-complete for learning phrase based models
(DeNero and Klein, 2008) or a high order poly-
nomial (O(|f|3|e|3))1 for a sub-class of weighted
synchronous context free grammars (Wu, 1997).
Consequently, for such models both the param-
eterisation and approximate inference techniques
are fundamental to their success.
In this paper we present a novel SCFG transla-
tion model using a non-parametric Bayesian for-
mulation. The model includes priors to impose a
bias towards small grammars with few rules, each
of which is as simple as possible (e.g., terminal
productions consisting of short phrase pairs). This
explicitly avoids the degenerate solutions of max-
imum likelihood estimation (DeNero et al., 2006),
without resort to the heuristic estimator of Koehn
et al. (2003). We develop a novel Gibbs sampler
to perform inference over the latent synchronous
derivation trees for our training instances. The
sampler reasons over the infinite space of possi-
ble translation units without recourse to arbitrary
restrictions (e.g., constraints drawn from a word-
alignment (Cherry and Lin, 2007; Zhang et al.,
2008b) or a grammar fixed a priori (Blunsom et al.,
</bodyText>
<footnote confidence="0.779033">
1f and a are the input and output sentences respectively.
</footnote>
<page confidence="0.875424">
782
</page>
<note confidence="0.9996225">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782–790,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999844117647059">
2008)). The sampler performs local edit operations
to nodes in the synchronous trees, each of which
is very fast, leading to a highly efficient inference
technique. This allows us to train the model on
large corpora without resort to punitive length lim-
its, unlike previous approaches which were only
applied to small data sets with short sentences.
This paper is structured as follows: In Sec-
tion 3 we argue for the use of efficient sam-
pling techniques over SCFGs as an effective solu-
tion to the modelling and scaling problems of
previous approaches. We describe our Bayesian
SCFG model in Section 4 and a Gibbs sampler
to explore its posterior. We apply this sampler
to build phrase-based and hierarchical translation
models and evaluate their performance on small
and large corpora.
</bodyText>
<sectionHeader confidence="0.87217" genericHeader="method">
2 Synchronous context free grammar
</sectionHeader>
<bodyText confidence="0.99997747826087">
A synchronous context free grammar (SCFG,
(Lewis II and Stearns, 1968)) generalizes context-
free grammars to generate strings concurrently in
two (or more) languages. A string pair is gener-
ated by applying a series of paired rewrite rules
of the form, X → he, f, ai, where X is a non-
terminal, e and f are strings of terminals and non-
terminals and a specifies a one-to-one alignment
between non-terminals in e and f. In the context of
SMT, by assigning the source and target languages
to the respective sides of a probabilistic SCFG it
is possible to describe translation as the process
of parsing the source sentence, which induces a
parallel tree structure and translation in the tar-
get language (Chiang, 2007). Figure 1 shows an
example derivation for Japanese to English trans-
lation using an SCFG. For efficiency reasons we
only consider binary or ternary branching rules
and don’t allow rules to mix terminals and non-
terminals. This allows our sampler to more effi-
ciently explore the space of grammars (Section
4.2), however more expressive grammars would be
a straightforward extension of our model.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.9986316">
Most machine translation systems adopt the
approach of Koehn et al. (2003) for ‘training’
a phrase-based translation model.2 This method
starts with a word-alignment, usually the latent
state of an unsupervised word-based aligner such
</bodyText>
<footnote confidence="0.708499333333333">
2We include grammar based transducers, such as Chiang
(2007) and Marcu et al. (2006), in our definition of phrase-
based models.
</footnote>
<equation confidence="0.9645345">
Grammar fragment:
hX
hJohn-ga, Johni
hringo-o, an applei
htabeta, atei
Sample derivation:
hS1,S1i ⇒ hX2, X2i
⇒ hX3 X4 X5, X3 X5 X4i
</equation>
<bodyText confidence="0.734348666666667">
⇒ hJohn-ga X4 X5, John X5 X4i
⇒ hJohn-ga ringo-o X5, John X5 an applei
⇒ hJohn-ga ringo-o tabeta, John ate an applei
</bodyText>
<figureCaption confidence="0.964112">
Figure 1: A fragment of an SCFG with a ternary
non-terminal expansion and three terminal rules.
</figureCaption>
<bodyText confidence="0.999533129032258">
as GIZA++. Various heuristics are used to com-
bine source-to-target and target-to-source align-
ments, after which a further heuristic is used to
read off phrase pairs which are ‘consistent’ with
the alignment. Although efficient, the sheer num-
ber of somewhat arbitrary heuristics makes this
approach overly complicated.
A number of authors have proposed alterna-
tive techniques for directly inducing phrase-based
translation models from sentence aligned data.
Marcu and Wong (2002) proposed a phrase-based
alignment model which suffered from a massive
parameter space and intractable inference using
expectation maximisation. Taking a different tack,
DeNero et al. (2008) presented an interesting new
model with inference courtesy of a Gibbs sampler,
which was better able to explore the full space of
phrase translations. However, the efficacy of this
model is unclear due to the small-scale experi-
ments and the short sampling runs. In this work we
also propose a Gibbs sampler but apply it to the
polynomial space of derivation trees, rather than
the exponential space of the DeNero et al. (2008)
model. The restrictions imposed by our tree struc-
ture make sampling considerably more efficient
for long sentences.
Following the broad shift in the field from finite
state transducers to grammar transducers (Chiang,
2007), recent approaches to phrase-based align-
ment have used synchronous grammar formalisms
permitting polynomial time inference (Wu, 1997;
</bodyText>
<equation confidence="0.995115375">
X →
X →
X →
X →
1 X
2 X3, X
1 X3 X
2i
</equation>
<page confidence="0.991496">
783
</page>
<bodyText confidence="0.98507975">
Cherry and Lin, 2007; Zhang et al., 2008b; Blun-
som et al., 2008). However this asymptotic time
complexity is of high enough order (O(|f|3|e|3))
that inference is impractical for real translation
data. Proposed solutions to this problem include
imposing sentence length limits, using small train-
ing corpora and constraining the search space
using a word-alignment model or parse tree. None
of these limitations are particularly desirable as
they bias inference. As a result phrase-based align-
ment models are not yet practical for the wider
machine translation community.
</bodyText>
<sectionHeader confidence="0.990171" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.999844454545455">
Our aim is to induce a grammar from a train-
ing set of sentence pairs. We use Bayes’ rule
to reason under the posterior over grammars,
P(g|x) a P(x|g)P(g), where g is a weighted
SCFG grammar and x is our training corpus. The
likelihood term, P(x|g), is the probability of the
training sentence pairs under the grammar, while
the prior term, P(g), describes our initial expec-
tations about what consitutes a plausible gram-
mar. Specifically we incorporate priors encoding
our preference for a briefer and more succinct
grammar, namely that: (a) the grammar should be
small, with few rules rewriting each non-terminal;
and (b) terminal rules which specify phrasal trans-
lation correspondence should be small, with few
symbols on their right hand side.
Further, Bayesian non-parametrics allow the
capacity of the model to grow with the data.
Thereby we avoid imposing hard limits on the
grammar (and the thorny problem of model selec-
tion), but instead allow the model to find a gram-
mar appropriately sized for its training data.
</bodyText>
<subsectionHeader confidence="0.955655">
4.1 Non-parametric form
</subsectionHeader>
<bodyText confidence="0.9999965">
Our Bayesian model of SCFG derivations resem-
bles that of Blunsom et al. (2008). Given a gram-
mar, each sentence is generated as follows. Start-
ing with a root non-terminal (z1), rewrite each
frontier non-terminal (zi) using a rule chosen from
our grammar expanding zi. Repeat until there are
no remaining frontier non-terminals. This gives
rise to the following derivation probability:
</bodyText>
<equation confidence="0.917972">
p(d) = p(z1) � p(ri|zi)
ri∈d
</equation>
<bodyText confidence="0.999873111111111">
where the derivation is a sequence of rules d =
(r1, ... , rn), and zi denotes the root node of ri.
We allow two types of rules: non-terminal and
terminal expansions. The former rewrites a non-
terminal symbol as a string of two or three non-
terminals along with an alignment, specifying
the corresponding ordering of the child trees in
the source and target language. Terminal expan-
sions rewrite a non-terminal as a pair of terminal
n-grams, representing a phrasal translation pair,
where either but not both may be empty.
Each rule in the grammar, ri, is generated from
its root symbol, zi, by first choosing a rule type
ti E {TERM, NON-TERM} from a Bernoulli distribu-
tion, ri — Bernoulli(-y). We treat -y as a random
variable with its own prior, -y — Beta(aR, aR) and
integrate out the parameters, -y. This results in the
following conditional probability for ti:
</bodyText>
<equation confidence="0.99825725">
−i R
p(ti |r−i, zi, aR) = nti,zi + a
n−i
·,zi + 2aR
</equation>
<bodyText confidence="0.997779">
where n−i
ri,zi is the number of times ri has been
used to rewrite zi in the set of all other rules, r−i,
and n−i r,zi is the total count of rewriting
</bodyText>
<equation confidence="0.787664">
·,zi = �r n−i
</equation>
<bodyText confidence="0.9996655">
zi. The Dirichlet (and thus Beta) distribution are
exchangeable, meaning that any permutation of its
events are equiprobable. This allows us to reason
about each event given previous and subsequent
events (i.e., treat each item as the ‘last’.)
When ti = NON-TERM, we generate a binary
or ternary non-terminal production. The non-
terminal sequence and alignment are drawn from
(z, a) — ON zi and, as before, we define a prior over
the parameters, ON zi — Dirichlet(aT), and inte-
grate out (Nzi. This results in the conditional prob-
ability:
</bodyText>
<equation confidence="0.982828333333333">
nN&apos;−i N
p(ri  |ti = NON-TERM, r−i, zi, aN) = N z,zi + a N
n·,zi−i+  |N |a
</equation>
<bodyText confidence="0.951632923076923">
where nN,−i
ri,zi is the count of rewriting zi with non-
terminal rule ri, nN,−i
·,zi the total count over all non-
terminal rules and |N |is the number of unique
non-terminal rules.
For terminal productions (ti = TERM) we first
decide whether to generate a phrase in both lan-
guages or in one language only, according to a
fixed probability pnull.3 Contingent on this deci-
sion, the terminal strings are then drawn from
3To discourage null alignments, we used Pnuu = 10−10
for this value in the experiments we report below.
</bodyText>
<page confidence="0.991209">
784
</page>
<bodyText confidence="0.999837333333333">
either φPzi for phrase pairs or φnull for single lan-
guage phrases. We choose Dirichlet process (DP)
priors for these parameters:
</bodyText>
<equation confidence="0.9984725">
φPzi — DP(αP, P1P)
φnull — DP(αnull P1null)
</equation>
<bodyText confidence="0.989751176470589">
zi
where the base distributions, P1P and Pnull
1 , range
over phrase pairs or monolingual phrases in either
language, respectively.
The most important choice for our model is
the priors on the parameters of these terminal
distributions. Phrasal SCFG models are subject
to a degenerate maximum likelihood solution in
which all probability mass is placed on long, or
whole sentence, phrase translations (DeNero et al.,
2006). Therefore, careful consideration must be
given when specifying the P1 distribution on ter-
minals in order to counter this behavior.
To construct a prior over string pairs, first we
define the probability of a monolingual string (s):
P0X (s) = PPoisson(|s|; 1) X
where the PPoisson(k; 1) is the probability under a
Poisson distribution of length k given an expected
length of 1, while VX is the vocabulary size of
language X. This distribution has a strong bias
towards short strings. In particular note that gener-
ally a string of length k will be less probable than
two of length k2, a property very useful for finding
‘minimal’ translation units. This contrasts with a
geometric distribution in which a string of length
k will be more probable than its segmentations.
We define Pnull
1 as the string probability of the
non-null part of the rule:
The terminal translation phrase pair distribution
is a hierarchical Dirichlet Process in which each
phrase are independently distributed according to
DPs:4
</bodyText>
<equation confidence="0.9725575">
P1P (z —* (e,f)) = φEz(e) X φFz (f)
φEz — DP(αPE, P0E)
</equation>
<footnote confidence="0.753921714285714">
4This prior is similar to one used by DeNero et al. (2008),
who used the expected table count approximation presented
in Goldwater et al. (2006). However, Goldwater et al. (2006)
contains two major errors: omitting Po, and using the trun-
cated Taylor series expansion (Antoniak, 1974) which fails
for small αP0 values common in these models. In this work
we track table counts directly.
</footnote>
<bodyText confidence="0.999315">
and φFz is defined analogously. This prior encour-
ages frequent phrases to participate in many differ-
ent translation pairs. Moreover, as longer strings
are likely to be less frequent in the corpus this has
a tendency to discourage long translation units.
</bodyText>
<subsectionHeader confidence="0.986648">
4.2 A Gibbs sampler for derivations
</subsectionHeader>
<bodyText confidence="0.999980976190477">
Markov chain Monte Carlo sampling allows us to
perform inference for the model described in 4.1
without restricting the infinite space of possible
translation rules. To do this we need a method for
sampling a derivation for a given sentence pair
from p(d|d−). One possible approach would be
to first build a packed chart representation of the
derivation forest, calculate the inside probabilities
of all cells in this chart, and then sample deriva-
tions top-down according to their inside probabil-
ities (analogous to monolingual parse tree sam-
pling described in Johnson et al. (2007)). A prob-
lem with this approach is that building the deriva-
tion forest would take O(|f|3|e|3) time, which
would be impractical for long sentences.
Instead we develop a collapsed Gibbs sam-
pler (Teh et al., 2006) which draws new sam-
ples by making local changes to the derivations
used in a previous sample. After a period of burn
in, the derivations produced by the sampler will
be drawn from the posterior distribution, p(d|x).
The advantage of this algorithm is that we only
store the current derivation for each training sen-
tence pair (together these constitute the state of
the sampler), but never need to reason over deriva-
tion forests. By integrating over (collapsing) the
parameters we only store counts of rules used
in the current sampled set of derivations, thereby
avoiding explicitly representing the possibly infi-
nite space of translation pairs.
We define two operators for our Gibbs sam-
pler, each of which re-samples local derivation
structures. Figures 2 and 4 illustrate the permu-
tations these operators make to derivation trees.
The omitted tree structure in these figures denotes
the Markov blanket of the operator: the structure
which is held constant when enumerating the pos-
sible outcomes for an operator.
The Split/Join operator iterates through the
positions between each source word sampling
whether a terminal boundary should exist at
that position (Figure 2). If the source position
</bodyText>
<equation confidence="0.688799923076923">
Pnull (z (e, f)) = i
1
1
F
|
|
P(e) if
f
= 0
{ 2P0 (f) if |e |= 0
1
V |�|
X
</equation>
<page confidence="0.738812">
785
</page>
<figure confidence="0.97636">
... ... ... ... ...
... ... ... ...
</figure>
<figureCaption confidence="0.99729075">
Figure 2: Split/Join sampler applied between a pair of adjacent terminals sharing the same parent. The
dashed line indicates the source position being sampled, boxes indicate source and target tokens, while a
solid line is a null alignment.
Figure 4: Rule insert/delete sampler. A pair of
</figureCaption>
<bodyText confidence="0.996876060606061">
adjacent nodes in a ternary rule can be re-parented
as a binary rule, or vice-versa.
falls between two existing terminals whose tar-
get phrases are adjacent, then any new target seg-
mentation within those target phrases can be sam-
pled, including null alignments. If the two exist-
ing terminals also share the same parent, then any
possible re-ordering is also a valid outcome, as
is removing the terminal boundary to form a sin-
gle phrase pair. Otherwise, if the visited boundary
point falls within an existing terminal, then all tar-
get split and re-orderings are possible outcomes.
The probability for each of these configurations
is evaluated (see Figure 3) from which the new
configuration is sampled.
While the first operator is theoretically capa-
ble of exploring the entire derivation forest (by
flattening the tree into a single phrase and then
splitting), the series of moves required would be
highly improbable. To allow for faster mixing we
employ the Insert/Delete operator which adds and
deletes the parent non-terminal of a pair of adja-
cent nodes. This is illustrated in Figure 4. The
update equations are analogous to those used for
the Split/Join operator in Figure 3. In order for this
operator to be effective we need to allow greater
than binary branching nodes, otherwise deleting a
nodes would require sampling from a much larger
set of outcomes. Hence our adoption of a ternary
branching grammar. Although such a grammar
would be very inefficient for a dynamic program-
ming algorithm, it allows our sampler to permute
the internal structure of the trees more easily.
</bodyText>
<subsectionHeader confidence="0.995683">
4.3 Hyperparameter Inference
</subsectionHeader>
<bodyText confidence="0.999118363636364">
Our model is parameterised by a vector of hyper-
parameters, α =
which control the sparsity assumption over var-
ious model parameters. We could optimise each
concentration parameter on the training corpus by
hand, however this would be quite an onerous task.
Instead we perform inference over the hyperpa-
rameters following Goldwater and Griffiths (2007)
by defining a vague gamma prior on each con-
centration parameter, αx — Gamma(10−4,104).
This hyper-prior is relatively benign, allowing the
model to consider a wide range of values for
the hyperparameter. We sample a new value for
each αx using a log-normal distribution with mean
αx and variance 0.3, which is then accepted into
the distribution p(αx|d, α−) using the Metropolis-
Hastings algorithm. Unlike the Gibbs updates, this
calculation cannot be distributed over a cluster
(see Section 4.4) and thus is very costly. Therefore
for small corpora we re-sample the hyperparame-
ter after every pass through the corpus, for larger
experiments we only re-sample every 20 passes.
</bodyText>
<subsectionHeader confidence="0.980739">
4.4 A Distributed approximation
</subsectionHeader>
<bodyText confidence="0.979868">
While employing a collapsed Gibbs sampler
allows us to efficiently perform inference over the
</bodyText>
<figure confidence="0.9993741">
...
...
...
...
...
...
...
...
(R N P PE PF null
a ,α ,α ,α ,a ,a ),
</figure>
<page confidence="0.889382">
786
</page>
<equation confidence="0.84965425">
p(JOIN) a p(ti = TERM|zi, r−) X p(ri = (zi — (e, f))|zi, r−) (1)
p(SPLIT) a p(ti = NON-TERM|zi, r−) X p(ri = (zi — (zl, zr, ai))|zi, r−) (2)
X p(tl = TERM|ti, zi, r−) X p(rl = (zl — (el, fl))|zl, r−)
X p(tr = TERM|ti, tl, zi, r−) X p(rr = (zr — (er, fr))|zl, r− U (zl — (el, fl)))
</equation>
<figureCaption confidence="0.969448333333333">
Figure 3: Gibbs sampling equations for the competing configurations of the Split/Join sampler, shown in
Figure 2. Eq. (1) corresponds to the top-left configuration, and (2) the remaining configurations where the
choice of el, fl, er, fr and ai specifies the string segmentation and the alignment (monotone or reordered).
</figureCaption>
<bodyText confidence="0.958580388888889">
massive space of possible grammars, it induces
dependencies between all the sentences in the
training corpus. These dependencies make it diffi-
cult to scale our approach to larger corpora by dis-
tributing it across a number of processors. Recent
work (Newman et al., 2007; Asuncion et al., 2008)
suggests that good practical parallel performance
can be achieved by having multiple processors
independently sample disjoint subsets of the cor-
pus. Each process maintains a set of rule counts for
the entire corpus and communicates the changes
it has made to its section of the corpus only
after sampling every sentence in that section. In
this way each process is sampling according to
a slightly ‘out-of-date’ distribution. However, as
we confirm in Section 5 the performance of this
approximation closely follows the exact collapsed
Gibbs sampler.
</bodyText>
<subsectionHeader confidence="0.998401">
4.5 Extracting a translation model
</subsectionHeader>
<bodyText confidence="0.999978076923077">
Although we could use our model directly as a
decoder to perform translation, its simple hier-
archical reordering parameterisation is too weak
to be effective in this mode. Instead we use our
sampler to sample a distribution over translation
models for state-of-the-art phrase based (Moses)
and hierarchical (Hiero) decoders (Koehn et al.,
2007; Chiang, 2007). Each sample from our model
defines a hierarchical alignment on which we can
apply the standard extraction heuristics of these
models. By extracting from a sequence of samples
we can directly infer a distribution over phrase
tables or Hiero grammars.
</bodyText>
<sectionHeader confidence="0.998589" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999987291666667">
Our evaluation aims to determine whether the
phrase/SCFG rule distributions created by sam-
pling from the model described in Section 4
impact upon the performance of state-of-the-
art translation systems. We conduct experiments
translating both Chinese (high reordering) and
Arabic (low reordering) into English. We use the
GIZA++ implementation of IBM Model 4 (Brown
et al., 1993; Och and Ney, 2003) coupled with the
phrase extraction heuristics of Koehn et al. (2003)
and the SCFG rule extraction heuristics of Chiang
(2007) as our benchmark. All the SCFG models
employ a single X non-terminal, we leave experi-
ments with multiple non-terminals to future work.
Our hypothesis is that our grammar based
induction of translation units should benefit lan-
guage pairs with significant reordering more than
those with less. While for mostly monotone trans-
lation pairs, such as Arabic-English, the bench-
mark GIZA++-based system is well suited due to
its strong monotone bias (the sequential Markov
model and diagonal growing heuristic).
We conduct experiments on both small and
large corpora to allow a range of alignment quali-
ties and also to verify the effectiveness of our dis-
tributed approximation of the Bayesian inference.
The samplers are initialised with trees created
from GIZA++ Model 4 alignments, altered such
that they are consistent with our ternary grammar.
This is achieved by using the factorisation algo-
rithm of Zhang et al. (2008a) to first create ini-
tial trees. Where these factored trees contain nodes
with mixed terminals and non-terminals, or more
than three non-terminals, we discard alignment
points until the node factorises correctly. As the
alignments contain many such non-factorisable
nodes, these trees are of poor quality. However,
all samplers used in these experiments are first
‘burnt-in’ for 1000 full passes through the data.
This allows the sampler to diverge from its ini-
tialisation condition, and thus gives us confidence
that subsequent samples will be drawn from the
posterior. An expectation over phrase tables and
Hiero grammars is built from every 50th sample
after the burn-in, up until the 1500th sample.
We evaluate the translation models using IBM
BLEU (Papineni et al., 2001). Table 1 lists the
statistics of the corpora used in these experiments.
</bodyText>
<page confidence="0.99125">
787
</page>
<table confidence="0.9992685">
IWSLT NIST
English+—Chinese English+—Chinese English+—Arabic
Sentences 40k 300k 290k
Segs./Words 380k 340k 11.0M 8.6M 9.3M 8.5M
Av. Sent. Len. 9 8 36 28 32 29
Longest Sent. 75 64 80 80 80 80
</table>
<tableCaption confidence="0.995111">
Table 1: Corpora statistics.
</tableCaption>
<table confidence="0.999903">
System Test 05
Moses (Heuristic) 47.3
Moses (Bayes SCFG) 49.6
Hiero (Heuristic) 48.3
Hiero (Bayes SCFG) 51.8
</table>
<tableCaption confidence="0.997461">
Table 2: IWSLT Chinese to English translation.
</tableCaption>
<subsectionHeader confidence="0.996026">
5.1 Small corpus
</subsectionHeader>
<bodyText confidence="0.99999325">
Firstly we evaluate models trained on a small
Chinese-English corpus using a Gibbs sampler on
a single CPU. This corpus consists of transcribed
utterances made available for the IWSLT work-
shop (Eck and Hori, 2005). The sparse counts and
high reordering for this corpus means the GIZA++
model produces very poor alignments.
Table 2 shows the results for the benchmark
Moses and Hiero systems on this corpus using
both the heuristic phrase estimation, and our pro-
posed Bayesian SCFG model. We can see that
our model has a slight advantage. When we look
at the grammars extracted by the two models we
note that the SCFG model creates considerably
more translation rules. Normally this would sug-
gest the alignments of the SCFG model are a lot
sparser (more unaligned tokens) than those of the
heuristic, however this is not the case. The pro-
jected SCFG derivations actually produce more
alignment points. However these alignments are
much more locally consistent, containing fewer
spurious off-diagonal alignments, than the heuris-
tic (see Figure 5), and thus produce far more valid
phrases/rules.
</bodyText>
<subsectionHeader confidence="0.999621">
5.2 Larger corpora
</subsectionHeader>
<bodyText confidence="0.999973125">
We now test our model’s performance on a larger
corpus, representing a realistic SMT experiment
with millions of words and long sentences. The
Chinese-English training data consists of the FBIS
corpus (LDC2003E14) and the first 100k sen-
tences from the Sinorama corpus (LDC2005E47).
The Arabic-English training data consists of
the eTIRR corpus (LDC2004E72), the Arabic
</bodyText>
<figure confidence="0.524459">
Number of Sampling Passes
</figure>
<figureCaption confidence="0.894938666666667">
Figure 6: The posterior for the single CPU sampler
and distributed approximation are roughly equiva-
lent over a sampling run.
</figureCaption>
<bodyText confidence="0.99973864">
news corpus (LDC2004T17), the Ummah cor-
pus (LDC2004T18), and the sentences with confi-
dence c &gt; 0.995 in the ISI automatically extracted
web parallel corpus (LDC2006T02). The Chinese
text was segmented with a CRF-based Chinese
segmenter optimized for MT (Chang et al., 2008).
The Arabic text was preprocessed according to the
D2 scheme of Habash and Sadat (2006), which
was identified as optimal for corpora this size. The
parameters of the NIST systems were tuned using
Och’s algorithm to maximize BLEU on the MT02
test set (Och, 2003).
To evaluate whether the approximate distributed
inference algorithm described in Section 4.4 is
effective, we compare the posterior probability of
the training corpus when using a single machine,
and when the inference is distributed on an eight
core machine. Figure 6 plots the mean posterior
and standard error for five independent runs for
each scenario. Both sets of runs performed hyper-
parameter inference every twenty passes through
the data. It is clear from the training curves that the
distributed approximation tracks the corpus prob-
ability of the correct sampler sufficiently closely.
This concurs with the findings of Newman et al.
</bodyText>
<figure confidence="0.994644615384615">
20 40 60 80 100 120 140 160 180 200 220 240
Negative Log−Posterior
476 478 480 482 484 486 488 490
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
single (exact)
distributed
788
balance balance
of of
rights rights
and and
obligations obligations
an an
important important
wto wto
characteristic characteristic
(a) Giza++ (b) Gibbs
</figure>
<figureCaption confidence="0.996870666666667">
Figure 5: Alignment example. The synchronous tree structure is shown for (b) using brackets to indicate
constituent spans; these are omitted for single token constituents. The right alignment is roughly correct,
except that ‘of’ and ‘an’ should be left unaligned (J9 ‘to be’ is missing from the English translation).
</figureCaption>
<table confidence="0.9997808">
System MT03 MT04 MT05
Moses (Heuristic) 26.2 30.0 25.3
Moses (Bayes SCFG) 26.4 30.2 25.8
Hiero (Heuristic) 26.4 30.8 25.4
Hiero (Bayes SCFG) 26.7 30.9 26.0
</table>
<tableCaption confidence="0.978274">
Table 3: NIST Chinese to English translation.
</tableCaption>
<table confidence="0.9998954">
System MT03 MT04 MT05
Moses (Heuristic) 48.5 43.9 49.2
Moses (Bayes SCFG) 48.5 43.5 48.7
Hiero (Heuristic) 48.1 43.5 48.4
Hiero (Bayes SCFG) 48.4 43.4 47.7
</table>
<tableCaption confidence="0.999328">
Table 4: NIST Arabic to English translation.
</tableCaption>
<bodyText confidence="0.99991152631579">
(2007) who also observed very little empirical dif-
ference between the sampler and its distributed
approximation.
Tables 3 and 4 show the result on the two NIST
corpora when running the distributed sampler on
a single 8-core machine.5 These scores tally with
our initial hypothesis: that the hierarchical struc-
ture of our model suits languages that exhibit less
monotone reordering.
Figure 5 shows the projected alignment of a
headline from the thousandth sample on the NIST
Chinese data set. The effect of the grammar based
alignment can clearly be seen. Where the combi-
nation of GIZA++ and the heuristics creates out-
lier alignments that impede rule extraction, the
SCFG imposes a more rigid hierarchical struc-
ture on the alignments. We hypothesise that this
property may be particularly useful for syntac-
tic translation models which often have difficulty
</bodyText>
<footnote confidence="0.786908">
5Producing the 1.5K samples for each experiment took
approximately one day.
</footnote>
<bodyText confidence="0.999373111111111">
with inconsistent word alignments not correspond-
ing to syntactic structure.
The combined evidence of the ability of our
Gibbs sampler to improve posterior likelihood
(Figure 6) and our translation experiments demon-
strate that we have developed a scalable and effec-
tive method for performing inference over phrasal
SCFG, without compromising the strong theoreti-
cal underpinnings of our model.
</bodyText>
<sectionHeader confidence="0.997455" genericHeader="conclusions">
6 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999990391304348">
We have presented a Bayesian model of SCFG
induction capable of capturing phrasal units of
translational equivalence. Our novel Gibbs sam-
pler over synchronous derivation trees can effi-
ciently draw samples from the posterior, overcom-
ing the limitations of previous models when deal-
ing with long sentences. This avoids explicitly
representing the full derivation forest required by
dynamic programming approaches, and thus we
are able to perform inference without resorting to
heuristic restrictions on the model.
Initial experiments suggest that this model per-
forms well on languages for which the monotone
bias of existing alignment and heuristic phrase
extraction approaches fail. These results open the
way for the development of more sophisticated
models employing grammars capable of capturing
a wide range of translation phenomena. In future
we envision it will be possible to use the tech-
niques developed here to directly induce gram-
mars which match state-of-the-art decoders, such
as Hiero grammars or tree substitution grammars
of the form used by Galley et al. (2004).
</bodyText>
<page confidence="0.997514">
789
</page>
<sectionHeader confidence="0.996491" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9910865">
The authors acknowledge the support of
the EPSRC (Blunsom &amp; Osborne, grant
EP/D074959/1; Cohn, grant GR/T04557/01)
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-2-001 (Dyer).
</bodyText>
<sectionHeader confidence="0.99761" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999899379310344">
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152–1174.
A. Asuncion, P. Smyth, M. Welling. 2008. Asynchronous
distributed learning of topic models. In NIPS. MIT Press.
P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian syn-
chronous grammar induction. In Proceedings of NIPS 21,
Vancouver, Canada.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, R. L. Mercer.
1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics,
19(2):263–311.
P.-C. Chang, D. Jurafsky, C. D. Manning. 2008. Optimizing
Chinese word segmentation for machine translation per-
formance. In Proc. of the Third Workshop on Machine
Translation, Prague, Czech Republic.
C. Cherry, D. Lin. 2007. Inversion transduction grammar for
joint phrasal translation modeling. In Proc. of the HLT-
NAACL Workshop on Syntax and Structure in Statistical
Translation (SSST 2007), Rochester, USA.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
J. DeNero, D. Klein. 2008. The complexity of phrase align-
ment problems. In Proceedings of ACL-08: HLT, Short
Papers, 25–28, Columbus, Ohio. Association for Compu-
tational Linguistics.
J. DeNero, D. Gillick, J. Zhang, D. Klein. 2006. Why gener-
ative phrase models underperform surface heuristics. In
Proc. of the HLT-NAACL 2006 Workshop on Statistical
Machine Translation, 31–38, New York City.
J. DeNero, A. Bouchard-Cˆot´e, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314–323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
M. Eck, C. Hori. 2005. Overview of the IWSLT 2005 eval-
uation campaign. In Proc. of the International Workshop
on Spoken Language Translation, Pittsburgh.
M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004. What’s
in a translation rule? In Proc. of the 4th International Con-
ference on Human Language Technology Research and
5th Annual Meeting of the NAACL (HLT-NAACL 2004),
Boston, USA.
S. Goldwater, T. Griffiths. 2007. A fully bayesian approach
to unsupervised part-of-speech tagging. In Proc. of the
45th Annual Meeting of the ACL (ACL-2007), 744–751,
Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
N. Habash, F. Sadat. 2006. Arabic preprocessing schemes
for statistical machine translation. In Proc. of the 6th
International Conference on Human Language Technol-
ogy Research and 7th Annual Meeting of the NAACL
(HLT-NAACL 2006), New York City. Association for
Computational Linguistics.
M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo. In
Proc. of the 7th International Conference on Human Lan-
guage Technology Research and 8th Annual Meeting of the
NAACL (HLT-NAACL 2007), 139–146, Rochester, New
York.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of the 3rd International Con-
ference on Human Language Technology Research and
4th Annual Meeting of the NAACL (HLT-NAACL 2003),
81–88, Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens,
C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses:
Open source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL (ACL-2007),
Prague.
P. M. Lewis II, R. E. Stearns. 1968. Syntax-directed trans-
duction. J. ACM, 15(3):465–488.
D. Marcu, W. Wong. 2002. A phrase-based, joint probability
model for statistical machine translation. In Proc. of the
2002 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), 133–139, Philadelphia.
Association for Computational Linguistics.
D. Marcu, W. Wang, A. Echihabi, K. Knight. 2006. SPMT:
Statistical machine translation with syntactified target lan-
guage phrases. In Proc. of the 2006 Conference on Empir-
ical Methods in Natural Language Processing (EMNLP-
2006), 44–52, Sydney, Australia.
D. Newman, A. Asuncion, P. Smyth, M. Welling. 2007.
Distributed inference for latent dirichlet allocation. In
NIPS. MIT Press.
F. J. Och, H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19–52.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of the 41st Annual Meeting
of the ACL (ACL-2003), 160–167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: a
method for automatic evaluation of machine translation,
2001.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566–1581.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377–403.
H. Zhang, D. Gildea, D. Chiang. 2008a. Extracting syn-
chronous grammar rules from word-level alignments in
linear time. In Proc. of the 22th International Con-
ference on Computational Linguistics (COLING-2008),
1081–1088, Manchester, UK.
H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008b.
Bayesian learning of non-compositional phrases with syn-
chronous parsing. In Proc. of the 46th Annual Conference
of the Association for Computational Linguistics: Human
Language Technologies (ACL-08:HLT), 97–105, Colum-
bus, Ohio.
</reference>
<page confidence="0.99718">
790
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.605643">
<title confidence="0.992389">A Gibbs Sampler for Phrasal Synchronous Grammar Induction</title>
<email confidence="0.8090245">pblunsom@inf.ed.ac.ukredpony@umd.edu</email>
<affiliation confidence="0.9949725">of Informatics University of Edinburgh</affiliation>
<address confidence="0.998349">Edinburgh, EH8 9AB, UK</address>
<abstract confidence="0.998638421052632">We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C E Antoniak</author>
</authors>
<title>Mixtures of dirichlet processes with applications to bayesian nonparametric problems.</title>
<date>1974</date>
<journal>The Annals of Statistics,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="14590" citStr="Antoniak, 1974" startWordPosition="2367" endWordPosition="2368">l be more probable than its segmentations. We define Pnull 1 as the string probability of the non-null part of the rule: The terminal translation phrase pair distribution is a hierarchical Dirichlet Process in which each phrase are independently distributed according to DPs:4 P1P (z —* (e,f)) = φEz(e) X φFz (f) φEz — DP(αPE, P0E) 4This prior is similar to one used by DeNero et al. (2008), who used the expected table count approximation presented in Goldwater et al. (2006). However, Goldwater et al. (2006) contains two major errors: omitting Po, and using the truncated Taylor series expansion (Antoniak, 1974) which fails for small αP0 values common in these models. In this work we track table counts directly. and φFz is defined analogously. This prior encourages frequent phrases to participate in many different translation pairs. Moreover, as longer strings are likely to be less frequent in the corpus this has a tendency to discourage long translation units. 4.2 A Gibbs sampler for derivations Markov chain Monte Carlo sampling allows us to perform inference for the model described in 4.1 without restricting the infinite space of possible translation rules. To do this we need a method for sampling </context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>C. E. Antoniak. 1974. Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The Annals of Statistics, 2(6):1152–1174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Asuncion</author>
<author>P Smyth</author>
<author>M Welling</author>
</authors>
<title>Asynchronous distributed learning of topic models. In NIPS.</title>
<date>2008</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="21110" citStr="Asuncion et al., 2008" startWordPosition="3464" endWordPosition="3467">, fl))) Figure 3: Gibbs sampling equations for the competing configurations of the Split/Join sampler, shown in Figure 2. Eq. (1) corresponds to the top-left configuration, and (2) the remaining configurations where the choice of el, fl, er, fr and ai specifies the string segmentation and the alignment (monotone or reordered). massive space of possible grammars, it induces dependencies between all the sentences in the training corpus. These dependencies make it difficult to scale our approach to larger corpora by distributing it across a number of processors. Recent work (Newman et al., 2007; Asuncion et al., 2008) suggests that good practical parallel performance can be achieved by having multiple processors independently sample disjoint subsets of the corpus. Each process maintains a set of rule counts for the entire corpus and communicates the changes it has made to its section of the corpus only after sampling every sentence in that section. In this way each process is sampling according to a slightly ‘out-of-date’ distribution. However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler. 4.5 Extracting a translation model Although we c</context>
</contexts>
<marker>Asuncion, Smyth, Welling, 2008</marker>
<rawString>A. Asuncion, P. Smyth, M. Welling. 2008. Asynchronous distributed learning of topic models. In NIPS. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS 21,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2505" citStr="Blunsom et al., 2008" startWordPosition="359" endWordPosition="362">ed on heuristics with little theoretical justification. A Trevor Cohn* tcohn@inf.ed.ac.uk Miles Osborne* miles@inf.ed.ac.uk †Department of Linguistics University of Maryland College Park, MD 20742, USA model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f|3|e|3))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such</context>
<context position="8467" citStr="Blunsom et al., 2008" startWordPosition="1324" endWordPosition="1328"> work we also propose a Gibbs sampler but apply it to the polynomial space of derivation trees, rather than the exponential space of the DeNero et al. (2008) model. The restrictions imposed by our tree structure make sampling considerably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; X → X → X → X → 1 X 2 X3, X 1 X3 X 2i 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f|3|e|3)) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine translation community. 4 Model Our aim is to induce a grammar from a training set of sentence pairs. We use Bayes’ ru</context>
<context position="10105" citStr="Blunsom et al. (2008)" startWordPosition="1588" endWordPosition="1591">re succinct grammar, namely that: (a) the grammar should be small, with few rules rewriting each non-terminal; and (b) terminal rules which specify phrasal translation correspondence should be small, with few symbols on their right hand side. Further, Bayesian non-parametrics allow the capacity of the model to grow with the data. Thereby we avoid imposing hard limits on the grammar (and the thorny problem of model selection), but instead allow the model to find a grammar appropriately sized for its training data. 4.1 Non-parametric form Our Bayesian model of SCFG derivations resembles that of Blunsom et al. (2008). Given a grammar, each sentence is generated as follows. Starting with a root non-terminal (z1), rewrite each frontier non-terminal (zi) using a rule chosen from our grammar expanding zi. Repeat until there are no remaining frontier non-terminals. This gives rise to the following derivation probability: p(d) = p(z1) � p(ri|zi) ri∈d where the derivation is a sequence of rules d = (r1, ... , rn), and zi denotes the root node of ri. We allow two types of rules: non-terminal and terminal expansions. The former rewrites a nonterminal symbol as a string of two or three nonterminals along with an al</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian synchronous grammar induction. In Proceedings of NIPS 21, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1064" citStr="Brown et al., 1993" startWordPosition="147" endWordPosition="150">lel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. 1 Introduction The field of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and c</context>
<context position="22697" citStr="Brown et al., 1993" startWordPosition="3709" endWordPosition="3712">odel defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. 5 Evaluation Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the GIZA++ implementation of IBM Model 4 (Brown et al., 1993; Och and Ney, 2003) coupled with the phrase extraction heuristics of Koehn et al. (2003) and the SCFG rule extraction heuristics of Chiang (2007) as our benchmark. All the SCFG models employ a single X non-terminal, we leave experiments with multiple non-terminals to future work. Our hypothesis is that our grammar based induction of translation units should benefit language pairs with significant reordering more than those with less. While for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequen</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-C Chang</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proc. of the Third Workshop on Machine Translation,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="26911" citStr="Chang et al., 2008" startWordPosition="4380" endWordPosition="4383">onsists of the FBIS corpus (LDC2003E14) and the first 100k sentences from the Sinorama corpus (LDC2005E47). The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize BLEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent ru</context>
</contexts>
<marker>Chang, Jurafsky, Manning, 2008</marker>
<rawString>P.-C. Chang, D. Jurafsky, C. D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proc. of the Third Workshop on Machine Translation, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>D Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In Proc. of the HLTNAACL Workshop on Syntax and Structure in Statistical Translation (SSST 2007),</booktitle>
<location>Rochester, USA.</location>
<contexts>
<context position="2461" citStr="Cherry and Lin, 2007" startWordPosition="351" endWordPosition="354">ed transducers from word-alignments are based on heuristics with little theoretical justification. A Trevor Cohn* tcohn@inf.ed.ac.uk Miles Osborne* miles@inf.ed.ac.uk †Department of Linguistics University of Maryland College Park, MD 20742, USA model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f|3|e|3))1 for a sub-class of weighted synchronous context free</context>
<context position="3976" citStr="Cherry and Lin, 2007" startWordPosition="583" endWordPosition="586">wards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., 1f and a are the input and output sentences respectively. 782 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782–790, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2008)). The sampler performs local edit operations to nodes in the synchronous trees, each of which is very fast, leading to a highly efficient inference technique. This allows us to train the model on large corpora without resort to punitive length limits, unlike previous approaches which were only applied to sma</context>
<context position="8423" citStr="Cherry and Lin, 2007" startWordPosition="1316" endWordPosition="1319">iments and the short sampling runs. In this work we also propose a Gibbs sampler but apply it to the polynomial space of derivation trees, rather than the exponential space of the DeNero et al. (2008) model. The restrictions imposed by our tree structure make sampling considerably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; X → X → X → X → 1 X 2 X3, X 1 X3 X 2i 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f|3|e|3)) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine translation community. 4 Model Our aim is to induce a grammar from a trai</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>C. Cherry, D. Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proc. of the HLTNAACL Workshop on Syntax and Structure in Statistical Translation (SSST 2007), Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1551" citStr="Chiang, 2007" startWordPosition="229" endWordPosition="230">eld of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and clearly undesirable. Word-based models are incapable of learning translational equivalences between non-compositional phrasal units, while the algorithms used for inducing weighted transducers from word-alignments are based on heuristics with little theoretical justification. A Trevor Cohn* tcohn@inf.ed.ac.uk Miles Osborne* miles@inf.ed.ac.uk †Department of Linguistics University of Maryland College Park, MD 20742, USA model which can fulfil both roles would address both the practica</context>
<context position="5792" citStr="Chiang, 2007" startWordPosition="894" endWordPosition="895">rammars to generate strings concurrently in two (or more) languages. A string pair is generated by applying a series of paired rewrite rules of the form, X → he, f, ai, where X is a nonterminal, e and f are strings of terminals and nonterminals and a specifies a one-to-one alignment between non-terminals in e and f. In the context of SMT, by assigning the source and target languages to the respective sides of a probabilistic SCFG it is possible to describe translation as the process of parsing the source sentence, which induces a parallel tree structure and translation in the target language (Chiang, 2007). Figure 1 shows an example derivation for Japanese to English translation using an SCFG. For efficiency reasons we only consider binary or ternary branching rules and don’t allow rules to mix terminals and nonterminals. This allows our sampler to more efficiently explore the space of grammars (Section 4.2), however more expressive grammars would be a straightforward extension of our model. 3 Related work Most machine translation systems adopt the approach of Koehn et al. (2003) for ‘training’ a phrase-based translation model.2 This method starts with a word-alignment, usually the latent state</context>
<context position="8226" citStr="Chiang, 2007" startWordPosition="1279" endWordPosition="1280">ith inference courtesy of a Gibbs sampler, which was better able to explore the full space of phrase translations. However, the efficacy of this model is unclear due to the small-scale experiments and the short sampling runs. In this work we also propose a Gibbs sampler but apply it to the polynomial space of derivation trees, rather than the exponential space of the DeNero et al. (2008) model. The restrictions imposed by our tree structure make sampling considerably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; X → X → X → X → 1 X 2 X3, X 1 X3 X 2i 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f|3|e|3)) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particula</context>
<context position="22055" citStr="Chiang, 2007" startWordPosition="3612" endWordPosition="3613">tion. In this way each process is sampling according to a slightly ‘out-of-date’ distribution. However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. 5 Evaluation Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the GIZA++ impleme</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>25--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2966" citStr="DeNero and Klein, 2008" startWordPosition="428" endWordPosition="431">m aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f|3|e|3))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood e</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>J. DeNero, D. Klein. 2008. The complexity of phrase alignment problems. In Proceedings of ACL-08: HLT, Short Papers, 25–28, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Gillick</author>
<author>J Zhang</author>
<author>D Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Proc. of the HLT-NAACL 2006 Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<location>New York City.</location>
<contexts>
<context position="3597" citStr="DeNero et al., 2006" startWordPosition="525" endWordPosition="528">order polynomial (O(|f|3|e|3))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., 1f and a are the input and output sentences respectively. 782 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 78</context>
<context position="13248" citStr="DeNero et al., 2006" startWordPosition="2140" endWordPosition="2143">nts we report below. 784 either φPzi for phrase pairs or φnull for single language phrases. We choose Dirichlet process (DP) priors for these parameters: φPzi — DP(αP, P1P) φnull — DP(αnull P1null) zi where the base distributions, P1P and Pnull 1 , range over phrase pairs or monolingual phrases in either language, respectively. The most important choice for our model is the priors on the parameters of these terminal distributions. Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al., 2006). Therefore, careful consideration must be given when specifying the P1 distribution on terminals in order to counter this behavior. To construct a prior over string pairs, first we define the probability of a monolingual string (s): P0X (s) = PPoisson(|s|; 1) X where the PPoisson(k; 1) is the probability under a Poisson distribution of length k given an expected length of 1, while VX is the vocabulary size of language X. This distribution has a strong bias towards short strings. In particular note that generally a string of length k will be less probable than two of length k2, a property very</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>J. DeNero, D. Gillick, J. Zhang, D. Klein. 2006. Why generative phrase models underperform surface heuristics. In Proc. of the HLT-NAACL 2006 Workshop on Statistical Machine Translation, 31–38, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>314–323, Honolulu, Hawaii.</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>J. DeNero, A. Bouchard-Cˆot´e, D. Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, 314–323, Honolulu, Hawaii. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Eck</author>
<author>C Hori</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2005</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation,</booktitle>
<location>Pittsburgh.</location>
<contexts>
<context position="25219" citStr="Eck and Hori, 2005" startWordPosition="4112" endWordPosition="4115">riments. 787 IWSLT NIST English+—Chinese English+—Chinese English+—Arabic Sentences 40k 300k 290k Segs./Words 380k 340k 11.0M 8.6M 9.3M 8.5M Av. Sent. Len. 9 8 36 28 32 29 Longest Sent. 75 64 80 80 80 80 Table 1: Corpora statistics. System Test 05 Moses (Heuristic) 47.3 Moses (Bayes SCFG) 49.6 Hiero (Heuristic) 48.3 Hiero (Bayes SCFG) 51.8 Table 2: IWSLT Chinese to English translation. 5.1 Small corpus Firstly we evaluate models trained on a small Chinese-English corpus using a Gibbs sampler on a single CPU. This corpus consists of transcribed utterances made available for the IWSLT workshop (Eck and Hori, 2005). The sparse counts and high reordering for this corpus means the GIZA++ model produces very poor alignments. Table 2 shows the results for the benchmark Moses and Hiero systems on this corpus using both the heuristic phrase estimation, and our proposed Bayesian SCFG model. We can see that our model has a slight advantage. When we look at the grammars extracted by the two models we note that the SCFG model creates considerably more translation rules. Normally this would suggest the alignments of the SCFG model are a lot sparser (more unaligned tokens) than those of the heuristic, however this </context>
</contexts>
<marker>Eck, Hori, 2005</marker>
<rawString>M. Eck, C. Hori. 2005. Overview of the IWSLT 2005 evaluation campaign. In Proc. of the International Workshop on Spoken Language Translation, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of the 4th International Conference on Human Language Technology Research and 5th Annual Meeting of the NAACL (HLT-NAACL 2004),</booktitle>
<location>Boston, USA.</location>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004. What’s in a translation rule? In Proc. of the 4th International Conference on Human Language Technology Research and 5th Annual Meeting of the NAACL (HLT-NAACL 2004), Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the ACL (ACL-2007), 744–751,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="19338" citStr="Goldwater and Griffiths (2007)" startWordPosition="3154" endWordPosition="3157">a much larger set of outcomes. Hence our adoption of a ternary branching grammar. Although such a grammar would be very inefficient for a dynamic programming algorithm, it allows our sampler to permute the internal structure of the trees more easily. 4.3 Hyperparameter Inference Our model is parameterised by a vector of hyperparameters, α = which control the sparsity assumption over various model parameters. We could optimise each concentration parameter on the training corpus by hand, however this would be quite an onerous task. Instead we perform inference over the hyperparameters following Goldwater and Griffiths (2007) by defining a vague gamma prior on each concentration parameter, αx — Gamma(10−4,104). This hyper-prior is relatively benign, allowing the model to consider a wide range of values for the hyperparameter. We sample a new value for each αx using a log-normal distribution with mean αx and variance 0.3, which is then accepted into the distribution p(αx|d, α−) using the MetropolisHastings algorithm. Unlike the Gibbs updates, this calculation cannot be distributed over a cluster (see Section 4.4) and thus is very costly. Therefore for small corpora we re-sample the hyperparameter after every pass t</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater, T. Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proc. of the 45th Annual Meeting of the ACL (ACL-2007), 744–751, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006),</booktitle>
<location>Sydney.</location>
<contexts>
<context position="14451" citStr="Goldwater et al. (2006)" startWordPosition="2344" endWordPosition="2347"> k2, a property very useful for finding ‘minimal’ translation units. This contrasts with a geometric distribution in which a string of length k will be more probable than its segmentations. We define Pnull 1 as the string probability of the non-null part of the rule: The terminal translation phrase pair distribution is a hierarchical Dirichlet Process in which each phrase are independently distributed according to DPs:4 P1P (z —* (e,f)) = φEz(e) X φFz (f) φEz — DP(αPE, P0E) 4This prior is similar to one used by DeNero et al. (2008), who used the expected table count approximation presented in Goldwater et al. (2006). However, Goldwater et al. (2006) contains two major errors: omitting Po, and using the truncated Taylor series expansion (Antoniak, 1974) which fails for small αP0 values common in these models. In this work we track table counts directly. and φFz is defined analogously. This prior encourages frequent phrases to participate in many different translation pairs. Moreover, as longer strings are likely to be less frequent in the corpus this has a tendency to discourage long translation units. 4.2 A Gibbs sampler for derivations Markov chain Monte Carlo sampling allows us to perform inference for</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T. Griffiths, M. Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006), Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>F Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the 6th International Conference on Human Language Technology Research and 7th Annual Meeting of the NAACL (HLT-NAACL</booktitle>
<institution>City. Association for Computational Linguistics.</institution>
<location>New York</location>
<contexts>
<context position="26999" citStr="Habash and Sadat (2006)" startWordPosition="4395" endWordPosition="4398">ma corpus (LDC2005E47). The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize BLEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty </context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>N. Habash, F. Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proc. of the 6th International Conference on Human Language Technology Research and 7th Annual Meeting of the NAACL (HLT-NAACL 2006), New York City. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proc. of the 7th International Conference on Human Language Technology Research and 8th Annual Meeting of the NAACL (HLT-NAACL</booktitle>
<pages>139--146</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="15565" citStr="Johnson et al. (2007)" startWordPosition="2524" endWordPosition="2527">s. 4.2 A Gibbs sampler for derivations Markov chain Monte Carlo sampling allows us to perform inference for the model described in 4.1 without restricting the infinite space of possible translation rules. To do this we need a method for sampling a derivation for a given sentence pair from p(d|d−). One possible approach would be to first build a packed chart representation of the derivation forest, calculate the inside probabilities of all cells in this chart, and then sample derivations top-down according to their inside probabilities (analogous to monolingual parse tree sampling described in Johnson et al. (2007)). A problem with this approach is that building the derivation forest would take O(|f|3|e|3) time, which would be impractical for long sentences. Instead we develop a collapsed Gibbs sampler (Teh et al., 2006) which draws new samples by making local changes to the derivations used in a previous sample. After a period of burn in, the derivations produced by the sampler will be drawn from the posterior distribution, p(d|x). The advantage of this algorithm is that we only store the current derivation for each training sentence pair (together these constitute the state of the sampler), but never </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of the 7th International Conference on Human Language Technology Research and 8th Annual Meeting of the NAACL (HLT-NAACL 2007), 139–146, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 3rd International Conference on Human Language Technology Research and 4th Annual Meeting of the NAACL (HLT-NAACL</booktitle>
<pages>81--88</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1152" citStr="Koehn et al., 2003" startWordPosition="163" endWordPosition="166">ct grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. 1 Introduction The field of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and clearly undesirable. Word-based models are incapable of learning translational equivalenc</context>
<context position="3663" citStr="Koehn et al. (2003)" startWordPosition="536" endWordPosition="539">nous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., 1f and a are the input and output sentences respectively. 782 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782–790, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 20</context>
<context position="6275" citStr="Koehn et al. (2003)" startWordPosition="970" endWordPosition="973">he process of parsing the source sentence, which induces a parallel tree structure and translation in the target language (Chiang, 2007). Figure 1 shows an example derivation for Japanese to English translation using an SCFG. For efficiency reasons we only consider binary or ternary branching rules and don’t allow rules to mix terminals and nonterminals. This allows our sampler to more efficiently explore the space of grammars (Section 4.2), however more expressive grammars would be a straightforward extension of our model. 3 Related work Most machine translation systems adopt the approach of Koehn et al. (2003) for ‘training’ a phrase-based translation model.2 This method starts with a word-alignment, usually the latent state of an unsupervised word-based aligner such 2We include grammar based transducers, such as Chiang (2007) and Marcu et al. (2006), in our definition of phrasebased models. Grammar fragment: hX hJohn-ga, Johni hringo-o, an applei htabeta, atei Sample derivation: hS1,S1i ⇒ hX2, X2i ⇒ hX3 X4 X5, X3 X5 X4i ⇒ hJohn-ga X4 X5, John X5 X4i ⇒ hJohn-ga ringo-o X5, John X5 an applei ⇒ hJohn-ga ringo-o tabeta, John ate an applei Figure 1: A fragment of an SCFG with a ternary non-terminal exp</context>
<context position="22786" citStr="Koehn et al. (2003)" startWordPosition="3724" endWordPosition="3727">istics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. 5 Evaluation Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the GIZA++ implementation of IBM Model 4 (Brown et al., 1993; Och and Ney, 2003) coupled with the phrase extraction heuristics of Koehn et al. (2003) and the SCFG rule extraction heuristics of Chiang (2007) as our benchmark. All the SCFG models employ a single X non-terminal, we leave experiments with multiple non-terminals to future work. Our hypothesis is that our grammar based induction of translation units should benefit language pairs with significant reordering more than those with less. While for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model and diagonal growing heuristic). We conduct experiments on both small a</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrasebased translation. In Proc. of the 3rd International Conference on Human Language Technology Research and 4th Annual Meeting of the NAACL (HLT-NAACL 2003), 81–88, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the ACL (ACL-2007),</booktitle>
<location>Prague.</location>
<contexts>
<context position="22040" citStr="Koehn et al., 2007" startWordPosition="3608" endWordPosition="3611">sentence in that section. In this way each process is sampling according to a slightly ‘out-of-date’ distribution. However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. 5 Evaluation Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th Annual Meeting of the ACL (ACL-2007), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Lewis R E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>J. ACM,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="5152" citStr="Stearns, 1968" startWordPosition="782" endWordPosition="783">roaches which were only applied to small data sets with short sentences. This paper is structured as follows: In Section 3 we argue for the use of efficient sampling techniques over SCFGs as an effective solution to the modelling and scaling problems of previous approaches. We describe our Bayesian SCFG model in Section 4 and a Gibbs sampler to explore its posterior. We apply this sampler to build phrase-based and hierarchical translation models and evaluate their performance on small and large corpora. 2 Synchronous context free grammar A synchronous context free grammar (SCFG, (Lewis II and Stearns, 1968)) generalizes contextfree grammars to generate strings concurrently in two (or more) languages. A string pair is generated by applying a series of paired rewrite rules of the form, X → he, f, ai, where X is a nonterminal, e and f are strings of terminals and nonterminals and a specifies a one-to-one alignment between non-terminals in e and f. In the context of SMT, by assigning the source and target languages to the respective sides of a probabilistic SCFG it is possible to describe translation as the process of parsing the source sentence, which induces a parallel tree structure and translati</context>
</contexts>
<marker>Stearns, 1968</marker>
<rawString>P. M. Lewis II, R. E. Stearns. 1968. Syntax-directed transduction. J. ACM, 15(3):465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002), 133–139, Philadelphia. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2439" citStr="Marcu and Wong, 2002" startWordPosition="347" endWordPosition="350">ed for inducing weighted transducers from word-alignments are based on heuristics with little theoretical justification. A Trevor Cohn* tcohn@inf.ed.ac.uk Miles Osborne* miles@inf.ed.ac.uk †Department of Linguistics University of Maryland College Park, MD 20742, USA model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f|3|e|3))1 for a sub-class of weighted sy</context>
<context position="7386" citStr="Marcu and Wong (2002)" startWordPosition="1145" endWordPosition="1148"> ⇒ hJohn-ga ringo-o tabeta, John ate an applei Figure 1: A fragment of an SCFG with a ternary non-terminal expansion and three terminal rules. as GIZA++. Various heuristics are used to combine source-to-target and target-to-source alignments, after which a further heuristic is used to read off phrase pairs which are ‘consistent’ with the alignment. Although efficient, the sheer number of somewhat arbitrary heuristics makes this approach overly complicated. A number of authors have proposed alternative techniques for directly inducing phrase-based translation models from sentence aligned data. Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. Taking a different tack, DeNero et al. (2008) presented an interesting new model with inference courtesy of a Gibbs sampler, which was better able to explore the full space of phrase translations. However, the efficacy of this model is unclear due to the small-scale experiments and the short sampling runs. In this work we also propose a Gibbs sampler but apply it to the polynomial space of derivation trees, rather than the exponential space of the DeN</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu, W. Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002), 133–139, Philadelphia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006), 44–52,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="6520" citStr="Marcu et al. (2006)" startWordPosition="1006" endWordPosition="1009">ons we only consider binary or ternary branching rules and don’t allow rules to mix terminals and nonterminals. This allows our sampler to more efficiently explore the space of grammars (Section 4.2), however more expressive grammars would be a straightforward extension of our model. 3 Related work Most machine translation systems adopt the approach of Koehn et al. (2003) for ‘training’ a phrase-based translation model.2 This method starts with a word-alignment, usually the latent state of an unsupervised word-based aligner such 2We include grammar based transducers, such as Chiang (2007) and Marcu et al. (2006), in our definition of phrasebased models. Grammar fragment: hX hJohn-ga, Johni hringo-o, an applei htabeta, atei Sample derivation: hS1,S1i ⇒ hX2, X2i ⇒ hX3 X4 X5, X3 X5 X4i ⇒ hJohn-ga X4 X5, John X5 X4i ⇒ hJohn-ga ringo-o X5, John X5 an applei ⇒ hJohn-ga ringo-o tabeta, John ate an applei Figure 1: A fragment of an SCFG with a ternary non-terminal expansion and three terminal rules. as GIZA++. Various heuristics are used to combine source-to-target and target-to-source alignments, after which a further heuristic is used to read off phrase pairs which are ‘consistent’ with the alignment. Alth</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>D. Marcu, W. Wang, A. Echihabi, K. Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006), 44–52, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>A Asuncion</author>
<author>P Smyth</author>
<author>M Welling</author>
</authors>
<title>Distributed inference for latent dirichlet allocation. In NIPS.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="21086" citStr="Newman et al., 2007" startWordPosition="3460" endWordPosition="3463">))|zl, r− U (zl — (el, fl))) Figure 3: Gibbs sampling equations for the competing configurations of the Split/Join sampler, shown in Figure 2. Eq. (1) corresponds to the top-left configuration, and (2) the remaining configurations where the choice of el, fl, er, fr and ai specifies the string segmentation and the alignment (monotone or reordered). massive space of possible grammars, it induces dependencies between all the sentences in the training corpus. These dependencies make it difficult to scale our approach to larger corpora by distributing it across a number of processors. Recent work (Newman et al., 2007; Asuncion et al., 2008) suggests that good practical parallel performance can be achieved by having multiple processors independently sample disjoint subsets of the corpus. Each process maintains a set of rule counts for the entire corpus and communicates the changes it has made to its section of the corpus only after sampling every sentence in that section. In this way each process is sampling according to a slightly ‘out-of-date’ distribution. However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler. 4.5 Extracting a transla</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2007</marker>
<rawString>D. Newman, A. Asuncion, P. Smyth, M. Welling. 2007. Distributed inference for latent dirichlet allocation. In NIPS. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="22717" citStr="Och and Ney, 2003" startWordPosition="3713" endWordPosition="3716">rchical alignment on which we can apply the standard extraction heuristics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. 5 Evaluation Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the GIZA++ implementation of IBM Model 4 (Brown et al., 1993; Och and Ney, 2003) coupled with the phrase extraction heuristics of Koehn et al. (2003) and the SCFG rule extraction heuristics of Chiang (2007) as our benchmark. All the SCFG models employ a single X non-terminal, we leave experiments with multiple non-terminals to future work. Our hypothesis is that our grammar based induction of translation units should benefit language pairs with significant reordering more than those with less. While for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model an</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och, H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the ACL (ACL-2003),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="27173" citStr="Och, 2003" startWordPosition="4427" endWordPosition="4428"> and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize BLEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty passes through the data. It is clear from the training curves that the distributed approximation tracks the corpus probability of the correct sampler sufficiently closely. Th</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the ACL (ACL-2003), 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation,</title>
<date>2001</date>
<contexts>
<context position="24536" citStr="Papineni et al., 2001" startWordPosition="4000" endWordPosition="4003">card alignment points until the node factorises correctly. As the alignments contain many such non-factorisable nodes, these trees are of poor quality. However, all samplers used in these experiments are first ‘burnt-in’ for 1000 full passes through the data. This allows the sampler to diverge from its initialisation condition, and thus gives us confidence that subsequent samples will be drawn from the posterior. An expectation over phrase tables and Hiero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM BLEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. 787 IWSLT NIST English+—Chinese English+—Chinese English+—Arabic Sentences 40k 300k 290k Segs./Words 380k 340k 11.0M 8.6M 9.3M 8.5M Av. Sent. Len. 9 8 36 28 32 29 Longest Sent. 75 64 80 80 80 80 Table 1: Corpora statistics. System Test 05 Moses (Heuristic) 47.3 Moses (Bayes SCFG) 49.6 Hiero (Heuristic) 48.3 Hiero (Bayes SCFG) 51.8 Table 2: IWSLT Chinese to English translation. 5.1 Small corpus Firstly we evaluate models trained on a small Chinese-English corpus using a Gibbs sampler on a single CPU. This corpus consists o</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="15775" citStr="Teh et al., 2006" startWordPosition="2560" endWordPosition="2563">s we need a method for sampling a derivation for a given sentence pair from p(d|d−). One possible approach would be to first build a packed chart representation of the derivation forest, calculate the inside probabilities of all cells in this chart, and then sample derivations top-down according to their inside probabilities (analogous to monolingual parse tree sampling described in Johnson et al. (2007)). A problem with this approach is that building the derivation forest would take O(|f|3|e|3) time, which would be impractical for long sentences. Instead we develop a collapsed Gibbs sampler (Teh et al., 2006) which draws new samples by making local changes to the derivations used in a previous sample. After a period of burn in, the derivations produced by the sampler will be drawn from the posterior distribution, p(d|x). The advantage of this algorithm is that we only store the current derivation for each training sentence pair (together these constitute the state of the sampler), but never need to reason over derivation forests. By integrating over (collapsing) the parameters we only store counts of rules used in the current sampled set of derivations, thereby avoiding explicitly representing the</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3081" citStr="Wu, 1997" startWordPosition="448" endWordPosition="449">al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f|3|e|3))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a nov</context>
<context position="8359" citStr="Wu, 1997" startWordPosition="1296" endWordPosition="1297">f this model is unclear due to the small-scale experiments and the short sampling runs. In this work we also propose a Gibbs sampler but apply it to the polynomial space of derivation trees, rather than the exponential space of the DeNero et al. (2008) model. The restrictions imposed by our tree structure make sampling considerably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; X → X → X → X → 1 X 2 X3, X 1 X3 X 2i 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f|3|e|3)) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine translati</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
<author>D Chiang</author>
</authors>
<title>Extracting synchronous grammar rules from word-level alignments in linear time.</title>
<date>2008</date>
<booktitle>In Proc. of the 22th International Conference on Computational Linguistics (COLING-2008),</booktitle>
<pages>1081--1088</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="2481" citStr="Zhang et al., 2008" startWordPosition="355" endWordPosition="358">rd-alignments are based on heuristics with little theoretical justification. A Trevor Cohn* tcohn@inf.ed.ac.uk Miles Osborne* miles@inf.ed.ac.uk †Department of Linguistics University of Maryland College Park, MD 20742, USA model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f|3|e|3))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997)</context>
<context position="3996" citStr="Zhang et al., 2008" startWordPosition="587" endWordPosition="590">ith few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., 1f and a are the input and output sentences respectively. 782 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782–790, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2008)). The sampler performs local edit operations to nodes in the synchronous trees, each of which is very fast, leading to a highly efficient inference technique. This allows us to train the model on large corpora without resort to punitive length limits, unlike previous approaches which were only applied to small data sets with sh</context>
<context position="8443" citStr="Zhang et al., 2008" startWordPosition="1320" endWordPosition="1323">ampling runs. In this work we also propose a Gibbs sampler but apply it to the polynomial space of derivation trees, rather than the exponential space of the DeNero et al. (2008) model. The restrictions imposed by our tree structure make sampling considerably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; X → X → X → X → 1 X 2 X3, X 1 X3 X 2i 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f|3|e|3)) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine translation community. 4 Model Our aim is to induce a grammar from a training set of sentence</context>
<context position="23759" citStr="Zhang et al. (2008" startWordPosition="3879" endWordPosition="3882"> for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model and diagonal growing heuristic). We conduct experiments on both small and large corpora to allow a range of alignment qualities and also to verify the effectiveness of our distributed approximation of the Bayesian inference. The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al. (2008a) to first create initial trees. Where these factored trees contain nodes with mixed terminals and non-terminals, or more than three non-terminals, we discard alignment points until the node factorises correctly. As the alignments contain many such non-factorisable nodes, these trees are of poor quality. However, all samplers used in these experiments are first ‘burnt-in’ for 1000 full passes through the data. This allows the sampler to diverge from its initialisation condition, and thus gives us confidence that subsequent samples will be drawn from the posterior. An expectation over phrase t</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>H. Zhang, D. Gildea, D. Chiang. 2008a. Extracting synchronous grammar rules from word-level alignments in linear time. In Proc. of the 22th International Conference on Computational Linguistics (COLING-2008), 1081–1088, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>C Quirk</author>
<author>R C Moore</author>
<author>D Gildea</author>
</authors>
<title>Bayesian learning of non-compositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), 97–105,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2481" citStr="Zhang et al., 2008" startWordPosition="355" endWordPosition="358">rd-alignments are based on heuristics with little theoretical justification. A Trevor Cohn* tcohn@inf.ed.ac.uk Miles Osborne* miles@inf.ed.ac.uk †Department of Linguistics University of Maryland College Park, MD 20742, USA model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f|3|e|3))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997)</context>
<context position="3996" citStr="Zhang et al., 2008" startWordPosition="587" endWordPosition="590">ith few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., 1f and a are the input and output sentences respectively. 782 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782–790, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2008)). The sampler performs local edit operations to nodes in the synchronous trees, each of which is very fast, leading to a highly efficient inference technique. This allows us to train the model on large corpora without resort to punitive length limits, unlike previous approaches which were only applied to small data sets with sh</context>
<context position="8443" citStr="Zhang et al., 2008" startWordPosition="1320" endWordPosition="1323">ampling runs. In this work we also propose a Gibbs sampler but apply it to the polynomial space of derivation trees, rather than the exponential space of the DeNero et al. (2008) model. The restrictions imposed by our tree structure make sampling considerably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; X → X → X → X → 1 X 2 X3, X 1 X3 X 2i 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f|3|e|3)) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine translation community. 4 Model Our aim is to induce a grammar from a training set of sentence</context>
<context position="23759" citStr="Zhang et al. (2008" startWordPosition="3879" endWordPosition="3882"> for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model and diagonal growing heuristic). We conduct experiments on both small and large corpora to allow a range of alignment qualities and also to verify the effectiveness of our distributed approximation of the Bayesian inference. The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al. (2008a) to first create initial trees. Where these factored trees contain nodes with mixed terminals and non-terminals, or more than three non-terminals, we discard alignment points until the node factorises correctly. As the alignments contain many such non-factorisable nodes, these trees are of poor quality. However, all samplers used in these experiments are first ‘burnt-in’ for 1000 full passes through the data. This allows the sampler to diverge from its initialisation condition, and thus gives us confidence that subsequent samples will be drawn from the posterior. An expectation over phrase t</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008b. Bayesian learning of non-compositional phrases with synchronous parsing. In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), 97–105, Columbus, Ohio.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>