<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000098">
<title confidence="0.998309">
Phrase-based Statistical Language Generation using
Graphical Models and Active Learning
</title>
<author confidence="0.962721">
Franc¸ois Mairesse, Milica Gaˇsi´c, Filip Jurˇc´ıˇcek,
</author>
<affiliation confidence="0.8875905">
Simon Keizer, Blaise Thomson, Kai Yu and Steve Young*
Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK
</affiliation>
<email confidence="0.974672">
{f.mairesse, mg436, fj228, sk561, brmt2, ky219, sjy}@eng.cam.ac.uk
</email>
<sectionHeader confidence="0.99721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887636363637">
Most previous work on trainable language
generation has focused on two paradigms:
(a) using a statistical model to rank a
set of generated utterances, or (b) using
statistics to inform the generation deci-
sion process. Both approaches rely on
the existence of a handcrafted generator,
which limits their scalability to new do-
mains. This paper presents BAGEL, a sta-
tistical language generator which uses dy-
namic Bayesian networks to learn from
semantically-aligned data produced by 42
untrained annotators. A human evalua-
tion shows that BAGEL can generate nat-
ural and informative utterances from un-
seen inputs in the information presentation
domain. Additionally, generation perfor-
mance on sparse datasets is improved sig-
nificantly by using certainty-based active
learning, yielding ratings close to the hu-
man gold standard with a fraction of the
data.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999256083333333">
The field of natural language generation (NLG) is
one of the last areas of computational linguistics to
embrace statistical methods. Over the past decade,
statistical NLG has followed two lines of research.
The first one, pioneered by Langkilde and Knight
(1998), introduces statistics in the generation pro-
cess by training a model which reranks candi-
date outputs of a handcrafted generator. While
their HALOGEN system uses an n-gram language
model trained on news articles, other systems have
used hierarchical syntactic models (Bangalore and
Rambow, 2000), models trained on user ratings of
</bodyText>
<footnote confidence="0.98948775">
This research was partly funded by the UK EPSRC un-
der grant agreement EP/F013930/1 and funded by the EU
FP7 Programme under grant agreement 216594 (CLASSiC
project: www.classic-project.org).
</footnote>
<figureCaption confidence="0.671712">
utterance quality (Walker et al., 2002), or align-
ment models trained on speaker-specific corpora
(Isard et al., 2006).
</figureCaption>
<bodyText confidence="0.999933131578948">
A second line of research has focused on intro-
ducing statistics at the generation decision level,
by training models that find the set of genera-
tion parameters maximising an objective function,
e.g. producing a target linguistic style (Paiva and
Evans, 2005; Mairesse and Walker, 2008), gener-
ating the most likely context-free derivations given
a corpus (Belz, 2008), or maximising the expected
reward using reinforcement learning (Rieser and
Lemon, 2009). While such methods do not suffer
from the computational cost of an overgeneration
phase, they still require a handcrafted generator to
define the generation decision space within which
statistics can be used to find an optimal solution.
This paper presents BAGEL (Bayesian networks
for generation using active learning), an NLG sys-
tem that can be fully trained from aligned data.
While the main requirement of the generator is to
produce natural utterances within a dialogue sys-
tem domain, a second objective is to minimise the
overall development effort. In this regard, a major
advantage of data-driven methods is the shift of
the effort from model design and implementation
to data annotation. In the case of NLG systems,
learning to produce paraphrases can be facilitated
by collecting data from a large sample of annota-
tors. Our meaning representation should therefore
(a) be intuitive enough to be understood by un-
trained annotators, and (b) provide useful gener-
alisation properties for generating unseen inputs.
Section 2 describes BAGEL’s meaning represen-
tation, which satisfies both requirements. Sec-
tion 3 then details how our meaning representation
is mapped to a phrase sequence, using a dynamic
Bayesian network with backoff smoothing.
Within a given domain, the same semantic
concept can occur in different utterances. Sec-
tion 4 details how BAGEL exploits this redundancy
</bodyText>
<page confidence="0.959899">
1552
</page>
<note confidence="0.943011">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552–1561,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996404375">
to improve generation performance on sparse
datasets, by guiding the data collection process
using certainty-based active learning (Lewis and
Catlett, 1994). We train BAGEL in the informa-
tion presentation domain, from a corpus of utter-
ances produced by 42 untrained annotators (see
Section 5.1). An automated evaluation metric is
used to compare preliminary model and training
configurations in Section 5.2, while Section 5.3
shows that the resulting system produces natural
and informative utterances, according to 18 hu-
man judges. Finally, our human evaluation shows
that training using active learning significantly im-
proves generation performance on sparse datasets,
yielding results close to the human gold standard
using a fraction of the data.
</bodyText>
<sectionHeader confidence="0.934838" genericHeader="introduction">
2 Phrase-based generation from
semantic stacks
</sectionHeader>
<bodyText confidence="0.99981832">
BAGEL uses a stack-based semantic representa-
tion to constrain the sequence of semantic con-
cepts to be searched. This representation can be
seen as a linearised semantic tree similar to the
one previously used for natural language under-
standing in the Hidden Vector State model (He
and Young, 2005). A stack representation provides
useful generalisation properties (see Section 3.1),
while the resulting stack sequences are relatively
easy to align (see Section 5.1). In the context of
dialogue systems, Table 1 illustrates how the input
dialogue act is first mapped to a set of stacks of
semantic concepts, and then aligned with a word
sequence. The bottom concept in the stack will
typically be a dialogue act type, e.g. an utterance
providing information about the object under dis-
cussion (inform) or specifying that the request
of the user cannot be met (reject). Other con-
cepts include attributes of that object (e.g., food,
area), values for those attributes (e.g., Chinese,
riverside), as well as special symbols for negat-
ing underlying concepts (e.g., not) or specifying
that they are irrelevant (e.g., dontcare).
The generator’s goal is thus finding the
most likely realisation given an unordered
set of mandatory semantic stacks Sm derived
from the input dialogue act. For example,
s =inform(area(centre)) is a mandatory stack
associated with the dialogue act in Table 1 (frame
8). While mandatory stacks must all be conveyed
in the output realisation, Sm does not contain the
optional intermediary stacks Si that can refer to
(a) general attributes of the object under discus-
sion (e.g., inform(area) in Table 1), or (b) to
concepts that are not in the input at all, which are
associated with the singleton stack inform (e.g.,
phrases expressing the dialogue act type, or clause
aggregation operations). For example, the stack
sequence in Table 1 contains 3 intermediary stacks
for t = 2, 5 and 7.
BAGEL’s granularity is defined by the semantic
annotation in the training data, rather than external
linguistic knowledge about what constitutes a unit
of meaning, i.e. contiguous words belonging to
the same semantic stack are modelled as an atomic
observation unit or phrase.1 In contrast with word-
level models, a major advantage of phrase-based
generation models is that they can model long-
range dependencies and domain-specific idiomatic
phrases with fewer parameters.
</bodyText>
<sectionHeader confidence="0.995521" genericHeader="method">
3 Dynamic Bayesian networks for NLG
</sectionHeader>
<bodyText confidence="0.999844111111111">
Dynamic Bayesian networks have been used suc-
cessfully for speech recognition, natural language
understanding, dialogue management and text-to-
speech synthesis (Rabiner, 1989; He and Young,
2005; Lef`evre, 2006; Thomson and Young, 2010;
Tokuda et al., 2000). Such models provide a
principled framework for predicting elements in a
large structured space, such as required for non-
trivial NLG tasks. Additionally, their probabilistic
nature makes them suitable for modelling linguis-
tic variation, i.e. there can be multiple valid para-
phrases for a given input.
BAGEL models the generation task as finding
the most likely sequence of realisation phrases
R* = (r1...rL) given an unordered set of manda-
tory semantic stacks Sm, with |Sm |G L. BAGEL
must thus derive the optimal sequence of semantic
stacks S* that will appear in the utterance given
Sm, i.e. by inserting intermediary stacks if needed
and by performing content ordering. Any num-
ber of intermediary stacks can be inserted between
two consecutive mandatory stacks, as long as all
their concepts are included in either the previous
or following mandatory stack, and as long as each
stack transition leads to a different stack (see ex-
ample in Table 1). Let us define the set of possi-
ble stack sequences matching these constraints as
</bodyText>
<equation confidence="0.777707">
5eq(Sm) C IS = (s1...sL) s.t. st E Sm U Si}.
</equation>
<bodyText confidence="0.978462">
We propose a model which estimates the dis-
</bodyText>
<footnote confidence="0.980725">
1The term phrase is thus defined here as any sequence of
one or more words.
</footnote>
<page confidence="0.964568">
1553
</page>
<bodyText confidence="0.8232512">
Charlie Chan is a Chinese restaurant near Cineworld in the centre of town
Charlie Chan inform Chinese restaurant near Cineworld area centre
name food type inform near inform area
inform inform inform inform inform
t = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8
</bodyText>
<tableCaption confidence="0.99562">
Table 1: Example semantic stacks aligned with an utterance for the dialogue act
</tableCaption>
<bodyText confidence="0.706874166666667">
inform(name(Charlie Chan) type(restaurant) area(centre) food(Chinese) near(Cineworld)). Mandatory
stacks are in bold.
tribution P(R|Sm) from a training set of real-
isation phrases aligned with semantic stack se-
quences, by marginalising over all stack sequences
in Seq(Sm):
</bodyText>
<equation confidence="0.998571166666667">
P(R|Sm) = E P(R, S|Sm)
SESeq(S—)
E= P(R|S, Sm)P(S|Sm)
SESeq(S—)
E= P(R|S)P(S|Sm) (1)
SESeq(S—)
</equation>
<bodyText confidence="0.999848375">
Inference over the model defined in (1) requires
the decoding algorithm to consider all possible or-
derings over Seq(Sm) together with all possible
realisations, which is intractable for non-trivial do-
mains. We thus make the additional assumption
that the most likely sequence of semantic stacks
S* given Sm is the one yielding the optimal reali-
sation phrase sequence:
</bodyText>
<equation confidence="0.992910333333333">
P(R|Sm) ≈ P(R|S*)P(S*|Sm) (2)
with S* = argmax P(S|Sm) (3)
SESeq(S—)
</equation>
<bodyText confidence="0.991897">
The semantic stacks are therefore decoded first
using the model in Fig. 1 to solve the argmax
in (3). The decoded stack sequence S* is then
treated as observed in the realisation phase, in
which the model in Fig. 2 is used to find the real-
isation phrase sequence R* maximising P(R|S*)
over all phrase sequences of length L = |S* |in
our vocabulary:
</bodyText>
<equation confidence="0.99961175">
R* = argmax P(R|S*)P(S*|Sm) (4)
R=(r1...rL)
= argmax P(R|S*) (5)
R=(r1...rL)
</equation>
<bodyText confidence="0.996109384615385">
In order to reduce model complexity, we fac-
torise our model by conditioning the realisation
phrase at time t on the previous phrase rt−1,
and the previous, current, and following semantic
stacks. The semantic stack st at time t is assumed
Figure 1: Graphical model for the semantic decod-
ing phase. Plain arrows indicate smoothed proba-
bility distributions, dashed arrows indicate deter-
ministic relations, and shaded nodes are observed.
The generation of the end semantic stack symbol
deterministically triggers the final frame.
to depend only on the previous two stacks and the
last mandatory stack s,, E Sm with 1 &lt; u &lt; t:
</bodyText>
<equation confidence="0.856210571428571">
P(S|Sm) = � IITt=1 P(st|st−1, st−2, su) (6)
� if S ∈ Seq(Sm)
� 0 otherwise
P(R|S*) = T P(rt|rt−1, s*t−1, s*t, s*t+1) (7)
77
771
t=1
</equation>
<bodyText confidence="0.999937473684211">
While dynamic Bayesian networks typically
take sequential inputs, mapping a set of seman-
tic stacks to a sequence of phrases is achieved
by keeping track of the mandatory stacks that
were visited in the current sequence (see stack set
tracker variable in Fig. 1), and pruning any se-
quence that has not included all mandatory input
stacks on reaching the final frame (see observed
stack set validator variable in Fig. 1). Since the
number of intermediary stacks is not known at de-
coding time, the network is unrolled for a fixed
number of frames T defining the maximum num-
ber of phrases that can be generated (e.g., T =
50). The end of the stack sequence is then deter-
mined by a special end symbol, which can only
be emitted within the T frames once all mandatory
stacks have been visited. The probability of the re-
sulting utterance is thus computed over all frames
up to the end symbol, which determines the length
</bodyText>
<figure confidence="0.961654555555555">
last mandatory
stack
stack set
validator
semantic
stack s
stack set tracker
first frame
repeated frame final frame
</figure>
<page confidence="0.732208">
1554
</page>
<bodyText confidence="0.980246384615385">
first frame
repeated frame final frame
L of S* and R*. While the decoding constraints
enforce that L &gt; |Sm|, the search for S* requires
comparing sequences of different lengths. A con-
sequence is that shorter sequences containing only
mandatory stacks are likely to be favoured. While
future work should investigate length normalisa-
tion strategies, we find that the learned transition
probabilities are skewed enough to favour stack
sequences including intermediary stacks.
Once the topology and the decoding constraints
of the network have been defined, any inference al-
gorithm can be used to search for S* and R*. We
use the junction tree algorithm implemented in the
Graphical Model ToolKit (GMTK) for our exper-
iments (Bilmes and Zweig, 2002), however both
problems can be solved using a standard Viterbi
search given the appropriate state representation.
In terms of computational complexity, it is impor-
tant to note that the number of stack sequences
Seq(Sm) to search over increases exponentially
with the number of input mandatory stacks. Nev-
ertheless, we find that real-time performance can
be achieved by pruning low probability sequences,
without affecting the quality of the solution.
</bodyText>
<subsectionHeader confidence="0.999816">
3.1 Generalisation to unseen semantic stacks
</subsectionHeader>
<bodyText confidence="0.95403208">
In order to generalise to semantic stacks which
have not been observed during training, the re-
alisation phrase r is made dependent on under-
specified stack configurations, i.e. the tail l
and the head h of the stack. For example, the
last stack in Table 1 is associated with the head
centre and the tail inform(area). As a re-
sult, BAGEL assigns non-zero probabilities to re-
alisation phrases in unseen semantic contexts, by
backing off to the head and the tail of the stack.
A consequence is that BAGEL’s lexical realisa-
tion can generalise across contexts. For exam-
ple, if reject(area(centre)) was never ob-
served at training time, P(r = centre of town|s =
reject(area(centre))) will be estimated by
backing off to P(r = centre of town|h =
centre). BAGEL can thus generate ‘there are
no venues in the centre of town’ if the phrase
‘centre of town’ was associated with the con-
cept centre in a different context, such as
inform(area(centre)). The final realisation
model is illustrated in Fig. 2:
realisation
phrase r
stack tail d
</bodyText>
<figureCaption confidence="0.857458666666667">
Figure 2: Graphical model for the realisation
phase. Dashed arrows indicate deterministic re-
lations, and shaded node are observed.
</figureCaption>
<equation confidence="0.8914842">
rt h t l t rt lt lt st st st
 |, , 1 , 1, 1, , 1, 1
− − + − +
rt  |ht, lt,rt−1,lt−1,lt+1,st
rt  |ht, lt,rt−1,lt−1,lt+1
</equation>
<figureCaption confidence="0.980469">
Figure 3: Backoff graphs for the semantic decod-
ing and realisation models.
</figureCaption>
<equation confidence="0.99966625">
P(R|S∗) _ �L P(rt|rt−1, ht, lt−1, lt, lt+1,
t=1
t+1)
∗ (8)
</equation>
<bodyText confidence="0.999984615384615">
Conditional probability distributions are repre-
sented as factored language models smoothed us-
ing Witten-Bell interpolated backoff smoothing
(Bilmes and Kirchhoff, 2003), according to the
backoff graphs in Fig. 3. Variables which are the
furthest away in time are dropped first, and par-
tial stack variables are dropped last as they are ob-
served the most.
It is important to note that generating unseen se-
mantic stacks requires all possible mandatory se-
mantic stacks in the target domain to be prede-
fined, in order for all stack unigrams to be assigned
a smoothed non-zero probability.
</bodyText>
<subsectionHeader confidence="0.999928">
3.2 High cardinality concept abstraction
</subsectionHeader>
<bodyText confidence="0.999973444444445">
While one should expect a trainable generator
to learn multiple lexical realisations for low-
cardinality semantic concepts, learning lexical
realisations for high-cardinality database entries
(e.g., proper names) would increase the number of
model parameters prohibitively. We thus divide
pre-terminal concepts in the semantic stacks into
two types: (a) enumerable attributes whose val-
ues are associated with distinct semantic stacks in
</bodyText>
<figure confidence="0.976598875">
rt  |ht,lt
rt|ht
rt
stack head h
!&amp;quot;
semantic
stack s
st  |st−1,st−2, su
t−2
st |st−1 , s
1
st  |st−
st
t, s
∗
s∗t−1, s
</figure>
<page confidence="0.97647">
1555
</page>
<bodyText confidence="0.9998785">
our model (e.g., inform(pricerange(cheap))),
and (b) non-enumerable attributes whose values
are replaced by a generic symbol before train-
ing in both the utterance and the semantic stack
(e.g., inform(name(X)). These symbolic values
are then replaced in the surface realisation by the
corresponding value in the input specification. A
consequence is that our model can only learn syn-
onymous lexical realisations for enumerable at-
tributes.
</bodyText>
<sectionHeader confidence="0.973329" genericHeader="method">
4 Certainty-based active learning
</sectionHeader>
<bodyText confidence="0.999798529411765">
A major issue with trainable NLG systems is the
lack of availability of domain-specific data. It is
therefore essential to produce NLG models that
minimise the data annotation cost.
BAGEL supports the optimisation of the data
collection process through active learning, in
which the next semantic input to annotate is de-
termined by the current model. The probabilis-
tic nature of BAGEL allows the use of certainty-
based active learning (Lewis and Catlett, 1994),
by querying the k semantic inputs for which the
model is the least certain about its output real-
isation. Given a finite semantic input space Z
representing all possible dialogue acts in our do-
main (i.e., the set of all sets of mandatory seman-
tic stacks Sm), BAGEL’s active learning training
process iterates over the following steps:
</bodyText>
<listItem confidence="0.97722575">
1. Generate an utterance for each semantic input S. E Z
using the current model.2
2. Annotate the k semantic inputs fS;,...Sk } yielding
the lowest realisation probability, i.e. for q E (L.k)
</listItem>
<equation confidence="0.88934525">
S; = argmin (max P(R|S�)) (9)
SmE�\{S1 m...S��1 �
m �
with P(R|S.) defined in (2).
</equation>
<sectionHeader confidence="0.536124" genericHeader="method">
3. Retrain the model with the additional k data points.
</sectionHeader>
<bodyText confidence="0.9999045">
The number of utterances to be queried k should
depend on the flexibility of the annotators and the
time required for generating all possible utterances
in the domain.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="method">
5 Experimental method
</sectionHeader>
<bodyText confidence="0.99763175">
BAGEL’s factored language models are trained us-
ing the SRILM toolkit (Stolcke, 2002), and de-
coding is performed using GMTK’s junction tree
inference algorithm (Bilmes and Zweig, 2002).
</bodyText>
<footnote confidence="0.517064">
2Sampling methods can be used if Z is infinite or too
large.
</footnote>
<bodyText confidence="0.99780775">
Since each active learning iteration requires gen-
erating all training utterances in our domain, they
are generated using a larger clique pruning thresh-
old than the test utterances used for evaluation.
</bodyText>
<subsectionHeader confidence="0.992523">
5.1 Corpus collection
</subsectionHeader>
<bodyText confidence="0.999816409090909">
We train BAGEL in the context of a dialogue
system providing information about restaurants
in Cambridge. The domain contains two dia-
logue act types: (a) inform: presenting infor-
mation about a restaurant (see Table 1), and (b)
reject: informing that the user’s constraints can-
not be met (e.g., ‘There is no cheap restaurant
in the centre’). Our domain contains 8 restau-
rant attributes: name, food, near, pricerange,
postcode, phone, address, and area, out of
which food, pricerange, and area are treated
as enumerable.3 Our input semantic space is ap-
proximated by the set of information presentation
dialogue acts produced over 20,000 simulated di-
alogues between our statistical dialogue manager
(Young et al., 2010) and an agenda-based user
simulator (Schatzmann et al., 2007), which results
in 202 unique dialogue acts after replacing non-
enumerable values by a generic symbol. Each di-
alogue act contains an average of 4.48 mandatory
semantic stacks.
As one of our objectives is to test whether
BAGEL can learn from data provided by a large
sample of untrained annotators, we collected a
corpus of semantically-aligned utterances using
Amazon’s Mechanical Turk data collection ser-
vice. A crucial aspect of data collection for
NLG is to ensure that the annotators under-
stand the meaning of the semantics to be con-
veyed. Annotators were first asked to provide
an utterance matching an abstract description
of the dialogue act, regardless of the order in
which the constraints are presented (e.g., Offer
the venue Taj Mahal and provide the information
type(restaurant), area(riverside), food(Indian),
near(The Red Lion)). The order of the constraints
in the description was randomised to reduce the
effect of priming. The annotators were then asked
to align the attributes (e.g., Indicate the region of
the utterance related to the concept ‘area’), and
the attribute values (e.g., Indicate only the words
related to the concept ‘riverside’). Two para-
phrases were collected for each dialogue act in
our domain, resulting in a total of 404 aligned ut-
</bodyText>
<footnote confidence="0.924452">
3With the exception of areas defined as proper nouns.
</footnote>
<page confidence="0.873767">
1556
</page>
<table confidence="0.999869">
rt st ht lt
&lt;s&gt; START START START
The Rice Boat inform(name(X)) X inform(name)
is a inform inform EMPTY
restaurant inform(type(restaurant)) restaurant inform(type)
in the inform(area) area inform
riverside inform(area(riverside)) riverside inform(area)
area inform(area) area inform
that inform inform EMPTY
serves inform(food) food inform
French inform(food(French)) French inform(food)
food inform(food) food inform
&lt;/s&gt; END END END
</table>
<tableCaption confidence="0.97869">
Table 2: Example utterance annotation used to estimate the conditional probability distributions of the
models in Figs. 1 and 2 ( rt=realisation phrase, st=semantic stack, ht=stack head, lt=stack tail).
</tableCaption>
<bodyText confidence="0.99962">
terances produced by 42 native speakers of En-
glish. After manually checking and normalising
the dataset,4 the layered annotations were auto-
matically mapped to phrase-level semantic stacks
by splitting the utterance into phrases at annotation
boundaries. Each annotated utterance is then con-
verted into a sequence of symbols such as in Ta-
ble 2, which are used to estimate the conditional
probability distributions defined in (6) and (8).
The resulting vocabulary consists of 52 distinct se-
mantic stacks and 109 distinct realisation phrases,
with an average of 8.35 phrases per utterance.
</bodyText>
<subsectionHeader confidence="0.993557">
5.2 BLEU score evaluation
</subsectionHeader>
<bodyText confidence="0.999987888888889">
We first evaluate BAGEL using the BLEU auto-
mated metric (Papineni et al., 2002), which mea-
sures the word n-gram overlap between the gen-
erated utterances and the 2 reference paraphrases
over a test corpus (with n up to 4). While BLEU
suffers from known issues such as a bias towards
statistical NLG systems (Reiter and Belz, 2009), it
provides useful information when comparing sim-
ilar systems. We evaluate BAGEL for different
training set sizes, model dependencies, and active
learning parameters. Our results are averaged over
a 10-fold cross-validation over distinct dialogue
acts, i.e. dialogue acts used for testing are not seen
at training time,5 and all systems are tested on the
same folds. The training and test sets respectively
contain an average of 181 and 21 distinct dialogue
acts, and each dialogue act is associated with two
paraphrases, resulting in 362 training utterances.
</bodyText>
<footnote confidence="0.989383">
4The normalisation process took around 4 person-hour for
404 utterances.
5We do not evaluate performance on dialogue acts used
for training, as the training examples can trivially be used as
generation templates.
</footnote>
<figure confidence="0.9478515">
10 20 40 60 80 100 120 150 200 250 300 362
Tra/n/ng set s/2e
</figure>
<figureCaption confidence="0.987776">
Figure 4: BLEU score averaged over a 10-fold
</figureCaption>
<bodyText confidence="0.981027833333333">
cross-validation for different training set sizes and
network topologies, using random sampling.
Results: Fig. 4 shows that adding a dependency
on the future semantic stack improves perfor-
mances for all training set sizes, despite the added
model complexity. Backing off to partial stacks
also improves performance, but only for sparse
training sets.
Fig. 5 compares the full model trained using
random sampling in Fig. 4 with the same model
trained using certainty-based active learning, for
different values of k. As our dataset only con-
tains two paraphrases per dialogue act, the same
dialogue act can only be queried twice during the
active learning procedure. A consequence is that
the training set used for active learning converges
towards the randomly sampled set as its size in-
creases. Results show that increasing the train-
ing set one utterance at a time using active learn-
ing (k = 1) significantly outperforms random
sampling when using 40, 80, and 100 utterances
(p &lt; .05, two-tailed). Increasing the number of
utterances to be queried at each iteration to k = 10
results in a smaller performance increase. A possi-
</bodyText>
<figure confidence="0.981962647058824">
Mean BLEU score .
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.75
0.7
Fu(( mode(
No stack backoff
No stack backoff, no future semantics
1557
Mean BLEU score
10 20 40 60 80 100 120 150 200 250 300 362
Tra/n/ng set s/2e
</figure>
<figureCaption confidence="0.929862">
Figure 5: BLEU score averaged over a 10-fold
cross-validation for different numbers of queries
per iteration, using the full model with the query
selection criterion (9).
</figureCaption>
<figure confidence="0.9940015">
10 20 40 60 80 100 120 150 200 250 300 362
Tra/n/ng set s/2e
</figure>
<figureCaption confidence="0.9948">
Figure 6: BLEU score averaged over a 10-fold
</figureCaption>
<bodyText confidence="0.996562363636364">
cross-validation for different query selection cri-
teria, using the full model with k = 1.
ble explanation is that the model is likely to assign
low probabilities to similar inputs, thus any value
above k = 1 might result in redundant queries
within an iteration.
As the length of the semantic stack sequence
is not known before decoding, the active learn-
ing selection criterion presented in (9) is biased
towards longer utterances, which tend to have a
lower probability. However, Fig. 6 shows that
normalising the log probability by the number of
semantic stacks does not improve overall learn-
ing performance. Although a possible explanation
is that longer inputs tend to contain more infor-
mation to learn from, Fig. 6 shows that a base-
line selecting the largest remaining semantic input
at each iteration performs worse than the active
learning scheme for training sets above 20 utter-
ances. The full log probability selection criterion
defined in (9) is therefore used throughout the rest
of the paper (with k = 1).
</bodyText>
<subsectionHeader confidence="0.998083">
5.3 Human evaluation
</subsectionHeader>
<bodyText confidence="0.99954976">
While automated metrics provide useful informa-
tion for comparing different systems, human feed-
back is needed to assess (a) the quality of BAGEL’s
outputs, and (b) whether training models using ac-
tive learning has a significant impact on user per-
ceptions. We evaluate BAGEL through a large-
scale subjective rating experiment using Amazon’s
Mechanical Turk service.
For each dialogue act in our domain, partici-
pants are presented with a ‘gold standard’ human
utterance from our dataset, which they must com-
pare with utterances generated by models trained
with and without active learning on a set of 20, 40,
100, and 362 utterances (full training set), as well
as with the second human utterance in our dataset.
See example utterances in Table 3. The judges are
then asked to evaluate the informativeness and nat-
uralness of each of the 8 utterances on a 5 point
likert-scale. Naturalness is defined as whether the
utterance could have been produced by a human,
and informativeness is defined as whether it con-
tains all the information in the gold standard utter-
ance. Each utterance is taken from the test folds of
the cross-validation experiment presented in Sec-
tion 5.2, i.e. the models are trained on up to 90%
of the data and the training set does not contain the
dialogue act being tested.
Results: Figs. 7 and 8 compare the naturalness
and informativeness scores of each system aver-
aged over all 202 dialogue acts. A paired t-test
shows that models trained on 40 utterances or
less produce utterances that are rated significantly
lower than human utterances for both naturalness
and informativeness (p &lt; .05, two-tailed). How-
ever, models trained on 100 utterances or more do
not perform significantly worse than human utter-
ances for both dimensions, with a mean difference
below .10 over 202 comparisons. Given the large
sample size, this result suggests that BAGEL can
successfully learn our domain using a fraction of
our initial dataset.
As far as the learning method is concerned, a
paired t-test shows that models trained on 20 and
40 utterances using active learning significantly
outperform models trained using random sam-
pling, for both dimensions (p &lt; .05). The largest
increase is observed using 20 utterances, i.e. the
naturalness increases by .49 and the informative-
ness by .37. When training on 100 utterances, the
effect of active learning becomes insignificant. In-
</bodyText>
<figure confidence="0.999656384615385">
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
Rand*m i
samp/ing
Active /earning k=1
Active /earning k=10
Fu(( (*gpr*b
L*gpr*b per frame
L*ngest input first
Mean BLEU score
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
</figure>
<page confidence="0.979564">
1558
</page>
<note confidence="0.92939">
Input inform(name(the Fountain) near(the Arts Picture House) area(centre) pricerange(cheap))
Human There is an inexpensive restaurant called the Fountain in the centre of town near the Arts Picture House
Rand-20 The Fountain is a restaurant near the Arts Picture House located in the city centre cheap price range
Rand-40 The Fountain is a restaurant in the cheap city centre area near the Arts Picture House
AL-20 The Fountain is a restaurant near the Arts Picture House in the city centre cheap
AL-40 The Fountain is an affordable restaurant near the Arts Picture House in the city centre
Full set The Fountain is a cheap restaurant in the city centre near the Arts Picture House
</note>
<table confidence="0.985287555555555">
Input reject(area(Barnwell) near(Saint Mary&apos;s Church))
Human I am sorry but I know of no venues near Saint Mary’s Church in the Barnwell area
Full set I am sorry but there are no venues near Saint Mary’s Church in the Barnwell area
Input inform(name(the Swan)area(Castle Hill) pricerange(expensive))
Human The Swan is a restaurant in Castle Hill if you are seeking something expensive
Full set The Swan is an expensive restaurant in the Castle Hill area
Input inform(name(Browns) area(centre) near(the Crowne Plaza) near(El Shaddai) pricerange(cheap))
Human Browns is an affordable restaurant located near the Crowne Plaza and El Shaddai in the centre of the city
Full set Browns is a cheap restaurant in the city centre near the Crowne Plaza and El Shaddai
</table>
<tableCaption confidence="0.962104">
Table 3: Example utterances for different input dialogue acts and system configurations. AL-20 = active
learning with 20 utterances, Rand = random sampling.
</tableCaption>
<figure confidence="0.996737214285714">
Mean informativeness score
4.5
3.5
2.5
1.5
4
5
3
2
1
5
3.60 3.84
4.07
4.00
3.97
4.01
3.50
3.11
Rand0m
Active learning
Human utterance= 4.07
4.5
4
3.5
3
2.5
2
Mean naturalness score
1.5
1
4.13
3.81
4.01
3.77
3.44
4.04
3.96 4.07
Rand0m
Active learning
Human utterance = 4.13
20 40 100 362
Training set size
</figure>
<figureCaption confidence="0.97525925">
Figure 7: Naturalness mean opinion scores for dif-
ferent training set sizes, using random sampling
and active learning. Differences for training set
sizes of 20 and 40 are all significant (p &lt; .05).
</figureCaption>
<figure confidence="0.977747">
20 40 100 362
Training set size
</figure>
<figureCaption confidence="0.7452535">
Figure 8: Informativeness mean opinion scores for
different training set sizes, using random sampling
and active learning. Differences for training set
sizes of 20 and 40 are all significant (p &lt; .05).
</figureCaption>
<bodyText confidence="0.999859052631579">
terestingly, while models trained on 100 utterances
outperform models trained on 40 utterances using
random sampling (p &lt; .05), they do not signifi-
cantly outperform models trained on 40 utterances
using active learning (p = .15 for naturalness and
p = .41 for informativeness). These results sug-
gest that certainty-based active learning is benefi-
cial for training a generator from a limited amount
of data given the domain size.
Looking back at the results presented in Sec-
tion 5.2, we find that the BLEU score correlates
with a Pearson correlation coefficient of .42 with
the mean naturalness score and .35 with the mean
informativeness score, over all folds of all systems
tested (n = 70, p &lt; .01). This is lower than
previous correlations reported by Reiter and Belz
(2009) in the shipping forecast domain with non-
expert judges (r = .80), possibly because our do-
main is larger and more open to subjectivity.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999873705882353">
While most previous work on trainable NLG re-
lies on a handcrafted component (see Section 1),
recent research has started exploring fully data-
driven NLG models.
Factored language models have recently been
used for surface realisation within the OpenCCG
framework (White et al., 2007; Espinosa et al.,
2008). More generally, chart generators for
different grammatical formalisms have been
trained from syntactic treebanks (White et al.,
2007; Nakanishi et al., 2005), as well as from
semantically-annotated treebanks (Varges and
Mellish, 2001). However, a major difference with
our approach is that BAGEL uses domain-specific
data to generate a surface form directly from se-
mantic concepts, without any syntactic annotation
(see Section 7 for further discussion).
</bodyText>
<page confidence="0.988587">
1559
</page>
<bodyText confidence="0.999915266666667">
This work is strongly related to Wong and
Mooney’s WASP−1 generation system (2007),
which combines a language model with an in-
verted synchronous CFG parsing model, effec-
tively casting the generation task as a translation
problem from a meaning representation to natu-
ral language. WASP−1 relies on GIZA++ to align
utterances with derivations of the meaning repre-
sentation (Och and Ney, 2003). Although early
experiments showed that GIZA++ did not perform
well on our data—possibly because of the coarse
granularity of our semantic representation—future
work should evaluate the generalisation perfor-
mance of synchronous CFGs in a dialogue system
domain.
Although we do not know of any work on ac-
tive learning for NLG, previous work has used
active learning for semantic parsing and informa-
tion extraction (Thompson et al., 1999; Tang et al.,
2002), spoken language understanding (Tur et al.,
2003), speech recognition (Hakkani-T¨ur et al.,
2002), word alignment (Sassano, 2002), and more
recently for statistical machine translation (Blood-
good and Callison-Burch, 2010). While certainty-
based methods have been widely used, future work
should investigate the performance of committee-
based active learning for NLG, in which examples
are selected based on the level of disagreement be-
tween models trained on subsets of the data (Fre-
und et al., 1997).
</bodyText>
<sectionHeader confidence="0.994752" genericHeader="discussions">
7 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.999955636363636">
This paper presents and evaluates BAGEL, a sta-
tistical language generator that can be trained en-
tirely from data, with no handcrafting required be-
yond the semantic annotation. All the required
subtasks—i.e. content ordering, aggregation, lex-
ical selection and realisation—are learned from
data using a unified model. To train BAGEL in a di-
alogue system domain, we propose a stack-based
semantic representation at the phrase level, which
is expressive enough to generate natural utterances
from unseen inputs, yet simple enough for data to
be collected from 42 untrained annotators with a
minimal normalisation step. A human evaluation
over 202 dialogue acts does not show any differ-
ence in naturalness and informativeness between
BAGEL’s outputs and human utterances. Addition-
ally, we find that the data collection process can
be optimised using active learning, resulting in a
significant increase in performance when training
data is limited, according to ratings from 18 hu-
man judges.6 These results suggest that the pro-
posed framework can largely reduce the develop-
ment time of NLG systems.
While this paper only evaluates the most likely
realisation given a dialogue act, we believe that
BAGEL’s probabilistic nature and generalisation
capabilities are well suited to model the linguis-
tic variation resulting from the diversity of annota-
tors. Our first objective is thus to evaluate the qual-
ity of BAGEL’s n-best outputs, and test whether
sampling from the output distribution can improve
naturalness and user satisfaction within a dialogue.
Our results suggest that explicitly modelling
syntax is not necessary for our domain, possi-
bly because of the lack of syntactic complexity
compared with formal written language. Never-
theless, future work should investigate whether
syntactic information can improve performance in
more complex domains. For example, the reali-
sation phrase can easily be conditioned on syntac-
tic constructs governing that phrase, and the recur-
sive nature of syntax can be modelled by keeping
track of the depth of the current embedded clause.
While syntactic information can be included with
no human effort by using syntactic parsers, their
robustness to dialogue system utterances must first
be evaluated.
Finally, recent years have seen HMM-based
synthesis models become competitive with unit se-
lection methods (Tokuda et al., 2000). Our long
term objective is to take advantage of those ad-
vances to jointly optimise the language genera-
tion and the speech synthesis process, by combin-
ing both components into a unified probabilistic
concept-to-speech generation model.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997097307692308">
S. Bangalore and O. Rambow. Exploiting a probabilistic hi-
erarchical model for generation. In Proceedings of the
18th International Conference on Computational Linguis-
tics (COLING), pages 42–48, 2000.
A. Belz. Automatic generation of weather forecast texts us-
ing comprehensive probabilistic generation-space models.
Natural Language Engineering, 14(4):431–455, 2008.
J. Bilmes and K. Kirchhoff. Factored language models and
generalized parallel backoff. In Proceedings of HLT-
NAACL, short papers, 2003.
J. Bilmes and G. Zweig. The Graphical Models ToolKit: An
open source software system for speech and time-series
processing. In Proceedings of ICASSP, 2002.
</reference>
<footnote confidence="0.967935666666667">
6The full training corpus and the generated
utterances used for evaluation are available at
http://mi.eng.cam.ac.uk/∼farm2/bagel.
</footnote>
<page confidence="0.974514">
1560
</page>
<reference confidence="0.999883439252336">
M. Bloodgood and C. Callison-Burch. Bucking the trend:
Large-scale cost-focused active learning for statistical ma-
chine translation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics
(ACL), 2010.
D. Espinosa, M. White, and D. Mehay. Hypertagging: Su-
pertagging for surface realization with CCG. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), 2008.
Y. Freund, H. S. Seung, E.Shamir, and N. Tishby. Selective
sampling using the query by committee algorithm. Ma-
chine Learning, 28:133–168, 1997.
D. Hakkani-T¨ur, G. Riccardi, and A. Gorin. Active learn-
ing for automatic speech recognition. In Proceedings of
ICASSP, 2002.
Y. He and S. Young. Semantic processing using the Hidden
Vector State model. Computer Speech &amp; Language, 19
(1):85–106, 2005.
A. Isard, C. Brockmann, and J. Oberlander. Individuality and
alignment in generated dialogues. In Proceedings of the
4th International Natural Language Generation Confer-
ence (INLG), pages 22–29, 2006.
I. Langkilde and K. Knight. Generation that exploits corpus-
based statistical knowledge. In Proceedings of the 36th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 704–710, 1998.
F. Lef`evre. A DBN-based multi-level stochastic spoken lan-
guage understanding system. In Proceedings of the IEEE
Workshop on Spoken Language Technology (SLT), 2006.
D. D. Lewis and J. Catlett. Heterogeneous uncertainty am-
pling for supervised learning. In Proceedings of ICML,
1994.
F. Mairesse and M. A. Walker. Trainable generation of Big-
Five personality styles through data-driven parameter esti-
mation. In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL), 2008.
H. Nakanishi, Y. Miyao, , and J. Tsujii. Probabilistic methods
for disambiguation of an HPSG-based chart generator. In
Proceedings of the IWPT, 2005.
F. J. Och and H. Ney. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19–51, 2003.
D. S. Paiva and R. Evans. Empirically-based control of nat-
ural language generation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational Lin-
guistics (ACL), pages 58–65, 2005.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a
method for automatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), 2002.
L. R. Rabiner. Tutorial on Hidden Markov Models and se-
lected applications in speech recognition. Proceedings of
the IEEE, 77(2):257–285, 1989.
E. Reiter and A. Belz. An investigation into the validity
of some metrics for automatically evaluating natural lan-
guage generation systems. Computational Linguistics, 25:
529–558, 2009.
V. Rieser and O. Lemon. Natural language generation as
planning under uncertainty for spoken dialogue systems.
In Proceedings of the Annual Meeting of the European
Chapter of the ACL (EACL), 2009.
M. Sassano. An empirical study of active learning with sup-
port vector machines for japanese word segmentation. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), 2002.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. Agenda-based user simulation for bootstrap-
ping a POMDP dialogue system. In Proceedings of HLT-
NAACL, short papers, pages 149–152, 2007.
A. Stolcke. SRILM – an extensible language modeling
toolkit. In Proceedings of the International Conference
on Spoken Language Processing, 2002.
M. Tang, X. Luo, and S. Roukos. Active learning for statis-
tical natural language parsing. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), 2002.
C. Thompson, M. E. Califf, and R. J. Mooney. Active learn-
ing for natural language parsing and information extrac-
tion. In Proceedings of ICML, 1999.
B. Thomson and S. Young. Bayesian update of dialogue state:
A POMDP framework for spoken dialogue systems. Com-
puter Speech &amp; Language, 24(4):562–588, 2010.
Y. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and
T. Kitamura. Speech parameter generation algorithms for
HMM-based speech synthesis. In Proceedings of ICASSP,
2000.
G. Tur, R. E. Schapire, and D. Hakkani-T¨ur. Active learn-
ing for spoken language understanding. In Proceedings of
ICASSP, 2003.
S. Varges and C. Mellish. Instance-based natural language
generation. In Proceedings of the Annual Meeting of the
North American Chapter of the ACL (NAACL), 2001.
M. A. Walker, O. Rambow, and M. Rogati. Training a sen-
tence planner for spoken dialogue using boosting. Com-
puter Speech and Language, 16(3-4), 2002.
M. White, R. Rajkumar, and S. Martin. Towards broad cov-
erage surface realization with CCG. In Proceedings of the
Workshop on Using Corpora for NLG: Language Genera-
tion and Machine Translation, 2007.
Y. W. Wong and R. Mooney. Generation by inverting a se-
mantic parser that uses statistical machine translation. In
Proceedings of HLT-NAACL, 2007.
S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and K. Yu. The Hidden Information State
model: a practical framework for POMDP-based spoken
dialogue management. Computer Speech and Language,
24(2):150–174, 2010.
</reference>
<page confidence="0.992753">
1561
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.329188">
<title confidence="0.992506">Phrase-based Statistical Language Generation using Graphical Models and Active Learning</title>
<author confidence="0.932187">Yu</author>
<affiliation confidence="0.691746">Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK</affiliation>
<email confidence="0.57171">mg436,fj228,sk561,brmt2,ky219,</email>
<abstract confidence="0.998560347826087">Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new do- This paper presents a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluashows that generate natand informative utterances from unin the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>42--48</pages>
<contexts>
<context position="1784" citStr="Bangalore and Rambow, 2000" startWordPosition="258" endWordPosition="261">o the human gold standard with a fraction of the data. 1 Introduction The field of natural language generation (NLG) is one of the last areas of computational linguistics to embrace statistical methods. Over the past decade, statistical NLG has followed two lines of research. The first one, pioneered by Langkilde and Knight (1998), introduces statistics in the generation process by training a model which reranks candidate outputs of a handcrafted generator. While their HALOGEN system uses an n-gram language model trained on news articles, other systems have used hierarchical syntactic models (Bangalore and Rambow, 2000), models trained on user ratings of This research was partly funded by the UK EPSRC under grant agreement EP/F013930/1 and funded by the EU FP7 Programme under grant agreement 216594 (CLASSiC project: www.classic-project.org). utterance quality (Walker et al., 2002), or alignment models trained on speaker-specific corpora (Isard et al., 2006). A second line of research has focused on introducing statistics at the generation decision level, by training models that find the set of generation parameters maximising an objective function, e.g. producing a target linguistic style (Paiva and Evans, 2</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>S. Bangalore and O. Rambow. Exploiting a probabilistic hierarchical model for generation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING), pages 42–48, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="2496" citStr="Belz, 2008" startWordPosition="369" endWordPosition="370">ment EP/F013930/1 and funded by the EU FP7 Programme under grant agreement 216594 (CLASSiC project: www.classic-project.org). utterance quality (Walker et al., 2002), or alignment models trained on speaker-specific corpora (Isard et al., 2006). A second line of research has focused on introducing statistics at the generation decision level, by training models that find the set of generation parameters maximising an objective function, e.g. producing a target linguistic style (Paiva and Evans, 2005; Mairesse and Walker, 2008), generating the most likely context-free derivations given a corpus (Belz, 2008), or maximising the expected reward using reinforcement learning (Rieser and Lemon, 2009). While such methods do not suffer from the computational cost of an overgeneration phase, they still require a handcrafted generator to define the generation decision space within which statistics can be used to find an optimal solution. This paper presents BAGEL (Bayesian networks for generation using active learning), an NLG system that can be fully trained from aligned data. While the main requirement of the generator is to produce natural utterances within a dialogue system domain, a second objective </context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>A. Belz. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):431–455, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>K Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL, short papers,</booktitle>
<contexts>
<context position="15011" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="2407" endWordPosition="2410">realisation model is illustrated in Fig. 2: realisation phrase r stack tail d Figure 2: Graphical model for the realisation phase. Dashed arrows indicate deterministic relations, and shaded node are observed. rt h t l t rt lt lt st st st |, , 1 , 1, 1, , 1, 1 − − + − + rt |ht, lt,rt−1,lt−1,lt+1,st rt |ht, lt,rt−1,lt−1,lt+1 Figure 3: Backoff graphs for the semantic decoding and realisation models. P(R|S∗) _ �L P(rt|rt−1, ht, lt−1, lt, lt+1, t=1 t+1) ∗ (8) Conditional probability distributions are represented as factored language models smoothed using Witten-Bell interpolated backoff smoothing (Bilmes and Kirchhoff, 2003), according to the backoff graphs in Fig. 3. Variables which are the furthest away in time are dropped first, and partial stack variables are dropped last as they are observed the most. It is important to note that generating unseen semantic stacks requires all possible mandatory semantic stacks in the target domain to be predefined, in order for all stack unigrams to be assigned a smoothed non-zero probability. 3.2 High cardinality concept abstraction While one should expect a trainable generator to learn multiple lexical realisations for lowcardinality semantic concepts, learning lexical rea</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>J. Bilmes and K. Kirchhoff. Factored language models and generalized parallel backoff. In Proceedings of HLTNAACL, short papers, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>G Zweig</author>
</authors>
<title>The Graphical Models ToolKit: An open source software system for speech and time-series processing.</title>
<date>2002</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<contexts>
<context position="12940" citStr="Bilmes and Zweig, 2002" startWordPosition="2061" endWordPosition="2064">or S* requires comparing sequences of different lengths. A consequence is that shorter sequences containing only mandatory stacks are likely to be favoured. While future work should investigate length normalisation strategies, we find that the learned transition probabilities are skewed enough to favour stack sequences including intermediary stacks. Once the topology and the decoding constraints of the network have been defined, any inference algorithm can be used to search for S* and R*. We use the junction tree algorithm implemented in the Graphical Model ToolKit (GMTK) for our experiments (Bilmes and Zweig, 2002), however both problems can be solved using a standard Viterbi search given the appropriate state representation. In terms of computational complexity, it is important to note that the number of stack sequences Seq(Sm) to search over increases exponentially with the number of input mandatory stacks. Nevertheless, we find that real-time performance can be achieved by pruning low probability sequences, without affecting the quality of the solution. 3.1 Generalisation to unseen semantic stacks In order to generalise to semantic stacks which have not been observed during training, the realisation </context>
<context position="17996" citStr="Bilmes and Zweig, 2002" startWordPosition="2891" endWordPosition="2894"> 2. Annotate the k semantic inputs fS;,...Sk } yielding the lowest realisation probability, i.e. for q E (L.k) S; = argmin (max P(R|S�)) (9) SmE�\{S1 m...S��1 � m � with P(R|S.) defined in (2). 3. Retrain the model with the additional k data points. The number of utterances to be queried k should depend on the flexibility of the annotators and the time required for generating all possible utterances in the domain. 5 Experimental method BAGEL’s factored language models are trained using the SRILM toolkit (Stolcke, 2002), and decoding is performed using GMTK’s junction tree inference algorithm (Bilmes and Zweig, 2002). 2Sampling methods can be used if Z is infinite or too large. Since each active learning iteration requires generating all training utterances in our domain, they are generated using a larger clique pruning threshold than the test utterances used for evaluation. 5.1 Corpus collection We train BAGEL in the context of a dialogue system providing information about restaurants in Cambridge. The domain contains two dialogue act types: (a) inform: presenting information about a restaurant (see Table 1), and (b) reject: informing that the user’s constraints cannot be met (e.g., ‘There is no cheap re</context>
</contexts>
<marker>Bilmes, Zweig, 2002</marker>
<rawString>J. Bilmes and G. Zweig. The Graphical Models ToolKit: An open source software system for speech and time-series processing. In Proceedings of ICASSP, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bloodgood</author>
<author>C Callison-Burch</author>
</authors>
<title>Bucking the trend: Large-scale cost-focused active learning for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="33185" citStr="Bloodgood and Callison-Burch, 2010" startWordPosition="5353" endWordPosition="5357">ZA++ did not perform well on our data—possibly because of the coarse granularity of our semantic representation—future work should evaluate the generalisation performance of synchronous CFGs in a dialogue system domain. Although we do not know of any work on active learning for NLG, previous work has used active learning for semantic parsing and information extraction (Thompson et al., 1999; Tang et al., 2002), spoken language understanding (Tur et al., 2003), speech recognition (Hakkani-T¨ur et al., 2002), word alignment (Sassano, 2002), and more recently for statistical machine translation (Bloodgood and Callison-Burch, 2010). While certaintybased methods have been widely used, future work should investigate the performance of committeebased active learning for NLG, in which examples are selected based on the level of disagreement between models trained on subsets of the data (Freund et al., 1997). 7 Discussion and conclusion This paper presents and evaluates BAGEL, a statistical language generator that can be trained entirely from data, with no handcrafting required beyond the semantic annotation. All the required subtasks—i.e. content ordering, aggregation, lexical selection and realisation—are learned from data</context>
</contexts>
<marker>Bloodgood, Callison-Burch, 2010</marker>
<rawString>M. Bloodgood and C. Callison-Burch. Bucking the trend: Large-scale cost-focused active learning for statistical machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Espinosa</author>
<author>M White</author>
<author>D Mehay</author>
</authors>
<title>Hypertagging: Supertagging for surface realization with CCG.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="31655" citStr="Espinosa et al., 2008" startWordPosition="5125" endWordPosition="5128">ith the mean informativeness score, over all folds of all systems tested (n = 70, p &lt; .01). This is lower than previous correlations reported by Reiter and Belz (2009) in the shipping forecast domain with nonexpert judges (r = .80), possibly because our domain is larger and more open to subjectivity. 6 Related work While most previous work on trainable NLG relies on a handcrafted component (see Section 1), recent research has started exploring fully datadriven NLG models. Factored language models have recently been used for surface realisation within the OpenCCG framework (White et al., 2007; Espinosa et al., 2008). More generally, chart generators for different grammatical formalisms have been trained from syntactic treebanks (White et al., 2007; Nakanishi et al., 2005), as well as from semantically-annotated treebanks (Varges and Mellish, 2001). However, a major difference with our approach is that BAGEL uses domain-specific data to generate a surface form directly from semantic concepts, without any syntactic annotation (see Section 7 for further discussion). 1559 This work is strongly related to Wong and Mooney’s WASP−1 generation system (2007), which combines a language model with an inverted synch</context>
</contexts>
<marker>Espinosa, White, Mehay, 2008</marker>
<rawString>D. Espinosa, M. White, and D. Mehay. Hypertagging: Supertagging for surface realization with CCG. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>H S Seung</author>
<author>E Shamir</author>
<author>N Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--133</pages>
<contexts>
<context position="33462" citStr="Freund et al., 1997" startWordPosition="5399" endWordPosition="5403"> work has used active learning for semantic parsing and information extraction (Thompson et al., 1999; Tang et al., 2002), spoken language understanding (Tur et al., 2003), speech recognition (Hakkani-T¨ur et al., 2002), word alignment (Sassano, 2002), and more recently for statistical machine translation (Bloodgood and Callison-Burch, 2010). While certaintybased methods have been widely used, future work should investigate the performance of committeebased active learning for NLG, in which examples are selected based on the level of disagreement between models trained on subsets of the data (Freund et al., 1997). 7 Discussion and conclusion This paper presents and evaluates BAGEL, a statistical language generator that can be trained entirely from data, with no handcrafting required beyond the semantic annotation. All the required subtasks—i.e. content ordering, aggregation, lexical selection and realisation—are learned from data using a unified model. To train BAGEL in a dialogue system domain, we propose a stack-based semantic representation at the phrase level, which is expressive enough to generate natural utterances from unseen inputs, yet simple enough for data to be collected from 42 untrained </context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Y. Freund, H. S. Seung, E.Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28:133–168, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hakkani-T¨ur</author>
<author>G Riccardi</author>
<author>A Gorin</author>
</authors>
<title>Active learning for automatic speech recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<marker>Hakkani-T¨ur, Riccardi, Gorin, 2002</marker>
<rawString>D. Hakkani-T¨ur, G. Riccardi, and A. Gorin. Active learning for automatic speech recognition. In Proceedings of ICASSP, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>Semantic processing using the Hidden Vector State model.</title>
<date>2005</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>19</volume>
<pages>1--85</pages>
<contexts>
<context position="5262" citStr="He and Young, 2005" startWordPosition="787" endWordPosition="790">nd informative utterances, according to 18 human judges. Finally, our human evaluation shows that training using active learning significantly improves generation performance on sparse datasets, yielding results close to the human gold standard using a fraction of the data. 2 Phrase-based generation from semantic stacks BAGEL uses a stack-based semantic representation to constrain the sequence of semantic concepts to be searched. This representation can be seen as a linearised semantic tree similar to the one previously used for natural language understanding in the Hidden Vector State model (He and Young, 2005). A stack representation provides useful generalisation properties (see Section 3.1), while the resulting stack sequences are relatively easy to align (see Section 5.1). In the context of dialogue systems, Table 1 illustrates how the input dialogue act is first mapped to a set of stacks of semantic concepts, and then aligned with a word sequence. The bottom concept in the stack will typically be a dialogue act type, e.g. an utterance providing information about the object under discussion (inform) or specifying that the request of the user cannot be met (reject). Other concepts include attribu</context>
<context position="7572" citStr="He and Young, 2005" startWordPosition="1150" endWordPosition="1153">ther than external linguistic knowledge about what constitutes a unit of meaning, i.e. contiguous words belonging to the same semantic stack are modelled as an atomic observation unit or phrase.1 In contrast with wordlevel models, a major advantage of phrase-based generation models is that they can model longrange dependencies and domain-specific idiomatic phrases with fewer parameters. 3 Dynamic Bayesian networks for NLG Dynamic Bayesian networks have been used successfully for speech recognition, natural language understanding, dialogue management and text-tospeech synthesis (Rabiner, 1989; He and Young, 2005; Lef`evre, 2006; Thomson and Young, 2010; Tokuda et al., 2000). Such models provide a principled framework for predicting elements in a large structured space, such as required for nontrivial NLG tasks. Additionally, their probabilistic nature makes them suitable for modelling linguistic variation, i.e. there can be multiple valid paraphrases for a given input. BAGEL models the generation task as finding the most likely sequence of realisation phrases R* = (r1...rL) given an unordered set of mandatory semantic stacks Sm, with |Sm |G L. BAGEL must thus derive the optimal sequence of semantic s</context>
</contexts>
<marker>He, Young, 2005</marker>
<rawString>Y. He and S. Young. Semantic processing using the Hidden Vector State model. Computer Speech &amp; Language, 19 (1):85–106, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Isard</author>
<author>C Brockmann</author>
<author>J Oberlander</author>
</authors>
<title>Individuality and alignment in generated dialogues.</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th International Natural Language Generation Conference (INLG),</booktitle>
<pages>22--29</pages>
<contexts>
<context position="2128" citStr="Isard et al., 2006" startWordPosition="310" endWordPosition="313">ics in the generation process by training a model which reranks candidate outputs of a handcrafted generator. While their HALOGEN system uses an n-gram language model trained on news articles, other systems have used hierarchical syntactic models (Bangalore and Rambow, 2000), models trained on user ratings of This research was partly funded by the UK EPSRC under grant agreement EP/F013930/1 and funded by the EU FP7 Programme under grant agreement 216594 (CLASSiC project: www.classic-project.org). utterance quality (Walker et al., 2002), or alignment models trained on speaker-specific corpora (Isard et al., 2006). A second line of research has focused on introducing statistics at the generation decision level, by training models that find the set of generation parameters maximising an objective function, e.g. producing a target linguistic style (Paiva and Evans, 2005; Mairesse and Walker, 2008), generating the most likely context-free derivations given a corpus (Belz, 2008), or maximising the expected reward using reinforcement learning (Rieser and Lemon, 2009). While such methods do not suffer from the computational cost of an overgeneration phase, they still require a handcrafted generator to define</context>
</contexts>
<marker>Isard, Brockmann, Oberlander, 2006</marker>
<rawString>A. Isard, C. Brockmann, and J. Oberlander. Individuality and alignment in generated dialogues. In Proceedings of the 4th International Natural Language Generation Conference (INLG), pages 22–29, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpusbased statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>704--710</pages>
<contexts>
<context position="1489" citStr="Langkilde and Knight (1998)" startWordPosition="214" endWordPosition="217">rs. A human evaluation shows that BAGEL can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data. 1 Introduction The field of natural language generation (NLG) is one of the last areas of computational linguistics to embrace statistical methods. Over the past decade, statistical NLG has followed two lines of research. The first one, pioneered by Langkilde and Knight (1998), introduces statistics in the generation process by training a model which reranks candidate outputs of a handcrafted generator. While their HALOGEN system uses an n-gram language model trained on news articles, other systems have used hierarchical syntactic models (Bangalore and Rambow, 2000), models trained on user ratings of This research was partly funded by the UK EPSRC under grant agreement EP/F013930/1 and funded by the EU FP7 Programme under grant agreement 216594 (CLASSiC project: www.classic-project.org). utterance quality (Walker et al., 2002), or alignment models trained on speake</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. Generation that exploits corpusbased statistical knowledge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL), pages 704–710, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Lef`evre</author>
</authors>
<title>A DBN-based multi-level stochastic spoken language understanding system.</title>
<date>2006</date>
<booktitle>In Proceedings of the IEEE Workshop on Spoken Language Technology (SLT),</booktitle>
<marker>Lef`evre, 2006</marker>
<rawString>F. Lef`evre. A DBN-based multi-level stochastic spoken language understanding system. In Proceedings of the IEEE Workshop on Spoken Language Technology (SLT), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty ampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of ICML,</booktitle>
<contexts>
<context position="4323" citStr="Lewis and Catlett, 1994" startWordPosition="642" endWordPosition="645">n 3 then details how our meaning representation is mapped to a phrase sequence, using a dynamic Bayesian network with backoff smoothing. Within a given domain, the same semantic concept can occur in different utterances. Section 4 details how BAGEL exploits this redundancy 1552 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552–1561, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics to improve generation performance on sparse datasets, by guiding the data collection process using certainty-based active learning (Lewis and Catlett, 1994). We train BAGEL in the information presentation domain, from a corpus of utterances produced by 42 untrained annotators (see Section 5.1). An automated evaluation metric is used to compare preliminary model and training configurations in Section 5.2, while Section 5.3 shows that the resulting system produces natural and informative utterances, according to 18 human judges. Finally, our human evaluation shows that training using active learning significantly improves generation performance on sparse datasets, yielding results close to the human gold standard using a fraction of the data. 2 Phr</context>
<context position="16956" citStr="Lewis and Catlett, 1994" startWordPosition="2716" endWordPosition="2719">the input specification. A consequence is that our model can only learn synonymous lexical realisations for enumerable attributes. 4 Certainty-based active learning A major issue with trainable NLG systems is the lack of availability of domain-specific data. It is therefore essential to produce NLG models that minimise the data annotation cost. BAGEL supports the optimisation of the data collection process through active learning, in which the next semantic input to annotate is determined by the current model. The probabilistic nature of BAGEL allows the use of certaintybased active learning (Lewis and Catlett, 1994), by querying the k semantic inputs for which the model is the least certain about its output realisation. Given a finite semantic input space Z representing all possible dialogue acts in our domain (i.e., the set of all sets of mandatory semantic stacks Sm), BAGEL’s active learning training process iterates over the following steps: 1. Generate an utterance for each semantic input S. E Z using the current model.2 2. Annotate the k semantic inputs fS;,...Sk } yielding the lowest realisation probability, i.e. for q E (L.k) S; = argmin (max P(R|S�)) (9) SmE�\{S1 m...S��1 � m � with P(R|S.) defin</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>D. D. Lewis and J. Catlett. Heterogeneous uncertainty ampling for supervised learning. In Proceedings of ICML, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mairesse</author>
<author>M A Walker</author>
</authors>
<title>Trainable generation of BigFive personality styles through data-driven parameter estimation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="2415" citStr="Mairesse and Walker, 2008" startWordPosition="355" endWordPosition="358">els trained on user ratings of This research was partly funded by the UK EPSRC under grant agreement EP/F013930/1 and funded by the EU FP7 Programme under grant agreement 216594 (CLASSiC project: www.classic-project.org). utterance quality (Walker et al., 2002), or alignment models trained on speaker-specific corpora (Isard et al., 2006). A second line of research has focused on introducing statistics at the generation decision level, by training models that find the set of generation parameters maximising an objective function, e.g. producing a target linguistic style (Paiva and Evans, 2005; Mairesse and Walker, 2008), generating the most likely context-free derivations given a corpus (Belz, 2008), or maximising the expected reward using reinforcement learning (Rieser and Lemon, 2009). While such methods do not suffer from the computational cost of an overgeneration phase, they still require a handcrafted generator to define the generation decision space within which statistics can be used to find an optimal solution. This paper presents BAGEL (Bayesian networks for generation using active learning), an NLG system that can be fully trained from aligned data. While the main requirement of the generator is t</context>
</contexts>
<marker>Mairesse, Walker, 2008</marker>
<rawString>F. Mairesse and M. A. Walker. Trainable generation of BigFive personality styles through data-driven parameter estimation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nakanishi</author>
<author>Y Miyao</author>
</authors>
<title>Probabilistic methods for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings of the IWPT,</booktitle>
<marker>Nakanishi, Miyao, 2005</marker>
<rawString>H. Nakanishi, Y. Miyao, , and J. Tsujii. Probabilistic methods for disambiguation of an HPSG-based chart generator. In Proceedings of the IWPT, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="32507" citStr="Och and Ney, 2003" startWordPosition="5253" endWordPosition="5256">1). However, a major difference with our approach is that BAGEL uses domain-specific data to generate a surface form directly from semantic concepts, without any syntactic annotation (see Section 7 for further discussion). 1559 This work is strongly related to Wong and Mooney’s WASP−1 generation system (2007), which combines a language model with an inverted synchronous CFG parsing model, effectively casting the generation task as a translation problem from a meaning representation to natural language. WASP−1 relies on GIZA++ to align utterances with derivations of the meaning representation (Och and Ney, 2003). Although early experiments showed that GIZA++ did not perform well on our data—possibly because of the coarse granularity of our semantic representation—future work should evaluate the generalisation performance of synchronous CFGs in a dialogue system domain. Although we do not know of any work on active learning for NLG, previous work has used active learning for semantic parsing and information extraction (Thompson et al., 1999; Tang et al., 2002), spoken language understanding (Tur et al., 2003), speech recognition (Hakkani-T¨ur et al., 2002), word alignment (Sassano, 2002), and more rec</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Paiva</author>
<author>R Evans</author>
</authors>
<title>Empirically-based control of natural language generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>58--65</pages>
<contexts>
<context position="2387" citStr="Paiva and Evans, 2005" startWordPosition="351" endWordPosition="354"> and Rambow, 2000), models trained on user ratings of This research was partly funded by the UK EPSRC under grant agreement EP/F013930/1 and funded by the EU FP7 Programme under grant agreement 216594 (CLASSiC project: www.classic-project.org). utterance quality (Walker et al., 2002), or alignment models trained on speaker-specific corpora (Isard et al., 2006). A second line of research has focused on introducing statistics at the generation decision level, by training models that find the set of generation parameters maximising an objective function, e.g. producing a target linguistic style (Paiva and Evans, 2005; Mairesse and Walker, 2008), generating the most likely context-free derivations given a corpus (Belz, 2008), or maximising the expected reward using reinforcement learning (Rieser and Lemon, 2009). While such methods do not suffer from the computational cost of an overgeneration phase, they still require a handcrafted generator to define the generation decision space within which statistics can be used to find an optimal solution. This paper presents BAGEL (Bayesian networks for generation using active learning), an NLG system that can be fully trained from aligned data. While the main requi</context>
</contexts>
<marker>Paiva, Evans, 2005</marker>
<rawString>D. S. Paiva and R. Evans. Empirically-based control of natural language generation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 58–65, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="21716" citStr="Papineni et al., 2002" startWordPosition="3466" endWordPosition="3469">cking and normalising the dataset,4 the layered annotations were automatically mapped to phrase-level semantic stacks by splitting the utterance into phrases at annotation boundaries. Each annotated utterance is then converted into a sequence of symbols such as in Table 2, which are used to estimate the conditional probability distributions defined in (6) and (8). The resulting vocabulary consists of 52 distinct semantic stacks and 109 distinct realisation phrases, with an average of 8.35 phrases per utterance. 5.2 BLEU score evaluation We first evaluate BAGEL using the BLEU automated metric (Papineni et al., 2002), which measures the word n-gram overlap between the generated utterances and the 2 reference paraphrases over a test corpus (with n up to 4). While BLEU suffers from known issues such as a bias towards statistical NLG systems (Reiter and Belz, 2009), it provides useful information when comparing similar systems. We evaluate BAGEL for different training set sizes, model dependencies, and active learning parameters. Our results are averaged over a 10-fold cross-validation over distinct dialogue acts, i.e. dialogue acts used for testing are not seen at training time,5 and all systems are tested </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>Tutorial on Hidden Markov Models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<contexts>
<context position="7552" citStr="Rabiner, 1989" startWordPosition="1148" endWordPosition="1149">aining data, rather than external linguistic knowledge about what constitutes a unit of meaning, i.e. contiguous words belonging to the same semantic stack are modelled as an atomic observation unit or phrase.1 In contrast with wordlevel models, a major advantage of phrase-based generation models is that they can model longrange dependencies and domain-specific idiomatic phrases with fewer parameters. 3 Dynamic Bayesian networks for NLG Dynamic Bayesian networks have been used successfully for speech recognition, natural language understanding, dialogue management and text-tospeech synthesis (Rabiner, 1989; He and Young, 2005; Lef`evre, 2006; Thomson and Young, 2010; Tokuda et al., 2000). Such models provide a principled framework for predicting elements in a large structured space, such as required for nontrivial NLG tasks. Additionally, their probabilistic nature makes them suitable for modelling linguistic variation, i.e. there can be multiple valid paraphrases for a given input. BAGEL models the generation task as finding the most likely sequence of realisation phrases R* = (r1...rL) given an unordered set of mandatory semantic stacks Sm, with |Sm |G L. BAGEL must thus derive the optimal se</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. Tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–285, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>A Belz</author>
</authors>
<title>An investigation into the validity of some metrics for automatically evaluating natural language generation systems.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<pages>529--558</pages>
<contexts>
<context position="21966" citStr="Reiter and Belz, 2009" startWordPosition="3510" endWordPosition="3513">ols such as in Table 2, which are used to estimate the conditional probability distributions defined in (6) and (8). The resulting vocabulary consists of 52 distinct semantic stacks and 109 distinct realisation phrases, with an average of 8.35 phrases per utterance. 5.2 BLEU score evaluation We first evaluate BAGEL using the BLEU automated metric (Papineni et al., 2002), which measures the word n-gram overlap between the generated utterances and the 2 reference paraphrases over a test corpus (with n up to 4). While BLEU suffers from known issues such as a bias towards statistical NLG systems (Reiter and Belz, 2009), it provides useful information when comparing similar systems. We evaluate BAGEL for different training set sizes, model dependencies, and active learning parameters. Our results are averaged over a 10-fold cross-validation over distinct dialogue acts, i.e. dialogue acts used for testing are not seen at training time,5 and all systems are tested on the same folds. The training and test sets respectively contain an average of 181 and 21 distinct dialogue acts, and each dialogue act is associated with two paraphrases, resulting in 362 training utterances. 4The normalisation process took around</context>
<context position="31200" citStr="Reiter and Belz (2009)" startWordPosition="5050" endWordPosition="5053">outperform models trained on 40 utterances using active learning (p = .15 for naturalness and p = .41 for informativeness). These results suggest that certainty-based active learning is beneficial for training a generator from a limited amount of data given the domain size. Looking back at the results presented in Section 5.2, we find that the BLEU score correlates with a Pearson correlation coefficient of .42 with the mean naturalness score and .35 with the mean informativeness score, over all folds of all systems tested (n = 70, p &lt; .01). This is lower than previous correlations reported by Reiter and Belz (2009) in the shipping forecast domain with nonexpert judges (r = .80), possibly because our domain is larger and more open to subjectivity. 6 Related work While most previous work on trainable NLG relies on a handcrafted component (see Section 1), recent research has started exploring fully datadriven NLG models. Factored language models have recently been used for surface realisation within the OpenCCG framework (White et al., 2007; Espinosa et al., 2008). More generally, chart generators for different grammatical formalisms have been trained from syntactic treebanks (White et al., 2007; Nakanishi</context>
</contexts>
<marker>Reiter, Belz, 2009</marker>
<rawString>E. Reiter and A. Belz. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 25: 529–558, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Natural language generation as planning under uncertainty for spoken dialogue systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the European Chapter of the ACL (EACL),</booktitle>
<contexts>
<context position="2585" citStr="Rieser and Lemon, 2009" startWordPosition="379" endWordPosition="382">6594 (CLASSiC project: www.classic-project.org). utterance quality (Walker et al., 2002), or alignment models trained on speaker-specific corpora (Isard et al., 2006). A second line of research has focused on introducing statistics at the generation decision level, by training models that find the set of generation parameters maximising an objective function, e.g. producing a target linguistic style (Paiva and Evans, 2005; Mairesse and Walker, 2008), generating the most likely context-free derivations given a corpus (Belz, 2008), or maximising the expected reward using reinforcement learning (Rieser and Lemon, 2009). While such methods do not suffer from the computational cost of an overgeneration phase, they still require a handcrafted generator to define the generation decision space within which statistics can be used to find an optimal solution. This paper presents BAGEL (Bayesian networks for generation using active learning), an NLG system that can be fully trained from aligned data. While the main requirement of the generator is to produce natural utterances within a dialogue system domain, a second objective is to minimise the overall development effort. In this regard, a major advantage of data-</context>
</contexts>
<marker>Rieser, Lemon, 2009</marker>
<rawString>V. Rieser and O. Lemon. Natural language generation as planning under uncertainty for spoken dialogue systems. In Proceedings of the Annual Meeting of the European Chapter of the ACL (EACL), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sassano</author>
</authors>
<title>An empirical study of active learning with support vector machines for japanese word segmentation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="33093" citStr="Sassano, 2002" startWordPosition="5344" endWordPosition="5345">entation (Och and Ney, 2003). Although early experiments showed that GIZA++ did not perform well on our data—possibly because of the coarse granularity of our semantic representation—future work should evaluate the generalisation performance of synchronous CFGs in a dialogue system domain. Although we do not know of any work on active learning for NLG, previous work has used active learning for semantic parsing and information extraction (Thompson et al., 1999; Tang et al., 2002), spoken language understanding (Tur et al., 2003), speech recognition (Hakkani-T¨ur et al., 2002), word alignment (Sassano, 2002), and more recently for statistical machine translation (Bloodgood and Callison-Burch, 2010). While certaintybased methods have been widely used, future work should investigate the performance of committeebased active learning for NLG, in which examples are selected based on the level of disagreement between models trained on subsets of the data (Freund et al., 1997). 7 Discussion and conclusion This paper presents and evaluates BAGEL, a statistical language generator that can be trained entirely from data, with no handcrafting required beyond the semantic annotation. All the required subtasks</context>
</contexts>
<marker>Sassano, 2002</marker>
<rawString>M. Sassano. An empirical study of active learning with support vector machines for japanese word segmentation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>S Young</author>
</authors>
<title>Agenda-based user simulation for bootstrapping a POMDP dialogue system.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL, short papers,</booktitle>
<pages>149--152</pages>
<contexts>
<context position="19058" citStr="Schatzmann et al., 2007" startWordPosition="3059" endWordPosition="3062"> inform: presenting information about a restaurant (see Table 1), and (b) reject: informing that the user’s constraints cannot be met (e.g., ‘There is no cheap restaurant in the centre’). Our domain contains 8 restaurant attributes: name, food, near, pricerange, postcode, phone, address, and area, out of which food, pricerange, and area are treated as enumerable.3 Our input semantic space is approximated by the set of information presentation dialogue acts produced over 20,000 simulated dialogues between our statistical dialogue manager (Young et al., 2010) and an agenda-based user simulator (Schatzmann et al., 2007), which results in 202 unique dialogue acts after replacing nonenumerable values by a generic symbol. Each dialogue act contains an average of 4.48 mandatory semantic stacks. As one of our objectives is to test whether BAGEL can learn from data provided by a large sample of untrained annotators, we collected a corpus of semantically-aligned utterances using Amazon’s Mechanical Turk data collection service. A crucial aspect of data collection for NLG is to ensure that the annotators understand the meaning of the semantics to be conveyed. Annotators were first asked to provide an utterance match</context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. Young. Agenda-based user simulation for bootstrapping a POMDP dialogue system. In Proceedings of HLTNAACL, short papers, pages 149–152, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<contexts>
<context position="17897" citStr="Stolcke, 2002" startWordPosition="2878" endWordPosition="2879">g steps: 1. Generate an utterance for each semantic input S. E Z using the current model.2 2. Annotate the k semantic inputs fS;,...Sk } yielding the lowest realisation probability, i.e. for q E (L.k) S; = argmin (max P(R|S�)) (9) SmE�\{S1 m...S��1 � m � with P(R|S.) defined in (2). 3. Retrain the model with the additional k data points. The number of utterances to be queried k should depend on the flexibility of the annotators and the time required for generating all possible utterances in the domain. 5 Experimental method BAGEL’s factored language models are trained using the SRILM toolkit (Stolcke, 2002), and decoding is performed using GMTK’s junction tree inference algorithm (Bilmes and Zweig, 2002). 2Sampling methods can be used if Z is infinite or too large. Since each active learning iteration requires generating all training utterances in our domain, they are generated using a larger clique pruning threshold than the test utterances used for evaluation. 5.1 Corpus collection We train BAGEL in the context of a dialogue system providing information about restaurants in Cambridge. The domain contains two dialogue act types: (a) inform: presenting information about a restaurant (see Table 1</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. SRILM – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tang</author>
<author>X Luo</author>
<author>S Roukos</author>
</authors>
<title>Active learning for statistical natural language parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="32963" citStr="Tang et al., 2002" startWordPosition="5325" endWordPosition="5328"> from a meaning representation to natural language. WASP−1 relies on GIZA++ to align utterances with derivations of the meaning representation (Och and Ney, 2003). Although early experiments showed that GIZA++ did not perform well on our data—possibly because of the coarse granularity of our semantic representation—future work should evaluate the generalisation performance of synchronous CFGs in a dialogue system domain. Although we do not know of any work on active learning for NLG, previous work has used active learning for semantic parsing and information extraction (Thompson et al., 1999; Tang et al., 2002), spoken language understanding (Tur et al., 2003), speech recognition (Hakkani-T¨ur et al., 2002), word alignment (Sassano, 2002), and more recently for statistical machine translation (Bloodgood and Callison-Burch, 2010). While certaintybased methods have been widely used, future work should investigate the performance of committeebased active learning for NLG, in which examples are selected based on the level of disagreement between models trained on subsets of the data (Freund et al., 1997). 7 Discussion and conclusion This paper presents and evaluates BAGEL, a statistical language generat</context>
</contexts>
<marker>Tang, Luo, Roukos, 2002</marker>
<rawString>M. Tang, X. Luo, and S. Roukos. Active learning for statistical natural language parsing. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Thompson</author>
<author>M E Califf</author>
<author>R J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML,</booktitle>
<contexts>
<context position="32943" citStr="Thompson et al., 1999" startWordPosition="5321" endWordPosition="5324">s a translation problem from a meaning representation to natural language. WASP−1 relies on GIZA++ to align utterances with derivations of the meaning representation (Och and Ney, 2003). Although early experiments showed that GIZA++ did not perform well on our data—possibly because of the coarse granularity of our semantic representation—future work should evaluate the generalisation performance of synchronous CFGs in a dialogue system domain. Although we do not know of any work on active learning for NLG, previous work has used active learning for semantic parsing and information extraction (Thompson et al., 1999; Tang et al., 2002), spoken language understanding (Tur et al., 2003), speech recognition (Hakkani-T¨ur et al., 2002), word alignment (Sassano, 2002), and more recently for statistical machine translation (Bloodgood and Callison-Burch, 2010). While certaintybased methods have been widely used, future work should investigate the performance of committeebased active learning for NLG, in which examples are selected based on the level of disagreement between models trained on subsets of the data (Freund et al., 1997). 7 Discussion and conclusion This paper presents and evaluates BAGEL, a statisti</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>C. Thompson, M. E. Califf, and R. J. Mooney. Active learning for natural language parsing and information extraction. In Proceedings of ICML, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Thomson</author>
<author>S Young</author>
</authors>
<title>Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="7613" citStr="Thomson and Young, 2010" startWordPosition="1156" endWordPosition="1159">dge about what constitutes a unit of meaning, i.e. contiguous words belonging to the same semantic stack are modelled as an atomic observation unit or phrase.1 In contrast with wordlevel models, a major advantage of phrase-based generation models is that they can model longrange dependencies and domain-specific idiomatic phrases with fewer parameters. 3 Dynamic Bayesian networks for NLG Dynamic Bayesian networks have been used successfully for speech recognition, natural language understanding, dialogue management and text-tospeech synthesis (Rabiner, 1989; He and Young, 2005; Lef`evre, 2006; Thomson and Young, 2010; Tokuda et al., 2000). Such models provide a principled framework for predicting elements in a large structured space, such as required for nontrivial NLG tasks. Additionally, their probabilistic nature makes them suitable for modelling linguistic variation, i.e. there can be multiple valid paraphrases for a given input. BAGEL models the generation task as finding the most likely sequence of realisation phrases R* = (r1...rL) given an unordered set of mandatory semantic stacks Sm, with |Sm |G L. BAGEL must thus derive the optimal sequence of semantic stacks S* that will appear in the utteranc</context>
</contexts>
<marker>Thomson, Young, 2010</marker>
<rawString>B. Thomson and S. Young. Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems. Computer Speech &amp; Language, 24(4):562–588, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tokuda</author>
<author>T Yoshimura</author>
<author>T Masuko</author>
<author>T Kobayashi</author>
<author>T Kitamura</author>
</authors>
<title>Speech parameter generation algorithms for HMM-based speech synthesis.</title>
<date>2000</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<contexts>
<context position="7635" citStr="Tokuda et al., 2000" startWordPosition="1160" endWordPosition="1163">s a unit of meaning, i.e. contiguous words belonging to the same semantic stack are modelled as an atomic observation unit or phrase.1 In contrast with wordlevel models, a major advantage of phrase-based generation models is that they can model longrange dependencies and domain-specific idiomatic phrases with fewer parameters. 3 Dynamic Bayesian networks for NLG Dynamic Bayesian networks have been used successfully for speech recognition, natural language understanding, dialogue management and text-tospeech synthesis (Rabiner, 1989; He and Young, 2005; Lef`evre, 2006; Thomson and Young, 2010; Tokuda et al., 2000). Such models provide a principled framework for predicting elements in a large structured space, such as required for nontrivial NLG tasks. Additionally, their probabilistic nature makes them suitable for modelling linguistic variation, i.e. there can be multiple valid paraphrases for a given input. BAGEL models the generation task as finding the most likely sequence of realisation phrases R* = (r1...rL) given an unordered set of mandatory semantic stacks Sm, with |Sm |G L. BAGEL must thus derive the optimal sequence of semantic stacks S* that will appear in the utterance given Sm, i.e. by in</context>
</contexts>
<marker>Tokuda, Yoshimura, Masuko, Kobayashi, Kitamura, 2000</marker>
<rawString>Y. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and T. Kitamura. Speech parameter generation algorithms for HMM-based speech synthesis. In Proceedings of ICASSP, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>R E Schapire</author>
<author>D Hakkani-T¨ur</author>
</authors>
<title>Active learning for spoken language understanding.</title>
<date>2003</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<marker>Tur, Schapire, Hakkani-T¨ur, 2003</marker>
<rawString>G. Tur, R. E. Schapire, and D. Hakkani-T¨ur. Active learning for spoken language understanding. In Proceedings of ICASSP, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Varges</author>
<author>C Mellish</author>
</authors>
<title>Instance-based natural language generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Chapter of the ACL (NAACL),</booktitle>
<contexts>
<context position="31891" citStr="Varges and Mellish, 2001" startWordPosition="5157" endWordPosition="5160">possibly because our domain is larger and more open to subjectivity. 6 Related work While most previous work on trainable NLG relies on a handcrafted component (see Section 1), recent research has started exploring fully datadriven NLG models. Factored language models have recently been used for surface realisation within the OpenCCG framework (White et al., 2007; Espinosa et al., 2008). More generally, chart generators for different grammatical formalisms have been trained from syntactic treebanks (White et al., 2007; Nakanishi et al., 2005), as well as from semantically-annotated treebanks (Varges and Mellish, 2001). However, a major difference with our approach is that BAGEL uses domain-specific data to generate a surface form directly from semantic concepts, without any syntactic annotation (see Section 7 for further discussion). 1559 This work is strongly related to Wong and Mooney’s WASP−1 generation system (2007), which combines a language model with an inverted synchronous CFG parsing model, effectively casting the generation task as a translation problem from a meaning representation to natural language. WASP−1 relies on GIZA++ to align utterances with derivations of the meaning representation (Oc</context>
</contexts>
<marker>Varges, Mellish, 2001</marker>
<rawString>S. Varges and C. Mellish. Instance-based natural language generation. In Proceedings of the Annual Meeting of the North American Chapter of the ACL (NAACL), 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>O Rambow</author>
<author>M Rogati</author>
</authors>
<title>Training a sentence planner for spoken dialogue using boosting. Computer Speech and Language,</title>
<date>2002</date>
<contexts>
<context position="2050" citStr="Walker et al., 2002" startWordPosition="298" endWordPosition="301">ch. The first one, pioneered by Langkilde and Knight (1998), introduces statistics in the generation process by training a model which reranks candidate outputs of a handcrafted generator. While their HALOGEN system uses an n-gram language model trained on news articles, other systems have used hierarchical syntactic models (Bangalore and Rambow, 2000), models trained on user ratings of This research was partly funded by the UK EPSRC under grant agreement EP/F013930/1 and funded by the EU FP7 Programme under grant agreement 216594 (CLASSiC project: www.classic-project.org). utterance quality (Walker et al., 2002), or alignment models trained on speaker-specific corpora (Isard et al., 2006). A second line of research has focused on introducing statistics at the generation decision level, by training models that find the set of generation parameters maximising an objective function, e.g. producing a target linguistic style (Paiva and Evans, 2005; Mairesse and Walker, 2008), generating the most likely context-free derivations given a corpus (Belz, 2008), or maximising the expected reward using reinforcement learning (Rieser and Lemon, 2009). While such methods do not suffer from the computational cost of</context>
</contexts>
<marker>Walker, Rambow, Rogati, 2002</marker>
<rawString>M. A. Walker, O. Rambow, and M. Rogati. Training a sentence planner for spoken dialogue using boosting. Computer Speech and Language, 16(3-4), 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>R Rajkumar</author>
<author>S Martin</author>
</authors>
<title>Towards broad coverage surface realization with CCG.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation,</booktitle>
<contexts>
<context position="31631" citStr="White et al., 2007" startWordPosition="5121" endWordPosition="5124">ness score and .35 with the mean informativeness score, over all folds of all systems tested (n = 70, p &lt; .01). This is lower than previous correlations reported by Reiter and Belz (2009) in the shipping forecast domain with nonexpert judges (r = .80), possibly because our domain is larger and more open to subjectivity. 6 Related work While most previous work on trainable NLG relies on a handcrafted component (see Section 1), recent research has started exploring fully datadriven NLG models. Factored language models have recently been used for surface realisation within the OpenCCG framework (White et al., 2007; Espinosa et al., 2008). More generally, chart generators for different grammatical formalisms have been trained from syntactic treebanks (White et al., 2007; Nakanishi et al., 2005), as well as from semantically-annotated treebanks (Varges and Mellish, 2001). However, a major difference with our approach is that BAGEL uses domain-specific data to generate a surface form directly from semantic concepts, without any syntactic annotation (see Section 7 for further discussion). 1559 This work is strongly related to Wong and Mooney’s WASP−1 generation system (2007), which combines a language mode</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>M. White, R. Rajkumar, and S. Martin. Towards broad coverage surface realization with CCG. In Proceedings of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. Mooney. Generation by inverting a semantic parser that uses statistical machine translation. In Proceedings of HLT-NAACL, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>M Gaˇsi´c</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The Hidden Information State model: a practical framework for POMDP-based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu. The Hidden Information State model: a practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2):150–174, 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>