<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012583">
<title confidence="0.9243405">
Using Non-lexical Features to Identify Effective Indexing Terms for
Biomedical Illustrations
</title>
<author confidence="0.7946165">
Matthew Simpson, Dina Demner-Fushman, Charles Sneiderman,
Sameer K. Antani, George R. Thoma
</author>
<affiliation confidence="0.8235065">
Lister Hill National Center for Biomedical Communications
National Library of Medicine, NIH, Bethesda, MD, USA
</affiliation>
<email confidence="0.981265">
{simpsonmatt, ddemner, csneiderman, santani, gthoma}@mail.nih.gov
</email>
<sectionHeader confidence="0.994624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939947368421">
Automatic image annotation is an attrac-
tive approach for enabling convenient ac-
cess to images found in a variety of docu-
ments. Since image captions and relevant
discussions found in the text can be useful
for summarizing the content of images, it
is also possible that this text can be used to
generate salient indexing terms. Unfortu-
nately, this problem is generally domain-
specific because indexing terms that are
useful in one domain can be ineffective
in others. Thus, we present a supervised
machine learning approach to image an-
notation utilizing non-lexical features1 ex-
tracted from image-related text to select
useful terms. We apply this approach to
several subdomains of the biomedical sci-
ences and show that we are able to reduce
the number of ineffective indexing terms.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995353625">
Authors of biomedical publications often utilize
images and other illustrations to convey informa-
tion essential to the article and to support and re-
inforce textual content. These images are useful
in support of clinical decisions, in rich document
summaries, and for instructional purposes. The
task of delivering these images, and the publica-
tions in which they are contained, to biomedical
clinicians and researchers in an accessible way is
an information retrieval problem.
Current research in the biomedical domain (e.g.,
Antani et al., 2008; Florea et al., 2007), has in-
vestigated hybrid approaches to image retrieval,
combining elements of content-based image re-
trieval (CBIR) and annotation-based image re-
trieval (ABIR). ABIR, compared to the image-
1Non-lexical features describe attributes of image-related
text but not the text itself, e.g., unlike a bag-of-words model.
only approach of CBIR, offers a practical advan-
tage in that queries can be more naturally specified
by a human user (Inoue, 2004). However, manu-
ally annotating biomedical images is a laborious
and subjective task that often leads to noisy results.
Automatic image annotation is a more robust
approach to ABIR than manual annotation. Un-
fortunately, automatically selecting the most ap-
propriate indexing terms is an especially challeng-
ing problem for biomedical images because of
the domain-specific nature of these images and
the many vocabularies used in the biomedical sci-
ences. For example, the term “sweat gland adeno-
carcinoma” could be a useful indexing term for an
image found in a dermatology publication, but it is
less likely to have much relevance in describing an
image from a cardiology publication. On the other
hand, the term “mitral annular calcification” may
be of great relevance for cardiology images, but of
little relevance for dermatology ones.
Our problem may be summarized as follows:
Given an image, its caption, its discussion in the
article text (henceforth the image mention), and a
list of potential indexing terms, select the terms
that are most effective at describing the content of
the image. For example, assume the image shown
in Figure 1, obtained from the article “Metastatic
Hidradenocarcinoma: Efficacy of Capecitabine”
by Thomas et al. (2006) in Archives of Dermatol-
ogy, has the following potential indexing terms,
</bodyText>
<listItem confidence="0.999825875">
• Histopathology finding
• Reviewed
• Confirmation
• Diagnosis aspect
• Diagnosis
• Eccrine
• Sweat gland adenocarcinoma
• Lesion
</listItem>
<bodyText confidence="0.959482">
which have been extracted from the image men-
tion. While most of these do not uniquely identify
</bodyText>
<note confidence="0.9237545">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 737–744,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.988254">
737
</page>
<figureCaption confidence="0.991288">
Figure 1: Example Image. We index an image
</figureCaption>
<bodyText confidence="0.996969428571429">
with concepts generated from its caption and dis-
cussion in the document text (mention). This im-
age is from “Metastatic Hidradenocarcinoma: Ef-
ficacy of Capecitabine” by Thomas et al. (2006)
and is reprinted with permission from the authors.
the image, we would like to automatically select
“sweat gland adenocarcinoma” and “eccrine” for
indexing because they clearly describe the content
and purpose of the image—supporting a diagno-
sis of hidradenocarinoma, an invasive cancer of
sweat glands. Note that effective indexing terms
need not be exact lexical matches of the text. Even
though “diagnosis” is an exact match, its meaning
is too broad in this context to be a useful term.
In a machine learning approach to image anno-
tation, training data based on lexical features alone
is not sufficient for finding salient indexing terms.
Indeed, we must classify terms that are not en-
countered while training. Therefore, we hypoth-
esize that non-lexical features, which have been
successfully used for speech and genre classifica-
tion tasks, among others (see Section 5 for related
work), may be useful in classifying text associated
with images. While this approach is broad enough
to apply to any retrieval task, given the goals of our
ongoing research, we restrict ourselves to studying
its feasibility in the biomedical domain.
In order to achieve this, we make use of the
previously developed MetaMap (Aronson, 2001)
tool, which maps text to concepts contained in
the Unified Medical Language System® (UMLS)
Metathesaurus ® (Lindberg et al., 1993). The
UMLS is a compendium of several controlled vo-
cabularies in the biomedical sciences that provides
a semantic mapping relating concepts from the
various vocabularies (Section 2). We then use a su-
pervised machine learning approach, described in
Section 3, to classify the UMLS concepts as useful
indexing terms based on their non-lexical features,
gleaned from the article text and MetaMap output.
Experimental results, presented in Section 4, in-
dicate that ineffective indexing terms can be re-
duced using this classification technique. We con-
clude that ABIR approaches to biomedical im-
age retrieval as well as hybrid CBIR/ABIR ap-
proaches, which rely on both image content and
annotations, can benefit from an automatic anno-
tation process utilizing non-lexical features to aid
in the selection of useful indexing terms.
</bodyText>
<sectionHeader confidence="0.985722" genericHeader="method">
2 Image Retrieval: Recent Work
</sectionHeader>
<bodyText confidence="0.999946892857143">
Automatic image annotation is a broad topic, and
the automatic annotation of biomedical images,
specifically, has been a frequent component of
the ImageCLEF2 cross-language image retrieval
workshop. In this section, we describe previous
work in biomedical image retrieval that forms the
basis of our approach. Refer to Section 5 for work
related to our method in general.
Demner-Fushman et al. (2007) developed a ma-
chine learning approach to identify images from
biomedical publications that are relevant to clin-
ical decision support. In this work, the authors
utilized both image and textual features to clas-
sify images based on their usefulness in evidence-
based medicine. In contrast, our work is focused
on selecting useful biomedical image indexing
terms; however, we utilize the methods developed
in their work to extract images and their related
captions and mentions.
Authors of biomedical publications often as-
semble multiple images into a single multi-panel
figure. Antani et al. (2008) developed a unique
two-phase approach for detecting and segmenting
these figures. The authors rely on cues from cap-
tions to inform an image analysis algorithm that
determines panel edge information. We make use
of this approach to uniquely associate caption and
mention text with a single image.
</bodyText>
<footnote confidence="0.855267">
2http://imageclef.org/
</footnote>
<bodyText confidence="0.954715428571429">
Caption: Figure 1. On recurrence, histologic features
of porocarcinoma with an intraepidermal spread of
neoplastic clusters (hematoxylin-eosin, original magni-
fication x100).
Mention: Histopathologic findings were reviewed
and confirmed a diagnosis of eccrine hidradenocarci-
noma for all lesions excised (Figure 1).
</bodyText>
<page confidence="0.996193">
738
</page>
<bodyText confidence="0.999642275862069">
Our current work most directly stems from the
results of a term extraction and image annota-
tion evaluation performed by Demner-Fushman
et al. (2008). In this study, the authors uti-
lized MetaMap to extract potential indexing terms
(UMLS concepts) from image captions and men-
tions. They then asked a group of five physicians
and one medical imaging specialist (four of whom
are trained in medical informatics) to manually
classify each concept as being “useful for index-
ing” its associated images or ineffective for this
purpose. The reviewers also had the opportunity
to identify additional indexing terms that were not
automatically extracted by MetaMap.
In total, the reviewers evaluated 4006 concepts
(3,281 of which were unique), associated with
186 images from 109 different biomedical articles.
Each reviewer was given 50 randomly chosen im-
ages from the 2006–2007 issues of Archives of Fa-
cial Plastic Surgery3 and Cardiovascular Ultra-
sound4. Since MetaMap did not automatically ex-
tract all of the useful indexing terms, this selection
process exhibited high recall averaging 0.64 but
a low precision of 0.11. Indeed, assuming all the
extracted terms were selected for indexing, this re-
sults in an average F1-score of only 0.182 for the
classification problem. Our work is aimed at im-
proving this baseline classification by reducing the
number of ineffective terms selected for indexing.
</bodyText>
<sectionHeader confidence="0.992224" genericHeader="method">
3 Term Selection Method
</sectionHeader>
<bodyText confidence="0.999892888888889">
A pictorial representation of our term extraction
and selection process is shown in Figure 2. We
rely on the previously described methods to ex-
tract images and their corresponding captions and
mentions, and the MetaMap tool to map this text
to UMLS concepts. These concepts are potential
indexing terms for the associated image.
We derive term features from various textual
items, such as the preferred name of the UMLS
concept, the MetaMap output for the concept, the
text that generated the concept, the article contain-
ing the image, and the document collection con-
taining the article. These are all described in more
detail in Section 3.2. Once the feature vectors are
built, we automatically classify the term as either
being useful for indexing the image or not.
To select useful indexing terms, we trained a
binary classifier, described in Section 3.3, in a
</bodyText>
<footnote confidence="0.998535">
3http://archfaci.ama-assn.org/
4http://www.cardiovascularultrasound.com/
</footnote>
<figureCaption confidence="0.975576">
Figure 2: Term Extraction and Selection. We
</figureCaption>
<bodyText confidence="0.999825958333333">
gather features for the extracted terms and use
them to train a classifier that selects the terms that
are useful for indexing the associated images.
supervised learning scenario with data obtained
from the previous study by Demner-Fushman et al.
(2008). We obtained our evaluation data from the
2006 Archives of Dermatology5 journal. Note that
our training and evaluation data represent distinct
subdomains of the biomedical sciences.
In order to reduce noise in the classification of
our evaluation data, we asked two of the review-
ers who participated in the initial study to man-
ually classify our extracted terms as they did for
our training data. In doing so, they each eval-
uated an identical set of 1539 potential indexing
terms relating to 50 randomly chosen images from
31 different articles. We measured the perfor-
mance of our classifier in terms of how well it per-
formed against this manual evaluation. These re-
sults, as well as a discussion pertaining to the inter-
annotator agreement of the two reviewers, are pre-
sented in Section 4.
Since our general approach is not specific to the
biomedical domain, it could equally be applied in
</bodyText>
<footnote confidence="0.974966">
5http://archderm.ama-assn.org/
</footnote>
<page confidence="0.997321">
739
</page>
<bodyText confidence="0.999885">
any domain with an existing ontology. For exam-
ple, the UMLS and MetaMap can be replaced by
the Art and Architecture Thesaurus6 and an equiv-
alent mapping tool to annotate images related to
art and art history (Klavans et al., 2008).
</bodyText>
<subsectionHeader confidence="0.990072">
3.1 Terminology
</subsectionHeader>
<bodyText confidence="0.9987535">
To describe our features, we adopt the following
terminology.
</bodyText>
<listItem confidence="0.992987058823529">
• A collection contains all the articles from a
given publication for a specified number of
years. For example, the 2006–2007 issues of
Cardiovascular Ultrasound represent a sin-
gle collection.
• A document is a specific biomedical article
from a particular collection and contains im-
ages and their captions and mentions.
• A phrase is the portion of text that MetaMap
maps to UMLS concepts. For example, from
the caption in Figure 1, the noun phrase “his-
tologic features” maps to four UMLS con-
cepts: “Histologic,” “Characteristics,” “Pro-
tein Domain” and “Array Feature.”
• A mapping is an assignment of a phrase to
a particular set of UMLS concepts. Each
phrase can have more than one mapping.
</listItem>
<subsectionHeader confidence="0.952026">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999793210526316">
Using this terminology, we define the following
features used to classify potential indexing terms.
We refer to these as non-lexical features because
they generally characterize UMLS concepts, go-
ing beyond the surface representation of words
and lexemes appearing in the article text.
F.1 CUI (nominal): The Concept Unique Iden-
tifier (CUI) assigned to the concept in the
UMLS Metathesaurus. We choose the con-
cept identifier as a feature because some fre-
quently mapped concepts are consistently
ineffective for indexing the images in our
training and evaluation data. For exam-
ple, the CUI for “Original,” another term
mapped from the caption shown in Figure
1, is “C0205313.” Our results indicate that
“C0205313,” which occurs 19 times in our
evaluation data, never identifies a useful in-
dexing term.
</bodyText>
<footnote confidence="0.9822685">
6http://www.getty.edu/research/conducting research/
vocabularies/aat/
</footnote>
<bodyText confidence="0.988244571428571">
F.2 Semantic Type (nominal): The concept’s se-
mantic categorization. There are currently
132 different semantic types7 in the UMLS
Metathesaurus. For example, The semantic
type of “Original” is “Idea or Concept.”
F.3 Presence in Caption (nominal): true if the
phrase that generated the concept is located
in the image caption; false if the phrase is
located in the image mention.
F.4 MeSH Ratio (real): The ratio of words ci in
the concept c that are also contained in the
Medical Subject Headings (MeSH terms)8
M assigned to the document to the total
number of words in the concept.
</bodyText>
<equation confidence="0.7822575">
R(m) =|{ci : ci ∈ M} |(1)
|c|
</equation>
<figureCaption confidence="0.69429">
MeSH is a controlled vocabulary created by
the US National Library of Medicine (NLM)
to index biomedical articles. For example,
“Adenoma, Sweat” is one MeSH term as-
signed to “Metastatic Hidradenocarcinoma:
Efficacy of Capecitabine” (Thomas et al.,
2006), the article containing the image from
Figure 1.
F.5 Abstract Ratio (real): The ratio of words
ci in the concept c that are also in the doc-
ument’s abstract A to the total number of
words in the concept.
</figureCaption>
<equation confidence="0.978257">
R(a) = |{ci : ci ∈ A} |(2)
|c|
</equation>
<bodyText confidence="0.985213">
F.6 Title Ratio (real): The ratio of words ci in
the concept c that are also in the document’s
title T to the total number of words in the
concept.
</bodyText>
<equation confidence="0.9424195">
(3)
|c|
</equation>
<bodyText confidence="0.9853035">
F.7 Parts-of-Speech Ratio (real): The ratio of
words pi in the phrase p that have been
tagged as having part of speech s to the total
number of words in the phrase.
</bodyText>
<equation confidence="0.9796575">
R(s) =|{pi : TAG(pi) = s} |(4)
|p|
</equation>
<bodyText confidence="0.9884785">
This feature is computed for noun, verb, ad-
jective and adverb part-of-speech tags. We
</bodyText>
<footnote confidence="0.98066">
7http://www.nlm.nih.gov/research/umls/META3 current
semantic types.html
8http://www.nlm.nih.gov/mesh/
</footnote>
<equation confidence="0.989375">
R(t) = |{ci : ci ∈ T }|
</equation>
<page confidence="0.946822">
740
</page>
<bodyText confidence="0.999475166666667">
obtain tagging information from the output
of MetaMap.
F.8 Concept Ambiguity (real): The ratio of the
number of mappings mi of phrase p that con-
tain concept c to the total number of map-
pings for the phrase:
</bodyText>
<equation confidence="0.9461765">
A = |{mpi : c E mpi } |(5)
|mp|
</equation>
<bodyText confidence="0.995304666666667">
F.9 Tf-idf (real): The frequency of term ti (i.e.,
the phrase that generated the concept) times
its inverse document frequency:
</bodyText>
<equation confidence="0.99416">
tfidfi,j = tfi,j x idfi (6)
The term frequency tfi,j of term ti in docu-
ment dj is given by
ni,j
tfi,j = (7)
E|k=1 nk,j
</equation>
<bodyText confidence="0.99971025">
where ni,j is the number of occurrences of ti
in dj, and the denominator is the number of
occurrences of all terms in dj. The inverse
document frequency idfi of ti is given by
</bodyText>
<equation confidence="0.998274">
idfi = log |D |(8)
|{dj : ti E dj}|
</equation>
<bodyText confidence="0.999027470588235">
where |D |is the total number of documents
in the collection, and the denominator is the
total number of documents that contain ti
(see Salton and Buckley, 1988).
F.10 Document Location (real): The location in
the document of the phrase that generated
the concept. This feature is continuous on
[0, 1] with 0 representing the beginning of
the document and 1 representing the end.
F.11 Concept Length (real): The length of the
concept, measured in number of characters.
For the purpose of computing F.9 and F.10, we in-
dexed each collection with the Terrier9 informa-
tion retrieval platform. Terrier was configured to
use a block indexing scheme with a Tf-idf weight-
ing model. Computation of all other features is
straightforward.
</bodyText>
<subsectionHeader confidence="0.977335">
3.3 Classifier
</subsectionHeader>
<bodyText confidence="0.977792333333333">
We explored these feature vectors using various
classification approaches available in the Rapid-
Miner10 tool. Unlike many similar text and image
</bodyText>
<footnote confidence="0.99802">
9http://ir.dcs.gla.ac.uk/terrier/
10http://rapid-i.com/
</footnote>
<bodyText confidence="0.995005058823529">
classification problems, we were unable to achieve
results with a Support Vector Machine (SVM)
learner (libSVMLearner) using the Radial Base
Function (RBF). Common cost and width parame-
ters were used, yet the SVM classified all terms as
ineffective. Identical results were observed using
a Naive Bayes (NB) learner.
For these reasons, we chose to use the Aver-
aged One-Dependence Estimator (AODE) learner
(Webb et al., 2005) available in RapidMiner.
AODE is capable of achieving highly accurate
classification results with the quick training time
usually associated with NB. Because this learner
does not handle continuous attributes, we pre-
processed our features with equal frequency dis-
cretization. The AODE learner was trained in a
ten-fold cross validation of our training data.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.980292666666667">
Results relating to specific aspects of our work
(annotation, features and classification) are pre-
sented below.
</bodyText>
<subsectionHeader confidence="0.721646">
4.1 Inter-Annotator Agreement
</subsectionHeader>
<bodyText confidence="0.999920363636364">
Two independent reviewers manually classified
the extracted terms from our evaluation data as
useful for indexing their associated images or not.
The inter-annotator agreement between reviewers
A and B is shown in the first row of Table 1. Al-
though both reviewers are physicians trained in
medical informatics, their initial agreement is only
moderate, with n = 0.519. This illustrates the
subjective nature of manual ABIR and, in general,
the difficultly in reliably classifying potential in-
dexing terms for biomedical images.
</bodyText>
<table confidence="0.99741375">
Annotator Pr(a) Pr(e) r.
A/B 0.847 0.682 0.519
A/Standard 0.975 0.601 0.938
B/Standard 0.872 0.690 0.586
</table>
<tableCaption confidence="0.996884">
Table 1: Inter-annotator Agreement. The prob-
</tableCaption>
<bodyText confidence="0.971069125">
ability of agreement Pr(a), expected probability of
chance agreement Pr(e), and the associated Co-
hen’s kappa coefficient n are given for each re-
viewer combination.
After their initial classification, the two review-
ers were instructed to collaboratively reevaluate
the subset of extracted terms upon which they dis-
agreed (roughly 15% of the terms) and create a
</bodyText>
<page confidence="0.99459">
741
</page>
<table confidence="0.999952333333333">
Feature Gain x2
F.1 CUI 0.003 13.331
F.2 Semantic Type 0.015 68.232
F.3 Presence in Caption 0.008 35.303
F.4 MeSH Ratio 0.043 285.701
F.5 Abstract Ratio 0.023 114.373
F.6 Title Ratio 0.021 132.651
F.7 Noun Ratio 0.053 287.494
Verb Ratio 0.009 26.723
Adjective Ratio 0.021 96.572
Adverb Ratio 0.002 5.271
F.8 Concept Ambiguity 0.008 33.824
F.9 Tf-idf 0.004 21.489
F.10 Document Location 0.002 12.245
F.11 Phrase Length 0.021 102.759
</table>
<tableCaption confidence="0.995546">
Table 2: Feature Comparison. The information
</tableCaption>
<bodyText confidence="0.913096090909091">
gain and chi-square statistic is shown for each fea-
ture. A higher score indicates greater influence on
term effectiveness.
gold standard evaluation. The second and third
rows of Table 1 suggest the resulting evaluation
strongly favors reviewer A’s initial classification
compared to that of reviewer B.
Since the reviewers of the training data each
classified terms from different sets of randomly
selected images, it is impossible to calculate their
inter-annotator agreement.
</bodyText>
<subsectionHeader confidence="0.999689">
4.2 Effectiveness of Features
</subsectionHeader>
<bodyText confidence="0.99998202631579">
The effectiveness of individual features in describ-
ing the potential indexing terms is shown in Ta-
ble 2. We used two measures, both of which in-
dicate a similar trend, to calculate feature effec-
tiveness: Information gain (Kullback-Leibler di-
vergence) and the chi-square statistic.
Under both measures, the MeSH ratio (F.4) is
one of the most effective features. This makes
intuitive sense because MeSH terms are assigned
to articles by specially trained NLM profession-
als. Given the large size of the MeSH vocabu-
lary, it is not unreasonable to assume that an arti-
cle’s MeSH terms could be descriptive, at a coarse
granularity, of the images it contains. Also, the
subjectivity of the reviewers’ initial data calls into
question the usefulness of our training data. It
may be that MeSH terms, consistently assigned
to all documents in a particular collection, are a
more reliable determiner of the usefulness of po-
tential indexing terms. Furthermore, the study by
Demner-Fushman et al. (2008) found that, on aver-
age, roughly 25% of the additional (useful) terms
the reviewers added to the set of extracted terms
were also found in the MeSH terms assigned to
the document containing the particular image.
The abstract and title ratios (F.6 and F.5) also
had a significant effect on the classification out-
come. Similar to the argument for MeSH terms, as
these constructs are a coarse summary of the con-
tents of an article, it is not unreasonable to assume
they summarize the images contained therein.
Finally, the noun ratio (F.7) was a particularly
effective feature, and the length of the UMLS con-
cept (F.11) was moderately effective. Interest-
ingly, tf-idf and document location (F.9 and F.10),
both features computed using standard informa-
tion retrieval techniques, are among the least ef-
fective features.
</bodyText>
<subsectionHeader confidence="0.996676">
4.3 Classification
</subsectionHeader>
<bodyText confidence="0.999969928571429">
While the AODE learner performed reasonably
well for this task, the difficulty encountered when
training the SVM learner may be explained as
follows. The initial inter-annotator agreement
of the evaluation data suggests that it is likely
that our training data contained contradictory or
mislabeled observations, preventing the construc-
tion of a maximal-margin hyperplane required by
the SVM. An SVM implementation utilizing soft
margins (Cortes and Vapnik, 1995) would likely
achieve better results on our data, although at the
expense of greater training time. The success of
the AODE learner in this case is probably due to
its resilience to mislabeled observations.
</bodyText>
<table confidence="0.999705714285714">
Annotator Precision Recall F1-score
A 0.258 0.442 0.326
B 0.200 0.225 0.212
Combined 0.326 0.224 0.266
Standard 0.453 0.229 0.304
Standarda 0.492 0.231 0.314
Training 0.502 0.332 0.400
</table>
<tableCaption confidence="0.999227">
Table 3: Classification Results. The classifier’s
</tableCaption>
<bodyText confidence="0.963475333333333">
precision and recall, as well as the corresponding
Fl-score, are given for the responses of each re-
viewer.
aFor comparison, the classifier was also trained using the
subset of training data containing responses from reviewers
A and B only.
</bodyText>
<page confidence="0.992894">
742
</page>
<bodyText confidence="0.999984697674419">
Classification results are shown in Table 3. The
precision and recall of the classification scheme is
shown for the manual classification by reviewers
A and B in the first and second rows. The third
row contains the results obtained from combining
the results of the two reviewers, and the fourth row
shows the classification results compared to the
gold standard obtained after discovering the initial
inter-annotator agreement.
We hypothesized that the training data labels
may have been highly sensitive to the subjectiv-
ity of the reviewers. Therefore, we retrained the
learner with only those observations made by re-
viewers A and B (of the five total reviewers) and
again compared the classification results with the
gold standard. Not surprisingly, the F1-score of
this classification (shown in the fifth row) is some-
what improved compared to that obtained when
utilizing the full training set.
The last row in Table 3 shows the results of clas-
sifying the training data. That is, it shows the re-
sults of classifying one tenth of the data after a ten-
fold cross validation and can be considered an up-
per bound for the performance of this classifier on
our evaluation data. Notice that the associated F1-
score for this experiment is only marginally bet-
ter than that of the unseen data. This implies that
it is possible to use training data from particular
subdomains of the biomedical sciences (cardiol-
ogy and plastic surgery) to classify potential in-
dexing terms in other subdomains (dermatology).
Overall, the classifier performed best when ver-
ified with reviewer A, with an F1-score of 0.326.
Although this is relatively low for a classification
task, these results improve upon the baseline clas-
sification scheme (all extracted terms are useful
for indexing) with an F1-score of 0.182 (Demner-
Fushman et al., 2008). Thus, non-lexical features
can be leveraged, albeit to a small degree with
our current features and classifier, in automatically
selecting useful image indexing terms. In future
work, we intend to explore additional features and
alternative tools for mapping text to the UMLS.
</bodyText>
<sectionHeader confidence="0.999969" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999897078947368">
Non-lexical features have been successful in many
contexts, particularly in the areas of genre classifi-
cation and text and speech summarization.
Genre classification, unlike text classification,
discriminates between document style instead of
topic. Dewdney et al. (2001) show that non-lexical
features, such as parts of speech and line-spacing,
can be successfully used to classify genres, and
Ferizis and Bailey (2006) demonstrate that accu-
rate classification of Internet documents is possi-
ble even without the expensive part-of-speech tag-
ging of similar methods. Recall that the noun ratio
(F.7) was among the most effective of our features.
Finn and Kushmerick (2006) describe a study
in which they classified documents from various
domains as “subjective” or “objective.” They, too,
found that part-of-speech statistics as well as gen-
eral text statistics (e.g., average sentence length)
are more effective than the traditional bag-of-
words representation when classifying documents
from multiple domains. This supports the notion
that we can use non-lexical features to classify po-
tential indexing terms in one biomedical subdo-
main using training data from another.
Maskey and Hirschberg (2005) found that
prosodic features (see Ward, 2004) combined with
structural features are sufficient to summarize spo-
ken news broadcasts. Prosodic features relate to
intonational variation and are associated with par-
ticularly important items, whereas structural fea-
tures are associated with the organization of a typ-
ical broadcast: headlines, followed by a descrip-
tion of the stories, etc.
Finally, Schilder and Kondadadi (2008) de-
scribe non-lexical word-frequency features, sim-
ilar to our ratio features (F.4–F.7), which are
used with a regression SVM to efficiently gener-
ate query-based multi-document summaries.
</bodyText>
<sectionHeader confidence="0.999199" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999928411764706">
Images convey essential information in biomedi-
cal publications. However, automatically extract-
ing and selecting useful indexing terms from the
article text is a difficult task given the domain-
specific nature of biomedical images and vocab-
ularies. In this work, we use the manual classifi-
cation results of a previous study to train a binary
classifier to automatically decide whether a poten-
tial indexing term is useful for this purpose or not.
We use non-lexical features generated for each
term with the most effective including whether the
term appears in the MeSH terms assigned to the
article and whether it is found in the article’s ti-
tle and caption. While our specific retrieval task
relates to the biomedical domain, our results in-
dicate that ABIR approaches to image retrieval in
any domain can benefit from an automatic annota-
</bodyText>
<page confidence="0.995302">
743
</page>
<bodyText confidence="0.999890666666667">
tion process utilizing non-lexical features to aid in
the selection of indexing terms or the reduction of
ineffective terms from a set of potential ones.
</bodyText>
<sectionHeader confidence="0.983413" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999841362637363">
Sameer Antani, Dina Demner-Fushman, Jiang Li,
Balaji V. Srinivasan, and George R. Thoma.
2008. Exploring use of images in clinical ar-
ticles for decision support in evidence-based
medicine. In Proc. of SPIE-IS&amp;T Electronic
Imaging, pages 1–10.
Alan R. Aronson. 2001. Effective mapping of
biomedical text to the UMLS metathesaurus:
The MetaMap program. In Proc. of the Annual
Symp. of the American Medical Informatics As-
sociation (AMIA), pages 17–21.
Corinna Cortes and Vladimir Vapnik. 1995.
Support-vector networks. Machine Learning,
20(3):273–297.
Dina Demner-Fushman, Sameer Antani, Matthew
Simpson, and George Thoma. 2008. Combin-
ing medical domain ontological knowledge and
low-level image features for multimedia index-
ing. In Proc. of the Language Resources for
Content-Based Image Retrieval Workshop (On-
toImage), pages 18–23.
Dina Demner-Fushman, Sameer K. Antani, and
George R. Thoma. 2007. Automatically finding
images for clinical decision support. In Proc. of
the Intl. Workshop on Data Mining in Medicine
(DM-Med), pages 139–144.
Nigel Dewdney, Carol VanEss-Dykema, and
Richard MacMillan. 2001. The form is the sub-
stance: Classification of genres in text. In Proc.
of the Workshop on Human Language Technol-
ogy and Knowledge Management, pages 1–8.
George Ferizis and Peter Bailey. 2006. Towards
practical genre classification of web documents.
In Proc. of the Intl. Conference on the World
Wide Web (WWW), pages 1013–1014.
Aidan Finn and Nicholas Kushmerick. 2006.
Learning to classify documents according to
genre. Journal of the American Society for
Information Science and Technology (JASIST),
57(11):1506–1518.
F. Florea, V. Buzuloiu, A. Rogozan, A. Bensrhair,
and S. Darmoni. 2007. Automatic image an-
notation: Combining the content and context of
medical images. In Intl. Symp. on Signals, Cir-
cuits and Systems (ISSCS), pages 1–4.
Masashi Inoue. 2004. On the need for annotation-
based image retrieval. In Proc. of the Workshop
on Information Retrieval in Context (IRiX),
pages 44–46.
Judith Klavans, Carolyn Sheffield, Eileen Abels,
Joan Beaudoin, Laura Jenemann, Tom Lipin-
cott, Jimmy Lin, Rebecca Passonneau, Tandeep
Sidhu, Dagobert Soergel, and Tae Yano. 2008.
Computational linguistics for metadata build-
ing: Aggregating text processing technologies
for enhanced image access. In Proc. of the Lan-
guage Resources for Content-Based Image Re-
trieval Workshop (OntoImage), pages 42–47.
D.A. Lindberg, B.L. Humphreys, and A.T. Mc-
Cray. 1993. The unified medical language
system. Methods of Information in Medicine,
32(4):281–291.
Sameer Maskey and Julia Hirschberg. 2005.
Comparing lexical, acoustic/prosodic, struc-
tural and discourse features for speech sum-
marization. In Proc. of the European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH), pages 621–624.
Gerard Salton and Christopher Buckley. 1988.
Term-weighting approaches in automatic text
retrieval. Information Processing &amp; Manage-
ment, 24(5):513–523.
Frank Schilder and Ravikumar Kondadadi. 2008.
FastSum: Fast and accurate query-based multi-
document summarization. In Proc. of the
Workshop on Human Language Technology and
Knowledge Management, pages 205–208.
Jouary Thomas, Kaiafa Anastasia, Lipinski
Philippe, Vergier B´eatrice, Lepreux S´ebastien,
Delaunay Mich`ele, and TaiebAlain. 2006.
Metastatic hidradenocarcinoma: Efficacy of
capecitabine. Archives of Dermatology,
142(10):1366–1367.
Nigel Ward. 2004. Pragmatic functions of
prosodic features in non-lexical utterances.
In Proc. of the Intl. Conference on Speech
Prosody, pages 325–328.
Geoffrey I. Webb, Janice R. Boughton, and Zhihai
Wang. 2005. Not so naive bayes: Aggregating
one-dependence estimators. Machine Learning,
58(1):5–24.
</reference>
<page confidence="0.998302">
744
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647117">
<title confidence="0.999759">Using Non-lexical Features to Identify Effective Indexing Terms for Biomedical Illustrations</title>
<author confidence="0.997605">Matthew Simpson</author>
<author confidence="0.997605">Dina Demner-Fushman</author>
<author confidence="0.997605">Charles Sneiderman</author>
<author confidence="0.997605">Sameer K Antani</author>
<author confidence="0.997605">George R Thoma</author>
<affiliation confidence="0.923471">Lister Hill National Center for Biomedical Communications</affiliation>
<address confidence="0.702372">National Library of Medicine, NIH, Bethesda, MD, USA</address>
<email confidence="0.985019">ddemner,csneiderman,santani,</email>
<abstract confidence="0.9995795">Automatic image annotation is an attractive approach for enabling convenient access to images found in a variety of documents. Since image captions and relevant discussions found in the text can be useful for summarizing the content of images, it is also possible that this text can be used to generate salient indexing terms. Unfortunately, this problem is generally domainspecific because indexing terms that are useful in one domain can be ineffective in others. Thus, we present a supervised machine learning approach to image anutilizing extracted from image-related text to select useful terms. We apply this approach to several subdomains of the biomedical sciences and show that we are able to reduce the number of ineffective indexing terms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sameer Antani</author>
<author>Dina Demner-Fushman</author>
<author>Jiang Li</author>
<author>Balaji V Srinivasan</author>
<author>George R Thoma</author>
</authors>
<title>Exploring use of images in clinical articles for decision support in evidence-based medicine.</title>
<date>2008</date>
<booktitle>In Proc. of SPIE-IS&amp;T Electronic Imaging,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="1713" citStr="Antani et al., 2008" startWordPosition="253" endWordPosition="256">e able to reduce the number of ineffective indexing terms. 1 Introduction Authors of biomedical publications often utilize images and other illustrations to convey information essential to the article and to support and reinforce textual content. These images are useful in support of clinical decisions, in rich document summaries, and for instructional purposes. The task of delivering these images, and the publications in which they are contained, to biomedical clinicians and researchers in an accessible way is an information retrieval problem. Current research in the biomedical domain (e.g., Antani et al., 2008; Florea et al., 2007), has investigated hybrid approaches to image retrieval, combining elements of content-based image retrieval (CBIR) and annotation-based image retrieval (ABIR). ABIR, compared to the image1Non-lexical features describe attributes of image-related text but not the text itself, e.g., unlike a bag-of-words model. only approach of CBIR, offers a practical advantage in that queries can be more naturally specified by a human user (Inoue, 2004). However, manually annotating biomedical images is a laborious and subjective task that often leads to noisy results. Automatic image an</context>
<context position="7341" citStr="Antani et al. (2008)" startWordPosition="1137" endWordPosition="1140">ner-Fushman et al. (2007) developed a machine learning approach to identify images from biomedical publications that are relevant to clinical decision support. In this work, the authors utilized both image and textual features to classify images based on their usefulness in evidencebased medicine. In contrast, our work is focused on selecting useful biomedical image indexing terms; however, we utilize the methods developed in their work to extract images and their related captions and mentions. Authors of biomedical publications often assemble multiple images into a single multi-panel figure. Antani et al. (2008) developed a unique two-phase approach for detecting and segmenting these figures. The authors rely on cues from captions to inform an image analysis algorithm that determines panel edge information. We make use of this approach to uniquely associate caption and mention text with a single image. 2http://imageclef.org/ Caption: Figure 1. On recurrence, histologic features of porocarcinoma with an intraepidermal spread of neoplastic clusters (hematoxylin-eosin, original magnification x100). Mention: Histopathologic findings were reviewed and confirmed a diagnosis of eccrine hidradenocarcinoma fo</context>
</contexts>
<marker>Antani, Demner-Fushman, Li, Srinivasan, Thoma, 2008</marker>
<rawString>Sameer Antani, Dina Demner-Fushman, Jiang Li, Balaji V. Srinivasan, and George R. Thoma. 2008. Exploring use of images in clinical articles for decision support in evidence-based medicine. In Proc. of SPIE-IS&amp;T Electronic Imaging, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan R Aronson</author>
</authors>
<title>Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program.</title>
<date>2001</date>
<booktitle>In Proc. of the Annual Symp. of the American Medical Informatics Association (AMIA),</booktitle>
<pages>17--21</pages>
<contexts>
<context position="5361" citStr="Aronson, 2001" startWordPosition="833" endWordPosition="834"> finding salient indexing terms. Indeed, we must classify terms that are not encountered while training. Therefore, we hypothesize that non-lexical features, which have been successfully used for speech and genre classification tasks, among others (see Section 5 for related work), may be useful in classifying text associated with images. While this approach is broad enough to apply to any retrieval task, given the goals of our ongoing research, we restrict ourselves to studying its feasibility in the biomedical domain. In order to achieve this, we make use of the previously developed MetaMap (Aronson, 2001) tool, which maps text to concepts contained in the Unified Medical Language System® (UMLS) Metathesaurus ® (Lindberg et al., 1993). The UMLS is a compendium of several controlled vocabularies in the biomedical sciences that provides a semantic mapping relating concepts from the various vocabularies (Section 2). We then use a supervised machine learning approach, described in Section 3, to classify the UMLS concepts as useful indexing terms based on their non-lexical features, gleaned from the article text and MetaMap output. Experimental results, presented in Section 4, indicate that ineffect</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>Alan R. Aronson. 2001. Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program. In Proc. of the Annual Symp. of the American Medical Informatics Association (AMIA), pages 17–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="22004" citStr="Cortes and Vapnik, 1995" startWordPosition="3483" endWordPosition="3486">ment location (F.9 and F.10), both features computed using standard information retrieval techniques, are among the least effective features. 4.3 Classification While the AODE learner performed reasonably well for this task, the difficulty encountered when training the SVM learner may be explained as follows. The initial inter-annotator agreement of the evaluation data suggests that it is likely that our training data contained contradictory or mislabeled observations, preventing the construction of a maximal-margin hyperplane required by the SVM. An SVM implementation utilizing soft margins (Cortes and Vapnik, 1995) would likely achieve better results on our data, although at the expense of greater training time. The success of the AODE learner in this case is probably due to its resilience to mislabeled observations. Annotator Precision Recall F1-score A 0.258 0.442 0.326 B 0.200 0.225 0.212 Combined 0.326 0.224 0.266 Standard 0.453 0.229 0.304 Standarda 0.492 0.231 0.314 Training 0.502 0.332 0.400 Table 3: Classification Results. The classifier’s precision and recall, as well as the corresponding Fl-score, are given for the responses of each reviewer. aFor comparison, the classifier was also trained us</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine Learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Demner-Fushman</author>
<author>Sameer Antani</author>
<author>Matthew Simpson</author>
<author>George Thoma</author>
</authors>
<title>Combining medical domain ontological knowledge and low-level image features for multimedia indexing.</title>
<date>2008</date>
<booktitle>In Proc. of the Language Resources for Content-Based Image Retrieval Workshop (OntoImage),</booktitle>
<pages>18--23</pages>
<contexts>
<context position="8127" citStr="Demner-Fushman et al. (2008)" startWordPosition="1250" endWordPosition="1253">m that determines panel edge information. We make use of this approach to uniquely associate caption and mention text with a single image. 2http://imageclef.org/ Caption: Figure 1. On recurrence, histologic features of porocarcinoma with an intraepidermal spread of neoplastic clusters (hematoxylin-eosin, original magnification x100). Mention: Histopathologic findings were reviewed and confirmed a diagnosis of eccrine hidradenocarcinoma for all lesions excised (Figure 1). 738 Our current work most directly stems from the results of a term extraction and image annotation evaluation performed by Demner-Fushman et al. (2008). In this study, the authors utilized MetaMap to extract potential indexing terms (UMLS concepts) from image captions and mentions. They then asked a group of five physicians and one medical imaging specialist (four of whom are trained in medical informatics) to manually classify each concept as being “useful for indexing” its associated images or ineffective for this purpose. The reviewers also had the opportunity to identify additional indexing terms that were not automatically extracted by MetaMap. In total, the reviewers evaluated 4006 concepts (3,281 of which were unique), associated with</context>
<context position="10629" citStr="Demner-Fushman et al. (2008)" startWordPosition="1641" endWordPosition="1644">ribed in more detail in Section 3.2. Once the feature vectors are built, we automatically classify the term as either being useful for indexing the image or not. To select useful indexing terms, we trained a binary classifier, described in Section 3.3, in a 3http://archfaci.ama-assn.org/ 4http://www.cardiovascularultrasound.com/ Figure 2: Term Extraction and Selection. We gather features for the extracted terms and use them to train a classifier that selects the terms that are useful for indexing the associated images. supervised learning scenario with data obtained from the previous study by Demner-Fushman et al. (2008). We obtained our evaluation data from the 2006 Archives of Dermatology5 journal. Note that our training and evaluation data represent distinct subdomains of the biomedical sciences. In order to reduce noise in the classification of our evaluation data, we asked two of the reviewers who participated in the initial study to manually classify our extracted terms as they did for our training data. In doing so, they each evaluated an identical set of 1539 potential indexing terms relating to 50 randomly chosen images from 31 different articles. We measured the performance of our classifier in term</context>
<context position="20707" citStr="Demner-Fushman et al. (2008)" startWordPosition="3280" endWordPosition="3283">his makes intuitive sense because MeSH terms are assigned to articles by specially trained NLM professionals. Given the large size of the MeSH vocabulary, it is not unreasonable to assume that an article’s MeSH terms could be descriptive, at a coarse granularity, of the images it contains. Also, the subjectivity of the reviewers’ initial data calls into question the usefulness of our training data. It may be that MeSH terms, consistently assigned to all documents in a particular collection, are a more reliable determiner of the usefulness of potential indexing terms. Furthermore, the study by Demner-Fushman et al. (2008) found that, on average, roughly 25% of the additional (useful) terms the reviewers added to the set of extracted terms were also found in the MeSH terms assigned to the document containing the particular image. The abstract and title ratios (F.6 and F.5) also had a significant effect on the classification outcome. Similar to the argument for MeSH terms, as these constructs are a coarse summary of the contents of an article, it is not unreasonable to assume they summarize the images contained therein. Finally, the noun ratio (F.7) was a particularly effective feature, and the length of the UML</context>
</contexts>
<marker>Demner-Fushman, Antani, Simpson, Thoma, 2008</marker>
<rawString>Dina Demner-Fushman, Sameer Antani, Matthew Simpson, and George Thoma. 2008. Combining medical domain ontological knowledge and low-level image features for multimedia indexing. In Proc. of the Language Resources for Content-Based Image Retrieval Workshop (OntoImage), pages 18–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Demner-Fushman</author>
<author>Sameer K Antani</author>
<author>George R Thoma</author>
</authors>
<title>Automatically finding images for clinical decision support.</title>
<date>2007</date>
<booktitle>In Proc. of the Intl. Workshop on Data Mining in Medicine (DM-Med),</booktitle>
<pages>139--144</pages>
<contexts>
<context position="6746" citStr="Demner-Fushman et al. (2007)" startWordPosition="1045" endWordPosition="1048">ABIR approaches, which rely on both image content and annotations, can benefit from an automatic annotation process utilizing non-lexical features to aid in the selection of useful indexing terms. 2 Image Retrieval: Recent Work Automatic image annotation is a broad topic, and the automatic annotation of biomedical images, specifically, has been a frequent component of the ImageCLEF2 cross-language image retrieval workshop. In this section, we describe previous work in biomedical image retrieval that forms the basis of our approach. Refer to Section 5 for work related to our method in general. Demner-Fushman et al. (2007) developed a machine learning approach to identify images from biomedical publications that are relevant to clinical decision support. In this work, the authors utilized both image and textual features to classify images based on their usefulness in evidencebased medicine. In contrast, our work is focused on selecting useful biomedical image indexing terms; however, we utilize the methods developed in their work to extract images and their related captions and mentions. Authors of biomedical publications often assemble multiple images into a single multi-panel figure. Antani et al. (2008) deve</context>
</contexts>
<marker>Demner-Fushman, Antani, Thoma, 2007</marker>
<rawString>Dina Demner-Fushman, Sameer K. Antani, and George R. Thoma. 2007. Automatically finding images for clinical decision support. In Proc. of the Intl. Workshop on Data Mining in Medicine (DM-Med), pages 139–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Dewdney</author>
<author>Carol VanEss-Dykema</author>
<author>Richard MacMillan</author>
</authors>
<title>The form is the substance: Classification of genres in text.</title>
<date>2001</date>
<booktitle>In Proc. of the Workshop on Human Language Technology and Knowledge Management,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="25075" citStr="Dewdney et al. (2001)" startWordPosition="3972" endWordPosition="3975">) with an F1-score of 0.182 (DemnerFushman et al., 2008). Thus, non-lexical features can be leveraged, albeit to a small degree with our current features and classifier, in automatically selecting useful image indexing terms. In future work, we intend to explore additional features and alternative tools for mapping text to the UMLS. 5 Related Work Non-lexical features have been successful in many contexts, particularly in the areas of genre classification and text and speech summarization. Genre classification, unlike text classification, discriminates between document style instead of topic. Dewdney et al. (2001) show that non-lexical features, such as parts of speech and line-spacing, can be successfully used to classify genres, and Ferizis and Bailey (2006) demonstrate that accurate classification of Internet documents is possible even without the expensive part-of-speech tagging of similar methods. Recall that the noun ratio (F.7) was among the most effective of our features. Finn and Kushmerick (2006) describe a study in which they classified documents from various domains as “subjective” or “objective.” They, too, found that part-of-speech statistics as well as general text statistics (e.g., aver</context>
</contexts>
<marker>Dewdney, VanEss-Dykema, MacMillan, 2001</marker>
<rawString>Nigel Dewdney, Carol VanEss-Dykema, and Richard MacMillan. 2001. The form is the substance: Classification of genres in text. In Proc. of the Workshop on Human Language Technology and Knowledge Management, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Ferizis</author>
<author>Peter Bailey</author>
</authors>
<title>Towards practical genre classification of web documents.</title>
<date>2006</date>
<booktitle>In Proc. of the Intl. Conference on the World Wide Web (WWW),</booktitle>
<pages>1013--1014</pages>
<contexts>
<context position="25224" citStr="Ferizis and Bailey (2006)" startWordPosition="3995" endWordPosition="3998">features and classifier, in automatically selecting useful image indexing terms. In future work, we intend to explore additional features and alternative tools for mapping text to the UMLS. 5 Related Work Non-lexical features have been successful in many contexts, particularly in the areas of genre classification and text and speech summarization. Genre classification, unlike text classification, discriminates between document style instead of topic. Dewdney et al. (2001) show that non-lexical features, such as parts of speech and line-spacing, can be successfully used to classify genres, and Ferizis and Bailey (2006) demonstrate that accurate classification of Internet documents is possible even without the expensive part-of-speech tagging of similar methods. Recall that the noun ratio (F.7) was among the most effective of our features. Finn and Kushmerick (2006) describe a study in which they classified documents from various domains as “subjective” or “objective.” They, too, found that part-of-speech statistics as well as general text statistics (e.g., average sentence length) are more effective than the traditional bag-ofwords representation when classifying documents from multiple domains. This suppor</context>
</contexts>
<marker>Ferizis, Bailey, 2006</marker>
<rawString>George Ferizis and Peter Bailey. 2006. Towards practical genre classification of web documents. In Proc. of the Intl. Conference on the World Wide Web (WWW), pages 1013–1014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aidan Finn</author>
<author>Nicholas Kushmerick</author>
</authors>
<title>Learning to classify documents according to genre.</title>
<date>2006</date>
<journal>Journal of the American Society for Information Science and Technology (JASIST),</journal>
<volume>57</volume>
<issue>11</issue>
<contexts>
<context position="25475" citStr="Finn and Kushmerick (2006)" startWordPosition="4034" endWordPosition="4037">many contexts, particularly in the areas of genre classification and text and speech summarization. Genre classification, unlike text classification, discriminates between document style instead of topic. Dewdney et al. (2001) show that non-lexical features, such as parts of speech and line-spacing, can be successfully used to classify genres, and Ferizis and Bailey (2006) demonstrate that accurate classification of Internet documents is possible even without the expensive part-of-speech tagging of similar methods. Recall that the noun ratio (F.7) was among the most effective of our features. Finn and Kushmerick (2006) describe a study in which they classified documents from various domains as “subjective” or “objective.” They, too, found that part-of-speech statistics as well as general text statistics (e.g., average sentence length) are more effective than the traditional bag-ofwords representation when classifying documents from multiple domains. This supports the notion that we can use non-lexical features to classify potential indexing terms in one biomedical subdomain using training data from another. Maskey and Hirschberg (2005) found that prosodic features (see Ward, 2004) combined with structural f</context>
</contexts>
<marker>Finn, Kushmerick, 2006</marker>
<rawString>Aidan Finn and Nicholas Kushmerick. 2006. Learning to classify documents according to genre. Journal of the American Society for Information Science and Technology (JASIST), 57(11):1506–1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Florea</author>
<author>V Buzuloiu</author>
<author>A Rogozan</author>
<author>A Bensrhair</author>
<author>S Darmoni</author>
</authors>
<title>Automatic image annotation: Combining the content and context of medical images.</title>
<date>2007</date>
<booktitle>In Intl. Symp. on Signals, Circuits and Systems (ISSCS),</booktitle>
<pages>1--4</pages>
<contexts>
<context position="1735" citStr="Florea et al., 2007" startWordPosition="257" endWordPosition="260">number of ineffective indexing terms. 1 Introduction Authors of biomedical publications often utilize images and other illustrations to convey information essential to the article and to support and reinforce textual content. These images are useful in support of clinical decisions, in rich document summaries, and for instructional purposes. The task of delivering these images, and the publications in which they are contained, to biomedical clinicians and researchers in an accessible way is an information retrieval problem. Current research in the biomedical domain (e.g., Antani et al., 2008; Florea et al., 2007), has investigated hybrid approaches to image retrieval, combining elements of content-based image retrieval (CBIR) and annotation-based image retrieval (ABIR). ABIR, compared to the image1Non-lexical features describe attributes of image-related text but not the text itself, e.g., unlike a bag-of-words model. only approach of CBIR, offers a practical advantage in that queries can be more naturally specified by a human user (Inoue, 2004). However, manually annotating biomedical images is a laborious and subjective task that often leads to noisy results. Automatic image annotation is a more rob</context>
</contexts>
<marker>Florea, Buzuloiu, Rogozan, Bensrhair, Darmoni, 2007</marker>
<rawString>F. Florea, V. Buzuloiu, A. Rogozan, A. Bensrhair, and S. Darmoni. 2007. Automatic image annotation: Combining the content and context of medical images. In Intl. Symp. on Signals, Circuits and Systems (ISSCS), pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masashi Inoue</author>
</authors>
<title>On the need for annotationbased image retrieval.</title>
<date>2004</date>
<booktitle>In Proc. of the Workshop on Information Retrieval in Context (IRiX),</booktitle>
<pages>44--46</pages>
<contexts>
<context position="2176" citStr="Inoue, 2004" startWordPosition="326" endWordPosition="327">icians and researchers in an accessible way is an information retrieval problem. Current research in the biomedical domain (e.g., Antani et al., 2008; Florea et al., 2007), has investigated hybrid approaches to image retrieval, combining elements of content-based image retrieval (CBIR) and annotation-based image retrieval (ABIR). ABIR, compared to the image1Non-lexical features describe attributes of image-related text but not the text itself, e.g., unlike a bag-of-words model. only approach of CBIR, offers a practical advantage in that queries can be more naturally specified by a human user (Inoue, 2004). However, manually annotating biomedical images is a laborious and subjective task that often leads to noisy results. Automatic image annotation is a more robust approach to ABIR than manual annotation. Unfortunately, automatically selecting the most appropriate indexing terms is an especially challenging problem for biomedical images because of the domain-specific nature of these images and the many vocabularies used in the biomedical sciences. For example, the term “sweat gland adenocarcinoma” could be a useful indexing term for an image found in a dermatology publication, but it is less li</context>
</contexts>
<marker>Inoue, 2004</marker>
<rawString>Masashi Inoue. 2004. On the need for annotationbased image retrieval. In Proc. of the Workshop on Information Retrieval in Context (IRiX), pages 44–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Klavans</author>
<author>Carolyn Sheffield</author>
<author>Eileen Abels</author>
<author>Joan Beaudoin</author>
<author>Laura Jenemann</author>
<author>Tom Lipincott</author>
<author>Jimmy Lin</author>
</authors>
<title>Rebecca Passonneau, Tandeep Sidhu, Dagobert Soergel, and Tae Yano.</title>
<date>2008</date>
<booktitle>In Proc. of the</booktitle>
<pages>42--47</pages>
<contexts>
<context position="11785" citStr="Klavans et al., 2008" startWordPosition="1835" endWordPosition="1838">t articles. We measured the performance of our classifier in terms of how well it performed against this manual evaluation. These results, as well as a discussion pertaining to the interannotator agreement of the two reviewers, are presented in Section 4. Since our general approach is not specific to the biomedical domain, it could equally be applied in 5http://archderm.ama-assn.org/ 739 any domain with an existing ontology. For example, the UMLS and MetaMap can be replaced by the Art and Architecture Thesaurus6 and an equivalent mapping tool to annotate images related to art and art history (Klavans et al., 2008). 3.1 Terminology To describe our features, we adopt the following terminology. • A collection contains all the articles from a given publication for a specified number of years. For example, the 2006–2007 issues of Cardiovascular Ultrasound represent a single collection. • A document is a specific biomedical article from a particular collection and contains images and their captions and mentions. • A phrase is the portion of text that MetaMap maps to UMLS concepts. For example, from the caption in Figure 1, the noun phrase “histologic features” maps to four UMLS concepts: “Histologic,” “Chara</context>
</contexts>
<marker>Klavans, Sheffield, Abels, Beaudoin, Jenemann, Lipincott, Lin, 2008</marker>
<rawString>Judith Klavans, Carolyn Sheffield, Eileen Abels, Joan Beaudoin, Laura Jenemann, Tom Lipincott, Jimmy Lin, Rebecca Passonneau, Tandeep Sidhu, Dagobert Soergel, and Tae Yano. 2008. Computational linguistics for metadata building: Aggregating text processing technologies for enhanced image access. In Proc. of the Language Resources for Content-Based Image Retrieval Workshop (OntoImage), pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Lindberg</author>
<author>B L Humphreys</author>
<author>A T McCray</author>
</authors>
<title>The unified medical language system.</title>
<date>1993</date>
<journal>Methods of Information in Medicine,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="5492" citStr="Lindberg et al., 1993" startWordPosition="851" endWordPosition="854">thesize that non-lexical features, which have been successfully used for speech and genre classification tasks, among others (see Section 5 for related work), may be useful in classifying text associated with images. While this approach is broad enough to apply to any retrieval task, given the goals of our ongoing research, we restrict ourselves to studying its feasibility in the biomedical domain. In order to achieve this, we make use of the previously developed MetaMap (Aronson, 2001) tool, which maps text to concepts contained in the Unified Medical Language System® (UMLS) Metathesaurus ® (Lindberg et al., 1993). The UMLS is a compendium of several controlled vocabularies in the biomedical sciences that provides a semantic mapping relating concepts from the various vocabularies (Section 2). We then use a supervised machine learning approach, described in Section 3, to classify the UMLS concepts as useful indexing terms based on their non-lexical features, gleaned from the article text and MetaMap output. Experimental results, presented in Section 4, indicate that ineffective indexing terms can be reduced using this classification technique. We conclude that ABIR approaches to biomedical image retriev</context>
</contexts>
<marker>Lindberg, Humphreys, McCray, 1993</marker>
<rawString>D.A. Lindberg, B.L. Humphreys, and A.T. McCray. 1993. The unified medical language system. Methods of Information in Medicine, 32(4):281–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Maskey</author>
<author>Julia Hirschberg</author>
</authors>
<title>Comparing lexical, acoustic/prosodic, structural and discourse features for speech summarization.</title>
<date>2005</date>
<booktitle>In Proc. of the European Conference on Speech Communication and Technology (EUROSPEECH),</booktitle>
<pages>621--624</pages>
<contexts>
<context position="26002" citStr="Maskey and Hirschberg (2005)" startWordPosition="4111" endWordPosition="4114">ll that the noun ratio (F.7) was among the most effective of our features. Finn and Kushmerick (2006) describe a study in which they classified documents from various domains as “subjective” or “objective.” They, too, found that part-of-speech statistics as well as general text statistics (e.g., average sentence length) are more effective than the traditional bag-ofwords representation when classifying documents from multiple domains. This supports the notion that we can use non-lexical features to classify potential indexing terms in one biomedical subdomain using training data from another. Maskey and Hirschberg (2005) found that prosodic features (see Ward, 2004) combined with structural features are sufficient to summarize spoken news broadcasts. Prosodic features relate to intonational variation and are associated with particularly important items, whereas structural features are associated with the organization of a typical broadcast: headlines, followed by a description of the stories, etc. Finally, Schilder and Kondadadi (2008) describe non-lexical word-frequency features, similar to our ratio features (F.4–F.7), which are used with a regression SVM to efficiently generate query-based multi-document s</context>
</contexts>
<marker>Maskey, Hirschberg, 2005</marker>
<rawString>Sameer Maskey and Julia Hirschberg. 2005. Comparing lexical, acoustic/prosodic, structural and discourse features for speech summarization. In Proc. of the European Conference on Speech Communication and Technology (EUROSPEECH), pages 621–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<contexts>
<context position="15974" citStr="Salton and Buckley, 1988" startWordPosition="2552" endWordPosition="2555">|mp| F.9 Tf-idf (real): The frequency of term ti (i.e., the phrase that generated the concept) times its inverse document frequency: tfidfi,j = tfi,j x idfi (6) The term frequency tfi,j of term ti in document dj is given by ni,j tfi,j = (7) E|k=1 nk,j where ni,j is the number of occurrences of ti in dj, and the denominator is the number of occurrences of all terms in dj. The inverse document frequency idfi of ti is given by idfi = log |D |(8) |{dj : ti E dj}| where |D |is the total number of documents in the collection, and the denominator is the total number of documents that contain ti (see Salton and Buckley, 1988). F.10 Document Location (real): The location in the document of the phrase that generated the concept. This feature is continuous on [0, 1] with 0 representing the beginning of the document and 1 representing the end. F.11 Concept Length (real): The length of the concept, measured in number of characters. For the purpose of computing F.9 and F.10, we indexed each collection with the Terrier9 information retrieval platform. Terrier was configured to use a block indexing scheme with a Tf-idf weighting model. Computation of all other features is straightforward. 3.3 Classifier We explored these </context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing &amp; Management, 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Schilder</author>
<author>Ravikumar Kondadadi</author>
</authors>
<title>FastSum: Fast and accurate query-based multidocument summarization.</title>
<date>2008</date>
<booktitle>In Proc. of the Workshop on Human Language Technology and Knowledge Management,</booktitle>
<pages>205--208</pages>
<contexts>
<context position="26425" citStr="Schilder and Kondadadi (2008)" startWordPosition="4173" endWordPosition="4176">ultiple domains. This supports the notion that we can use non-lexical features to classify potential indexing terms in one biomedical subdomain using training data from another. Maskey and Hirschberg (2005) found that prosodic features (see Ward, 2004) combined with structural features are sufficient to summarize spoken news broadcasts. Prosodic features relate to intonational variation and are associated with particularly important items, whereas structural features are associated with the organization of a typical broadcast: headlines, followed by a description of the stories, etc. Finally, Schilder and Kondadadi (2008) describe non-lexical word-frequency features, similar to our ratio features (F.4–F.7), which are used with a regression SVM to efficiently generate query-based multi-document summaries. 6 Conclusion Images convey essential information in biomedical publications. However, automatically extracting and selecting useful indexing terms from the article text is a difficult task given the domainspecific nature of biomedical images and vocabularies. In this work, we use the manual classification results of a previous study to train a binary classifier to automatically decide whether a potential index</context>
</contexts>
<marker>Schilder, Kondadadi, 2008</marker>
<rawString>Frank Schilder and Ravikumar Kondadadi. 2008. FastSum: Fast and accurate query-based multidocument summarization. In Proc. of the Workshop on Human Language Technology and Knowledge Management, pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jouary Thomas</author>
<author>Kaiafa Anastasia</author>
<author>Lipinski Philippe</author>
<author>Vergier B´eatrice</author>
<author>Lepreux S´ebastien</author>
<author>Delaunay Mich`ele</author>
<author>TaiebAlain</author>
</authors>
<title>Metastatic hidradenocarcinoma: Efficacy of capecitabine.</title>
<date>2006</date>
<journal>Archives of Dermatology,</journal>
<volume>142</volume>
<issue>10</issue>
<marker>Thomas, Anastasia, Philippe, B´eatrice, S´ebastien, Mich`ele, TaiebAlain, 2006</marker>
<rawString>Jouary Thomas, Kaiafa Anastasia, Lipinski Philippe, Vergier B´eatrice, Lepreux S´ebastien, Delaunay Mich`ele, and TaiebAlain. 2006. Metastatic hidradenocarcinoma: Efficacy of capecitabine. Archives of Dermatology, 142(10):1366–1367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Ward</author>
</authors>
<title>Pragmatic functions of prosodic features in non-lexical utterances.</title>
<date>2004</date>
<booktitle>In Proc. of the Intl. Conference on Speech Prosody,</booktitle>
<pages>325--328</pages>
<contexts>
<context position="26048" citStr="Ward, 2004" startWordPosition="4120" endWordPosition="4121">r features. Finn and Kushmerick (2006) describe a study in which they classified documents from various domains as “subjective” or “objective.” They, too, found that part-of-speech statistics as well as general text statistics (e.g., average sentence length) are more effective than the traditional bag-ofwords representation when classifying documents from multiple domains. This supports the notion that we can use non-lexical features to classify potential indexing terms in one biomedical subdomain using training data from another. Maskey and Hirschberg (2005) found that prosodic features (see Ward, 2004) combined with structural features are sufficient to summarize spoken news broadcasts. Prosodic features relate to intonational variation and are associated with particularly important items, whereas structural features are associated with the organization of a typical broadcast: headlines, followed by a description of the stories, etc. Finally, Schilder and Kondadadi (2008) describe non-lexical word-frequency features, similar to our ratio features (F.4–F.7), which are used with a regression SVM to efficiently generate query-based multi-document summaries. 6 Conclusion Images convey essential</context>
</contexts>
<marker>Ward, 2004</marker>
<rawString>Nigel Ward. 2004. Pragmatic functions of prosodic features in non-lexical utterances. In Proc. of the Intl. Conference on Speech Prosody, pages 325–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey I Webb</author>
<author>Janice R Boughton</author>
<author>Zhihai Wang</author>
</authors>
<title>Not so naive bayes: Aggregating one-dependence estimators.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>58</volume>
<issue>1</issue>
<contexts>
<context position="17180" citStr="Webb et al., 2005" startWordPosition="2735" endWordPosition="2738">lored these feature vectors using various classification approaches available in the RapidMiner10 tool. Unlike many similar text and image 9http://ir.dcs.gla.ac.uk/terrier/ 10http://rapid-i.com/ classification problems, we were unable to achieve results with a Support Vector Machine (SVM) learner (libSVMLearner) using the Radial Base Function (RBF). Common cost and width parameters were used, yet the SVM classified all terms as ineffective. Identical results were observed using a Naive Bayes (NB) learner. For these reasons, we chose to use the Averaged One-Dependence Estimator (AODE) learner (Webb et al., 2005) available in RapidMiner. AODE is capable of achieving highly accurate classification results with the quick training time usually associated with NB. Because this learner does not handle continuous attributes, we preprocessed our features with equal frequency discretization. The AODE learner was trained in a ten-fold cross validation of our training data. 4 Results Results relating to specific aspects of our work (annotation, features and classification) are presented below. 4.1 Inter-Annotator Agreement Two independent reviewers manually classified the extracted terms from our evaluation dat</context>
</contexts>
<marker>Webb, Boughton, Wang, 2005</marker>
<rawString>Geoffrey I. Webb, Janice R. Boughton, and Zhihai Wang. 2005. Not so naive bayes: Aggregating one-dependence estimators. Machine Learning, 58(1):5–24.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>