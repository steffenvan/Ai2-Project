<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.964419">
Topic Analysis for Psychiatric Document Retrieval
</title>
<author confidence="0.998127">
Liang-Chih Yu*‡, Chung-Hsien Wu*, Chin-Yew Lin†, Eduard Hovy‡ and Chia-Ling Lin*
</author>
<affiliation confidence="0.998948666666667">
*Department of CSIE, National Cheng Kung University, Tainan, Taiwan, R.O.C.
†Microsoft Research Asia, Beijing, China
‡Information Sciences Institute, University of Southern California, Marina del Rey, CA, USA
</affiliation>
<email confidence="0.997894">
liangchi@isi.edu,{chwu,totalwhite}@csie.ncku.edu.tw,cyl@microsoft.com,hovy@isi.edu
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99984345">
Psychiatric document retrieval attempts to
help people to efficiently and effectively
locate the consultation documents relevant
to their depressive problems. Individuals
can understand how to alleviate their symp-
toms according to recommendations in the
relevant documents. This work proposes
the use of high-level topic information ex-
tracted from consultation documents to im-
prove the precision of retrieval results. The
topic information adopted herein includes
negative life events, depressive symptoms
and semantic relations between symptoms,
which are beneficial for better understand-
ing of users&apos; queries. Experimental results
show that the proposed approach achieves
higher precision than the word-based re-
trieval models, namely the vector space
model (VSM) and Okapi model, adopting
word-level information alone.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999520387755102">
Individuals may suffer from negative or stressful
life events, such as death of a family member, ar-
gument with a spouse and loss of a job. Such
events play an important role in triggering depres-
sive symptoms, such as depressed moods, suicide
attempts and anxiety. Individuals under these cir-
cumstances can consult health professionals using
message boards and other services. Health profes-
sionals respond with suggestions as soon as possi-
ble. However, the response time is generally sev-
eral days, depending on both the processing time
required by health professionals and the number of
problems to be processed. Such a long response
time is unacceptable, especially for patients suffer-
ing from psychiatric emergencies such as suicide
attempts. A potential solution considers the prob-
lems that have been processed and the correspond-
ing suggestions, called consultation documents, as
the psychiatry web resources. These resources gen-
erally contain thousands of consultation documents
(problem-response pairs), making them a useful
information resource for mental health care and
prevention. By referring to the relevant documents,
individuals can become aware that they are not
alone because many people have suffered from the
same or similar problems. Additionally, they can
understand how to alleviate their symptoms ac-
cording to recommendations. However, browsing
and searching all consultation documents to iden-
tify the relevant documents is time consuming and
tends to become overwhelming. Individuals need
to be able to retrieve the relevant consultation
documents efficiently and effectively. Therefore,
this work presents a novel mechanism to automati-
cally retrieve the relevant consultation documents
with respect to users&apos; problems.
Traditional information retrieval systems repre-
sent queries and documents using a bag-of-words
approach. Retrieval models, such as the vector
space model (VSM) (Baeza-Yates and Ribeiro-
Neto, 1999) and Okapi model (Robertson et al.,
1995; Robertson et al., 1996; Okabe et al., 2005),
are then adopted to estimate the relevance between
queries and documents. The VSM represents each
query and document as a vector of words, and
adopts the cosine measure to estimate their rele-
vance. The Okapi model, which has been used on
the Text REtrieval Conference (TREC) collections,
developed a family of word-weighting functions
</bodyText>
<page confidence="0.968764">
1024
</page>
<note confidence="0.936523">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024–1031,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<figure confidence="0.9335819375">
Query: Consultation Document
In recent months, I often lose my temper for no reason.
After that, it took me a long time to fall asleep at night.
I broke up with my boyfriend.
I often felt like crying and felt pain every day.
So, I tried to kill myself several times.
cause-effect temporal
&lt;Suicide&gt;
&lt;Depressed&gt;
&lt;Insomnia&gt;
&lt;Anxiety&gt;
Recommendation:
It&apos;s normal to feel this way when going through these kinds of struggles, but over
time your emotions should level out Suicide doesn&apos;t solve anything; think about
how it would affect your family There are a few things you can try to help
you get to sleep at night, like drinking warm milk, listening to relaxing music
</figure>
<figureCaption confidence="0.997771">
Figure 1. Example of a consultation document. The bold arrowed lines denote cause-effect relations; ar-
</figureCaption>
<bodyText confidence="0.992971346153846">
rowed lines denote temporal relations; dashed lines denote temporal boundaries, and angle brackets de-
note depressive symptoms
for relevance estimation. These functions consider
word frequencies and document lengths for word
weighting. Both the VSM and Okapi models esti-
mate the relevance by matching the words in a
query with the words in a document. Additionally,
query words can further be expanded by the con-
cept hierarchy within general-purpose ontologies
such as WordNet (Fellbaum, 1998), or automati-
cally constructed ontologies (Yeh et al., 2004).
However, such word-based approaches only
consider the word-level information in queries and
documents, ignoring the high-level topic informa-
tion that can help improve understanding of users&apos;
queries. Consider the example consultation docu-
ment in Figure 1. A consultation document com-
prises two parts: the query part and recommenda-
tion part. The query part is a natural language text,
containing rich topic information related to users&apos;
depressive problems. The topic information in-
cludes negative life events, depressive symptoms,
and semantic relations between symptoms. As in-
dicated in Figure 1, the subject suffered from a
love-related event, and several depressive symp-
toms, such as &lt;Depressed&gt;, &lt;Suicide&gt;, &lt;Insom-
nia&gt; and &lt;Anxiety&gt;. Moreover, there is a cause-
effect relation holding between &lt;Depressed&gt; and
&lt;Suicide&gt;, and a temporal relation holding be-
tween &lt;Depressed&gt; and &lt;Insomnia&gt;. Different
topics may lead to different suggestions decided by
experts. Therefore, an ideal retrieval system for
consultation documents should consider such topic
information so as to improve the retrieval precision.
Natural language processing (NLP) techniques
can be used to extract more precise information
from natural language texts (Wu et al., 2005a; Wu
et al., 2005b; Wu et al., 2006; Yu et al., 2007).
This work adopts the methodology presented in
(Wu et al. 2005a) to extract depressive symptoms
and their relations, and adopts the pattern-based
method presented in (Yu et al., 2007) to extract
negative life events from both queries and consul-
tation documents. This work also proposes a re-
trieval model to calculate the similarity between a
query and a document by combining the similari-
ties of the extracted topic information.
The rest of this work is organized as follows.
Section 2 briefly describes the extraction of topic
information. Section 3 presents the retrieval model.
Section 4 summarizes the experimental results.
Conclusions are finally drawn in Section 5.
</bodyText>
<sectionHeader confidence="0.9941705" genericHeader="method">
2 Framework of Consultation Document
Retrieval
</sectionHeader>
<figureCaption confidence="0.963406285714286">
Figure 2 shows the framework of consultation
document retrieval. The retrieval process begins
with receiving a user’s query about his depressive
problems in natural language. The example query
is shown in Figure 1. The topic information is then
extracted from the query, as shown in the center of
Figure 2. The extracted topic information is repre-
</figureCaption>
<page confidence="0.98271">
1025
</page>
<figureCaption confidence="0.999184">
Figure 2. Framework of consultation document retrieval. The rectangle denotes a negative life event re-
lated to love relation. Each circle denotes a symptom. D: Depressed, S: Suicide, I: Insomnia, A: Anxiety.
</figureCaption>
<figure confidence="0.998195571428572">
Query
(Figure 1)
Negative Life Event
Identification
Topic Analysis
Symptom
Identification
Relation
Identification
&lt;Love&gt;
D S I A
D S Cause-Effect
D I A
S I A
Temporal
Topic Information
Consultation
Documents
Relevance
Estimation
Ranking
</figure>
<bodyText confidence="0.998209051282051">
sented by the sets of negative life events, depres-
sive symptoms, and semantic relations. Each ele-
ment in the event set and symptom set denotes an
individual event and symptom, respectively, while
each element in the relation set denotes a symptom
chain to retain the order of symptoms. Similarly,
the query parts of consultation documents are rep-
resented in the same manner. The relevance esti-
mation then calculates the similarity between the
input query and the query part of each consultation
document by combining the similarities of the sets
of events, symptoms, and relations within them.
Finally, a list of consultation documents ranked in
the descending order of similarities is returned to
the user.
In the following, the extraction of topic informa-
tion is described briefly. The detailed process is
described in (Wu et al. 2005a) for symptom and
relation identification, and in (Yu et al., 2007) for
event identification.
1) Symptom identification: A total of 17 symp-
toms are defined based on the Hamilton De-
pression Rating Scale (HDRS) (Hamilton,
1960). The identification of symptoms is sen-
tence-based. For each sentence, its structure is
first analyzed by a probabilistic context free
grammar (PCFG), built from the Sinica Tree-
bank corpus developed by Academia Sinica,
Taiwan (http://treebank.sinica.edu.tw), to gen-
erate a set of dependencies between word to-
kens. Each dependency has the format (modi-
fier, head, relmodifier,head). For instance, the de-
pendency (matters, worry about, goal) means
that &amp;quot;matters&amp;quot; is the goal to the head of the sen-
tence &amp;quot;worry about&amp;quot;. Each sentence can then
be associated with a symptom based on the
probabilities that dependencies occur in all
symptoms, which are obtained from a set of
training sentences.
</bodyText>
<listItem confidence="0.955601409090909">
2) Relation Identification: The semantic rela-
tions of interest include cause-effect and tem-
poral relations. After the symptoms are ob-
tained, the relations holding between symp-
toms (sentences) are identified by a set of dis-
course markers. For instance, the discourse
markers &amp;quot;because&amp;quot; and &amp;quot;therefore&amp;quot; may signal
cause-effect relations, and &amp;quot;before&amp;quot; and &amp;quot;after&amp;quot;
may signal temporal relations.
3) Negative life event identification: A total of 5
types of events, namely &lt;Family&gt;, &lt;Love&gt;,
&lt;School&gt;, &lt;Work&gt; and &lt;Social&gt; are defined
based on Pagano et al’s (2004) research. The
identification of events is a pattern-based ap-
proach. A pattern denotes a semantically plau-
sible combination of words, such as &lt;parents,
divorce&gt; and &lt;exam, fail&gt;. First, a set of pat-
terns is acquired from psychiatry web corpora
by using an evolutionary inference algorithm.
The event of each sentence is then identified
by using an SVM classifier with the acquired
patterns as features.
</listItem>
<sectionHeader confidence="0.999444" genericHeader="method">
3 Retrieval Model
</sectionHeader>
<bodyText confidence="0.99948075">
The similarity between a query and a document,
Sim(q, d) , is calculated by combining the similari-
ties of the sets of events, symptoms and relations
within them, as shown in (1).
</bodyText>
<page confidence="0.724298">
1026
</page>
<bodyText confidence="0.4701105">
=
Sim q d
</bodyText>
<equation confidence="0.9892745">
( , )
α Sim q d β Sim q d
Evn ( , ) + Sym ( , ) (1
+ − −
</equation>
<bodyText confidence="0.985539666666667">
α β)Sim Rel (q, d ),
where SimEvn (q, d) , SimSym (q, d) and SimRel (q, d) ,
denote the similarities of the sets of events, symp-
toms and relations, respectively, between a query
and a document, and α and β denote the combi-
nation factors.
</bodyText>
<subsectionHeader confidence="0.999952">
3.1 Similarity of events and symptoms
</subsectionHeader>
<bodyText confidence="0.971066192307693">
The similarities of the sets of events and symptoms
are calculated in the same method. The similarity
of the event set (or symptom set) is calculated by
comparing the events (or symptoms) in a query
with those in a document. Additionally, only the
events (or symptoms) with the same type are
considered. The events (or symptoms) with
different types are considered as irrelevant, i.e., no
similarity. For instance, the event &lt;Love&gt; is
considered as irrelevant to &lt;Work&gt;. The similarity
of the event set is calculated by
where Evnq and Evnd denote the event set in a
query and a document, respectively; eq and ed
denote the events; N(Evnq ∪ Evnd ) denotes the
cardinality of the union of Evnq and Evnd as a
normalization factor, and Type(eq , ed) denotes an
identity function to check whether two events have
the same type, defined as
The cos(eq, ed) denotes the cosine angle between
two vectors of words representing eq and ed , as
shown below.
tionally, cos(eq , ed) and const. can
be considered
ing of const. is deter-
mined empirically.
be calculated as
</bodyText>
<equation confidence="0.997810333333333">
SimRel(q,d)= Z ∑ Type(rq,rd)Sim(rq,rd), (5)
rq,rd
(6)
</equation>
<bodyText confidence="0.984551166666667">
where rq and rd denote the relations in a query an
Z=NC(rq)NC(rd)+NT(rq)NT(rd),
d
a document, respectively; Z denotes the normaliza-
tion factor for the number of relations; Type(eq , ed )
denotes an identity function similar to (3), and
where w denotes a word in a vector, and T denotes
the dimensionality of vectors. Accordingly, when
two events have the same type, their similarity is
given as cos(eq , ed) plus a constant, const.. Addi-
1027 as the word-level and topic-level similarities, re-
spectively. The optimal sett
</bodyText>
<subsectionHeader confidence="0.999376">
3.2 Similarity of relations
</subsectionHeader>
<bodyText confidence="0.983595736842105">
When calculating the similarity of relations, only
the relations with the same type are considered.
That is, the cause-effect (or temporal) relations in a
query are only compared with the cause-effect (or
temporal) relations in a document. Therefore, the
similarity of relation sets can
(•) and NT (•) denote the numbers of cause-
effect and temporal relations.
Both cause-effect and temporal relations are rep-
resented by symptom chains. Hence, the similarity
of relations is measured by the similarity of symp-
tom chains. The main characteristic of a symptom
chain is that it retains the cause-effect or temporal
order of the symptoms within it. Therefore, the
order of the symptoms must be considered when
calculating the similarity of two symptom chains.
Accordingly, a sequence kernel function (Lodhi et
al., 2002; Cancedda et al., 2003) is adopted to cal-
culate the similarity of two symptom chains. A
sequence kernel compares two sequences of sym-
bols (e.g., characters, words) based on the subse-
quences within them, but not individual symbols.
Thereby, the order of the symptoms can be incor-
porated into the comparison process.
The sequence kernel calculates the similarity of
two symptom chains by comparing their sub-
symptom chains at different lengths. An increasing
number of common sub-symptom chains indicates
a greater similarity between two symptom chains.
For instance, both the two symptom chains
and
contain the same symptoms s1,
and
but in different orders. To calculate the
similarity between these two symptom chains, the
sequence kernel first calculates their similarities at
length 2 and 3, and then averages the similarities at
the two lengths. To calculate the similari
</bodyText>
<equation confidence="0.761381">
NC
s1s2s3s4
s3s2s1
s2
s3,
ty at
( q, d)
N(Evnq ∪ Evnd)
1 ∑ Type(eq,ed)cos(eq,ed) +const.,
e q d
∈ ∩
SimEvn
=
(eq, ed) =
⎧⎪ 1 ( )
Type e Type e
= ( )
q d
⎨
Type
.
⎪⎩ 0 otherwise
∑ Ti i
w w
= e e
i 1 q d
cos( , )
e e = ,
q d T 2 T 2
∑ i= 1
i i
( ) ( )
w w
e e
∑
q i = 1 d
</equation>
<bodyText confidence="0.98043985">
length 2, the sequence kernel compares their sub-
symptom chains of length 2, i.e.,
{s1s2, s1s3, s1s4, s2s3, s2s4, s3s4} and {s3s2, s3s1, s2s1} .
Similarly, their similarity at length 3 is calculated
by comparing their sub-symptom chains of length
3, i.e., {s1s2s3, s1s2s4, s1s3s4, s2s3s4} and {s3s2s1} .
Obviously, no similarity exists between s1s2s3s4
and s3s2s1 , since no sub-symptom chains are
matched at both lengths. In this example, the sub-
symptom chains of length 1, i.e., individual symp-
toms, do not have to be compared because they
contain no information about the order of symp-
toms. Additionally, the sub-symptom chains of
length 4 do not have to be compared, because the
two symptom chains share no sub-symptom chains
at this length. Hence, for any two symptom chains,
the length of the sub-symptom chains to be com-
pared ranges from two to the minimum length of
the two symptom chains. The similarity of two
symptom chains can be formally denoted as
</bodyText>
<equation confidence="0.990003181818182">
N N
q d q d
Sim ( , ) ( , )
≡
r r Sim sc sc
1 2
K sc sc
( , )
N 1 2
= N
q d
</equation>
<bodyText confidence="0.948576">
where N1
scq and N2
scd denote the symptom chains
corresponding to rq and rd , respectively; N1 and
N2 denote the length of N1
scq and N2
scd , respec-
tively; K( i , i ) denotes the sequence kernel for
calculating the similarity between two symptom
chains; Kn ( i , i ) denotes the sequence kernel for
calculating the similarity between two symptom
chains at length n, and N is the minimum length of
the two symptom chains, i.e., N = min(N1, N2) .
The sequence kernel ( 1 , 2 )
</bodyText>
<equation confidence="0.992441">
K n sc i sc j is defined as
N
</equation>
<bodyText confidence="0.942816">
where ( 1 , 2 )
Kn sci scj is the normalized inner
</bodyText>
<equation confidence="0.7153454">
N N
product of vectors ( 1)
Φ n sci and ( 2)
N Φ n scj ; Φ n (i)
N
</equation>
<figureCaption confidence="0.9412765">
Figure 3. Illustrative example of relevance com-
putation using the sequence kernel function.
</figureCaption>
<bodyText confidence="0.996766375">
denotes a mapping that transforms a given symp-
tom chain into a vector of the sub-symptom chains
of length n; φu (i) denotes an element of the vector,
representing the weight of a sub-symptom chain u ,
and n
SC denotes the set of all possible sub-
symptom chains of length n. The weight of a sub-
symptom chain, i.e., φu (i) , is defined as
</bodyText>
<figure confidence="0.865344555555556">
u is a contiguous sub-symptom chain of
u is a non-contiguous sub-symptom chain (9)
with skipped symptoms
θ
0 does not appear in ,
1
scN
u i
( )
</figure>
<page confidence="0.248915">
2
</page>
<equation confidence="0.931378727272727">
scN
n j
N 1 2 2
) ( )
φ ( ) ( )
N N
sc φ
u j u i
sc u j
∑ φ sc
θ
</equation>
<bodyText confidence="0.9223548">
where λ ∈ [0,1] denotes a decay factor that is
adopted to penalize the non-contiguous sub-
symptom chains occurred in a symptom chain
based on the skipped symptoms. For instance,
φs s (s1s2s3) = φs2s3 (s1s2s3) =1 since s1s2 and s2s3
</bodyText>
<equation confidence="0.8625754">
1 2
are considered as contiguous in s1s2s3 , and
φ s s (s1s2s3) = λ since s1s3 is a non-contiguous
1
1 3
</equation>
<bodyText confidence="0.999865125">
sub-symptom chain with one skipped symptom.
The decay factor is adopted because a contiguous
sub-symptom chain is preferable to a non-
contiguous chain when comparing two symptom
chains. The setting of the decay factor is domain
dependent. If λ =1, then no penalty is applied for
skipping symptoms, and the cause-effect and tem-
poral relations are transitive. The optimal setting of
</bodyText>
<equation confidence="0.5626585">
N
1
1 2
= K sc sc
( , ),
N N
n q
∑ d
− 1
N
2
=
n
( ) ( )
1 2
sc N N
φ
i u j
sc
∑ φu
(8)
,
u SCn
∈
=
N 1
∑ φu i
( sc
u SC n n
∈ u SC
∈
1
Φ ( )
N
n i
sc
K sc sc N
( , )
N 1 2 =
n i j
</equation>
<figure confidence="0.95594488">
( )
N 1
Φ n i
sc
Φ
i
( )
2
scN
n
Φ
(7)
⎧1
⎪
⎪ λ
( )
N 1
φ = ⎨
u i
sc
⎪
⎪⎩
N1
c
s
</figure>
<page confidence="0.959765">
1028
</page>
<table confidence="0.99807175">
Topic Avg. Number
Negative Life Event 1.45
Depressive Symptom 4.40
Semantic Relation 3.35
</table>
<tableCaption confidence="0.99988">
Table 1. Characteristics of the test query set.
</tableCaption>
<bodyText confidence="0.997807">
A is determined empirically. Figure 3 presents an
example to summarize the computation of the
similarity between two symptom chains.
</bodyText>
<sectionHeader confidence="0.984917" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.955915">
4.1 Experiment setup
</subsectionHeader>
<bodyText confidence="0.878739857142857">
1) Corpus: The consultation documents were
collected from the mental health website of the
John Tung Foundation (http://www.jtf.org.tw)
and the PsychPark (http://www.psychpark.org),
a virtual psychiatric clinic, maintained by a
group of volunteer professionals of Taiwan
Association of Mental Health Informatics (Bai
et al. 2001). Both of the web sites provide
various kinds of free psychiatric services and
update the consultation documents periodically.
For privacy consideration, all personal infor-
mation has been removed. A total of 3,650
consultation documents were collected for
evaluating the retrieval model, of which 20
documents were randomly selected as the test
query set, 100 documents were randomly se-
lected as the tuning set to obtain the optimal
parameter settings of involved retrieval models,
and the remaining 3,530 documents were the
reference set to be retrieved. Table 1 shows the
average number of events, symptoms and rela-
tions in the test query set.
2) Baselines: The proposed method, denoted as
Topic, was compared to two word-based re-
trieval models: the VSM and Okapi BM25
models. The VSM was implemented in terms
of the standard TF-IDF weight. The Okapi
BM25 model is defined as
</bodyText>
<equation confidence="0.992686">
t Q
∈
∑w(1) (1 )f (k3 )qtf +k avdl−dl (10)
K+tf k3+qtf 2 |Q |avdl+dl,
+1 +1
</equation>
<bodyText confidence="0.999819333333333">
where t denotes a word in a query Q; qtf and tf
denote the word frequencies occurring in a
query and a document, respectively, and (1)
</bodyText>
<equation confidence="0.78565875">
w
denotes the Robertson-Sparck Jones weight of
t (without relevance feedback), defined as
N n
− + 0.5
w (1) = log
,
n+ 0.5
</equation>
<bodyText confidence="0.999860333333333">
where N denotes the total number of docu-
ments, and n denotes the number of documents
containing t. In (10), K is defined as
</bodyText>
<equation confidence="0.999555">
K = k1((1−b)+ b ⋅ dl / avdl), (12)
</equation>
<bodyText confidence="0.917243625">
where dl and avdl denote the length and aver-
age length of a document, respectively. The
default values of k1, k2, k3 and b are describe
in (Robertson et al., 1996), where k1 ranges
from 1.0 to 2.0; k2 is set to 0; k3 is set to 8,
and b ranges from 0.6 to 0.75. Additionally,
BM25 can be considered as BM15 and BM11
when b is set to 1 and 0, respectively.
</bodyText>
<listItem confidence="0.986865545454546">
3) Evaluation metric: To evaluate the retrieval
models, a multi-level relevance criterion was
adopted. The relevance criterion was divided
into four levels, as described below.
• Level 0: No topics are matched between a
query and a document.
• Level 1: At least one topic is partially
matched between a query and a document.
• Level 2: All of the three topics are partially
matched between a query and a document.
• Level 3: All of the three topics are partially
</listItem>
<bodyText confidence="0.959257833333333">
matched, and at least one topic is exactly
matched between a query and a document.
To deal with the multi-level relevance, the dis-
counted cumulative gain (DCG) (Jarvelin and
Kekalainen, 2000) was adopted as the evalua-
tion metric, defined as
</bodyText>
<equation confidence="0.999286333333333">
DCG[i] ⎧⎪⎨ G[1], if i=1 (13)
⎩DCG[i −1] + [ ]/ logc , otherwise
G i i
</equation>
<bodyText confidence="0.999892">
where i denotes the i-th document in the re-
trieved list; G[ij denotes the gain value, i.e.,
relevance levels, of the i-th document, and c
denotes the parameter to penalize a retrieved
document in a lower rank. That is, the DCG
simultaneously considers the relevance levels,
and the ranks in the retrieved list to measure
the retrieval precision. For instance, let
&lt;3,2,3,0,0&gt; denotes the retrieved list of five
documents with their relevance levels. If no
penalization is used, then the DCG values for
</bodyText>
<equation confidence="0.508245">
(11)
</equation>
<page confidence="0.976529">
1029
</page>
<table confidence="0.99295325">
Relevance Level Avg. Number
Level 1 18.50
Level 2 9.15
Level 3 2.20
</table>
<tableCaption confidence="0.983704">
Table 2. Average number of relevant documents
for the test query set.
</tableCaption>
<table confidence="0.99430825">
Retrieval Model Avg. Time (seconds)
Topic 17.13
VSM 0.68
BM25 0.48
</table>
<tableCaption confidence="0.9450625">
Table 4. Average query processing time of differ-
ent retrieval models.
</tableCaption>
<table confidence="0.9998445">
DCG(5) DCG(10) DCG(20) DCG(50) DCG(100)
Topic 4.7516* 6.9298 7.6040* 8.3606* 9.3974*
BM25 4.4624 6.7023 7.1156 7.8129 8.6597
BM11 3.8877 4.9328 5.9589 6.9703 7.7057
VSM 2.3454 3.3195 4.4609 5.8179 6.6945
BM15 2.1362 2.6120 3.4487 4.5452 5.7020
</table>
<tableCaption confidence="0.9992">
Table 3. DCG values of different retrieval models. * Topic vs BM25 significantly different (p&lt;0.05)
</tableCaption>
<bodyText confidence="0.999798548387097">
the retrieved list are &lt;3,5,8,8,8&gt;, and thus
DCG[5]=8. Conversely, if c=2, then the docu-
ments retrieved at ranks lower than two are pe-
nalized. Hence, the DCG values for the re-
trieved list are &lt;3,5,6.89,6.89,6.89&gt;, and
DCG[5]=6.89.
The relevance judgment was performed by
three experienced physicians. First, the pooling
method (Voorhees, 2000) was adopted to gen-
erate the candidate relevant documents for
each test query by taking the top 50 ranked
documents retrieved by each of the involved
retrieval models, namely the VSM, BM25 and
Topic. Two physicians then judged each can-
didate document based on the multilevel rele-
vance criterion. Finally, the documents with
disagreements between the two physicians
were judged by the third physician. Table 2
shows the average number of relevant docu-
ments for the test query set.
4) Optimal parameter setting: The parameter
settings of BM25 and Topic were evaluated us-
ing the tuning set. The optimal setting of
BM25 were k1 =1 and b=0.6. The other two pa-
rameters were set to the default values, i.e.,
k2 = 0 and k3 = 8. For the Topic model, the
parameters required to be evaluated include the
combination factors, α and β , described in
(1); the constant const. described in (2), and
the decay factor, A , described in (9). The op-
timal settings were α = 0.3 ; β = 0.5 ;
</bodyText>
<listItem confidence="0.374789">
const.=0.6 and A = 0.8.
</listItem>
<subsectionHeader confidence="0.951583">
4.2 Retrieval results
</subsectionHeader>
<bodyText confidence="0.999992347826087">
The results are divided into two groups: the preci-
sion and efficiency. The retrieval precision was
measured by DCG values. Additionally, a paired,
two-tailed t-test was used to determine whether the
performance difference was statistically significant.
The retrieval efficiency was measure by the query
processing time, i.e., the time for processing all the
queries in the test query set.
Table 3 shows the comparative results of re-
trieval precision. The two variants of BM25,
namely BM11 and BM15, are also considered in
comparison. For the word-based retrieval models,
both BM25 and BM11 outperformed the VSM, and
BM15 performed worst. The Topic model
achieved higher DCG values than both the BM-
series models and VSM. The reasons are three-fold.
First, a negative life event and a symptom can each
be expressed by different words with the same or
similar meaning. Therefore, the word-based mod-
els often failed to retrieve the relevant documents
when different words were used in the input query.
Second, a word may relate to different events and
symptoms. For instance, the term &amp;quot;worry about&amp;quot; is
</bodyText>
<page confidence="0.971106">
1030
</page>
<bodyText confidence="0.99998085">
a good indicator for both the symptoms &lt;Anxiety&gt;
and &lt;Hypochondriasis&gt;. This may result in ambi-
guity for the word-based models. Third, the word-
based models cannot capture semantic relations
between symptoms. The Topic model incorporates
not only the word-level information, but also more
useful topic information about depressive problems,
thus improving the retrieval results.
The query processing time was measured using
a personal computer with Windows XP operating
system, a 2.4GHz Pentium IV processor and
512MB RAM. Table 4 shows the results. The topic
model required more processing time than both
VSM and BM25, since identification of topics in-
volves more detailed analysis, such as semantic
parsing of sentences and symptom chain construc-
tion. This finding indicates that although the topic
information can improve the retrieval precision,
incorporating such high-precision features reduces
the retrieval efficiency.
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999994833333333">
This work has presented the use of topic informa-
tion for retrieving psychiatric consultation docu-
ments. The topic information can provide more
precise information about users&apos; depressive prob-
lems, thus improving the retrieval precision. The
proposed framework can also be applied to differ-
ent domains as long as the domain-specific topic
information is identified. Future work will focus on
more detailed experiments, including the contribu-
tion of each topic to retrieval precision, the effect
of using different methods to combine topic infor-
mation, and the evaluation on real users.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999534451612904">
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison-Wesley, Reading,
MA.
Cancedda, N., E. Gaussier, C. Goutte, and J. M. Renders.
2003. Word-Sequence Kernels. Journal of Machine
Learning Research, 3(6):1059-1082.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Hamilton, M. 1960. A Rating Scale for Depression.
Journal of Neurology, Neurosurgery and Psychiatry,
23:56-62
Jarvelin, K. and J. Kekalainen. 2000. IR Evaluation
Methods for Retrieving Highly Relevant Documents.
In Proc. of the 23rd Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 41-48.
Lodhi, H., C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text Classification Using
String Kernels. Journal of Machine Learning Re-
search, 2(3):419-444.
Okabe, M., K. Umemura and S. Yamada. 2005. Query
Expansion with the Minimum User Feedback by
Transductive Learning. In Proc. of HLT/EMNLP,
Vancouver, Canada, pages 963-970.
Pagano, M.E., A.E. Skodol, R.L. Stout, M.T. Shea, S.
Yen, C.M. Grilo, C.A. Sanislow, D.S. Bender, T.H.
McGlashan, M.C. Zanarini, and J.G. Gunderson.
2004. Stressful Life Events as Predictors of Function-
ing: Findings from the Collaborative Longitudinal
Personality Disorders Study. Acta Psychiatrica Scan-
dinavica, 110: 421-429.
Robertson, S. E., S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M.Gatford. 1995. Okapi at TREC-3. In
Proc. of the Third Text REtrieval Conference (TREC-
3), NIST.
Robertson, S. E., S. Walker, M. M. Beaulieu, and
M.Gatford. 1996. Okapi at TREC-4. In Proc. of the
fourth Text REtrieval Conference (TREC-4), NIST.
Voorhees, E. M. and D. K. Harman. 2000. Overview of
the Sixth Text REtrieval Conference (TREC-6). In-
formation Processing and Management, 36(1):3-35.
Wu, C. H., L. C. Yu, and F. L. Jang. 2005a. Using Se-
mantic Dependencies to Mine Depressive Symptoms
from Consultation Records. IEEE Intelligent System,
20(6):50-58.
Wu, C. H., J. F. Yeh, and M. J. Chen. 2005b. Domain-
Specific FAQ Retrieval Using Independent Aspects.
ACM Trans. Asian Language Information Processing,
4(1):1-17.
Wu, C. H., J. F. Yeh, and Y. S. Lai. 2006. Semantic
Segment Extraction and Matching for Internet FAQ
Retrieval. IEEE Trans. Knowledge and Data Engi-
neering, 18(7):930-940.
Yeh, J. F., C. H. Wu, M. J. Chen, and L. C. Yu. 2004.
Automated Alignment and Extraction of Bilingual
Domain Ontology for Cross-Language Domain-
Specific Applications. In Proc. of the 20th COLING,
Geneva, Switzerland, pages 1140-1146.
Yu, L. C., C. H. Wu, Yeh, J. F., and F. L. Jang. 2007.
HAL-based Evolutionary Inference for Pattern Induc-
tion from Psychiatry Web Resources. Accepted by
IEEE Trans. Evolutionary Computation.
</reference>
<page confidence="0.993159">
1031
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.129929">
<title confidence="0.936514">Topic Analysis for Psychiatric Document Retrieval</title>
<author confidence="0.429165">Chung-Hsien Chin-Yew Eduard</author>
<author confidence="0.429165">Chia-Ling</author>
<affiliation confidence="0.304335">of CSIE, National Cheng Kung University, Tainan, Taiwan, R.O.C.</affiliation>
<address confidence="0.521545">Research Asia, Beijing, China Sciences Institute, University of Southern California, Marina del Rey, CA, USA</address>
<email confidence="0.999192">liangchi@isi.edu,{chwu,totalwhite}@csie.ncku.edu.tw,cyl@microsoft.com,hovy@isi.edu</email>
<abstract confidence="0.999619952380953">Psychiatric document retrieval attempts to help people to efficiently and effectively locate the consultation documents relevant to their depressive problems. Individuals can understand how to alleviate their symptoms according to recommendations in the relevant documents. This work proposes the use of high-level topic information extracted from consultation documents to improve the precision of retrieval results. The topic information adopted herein includes life symptoms relations symptoms, which are beneficial for better understanding of users&apos; queries. Experimental results show that the proposed approach achieves higher precision than the word-based retrieval models, namely the vector space model (VSM) and Okapi model, adopting word-level information alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Baeza-Yates</author>
<author>B Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern Information Retrieval. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cancedda</author>
<author>E Gaussier</author>
<author>C Goutte</author>
<author>J M Renders</author>
</authors>
<title>Word-Sequence Kernels.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--6</pages>
<contexts>
<context position="13707" citStr="Cancedda et al., 2003" startWordPosition="2135" endWordPosition="2138">elations in a document. Therefore, the similarity of relation sets can (•) and NT (•) denote the numbers of causeeffect and temporal relations. Both cause-effect and temporal relations are represented by symptom chains. Hence, the similarity of relations is measured by the similarity of symptom chains. The main characteristic of a symptom chain is that it retains the cause-effect or temporal order of the symptoms within it. Therefore, the order of the symptoms must be considered when calculating the similarity of two symptom chains. Accordingly, a sequence kernel function (Lodhi et al., 2002; Cancedda et al., 2003) is adopted to calculate the similarity of two symptom chains. A sequence kernel compares two sequences of symbols (e.g., characters, words) based on the subsequences within them, but not individual symbols. Thereby, the order of the symptoms can be incorporated into the comparison process. The sequence kernel calculates the similarity of two symptom chains by comparing their subsymptom chains at different lengths. An increasing number of common sub-symptom chains indicates a greater similarity between two symptom chains. For instance, both the two symptom chains and contain the same symptoms </context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>Cancedda, N., E. Gaussier, C. Goutte, and J. M. Renders. 2003. Word-Sequence Kernels. Journal of Machine Learning Research, 3(6):1059-1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5080" citStr="Fellbaum, 1998" startWordPosition="741" endWordPosition="742">ng to relaxing music Figure 1. Example of a consultation document. The bold arrowed lines denote cause-effect relations; arrowed lines denote temporal relations; dashed lines denote temporal boundaries, and angle brackets denote depressive symptoms for relevance estimation. These functions consider word frequencies and document lengths for word weighting. Both the VSM and Okapi models estimate the relevance by matching the words in a query with the words in a document. Additionally, query words can further be expanded by the concept hierarchy within general-purpose ontologies such as WordNet (Fellbaum, 1998), or automatically constructed ontologies (Yeh et al., 2004). However, such word-based approaches only consider the word-level information in queries and documents, ignoring the high-level topic information that can help improve understanding of users&apos; queries. Consider the example consultation document in Figure 1. A consultation document comprises two parts: the query part and recommendation part. The query part is a natural language text, containing rich topic information related to users&apos; depressive problems. The topic information includes negative life events, depressive symptoms, and sem</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hamilton</author>
</authors>
<title>A Rating Scale for Depression.</title>
<date>1960</date>
<journal>Journal of Neurology, Neurosurgery and Psychiatry,</journal>
<pages>23--56</pages>
<contexts>
<context position="9018" citStr="Hamilton, 1960" startWordPosition="1351" endWordPosition="1352">t query and the query part of each consultation document by combining the similarities of the sets of events, symptoms, and relations within them. Finally, a list of consultation documents ranked in the descending order of similarities is returned to the user. In the following, the extraction of topic information is described briefly. The detailed process is described in (Wu et al. 2005a) for symptom and relation identification, and in (Yu et al., 2007) for event identification. 1) Symptom identification: A total of 17 symptoms are defined based on the Hamilton Depression Rating Scale (HDRS) (Hamilton, 1960). The identification of symptoms is sentence-based. For each sentence, its structure is first analyzed by a probabilistic context free grammar (PCFG), built from the Sinica Treebank corpus developed by Academia Sinica, Taiwan (http://treebank.sinica.edu.tw), to generate a set of dependencies between word tokens. Each dependency has the format (modifier, head, relmodifier,head). For instance, the dependency (matters, worry about, goal) means that &amp;quot;matters&amp;quot; is the goal to the head of the sentence &amp;quot;worry about&amp;quot;. Each sentence can then be associated with a symptom based on the probabilities that d</context>
</contexts>
<marker>Hamilton, 1960</marker>
<rawString>Hamilton, M. 1960. A Rating Scale for Depression. Journal of Neurology, Neurosurgery and Psychiatry, 23:56-62</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jarvelin</author>
<author>J Kekalainen</author>
</authors>
<title>IR Evaluation Methods for Retrieving Highly Relevant Documents.</title>
<date>2000</date>
<booktitle>In Proc. of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="21136" citStr="Jarvelin and Kekalainen, 2000" startWordPosition="3600" endWordPosition="3603"> evaluate the retrieval models, a multi-level relevance criterion was adopted. The relevance criterion was divided into four levels, as described below. • Level 0: No topics are matched between a query and a document. • Level 1: At least one topic is partially matched between a query and a document. • Level 2: All of the three topics are partially matched between a query and a document. • Level 3: All of the three topics are partially matched, and at least one topic is exactly matched between a query and a document. To deal with the multi-level relevance, the discounted cumulative gain (DCG) (Jarvelin and Kekalainen, 2000) was adopted as the evaluation metric, defined as DCG[i] ⎧⎪⎨ G[1], if i=1 (13) ⎩DCG[i −1] + [ ]/ logc , otherwise G i i where i denotes the i-th document in the retrieved list; G[ij denotes the gain value, i.e., relevance levels, of the i-th document, and c denotes the parameter to penalize a retrieved document in a lower rank. That is, the DCG simultaneously considers the relevance levels, and the ranks in the retrieved list to measure the retrieval precision. For instance, let &lt;3,2,3,0,0&gt; denotes the retrieved list of five documents with their relevance levels. If no penalization is used, th</context>
</contexts>
<marker>Jarvelin, Kekalainen, 2000</marker>
<rawString>Jarvelin, K. and J. Kekalainen. 2000. IR Evaluation Methods for Retrieving Highly Relevant Documents. In Proc. of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lodhi</author>
<author>C Saunders</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
<author>C Watkins</author>
</authors>
<title>Text Classification Using String Kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--3</pages>
<contexts>
<context position="13683" citStr="Lodhi et al., 2002" startWordPosition="2131" endWordPosition="2134">fect (or temporal) relations in a document. Therefore, the similarity of relation sets can (•) and NT (•) denote the numbers of causeeffect and temporal relations. Both cause-effect and temporal relations are represented by symptom chains. Hence, the similarity of relations is measured by the similarity of symptom chains. The main characteristic of a symptom chain is that it retains the cause-effect or temporal order of the symptoms within it. Therefore, the order of the symptoms must be considered when calculating the similarity of two symptom chains. Accordingly, a sequence kernel function (Lodhi et al., 2002; Cancedda et al., 2003) is adopted to calculate the similarity of two symptom chains. A sequence kernel compares two sequences of symbols (e.g., characters, words) based on the subsequences within them, but not individual symbols. Thereby, the order of the symptoms can be incorporated into the comparison process. The sequence kernel calculates the similarity of two symptom chains by comparing their subsymptom chains at different lengths. An increasing number of common sub-symptom chains indicates a greater similarity between two symptom chains. For instance, both the two symptom chains and co</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Lodhi, H., C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text Classification Using String Kernels. Journal of Machine Learning Research, 2(3):419-444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Okabe</author>
<author>K Umemura</author>
<author>S Yamada</author>
</authors>
<title>Query Expansion with the Minimum User Feedback by Transductive Learning.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP,</booktitle>
<pages>963--970</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="3289" citStr="Okabe et al., 2005" startWordPosition="457" endWordPosition="460">nts to identify the relevant documents is time consuming and tends to become overwhelming. Individuals need to be able to retrieve the relevant consultation documents efficiently and effectively. Therefore, this work presents a novel mechanism to automatically retrieve the relevant consultation documents with respect to users&apos; problems. Traditional information retrieval systems represent queries and documents using a bag-of-words approach. Retrieval models, such as the vector space model (VSM) (Baeza-Yates and RibeiroNeto, 1999) and Okapi model (Robertson et al., 1995; Robertson et al., 1996; Okabe et al., 2005), are then adopted to estimate the relevance between queries and documents. The VSM represents each query and document as a vector of words, and adopts the cosine measure to estimate their relevance. The Okapi model, which has been used on the Text REtrieval Conference (TREC) collections, developed a family of word-weighting functions 1024 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024–1031, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics Query: Consultation Document In recent months, I often lose my temper</context>
</contexts>
<marker>Okabe, Umemura, Yamada, 2005</marker>
<rawString>Okabe, M., K. Umemura and S. Yamada. 2005. Query Expansion with the Minimum User Feedback by Transductive Learning. In Proc. of HLT/EMNLP, Vancouver, Canada, pages 963-970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Pagano</author>
<author>A E Skodol</author>
<author>R L Stout</author>
<author>M T Shea</author>
<author>S Yen</author>
<author>C M Grilo</author>
<author>C A Sanislow</author>
<author>D S Bender</author>
<author>T H McGlashan</author>
<author>M C Zanarini</author>
<author>J G Gunderson</author>
</authors>
<date>2004</date>
<booktitle>Stressful Life Events as Predictors of Functioning: Findings from the Collaborative Longitudinal Personality Disorders Study. Acta Psychiatrica Scandinavica,</booktitle>
<volume>110</volume>
<pages>421--429</pages>
<marker>Pagano, Skodol, Stout, Shea, Yen, Grilo, Sanislow, Bender, McGlashan, Zanarini, Gunderson, 2004</marker>
<rawString>Pagano, M.E., A.E. Skodol, R.L. Stout, M.T. Shea, S. Yen, C.M. Grilo, C.A. Sanislow, D.S. Bender, T.H. McGlashan, M.C. Zanarini, and J.G. Gunderson. 2004. Stressful Life Events as Predictors of Functioning: Findings from the Collaborative Longitudinal Personality Disorders Study. Acta Psychiatrica Scandinavica, 110: 421-429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>S Jones</author>
<author>M M HancockBeaulieu</author>
<author>M Gatford</author>
</authors>
<title>Okapi at TREC-3.</title>
<date>1995</date>
<booktitle>In Proc. of the Third Text REtrieval Conference (TREC3),</booktitle>
<location>NIST.</location>
<contexts>
<context position="3244" citStr="Robertson et al., 1995" startWordPosition="449" endWordPosition="452">, browsing and searching all consultation documents to identify the relevant documents is time consuming and tends to become overwhelming. Individuals need to be able to retrieve the relevant consultation documents efficiently and effectively. Therefore, this work presents a novel mechanism to automatically retrieve the relevant consultation documents with respect to users&apos; problems. Traditional information retrieval systems represent queries and documents using a bag-of-words approach. Retrieval models, such as the vector space model (VSM) (Baeza-Yates and RibeiroNeto, 1999) and Okapi model (Robertson et al., 1995; Robertson et al., 1996; Okabe et al., 2005), are then adopted to estimate the relevance between queries and documents. The VSM represents each query and document as a vector of words, and adopts the cosine measure to estimate their relevance. The Okapi model, which has been used on the Text REtrieval Conference (TREC) collections, developed a family of word-weighting functions 1024 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024–1031, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics Query: Consultation Docu</context>
</contexts>
<marker>Robertson, Walker, Jones, HancockBeaulieu, Gatford, 1995</marker>
<rawString>Robertson, S. E., S. Walker, S. Jones, M. M. HancockBeaulieu, and M.Gatford. 1995. Okapi at TREC-3. In Proc. of the Third Text REtrieval Conference (TREC3), NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>M M Beaulieu</author>
<author>M Gatford</author>
</authors>
<title>Okapi at TREC-4.</title>
<date>1996</date>
<booktitle>In Proc. of the fourth Text REtrieval Conference (TREC-4),</booktitle>
<location>NIST.</location>
<contexts>
<context position="3268" citStr="Robertson et al., 1996" startWordPosition="453" endWordPosition="456"> all consultation documents to identify the relevant documents is time consuming and tends to become overwhelming. Individuals need to be able to retrieve the relevant consultation documents efficiently and effectively. Therefore, this work presents a novel mechanism to automatically retrieve the relevant consultation documents with respect to users&apos; problems. Traditional information retrieval systems represent queries and documents using a bag-of-words approach. Retrieval models, such as the vector space model (VSM) (Baeza-Yates and RibeiroNeto, 1999) and Okapi model (Robertson et al., 1995; Robertson et al., 1996; Okabe et al., 2005), are then adopted to estimate the relevance between queries and documents. The VSM represents each query and document as a vector of words, and adopts the cosine measure to estimate their relevance. The Okapi model, which has been used on the Text REtrieval Conference (TREC) collections, developed a family of word-weighting functions 1024 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024–1031, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics Query: Consultation Document In recent months, I</context>
<context position="20290" citStr="Robertson et al., 1996" startWordPosition="3446" endWordPosition="3449">l−dl (10) K+tf k3+qtf 2 |Q |avdl+dl, +1 +1 where t denotes a word in a query Q; qtf and tf denote the word frequencies occurring in a query and a document, respectively, and (1) w denotes the Robertson-Sparck Jones weight of t (without relevance feedback), defined as N n − + 0.5 w (1) = log , n+ 0.5 where N denotes the total number of documents, and n denotes the number of documents containing t. In (10), K is defined as K = k1((1−b)+ b ⋅ dl / avdl), (12) where dl and avdl denote the length and average length of a document, respectively. The default values of k1, k2, k3 and b are describe in (Robertson et al., 1996), where k1 ranges from 1.0 to 2.0; k2 is set to 0; k3 is set to 8, and b ranges from 0.6 to 0.75. Additionally, BM25 can be considered as BM15 and BM11 when b is set to 1 and 0, respectively. 3) Evaluation metric: To evaluate the retrieval models, a multi-level relevance criterion was adopted. The relevance criterion was divided into four levels, as described below. • Level 0: No topics are matched between a query and a document. • Level 1: At least one topic is partially matched between a query and a document. • Level 2: All of the three topics are partially matched between a query and a docu</context>
</contexts>
<marker>Robertson, Walker, Beaulieu, Gatford, 1996</marker>
<rawString>Robertson, S. E., S. Walker, M. M. Beaulieu, and M.Gatford. 1996. Okapi at TREC-4. In Proc. of the fourth Text REtrieval Conference (TREC-4), NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
<author>D K Harman</author>
</authors>
<date>2000</date>
<booktitle>Overview of the Sixth Text REtrieval Conference (TREC-6). Information Processing and Management,</booktitle>
<pages>36--1</pages>
<marker>Voorhees, Harman, 2000</marker>
<rawString>Voorhees, E. M. and D. K. Harman. 2000. Overview of the Sixth Text REtrieval Conference (TREC-6). Information Processing and Management, 36(1):3-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Wu</author>
<author>L C Yu</author>
<author>F L Jang</author>
</authors>
<title>Using Semantic Dependencies to Mine Depressive Symptoms from Consultation Records.</title>
<date>2005</date>
<journal>IEEE Intelligent System,</journal>
<pages>20--6</pages>
<contexts>
<context position="6385" citStr="Wu et al., 2005" startWordPosition="929" endWordPosition="932">ve-related event, and several depressive symptoms, such as &lt;Depressed&gt;, &lt;Suicide&gt;, &lt;Insomnia&gt; and &lt;Anxiety&gt;. Moreover, there is a causeeffect relation holding between &lt;Depressed&gt; and &lt;Suicide&gt;, and a temporal relation holding between &lt;Depressed&gt; and &lt;Insomnia&gt;. Different topics may lead to different suggestions decided by experts. Therefore, an ideal retrieval system for consultation documents should consider such topic information so as to improve the retrieval precision. Natural language processing (NLP) techniques can be used to extract more precise information from natural language texts (Wu et al., 2005a; Wu et al., 2005b; Wu et al., 2006; Yu et al., 2007). This work adopts the methodology presented in (Wu et al. 2005a) to extract depressive symptoms and their relations, and adopts the pattern-based method presented in (Yu et al., 2007) to extract negative life events from both queries and consultation documents. This work also proposes a retrieval model to calculate the similarity between a query and a document by combining the similarities of the extracted topic information. The rest of this work is organized as follows. Section 2 briefly describes the extraction of topic information. Sect</context>
<context position="8792" citStr="Wu et al. 2005" startWordPosition="1313" endWordPosition="1316">on set denotes a symptom chain to retain the order of symptoms. Similarly, the query parts of consultation documents are represented in the same manner. The relevance estimation then calculates the similarity between the input query and the query part of each consultation document by combining the similarities of the sets of events, symptoms, and relations within them. Finally, a list of consultation documents ranked in the descending order of similarities is returned to the user. In the following, the extraction of topic information is described briefly. The detailed process is described in (Wu et al. 2005a) for symptom and relation identification, and in (Yu et al., 2007) for event identification. 1) Symptom identification: A total of 17 symptoms are defined based on the Hamilton Depression Rating Scale (HDRS) (Hamilton, 1960). The identification of symptoms is sentence-based. For each sentence, its structure is first analyzed by a probabilistic context free grammar (PCFG), built from the Sinica Treebank corpus developed by Academia Sinica, Taiwan (http://treebank.sinica.edu.tw), to generate a set of dependencies between word tokens. Each dependency has the format (modifier, head, relmodifier,</context>
</contexts>
<marker>Wu, Yu, Jang, 2005</marker>
<rawString>Wu, C. H., L. C. Yu, and F. L. Jang. 2005a. Using Semantic Dependencies to Mine Depressive Symptoms from Consultation Records. IEEE Intelligent System, 20(6):50-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Wu</author>
<author>J F Yeh</author>
<author>M J Chen</author>
</authors>
<title>DomainSpecific FAQ Retrieval Using Independent Aspects.</title>
<date>2005</date>
<journal>ACM Trans. Asian Language Information Processing,</journal>
<pages>4--1</pages>
<contexts>
<context position="6385" citStr="Wu et al., 2005" startWordPosition="929" endWordPosition="932">ve-related event, and several depressive symptoms, such as &lt;Depressed&gt;, &lt;Suicide&gt;, &lt;Insomnia&gt; and &lt;Anxiety&gt;. Moreover, there is a causeeffect relation holding between &lt;Depressed&gt; and &lt;Suicide&gt;, and a temporal relation holding between &lt;Depressed&gt; and &lt;Insomnia&gt;. Different topics may lead to different suggestions decided by experts. Therefore, an ideal retrieval system for consultation documents should consider such topic information so as to improve the retrieval precision. Natural language processing (NLP) techniques can be used to extract more precise information from natural language texts (Wu et al., 2005a; Wu et al., 2005b; Wu et al., 2006; Yu et al., 2007). This work adopts the methodology presented in (Wu et al. 2005a) to extract depressive symptoms and their relations, and adopts the pattern-based method presented in (Yu et al., 2007) to extract negative life events from both queries and consultation documents. This work also proposes a retrieval model to calculate the similarity between a query and a document by combining the similarities of the extracted topic information. The rest of this work is organized as follows. Section 2 briefly describes the extraction of topic information. Sect</context>
<context position="8792" citStr="Wu et al. 2005" startWordPosition="1313" endWordPosition="1316">on set denotes a symptom chain to retain the order of symptoms. Similarly, the query parts of consultation documents are represented in the same manner. The relevance estimation then calculates the similarity between the input query and the query part of each consultation document by combining the similarities of the sets of events, symptoms, and relations within them. Finally, a list of consultation documents ranked in the descending order of similarities is returned to the user. In the following, the extraction of topic information is described briefly. The detailed process is described in (Wu et al. 2005a) for symptom and relation identification, and in (Yu et al., 2007) for event identification. 1) Symptom identification: A total of 17 symptoms are defined based on the Hamilton Depression Rating Scale (HDRS) (Hamilton, 1960). The identification of symptoms is sentence-based. For each sentence, its structure is first analyzed by a probabilistic context free grammar (PCFG), built from the Sinica Treebank corpus developed by Academia Sinica, Taiwan (http://treebank.sinica.edu.tw), to generate a set of dependencies between word tokens. Each dependency has the format (modifier, head, relmodifier,</context>
</contexts>
<marker>Wu, Yeh, Chen, 2005</marker>
<rawString>Wu, C. H., J. F. Yeh, and M. J. Chen. 2005b. DomainSpecific FAQ Retrieval Using Independent Aspects. ACM Trans. Asian Language Information Processing, 4(1):1-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Wu</author>
<author>J F Yeh</author>
<author>Y S Lai</author>
</authors>
<title>Semantic Segment Extraction and Matching for Internet FAQ Retrieval.</title>
<date>2006</date>
<journal>IEEE Trans. Knowledge and Data Engineering,</journal>
<pages>18--7</pages>
<contexts>
<context position="6421" citStr="Wu et al., 2006" startWordPosition="937" endWordPosition="940">sive symptoms, such as &lt;Depressed&gt;, &lt;Suicide&gt;, &lt;Insomnia&gt; and &lt;Anxiety&gt;. Moreover, there is a causeeffect relation holding between &lt;Depressed&gt; and &lt;Suicide&gt;, and a temporal relation holding between &lt;Depressed&gt; and &lt;Insomnia&gt;. Different topics may lead to different suggestions decided by experts. Therefore, an ideal retrieval system for consultation documents should consider such topic information so as to improve the retrieval precision. Natural language processing (NLP) techniques can be used to extract more precise information from natural language texts (Wu et al., 2005a; Wu et al., 2005b; Wu et al., 2006; Yu et al., 2007). This work adopts the methodology presented in (Wu et al. 2005a) to extract depressive symptoms and their relations, and adopts the pattern-based method presented in (Yu et al., 2007) to extract negative life events from both queries and consultation documents. This work also proposes a retrieval model to calculate the similarity between a query and a document by combining the similarities of the extracted topic information. The rest of this work is organized as follows. Section 2 briefly describes the extraction of topic information. Section 3 presents the retrieval model. </context>
</contexts>
<marker>Wu, Yeh, Lai, 2006</marker>
<rawString>Wu, C. H., J. F. Yeh, and Y. S. Lai. 2006. Semantic Segment Extraction and Matching for Internet FAQ Retrieval. IEEE Trans. Knowledge and Data Engineering, 18(7):930-940.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Yeh</author>
<author>C H Wu</author>
<author>M J Chen</author>
<author>L C Yu</author>
</authors>
<title>Automated Alignment and Extraction of Bilingual Domain Ontology for Cross-Language DomainSpecific Applications.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th COLING,</booktitle>
<pages>1140--1146</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5140" citStr="Yeh et al., 2004" startWordPosition="748" endWordPosition="751">ocument. The bold arrowed lines denote cause-effect relations; arrowed lines denote temporal relations; dashed lines denote temporal boundaries, and angle brackets denote depressive symptoms for relevance estimation. These functions consider word frequencies and document lengths for word weighting. Both the VSM and Okapi models estimate the relevance by matching the words in a query with the words in a document. Additionally, query words can further be expanded by the concept hierarchy within general-purpose ontologies such as WordNet (Fellbaum, 1998), or automatically constructed ontologies (Yeh et al., 2004). However, such word-based approaches only consider the word-level information in queries and documents, ignoring the high-level topic information that can help improve understanding of users&apos; queries. Consider the example consultation document in Figure 1. A consultation document comprises two parts: the query part and recommendation part. The query part is a natural language text, containing rich topic information related to users&apos; depressive problems. The topic information includes negative life events, depressive symptoms, and semantic relations between symptoms. As indicated in Figure 1, </context>
</contexts>
<marker>Yeh, Wu, Chen, Yu, 2004</marker>
<rawString>Yeh, J. F., C. H. Wu, M. J. Chen, and L. C. Yu. 2004. Automated Alignment and Extraction of Bilingual Domain Ontology for Cross-Language DomainSpecific Applications. In Proc. of the 20th COLING, Geneva, Switzerland, pages 1140-1146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L C Yu</author>
<author>C H Wu</author>
<author>J F Yeh</author>
<author>F L Jang</author>
</authors>
<title>HAL-based Evolutionary Inference for Pattern Induction from Psychiatry Web Resources. Accepted by IEEE Trans. Evolutionary Computation.</title>
<date>2007</date>
<contexts>
<context position="6439" citStr="Yu et al., 2007" startWordPosition="941" endWordPosition="944">ch as &lt;Depressed&gt;, &lt;Suicide&gt;, &lt;Insomnia&gt; and &lt;Anxiety&gt;. Moreover, there is a causeeffect relation holding between &lt;Depressed&gt; and &lt;Suicide&gt;, and a temporal relation holding between &lt;Depressed&gt; and &lt;Insomnia&gt;. Different topics may lead to different suggestions decided by experts. Therefore, an ideal retrieval system for consultation documents should consider such topic information so as to improve the retrieval precision. Natural language processing (NLP) techniques can be used to extract more precise information from natural language texts (Wu et al., 2005a; Wu et al., 2005b; Wu et al., 2006; Yu et al., 2007). This work adopts the methodology presented in (Wu et al. 2005a) to extract depressive symptoms and their relations, and adopts the pattern-based method presented in (Yu et al., 2007) to extract negative life events from both queries and consultation documents. This work also proposes a retrieval model to calculate the similarity between a query and a document by combining the similarities of the extracted topic information. The rest of this work is organized as follows. Section 2 briefly describes the extraction of topic information. Section 3 presents the retrieval model. Section 4 summariz</context>
<context position="8860" citStr="Yu et al., 2007" startWordPosition="1324" endWordPosition="1327">milarly, the query parts of consultation documents are represented in the same manner. The relevance estimation then calculates the similarity between the input query and the query part of each consultation document by combining the similarities of the sets of events, symptoms, and relations within them. Finally, a list of consultation documents ranked in the descending order of similarities is returned to the user. In the following, the extraction of topic information is described briefly. The detailed process is described in (Wu et al. 2005a) for symptom and relation identification, and in (Yu et al., 2007) for event identification. 1) Symptom identification: A total of 17 symptoms are defined based on the Hamilton Depression Rating Scale (HDRS) (Hamilton, 1960). The identification of symptoms is sentence-based. For each sentence, its structure is first analyzed by a probabilistic context free grammar (PCFG), built from the Sinica Treebank corpus developed by Academia Sinica, Taiwan (http://treebank.sinica.edu.tw), to generate a set of dependencies between word tokens. Each dependency has the format (modifier, head, relmodifier,head). For instance, the dependency (matters, worry about, goal) mea</context>
</contexts>
<marker>Yu, Wu, Yeh, Jang, 2007</marker>
<rawString>Yu, L. C., C. H. Wu, Yeh, J. F., and F. L. Jang. 2007. HAL-based Evolutionary Inference for Pattern Induction from Psychiatry Web Resources. Accepted by IEEE Trans. Evolutionary Computation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>