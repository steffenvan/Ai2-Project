<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008239">
<title confidence="0.9987845">
User Simulations for context-sensitive speech recognition in Spoken
Dialogue Systems
</title>
<author confidence="0.994333">
Oliver Lemon Ioannis Konstas
</author>
<affiliation confidence="0.999965">
Edinburgh University University of Glasgow
</affiliation>
<email confidence="0.98863">
olemon@inf.ed.ac.uk konstas@dcs.gla.ac.uk
</email>
<sectionHeader confidence="0.997269" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99999675">
We use a machine learner trained on a
combination of acoustic and contextual
features to predict the accuracy of incom-
ing n-best automatic speech recognition
(ASR) hypotheses to a spoken dialogue
system (SDS). Our novel approach is to
use a simple statistical User Simulation
(US) for this task, which measures the
likelihood that the user would say each
hypothesis in the current context. Such
US models are now common in machine
learning approaches to SDS, are trained on
real dialogue data, and are related to the-
ories of “alignment” in psycholinguistics.
We use a US to predict the user’s next dia-
logue move and thereby re-rank n-best hy-
potheses of a speech recognizer for a cor-
pus of 2564 user utterances. The method
achieved a significant relative reduction of
Word Error Rate (WER) of 5% (this is
44% of the possible WER improvement
on this data), and 62% of the possible se-
mantic improvement (Dialogue Move Ac-
curacy), compared to the baseline policy
of selecting the topmost ASR hypothesis.
The majority of the improvement is at-
tributable to the User Simulation feature,
as shown by Information Gain analysis.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937644444444">
A crucial problem in the design of spoken dia-
logue systems (SDS) is to decide for incoming
recognition hypotheses whether a system should
accept (consider correctly recognized), reject (as-
sume misrecognition), or ignore (classify as noise
or speech not directed to the system) them.
Obviously, incorrect decisions at this point can
have serious negative effects on system usability
and user satisfaction. On the one hand, accept-
ing misrecognized hypotheses leads to misunder-
standings and unintended system behaviors which
are usually difficult to recover from. On the other
hand, users might get frustrated with a system that
behaves too cautiously and rejects or ignores too
many utterances. Thus an important feature in di-
alogue system engineering is the tradeoff between
avoiding task failure (due to misrecognitions) and
promoting overall dialogue efficiency, flow, and
naturalness.
In this paper, we investigate the use of machine
learning trained on a combination of acoustic fea-
tures and features computed from dialogue context
to predict the quality of incoming n-best recogni-
tion hypotheses to a SDS. These predictions are
then used to select a “best” hypothesis and to de-
cide on appropriate system reactions. We evalu-
ate this approach in comparison with a baseline
system that works in the standard way: always
choosing the topmost hypothesis in the n-best list.
In such systems, complex repair strategies are re-
quired when the top hypothesis is incorrect.
The main novelty of this work is that we ex-
plore the use of predictions from simple statisti-
cal User Simulations to re-rank n-best lists of ASR
hypotheses. These User Simulations are now com-
monly used in statistical learning approaches to di-
alogue management (Williams and Young, 2003;
Schatzmann et al., 2006; Young, 2006; Young et
al., 2007; Schatzmann et al., 2007), but they have
not been used for context-sensitive ASR before.
In our model, the system’s “belief” b(h) in a
recognition hypothesis h is factored in two parts:
the observation probability P(oIh) (approximated
by the ASR confidence score) and the User Simu-
lation probability P(hIus, C) of the hypothesis:
</bodyText>
<equation confidence="0.988328">
b(h) = P(olh).P(hlus, C) (1)
</equation>
<bodyText confidence="0.971477">
where us is the state of the User Simulation in
context C. The context is simply a window of di-
</bodyText>
<note confidence="0.9239305">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 505–513,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997774">
505
</page>
<bodyText confidence="0.9991552">
alogue acts in the dialogue history, that the US is
sensitive to (see section 3).
The paper is organized as follows. After a short
relation to previous work, we describe the data
(Section 5) and derive baseline results (Section
6). Section 3 describes the User Simulations that
we use for re-ranking hypotheses. Section 7 de-
scribes our learning experiments for classifying
and selecting from n-best recognition hypotheses
and Section 9 reports our results.
</bodyText>
<sectionHeader confidence="0.984043" genericHeader="introduction">
2 Relation to Previous Work
</sectionHeader>
<bodyText confidence="0.999940073529412">
In psycholinguistics, the idea that human dialogue
participants simulate each other to some extent is
gaining currency. (Pickering and Garrod, 2007)
write:
“if B overtly imitates A, then A’s com-
prehension of B’s utterance is facilitated
by A’s memory for A’s previous utter-
ance.”
We explore aspects of this idea in a computa-
tional manner. Similar work in the area of spoken
dialogue systems is described below.
(Litman et al., 2000) use acoustic-prosodic in-
formation extracted from speech waveforms, to-
gether with information derived from their speech
recognizer, to automatically predict misrecog-
nized turns in a corpus of train-timetable informa-
tion dialogues. In our experiments, we also use
recognizer confidence scores and a limited num-
ber of acoustic-prosodic features (e.g. amplitude
in the speech signal) for hypothesis classification,
but we also use User Simulation predictions.
(Walker et al., 2000) use a combination of fea-
tures from the speech recognizer, natural language
understanding, and dialogue manager/discourse
history to classify hypotheses as correct, partially
correct, or misrecognized. Our work is related to
these experiments in that we also combine con-
fidence scores and higher-level features for clas-
sification. However, both (Litman et al., 2000)
and (Walker et al., 2000) consider only single-best
recognition results and thus use their classifiers as
“filters” to decide whether the best recognition hy-
pothesis for a user utterance is correct or not. We
go a step further in that we classify n-best hypothe-
ses and then select among the alternatives. We also
explore the use of more dialogue and task-oriented
features (e.g. the dialogue move type of a recogni-
tion hypothesis) for classification.
(Gabsdil and Lemon, 2004) similarly perform
reordering of n-best lists by combining acoustic
and pragmatic features. Their study shows that di-
alogue features such as the previous system ques-
tion and whether a hypothesis is the correct answer
to a particular question contributed more to classi-
fication accuracy than the other attributes.
(Jonson, 2006) classifies recognition hypothe-
ses with labels denoting acceptance, clarification,
confirmation and rejection. These labels were
learned in a similar way to (Gabsdil and Lemon,
2004) and correspond to varying levels of con-
fidence, being essentially potential directives to
the dialogue manager. Apart from standard fea-
tures Jonson includes attributes that account for
the whole n-best list, i.e. standard deviation of
confidence scores.
As well as the use of a User Simulation, the
main difference between our approach and work
on hypothesis reordering (e.g. (Chotimongkol and
Rudnicky, 2001)) is that we make a decision re-
garding whether a dialogue system should accept,
clarify, reject, or ignore a user utterance. Like
(Gabsdil and Lemon, 2004; Jonson, 2006), our
approach is more generally applicable than pre-
ceding research, since we frame our methodology
in the Information State Update (ISU) approach
to dialogue management (Traum et al., 1999) and
therefore expect it to be applicable to a range of
related multimodal dialogue systems.
</bodyText>
<sectionHeader confidence="0.992622" genericHeader="method">
3 User Simulations
</sectionHeader>
<bodyText confidence="0.999678263157895">
What makes this study different from the previous
work in the area of post-processing of the ASR hy-
potheses is the incorporation of a User Simulation
output as an additional feature. The history of a di-
alogue between a user and a dialogue system plays
an important role as to what the user might be ex-
pected to say next. As a result, most of the stud-
ies mentioned in the previous section make vari-
ous efforts to capture history by including relevant
features directly in their classifiers.
Various statistical User Simulations have been
trained on corpora of dialogue data in order to
simulate real user behaviour (Schatzmann et al.,
2006; Young, 2006; Georgila et al., 2006; Young
et al., 2007; Schatzmann et al., 2007). We devel-
oped a simple n-gram User Simulation, using n-
grams of dialogue moves. It treats a dialogue as
a sequence of lists of consecutive user and system
turns in a high level semantic representation, i.e.
</bodyText>
<page confidence="0.996535">
506
</page>
<bodyText confidence="0.994062222222222">
&lt; 5peechAct &gt;, &lt; Task &gt; pairs, for example
&lt; provide info &gt;, &lt; music genre(punk) &gt;.
It takes as input the n − 1 most recent lists of
&lt; 5peechAct &gt;, &lt; Task &gt; pairs in the dialogue
history, and uses the statistics in the training set
to compute a distribution over the possible next
user actions. If no n-grams match the current his-
tory, the model can back-off to n-grams of lower
order. We use this model to assess the likelihood
of each candidate ASR hypothesis. Intuitively, this
is the likelihood that the user really would say the
hypothesis in the current dialogue situation. The
benefit of using n-gram models is that they are fast
and simple to train even on large corpora.
The main hypothesis that we investigate is that
by using the User Simulation model to predict the
next user utterance, we can effectively increase the
performance of the speech recogniser module.
</bodyText>
<sectionHeader confidence="0.998503" genericHeader="method">
4 Evaluation metrics
</sectionHeader>
<bodyText confidence="0.9997773">
To evaluate performance we use Dialogue Move
Accuracy (DMA), a strict variant of Concept Er-
ror Rate (CER) as defined by (Boros et al., 1996),
which takes into account the semantic aspects of
the difference between the classified utterance and
the true transcription. CER is similar to WER,
since it takes into account deletions, insertions
and substitutions on the semantic (rather than the
word) level of the utterance. DMA is stricter than
CER in the sense that it does not allow for par-
tial matches in the semantic representation. In
other words, if the classified utterance corresponds
to the same semantic representation as the tran-
scribed then we have 100% DMA, otherwise 0%.
Sentence Accuracy (SA) is the alignment of a
single hypothesis in the n-best list with the true
transcription. Similarly to DMA, it accounts for
perfect alignment between the hypothesis and the
transcription, i.e. if they match perfectly we have
100% SA, otherwise 0%.
</bodyText>
<sectionHeader confidence="0.992164" genericHeader="method">
5 Data Collection
</sectionHeader>
<bodyText confidence="0.999839933333333">
For our experiments, we use data collected in a
user study with the Town-Info spoken dialogue
system, using the HTK speech recognizer (Young,
2007). In this study 18 subjects had to solve 10
search/browsing tasks with the system, resulting in
180 complete dialogues and 2564 utterances (av-
erage 14.24 user utterances per dialogue).
For each utterance we have a series of files of
60-best lists produced by the speech recogniser,
namely the transcription hypotheses on a sentence
level along with the acoustic model score and the
equivalent transcriptions on a word level, with in-
formation such as the duration of each recognised
frame and the confidence score of the acoustic and
language model of each word.
</bodyText>
<subsectionHeader confidence="0.996858">
5.1 Labeling
</subsectionHeader>
<bodyText confidence="0.999833826086957">
We transcribed all user utterances and parsed the
transcriptions offline using a natural language un-
derstanding component (a robust Keyword Parser)
in order to get a gold-standard labeling of the data.
We devised four labels with decreasing order of
confidence: ’opt’ (optimal), ’pos’ (positive), ’neg’
(negative), ’ign’ (ignore). These are automatically
generated using two different modules: a key-
word parser that computes the &lt; 5peechAct &gt;&lt;
Task &gt; pair as described in the previous sec-
tion and a Levenshtein Distance calculator, for the
computation of the DMA and WER of each hy-
pothesis respectively. The reason for opting for a
more abstract level, namely the semantics of the
hypotheses rather than individual word recogni-
tion, is that in SDS it is usually sufficient to rely
on the meaning of message that is being conveyed
by the user rather than the precise words that they
used.
Similar to (Gabsdil and Lemon, 2004; Jonson,
2006) we ascribe to each utterance either of the
’opt’, ’pos’, ’neg’, ’ign’ labels according to the
following schema:
</bodyText>
<listItem confidence="0.996602916666667">
• opt: The hypothesis is perfectly aligned and
semantically identical to the transcription
• pos: The hypothesis is not entirely aligned
(WER &lt; 50) but is semantically identical to
the transcription
• neg: The hypothesis is semantically identical
to the transcription but does not align well
(WER &gt; 50) or is semantically different to
the transcription
• ign: The hypothesis was not addressed to
the system (crosstalk), or the user laughed,
coughed, etc.
</listItem>
<bodyText confidence="0.999980833333333">
The 50% value for the WER as a threshold for
the distinction between the ’pos’ and ’neg’ cate-
gory is adopted from (Gabsdil, 2003), based on
the fact that WER is affected by concept accuracy
(Boros et al., 1996). In other words, if a hypothe-
sis is erroneous as far as its transcript is concerned
</bodyText>
<page confidence="0.989516">
507
</page>
<table confidence="0.90945">
Transcript: I’d like to find a bar please
I WOULD LIKE TO FIND A BAR PLEASE pos
I LIKE TO FIND A FOUR PLEASE neg
I’D LIKE TO FIND A BAR PLEASE opt
WOULD LIKE TO FIND THE OR PLEASE ign
</table>
<tableCaption confidence="0.999867">
Table 1: Example hypothesis labelling
</tableCaption>
<bodyText confidence="0.999552875">
then it is highly likely that it does not convey the
correct message from a semantic point of view.
We always label conceptually equivalent hypothe-
ses to a particular transcription as potential candi-
date dialogue strategy moves, and total misrecog-
nitions as rejections. In table 5.1 we show exam-
ples of the four labels. Note that in the case of
silence, we give an ’opt’ to the empty hypothesis.
</bodyText>
<sectionHeader confidence="0.805283" genericHeader="method">
6 The Baseline and Oracle Systems
</sectionHeader>
<bodyText confidence="0.99999025">
The baseline for our experiments is the behavior
of the Town-Info spoken dialogue system that was
used to collect the experimental data. We evaluate
the performance of the baseline system by analyz-
ing the dialogue logs from the user study.
As an oracle for the system we defined the
choice of either the first ’opt’ in the n-best list,
or if this does not exist the first ’pos’ in the list.
In this way it is guaranteed that we always get as
output a perfect match to the true transcript as far
as its Dialogue Move is concerned, provided there
exists a perfect match somewhere in the list.
</bodyText>
<subsectionHeader confidence="0.983335">
6.1 Baseline and Oracle Results
</subsectionHeader>
<bodyText confidence="0.9955868">
Table 2 summarizes the evaluation of the baseline
and oracle systems. We note that the Baseline sys-
tem already performs quite well on this data, when
we consider that in about 20% of n-best lists there
is no semantically correct hypothesis.
</bodyText>
<table confidence="0.999542">
Baseline Oracle
WER 47.72% 42.16%
DMA 75.05% 80.20%
SA 40.48% 45.27%
</table>
<tableCaption confidence="0.9546575">
Table 2: Baseline and Oracle results (statistically
significant at p &lt; 0.001)
</tableCaption>
<sectionHeader confidence="0.9908515" genericHeader="method">
7 Classifying and Selecting N-best
Recognition Hypotheses
</sectionHeader>
<bodyText confidence="0.9999502">
We use a threshold (50%) on a hypothesis’ WER
as an indicator for whether hypotheses should be
clarified or rejected. This is adopted from (Gabs-
dil, 2003), based on the fact that WER correlates
with concept accuracy (CA, (Boros et al., 1996)).
</bodyText>
<subsectionHeader confidence="0.976872">
7.1 Classification: Feature Groups
</subsectionHeader>
<bodyText confidence="0.992865166666667">
We represent recognition hypotheses as 13-
dimensional feature vectors for automatic classi-
fication. The feature vectors combine recognizer
confidence scores, low-level acoustic information,
and information from the User Simulation.
All the features used by the system are extracted
by the dialogue logs, the n-best lists per utterance
and per word and the audio files. The majority
of the features chosen are based on their success
in previous systems as described in the literature
(see section 2). The novel feature here is the User
Simulation score which may make redundant most
of the dialogue features used in other studies.
In order to measure the usefulness of each can-
didate feature and thus choose the most important
we use the metrics of Information Gain and Gain
Ratio (see table 3 in section 8.1) on the whole
training set, i.e. 93240 hypotheses.
In total 13 attributes were extracted, that can be
grouped into 4 main categories; those that concern
the current hypothesis to be classified, those that
concern low-level statistics of the audio files, those
that concern the whole n-best list, and finally the
User Simulation feature.
</bodyText>
<listItem confidence="0.994168333333333">
• Current Hypothesis Features (CHF) (6):
acoustic score, overall model confidence
score, minimum word confidence score,
grammar parsability, hypothesis length and
hypothesis duration.
• Acoustic Features (AF) (3): minimum, max-
imum and RMS amplitude
• List Features (LF) (3): n-best rank, deviation
of confidence scores in the list, match with
most frequent Dialogue Move
• User Simulation (US) (1): User Simulation
confidence score
</listItem>
<bodyText confidence="0.999707571428571">
The Current Hypothesis features (CHF) were
extracted from the n-best list files that contained
the hypotheses’ transcription along with overall
acoustic score per utterance and from the equiv-
alent files that contained the transcription of each
word along with the start of frame, end of frame
and confidence score:
</bodyText>
<page confidence="0.988815">
508
</page>
<bodyText confidence="0.99834082">
Acoustic score is the negative log likelihood as-
cribed by the speech recogniser to the whole hy-
pothesis, being the sum of the individual word
acoustic scores. Intuitively this is considered to
be helpful since it depicts the confidence of the
statistical model only for each word and is also
adopted in previous studies. Incorrect alignments
shall tend to adapt less well to the model and thus
have low log likelihood.
Overall model confidence score is the average
of the individual word confidence scores.
Minimum word confidence score is also com-
puted by the individual word transcriptions and ac-
counts for the confidence score of the word which
the speech recogniser is least certain of. It is ex-
pected to help our classifier distinguish between
poor overall hypothesis recognitions since a high
overall confidence score can sometimes be mis-
leading.
Grammar Parsability is the negative log
likelihood of the transcript for the current hy-
pothesis as produced by the Stanford Parser, a
wide-coverage Probabilistic Context-Free Gram-
mar (PCFG) (Klein and Manning, 2003) 1. This
feature seems helpful since we expect that a highly
ungrammatical hypothesis is likely not to match
with the true transcription semantically.
Hypothesis duration is the length of the hy-
pothesis in milliseconds as extracted from the n-
best list files with transcriptions per word that in-
clude the start and the end time of the recognised
frame. The reason for the inclusion of this fea-
ture is that it can help distinguish between short
utterances such as yes/no answers, medium-sized
utterances of normal answers and long utterances
caused by crosstalk.
Hypothesis length is the number of words in a
hypothesis and is considered to help in a similar
way as the above feature.
The Acoustic Features (AF) were extracted di-
rectly from the wave files using SoX: Minimum,
maximum and RMS amplitude are straightforward
features common in the previous studies men-
tioned in section 2.
The List Features (LF) were calculated based
on the n-best list files with transcriptions per utter-
ance and per word and take into account the whole
list:
N-best rank is the position of the hypothesis in
the list and could be useful in the sense that ’opt’
</bodyText>
<footnote confidence="0.874304">
1http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<bodyText confidence="0.999336648648648">
and ’pos’ may be found in the upper part of the list
rather than the bottom.
Deviation of confidence scores in the list is
the deviation of the overall model confidence score
of the hypothesis from the mean confidence score
in the list. This feature is extracted in the hope
that it will indicate potential clusters of confidence
scores in particular positions in the list, i.e. group
hypotheses that deviate in a specific fashion from
the mean and thus indicating them being classified
with the same label.
Match with most frequent Dialogue Move is
the only boolean feature and indicates whether the
Dialogue Move of the current hypothesis, i.e. the
pair of &lt; 5peechAct &gt;&lt; Task &gt; coincides with
the most frequent one. The trend in n-best lists
is to have a majority of utterances that belong to
one or two labels and only one hypothesis belong-
ing to the ’opt’ category and/or a few to the ’pos’
category. As a result, the idea behind this feature
is to extract such potential outliers which are the
desired goal for the re-ranker.
Finally, the User Simulation score is given as
an output from the User Simulation model and
adapted for the purposes of this study (see section
3 for more details). The model is operating with 5-
grams. Its input is given by two different sources:
the history of the dialogue, namely the 4 previous
Dialogue Moves, is taken from the dialogue log
and the current hypothesis’ semantic parse which
is generated on the fly by the same keyword parser
used in the automatic labelling.
User Simulation score is the probability that
the current hypothesis’ Dialogue Move has really
been said by the user given the 4 previous Dia-
logue Moves. The potential advantages of this fea-
ture have been discussed in section 3.
</bodyText>
<subsectionHeader confidence="0.996772">
7.2 Learner and Selection Procedure
</subsectionHeader>
<bodyText confidence="0.999973916666667">
We use the memory based learner TiMBL (Daele-
mans et al., 2002) to predict the class of each of
the 60-best recognition hypotheses for a given ut-
terance.
TiMBL was trained using different parameter
combinations mainly choosing between number of
k-nearest neighbours (1 to 5) and distance metrics
(Weighted Overlap and Modified Value Difference
Metric). In a second step, we decide which (if any)
of the classified hypotheses we actually want to
pick as the best result and how the user utterance
should be classified as a whole.
</bodyText>
<page confidence="0.993596">
509
</page>
<listItem confidence="0.998802181818182">
1. Scan the list of classified n-best recognition
hypotheses top-down. Return the first result
that is classified as ’opt’.
2. If 1. fails, scan the list of classified n-best
recognition hypotheses top-down. Return the
first result that is classified as ’pos’.
3. If 2. fails, count the number of negs and igns
in the classified recognition hypotheses. If
the number of negs is larger or equal than the
number of igns then return the first ’neg’.
4. Else return the first ’ign’ utterance.
</listItem>
<sectionHeader confidence="0.95909" genericHeader="method">
8 Experiments
</sectionHeader>
<bodyText confidence="0.99997712">
Experiments were conducted in two layers: the
first layer concerns only the classifier, i.e. the abil-
ity of the system to correctly classify each hypoth-
esis to either of the four labels ’opt’, ’pos’, ’neg’,
’ign’ and the second layer the re-ranker, i.e. the
ability of the system to boost the speech recog-
niser’s accuracy.
All results are drawn from the TiMBL classi-
fier trained with the Weighted Overlap metric and
k = 1 nearest neighbours settings. Both layers
are trained on 75% of the same Town-Info Corpus
of 126 dialogues containing 60-best lists for 1554
user utterances or a total of 93240 hypotheses. The
first layer was tested against a separate Town-Info
Corpus of 58 dialogues containing 510 user utter-
ances or a total of 30600 hypotheses, while the
second was tested on the whole training set with
10-fold cross-validation.
Using this corpus, a series of experiments was
carried out using different sets of features in order
to both determine and illustrate the increasing per-
formance of the classifier. These sets were deter-
mined not only by the literature but also by the In-
formation Gain measures that were calculated on
the training set using WEKA, as shown in table 3.
</bodyText>
<subsectionHeader confidence="0.942862">
8.1 Information Gain
</subsectionHeader>
<bodyText confidence="0.995522125">
Quite surprisingly, we note that the rank given by
the Information Gain measure coincides perfectly
with the logical grouping of the attributes that was
initially performed (see table 3).
As a result, we chose to use this grouping for
the final 4 feature sets on which the classifier
experiments were performed, in the following
order:
</bodyText>
<table confidence="0.9839974">
Experiment 1: List Features (LF)
InfoGain Attribute
1.0324 userSimulationScore
0.9038 rmsAmp
0.8280 minAmp
0.8087 maxAmp
0.4861 parsability
0.3975 acousScore
0.3773 hypothesisDuration
0.2545 hypothesisLength
0.1627 avgConfScore
0.1085 minWordConfidence
0.0511 nBestRank
0.0447 standardDeviation
0.0408 matchesFrequentDM
</table>
<tableCaption confidence="0.991398">
Table 3: Information Gain
</tableCaption>
<table confidence="0.888700857142857">
Experiment 2: List Features + Current Hypothe-
sis Features (LF+CHF)
Experiment 3: List Features + Current Hypothe-
sis Features + Acoustic Features (LF+CHF+AF)
Experiment 4: List Features + Current Hy-
pothesis Features + Acoustic Features + User
Simulation (LF+CHF+AF+US)
</table>
<bodyText confidence="0.999926333333333">
Note that the User Simulation score is a very
strong feature, scoring first in the Information
Gain rank, validating our central hypothesis.
The testing of the classifier using each of the
above feature sets was performed on the remain-
ing 25% of the Town-Info corpus comprising of 58
dialogues, consisting of 510 utterances and taking
the 60-best lists resulting in a total of 30600 vec-
tors. In each experiment we measured Precision,
Recall, F-measure per class and total Accuracy of
the classifier.
For the second layer, we used a trained instance
of the TiMBL classifier on the 4th feature set (List
Features + Current Hypothesis Features + Acous-
tic Features + User Simulation) and performed re-
ranking using the algorithm presented in section
7.2 on the same training set used in the first layer
using 10-fold cross validation.
</bodyText>
<sectionHeader confidence="0.997351" genericHeader="evaluation">
9 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.9997555">
We performed two series of experiments in two
layers: the first corresponds to the training of the
classifier alone and the second to the system as a
whole measuring the re-ranker’s output.
</bodyText>
<page confidence="0.988047">
510
</page>
<table confidence="0.999869">
Feature set (opt) Precision Recall F1
LF 42.5% 58.4% 49.2%
LF+CHF 62.4% 65.7% 64.0%
LF+CHF+AF 55.6% 61.6% 58.4%
LF+CHF+AF+US 70.5% 73.7% 72.1%
</table>
<tableCaption confidence="0.891839">
Table 4: Results for the ’opt’ category
</tableCaption>
<table confidence="0.9999106">
Feature set (pos) Precision Recall F1
LF 25.2% 1.7% 3.2%
LF+CHF 51.2% 57.4% 54.1%
LF+CHF+AF 51.5% 54.6% 53.0%
LF+CHF+AF+US 64.8% 61.8% 63.3%
</table>
<tableCaption confidence="0.997326333333333">
Table 5: Results for the ’pos’ category
Table 8: F1-Measure and Accuracy for the four
attribute sets
</tableCaption>
<table confidence="0.963743166666667">
Feature set (ign) Precision Recall F1
LF 19.6% 1.3% 2.5%
LF+CHF 63.5% 48.7% 55.2%
LF+CHF+AF 59.3% 48.9% 53.6%
LF+CHF+AF+US 99.9% 99.9% 99.9%
Table 7: Results for the ’ign’ category
Feature set F1 Accuracy
Baseline - 51.1%
LF 37.3% 53.1%
LF+CHF 64.1% 64.8%
LF+CHF+AF 62.6% 63.4%
LF+CHF+AF+US 86.0% 84.9%
</table>
<subsectionHeader confidence="0.930566">
9.1 First Layer: Classifier Experiments
</subsectionHeader>
<bodyText confidence="0.99971732">
In these series of experiments we measure preci-
sion, recall and F1-measure for each of the four
labels and overall F1-measure and accuracy of the
classifier. In order to have a better view of the
classifier’s performance we have also included the
confusion matrix for the final experiment with all
13 attributes. Tables 4 -7 show per class and per
attribute set measures, while Table 8 shows a col-
lective view of the results for the four sets of at-
tributes and the baseline being the majority class
label ’neg’. Table 9 shows the confusion matrix
for the final experiment.
In tables 4 - 8 we generally notice an increase
in precision, recall and F1-measure as we pro-
gressively add more attributes to the system with
the exception of the addition of the Acoustic Fea-
tures which seem to impair the classifier’s perfor-
mance. We also make note of the fact that in the
case of the 4th attribute set the classifier can dis-
tinguish very well the ’neg’ and ’ign’ categories
with 86.3% and 99.9% F1-measure respectively.
Most importantly, we observe a remarkable boost
in F1-measure and accuracy with the addition of
the User Simulation score. We find a 37.36% rel-
ative increase in F1-measure and 34.02% increase
</bodyText>
<table confidence="0.9959914">
Feature set (neg) Precision Recall F1
LF 54.2% 96.4% 69.4%
LF+CHF 70.7% 75.0% 72.8%
LF+CHF+AF 69.5% 73.4% 71.4%
LF+CHF+AF+US 85.6% 87.0% 86.3%
</table>
<tableCaption confidence="0.995908">
Table 6: Results for the ’neg’ category
</tableCaption>
<bodyText confidence="0.999944866666666">
in the accuracy compared to the 3rd experiment,
which contains all but the User Simulation score
attribute and a 66.20% relative increase of the ac-
curacy compared to the Baseline. In table 7 we
make note of a rather low recall measure for the
’ign’ category in the case of the LF experiment,
suggesting that the list features do not add extra
value to the classifier, partially validating the In-
formation Gain measure (Table 3).
Taking a closer look at the 4th experiment with
all 13 features we notice in table 9 that most er-
rors occur between the ’pos’ and ’neg’ category.
In fact, for the ’neg’ category the False Positive
Rate (FPR) is 18.17% and for the ’pos’ 8.9%, all
in all a lot larger than for the other categories.
</bodyText>
<subsectionHeader confidence="0.986036">
9.2 Second Layer: Re-ranker Experiments
</subsectionHeader>
<bodyText confidence="0.913362307692308">
In these experiments we measure WER, DMA
and SA for the system as a whole. In order to
make sure that the improvement noted was re-
ally attributed to the classifier we computed the
p-values for each of these measures using the
Wilcoxon signed rank test for WER and McNemar
chi-square test for the DMA and SA measures.
In table 10 we note that the classifier scores
opt pos neg ign
opt 232 37 46 0
pos 47 4405 2682 8
neg 45 2045 13498 0
ign 5 0 0 7550
</bodyText>
<tableCaption confidence="0.992397">
Table 9: Confusion Matrix for LF+CHF+AF+US
</tableCaption>
<page confidence="0.907514">
511
</page>
<table confidence="0.9888345">
Baseline Classifier Oracle
WER 47.72% 45.27% ** 42.16%***
DMA 75.05% 78.22% * 80.20% ***
SA 40.48% 42.26% 45.27%***
</table>
<tableCaption confidence="0.984697">
Table 10: Baseline, Classifier, and Oracle results
</tableCaption>
<table confidence="0.936115">
(*** = p &lt; 0.001, ** = p &lt; 0.01, * = p &lt; 0.05)
Label Precision Recall F1
opt 74.0% 64.1% 68.7%
pos 76.3% 46.2% 57.6%
neg 81.9% 94.4% 87.7%
ign 99.9% 99.9% 99.9%
</table>
<tableCaption confidence="0.9288455">
Table 11: Precision, Recall and F1: high-level fea-
tures
</tableCaption>
<bodyText confidence="0.999895142857143">
45.27% WER making a notable relative reduction
of 5.13% compared to the baseline and 78.22%
DMA incurring a relative improvement of 4.22%.
The classifier scored 42.26% on SA but it was
not considered significant compared to the base-
line (0.05 &lt; p &lt; 0.10). Comparing the classifier’s
performance with the Oracle it achieves a 44.06%
of the possible WER improvement on this data,
61.55% for the DMA measure and 37.16% for the
SA measure.
Finally, we also notice that the Oracle has a
80.20% for the DMA, which means that 19.80%
of the n-best lists did not include at all a hypothe-
sis that matched semantically to the true transcript.
</bodyText>
<sectionHeader confidence="0.939047" genericHeader="evaluation">
10 Experiment with high-level features
</sectionHeader>
<bodyText confidence="0.999476529411765">
We trained a Memory Based Classifier based only
on the higher level features of merely the User
Simulation score and the Grammar Parsability
(US + GP). The idea behind this choice is to try
and find a combination of features that ignores low
level characteristics of the user’s utterances as well
as features that heavily rely on the speech recog-
niser and thus by default are not considered to be
very trustworthy.
Quite surprisingly, the results taken from an ex-
periment with just the User Simulation score and
the Grammar Parsability are very promising and
comparable with those acquired from the 4th ex-
periment with all 13 attributes. Table 11 shows
the precision, recall and F1-measure per label and
table 12 illustrates the classifier’s performance in
comparison with the 4th experiment.
</bodyText>
<tableCaption confidence="0.877419">
Table 12 shows that there is a somewhat consid-
</tableCaption>
<table confidence="0.990364">
Feature set F1 Accuracy Ties
LF+CHF+AF+US 86.0% 84.9% 4993
US+GP 85.7% 85.6% 115
</table>
<tableCaption confidence="0.918634">
Table 12: F1, Accuracy and number of ties cor-
</tableCaption>
<bodyText confidence="0.9249612">
rectly resolved for LF+CHF+AF+US and US+GP
feature sets
erable decrease in the recall and a corresponding
increase in the precision of the ’pos’ and ’opt’ cat-
egories compared to the LF + CHF + AF + US at-
tribute set, which account for lower F1-measures.
However, all in all the US + GP set manages to
classify correctly 207 more vectors and quite in-
terestingly commits far fewer ties and manages to
resolve more compared to the full 13 attribute set.
</bodyText>
<sectionHeader confidence="0.993035" genericHeader="conclusions">
11 Conclusion
</sectionHeader>
<bodyText confidence="0.999983666666667">
We used a combination of acoustic features and
features computed from dialogue context to pre-
dict the quality of incoming recognition hypothe-
ses to an SDS. In particular we use a score com-
puted from a simple statistical User Simulation,
which measures the likelihood that the user re-
ally said each hypothesis. The approach is novel
in combining User Simulations, machine learning,
and n-best processing for spoken dialogue sys-
tems. We employed a User Simulation model,
trained on real dialogue data, to predict the user’s
next dialogue move. This prediction was used to
re-rank n-best hypotheses of a speech recognizer
for a corpus of 2564 user utterances. The results,
obtained using TiMBL and an n-gram User Sim-
ulation, show a significant relative reduction of
Word Error Rate of 5% (this is 44% of the pos-
sible WER improvement on this data), and 62%
of the possible Dialogue Move Accuracy improve-
ment, compared to the baseline policy of selecting
the topmost ASR hypothesis. The majority of the
improvement is attributable to the User Simulation
feature. Clearly, this improvement would result in
better dialogue system performance overall.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.961407714285714">
We thank Helen Hastie and Kallirroi Georgila.
The research leading to these results has re-
ceived funding from the EPSRC (project no.
EP/E019501/1) and from the European Commu-
nity’s Seventh Framework Programme (FP7/2007-
2013) under grant agreement no. 216594 (CLAS-
SiC project www.classic-project.org)
</bodyText>
<page confidence="0.995259">
512
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999787366197183">
M. Boros, W. Eckert, F. Gallwitz, G. G¨orz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In Proceedings ICSLP ’96, volume 2,
pages 1009–1012, Philadelphia, PA.
Ananlada Chotimongkol and Alexander I. Rudnicky.
2001. N-best Speech Hypotheses Reordering Using
Linear Regression. In Proceedings of EuroSpeech
2001, pages 1829–1832.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2002. TIMBL: Tilburg Mem-
ory Based Learner, version 4.2, Reference Guide. In
ILK Technical Report 02-01.
Malte Gabsdil and Oliver Lemon. 2004. Combining
acoustic and pragmatic features to predict recogni-
tion performance in spoken dialogue systems. In
Proceedings of ACL-04, pages 344–351.
Malte Gabsdil. 2003. Classifying Recognition Results
for Spoken Dialogue Systems. In Proceedings of the
Student Research Workshop at ACL-03.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2006. User simulation for spoken dialogue
systems: Learning and evaluation. In Proceedings
of Interspeech/ICSLP, pages 1065–1068.
R. Jonson. 2006. Dialogue Context-Based Re-ranking
of ASR Hypotheses. In Proceedings IEEE 2006
Workshop on Spoken Language Technology.
D. Klein and C. Manning. 2003. Fast exact inference
with a factored model for natural language parsing.
Journal ofAdvances in Neural Information Process-
ing Systems, 15(2).
Diane J. Litman, Julia Hirschberg, and Marc Swerts.
2000. Predicting Automatic Speech Recognition
Performance Using Prosodic Cues. In Proceedings
of NAACL.
M. Pickering and S. Garrod. 2007. Do people use lan-
guage production to make predictions during com-
prehension? Journal of Trends in Cognitive Sci-
ences, 11(3).
J Schatzmann, K Weilhammer, M N Stuttle, and S J
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. Knowledge En-
gineering Review, 21:97–126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proceedings of HLT/NAACL.
David Traum, Johan Bos, Robin Cooper, Staffan Lars-
son, Ian Lewin, Colin Matheson, and Massimo Poe-
sio. 1999. A Model of Dialogue Moves and In-
formation State Revision. Technical Report D2.1,
Trindi Project.
Marilyn Walker, Jerry Wright, and Irene Langkilde.
2000. Using Natural Language Processing and Dis-
course Features to Identify Understanding Errors
in a Spoken Dialogue System. In Proceedings of
ICML-2000.
Jason Williams and Steve Young. 2003. Using wizard-
of-oz simulations to bootstrap reinforcement-
learning-based dialog management systems. In
Proc. 4th SIGdial workshop.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
SJ Young. 2006. Using POMDPs for Dialog Manage-
ment. In IEEE/ACL Workshop on Spoken Language
Technology (SLT 2006), Aruba.
Steve Young. 2007. ATK: An Application Toolkit
for HTK, Version 1.6. Technical report, Cambridge
University Engineering Department.
</reference>
<page confidence="0.998857">
513
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919455">
<title confidence="0.9980375">User Simulations for context-sensitive speech recognition in Spoken Dialogue Systems</title>
<author confidence="0.981025">Oliver Lemon Ioannis Konstas</author>
<affiliation confidence="0.999128">Edinburgh University University of Glasgow</affiliation>
<email confidence="0.985954">olemon@inf.ed.ac.ukkonstas@dcs.gla.ac.uk</email>
<abstract confidence="0.997673068965517">We use a machine learner trained on a combination of acoustic and contextual features to predict the accuracy of incoming n-best automatic speech recognition (ASR) hypotheses to a spoken dialogue system (SDS). Our novel approach is to use a simple statistical User Simulation (US) for this task, which measures the likelihood that the user would say each hypothesis in the current context. Such US models are now common in machine learning approaches to SDS, are trained on real dialogue data, and are related to theories of “alignment” in psycholinguistics. We use a US to predict the user’s next dialogue move and thereby re-rank n-best hypotheses of a speech recognizer for a corpus of 2564 user utterances. The method achieved a significant relative reduction of Word Error Rate (WER) of 5% (this is 44% of the possible WER improvement on this data), and 62% of the possible semantic improvement (Dialogue Move Accuracy), compared to the baseline policy of selecting the topmost ASR hypothesis. The majority of the improvement is attributable to the User Simulation feature, as shown by Information Gain analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Boros</author>
<author>W Eckert</author>
<author>F Gallwitz</author>
<author>G G¨orz</author>
<author>G Hanrieder</author>
<author>H Niemann</author>
</authors>
<title>Towards understanding spontaneous speech: Word accuracy vs. concept accuracy.</title>
<date>1996</date>
<booktitle>In Proceedings ICSLP ’96,</booktitle>
<volume>2</volume>
<pages>1009--1012</pages>
<location>Philadelphia, PA.</location>
<marker>Boros, Eckert, Gallwitz, G¨orz, Hanrieder, Niemann, 1996</marker>
<rawString>M. Boros, W. Eckert, F. Gallwitz, G. G¨orz, G. Hanrieder, and H. Niemann. 1996. Towards understanding spontaneous speech: Word accuracy vs. concept accuracy. In Proceedings ICSLP ’96, volume 2, pages 1009–1012, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananlada Chotimongkol</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>N-best Speech Hypotheses Reordering Using Linear Regression.</title>
<date>2001</date>
<booktitle>In Proceedings of EuroSpeech</booktitle>
<pages>1829--1832</pages>
<contexts>
<context position="6942" citStr="Chotimongkol and Rudnicky, 2001" startWordPosition="1085" endWordPosition="1088">han the other attributes. (Jonson, 2006) classifies recognition hypotheses with labels denoting acceptance, clarification, confirmation and rejection. These labels were learned in a similar way to (Gabsdil and Lemon, 2004) and correspond to varying levels of confidence, being essentially potential directives to the dialogue manager. Apart from standard features Jonson includes attributes that account for the whole n-best list, i.e. standard deviation of confidence scores. As well as the use of a User Simulation, the main difference between our approach and work on hypothesis reordering (e.g. (Chotimongkol and Rudnicky, 2001)) is that we make a decision regarding whether a dialogue system should accept, clarify, reject, or ignore a user utterance. Like (Gabsdil and Lemon, 2004; Jonson, 2006), our approach is more generally applicable than preceding research, since we frame our methodology in the Information State Update (ISU) approach to dialogue management (Traum et al., 1999) and therefore expect it to be applicable to a range of related multimodal dialogue systems. 3 User Simulations What makes this study different from the previous work in the area of post-processing of the ASR hypotheses is the incorporation </context>
</contexts>
<marker>Chotimongkol, Rudnicky, 2001</marker>
<rawString>Ananlada Chotimongkol and Alexander I. Rudnicky. 2001. N-best Speech Hypotheses Reordering Using Linear Regression. In Proceedings of EuroSpeech 2001, pages 1829–1832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TIMBL: Tilburg Memory Based Learner, version 4.2, Reference Guide. In ILK</title>
<date>2002</date>
<tech>Technical Report 02-01.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2002</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2002. TIMBL: Tilburg Memory Based Learner, version 4.2, Reference Guide. In ILK Technical Report 02-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Gabsdil</author>
<author>Oliver Lemon</author>
</authors>
<title>Combining acoustic and pragmatic features to predict recognition performance in spoken dialogue systems.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04,</booktitle>
<pages>344--351</pages>
<contexts>
<context position="6023" citStr="Gabsdil and Lemon, 2004" startWordPosition="948" endWordPosition="951">ed to these experiments in that we also combine confidence scores and higher-level features for classification. However, both (Litman et al., 2000) and (Walker et al., 2000) consider only single-best recognition results and thus use their classifiers as “filters” to decide whether the best recognition hypothesis for a user utterance is correct or not. We go a step further in that we classify n-best hypotheses and then select among the alternatives. We also explore the use of more dialogue and task-oriented features (e.g. the dialogue move type of a recognition hypothesis) for classification. (Gabsdil and Lemon, 2004) similarly perform reordering of n-best lists by combining acoustic and pragmatic features. Their study shows that dialogue features such as the previous system question and whether a hypothesis is the correct answer to a particular question contributed more to classification accuracy than the other attributes. (Jonson, 2006) classifies recognition hypotheses with labels denoting acceptance, clarification, confirmation and rejection. These labels were learned in a similar way to (Gabsdil and Lemon, 2004) and correspond to varying levels of confidence, being essentially potential directives to </context>
<context position="11855" citStr="Gabsdil and Lemon, 2004" startWordPosition="1909" endWordPosition="1912">g’ (negative), ’ign’ (ignore). These are automatically generated using two different modules: a keyword parser that computes the &lt; 5peechAct &gt;&lt; Task &gt; pair as described in the previous section and a Levenshtein Distance calculator, for the computation of the DMA and WER of each hypothesis respectively. The reason for opting for a more abstract level, namely the semantics of the hypotheses rather than individual word recognition, is that in SDS it is usually sufficient to rely on the meaning of message that is being conveyed by the user rather than the precise words that they used. Similar to (Gabsdil and Lemon, 2004; Jonson, 2006) we ascribe to each utterance either of the ’opt’, ’pos’, ’neg’, ’ign’ labels according to the following schema: • opt: The hypothesis is perfectly aligned and semantically identical to the transcription • pos: The hypothesis is not entirely aligned (WER &lt; 50) but is semantically identical to the transcription • neg: The hypothesis is semantically identical to the transcription but does not align well (WER &gt; 50) or is semantically different to the transcription • ign: The hypothesis was not addressed to the system (crosstalk), or the user laughed, coughed, etc. The 50% value for</context>
</contexts>
<marker>Gabsdil, Lemon, 2004</marker>
<rawString>Malte Gabsdil and Oliver Lemon. 2004. Combining acoustic and pragmatic features to predict recognition performance in spoken dialogue systems. In Proceedings of ACL-04, pages 344–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Gabsdil</author>
</authors>
<title>Classifying Recognition Results for Spoken Dialogue Systems.</title>
<date>2003</date>
<booktitle>In Proceedings of the Student Research Workshop at ACL-03.</booktitle>
<contexts>
<context position="12567" citStr="Gabsdil, 2003" startWordPosition="2028" endWordPosition="2029">ding to the following schema: • opt: The hypothesis is perfectly aligned and semantically identical to the transcription • pos: The hypothesis is not entirely aligned (WER &lt; 50) but is semantically identical to the transcription • neg: The hypothesis is semantically identical to the transcription but does not align well (WER &gt; 50) or is semantically different to the transcription • ign: The hypothesis was not addressed to the system (crosstalk), or the user laughed, coughed, etc. The 50% value for the WER as a threshold for the distinction between the ’pos’ and ’neg’ category is adopted from (Gabsdil, 2003), based on the fact that WER is affected by concept accuracy (Boros et al., 1996). In other words, if a hypothesis is erroneous as far as its transcript is concerned 507 Transcript: I’d like to find a bar please I WOULD LIKE TO FIND A BAR PLEASE pos I LIKE TO FIND A FOUR PLEASE neg I’D LIKE TO FIND A BAR PLEASE opt WOULD LIKE TO FIND THE OR PLEASE ign Table 1: Example hypothesis labelling then it is highly likely that it does not convey the correct message from a semantic point of view. We always label conceptually equivalent hypotheses to a particular transcription as potential candidate dial</context>
<context position="14612" citStr="Gabsdil, 2003" startWordPosition="2391" endWordPosition="2393">racle Results Table 2 summarizes the evaluation of the baseline and oracle systems. We note that the Baseline system already performs quite well on this data, when we consider that in about 20% of n-best lists there is no semantically correct hypothesis. Baseline Oracle WER 47.72% 42.16% DMA 75.05% 80.20% SA 40.48% 45.27% Table 2: Baseline and Oracle results (statistically significant at p &lt; 0.001) 7 Classifying and Selecting N-best Recognition Hypotheses We use a threshold (50%) on a hypothesis’ WER as an indicator for whether hypotheses should be clarified or rejected. This is adopted from (Gabsdil, 2003), based on the fact that WER correlates with concept accuracy (CA, (Boros et al., 1996)). 7.1 Classification: Feature Groups We represent recognition hypotheses as 13- dimensional feature vectors for automatic classification. The feature vectors combine recognizer confidence scores, low-level acoustic information, and information from the User Simulation. All the features used by the system are extracted by the dialogue logs, the n-best lists per utterance and per word and the audio files. The majority of the features chosen are based on their success in previous systems as described in the li</context>
</contexts>
<marker>Gabsdil, 2003</marker>
<rawString>Malte Gabsdil. 2003. Classifying Recognition Results for Spoken Dialogue Systems. In Proceedings of the Student Research Workshop at ACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>User simulation for spoken dialogue systems: Learning and evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech/ICSLP,</booktitle>
<pages>1065--1068</pages>
<contexts>
<context position="8087" citStr="Georgila et al., 2006" startWordPosition="1277" endWordPosition="1280">k in the area of post-processing of the ASR hypotheses is the incorporation of a User Simulation output as an additional feature. The history of a dialogue between a user and a dialogue system plays an important role as to what the user might be expected to say next. As a result, most of the studies mentioned in the previous section make various efforts to capture history by including relevant features directly in their classifiers. Various statistical User Simulations have been trained on corpora of dialogue data in order to simulate real user behaviour (Schatzmann et al., 2006; Young, 2006; Georgila et al., 2006; Young et al., 2007; Schatzmann et al., 2007). We developed a simple n-gram User Simulation, using ngrams of dialogue moves. It treats a dialogue as a sequence of lists of consecutive user and system turns in a high level semantic representation, i.e. 506 &lt; 5peechAct &gt;, &lt; Task &gt; pairs, for example &lt; provide info &gt;, &lt; music genre(punk) &gt;. It takes as input the n − 1 most recent lists of &lt; 5peechAct &gt;, &lt; Task &gt; pairs in the dialogue history, and uses the statistics in the training set to compute a distribution over the possible next user actions. If no n-grams match the current history, the mod</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2006</marker>
<rawString>Kallirroi Georgila, James Henderson, and Oliver Lemon. 2006. User simulation for spoken dialogue systems: Learning and evaluation. In Proceedings of Interspeech/ICSLP, pages 1065–1068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jonson</author>
</authors>
<title>Dialogue Context-Based Re-ranking of ASR Hypotheses.</title>
<date>2006</date>
<booktitle>In Proceedings IEEE 2006 Workshop on Spoken Language Technology.</booktitle>
<contexts>
<context position="6350" citStr="Jonson, 2006" startWordPosition="1000" endWordPosition="1001">correct or not. We go a step further in that we classify n-best hypotheses and then select among the alternatives. We also explore the use of more dialogue and task-oriented features (e.g. the dialogue move type of a recognition hypothesis) for classification. (Gabsdil and Lemon, 2004) similarly perform reordering of n-best lists by combining acoustic and pragmatic features. Their study shows that dialogue features such as the previous system question and whether a hypothesis is the correct answer to a particular question contributed more to classification accuracy than the other attributes. (Jonson, 2006) classifies recognition hypotheses with labels denoting acceptance, clarification, confirmation and rejection. These labels were learned in a similar way to (Gabsdil and Lemon, 2004) and correspond to varying levels of confidence, being essentially potential directives to the dialogue manager. Apart from standard features Jonson includes attributes that account for the whole n-best list, i.e. standard deviation of confidence scores. As well as the use of a User Simulation, the main difference between our approach and work on hypothesis reordering (e.g. (Chotimongkol and Rudnicky, 2001)) is tha</context>
<context position="11870" citStr="Jonson, 2006" startWordPosition="1913" endWordPosition="1914">ore). These are automatically generated using two different modules: a keyword parser that computes the &lt; 5peechAct &gt;&lt; Task &gt; pair as described in the previous section and a Levenshtein Distance calculator, for the computation of the DMA and WER of each hypothesis respectively. The reason for opting for a more abstract level, namely the semantics of the hypotheses rather than individual word recognition, is that in SDS it is usually sufficient to rely on the meaning of message that is being conveyed by the user rather than the precise words that they used. Similar to (Gabsdil and Lemon, 2004; Jonson, 2006) we ascribe to each utterance either of the ’opt’, ’pos’, ’neg’, ’ign’ labels according to the following schema: • opt: The hypothesis is perfectly aligned and semantically identical to the transcription • pos: The hypothesis is not entirely aligned (WER &lt; 50) but is semantically identical to the transcription • neg: The hypothesis is semantically identical to the transcription but does not align well (WER &gt; 50) or is semantically different to the transcription • ign: The hypothesis was not addressed to the system (crosstalk), or the user laughed, coughed, etc. The 50% value for the WER as a t</context>
</contexts>
<marker>Jonson, 2006</marker>
<rawString>R. Jonson. 2006. Dialogue Context-Based Re-ranking of ASR Hypotheses. In Proceedings IEEE 2006 Workshop on Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>Journal ofAdvances in Neural Information Processing Systems,</booktitle>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="17702" citStr="Klein and Manning, 2003" startWordPosition="2878" endWordPosition="2881">score is the average of the individual word confidence scores. Minimum word confidence score is also computed by the individual word transcriptions and accounts for the confidence score of the word which the speech recogniser is least certain of. It is expected to help our classifier distinguish between poor overall hypothesis recognitions since a high overall confidence score can sometimes be misleading. Grammar Parsability is the negative log likelihood of the transcript for the current hypothesis as produced by the Stanford Parser, a wide-coverage Probabilistic Context-Free Grammar (PCFG) (Klein and Manning, 2003) 1. This feature seems helpful since we expect that a highly ungrammatical hypothesis is likely not to match with the true transcription semantically. Hypothesis duration is the length of the hypothesis in milliseconds as extracted from the nbest list files with transcriptions per word that include the start and the end time of the recognised frame. The reason for the inclusion of this feature is that it can help distinguish between short utterances such as yes/no answers, medium-sized utterances of normal answers and long utterances caused by crosstalk. Hypothesis length is the number of word</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Fast exact inference with a factored model for natural language parsing. Journal ofAdvances in Neural Information Processing Systems, 15(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Julia Hirschberg</author>
<author>Marc Swerts</author>
</authors>
<title>Predicting Automatic Speech Recognition Performance Using Prosodic Cues.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="4702" citStr="Litman et al., 2000" startWordPosition="749" endWordPosition="752">ng hypotheses. Section 7 describes our learning experiments for classifying and selecting from n-best recognition hypotheses and Section 9 reports our results. 2 Relation to Previous Work In psycholinguistics, the idea that human dialogue participants simulate each other to some extent is gaining currency. (Pickering and Garrod, 2007) write: “if B overtly imitates A, then A’s comprehension of B’s utterance is facilitated by A’s memory for A’s previous utterance.” We explore aspects of this idea in a computational manner. Similar work in the area of spoken dialogue systems is described below. (Litman et al., 2000) use acoustic-prosodic information extracted from speech waveforms, together with information derived from their speech recognizer, to automatically predict misrecognized turns in a corpus of train-timetable information dialogues. In our experiments, we also use recognizer confidence scores and a limited number of acoustic-prosodic features (e.g. amplitude in the speech signal) for hypothesis classification, but we also use User Simulation predictions. (Walker et al., 2000) use a combination of features from the speech recognizer, natural language understanding, and dialogue manager/discourse </context>
</contexts>
<marker>Litman, Hirschberg, Swerts, 2000</marker>
<rawString>Diane J. Litman, Julia Hirschberg, and Marc Swerts. 2000. Predicting Automatic Speech Recognition Performance Using Prosodic Cues. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pickering</author>
<author>S Garrod</author>
</authors>
<title>Do people use language production to make predictions during comprehension?</title>
<date>2007</date>
<journal>Journal of Trends in Cognitive Sciences,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="4418" citStr="Pickering and Garrod, 2007" startWordPosition="699" endWordPosition="702"> acts in the dialogue history, that the US is sensitive to (see section 3). The paper is organized as follows. After a short relation to previous work, we describe the data (Section 5) and derive baseline results (Section 6). Section 3 describes the User Simulations that we use for re-ranking hypotheses. Section 7 describes our learning experiments for classifying and selecting from n-best recognition hypotheses and Section 9 reports our results. 2 Relation to Previous Work In psycholinguistics, the idea that human dialogue participants simulate each other to some extent is gaining currency. (Pickering and Garrod, 2007) write: “if B overtly imitates A, then A’s comprehension of B’s utterance is facilitated by A’s memory for A’s previous utterance.” We explore aspects of this idea in a computational manner. Similar work in the area of spoken dialogue systems is described below. (Litman et al., 2000) use acoustic-prosodic information extracted from speech waveforms, together with information derived from their speech recognizer, to automatically predict misrecognized turns in a corpus of train-timetable information dialogues. In our experiments, we also use recognizer confidence scores and a limited number of </context>
</contexts>
<marker>Pickering, Garrod, 2007</marker>
<rawString>M. Pickering and S. Garrod. 2007. Do people use language production to make predictions during comprehension? Journal of Trends in Cognitive Sciences, 11(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>M N Stuttle</author>
<author>S J Young</author>
</authors>
<title>A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies. Knowledge Engineering Review,</title>
<date>2006</date>
<pages>21--97</pages>
<contexts>
<context position="3117" citStr="Schatzmann et al., 2006" startWordPosition="489" endWordPosition="492"> “best” hypothesis and to decide on appropriate system reactions. We evaluate this approach in comparison with a baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list. In such systems, complex repair strategies are required when the top hypothesis is incorrect. The main novelty of this work is that we explore the use of predictions from simple statistical User Simulations to re-rank n-best lists of ASR hypotheses. These User Simulations are now commonly used in statistical learning approaches to dialogue management (Williams and Young, 2003; Schatzmann et al., 2006; Young, 2006; Young et al., 2007; Schatzmann et al., 2007), but they have not been used for context-sensitive ASR before. In our model, the system’s “belief” b(h) in a recognition hypothesis h is factored in two parts: the observation probability P(oIh) (approximated by the ASR confidence score) and the User Simulation probability P(hIus, C) of the hypothesis: b(h) = P(olh).P(hlus, C) (1) where us is the state of the User Simulation in context C. The context is simply a window of diProceedings of the 12th Conference of the European Chapter of the ACL, pages 505–513, Athens, Greece, 30 March –</context>
<context position="8051" citStr="Schatzmann et al., 2006" startWordPosition="1271" endWordPosition="1274"> study different from the previous work in the area of post-processing of the ASR hypotheses is the incorporation of a User Simulation output as an additional feature. The history of a dialogue between a user and a dialogue system plays an important role as to what the user might be expected to say next. As a result, most of the studies mentioned in the previous section make various efforts to capture history by including relevant features directly in their classifiers. Various statistical User Simulations have been trained on corpora of dialogue data in order to simulate real user behaviour (Schatzmann et al., 2006; Young, 2006; Georgila et al., 2006; Young et al., 2007; Schatzmann et al., 2007). We developed a simple n-gram User Simulation, using ngrams of dialogue moves. It treats a dialogue as a sequence of lists of consecutive user and system turns in a high level semantic representation, i.e. 506 &lt; 5peechAct &gt;, &lt; Task &gt; pairs, for example &lt; provide info &gt;, &lt; music genre(punk) &gt;. It takes as input the n − 1 most recent lists of &lt; 5peechAct &gt;, &lt; Task &gt; pairs in the dialogue history, and uses the statistics in the training set to compute a distribution over the possible next user actions. If no n-gram</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>J Schatzmann, K Weilhammer, M N Stuttle, and S J Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies. Knowledge Engineering Review, 21:97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>S Young</author>
</authors>
<title>Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In</title>
<date>2007</date>
<booktitle>Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="3176" citStr="Schatzmann et al., 2007" startWordPosition="499" endWordPosition="502">ctions. We evaluate this approach in comparison with a baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list. In such systems, complex repair strategies are required when the top hypothesis is incorrect. The main novelty of this work is that we explore the use of predictions from simple statistical User Simulations to re-rank n-best lists of ASR hypotheses. These User Simulations are now commonly used in statistical learning approaches to dialogue management (Williams and Young, 2003; Schatzmann et al., 2006; Young, 2006; Young et al., 2007; Schatzmann et al., 2007), but they have not been used for context-sensitive ASR before. In our model, the system’s “belief” b(h) in a recognition hypothesis h is factored in two parts: the observation probability P(oIh) (approximated by the ASR confidence score) and the User Simulation probability P(hIus, C) of the hypothesis: b(h) = P(olh).P(hlus, C) (1) where us is the state of the User Simulation in context C. The context is simply a window of diProceedings of the 12th Conference of the European Chapter of the ACL, pages 505–513, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguis</context>
<context position="8133" citStr="Schatzmann et al., 2007" startWordPosition="1285" endWordPosition="1288"> hypotheses is the incorporation of a User Simulation output as an additional feature. The history of a dialogue between a user and a dialogue system plays an important role as to what the user might be expected to say next. As a result, most of the studies mentioned in the previous section make various efforts to capture history by including relevant features directly in their classifiers. Various statistical User Simulations have been trained on corpora of dialogue data in order to simulate real user behaviour (Schatzmann et al., 2006; Young, 2006; Georgila et al., 2006; Young et al., 2007; Schatzmann et al., 2007). We developed a simple n-gram User Simulation, using ngrams of dialogue moves. It treats a dialogue as a sequence of lists of consecutive user and system turns in a high level semantic representation, i.e. 506 &lt; 5peechAct &gt;, &lt; Task &gt; pairs, for example &lt; provide info &gt;, &lt; music genre(punk) &gt;. It takes as input the n − 1 most recent lists of &lt; 5peechAct &gt;, &lt; Task &gt; pairs in the dialogue history, and uses the statistics in the training set to compute a distribution over the possible next user actions. If no n-grams match the current history, the model can back-off to n-grams of lower order. We </context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. Young. 2007. Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>Johan Bos</author>
<author>Robin Cooper</author>
<author>Staffan Larsson</author>
<author>Ian Lewin</author>
<author>Colin Matheson</author>
<author>Massimo Poesio</author>
</authors>
<title>A Model of Dialogue Moves and Information State Revision.</title>
<date>1999</date>
<tech>Technical Report D2.1, Trindi Project.</tech>
<contexts>
<context position="7301" citStr="Traum et al., 1999" startWordPosition="1142" endWordPosition="1145">ncludes attributes that account for the whole n-best list, i.e. standard deviation of confidence scores. As well as the use of a User Simulation, the main difference between our approach and work on hypothesis reordering (e.g. (Chotimongkol and Rudnicky, 2001)) is that we make a decision regarding whether a dialogue system should accept, clarify, reject, or ignore a user utterance. Like (Gabsdil and Lemon, 2004; Jonson, 2006), our approach is more generally applicable than preceding research, since we frame our methodology in the Information State Update (ISU) approach to dialogue management (Traum et al., 1999) and therefore expect it to be applicable to a range of related multimodal dialogue systems. 3 User Simulations What makes this study different from the previous work in the area of post-processing of the ASR hypotheses is the incorporation of a User Simulation output as an additional feature. The history of a dialogue between a user and a dialogue system plays an important role as to what the user might be expected to say next. As a result, most of the studies mentioned in the previous section make various efforts to capture history by including relevant features directly in their classifiers</context>
</contexts>
<marker>Traum, Bos, Cooper, Larsson, Lewin, Matheson, Poesio, 1999</marker>
<rawString>David Traum, Johan Bos, Robin Cooper, Staffan Larsson, Ian Lewin, Colin Matheson, and Massimo Poesio. 1999. A Model of Dialogue Moves and Information State Revision. Technical Report D2.1, Trindi Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Jerry Wright</author>
<author>Irene Langkilde</author>
</authors>
<title>Using Natural Language Processing and Discourse Features to Identify Understanding Errors in a Spoken Dialogue System. In</title>
<date>2000</date>
<booktitle>Proceedings of ICML-2000.</booktitle>
<contexts>
<context position="5180" citStr="Walker et al., 2000" startWordPosition="817" endWordPosition="820"> aspects of this idea in a computational manner. Similar work in the area of spoken dialogue systems is described below. (Litman et al., 2000) use acoustic-prosodic information extracted from speech waveforms, together with information derived from their speech recognizer, to automatically predict misrecognized turns in a corpus of train-timetable information dialogues. In our experiments, we also use recognizer confidence scores and a limited number of acoustic-prosodic features (e.g. amplitude in the speech signal) for hypothesis classification, but we also use User Simulation predictions. (Walker et al., 2000) use a combination of features from the speech recognizer, natural language understanding, and dialogue manager/discourse history to classify hypotheses as correct, partially correct, or misrecognized. Our work is related to these experiments in that we also combine confidence scores and higher-level features for classification. However, both (Litman et al., 2000) and (Walker et al., 2000) consider only single-best recognition results and thus use their classifiers as “filters” to decide whether the best recognition hypothesis for a user utterance is correct or not. We go a step further in tha</context>
</contexts>
<marker>Walker, Wright, Langkilde, 2000</marker>
<rawString>Marilyn Walker, Jerry Wright, and Irene Langkilde. 2000. Using Natural Language Processing and Discourse Features to Identify Understanding Errors in a Spoken Dialogue System. In Proceedings of ICML-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Williams</author>
<author>Steve Young</author>
</authors>
<title>Using wizardof-oz simulations to bootstrap reinforcementlearning-based dialog management systems.</title>
<date>2003</date>
<booktitle>In Proc. 4th SIGdial workshop.</booktitle>
<contexts>
<context position="3092" citStr="Williams and Young, 2003" startWordPosition="485" endWordPosition="488"> are then used to select a “best” hypothesis and to decide on appropriate system reactions. We evaluate this approach in comparison with a baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list. In such systems, complex repair strategies are required when the top hypothesis is incorrect. The main novelty of this work is that we explore the use of predictions from simple statistical User Simulations to re-rank n-best lists of ASR hypotheses. These User Simulations are now commonly used in statistical learning approaches to dialogue management (Williams and Young, 2003; Schatzmann et al., 2006; Young, 2006; Young et al., 2007; Schatzmann et al., 2007), but they have not been used for context-sensitive ASR before. In our model, the system’s “belief” b(h) in a recognition hypothesis h is factored in two parts: the observation probability P(oIh) (approximated by the ASR confidence score) and the User Simulation probability P(hIus, C) of the hypothesis: b(h) = P(olh).P(hlus, C) (1) where us is the state of the User Simulation in context C. The context is simply a window of diProceedings of the 12th Conference of the European Chapter of the ACL, pages 505–513, A</context>
</contexts>
<marker>Williams, Young, 2003</marker>
<rawString>Jason Williams and Steve Young. 2003. Using wizardof-oz simulations to bootstrap reinforcementlearning-based dialog management systems. In Proc. 4th SIGdial workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>H Ye</author>
</authors>
<title>The Hidden Information State Approach to Dialog Management.</title>
<date>2007</date>
<booktitle>In ICASSP</booktitle>
<contexts>
<context position="3150" citStr="Young et al., 2007" startWordPosition="495" endWordPosition="498">propriate system reactions. We evaluate this approach in comparison with a baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list. In such systems, complex repair strategies are required when the top hypothesis is incorrect. The main novelty of this work is that we explore the use of predictions from simple statistical User Simulations to re-rank n-best lists of ASR hypotheses. These User Simulations are now commonly used in statistical learning approaches to dialogue management (Williams and Young, 2003; Schatzmann et al., 2006; Young, 2006; Young et al., 2007; Schatzmann et al., 2007), but they have not been used for context-sensitive ASR before. In our model, the system’s “belief” b(h) in a recognition hypothesis h is factored in two parts: the observation probability P(oIh) (approximated by the ASR confidence score) and the User Simulation probability P(hIus, C) of the hypothesis: b(h) = P(olh).P(hlus, C) (1) where us is the state of the User Simulation in context C. The context is simply a window of diProceedings of the 12th Conference of the European Chapter of the ACL, pages 505–513, Athens, Greece, 30 March – 3 April 2009. c�2009 Association</context>
<context position="8107" citStr="Young et al., 2007" startWordPosition="1281" endWordPosition="1284">rocessing of the ASR hypotheses is the incorporation of a User Simulation output as an additional feature. The history of a dialogue between a user and a dialogue system plays an important role as to what the user might be expected to say next. As a result, most of the studies mentioned in the previous section make various efforts to capture history by including relevant features directly in their classifiers. Various statistical User Simulations have been trained on corpora of dialogue data in order to simulate real user behaviour (Schatzmann et al., 2006; Young, 2006; Georgila et al., 2006; Young et al., 2007; Schatzmann et al., 2007). We developed a simple n-gram User Simulation, using ngrams of dialogue moves. It treats a dialogue as a sequence of lists of consecutive user and system turns in a high level semantic representation, i.e. 506 &lt; 5peechAct &gt;, &lt; Task &gt; pairs, for example &lt; provide info &gt;, &lt; music genre(punk) &gt;. It takes as input the n − 1 most recent lists of &lt; 5peechAct &gt;, &lt; Task &gt; pairs in the dialogue history, and uses the statistics in the training set to compute a distribution over the possible next user actions. If no n-grams match the current history, the model can back-off to n</context>
</contexts>
<marker>Young, Schatzmann, Weilhammer, Ye, 2007</marker>
<rawString>SJ Young, J Schatzmann, K Weilhammer, and H Ye. 2007. The Hidden Information State Approach to Dialog Management. In ICASSP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
</authors>
<title>Using POMDPs for Dialog Management.</title>
<date>2006</date>
<booktitle>In IEEE/ACL Workshop on Spoken Language Technology (SLT 2006),</booktitle>
<location>Aruba.</location>
<contexts>
<context position="3130" citStr="Young, 2006" startWordPosition="493" endWordPosition="494"> decide on appropriate system reactions. We evaluate this approach in comparison with a baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list. In such systems, complex repair strategies are required when the top hypothesis is incorrect. The main novelty of this work is that we explore the use of predictions from simple statistical User Simulations to re-rank n-best lists of ASR hypotheses. These User Simulations are now commonly used in statistical learning approaches to dialogue management (Williams and Young, 2003; Schatzmann et al., 2006; Young, 2006; Young et al., 2007; Schatzmann et al., 2007), but they have not been used for context-sensitive ASR before. In our model, the system’s “belief” b(h) in a recognition hypothesis h is factored in two parts: the observation probability P(oIh) (approximated by the ASR confidence score) and the User Simulation probability P(hIus, C) of the hypothesis: b(h) = P(olh).P(hlus, C) (1) where us is the state of the User Simulation in context C. The context is simply a window of diProceedings of the 12th Conference of the European Chapter of the ACL, pages 505–513, Athens, Greece, 30 March – 3 April 2009</context>
<context position="8064" citStr="Young, 2006" startWordPosition="1275" endWordPosition="1276"> previous work in the area of post-processing of the ASR hypotheses is the incorporation of a User Simulation output as an additional feature. The history of a dialogue between a user and a dialogue system plays an important role as to what the user might be expected to say next. As a result, most of the studies mentioned in the previous section make various efforts to capture history by including relevant features directly in their classifiers. Various statistical User Simulations have been trained on corpora of dialogue data in order to simulate real user behaviour (Schatzmann et al., 2006; Young, 2006; Georgila et al., 2006; Young et al., 2007; Schatzmann et al., 2007). We developed a simple n-gram User Simulation, using ngrams of dialogue moves. It treats a dialogue as a sequence of lists of consecutive user and system turns in a high level semantic representation, i.e. 506 &lt; 5peechAct &gt;, &lt; Task &gt; pairs, for example &lt; provide info &gt;, &lt; music genre(punk) &gt;. It takes as input the n − 1 most recent lists of &lt; 5peechAct &gt;, &lt; Task &gt; pairs in the dialogue history, and uses the statistics in the training set to compute a distribution over the possible next user actions. If no n-grams match the c</context>
</contexts>
<marker>Young, 2006</marker>
<rawString>SJ Young. 2006. Using POMDPs for Dialog Management. In IEEE/ACL Workshop on Spoken Language Technology (SLT 2006), Aruba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
</authors>
<title>ATK: An Application Toolkit for HTK, Version 1.6. Technical report,</title>
<date>2007</date>
<institution>Cambridge University Engineering Department.</institution>
<contexts>
<context position="10357" citStr="Young, 2007" startWordPosition="1666" endWordPosition="1667"> the semantic representation. In other words, if the classified utterance corresponds to the same semantic representation as the transcribed then we have 100% DMA, otherwise 0%. Sentence Accuracy (SA) is the alignment of a single hypothesis in the n-best list with the true transcription. Similarly to DMA, it accounts for perfect alignment between the hypothesis and the transcription, i.e. if they match perfectly we have 100% SA, otherwise 0%. 5 Data Collection For our experiments, we use data collected in a user study with the Town-Info spoken dialogue system, using the HTK speech recognizer (Young, 2007). In this study 18 subjects had to solve 10 search/browsing tasks with the system, resulting in 180 complete dialogues and 2564 utterances (average 14.24 user utterances per dialogue). For each utterance we have a series of files of 60-best lists produced by the speech recogniser, namely the transcription hypotheses on a sentence level along with the acoustic model score and the equivalent transcriptions on a word level, with information such as the duration of each recognised frame and the confidence score of the acoustic and language model of each word. 5.1 Labeling We transcribed all user u</context>
</contexts>
<marker>Young, 2007</marker>
<rawString>Steve Young. 2007. ATK: An Application Toolkit for HTK, Version 1.6. Technical report, Cambridge University Engineering Department.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>