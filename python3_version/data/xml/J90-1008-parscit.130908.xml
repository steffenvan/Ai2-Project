<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000357">
<note confidence="0.515358">
Book Reviews Prosody and Speech Recognition
</note>
<bodyText confidence="0.999966432432432">
chart parsing is reduced into a set of matrix operations
dealing with sparse matrices. Appendices list sample gram-
mars and lexicons, which brings substance to the claims.
&amp;quot;Speech&amp;quot; in this book refers to English only, which is
never made explicit. This seems to be the normal case in
American literature, however. Of course, most of the contri-
bution is relevant to other languages as well.
The book also provides an interesting contribution in the
area of finite-state properties of language, because the
phrase structure grammars used are essentially finite-state.
Other finite-state accounts (such as the two-level model by
Koskenniemi [1984] and cascaded transducers by Kaplan
and Kay [Kay 1983] seem to have been less successful in
combining structural information with segmental pro-
cesses. Both other models are purely segmental, although
syllables are sometimes referred to as contexts.
An interesting problem concerning rule interaction in the
proposed formalism is dealt with on page 113. There would
be an obvious need for subtraction (for defining negative
contexts) and intersection (combining effects). Subtrac-
tion, however, turns out to exclude too much, whereas
intersection is too permissive.
The book is well written and the argumentation proceeds
logically. Both strong and weak points of the theories
proposed are clearly presented. It gives a fair overall pic-
ture of the field of speech recognition, and much of the book
could be suitable as a textbook. Nevertheless, some pas-
sages address mostly readers with a considerable back-
ground. The main topic covers, of course, a specific slice of
the whole field, namely the treatment of allophonic varia-
tion. One minor inconvenience is the use of a reference
format that cites a number only, not the author and the
year. This results in a small savings in space but a larger
burden for the reader. The book appears to be Church&apos;s
(previously unpublished) doctoral dissertation from MIT,
though this is not clearly indicated in the volume. Although
not particularly new, it still is very valuable.
</bodyText>
<sectionHeader confidence="0.988295" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.765957285714286">
Kay, M. 1983 &amp;quot;When Meta-Rules Are not Meta-Rules,&amp;quot; In Sparck Jones,
K. and Wilks, Y. (eds.), Automatic Natural Language Parsing. Ellis
Horwood, Chichester, U.K.
Koskenniemi, K. 1984 &amp;quot;A General Computational Model for Word-Form
Recognition and Production,&amp;quot; In COLING &apos;84: Proceedings of the 10th
International Conference on Computational Linguistics, Stanford, 178-
181.
</reference>
<bodyText confidence="0.965461625">
Kimmo Koskenniemi&apos;s Ph.D. thesis introduced a finite-state two-
level model to account for inflection, derivation, and compounding
in an efficient and language-independent way. He works on
finite-state morphology and syntax at the Research Unit for
Computational Linguistics at the University of Helsinki. Kosken-
niemi&apos;s address is: Department of General Linguistics, University
of Helsinki, Hallituslcatu 11, SF 00100 Helsinki, Finland. E-mail:
koskenniemi@opmvax.csc.fi
</bodyText>
<sectionHeader confidence="0.752813" genericHeader="conclusions">
PROSODY AND SPEECH RECOGNITION
</sectionHeader>
<subsectionHeader confidence="0.613349">
Alex Waibel
</subsectionHeader>
<bodyText confidence="0.751844833333333">
(Carnegie Mellon University)
San Mateo, CA: Morgan Kaufmann Publishers and
London: Pitman, 1988, xii + 212 pp.
(Research Notes in Artificial Intelligence)
Softbound, ISBN 0-934613-70-2 and 0-273-08787-8,
$22.95
</bodyText>
<figure confidence="0.843844333333333">
Reviewed by
;loan Bachenko
AT&amp;T Bell Laboratories
</figure>
<bodyText confidence="0.998959731707317">
Most research in natural language processing (NLP) con-
centrates on the syntax and semantics of written language,
a situation that exists in part because most NLP applica-
tions are concerned with systems that rely on written
language analysis, e.g. information retrieval and text-
generation systems. Recently, however, we have begun to
see a growing interest in spoken language and the applica-
tion of natural language processing to text-to-speech synthe-
sis and speech recognition. Waibel&apos;s volume, which de-
scribes new results in automated speech recognition, makes
an important contribution to this research direction.
At present, speech recognition technology gives us two
choices: speaker-independent systems that handle small
vocabularies (one to five phonetically distinct words) and
require no training, or speaker-dependent systems that
recognize somewhat larger vocabularies (up to 1,000 words
online) and require training sessions for each user. Al-
though experimental systems can recognize limited contin-
uous speech, freely generated phrases and sentences cannot
be processed, nor can words that are &amp;quot;unknown&amp;quot; to a
recognizer. Waibel believes that this condition can change
if recognition systems, which currently focus on identifying
acoustic phonetic segments, are expanded to include pro-
sodic information, i.e. information about nonsegmental
features such as duration and pitch. His central claim is
that a system equipped with prosody rules can achieve very
large vocabulary recognition, up to 20,000 words, in both
continuous speech and isolated word tasks. To make his
point, Waibel examines four prosodic features—duration,
intensity, pitch, and stress. For each feature, he discusses in
detail a series of experiments that demonstrate the tech-
niques that he used (and, in some cases, invented) for
extracting the prosodic features, and rules that use prosodic
feature patterns to narrow the search space for word hypoth-
esization to a small subset of the total vocabulary. Waibel&apos;s
results make a convincing case that prosody can play a
valuable role in machine perception of speech; however,
they fall short of establishing his strongest claims, as he
lacks a complete implementation of the system.
Most of the book is organized around each of the pro-
sodic features that Waibel investigates. His explication is
</bodyText>
<page confidence="0.931973">
46 Computational Linguistics Volume 16, Number 1, March 1990
</page>
<note confidence="0.513895">
Book Reviews From Syntax to Semantics: Insights from Machine Translation
</note>
<bodyText confidence="0.999366380952381">
generally very thorough, although the chapter on pitch is
unusually skimpy. For some reason, Waibel chooses to
limit his discussion to two sentence tunes—yes-or-no ques-
tions (with a rising pitch contour) and declarative state-
ments (falling pitch contour)—that he feels are relevant to
connected speech recognition. Waibel does not investigate
this in any depth, however; nor does he consider the possible
contribution of pitch to the recognition of polysyllabic
words, which themselves can make up a single intonation
phrase.
The discussion of other prosodic features is much more
satisfying, and I was especially intrigued by the chapter on
stress. Previous researchers on speech recognition and pros-
ody (e.g. Lea 1980) have held that stressed syllables pro-
vide &amp;quot;islands of phonetic reliability&amp;quot; because stress in itself
makes a speech segment more identifiable to human and
machine listeners. Waibel&apos;s results contradict this claim.
His experiments show no significant effect of stress on
recognition accuracies. What makes stressed syllables spe-
cial is their closeness to lexical representation; stressed
syllables have closer agreement between acoustic reality
and abstract representation than unstressed syllables. A
recognizer will therefore have better success at using stress
to find syllables and words than particular phonetic seg-
ments; in Waibel&apos;s implementation, stress (specifically,
stress probabilities) is used for locating word boundaries
and for distinguishing between function and content words.
Such results have important implications for the psycholog-
ical reality of lexical representations and for the place of
cognitive modeling in speech recognition systems. Unfortu-
nately, the effects of Waibel&apos;s experiments are limited by
his vocabulary sample, which contained a large number of
monosyllabic words. I hope Waibel and others will repli-
cate this study with a different and larger set of materials.
I recommend this book as a text and reference. Readers
will benefit from a knowledge of speech basics, but no
special knowledge of synthesis or recognition technology is
necessary. Finally, I wish to recommend this volume espe-
cially to those who are currently looking at ways of improv-
ing a recognizer&apos;s performance through using better-known
tools of computational linguistics, such as morphological
analysis and parsing.
</bodyText>
<sectionHeader confidence="0.973006" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.5332">
Lea, W. A. 1980 Trends in Speech Recognition. Prentice-Hall, Engle-
wood Cliffs, NJ.
Joan Bachenko holds a Ph.D. in linguistics from New York
University. Her research interests include prosody and speech
synthesis, telegraphic sublanguages, and Deaf English. Bachen-
ko&apos;s address is: AT&amp;T Bell Laboratories, 3D462, Murray Hill,
NJ 07974. E-mail:joan-b@allegra.att.com.
</bodyText>
<note confidence="0.63830075">
FROM SYNTAX TO SEMANTICS: INSIGHTS FROM
MACHINE TRANSLATION
Erich Steiner, Paul Schmidt, and Cornelia
Zelinsky-Wibbelt (eds.)
</note>
<bodyText confidence="0.688040625">
(Institut ftir Angewandte Informationsforschung,
Saarbriicken)
London, U.K.: Pinter Publishers, 1988, vii + 262 pp.
(Communication in artificial intelligence series)
Hardbound, ISBN 0-86187-960-0, £29.50 and Norwood,
NJ: Ablex Publishing Corp, 1988, vii + 262 pp.
Hardbound, ISBN 0-89391-526-2, $48.50
(institutional), $29.50 (personal)
</bodyText>
<figure confidence="0.786890666666667">
Reviewed by
Harold Somers
UMIST
</figure>
<bodyText confidence="0.999810282051282">
Keen observers of the world of machine translation have
long awaited the first book-length publication on the Com-
mission of the European Communities&apos; MT project EURO-
TRA from the Saarbriicken-based German group; other
readers may have been attracted by this book&apos;s title. Both,
regrettably, risk disappointment.
The former will find this book nothing like the hoped-for
detailed description of the world&apos;s biggest and best-funded
MT project. Not only is the book explicitly &amp;quot;not some
official report on EUROTRA work&amp;quot; (p. 1), but in several
places it contradicts or argues against EUROTRA doctrine
(&amp;quot;. . the concept of transfer developed in this chapter is
the opinion of the author, not necessarily the view underly-
ing the project as a whole&amp;quot;) (p. 161). However, it assumes
in the readership either some prior knowledge of the fine
details of the project and, especially, its formalisms and
jargon, or else access to numerous Commission documents
to which it makes frequent reference (particularly the
&amp;quot;EUROTRA Reference Manual&amp;quot;), even though they are
not in the public domain. A good (or bad) example of this
occurs when we are asked (p. 13) to consult Arnold et al.
1985 (an internal report) for an explanation of the use in
EUROTRA of the term &amp;quot;unification.&amp;quot; What is the reader
to make of this? All we get is a hint that this term might be
being used in a nonstandard way, with no hint as to what it
might mean here, and no reasonable chance of following up
the reference. Similarly, the dual use of the term
&amp;quot;translation&amp;quot; both in its everyday meaning (i.e. between
natural languages) and to describe a relationship between
representations (e.g. p. 21) is very misleading. Other exam-
ples include &amp;quot;euroversal&amp;quot; (e.g. pp. 5, 187), and reference to
&amp;quot;the corpus&amp;quot; (p. 6), though we are not told which corpus, or
of what it is a corpus.
This brings us to the second set of potentially disap-
pointed readers: those expecting to read about syntax and
semantics in an MT system. The recurring theme, inas-
much as there is one, is that a purely syntactic representa-
tion is not suitable for a 72-language-pair transfer MT
system, and so some sort of &amp;quot;semantics&amp;quot; must be used. This
</bodyText>
<footnote confidence="0.333208">
Computational Linguistics Volume 16, Number 1, March 1990 47
</footnote>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000413">
<title confidence="0.975733">Book Reviews Prosody and Speech Recognition</title>
<abstract confidence="0.956899653061224">chart parsing is reduced into a set of matrix operations dealing with sparse matrices. Appendices list sample grammars and lexicons, which brings substance to the claims. &amp;quot;Speech&amp;quot; in this book refers to English only, which is never made explicit. This seems to be the normal case in American literature, however. Of course, most of the contribution is relevant to other languages as well. The book also provides an interesting contribution in the area of finite-state properties of language, because the phrase structure grammars used are essentially finite-state. Other finite-state accounts (such as the two-level model by Koskenniemi [1984] and cascaded transducers by Kaplan and Kay [Kay 1983] seem to have been less successful in combining structural information with segmental processes. Both other models are purely segmental, although syllables are sometimes referred to as contexts. An interesting problem concerning rule interaction in the proposed formalism is dealt with on page 113. There would be an obvious need for subtraction (for defining negative contexts) and intersection (combining effects). Subtraction, however, turns out to exclude too much, whereas intersection is too permissive. The book is well written and the argumentation proceeds logically. Both strong and weak points of the theories proposed are clearly presented. It gives a fair overall picture of the field of speech recognition, and much of the book could be suitable as a textbook. Nevertheless, some passages address mostly readers with a considerable background. The main topic covers, of course, a specific slice of the whole field, namely the treatment of allophonic variation. One minor inconvenience is the use of a reference format that cites a number only, not the author and the year. This results in a small savings in space but a larger burden for the reader. The book appears to be Church&apos;s (previously unpublished) doctoral dissertation from MIT, though this is not clearly indicated in the volume. Although not particularly new, it still is very valuable. REFERENCES 1983 Meta-Rules Are not Meta-Rules,&amp;quot; In Sparck Jones, and Wilks, Natural Language Parsing. Horwood, Chichester, U.K. K. General Computational Model for Word-Form and Production,&amp;quot; In &apos;84: Proceedings of the 10th Conference on Computational Linguistics, 178- 181. Koskenniemi&apos;s thesis introduced a finite-state twolevel model to account for inflection, derivation, and compounding in an efficient and language-independent way. He works on finite-state morphology and syntax at the Research Unit for</abstract>
<affiliation confidence="0.4685625">Computational Linguistics at the University of Helsinki. Koskenniemi&apos;s address is: Department of General Linguistics, University</affiliation>
<address confidence="0.664118">of Helsinki, Hallituslcatu 11, SF 00100 Helsinki, Finland. E-mail:</address>
<email confidence="0.860298">koskenniemi@opmvax.csc.fi</email>
<title confidence="0.880983">PROSODY AND SPEECH RECOGNITION</title>
<author confidence="0.999793">Alex Waibel</author>
<affiliation confidence="0.999587">(Carnegie Mellon University)</affiliation>
<address confidence="0.690695">San Mateo, CA: Morgan Kaufmann Publishers and</address>
<note confidence="0.9543266">London: Pitman, 1988, xii + 212 pp. (Research Notes in Artificial Intelligence) Softbound, ISBN 0-934613-70-2 and 0-273-08787-8, $22.95 Reviewed by</note>
<author confidence="0.991437">loan Bachenko</author>
<affiliation confidence="0.99666">AT&amp;T Bell Laboratories</affiliation>
<abstract confidence="0.994301047058823">Most research in natural language processing (NLP) concentrates on the syntax and semantics of written language, a situation that exists in part because most NLP applications are concerned with systems that rely on written language analysis, e.g. information retrieval and textgeneration systems. Recently, however, we have begun to see a growing interest in spoken language and the application of natural language processing to text-to-speech synthesis and speech recognition. Waibel&apos;s volume, which describes new results in automated speech recognition, makes an important contribution to this research direction. At present, speech recognition technology gives us two choices: speaker-independent systems that handle small vocabularies (one to five phonetically distinct words) and require no training, or speaker-dependent systems that recognize somewhat larger vocabularies (up to 1,000 words online) and require training sessions for each user. Although experimental systems can recognize limited continuous speech, freely generated phrases and sentences cannot be processed, nor can words that are &amp;quot;unknown&amp;quot; to a recognizer. Waibel believes that this condition can change if recognition systems, which currently focus on identifying acoustic phonetic segments, are expanded to include prosodic information, i.e. information about nonsegmental features such as duration and pitch. His central claim is that a system equipped with prosody rules can achieve very large vocabulary recognition, up to 20,000 words, in both continuous speech and isolated word tasks. To make his point, Waibel examines four prosodic features—duration, intensity, pitch, and stress. For each feature, he discusses in detail a series of experiments that demonstrate the techniques that he used (and, in some cases, invented) for extracting the prosodic features, and rules that use prosodic feature patterns to narrow the search space for word hypothesization to a small subset of the total vocabulary. Waibel&apos;s results make a convincing case that prosody can play a valuable role in machine perception of speech; however, they fall short of establishing his strongest claims, as he lacks a complete implementation of the system. Most of the book is organized around each of the prosodic features that Waibel investigates. His explication is 46 Computational Linguistics Volume 16, Number 1, March 1990 Reviews From to Semantics: from Machine Translation generally very thorough, although the chapter on pitch is unusually skimpy. For some reason, Waibel chooses to limit his discussion to two sentence tunes—yes-or-no questions (with a rising pitch contour) and declarative statements (falling pitch contour)—that he feels are relevant to connected speech recognition. Waibel does not investigate this in any depth, however; nor does he consider the possible contribution of pitch to the recognition of polysyllabic words, which themselves can make up a single intonation phrase. The discussion of other prosodic features is much more satisfying, and I was especially intrigued by the chapter on stress. Previous researchers on speech recognition and prosody (e.g. Lea 1980) have held that stressed syllables provide &amp;quot;islands of phonetic reliability&amp;quot; because stress in itself makes a speech segment more identifiable to human and machine listeners. Waibel&apos;s results contradict this claim. His experiments show no significant effect of stress on recognition accuracies. What makes stressed syllables special is their closeness to lexical representation; stressed syllables have closer agreement between acoustic reality and abstract representation than unstressed syllables. A recognizer will therefore have better success at using stress to find syllables and words than particular phonetic segments; in Waibel&apos;s implementation, stress (specifically, stress probabilities) is used for locating word boundaries and for distinguishing between function and content words. Such results have important implications for the psychological reality of lexical representations and for the place of cognitive modeling in speech recognition systems. Unfortunately, the effects of Waibel&apos;s experiments are limited by his vocabulary sample, which contained a large number of monosyllabic words. I hope Waibel and others will replicate this study with a different and larger set of materials. I recommend this book as a text and reference. Readers will benefit from a knowledge of speech basics, but no special knowledge of synthesis or recognition technology is necessary. Finally, I wish to recommend this volume especially to those who are currently looking at ways of improving a recognizer&apos;s performance through using better-known tools of computational linguistics, such as morphological analysis and parsing.</abstract>
<note confidence="0.751828125">REFERENCES W. 1980 in Speech Recognition. Englewood Cliffs, NJ. Bachenko a Ph.D. in linguistics from New York University. Her research interests include prosody and speech synthesis, telegraphic sublanguages, and Deaf English. Bachenko&apos;s address is: AT&amp;T Bell Laboratories, 3D462, Murray Hill, NJ 07974. E-mail:joan-b@allegra.att.com.</note>
<title confidence="0.751828">FROM SYNTAX TO SEMANTICS: INSIGHTS FROM MACHINE TRANSLATION</title>
<author confidence="0.795154">Erich Steiner</author>
<author confidence="0.795154">Paul Schmidt</author>
<author confidence="0.795154">Cornelia</author>
<note confidence="0.9418325">Zelinsky-Wibbelt (eds.) (Institut ftir Angewandte Informationsforschung, Saarbriicken) London, U.K.: Pinter Publishers, 1988, vii + 262 pp. (Communication in artificial intelligence series) Hardbound, ISBN 0-86187-960-0, £29.50 and Norwood, NJ: Ablex Publishing Corp, 1988, vii + 262 pp. Hardbound, ISBN 0-89391-526-2, $48.50 (institutional), $29.50 (personal) Reviewed by</note>
<author confidence="0.552743">Harold Somers</author>
<affiliation confidence="0.309262">UMIST</affiliation>
<abstract confidence="0.994656794871795">Keen observers of the world of machine translation have long awaited the first book-length publication on the Commission of the European Communities&apos; MT project EURO- TRA from the Saarbriicken-based German group; other readers may have been attracted by this book&apos;s title. Both, regrettably, risk disappointment. The former will find this book nothing like the hoped-for detailed description of the world&apos;s biggest and best-funded MT project. Not only is the book explicitly &amp;quot;not some official report on EUROTRA work&amp;quot; (p. 1), but in several places it contradicts or argues against EUROTRA doctrine (&amp;quot;. . the concept of transfer developed in this chapter is the opinion of the author, not necessarily the view underlying the project as a whole&amp;quot;) (p. 161). However, it assumes in the readership either some prior knowledge of the fine details of the project and, especially, its formalisms and jargon, or else access to numerous Commission documents to which it makes frequent reference (particularly the &amp;quot;EUROTRA Reference Manual&amp;quot;), even though they are not in the public domain. A good (or bad) example of this occurs when we are asked (p. 13) to consult Arnold et al. 1985 (an internal report) for an explanation of the use in EUROTRA of the term &amp;quot;unification.&amp;quot; What is the reader to make of this? All we get is a hint that this term might be being used in a nonstandard way, with no hint as to what it might mean here, and no reasonable chance of following up the reference. Similarly, the dual use of the term &amp;quot;translation&amp;quot; both in its everyday meaning (i.e. between natural languages) and to describe a relationship between representations (e.g. p. 21) is very misleading. Other examples include &amp;quot;euroversal&amp;quot; (e.g. pp. 5, 187), and reference to &amp;quot;the corpus&amp;quot; (p. 6), though we are not told which corpus, or of what it is a corpus. This brings us to the second set of potentially disappointed readers: those expecting to read about syntax and semantics in an MT system. The recurring theme, inasmuch as there is one, is that a purely syntactic representation is not suitable for a 72-language-pair transfer MT system, and so some sort of &amp;quot;semantics&amp;quot; must be used. This</abstract>
<intro confidence="0.407821">Computational Linguistics Volume 16, Number 1, March 1990 47</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>When Meta-Rules Are not Meta-Rules,&amp;quot;</title>
<date>1983</date>
<booktitle>Automatic Natural Language Parsing.</booktitle>
<editor>In Sparck Jones, K. and Wilks, Y. (eds.),</editor>
<publisher>Ellis Horwood, Chichester, U.K.</publisher>
<contexts>
<context position="740" citStr="Kay 1983" startWordPosition="113" endWordPosition="114">ppendices list sample grammars and lexicons, which brings substance to the claims. &amp;quot;Speech&amp;quot; in this book refers to English only, which is never made explicit. This seems to be the normal case in American literature, however. Of course, most of the contribution is relevant to other languages as well. The book also provides an interesting contribution in the area of finite-state properties of language, because the phrase structure grammars used are essentially finite-state. Other finite-state accounts (such as the two-level model by Koskenniemi [1984] and cascaded transducers by Kaplan and Kay [Kay 1983] seem to have been less successful in combining structural information with segmental processes. Both other models are purely segmental, although syllables are sometimes referred to as contexts. An interesting problem concerning rule interaction in the proposed formalism is dealt with on page 113. There would be an obvious need for subtraction (for defining negative contexts) and intersection (combining effects). Subtraction, however, turns out to exclude too much, whereas intersection is too permissive. The book is well written and the argumentation proceeds logically. Both strong and weak p</context>
</contexts>
<marker>Kay, 1983</marker>
<rawString>Kay, M. 1983 &amp;quot;When Meta-Rules Are not Meta-Rules,&amp;quot; In Sparck Jones, K. and Wilks, Y. (eds.), Automatic Natural Language Parsing. Ellis Horwood, Chichester, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>A General Computational Model for Word-Form Recognition and Production,&amp;quot;</title>
<date>1984</date>
<booktitle>In COLING &apos;84: Proceedings of the 10th International Conference on Computational Linguistics, Stanford,</booktitle>
<pages>178--181</pages>
<marker>Koskenniemi, 1984</marker>
<rawString>Koskenniemi, K. 1984 &amp;quot;A General Computational Model for Word-Form Recognition and Production,&amp;quot; In COLING &apos;84: Proceedings of the 10th International Conference on Computational Linguistics, Stanford, 178-181.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>