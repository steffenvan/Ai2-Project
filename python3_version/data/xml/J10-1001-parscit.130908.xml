<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9977665">
Broad-Coverage Parsing Using Human-Like
Memory Constraints
</title>
<author confidence="0.999682">
William Schuler*
</author>
<affiliation confidence="0.99781">
University of Minnesota
</affiliation>
<author confidence="0.984306">
Samir AbdelRahman**
</author>
<affiliation confidence="0.981307">
Cairo University
</affiliation>
<author confidence="0.997459">
Tim Miller*
</author>
<affiliation confidence="0.998709">
University of Minnesota
</affiliation>
<author confidence="0.995963">
Lane Schwartz*
</author>
<affiliation confidence="0.995403">
University of Minnesota
</affiliation>
<bodyText confidence="0.997611">
Human syntactic processing shows many signs of taking place within a general-purpose
short-term memory. But this kind of memory is known to have a severely constrained storage
capacity — possibly constrained to as few as three or four distinct elements. This article describes
a model of syntactic processing that operates successfully within these severe constraints, by
recognizing constituents in a right-corner transformed representation (a variant of left-corner
parsing) and mapping this representation to random variables in a Hierarchical Hidden Markov
Model, a factored time-series model which probabilistically models the contents of a bounded
memory store over time. Evaluations of the coverage of this model on a large syntactically
annotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategy
based on this model, suggest this model may be cognitively plausible.
</bodyText>
<sectionHeader confidence="0.996952" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999150333333333">
It is an interesting possibility that human syntactic processing may occur entirely within
a general-purpose short-term memory. Like other short-term memory processes, syn-
tactic processing is susceptible to degradation if short-term memory capacity is loaded,
for example, when readers are asked to retain lists of words while reading (Just
and Carpenter 1992); and memory of words and syntax degrades over time within
and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse
information about referents from other sentences (Ericsson and Kintsch 1995). But
short-term memory is known to have severe capacity limitations of perhaps no more
than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem
</bodyText>
<affiliation confidence="0.624297">
* Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455.
</affiliation>
<email confidence="0.98731">
E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu.
</email>
<note confidence="0.946447666666667">
** Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street,
Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg.
Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for
publication: 27 May 2009.
© 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.999876">
too austere to process the rich tree-like phrase structure commonly invoked to explain
word-order regularities in natural language.
This article aims to show that they are not. The article describes a comprehension
model, based on a right-corner transform—a reversible tree transform related to the
left-corner transform of Johnson (1998a)—that associates familiar phrase structure trees
with the contents of a memory store of three to four partially completed constituents
over time. Coverage results on the large syntactically annotated Penn Treebank corpus
show a vast majority of naturally occurring sentences can be recognized using a mem-
ory store containing a maximum of only three incomplete constituents, and nearly all
sentences can be recognized using four, consistent with estimates of human short-term
memory capacity.
This transform reduces memory usage in incremental (left to right) processing
by transforming right-branching constituent structures into left-branching structures,
allowing child constituents to be composed with parent constituents before either have
been completely recognized. But because this composition identifies an incomplete
child as the awaited portion of an incomplete parent, it implicitly predicts that this
child constituent will be the rightmost (i.e., last) child of the parent, before this child
has been completely recognized. Parsing accuracy results on the Penn Treebank
using a Hierarchical Hidden Markov Model (Murphy and Paskin 2001)—essentially a
probabilistic pushdown automaton with a bounded pushdown store—show that this
prediction can be reliably learned from training data.
The remainder of this article is organized as follows: Section 2 describes some
related models of human syntactic processing using a bounded memory store; Section 3
describes a Hierarchical Hidden Markov Model (HHMM) framework for statistical
parsing using this bounded store of incomplete constituents; Section 4 describes the
right-corner transform and how it relates conventional phrase structure to incomplete
constituents in a bounded memory store; Section 5 describes an experiment to estimate
the level of coverage of the Penn Treebank corpus that can be achieved using this
transform with various memory limits, given a linguistically motivated binarization of
this corpus; and Section 6 gives accuracy results of this bounded-memory model trained
on this corpus, given that some amount of incremental prediction (as described earlier)
must be involved.
</bodyText>
<sectionHeader confidence="0.99622" genericHeader="method">
2. Bounded-Memory Parsing
</sectionHeader>
<bodyText confidence="0.994749533333333">
One of the earliest bounded-memory parsing models is that of Marcus (1980). This
model maintains a bounded store of complete but unattached constituents as a buffer,
and operates on them using a variety of specialized memory manipulation operations,
deferring certain attachment decisions until the contents of this buffer indicate it is safe
to do so. (In contrast, the model described in this article maintains a store of incom-
plete constituents using ordinary stack-like push and pop operations, defined to allow
constituents to be composed before being completely recognized.) The Marcus parser
provides a bounded-memory explanation for human difficulties in processing garden
path sentences: for example, the horse raced past the barn fell, with intended interpretation
[NP the horse [RC (which was) raced past the barn]] fell (Bever 1970), in which raced seems like
the main verb of the sentence until the word fell is encountered. But this explanation due
to memory exhaustion is not compatible with observations of unproblematic parsing
of sentences such as these when contextual information is provided in advance: for
example, two men on horseback had a race; one went by the meadow, and the other went by the
barn (Crain and Steedman 1985).
</bodyText>
<page confidence="0.992809">
2
</page>
<note confidence="0.929361">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<bodyText confidence="0.974771183673469">
Ades and Steedman (1982) introduce the idea of composing incomplete constituents
to reduce storage demands in incremental processing using Combinatorial Catego-
rial Grammar (CCG), avoiding the need to maintain large buffers of complete but
unattached constituents. The right-corner transform described in this article composes
incomplete constituents in very much the same way, but CCG is essentially a compe-
tence model, in that it seeks to unify lexical category representations used in processing
with learned generalizations about argument structure, whereas the model described
herein is exclusively a performance model, allowing generalizations about lexical ar-
gument structures to be learned in some other representation, then combined with
probabilistic information about parsing strategies to yield a set of derived incomplete
constituents. As a result, the model described in this article has a freer hand to satisfy
strict working memory bounds, which may not permit some of the alternative compo-
sition operations proposed in the CCG account, thought to be associated with available
prosody and quantifier scope analyses.1
Johnson-Laird (1983) and Abney and Johnson (1991) propose a pure processing
account of memory capacity limits in parsing ordinary phrase structure trees. The
Johnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, of
which the right-corner transform introduced in this article is a variant, in order to bring
memory usage for most parsable sentences to within seven or so active or awaited
phrase structure constituents. This account may be used to explain human processing
difficulties in processing triply center-embedded sentences like the rat that the cat that the
dog chased killed ate the malt, with intended interpretation SNP the rat that SNP the cat that SNP
the dog] chased] killed] ate the malt (Chomsky and Miller 1963). But this explanation does
not account for examples of triply center-embedded sentences that typically do not
cause processing problems: SNP that SNP the food that SNP John] ordered] tasted good] pleased
him (Gibson 1991). Moreover, the apparent competition between comprehension of
center-embedded object relatives and retention of unrelated words in general-purpose
memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at
least, can be) used to store incomplete constituents during comprehension. This would
predict three or four elements of reliable storage, rather than seven (Cowan 2001).
The transform-based model described in this article exploits a conception of chunking
(Miller 1956) to combine pairs of active and awaited constituents from the Abney
and Johnson analysis, connected by recognized structure, in order to operate within
estimates of human short-term memory bounds.
Because of these counterexamples to the memory-exhaustion explanation of garden
path and center-embedding difficulties, recent work has turned to explanations other
than memory exhaustion for these phenomena. Lewis and Vasishth (2005) attribute
processing errors to activation interference among stored constituents that have sim-
ilar syntactic and semantic roles. Hale’s surprisal (2001) and entropic model (2006)
link human processing difficulties to significant changes in the relative probability of
competing hypotheses in incremental parsing, such that if activation is taken to be a
mechanism for probability estimation, processing difficulties may be ascribed to the
relatively slow speed of activation change within the brain (or to collapsing activation
when probabilities grow too small, as in the case of garden path sentences). These
models explain many processing difficulties without invoking memory limits, and are
1 The lack of support for some of these available scope analyses may not necessarily be problematic for the
present model. The complexity of interpreting nested raised quantifiers may place them beyond the
capability of human interactive incremental interpretation, but not beyond the capability of post hoc
interpretation (understood after the listener has had time to think about it).
</bodyText>
<page confidence="0.996341">
3
</page>
<note confidence="0.798127">
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.99087716">
compatible with brain imaging evidence of increased cortical activity and recruitment
of auxiliary brain areas during periods of increased uncertainty in sentence processing
(Just and Varma 2007). But if interference or changing activation is posited as the source
of processing difficulty, and delays are not linked to memory exhaustion per se, then
these theories do not explain how (or whether) syntactic processing operates within
general-purpose short-term memory.
Toward this end, this article will specifically evaluate the claim that syntactic
processing can be performed entirely within general-purpose short-term memory by
using this memory to store unassimilated incomplete syntactic constituents, derived
through a right-corner transform from basic properties of phrase structure trees. As
a probabilistic incremental parser, the model described in this article is compatible
with surprisal-based explanations of processing difficulties; it is, however, in some
sense orthogonal, because it models a different dimension of resource allocation. The
surprisal framework models allocation of processing resources (in this case, activation)
among disjunctions of competing hypotheses, which are maintained for some amount
of time in parallel, whereas the framework described here can be taken to model the
allocation of processing resources (in this case, memory elements) among conjunctions
of incompletely recognized constituents within each competing hypothesis.2 Thus, in
this view, there are two ways to simultaneously activate multiple concepts: disjunctively
(sharing activation among competing hypotheses) and conjunctively (sharing activation
among unassimilated constituents within a hypothesis). But only the inner conjunctive
allocation corresponds to the familiar discretely bounded store of short-term memory
as described by Miller (1956); the outer disjunctive allocation treats activation as a
continuous resource in which like-valued pockets expand and contract as they are
reinforced or contradicted by incoming observations. Indeed, it would be surprising
if these two dimensions of resource allocation did not exist: the former, because it
would contradict years of observations about the behavior of short-term memory; and
the latter, because it would require neural activation spreading to be instantaneous
and uniform, contradicting most neuropsychological evidence. Levy (2008) compares
the allocation of activation in this kind of framework to the distributed allocation
of resources in a particle filter (Gordon, Salmond, and Smith 1993), an approximate
inference technique for probabilistic time-series models in which particles in a (typically
fixed) reservoir are assigned randomly sampled hypotheses from learned transition
probabilities, essentially functioning as units of activation. The model described in this
paper qualifies this analogy by positing that each individual particle in this reservoir
endorses a coherent hypothesis about the contents of a three- to four-element memory
store at any given time, rather than about an entire unbounded phrase structure tree.3
2 Probability distributions in entropy-based models like Hale’s are typically assumed to be defined over
sets of hypotheses pursued in parallel, but other interpretations (for example, lookahead-based
deterministic models) are possible. The model described in this article is also compatible with
deterministic parsing frameworks, in which case it models allocation of processing resources among
incompletely-recognized constituents within a single non-competing hypothesis.
3 Pure connectionist models of syntactic processing (Elman 1991; Berg 1992; Rohde 2002) attempt to unify
storage of constituent structure with that of ambiguous alternative analyses, but the memory demands of
systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for
using self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen
2003), but are still limited to parsing relatively short travel planning queries with limited syntactic
complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up
constituents, and use connectionist models for probability estimation over these hypotheses (Henderson
2004) typically achieve better performance in practice.
</bodyText>
<page confidence="0.937827">
4
</page>
<bodyText confidence="0.990554826086957">
Schuler et al. Parsing Using Human-Like Memory Constraints
Previous memory-based explanations of problematic sentences (explaining garden
path effects as exceeding a bound of four complete but unattached constituents, or ex-
plaining center embedding difficulties as exceeding a bound of seven active or awaited
constituents) have been shown to underestimate human sentence processing capacity
when equally complex but unproblematic sentences were examined. The hypothesis
advanced in this article, that human sentence processing uses general-purpose short-
term memory to store incomplete constituents as defined by a right-corner transform,
leaves the explanation of several negative examples of unparsable garden path and cen-
ter embedding sentences to orthogonal models of surprisal or interference. But in order
to determine whether this right-corner memory hypothesis still underestimates human
sentence processing capacity, a corpus study was performed on two complementary
corpora of transcribed spontaneous speech and newspaper text, manually annotated
with phrase structure trees (Marcus, Santorini, and Marcinkiewicz 1993). These spon-
taneous speech and newspaper text corpora contain only attested positive examples
of parsable sentences, but they may be considered complementary for this purpose
because the complexity of spontaneous speech may somewhat understate human recog-
nition capacity (potentially limiting it to the cost of spontaneously generating sentences
in an unusual social context), and the complexity of newspaper text may somewhat
overstate human recognition capacity (though it is composed and edited to be readable,
it is still composed and edited off-line), so results from these corpora may be taken
together to suggest generous and conservative upper bounds on human processing
capacity.
</bodyText>
<sectionHeader confidence="0.562596" genericHeader="method">
3. Bounded-Memory Parsing with a Time Series Model
</sectionHeader>
<bodyText confidence="0.999887958333333">
The framework adopted in this article is a factored HMM-like time series model, which
maintains a probability distribution over the contents of a bounded set of random
variables over time, corresponding to hypothesized stores of memory elements. The
random variables in this store may be understood as simultaneous activations in a cog-
nitive model (similar to the superimposed roles described by Smolensky and Legendre
[2006]), and the probability distribution over these stores may be thought of as compet-
ing pockets of activation, as described in the previous section. Some of these variables
persist as elements of the short-term memory store, and some are transient as results of
hypothesized compositions, which are estimated and immediately discarded or folded
into the persistent store according to the dependencies in the model. The variables
have values or contents (or fillers)—in this case incomplete constituent categories—that
change over time, and although these values may be uncertain, the set of hypothesized
contents of this memory store at any given point in time are collectively constrained to
form a coherent (but possibly incomplete) syntactic analysis of a sentence.
The particular model used here is an HHMM (Murphy and Paskin 2001), which
mimics a bounded-memory pushdown automaton (PDA), supporting simple push and
pop operations on a bounded stack-like memory store. A time-series model is used
here instead of an explicit stack machine, first because the probability model is well
defined on a bounded memory store, and second because the plasticity of the random
variables that mimic stack behavior in this model makes the model cross-linguistically
attractive. By evoking additional random variables and dependencies, the model can
be defined (or presumably, trained) to mimic other types of automata, such as extended
pushdown automata (EPDAs) recognizing tree-adjoining languages with crossed and
nested dependencies, as have been hypothesized for languages like Dutch (Shieber
</bodyText>
<page confidence="0.964652">
5
</page>
<note confidence="0.286729">
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.9517835">
1985). However, the remainder of this article will only discuss random variables and
dependencies necessary to mimic a bounded stack pushdown automaton.
</bodyText>
<subsectionHeader confidence="0.999424">
3.1 Hierarchical HMMs
</subsectionHeader>
<bodyText confidence="0.998250555555555">
Hierarchical Hidden Markov Models (Murphy and Paskin 2001) are essentially Hidden
Markov Models factored into some fixed number of stack-like elements at each time
step.
HMMs characterize speech or text as sequences of hidden states qt (which may
consist of phones, words, or other hypothesized syntactic or semantic information), and
observed states ot at corresponding time steps t (typically short, overlapping frames
of an audio signal, or words or characters in a text processing application). A most
likely sequence of hidden states ˆq1..T can then be hypothesized given any sequence of
observed states o1..T:
</bodyText>
<equation confidence="0.999943">
ˆq1..T =argmax P(q1..T |o1..T) (1)
q1..T
= argmax P(q1..T) · P(o1..T  |q1..T) (2)
q1..T
def T PΘA(qt  |qt−1) · PΘB(ot  |qt) (3)
= argmax ri
q1..T t=1
</equation>
<bodyText confidence="0.998928142857143">
using Bayes’ Law (Equation 2) and Markov independence assumptions (Equation 3)
to define a full P(q1..T |o1..T) probability as the product of a Language Model (ΘA)
prior probability P(q1..T) deft PΘA(qt  |qt−1) and an Observation Model (ΘB) likelihood
probability P(o1..T  |q1..T) def f1t PΘB(ot  |qt) (Baker 1975; Jelinek, Bahl, and Mercer 1975).
Language model transitions PΘA(qt  |qt−1) over complex hidden states qt can be
modeled using synchronized levels of stacked-up component HMMs in an HHMM,
analogous to a shift-reduce parser or pushdown automaton with a bounded stack.
HHMM transition probabilities are calculated in two phases: a “reduce” phase (result-
ing in an intermediate, transient final-state variable ft), modeling whether component
HMMs terminate; and a “shift” phase (resulting in a persistent modeled state qt), in
which unterminated HMMs transition and terminated HMMs are re-initialized from
their parent HMMs. Variables over intermediate ft and modeled qt states are factored
into sequences of depth-specific variables—one for each of D levels in the HHMM
hierarchy:
</bodyText>
<equation confidence="0.9999535">
ft = (f1t ... fDt ) (4)
qt = (q1t ... qDt (5)
</equation>
<bodyText confidence="0.995146">
Transition probabilities are then calculated as a product of transition probabilities at
each level, using level-specific ‘reduce’ ΘF,d and ‘shift’ ΘQ,d models:
</bodyText>
<equation confidence="0.936669">
PΘA(qt  |qt−1) = E P(ft  |qt−1) · P(qt |ft qt−1) (6)
ft
</equation>
<page confidence="0.996641">
6
</page>
<note confidence="0.903751">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<figureCaption confidence="0.979635">
Figure 1
</figureCaption>
<bodyText confidence="0.741620333333333">
Graphical representation of a Hierarchical Hidden Markov Model. Circles denote random
variables, and edges denote conditional dependencies. Shaded circles denote variables with
observed values.
</bodyText>
<equation confidence="0.998241">
�def = D PΘF,d(fdt |fd+1 D PΘQ,d(qd t |fd+1
f1t ftD H t qd t−1qd−1 H t fd t qd t−1qd−1
d=1 t−1 ) · d=1 t )(7)
</equation>
<bodyText confidence="0.999877842105263">
with fD+1 tand q0tdefined as constants. In these equations, probabilities are marginalized
or summed over all combinations of intermediate variables ft 1...fDt, so only the memory
store contents q1t ...qDt persist across time steps.4 A graphical representation of an HHMM
with three depth levels is shown in Figure 1.
The independence assumptions in this model can be psycholinguistically moti-
vated. Independence across time points t (Equation 3) arise naturally from causality:
Any change to a memory store configuration to generate a configuration at time step
t + 1 should depend only on the current memory store configuration at time step t;
memory operations should not be able to peek backward or forward in time to consult
past or future memory stores. Independence across depth levels d (Equation 7) arise
naturally from uncertainty about the structure between incomplete constituent chunks
(this property of right-corner transform categories is elaborated in Section 4).5
Shift and reduce probabilities can now be defined in terms of finitely recursive
HMMs with probability distributions over recursive expansion, transition, and reduc-
tion of states at each depth level. In the version of HHMMs used in this paper, each
modeled variable is a syntactic state qdt E GxG (describing an incomplete constituent
consisting of an active grammatical category from domain G and an awaited grammat-
ical category from domain G—for example, an incomplete constituent S/NP consisting
of an active sentence S awaiting a noun phrase constituent NP); and each intermediate
</bodyText>
<footnote confidence="0.9908625">
4 In Viterbi decoding, probabilities over intermediate variables may be maximized rather than
marginalized, but in any case the intermediate variables do not persist.
5 Also, the fact that this is a generative model, in which observations are conditioned on hypotheses, then
flipped using Bayes’ Law (Equation 2)—as opposed to a discriminative or conditional model, in which
hypotheses are conditioned directly on observations—is also appealing as a human model, in that it
allows the same architecture to be used for both recognition and generation. This is a desirable property
for modeling split utterances, in which interlocutors complete one another’s sentences (Lerner 1991;
Helasvuo 2004).
</footnote>
<page confidence="0.996809">
7
</page>
<note confidence="0.295196">
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.996644285714286">
variable is a reduction or non-reduction state ftd E GU{1, 0} (indicating, respectively, a
reduction of incomplete constituent qd t−1 to a complete right child constituent of some
grammatical category from domain G, or a non-reduction of qd t−1 as a unary or left child,
as defined in Section 4). An intermediate variable f t d at depth d may indicate reduction
or non-reduction according to ΘF-Reduction,d if there is a reduction at the depth level
immediately below d, but must indicate non-reduction (ftd = 0) with probability 1 if
there is no reduction below:6
</bodyText>
<equation confidence="0.843301">
P- (f d|f dF1 d d−1 def if fd+1 V G:[ft d= 0] ( 8
OF,d t t gt−1qt −1 ) if fd+1 E G : PΘF-Reduction,d U }d  |qdt−1, qd−1
t−1 ) ( )
where f D+1
t = 1 and q0t = ROOT.
</equation>
<bodyText confidence="0.950355">
Shift probabilities over the modeled variable qdt at each level are defined using level-
specific transition ΘQ-Transition,d and expansion ΘQ-Expansion,d models:
</bodyText>
<equation confidence="0.967921333333333">
PΘQ,d(qdt  |fd+1 { if fd+1 (9)
tfdt qd t−1qd−1 t �EG,f t d�EG: [qdt=qdt−1]
t ) def = if fd+1EG,ftdVG:PΘQ-Transition,d(qdt|fd+1fdqdt−1qtd−1)
if fd+1
t EG,f t dEG: PΘQ-Expansion,d(qdt  |qd−1
t )
</equation>
<bodyText confidence="0.970233833333333">
where f D+1
t = 1 and q0t = ROOT. This model is conditioned on final-state intermediate
variables ftd and f d+1
t at and immediately below each HHMM level. If there is no re-
duction immediately below a given level (the first case provided), it deterministically
copies the current HHMM state forward to the next time step. If there is a reduction
immediately below the current level but no reduction at the current level (the second
case provided), it transitions the HHMM state at the current level, according to the
distribution ΘQ-Transition,d. And if there is a reduction at the current level (the third case
above), it re-initializes this state given the state at the level above, according to the
distribution ΘQ-Expansion,d.
Models ΘF-Reduction,d, ΘQ-Transition,d, and ΘQ-Expansion,d are defined directly from train-
ing examples, for example (in the experiments described in this article), using relative
frequency estimation. The overall effect is that higher-level HMMs are allowed to
transition only when lower-level HMMs terminate. An HHMM therefore behaves like
a probabilistic implementation of a shift–reduce parser or pushdown automaton with a
bounded stack, where the maximum stack depth is equal to the number of depth levels
in the HHMM hierarchy.
</bodyText>
<sectionHeader confidence="0.792242" genericHeader="method">
4. Right-Corner Transform and Incomplete Constituents
</sectionHeader>
<bodyText confidence="0.999047">
The model described in this article recognizes trees in a right-corner transformed
representation to minimize usage of a bounded short-term memory store. This right-
corner transform is a variant of the left-corner transform described by Johnson (1998a),
but whereas the left-corner transform changes left-branching structure into right-
branching structure, the right-corner transform changes right-branching structure into
</bodyText>
<page confidence="0.938379">
6 Here [·] is an indicator function: [φ] = 1 if φ is true, 0 otherwise.
8
</page>
<note confidence="0.614507">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<bodyText confidence="0.999786023809524">
left-branching structure. Recognition using this transformed grammar, extracted from
a transformed corpus, is similar to recognition using a left-corner parsing strategy (Aho
and Ullman 1972). This kind of strategy was shown to reduce memory requirements
for parsing sentences with mainly left- or right-recursive phrase structure to fewer than
seven active or awaited constituent categories (Abney and Johnson 1991). This is within
Miller’s (1956) estimate of human short-term memory capacity (if memory elements
store individual categories), whereas parsing heavily center-embedded sentences
(known to be difficult for human readers) would require seven or more elements at the
frontier of this capacity.
But recent research suggests that human memory capacity may be limited to as few
as three or four distinct items (Cowan 2001), with longer estimates of seven or more
possibly due to the human capacity to chunk remembered items into associated groups
(Miller 1956). The right-corner strategy described in this paper therefore assumes
constituent categories can similarly be chunked into incomplete constituents A/B formed
by pairing an active category A with an awaited category B somewhere along the active
category’s right progeny (so, for example, a transitive verb may become an incomplete
constituent VP/NP consisting of an active verb phrase lacking an awaited noun phrase
yet to come).7 These chunked incomplete constituent categories A and B are naturally
related through fixed contiguous phrase structure between them, established during
the course of parsing prior to the beginning of B, and these incomplete constituents can
be composed with other incomplete constituents B/C to form similarly related category
pairs A/C.
These chunks are not only contiguous sections of phrase structure trees, they also
have contiguous string yields, so they correspond to the familiar notion of text chunks
used in shallow parsing approaches (Hobbs et al. 1996). For example, a hypothesized
memory store may contain incomplete constituents S/NP (a sentence without a noun
phrase), followed by NP/NN (a noun phrase lacking a common noun), with cor-
responding string yields demand for bonds propped up and the municipal, respectively,
forming a complete contiguous segmentation of a sentence at any point in processing.
Although these two chunks could be composed into an incomplete constituent S/NN,
doing so at this point would close off the possibility of introducing another constituent
between these two, containing the recognized noun phrase as a left child (e.g., demand
for bonds propped up SNP SNP the municipal bonds]’s prices]).
This conception of chunking applied to right-branching progeny in phrase structure
trees does not have the power to eliminate the bounds of a memory store, however. In a
larger cognitive model, syntactic processing is assumed to occur as part of an interactive
semantic interpretation process, in which referents of constituents are calculated as
these constituents are recognized, and are used to constrain subsequent processing
decisions (Tanenhaus et al. 1995; Brown-Schmidt, Campana, and Tanenhaus 2002).8
The chunked category pairs A and B in these incomplete constituents A/B result from
successful compositions of other such constituents earlier in the recognition process,
which means that the relationship between the referents of A and B is known and fixed
</bodyText>
<footnote confidence="0.955002142857143">
7 Incomplete constituents may also be defined through a left-corner transform, but left-corner transformed
categories are incomplete in the other direction—a goal category yet to come lacking an
already-recognized constituent—so stored incomplete constituent categories resulting from a left-corner
transform would have the character of future goal events, rather than remembered past events. This is
discussed in greater detail in Section 4.4.
8 This can be implemented in a time-series model by factoring the model to include additional random
variables over referents, as described in Schuler, Wu, and Schwartz (2009).
</footnote>
<page confidence="0.982526">
9
</page>
<note confidence="0.282823">
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.99998518">
in any hypothesized incomplete constituent. But syntactic and semantic relationships
between chunks in a hypothesized memory store are unspecified. Chunking beyond
the level of incomplete constituents would therefore involve grouping referents whose
interrelations have not necessarily been established by the parser. Because the set
of referents is presumably much larger than the set of syntactic categories, one may
assume there are real barriers to reliably chunking them in the absence of these fixed
relationships.
There certainly may be cases where syntactically unconnected referents (belonging
to different incomplete constituents) could be grouped together as chunks. But for
simplicity, this article will assume a very strict condition that only a single incomplete
constituent can be stored in each short-term memory element. Experimental results
described in Section 5 suggest that a vast majority of English sentences can be recog-
nized within these human-like memory bounds, even with this strict condition on
chunking. If parsing can be performed in bounded memory under such strict condi-
tions, it can reasonably be assumed to operate at least as well under more permissive
circumstances, where some amount of syntactically-unrelated referential chunking is
allowed.
Several existing incremental systems are organized around a left-corner parsing
strategy (Roark 2001; Henderson 2004). But these systems generally keep large numbers
of constituents open for modifier attachment in each hypothesis. This allows modifiers
to be attached as right children of any such open constituent. But if any number of
open constituents are allowed, then either the assumption that stored elements have
fixed syntactic (and semantic) internal structure will be violated, or the assumption that
syntax operates within a bounded memory store will be violated, both of which are psy-
cholinguistically attractive as simplifying assumptions. The HHMM model described
in this article upholds both the fixed-element and bounded-memory assumptions by
hypothesizing fixed reductions of right child constituents into incomplete parents in the
same memory element, to make room for new constituents that may be introduced at a
later time. These in-element reductions are defined naturally on phrase structure trees
as the result of aligning right-corner transformed constituent structures to sequences of
random variables in a factored time-series model. The success of this predictive strategy
in corpus-based coverage and accuracy results described in Sections 5 and 6 suggests
that it may be plausible as a cognitive model.
Other accounts may model reductions in bounded memory as occurring as soon
as possible, by maintaining the option of undoing them when necessary (Stevenson
1998). This seems unattractive in the context of an interactive semantic model, however,
where syntactic constituents and semantic referents are composed in tandem, because
potentially very rich referential constraints introduced by composing a child constituent
into a parent would have to be systematically undone. An interesting possibility might
be that the appearance of syntactic restructuring may arise from a collection of hypoth-
esized stores of syntactically fixed incomplete constituents, pursued in parallel. The
results presented in this article suggest that this mechanism is possible, but these two
possibilities might be very difficult to distinguish empirically.
There is also a tradition of defining incomplete constituents as head-driven—
introduced in parsing only at the point in incremental recognition at which they can
be associated with a head word (Gibson 1991: Pritcher 1991: Gorrell 1995). In typically
head-initial languages such as English, incomplete constituents derived from these
head-driven models resemble those derived from a right-corner transform. But head-
driven incomplete constituents do not appear to obey general-purpose memory bounds
in head-final languages such as Japanese, and do not appear to obey attachment prefer-
</bodyText>
<page confidence="0.974802">
10
</page>
<note confidence="0.503271">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<bodyText confidence="0.999822">
ences predicted by a head-driven account (Kamide and Mitchell 1999), favoring a pre-
head attachment account, as a right-corner transform would predict.
</bodyText>
<subsectionHeader confidence="0.997778">
4.1 Tree Transforms Using Rewrite Rules
</subsectionHeader>
<bodyText confidence="0.999894538461539">
The incomplete constituents used in the present model are defined in terms of tree
transforms, which consist of recursive operations that change tree structures into other
tree structures. These transforms are not cognitive processes—syntax in this model is
learned and used entirely as time-series probabilities over random variable values in
the memory store. The role of these transforms is as a means to associate sequences of
configurations of incomplete constituents in a memory store with linguistically familiar
phrase structure representations, such as those studied in competence models or found
in annotated corpora.
The transforms presented in this article will be defined in terms of destructive rewrite
rules applied iteratively to each constituent of a source tree, from leaves to root, and from
left to right among siblings, to derive a target tree. These rewrites are ordered; when
multiple rewrite rules apply to the same constituent, the later rewrites are applied to
the results of the earlier ones.9 For example, the rewrite
</bodyText>
<figure confidence="0.9305205">
A0
... A1
α2 α3
... ⇒
... α2 α3 ...
A0
</figure>
<bodyText confidence="0.874444666666667">
could be used to iteratively eliminate all binary-branching nonterminal nodes in a tree,
except the root.
In the notation used in this article,
</bodyText>
<listItem confidence="0.904284333333333">
• Roman uppercase letters (Ai) are variables matching constituent labels,
• Roman lowercase letters (ai) are variables matching terminal symbols,
• Greek lowercase letters (αi) are variables matching entire subtree structure,
• Roman letters followed by colons, followed by Greek letters (Ai:αi) are
variables matching the label and structure, respectively, of the same
subtree, and
• ellipses (... ) are taken to match zero or more subtree structures,
preserving the order of ellipses in cases where there are more than one (as
in the rewrite shown herein).
</listItem>
<bodyText confidence="0.966840166666667">
Many of the transforms used in this article are reversible, meaning that the result
of applying a transform to a tree, then applying the reverse of that transform to the
resulting tree, will be the original tree itself. In general, a transform can be reversed
if the direction of its rewrite rules is reversed, and if each constituent in a target tree
9 The appropriate analogy here is to a Unix sed script, made sensitive to the beginning and end brackets of
a constituent and those of its children.
</bodyText>
<page confidence="0.986708">
11
</page>
<note confidence="0.281806">
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.9991188">
matches a unique rewrite rule in the reversed transform. The fact that not all rewrites
can be unambiguously matched to HHMM output means that parse accuracy must be
evaluated on partially-binarized gold-standard trees, in order to remove the effect of
this ambiguous matching from the evaluation. This will be discussed in greater detail
in Section 6.
</bodyText>
<subsectionHeader confidence="0.991242">
4.2 Right-Corner Transform Using Rewrite Rules
</subsectionHeader>
<bodyText confidence="0.9922825">
Rewrite rules for the right-corner transform are shown here, first to flatten out right-
recursive structure,
</bodyText>
<figure confidence="0.921887407407407">
α1
α2
a3
a3
⇒
A1/A2
A2/A3
. . .
α1
α2
A1
α1 A2
A2/A3
α2
. . .
A1
A1
α1 A2
α2 A3
A1
,
⇒
A3
A1/A2
A2/A3
then to replace it with left-recursive structure,
A1
</figure>
<equation confidence="0.912535714285714">
A1/A2:α1 A2/A3
α2
A1
A1/A3
A1/A2:α1 α2
α3 ... ⇒
α3 ...
</equation>
<bodyText confidence="0.99992915">
Here, the first two rewrite rules are applied iteratively (bottom-up on the tree) to flatten
all right recursion, using incomplete constituents to record the original nonterminal
ordering. The second rule is then applied to generate left-recursive structure, preserving
this ordering. Note that the last rewrite leaves a unary branch at the leftmost child of
each flattened node. This preserves the simple category labels of nodes that correspond
directly to nodes in the original tree, so the original tree can be reconstructed when
the right-corner transform concatenates multiple right-recursive sequences into a single
left-recursive sequence.
An example of a right-corner transformed tree is shown in Figure 2(c). An important
property of this transform is that it is reversible. Rewrite rules for reversing a right-
corner transform are simply the converse of those shown here. The correctness of this
can be demonstrated by dividing a tree into maximal sequences of right-recursive
branches (that is, maximal sequences of adjacent right children). The first two “flatten-
ing” rewrites of the right-corner transform, applied to any such sequence, will replace
the right-branching nonterminal nodes with a flat sequence of nodes labeled with
slash categories, which preserves the order of the nonterminal category symbols in the
original nodes. Reversing this rewrite will therefore generate the original sequence of
nonterminal nodes. The final rewrite similarly preserves the order of these nonterminal
symbols while grouping them from the left to the right, so reversing this rewrite will
reproduce the flattened tree.
</bodyText>
<page confidence="0.995193">
12
</page>
<note confidence="0.943685">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<figureCaption confidence="0.979849">
Figure 2
</figureCaption>
<bodyText confidence="0.983037">
A sample phrase structure tree (a) as it appears in the Penn Treebank, (b) after it has been
binarized, and (c) after it has been right-corner transformed.
</bodyText>
<subsectionHeader confidence="0.977233">
4.3 Mapping Trees to HHMM Derivations
</subsectionHeader>
<bodyText confidence="0.823409333333333">
Any tree can be mapped to an HHMM derivation by aligning the nonterminals with qdt
categories. First, it is necessary to define rightward depth d, right index position t, and
final (rightmost) child status fdt+1, for every nonterminal node A in a tree, where
</bodyText>
<listItem confidence="0.89113">
• d is defined to be the number of right branches between node A and the
root,
</listItem>
<page confidence="0.973665">
13
</page>
<note confidence="0.25889">
Computational Linguistics Volume 36, Number 1
</note>
<listItem confidence="0.940195666666667">
• t is defined to be the number of words beneath or to the left of node A, and
• fdt+1 is defined to be 0 if A is a left child, 1 if A is a unary child, and A if A is
right.
</listItem>
<bodyText confidence="0.634297666666667">
Any right-corner transformed tree can then be annotated with these values and rewrit-
ten to define labels and final-state values for every combination of d and t covered by
the tree. This is done using the rewrite rule
</bodyText>
<equation confidence="0.984307416666667">
d, t, A0, 0
⇒ d, t,A1,1
d, t, A1, 1
to replace unary branches with fdt+1 flags, and
d, t, A0,fdt+1
d, t, A0,fdt+1
d,t&apos;, A1,fd t�+1 d+1,t,A2,A2
d,t−1,A1,0
⇒
d,t&apos;+1,A1,0
d, t&apos;,A1,fdt�+1
d+1, t, A2, A2
</equation>
<bodyText confidence="0.9122230625">
to copy stacked-up left child constituents over multiple time steps, while lower-level
(right child) constituents are being recognized. The dashed line on the right side of the
rewrite rule represents the variable number of time steps for a stacked-up higher-level
constituent (as seen, for example, in time steps 4–7 at depth 1 in Figure 3). Coordinates
d, t ≤ D, and T that have fdt+1=1 are assigned label ‘−’, and coordinates not covered by
the tree are assigned label‘−’ andf d t+1=1.
The resulting label and final-state values at each node now define a value of qdt
andftd for each depth d and time step t of the HHMM (see Figure 3). Probabilities for
HHMM models ©Q-Expansion,d, ©Q-Transition,d, and ©F-Reduction,d can then be estimated from
these values directly. Like the right-corner transform, this mapping is reversible, so qdt
andftd values can be taken from a hypothesized most likely sequence and mapped back
Figure 3
Sample tree from Figure 2 mapped to qdt variable positions of an HHMM at each stack depth d
(vertical) and time step t (horizontal). This tree uses only two levels of stack memory. Values for
final-state variables ft d are not shown. Note that the mapping transform omits some nonterminal
labels; labels for these nodes can be reconstructed from their children.
</bodyText>
<page confidence="0.990949">
14
</page>
<note confidence="0.767361">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<bodyText confidence="0.999674666666667">
to trees (which can then undergo the reverse of the right-corner transform to become
ordinary phrase structure trees). Inspection of this rewrite rule will reveal the reverse of
this transform simply involves deleting unary-branching sequences that differ only in
the value of t and restoring unary branches when fdt+1=1.
This alignment of right-corner transformed trees also has the interesting property
that the categories on the stack at any given time step represent a segmentation of the
input up to that time step. For example, in Figure 3 at t = 12 the stack contains a sentence
lacking a verb phrase: S/VP (strong demand for ... bonds), followed by a verb projection
lacking a particle: VBN/PRT (propped).
</bodyText>
<subsectionHeader confidence="0.999107">
4.4 Comparison with Left-Corner Transform
</subsectionHeader>
<bodyText confidence="0.999936666666667">
A right-corner transform is used in this study, rather than a left-corner transform,
mainly because the right-corner version coincides with an intuition about how incom-
plete constituents might be stored in human memory. Stacked-up constituents in the
right-corner form correspond to chunks of words that have been encountered, rather
than hypothesized goal constituents. Intuitively, in the right-corner view, after a sen-
tence has been recognized, the stack memory contains a complete sentential constituent
(and some associated referent). In the left corner view, on the other hand, the stack mem-
ory after a sentence has been recognized contains only the lower-rightmost constituent
in the corresponding phrase structure tree (see Figure 4). This is because a time-order
</bodyText>
<figureCaption confidence="0.796298">
Figure 4
</figureCaption>
<figure confidence="0.369310333333333">
A left-corner transformed version of the tree (a) and memory store (b) from Figures 2 and 3.
15
Computational Linguistics Volume 36, Number 1
</figure>
<bodyText confidence="0.994795142857143">
alignment of a left-corner tree to elements in a bounded memory store corresponds to
a top-down traversal of the tree, whereas a time-order alignment of a right-corner tree
to elements in a bounded memory store corresponds to a bottom-up traversal of the
tree. If referential semantics are assumed to be calculated in tandem (as suggested by
the Tanenhaus et al. [1995] results), a top-down traversal through time requires some
effort to reconcile with the traditional compositional semantic notion that the meanings
of constituents are composed from the meanings of their parts (Frege 1892).
</bodyText>
<subsectionHeader confidence="0.998077">
4.5 Comparison with CCG
</subsectionHeader>
<bodyText confidence="0.997155333333333">
The incomplete constituent categories generated in the right-corner transform have the
same form and much of the same meaning as non-constituent categories in a CCG
(Steedman 2000).10 Both CCG operations of forward function application:
</bodyText>
<equation confidence="0.637024">
A1 - A1/A2 A2
</equation>
<bodyText confidence="0.94828125">
and forward function composition:
A1/A3 - A1/A2 A2/A3
appear in the branching structure of right-corner transformed trees. Nested operations
can also occur in CCG derivations:
</bodyText>
<equation confidence="0.764329">
A1/A2 - A1/A2/A3 A3
</equation>
<bodyText confidence="0.9964105">
as well as in right-corner transformed trees (using underscore delimiters to denote
sequences of constituent categories, described in Section 5.1):
</bodyText>
<equation confidence="0.642676">
A1/A2 - A1/A3 A2 A3
</equation>
<bodyText confidence="0.983847">
There are also correlates of type-raising (unary branches introduced by the right-corner
transform operations described in Section 4):
</bodyText>
<equation confidence="0.738817">
A1/A2 - A3
</equation>
<bodyText confidence="0.9790255">
But, importantly, the right-corner transform generates no correlates to the CCG
operations of backward function application or composition:
</bodyText>
<equation confidence="0.977673">
A1 - A2 A1\A2
A1\A3 -+ A2\A3 A1\A2
</equation>
<bodyText confidence="0.99997625">
This has two consequences. First, right-corner transform models do not introduce am-
biguity between type-raised forward and backward operations, as CCG derivations do.
Second, because leftward dependencies (as between a verb and its subject in English)
cannot be incorporated into lexical categories, right-corner transform models cannot be
taken to explicitly encode argument structure, as CCGs are. The right-corner transform
model described in this article is therefore perhaps better regarded as a performance
model of processing, given subcategorizations specified in some other grammar (such
as in this case the Treebank grammar), rather than a constraint on grammar itself.
</bodyText>
<footnote confidence="0.5309915">
10 In fact, one of the original motivations for CCG as a model of language was to minimize stack usage in
incremental processing (Ades and Steedman 1982).
</footnote>
<page confidence="0.988458">
16
</page>
<note confidence="0.867671">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<subsectionHeader confidence="0.988007">
4.6 Comparison with Cascaded FSAs in Information Extraction
</subsectionHeader>
<bodyText confidence="0.999880642857143">
Hierarchies of weighted finite-state automata (FSA)–equivalent HMMs may also be
viewed as probabilistic implementations of cascaded FSAs, used for modeling syntax
in information extraction systems such as FASTUS (Hobbs et al. 1996). Indeed, the left-
branching sequences of transformed constituents recognized by this model (as shown
in Figure 3) bear a strong resemblance to the flattened phrase structure representations
recognized by cascaded FSA systems, in that most phrases are consolidated to flat
sequences at one hierarchy level. This flat structure is desirable in cascaded FSA systems
because it allows information to be extracted from noun or verb phrases using straight-
forward pattern matching rules, implemented as FSA-equivalent regular expressions.
Like FASTUS, this system produces layers of flat phrases that can be searched
using regular expression pattern-matching rules. It also has a fixed number of levels
and linear-time recognition complexity. But unlike FASTUS, the model described here
can produce—and can be trained on—complete phrase structure trees (accessible by
reversing the transforms described previously).
</bodyText>
<sectionHeader confidence="0.97437" genericHeader="method">
5. Coverage
</sectionHeader>
<bodyText confidence="0.999960166666667">
The coverage of this model was evaluated on the large Penn Treebank corpus of
syntactically annotated sentences from the Switchboard corpus of transcribed speech
(Godfrey, Holliman, and McDaniel 1992) and the Wall Street Journal (Marcus, Santorini,
and Marcinkiewicz 1993). These sentences were right-corner transformed and mapped
to a time-aligned bounded memory store as described in Section 4 to determine the
amount of memory each sentence would require.
</bodyText>
<subsectionHeader confidence="0.990018">
5.1 Binary Branching Structure
</subsectionHeader>
<bodyText confidence="0.999569272727273">
In order to obtain a linguistically plausible right-corner transform representation of
incomplete constituents, the corpus is subjected to another pre-process transform to
introduce binary-branching nonterminal projections, and fold empty categories into
nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein
and Manning (2003). This binarization is done in such a way as to preserve linguistic
intuitions of head projection, so that the depth requirements of right-corner transformed
trees will be reasonable approximations to the working memory requirements of a
human reader or listener.
First, ordinary phrases and clauses are decomposed into head projections, each
consisting of one subordinate head projection and one argument or modifier, for
example:
</bodyText>
<equation confidence="0.9771678">
A0
A0
... VB:α1 NP:α2 ...
⇒ ... VB . . .
VB:α1 NP:α2
</equation>
<bodyText confidence="0.9980515">
The selection of head constituents is done using rewrite rules similar to the Magerman-
Black head rules (Magerman 1995). Any new constituent created by this process is
</bodyText>
<page confidence="0.989391">
17
</page>
<note confidence="0.479809">
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.99623725">
assigned the label of the subordinate head projection. The subordinate projection may
be the left or complete list of head-projection rewrite rules is provided in Appendix A.11
Conjunctions are decomposed into purely right-branching structures using non-
terminals appended with a “-LIST” suffix:
</bodyText>
<figure confidence="0.9619897">
A0
... A1:α1 CC A1:α2
A0
⇒ ... A1-LIST
A1:α1 CC A1:α2
A0
... A1:α1 A1-LIST:α2
A0
⇒ ... A1-LIST
A1:α1 A1-LIST:α2
</figure>
<bodyText confidence="0.999868888888889">
This right-branching decomposition of conjoined lists is motivated by the general
preference in English toward right branching structure, and the distinction of right
children as “-LIST” categories is motivated by the asymmetry of conjunctions such as
and and or generally occurring only between constituents at the end of a list, not at the
beginning. (Thus, in decomposing coffee, tea or milk, the words tea or milk form an NP-
LIST constituent, whereas the words coffee, tea do not.)
Empty constituents are removed outright, along with any unary projections that
may arise from this removal. In the case of empty constituents representing traces, the
extracted category label is annotated onto the lowest nonterminal dominating the trace
using the suffix “-extrX,” where “X” is the category of the extracted constituent. To
preserve grammaticality, this annotation is then passed up the tree and eliminated when
a wh-, topicalized, or other moved constituent is encountered, in a manner similar to that
used in Head-driven Phrase Structure Grammar (Pollard and Sag 1994), but this does
not affect branching structure.
Together these rewrites remove about 65% of super-binary branches from the un-
processed Treebank. All remaining super-binary branches are “nominally” decomposed
into right-branching structures by introducing intermediate nodes, each with a label
concatenated from the labels of its children, delimited by underscores:
</bodyText>
<equation confidence="0.9783966">
A0
A0
⇒ ... A1 A2
... A1:α1 A2:α2
A1:α1 A2:α2
</equation>
<bodyText confidence="0.999264666666667">
This decomposition is “nominal” in that the concatenated labels leave the resulting bi-
nary branches just as complex as the original n-ary branches prior to this decomposition.
It is equivalent to leaving super-binary branches intact and using dot rules in parsing
</bodyText>
<footnote confidence="0.797393142857143">
11 Although it is possible that in some cases these rules may generate counterintuitive branching patterns,
inspection of transformed trees during this experiment showed no unusual branching structure, except in
the case of noun sequences in base noun phrases (e.g. [general obligation] bonds or general [obligation
bonds]), which were left flat in the Treebank. Correct binarization of these structures would require
extensive annotator effort. However, because base noun phrases are often very small, and seldom contain
any sub-structure, it seems safe to assume that structural errors in these base noun phrases would not
drastically alter coverage results reported in this section.
</footnote>
<page confidence="0.997396">
18
</page>
<note confidence="0.752038">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<bodyText confidence="0.9885155">
(Earley 1970). This decomposition therefore does nothing to reduce sparse data effects
in statistical parsing.
</bodyText>
<subsectionHeader confidence="0.999398">
5.2 Coverage Results
</subsectionHeader>
<bodyText confidence="0.99977835483871">
Sections 2 and 3 (the standard training set) of the Penn Treebank Switchboard corpus
were binarized as described in Section 5.1, then right-corner transformed and mapped
to elements in a bounded memory store as described in Section 4. Punctuation added by
transcribers was removed. Coverage of this corpus, in sentences, for a recognizer using
right-corner transform chunking with one to five levels of stack memory, is shown in
Table 1. These results show that a simple syntax-based chunking into incomplete con-
stituents, using the right-corner transform defined in Section 4 of this article, allows a
vast majority of Switchboard sentences (over 99%) to be recognized using three or fewer
elements of memory, with no sentences requiring more than five elements, essentially
as predicted by studies of human short-term memory.
Although spontaneous speech is arguably more natural test data than prepared
speech or edited text, it is possible that coverage results on these data may under-
estimate processing requirements, due to the preponderance of very short sentences
and sentence fragments in spontaneous speech (for example, nearly 30% of sentences in
the Switchboard corpus are only one word long). It may also be argued that coverage
results on this corpus more accurately reflect the complexity of speech planning under
somewhat awkward social circumstances (being asked to start a conversation with
a stranger), which may be more cognitively demanding than recognition. For these
reasons, the right-corner transform chunking was also evaluated on Sections 2–21 (the
standard training set) of the Penn Treebank Wall Street Journal (WSJ) text corpus (see
Table 2, column 1).
The WSJ text corpus results appear to show substantially higher memory
requirements than Switchboard, with only 93% of sentences recognizable using three or
fewer memory elements. But much of this increase is due to arguably arbitrary treebank
conventions in annotating punctuation (for example, commas between phrases are
attached to the leftmost phrase: ([Pierre Vinken ... [61 years old] ,] joined ... ) which
can lead to psycholinguistically implausible analyses in which phrases (in this case
61 years old) are center-embedded by lone punctuation marks on one side or the other.
In general, branching structure for punctuation can be difficult to motivate on linguistic
grounds, because punctuation marks do not have lexical projections or argument
structure in most linguistic theories. In spoken language, punctuation corresponds to
</bodyText>
<tableCaption confidence="0.988936">
Table 1
</tableCaption>
<table confidence="0.787233">
Percent coverage of right-corner transformed Switchboard Treebank Sections 2–3.
memory capacity (right-corner, no punct) sentences coverage
no stack memory 26,201 28.38%
1 stack element 53,740 58.21%
2 stack elements 85,068 92.14%
3 stack elements 91,890 99.53%
4 stack elements 92,315 99.99%
5 stack elements 92,328 100.00%
TOTAL 92,328 100.00%
19
Computational Linguistics Volume 36, Number 1
</table>
<tableCaption confidence="0.820985">
Table 2
Percent coverage of left- and right-corner transformed WSJ Treebank Sections 2–21.
</tableCaption>
<table confidence="0.8838802">
memory capacity right-corner, with punct right-corner, no punct left-corner, no punct
sentences coverage sentences coverage sentences coverage
no stack elements 35 0.09% 127 0.32% 127 0.32%
1 stack elements 3,021 7.57% 3,550 8.90% 4,284 10.74%
2 stack elements 21,916 54.95% 25,948 65.06% 26,750 67.07%
3 stack elements 37,203 93.28% 38,948 97.66% 38,853 97.42%
4 stack elements 39,703 99.54% 39,866 99.96% 39,854 99.93%
5 stack elements 39,873 99.97% 39,883 100.00% 39,883 100.00%
6 stack elements 39,883 100.00% - - - -
TOTAL 39,883 100.00% 39,883 100.00% 39,883 100.00%
</table>
<bodyText confidence="0.994097303030303">
pauses or patterns of inflection, distributed throughout an utterance. It therefore seems
questionable to account for punctuation marks in a psycholinguistic model as explicit
composable concepts in a memory store. In order to counter possible undesirable
effects of an arbitrary branching analysis of punctuation, a second evaluation of the
model was performed on a version of the WSJ corpus with punctuation removed.
An analysis (Table 2, column 2) of the Penn Treebank WSJ corpus Sections 2–21
without punctuation, using the right-corner transformed trees just described, shows
that 97.66% of trees can be recognized using three hidden levels, and 99.96% can be
recognized using four, and again (similar to the Switchboard results), no sentences
require more than five remembered incomplete constituents. Table 2, column 3, shows
similar results for a left-corner transformed corpus, using left-right reflections of the
rewrite rules presented in Section 4.
Cowan (2001) documents empirically observed short-term memory limits of about
four elements across a wide variety of tasks. It is therefore not surprising to find a similar
limit in the memory required to parse the Treebank, assuming elements corresponding
to right-corner-transformed incomplete constituents.
As the table shows, some quintuply center-embedded constituents were found in
both corpora, suggesting that a three- to four-element limit may be soft, and can be
relaxed for short durations. Indeed, all quintuply embedded constituents were only a
few words long. Interestingly, many of the most heavily embedded words seemed to
strongly co-occur, which may suggest that these words arise from fixed expressions and
are not compositional. For example, Figure 5 shows one of the 13 phrase structure trees
in the Switchboard corpus which require five stack elements in right-corner parsing.
The complete sentence is:
So if there’s no one else around and I have a chance to listen to something I’ll turn that on.
If the construction there ’s NP AP in this sentence is parsed non-compositionally as a
single expression (and thus is rendered left-branching by the right-corner transform as
defined in Section 4), the sentence could be parsed using only four memory elements.
Even constrained to only four center embeddings, the existence of such sentences
confounds explanations of the center-embedding difficulties as directly arising from
stack limits in a left-corner (or right-corner) parser (Abney and Johnson 1991). It is
also interesting to note that three of the incomplete constituents in this example are
recursively nested or self-embedded instances of sentential projections, essentially with
</bodyText>
<page confidence="0.985962">
20
</page>
<note confidence="0.916434">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<figureCaption confidence="0.938097">
Figure 5
</figureCaption>
<bodyText confidence="0.957792076923077">
A phrase structure tree requiring five stack elements. Categories in bold will be incomplete at a
point after recognizing so if there’s no ...
the same category, similar to the center-embedded constructions which human readers
found difficult to process. This suggests that restrictions on self-embedding of identical
constituent categories would also fail to predict readability.
Instead, these data seem to argue in favor of an explanation due to probability:
Although the five-element sentences found in the Treebank use mostly common phrase
structure rules, problematic center-embedded sentences like the salmon the man the dog
chased smoked fell may cause difficulty simply because they are examples of an unusual
construction: a nested object relative clause. The fact that this is an unusual construction
may in turn be a result of the fact that speakers tend to avoid nesting object relative
clauses because they can lead to memory exhaustion, though such constructions may
become readable with practice.
</bodyText>
<sectionHeader confidence="0.970568" genericHeader="method">
6. In-Element Composition Ambiguity and Parsing Accuracy
</sectionHeader>
<bodyText confidence="0.999881705882353">
The right-corner transform described in Section 4 saves memory because it transforms
any right-branching sequence with left-child subtrees into a left-branching sequence of
incomplete constituents, with the same sequence of subtrees as right children. The left-
branching sequences of siblings resulting from this transform can then be composed
bottom-up through time by replacing each left child category with the category of the
resulting parent, within the same memory element (or depth level). For example, in
Figure 6(a) a left-child category NP/NP at time t = 4 is composed with a noun new of
category NP/NNP (a noun phrase lacking a proper noun yet to come), resulting in a
new parent category NP/NNP at time t = 5 replacing the left child category NP/NP in
the topmost d = 1 memory element.
This in-element composition preserves elements of the bounded memory store for
use in processing descendants of this composed constituent, yielding the human-like
memory demands reported in Section 5. But whenever an in-element composition like
this is hypothesized, it isolates an intermediate constituent (in this example, the noun
phrase new york city) from subsequent composition. Allowing access to this intermediate
constituent—for example, to allow new york city to become a modifier of bonds, which
itself becomes an argument of for—requires an analysis in which the intermediate
</bodyText>
<page confidence="0.996819">
21
</page>
<figure confidence="0.840033">
Computational Linguistics Volume 36, Number 1
</figure>
<figureCaption confidence="0.960037916666666">
Figure 6
Alternative analyses of strong demand for new york city ...: (a) using in-element composition,
compatible with strong demand for new york city is ... (in which the demand is for the city); and (b)
using cross-element (or delayed) composition, compatible with either strong demand for new york
city is ... (in which the demand is for the city) or strong demand for new york city bonds is ... (in
which a forthcoming referent—in this case, bonds—is associated with the city, and is in
demand). In-element composition (a) saves memory but closes off access to the noun phrase
headed by city, and so is not incompatible with the ...bonds completion. Cross-element
composition (b) requires more memory, but allows access to the noun phrase headed by city, so
is compatible with either completion. This ambiguity is introduced at t = 4 and propagated until
at least t = 7. An ordinary, non-right-corner stack machine would exclusively use analysis (b),
avoiding ambiguity.
</figureCaption>
<bodyText confidence="0.993541307692308">
constituent is stored in a separate memory element, shown in Figure 6(b). This creates
a local ambiguity in the parser (in this case, from time step t = 4) that may have to be
propagated across several words before it can be resolved (in this case, at time step
t = 7). This is essentially an ambiguity between arc-eager (in-element) and arc-standard
(cross-element) composition strategies, as described by Abney and Johnson (1991). In
contrast, an ordinary (purely arc-standard) parser with an unbounded stack would only
hypothesize analysis (b), avoiding this ambiguity.12
The right-corner HHMM approach described in this article relies on a learned
statistical model to predict when in-element (arc-eager) compositions will occur, in
addition to hypothesizing parse trees. The model encodes a mixed strategy: with some
probability arc-eager or arc-standard for each possible expansion. Accuracy results on
a right-corner HHMM model trained on the Penn Wall Street Journal Treebank suggest
that this kind of optionally arc-eager strategy can be reliably statistically learned.
</bodyText>
<subsectionHeader confidence="0.982055">
6.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.9997436">
In order to determine whether a memory-preserving parsing strategy, like the optionally
arc-eager strategy, can be reliably learned, a baseline Cocke-Kasami-Younger (CKY)
parser and bounded-memory right-corner HHMM parser were evaluated on the stan-
dard Penn Treebank WSJ Section 23 parsing task, using the binarized tree set described
in Section 5.2 (WSJ Sections 2–21) as training data. Training examples requiring more
</bodyText>
<footnote confidence="0.818434714285714">
12 It is important to note that neither the right-corner nor left-corner parsing strategy by itself creates this
ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce
memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as
that of Henderson (2004) adopt an arc-standard strategy, essentially always choosing analysis (b), and
thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must
maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in
short-term memory (Resnik 1992).
</footnote>
<page confidence="0.996317">
22
</page>
<note confidence="0.830452">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<bodyText confidence="0.999685488888889">
than four stack elements were excluded from training, in order to avoid generating
inconsistent model probabilities (e.g., from expansions that could not be re-composed
within the bounded memory store).
Most likely sequences of HHMM stack configurations are evaluated by reversing
the binarization, right-corner, and time-series mapping transforms described in Sec-
tions 4 and 5. But some of the binarization rewrites cannot be completely reversed,
because they cannot be unambiguously matched to output trees. Automatically derived
lexical projections below the annotated phrase level (e.g., binarizations of base noun
phrases) can be completely reversed, because the derived categories are character-
istically labeled with terminal symbols. So, too, can the conjunction and “nominal”
binarizations described in Section 5.1, because they can be identified by characteristic
“-LIST” and underscore delimiters. But automatically derived projections above the
annotated phrase level cannot be reliably identified in parser output (for example, an
intermediate projection “S -+ PP S” may or may not be annotated in the corpus). In order
to isolate the evaluation from the effects of these ambiguous matchings, the evaluation
was performed using trees in a partially binarized format, obtained by reversing only
those rewrites that result in unambiguous matches. Evaluating on this partially bina-
rized data does not seem to unfairly increase parsing performance compared to other
published results—quite the contrary: an evaluation using the state-of-the-art Charniak
(2000) parser scores about half a point worse on labeled F-score (89.3% vs. 89.9%) when
its hypotheses and gold standard trees are converted into this format.13
Both CKY baseline and HHMM test systems were run with a simple part of speech
(POS) model using relative frequency estimates from the training set, backed off to a
discriminative (decision tree) model conditioned on the last five letters of each word,
normalized over unigram POS probabilities. The CKY baseline and HHMM results were
obtained by training and evaluating on binarized trees, which is a necessary condition
for the right-corner transform. The CKY baseline results appear to be better than those
for a baseline probabilistic context-free grammar (PCFG) system reported by Klein and
Manning (2003) using no modifications to the corpus, and no parent or sibling condi-
tioning (see Table 3, top) because the binarization process allows the parser to avoid
some sparse data effects due to large flat branching structures in the Treebank, resulting
in improved parsing accuracy. Klein and Manning note that applying linguistically
motivated binarization transforms can yield substantial improvements in accuracy—as
much as nine points, in their study (in comparison, binarization only seems to improve
accuracy by about seven points above an unmodified baseline in the present study). But
the Klein and Manning results for binarization are provided only for models already
augmented with Markov dependencies (that is, conditioning on parent and sibling
categories, analogous to HHMM dependencies), so it was not possible to compare to
a binarized and un-Markovized benchmark.
The results for HHMM parsing, training, and evaluating on these same binarized
trees (modulo right-corner and variable-mapping transforms) were substantially bet-
ter than binarized CKY, most likely due to the expanded HHMM dependencies on
previous (qdt−1) and parent (qd−1
t ) variables at each qdt . For example, binarized PCFG
probabilities may be defined in terms of three category symbols A, B, and C: P(A
</bodyText>
<figure confidence="0.571909">
B C I A); whereas some of the HHMM probabilities are defined in terms of five category
</figure>
<footnote confidence="0.980016666666667">
13 This is presumably because the probability that a human annotator will annotate phrase structure
brackets at a particular projection or not is something existing parsers learn and exploit to improve their
accuracy. But it is not clear that this distinction is linguistically motivated.
</footnote>
<page confidence="0.993068">
23
</page>
<note confidence="0.452847">
Computational Linguistics Volume 36, Number 1
</note>
<tableCaption confidence="0.922812571428571">
Table 3
Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure
(% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on
unmodified and binarized WSJ Sections 22 (sentences 1–393: “devset”) and 23–24 (all sentences).
Results are shown with and without punctuation, compared to Klein and Manning 2003
(KM’03) using baseline and parent+sibling (par+sib) conditioning, and Roark 2001 (R’01) using
parent+sibling conditioning. Baseline CKY and test (parent+sibling) cases for the HHMM
system start out at a higher accuracy than for the Klein-Manning system because the HHMM
system requires binarization of trees, which removes some data sparsity in the raw Treebank
annotation, whereas the Klein-Manning results are computed prior to binarization. Because it is
incremental, the parser occasionally eliminates all continuable analyses from the beam, and
therefore fails to find a parse. HHMM parse failures are accounted as zeros in the recall statistics,
but are also listed separately, because in principle it might be possible to recover useful syntactic
structure from partial sequences.
</tableCaption>
<table confidence="0.998862636363636">
with punctuation: (&lt;40 wds) LR LP F-score sentence error
failure reduction
KM’03: unmodified, devset − − 72.6 0
KM’03: par+sib, devset − − 77.4 0 17.5%
CKY: binarized, devset 80.3 79.9 80.1 0.8
HHMM: par+sib, devset 84.1 83.5 83.8 0.5 18.6%
CKY: binarized, sect 23 78.8 79.4 79.1 0.1
HHMM: par+sib, sect 23 83.4 83.7 83.5 0.1 21.1%
no punctuation: (&lt;120 wds) LR LP F fail
R’01: par+sib, sect 23–24 75.2 77.4 − 0.1
HHMM: par+sib, sect 23–24 77.2 78.3 77.7 0.0
</table>
<bodyText confidence="0.990344315789474">
labels: P(A/B  |C/D, E) (transitioning from incomplete constituent C/D to incomplete
constituent A/B in the context of an expanding category E). This increases the number
of free parameters (estimated conditional probabilities) in the model,14 but apparently
not to the point of sparsity; this is similar to the effect of horizontal Markovization (con-
ditioning on the sibling category immediately previous to an expanded category) and
vertical Markovization (conditioning on the parent of an expanded category) commonly
used in PCFG parsing models (Collins 1999).
The improvement due to HHMM parsing over the PCFG baseline (18.6% reduction
in error) is comparable to that reported by Klein and Manning for parent and sibling
dependencies (first-order vertical and horizontal Markovization) over a baseline PCFG
without binarization (17.5% reduction in error). However, because it is not possible
to run the HHMM parser without binarization, and because Klein and Manning do
not report results for binarization transforms in the absence of parent and sibling
Markovization, it is potentially misleading to compare the results directly. For example,
it is possible that the binarization transforms described here may have performance-
optimizing effects that are latent in the binarized PCFG, but are brought out in HHMM
parsing.
Results on Section 23 of this corpus show close to 84% recall and precision, compa-
rable to that reported for state-of-the-art cubic-time parsers (with no constant bounds
</bodyText>
<footnote confidence="0.5474345">
14 Without punctuation, the HHMM model has 50,429 free parameters (including both Q and F models),
whereas the binarized PCFG has 12,373.
</footnote>
<page confidence="0.996494">
24
</page>
<note confidence="0.830832">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<bodyText confidence="0.99989692">
on processing storage) using similar configurations of conditioning information, that is,
without lexicalization or smoothing.
Roark (2001) describes a similar incremental parser based on left-corner trans-
formed grammars, and also reports results for parsing with and without parent and
sibling Markovization. Again the performance is comparable under similar conditions
(Table 3, bottom).
This system was run with a beam width of 2,000 hypotheses. This beam width
was selected in order to compare the performance of the bounded-memory model,
which predicts in-element or cross-element composition, with that of conventional
broad-coverage parsers, which also maintain large beams. With better modeling and
vastly more data from which to learn, it is possible that the human processor may
need to maintain far fewer alternative analyses, or perhaps only one, conditioned on
a lookahead window of observations (Henderson 2004).15
These experiments used a maximum stack depth of four, and conditioned expan-
sion and transition probabilities for each qdt on only the portion of the parent category
following the slash (that is, only A2 of A1/A2), in order to avoid sparse data effects.
Examples requiring more than four stack elements were excluded from training. This
is because in the basic relative frequency estimation used here, training examples are
depth-specific. Because the (unpunctuated) training set contains only about a dozen
sentences requiring more than four depth levels, each occupying that level for only a
few words, the data on which the fifth level of this model would be trained are very
sparse. Models at greater stack depths, and models depending on complete parent cate-
gories (or grandparent categories, etc., as in state-of-the-art parsers) could be developed
using smoothing and backoff techniques or feature-based log-linear models, but this is
left for later work (see Section 7).
</bodyText>
<sectionHeader confidence="0.826405" genericHeader="method">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.9999378">
This article has described a model of human syntactic processing that recognizes com-
plete phrase structure trees using only a small store of memory elements of limited
complexity. Sequences of hypothesized contents of this memory store can be mapped to
and from conventional phrase structure trees using a reversible right-corner transform.
If this syntactic processing model is combined with a bounded-memory interpreter
(Schuler, Wu, and Schwartz 2009), however, allowing the contents of this store to be
incrementally interpreted within the same bounded memory, it stands to reason that
complete, explicit phrase structure trees would not need to be constructed at any time
in processing, in keeping with experimental results showing similar lack of retention of
words and syntactic structure during human processing (Sachs 1967; Jarvella 1971).
Initial results show the use of a memory store consisting of only three to four mem-
ory elements within this framework provides nearly complete coverage of the Penn
Treebank Switchboard and WSJ corpora, consistent with recent estimates of general-
purpose short-term memory capacity. This suggests that, unlike some earlier mod-
els, the hypothesis that human sentence processing uses general-purpose short-term
</bodyText>
<footnote confidence="0.9908406">
15 Although, if most competing analyses are unconscious, they would be difficult to detect. Formally, the
competing pockets of activation hypothesized in a parallel-processing version of this model could be
arbitrarily small and numerous, but it seems unlikely that very small pockets of activation would persist
for very long (just as low probability analyses would be unlikely to remain on the HHMM beam). This
possibility is discussed in the particle filter account of Levy (2008).
</footnote>
<page confidence="0.994592">
25
</page>
<note confidence="0.592875">
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.99994838">
memory to store incomplete constituents, as defined by a right-corner transform, does
not seem to substantially underestimate human processing capacity. Moreover, despite
additional predictions that must take place within this model to manage parsing in such
close quarters, preliminary accuracy results for an unlexicalized, un-smoothed version
of this model, using only a four-element memory store, show close to 84% recall and
precision on the standard parsing evaluation. This result is comparable to that reported
for state-of-the-art cubic-time parsers (with no constant bounds on processing storage)
using similar configurations of conditioning information, namely, without lexicalization
or smoothing.
This model does not attempt to derive processing difficulties from memory bounds,
following evidence that garden path and center-embedding processing difficulties are
caused by interference or local probability estimates rather than encounters with mem-
ory capacity limits. But this does not mean that memory store capacity and probabilistic
explanations of processing difficulty are completely independent. Probability estima-
tion seems likely to be dependent on structural information from the memory store (for
example, incomplete object relative clauses seem to be very improbable in the context
of other incomplete object relative clauses). As hypotheses use more elements in the
memory store, the distribution over these hypotheses will tend to become broader,
taxing the reservoir of activation capacity, and making it more likely for low proba-
bility hypotheses to disappear, increasing the incidence of garden path errors. Further
investigations into how the memory store elements are allocated in various syntactic
contexts may allow these apparently disparate dimensions of processing capacity to be
unified.
The model described here may be promising as an engineering tool as well. But
to achieve competitive performance with unconstrained state-of-the-art parsers will
require the development of additional approximation algorithms beyond the scope of
this article. This is because most modern parsers are lexicalized, incorporating head-
word dependencies into parsing decisions, and employing finely tuned smoothing and
backoff techniques to integrate these potentially sparse head-word dependencies with
denser unlexicalized models. The bounded-memory right-corner HHMM described
in this article can also be lexicalized in this way, but because head word dependencies
are most straightforwardly defined in terms of top-down PCFG-like dependency
structures, this lexicalization requires the introduction of additional formal machinery
to transform PCFG probabilities into right-corner form (Schuler 2009). In other
words, rather than transforming a training set of trees and mapping them to a time
series model, it is necessary to transform a consistent probabilistically weighted
grammar (in some sense, an infinite set of trees) into appropriately weighted and
consistent right-corner PCFG and HHMM models. This requires the introduction of
an approximate inference algorithm, similar to that used in value iteration (Bellman
1957), which estimates probabilities of infinite left-recursive or right-recursive chains
by exploiting the fact that increasingly longer chains of events contribute exponentially
decreasing probability mass. On top of this, preserving head-word dependencies in
incremental processing also requires the introduction of a framework for storing head
words of modifier constituents that precede the head word of a parent constituent;
including some mechanism to ensure that probability assignments are fairly distributed
among competing hypotheses (e.g., by marginalizing over possible head words) in
cases where the calculation of accurate dependency probabilities must be deferred
until the head word of the parent constituent is encountered. For these reasons, a
complete lexicalized model is considered beyond the scope of this article, and is left for
future work.
</bodyText>
<page confidence="0.988304">
26
</page>
<note confidence="0.938836">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<sectionHeader confidence="0.940767" genericHeader="method">
Appendix A: Head Transform Rules
</sectionHeader>
<bodyText confidence="0.996544">
The experiments described in this article used a binarization process that included the
following rewrite rules, designed to binarize flat Treebank constituents into linguisti-
cally motivated head projections:
</bodyText>
<figure confidence="0.971912829787234">
1. NP: right-binarize basal NPs as much as possible; then left-binarize NPs
after left context reduced to nil:
A1:α1 A2:α2
A0
. . .
A0
⇒ ... A2
A0=NP|WHNP
... A1=[A-Z]*:α1 A2=NN[A-Z]*:α2 ...
A0=NP
⇒ A1 . . .
A1=NN[A-Z]*|NP:α1 A2=PP|S|VP|WHSBAR:α2 ...
A1:α1 A2:α2
2. VP: left-binarize basal VPs as much as possible; then right-binarize VPs
after right context reduced to nil:
A0=VP|SQ
... A1=VB[A-Z]*|BES:α1 A2=[A-Z]*:α2 ...
A0
⇒ ... A1
A1:α1 A2:α2
. . .
A0=VP
... A1=ADVP|RB[A-Z]*|PP:α1 A2=VB[A-Z]*|VP:α2
A0
⇒ ... A2
A1:α1 A2:α2
3. ADJP: right-binarize basal ADJPs as much as possible; then left-binarize
ADJPs after left context reduced to nil:
A0=ADJP[A-Z]*
. . .
4. ADVP: right-binarize basal ADVPs as much as possible; then left-binarize
ADVPs after left context reduced to nil:
... A1=RB[A-Z]*:α1 A2=JJ[A-Z]*:α2 ...
A1:α1 A2:α2
A0=ADJP
A0
⇒ A1 . . .
A1=JJ[A-Z]*|ADJP:α1 A2=PP|S:α2 ...
A1:α1 A2:α2
A0
⇒ ... A2
A1:α1 A2:α2
... A1=RB[A-Z]*:α1 A2=RB[A-Z]*:α2 ...
. . .
A0
⇒ ... A2
A0=ADVP
A0
A0=ADVP
⇒ A1 . . .
A1=RB[A-Z]*|ADVP:α1 A2=PP|S:α2 ...
A1:α1 A2:α2
27
Computational Linguistics Volume 36, Number 1
5. PP: left-binarize PPs as much as possible; then right-binarize PPs after
right context reduced to nil:
A0=PP|SBAR
. . .
A0
⇒ ... A1
... A1=IN|TO:α1 A2=[A-Z]*:α2 ...
A1:α1 A2:α2
A0=PP
... A1=ADVP|RB|PP:α1 A2=PP:α2
A0
⇒ ... A2
A1:α1 A2:α2
6. S: group subject NP and predicate VP of a sentence; then group modifiers
to right and left:
A0=S[A-Z]*
... A1=NP:α1 A2=VP:α2 ...
A0
⇒
... S
. . .
A1:α1 A2:α2
A1:α1 A2:α2
... A1=ADVP|RB[A-Z]*|PP:α1 A2=VB[A-Z]*|VP:α2 ...
. . .
A0
⇒ ... A2
A0=S[A-Z]*
A1:α1 A2:α2
... A1=ADVP|RB[A-Z]*|PP:α1 A2=A0:α2 ...
. . .
A0
⇒ ... A2
A0=S[A-Z]*
A1:α1 A2:α2
... A1=A0:α1 A2=ADVP|RB[A-Z]*|PP:α2 ...
. . .
A0
⇒ ... A1
A0=S[A-Z]*
</figure>
<sectionHeader confidence="0.995538" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998938">
The authors would like to thank
the anonymous reviewers for their input.
This research was supported by National
Science Foundation CAREER/PECASE
award 0447685 and by NASA award
NNX08AC36A. The views expressed are not
necessarily endorsed by the sponsors.
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997499766666667">
Abney, Steven P. and Mark Johnson. 1991.
Memory requirements and local
ambiguities of parsing strategies.
J. Psycholinguistic Research, 20(3):233–250.
Ades, Anthony E. and Mark Steedman. 1982.
On the order of words. Linguistics and
Philosophy, 4:517–558.
Aho, Alfred V. and Jeffery D. Ullman. 1972.
The Theory of Parsing, Translation and
Compiling; Volume. I: Parsing. Prentice-Hall,
Englewood Cliffs, NJ.
Baker, James. 1975. The Dragon system: an
overview. IEEE Transactions on Acoustics,
Speech and Signal Processing, 23(1):24–29.
Bellman, Richard. 1957. Dynamic
Programming. Princeton University Press,
Princeton, NJ.
Berg, George. 1992. A connectionist parser
with recursive sentence structure and
lexical disambiguation. In Proceedings of the
Tenth National Conference on Artificial
Intelligence, pages 32–37, San Jose, CA.
Bever, Thomas G.˙1970. The cognitive basis
for linguistic structure. In J. ˜R. Hayes,
editor, Cognition and the Development of
Language. Wiley, New York, pages 279–362.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference
resolution in the wild: Online
circumscription of referential domains in a
</reference>
<page confidence="0.997662">
28
</page>
<note confidence="0.850386">
Schuler et al. Parsing Using Human-Like Memory Constraints
</note>
<reference confidence="0.998476381355932">
natural interactive problem-solving task.
In Proceedings of the 24th Annual Meeting of
the Cognitive Science Society, pages 148–153,
Fairfax, VA.
Charniak, Eugene. 2000. A maximum-
entropy inspired parser. In Proceedings
of the First Meeting of the North American
Chapter of the Association for Computational
Linguistics (ANLP-NAACL’00),
pages 132–139, Seattle, WA.
Chomsky, Noam and George A. Miller.
1963. Introduction to the formal
analysis of natural languages.
In Handbook of Mathematical Psychology.
Wiley, New York, pages 269–321.
Collins, Michael. 1999. Head-driven
statistical models for natural language parsing.
Ph.D. thesis, University of Pennsylvania.
Cowan, Nelson. 2001. The magical
number 4 in short-term memory:
A reconsideration of mental storage
capacity. Behavioral and Brain Sciences,
24:87–185.
Crain, Stephen and Mark Steedman.
1985. On not being led up the garden path:
The use of context by the psychological
syntax processor. In D. R. Dowty,
L. Karttunen, and A. M. Zwicky, editors,
Natural Language Parsing: Psychological,
Computational, and Theoretical Perspectives,
number 1 in Studies in Natural Language
Processing. Cambridge University
Press, Cambridge, pages 320–358.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. CACM, 13(2):94–102.
Elman, Jeffrey L. 1991. Distributed
representations, simple recurrent
networks, and grammatical structure.
Machine Learning, 7:195–225.
Ericsson, K. Anders and Walter Kintsch.
1995. Long-term working memory.
Psychological Review, 102:211–245.
Frege, Gottlob.1892. Uber sinn
und bedeutung. Zeitschrift fur Philosophie
und Philosophischekritik, 100:25–50.
Gibson, Edward. 1991. A computational theory
of human linguistic processing: Memory
limitations and processing breakdown.
Ph.D. thesis, Carnegie Mellon University.
Godfrey, John J., Edward C. Holliman,
and Jane McDaniel. 1992. Switchboard:
Telephone speech corpus for research
and development. In Proceedings of
ICASSP, pages 517–520, San Francisco, CA.
Gordon, N. J., D. J. Salmond, and A. F. M.
Smith. 1993. Novel approach to nonlinear/
non-gaussian bayesian state estimation.
IEE Proceedings F (Radar and Signal
Processing), 140(2):107–113.
Gorrell, Paul. 1995. Syntax and Parsing.
Cambridge University Press, Cambridge.
Hale, John. 2001. A probabilistic earley parser
as a psycholinguistic model. In Proceedings
of the Second Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 159–166, Pittsburgh, PA.
Hale, John. 2006. Uncertainty about the
rest of the sentence. Cognitive Science,
30(4):609–642.
Helasvuo, Marja-Liisa. 2004. Shared
syntax: the grammar of co-constructions.
Journal of Pragmatics, 36:1315–1336.
Henderson, James. 2004. Lookahead
in deterministic left-corner parsing.
In Proceedings Workshop on Incremental
Parsing: Bringing Engineering and
Cognition Together, pages 26–33, Barcelona.
Hobbs, Jerry R., Douglas E. Appelt, John Bear,
David Israel, Megumi Kameyama, Mark
Stickel, and Mabry Tyson. 1996. Fastus:
A cascaded finite-state transducer for
extracting information from natural-
language text. In Yves Schabes, editor,
Finite State Devices for Natural Language
Processing. MIT Press, Cambridge, MA,
pages 383–406.
Jarvella, Robert J. 1971. Syntactic
processing of connected speech. Journal
of Verbal Learning and Verbal Behavior,
10:409–416.
Jelinek, Frederick, Lalit R. Bahl, and Robert L.
Mercer. 1975. Design of a linguistic
statistical decoder for the recognition
of continuous speech. IEEE Transactions
on Information Theory, 21:250–256.
Johnson, Mark. 1998a. Finite state
approximation of constraint-based
grammars using left-corner grammar
transforms. In Proceedings of COLING/ACL,
pages 619–623, Montreal.
Johnson, Mark. 1998b. PCFG models
of linguistic tree representation.
Computational Linguistics, 24:613–632.
Johnson-Laird, P. N. 1983. Mental Models:
Towards a Cognitive Science of Language,
Inference and Consciousness. Harvard
University Press, Cambridge, MA.
Just, Marcel Adam and Patricia A.
Carpenter. 1992. A capacity theory
of comprehension: Individual differences
in working memory. Psychological Review,
99:122–149.
Just, Marcel Adam and Sashank Varma.
2007. The organization of thinking:
What functional brain imaging
reveals about the neuroarchitecture of
complex cognition. Cognitive, Affective,
&amp; Behavioral Neuroscience, 7:153–191.
</reference>
<page confidence="0.939914">
29
</page>
<reference confidence="0.993840933333334">
Computational Linguistics Volume 36, Number 1
Kamide, Yuki and Don C. Mitchell. 1999.
Incremental pre-head attachment in
Japanese parsing. Language and
Cognitive Processes, 14:631–662.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423–430, Sapporo.
Lerner, Gene H. 1991. On the syntax of
sentences in progress. Language in
Society, 20:441–458.
Levy, Roger. 2008. Modeling the effects of
memory on human online sentence
processing with particle filters. In
Proceedings of NIPS, pages 937–944,
Vancouver.
Lewis, Richard L. and Shravan Vasishth.
2005. An activation-based model of
sentence processing as skilled
memory retrieval. Cognitive Science,
29(3):375–419.
Magerman, David. 1995. Statistical decision-
tree models for parsing. In Proceedings of the
33rd Annual Meeting of the Association
for Computational Linguistics (ACL’95),
pages 276–283, Cambridge, MA.
Marcus, Mitch. 1980. Theory of Syntactic
Recognition for Natural Language. MIT Press,
Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: the
Penn Treebank. Computational Linguistics,
19(2):313–330.
Mayberry, III, Marshall R. and Risto
Miikkulainen. 2003. Incremental
nonmonotonic parsing through semantic
self-organization. In Proceedings of the
25th Annual Conference of the Cognitive
Science Society, pages 798–803,
Boston, MA.
Miller, George A. 1956. The magical number
seven, plus or minus two: Some limits
on our capacity for processing information.
Psychological Review, 63:81–97.
Murphy, Kevin P. and Mark A. Paskin. 2001.
Linear time inference in hierarchical
HMMs. In Proceedings of NIPS,
pages 833–840, Vancouver.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure
Grammar. Chicago: University of Chicago
Press and Stanford: CSLI Publications.
Pritchett, Bradley L. 1991. Head position
and parsing ambiguity. Journal of
Psycholinguistic Research, 20:251–270.
Resnik, Philip. 1992. Left-corner parsing and
psychological plausibility. In Proceedings
of COLING, pages 191–197, Nantes.
Roark, Brian. 2001. Probabilistic top-down
parsing and language modeling.
Computational Linguistics, 27(2):249–276.
Rohde, Douglas L. T. 2002. A connectionist
model of sentence comprehension and
production. Ph.D. thesis, Computer Science
Department, Carnegie Mellon University.
Sachs, Jacqueline. 1967. Recognition memory
for syntactic and semantic aspects of
connected discourse. Perception and
Psychophysics, 2:437–442.
Schuler, William. 2009. Parsing with a
bounded stack using a model-based
right-corner transform. In Proceedings
of the North American Association for
Computational Linguistics (NAACL ’09),
pages 344–352, Boulder, CO.
Schuler, William, Stephen Wu, and
Lane Schwartz. 2009. A framework for
fast incremental interpretation during
speech decoding. Computational
Linguistics, 35(3):313–343.
Shieber, Stuart. 1985. Evidence
against the context-freeness of natural
language. Linguistics and Philosophy,
8:333–343.
Smolensky, P. and G. Legendre. 2006. The
Harmonic Mind: From Neural Computation
to Optimality-Theoretic Grammar. MIT Press,
Cambridge, MA.
Steedman, Mark. 2000. The Syntactic
Process. MIT Press/Bradford Books,
Cambridge, MA.
Stevenson, Suzanne. 1998. Parsing as
incremental restructuring. In J. D. Fodor
and F. Ferreira, editors, Reanalysis in
Sentence Processing. Kluwer Academic,
Boston, MA, pages 327–363.
Tanenhaus, Michael K., Michael J.
Spivey-Knowlton, Kathy M. Eberhard,
and Julie E. Sedivy. 1995. Integration of
visual and linguistic information in
spoken language comprehension. Science,
268:1632–1634.
</reference>
<page confidence="0.998794">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.530288">
<title confidence="0.7751775">Broad-Coverage Parsing Using Human-Like Memory Constraints</title>
<affiliation confidence="0.9995855">University of Minnesota Cairo University University of Minnesota University of Minnesota</affiliation>
<abstract confidence="0.9962163">Human syntactic processing shows many signs of taking place within a general-purpose short-term memory. But this kind of memory is known to have a severely constrained storage capacity — possibly constrained to as few as three or four distinct elements. This article describes a model of syntactic processing that operates successfully within these severe constraints, by recognizing constituents in a right-corner transformed representation (a variant of left-corner parsing) and mapping this representation to random variables in a Hierarchical Hidden Markov Model, a factored time-series model which probabilistically models the contents of a bounded memory store over time. Evaluations of the coverage of this model on a large syntactically annotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategy based on this model, suggest this model may be cognitively plausible.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
<author>Mark Johnson</author>
</authors>
<title>Memory requirements and local ambiguities of parsing strategies.</title>
<date>1991</date>
<journal>J. Psycholinguistic Research,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="7450" citStr="Abney and Johnson (1991)" startWordPosition="1073" endWordPosition="1076">eas the model described herein is exclusively a performance model, allowing generalizations about lexical argument structures to be learned in some other representation, then combined with probabilistic information about parsing strategies to yield a set of derived incomplete constituents. As a result, the model described in this article has a freer hand to satisfy strict working memory bounds, which may not permit some of the alternative composition operations proposed in the CCG account, thought to be associated with available prosody and quantifier scope analyses.1 Johnson-Laird (1983) and Abney and Johnson (1991) propose a pure processing account of memory capacity limits in parsing ordinary phrase structure trees. The Johnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, of which the right-corner transform introduced in this article is a variant, in order to bring memory usage for most parsable sentences to within seven or so active or awaited phrase structure constituents. This account may be used to explain human processing difficulties in processing triply center-embedded sentences like the rat that the cat that the dog chased killed ate the malt, with intended interpret</context>
<context position="27068" citStr="Abney and Johnson 1991" startWordPosition="3984" endWordPosition="3987">ucture, the right-corner transform changes right-branching structure into 6 Here [·] is an indicator function: [φ] = 1 if φ is true, 0 otherwise. 8 Schuler et al. Parsing Using Human-Like Memory Constraints left-branching structure. Recognition using this transformed grammar, extracted from a transformed corpus, is similar to recognition using a left-corner parsing strategy (Aho and Ullman 1972). This kind of strategy was shown to reduce memory requirements for parsing sentences with mainly left- or right-recursive phrase structure to fewer than seven active or awaited constituent categories (Abney and Johnson 1991). This is within Miller’s (1956) estimate of human short-term memory capacity (if memory elements store individual categories), whereas parsing heavily center-embedded sentences (known to be difficult for human readers) would require seven or more elements at the frontier of this capacity. But recent research suggests that human memory capacity may be limited to as few as three or four distinct items (Cowan 2001), with longer estimates of seven or more possibly due to the human capacity to chunk remembered items into associated groups (Miller 1956). The right-corner strategy described in this </context>
<context position="58293" citStr="Abney and Johnson 1991" startWordPosition="8778" endWordPosition="8781"> complete sentence is: So if there’s no one else around and I have a chance to listen to something I’ll turn that on. If the construction there ’s NP AP in this sentence is parsed non-compositionally as a single expression (and thus is rendered left-branching by the right-corner transform as defined in Section 4), the sentence could be parsed using only four memory elements. Even constrained to only four center embeddings, the existence of such sentences confounds explanations of the center-embedding difficulties as directly arising from stack limits in a left-corner (or right-corner) parser (Abney and Johnson 1991). It is also interesting to note that three of the incomplete constituents in this example are recursively nested or self-embedded instances of sentential projections, essentially with 20 Schuler et al. Parsing Using Human-Like Memory Constraints Figure 5 A phrase structure tree requiring five stack elements. Categories in bold will be incomplete at a point after recognizing so if there’s no ... the same category, similar to the center-embedded constructions which human readers found difficult to process. This suggests that restrictions on self-embedding of identical constituent categories wou</context>
<context position="62460" citStr="Abney and Johnson (1991)" startWordPosition="9432" endWordPosition="9435">ither completion. This ambiguity is introduced at t = 4 and propagated until at least t = 7. An ordinary, non-right-corner stack machine would exclusively use analysis (b), avoiding ambiguity. constituent is stored in a separate memory element, shown in Figure 6(b). This creates a local ambiguity in the parser (in this case, from time step t = 4) that may have to be propagated across several words before it can be resolved (in this case, at time step t = 7). This is essentially an ambiguity between arc-eager (in-element) and arc-standard (cross-element) composition strategies, as described by Abney and Johnson (1991). In contrast, an ordinary (purely arc-standard) parser with an unbounded stack would only hypothesize analysis (b), avoiding this ambiguity.12 The right-corner HHMM approach described in this article relies on a learned statistical model to predict when in-element (arc-eager) compositions will occur, in addition to hypothesizing parse trees. The model encodes a mixed strategy: with some probability arc-eager or arc-standard for each possible expansion. Accuracy results on a right-corner HHMM model trained on the Penn Wall Street Journal Treebank suggest that this kind of optionally arc-eager </context>
</contexts>
<marker>Abney, Johnson, 1991</marker>
<rawString>Abney, Steven P. and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. J. Psycholinguistic Research, 20(3):233–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony E Ades</author>
<author>Mark Steedman</author>
</authors>
<title>On the order of words. Linguistics and Philosophy,</title>
<date>1982</date>
<pages>4--517</pages>
<contexts>
<context position="6293" citStr="Ades and Steedman (1982)" startWordPosition="906" endWordPosition="909"> horse raced past the barn fell, with intended interpretation [NP the horse [RC (which was) raced past the barn]] fell (Bever 1970), in which raced seems like the main verb of the sentence until the word fell is encountered. But this explanation due to memory exhaustion is not compatible with observations of unproblematic parsing of sentences such as these when contextual information is provided in advance: for example, two men on horseback had a race; one went by the meadow, and the other went by the barn (Crain and Steedman 1985). 2 Schuler et al. Parsing Using Human-Like Memory Constraints Ades and Steedman (1982) introduce the idea of composing incomplete constituents to reduce storage demands in incremental processing using Combinatorial Categorial Grammar (CCG), avoiding the need to maintain large buffers of complete but unattached constituents. The right-corner transform described in this article composes incomplete constituents in very much the same way, but CCG is essentially a competence model, in that it seeks to unify lexical category representations used in processing with learned generalizations about argument structure, whereas the model described herein is exclusively a performance model, </context>
<context position="46359" citStr="Ades and Steedman 1982" startWordPosition="6987" endWordPosition="6990">etween a verb and its subject in English) cannot be incorporated into lexical categories, right-corner transform models cannot be taken to explicitly encode argument structure, as CCGs are. The right-corner transform model described in this article is therefore perhaps better regarded as a performance model of processing, given subcategorizations specified in some other grammar (such as in this case the Treebank grammar), rather than a constraint on grammar itself. 10 In fact, one of the original motivations for CCG as a model of language was to minimize stack usage in incremental processing (Ades and Steedman 1982). 16 Schuler et al. Parsing Using Human-Like Memory Constraints 4.6 Comparison with Cascaded FSAs in Information Extraction Hierarchies of weighted finite-state automata (FSA)–equivalent HMMs may also be viewed as probabilistic implementations of cascaded FSAs, used for modeling syntax in information extraction systems such as FASTUS (Hobbs et al. 1996). Indeed, the leftbranching sequences of transformed constituents recognized by this model (as shown in Figure 3) bear a strong resemblance to the flattened phrase structure representations recognized by cascaded FSA systems, in that most phrase</context>
</contexts>
<marker>Ades, Steedman, 1982</marker>
<rawString>Ades, Anthony E. and Mark Steedman. 1982. On the order of words. Linguistics and Philosophy, 4:517–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffery D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation and Compiling; Volume. I: Parsing. Prentice-Hall,</booktitle>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="26843" citStr="Aho and Ullman 1972" startWordPosition="3951" endWordPosition="3954">ded short-term memory store. This rightcorner transform is a variant of the left-corner transform described by Johnson (1998a), but whereas the left-corner transform changes left-branching structure into rightbranching structure, the right-corner transform changes right-branching structure into 6 Here [·] is an indicator function: [φ] = 1 if φ is true, 0 otherwise. 8 Schuler et al. Parsing Using Human-Like Memory Constraints left-branching structure. Recognition using this transformed grammar, extracted from a transformed corpus, is similar to recognition using a left-corner parsing strategy (Aho and Ullman 1972). This kind of strategy was shown to reduce memory requirements for parsing sentences with mainly left- or right-recursive phrase structure to fewer than seven active or awaited constituent categories (Abney and Johnson 1991). This is within Miller’s (1956) estimate of human short-term memory capacity (if memory elements store individual categories), whereas parsing heavily center-embedded sentences (known to be difficult for human readers) would require seven or more elements at the frontier of this capacity. But recent research suggests that human memory capacity may be limited to as few as </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, Alfred V. and Jeffery D. Ullman. 1972. The Theory of Parsing, Translation and Compiling; Volume. I: Parsing. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Baker</author>
</authors>
<title>The Dragon system: an overview.</title>
<date>1975</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="19977" citStr="Baker 1975" startWordPosition="2874" endWordPosition="2875"> in a text processing application). A most likely sequence of hidden states ˆq1..T can then be hypothesized given any sequence of observed states o1..T: ˆq1..T =argmax P(q1..T |o1..T) (1) q1..T = argmax P(q1..T) · P(o1..T |q1..T) (2) q1..T def T PΘA(qt |qt−1) · PΘB(ot |qt) (3) = argmax ri q1..T t=1 using Bayes’ Law (Equation 2) and Markov independence assumptions (Equation 3) to define a full P(q1..T |o1..T) probability as the product of a Language Model (ΘA) prior probability P(q1..T) deft PΘA(qt |qt−1) and an Observation Model (ΘB) likelihood probability P(o1..T |q1..T) def f1t PΘB(ot |qt) (Baker 1975; Jelinek, Bahl, and Mercer 1975). Language model transitions PΘA(qt |qt−1) over complex hidden states qt can be modeled using synchronized levels of stacked-up component HMMs in an HHMM, analogous to a shift-reduce parser or pushdown automaton with a bounded stack. HHMM transition probabilities are calculated in two phases: a “reduce” phase (resulting in an intermediate, transient final-state variable ft), modeling whether component HMMs terminate; and a “shift” phase (resulting in a persistent modeled state qt), in which unterminated HMMs transition and terminated HMMs are re-initialized fro</context>
</contexts>
<marker>Baker, 1975</marker>
<rawString>Baker, James. 1975. The Dragon system: an overview. IEEE Transactions on Acoustics, Speech and Signal Processing, 23(1):24–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Bellman</author>
</authors>
<title>Dynamic Programming.</title>
<date>1957</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ.</location>
<contexts>
<context position="78471" citStr="Bellman 1957" startWordPosition="11808" endWordPosition="11809">down PCFG-like dependency structures, this lexicalization requires the introduction of additional formal machinery to transform PCFG probabilities into right-corner form (Schuler 2009). In other words, rather than transforming a training set of trees and mapping them to a time series model, it is necessary to transform a consistent probabilistically weighted grammar (in some sense, an infinite set of trees) into appropriately weighted and consistent right-corner PCFG and HHMM models. This requires the introduction of an approximate inference algorithm, similar to that used in value iteration (Bellman 1957), which estimates probabilities of infinite left-recursive or right-recursive chains by exploiting the fact that increasingly longer chains of events contribute exponentially decreasing probability mass. On top of this, preserving head-word dependencies in incremental processing also requires the introduction of a framework for storing head words of modifier constituents that precede the head word of a parent constituent; including some mechanism to ensure that probability assignments are fairly distributed among competing hypotheses (e.g., by marginalizing over possible head words) in cases w</context>
</contexts>
<marker>Bellman, 1957</marker>
<rawString>Bellman, Richard. 1957. Dynamic Programming. Princeton University Press, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Berg</author>
</authors>
<title>A connectionist parser with recursive sentence structure and lexical disambiguation.</title>
<date>1992</date>
<booktitle>In Proceedings of the Tenth National Conference on Artificial Intelligence,</booktitle>
<pages>32--37</pages>
<location>San Jose, CA.</location>
<contexts>
<context position="14100" citStr="Berg 1992" startWordPosition="2016" endWordPosition="2017">ther than about an entire unbounded phrase structure tree.3 2 Probability distributions in entropy-based models like Hale’s are typically assumed to be defined over sets of hypotheses pursued in parallel, but other interpretations (for example, lookahead-based deterministic models) are possible. The model described in this article is also compatible with deterministic parsing frameworks, in which case it models allocation of processing resources among incompletely-recognized constituents within a single non-competing hypothesis. 3 Pure connectionist models of syntactic processing (Elman 1991; Berg 1992; Rohde 2002) attempt to unify storage of constituent structure with that of ambiguous alternative analyses, but the memory demands of systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for using self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen 2003), but are still limited to parsing relatively short travel planning queries with limited syntactic complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up constituents, and use connectionist models for probability </context>
</contexts>
<marker>Berg, 1992</marker>
<rawString>Berg, George. 1992. A connectionist parser with recursive sentence structure and lexical disambiguation. In Proceedings of the Tenth National Conference on Artificial Intelligence, pages 32–37, San Jose, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bever</author>
</authors>
<title>Thomas G.˙1970. The cognitive basis for linguistic structure.</title>
<booktitle>Cognition and the Development of Language.</booktitle>
<pages>279--362</pages>
<editor>In J. ˜R. Hayes, editor,</editor>
<publisher>Wiley,</publisher>
<location>New York,</location>
<marker>Bever, </marker>
<rawString>Bever, Thomas G.˙1970. The cognitive basis for linguistic structure. In J. ˜R. Hayes, editor, Cognition and the Development of Language. Wiley, New York, pages 279–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Brown-Schmidt</author>
<author>Ellen Campana</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>Reference resolution in the wild: Online circumscription of referential domains in a natural interactive problem-solving task.</title>
<date>2002</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>148--153</pages>
<location>Fairfax, VA.</location>
<marker>Brown-Schmidt, Campana, Tanenhaus, 2002</marker>
<rawString>Brown-Schmidt, Sarah, Ellen Campana, and Michael K. Tanenhaus. 2002. Reference resolution in the wild: Online circumscription of referential domains in a natural interactive problem-solving task. In Proceedings of the 24th Annual Meeting of the Cognitive Science Society, pages 148–153, Fairfax, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximumentropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (ANLP-NAACL’00),</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="65826" citStr="Charniak (2000)" startWordPosition="9920" endWordPosition="9921">he annotated phrase level cannot be reliably identified in parser output (for example, an intermediate projection “S -+ PP S” may or may not be annotated in the corpus). In order to isolate the evaluation from the effects of these ambiguous matchings, the evaluation was performed using trees in a partially binarized format, obtained by reversing only those rewrites that result in unambiguous matches. Evaluating on this partially binarized data does not seem to unfairly increase parsing performance compared to other published results—quite the contrary: an evaluation using the state-of-the-art Charniak (2000) parser scores about half a point worse on labeled F-score (89.3% vs. 89.9%) when its hypotheses and gold standard trees are converted into this format.13 Both CKY baseline and HHMM test systems were run with a simple part of speech (POS) model using relative frequency estimates from the training set, backed off to a discriminative (decision tree) model conditioned on the last five letters of each word, normalized over unigram POS probabilities. The CKY baseline and HHMM results were obtained by training and evaluating on binarized trees, which is a necessary condition for the right-corner tra</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, Eugene. 2000. A maximumentropy inspired parser. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (ANLP-NAACL’00), pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>George A Miller</author>
</authors>
<title>Introduction to the formal analysis of natural languages.</title>
<date>1963</date>
<booktitle>In Handbook of Mathematical Psychology.</booktitle>
<pages>269--321</pages>
<publisher>Wiley,</publisher>
<location>New York,</location>
<contexts>
<context position="8157" citStr="Chomsky and Miller 1963" startWordPosition="1185" endWordPosition="1188">rase structure trees. The Johnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, of which the right-corner transform introduced in this article is a variant, in order to bring memory usage for most parsable sentences to within seven or so active or awaited phrase structure constituents. This account may be used to explain human processing difficulties in processing triply center-embedded sentences like the rat that the cat that the dog chased killed ate the malt, with intended interpretation SNP the rat that SNP the cat that SNP the dog] chased] killed] ate the malt (Chomsky and Miller 1963). But this explanation does not account for examples of triply center-embedded sentences that typically do not cause processing problems: SNP that SNP the food that SNP John] ordered] tasted good] pleased him (Gibson 1991). Moreover, the apparent competition between comprehension of center-embedded object relatives and retention of unrelated words in general-purpose memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at least, can be) used to store incomplete constituents during comprehension. This would predict three or four elements of reliable storage, rather than s</context>
</contexts>
<marker>Chomsky, Miller, 1963</marker>
<rawString>Chomsky, Noam and George A. Miller. 1963. Introduction to the formal analysis of natural languages. In Handbook of Mathematical Psychology. Wiley, New York, pages 269–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="70473" citStr="Collins 1999" startWordPosition="10637" endWordPosition="10638"> − 0.1 HHMM: par+sib, sect 23–24 77.2 78.3 77.7 0.0 labels: P(A/B |C/D, E) (transitioning from incomplete constituent C/D to incomplete constituent A/B in the context of an expanding category E). This increases the number of free parameters (estimated conditional probabilities) in the model,14 but apparently not to the point of sparsity; this is similar to the effect of horizontal Markovization (conditioning on the sibling category immediately previous to an expanded category) and vertical Markovization (conditioning on the parent of an expanded category) commonly used in PCFG parsing models (Collins 1999). The improvement due to HHMM parsing over the PCFG baseline (18.6% reduction in error) is comparable to that reported by Klein and Manning for parent and sibling dependencies (first-order vertical and horizontal Markovization) over a baseline PCFG without binarization (17.5% reduction in error). However, because it is not possible to run the HHMM parser without binarization, and because Klein and Manning do not report results for binarization transforms in the absence of parent and sibling Markovization, it is potentially misleading to compare the results directly. For example, it is possible</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, Michael. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelson Cowan</author>
</authors>
<title>The magical number 4 in short-term memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences,</title>
<date>2001</date>
<pages>24--87</pages>
<contexts>
<context position="1862" citStr="Cowan 2001" startWordPosition="264" endWordPosition="265">term memory. Like other short-term memory processes, syntactic processing is susceptible to degradation if short-term memory capacity is loaded, for example, when readers are asked to retain lists of words while reading (Just and Carpenter 1992); and memory of words and syntax degrades over time within and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse information about referents from other sentences (Ericsson and Kintsch 1995). But short-term memory is known to have severe capacity limitations of perhaps no more than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem * Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455. E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu. ** Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street, Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg. Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for publication: 27 May 2009. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 too austere to process the ri</context>
<context position="8774" citStr="Cowan 2001" startWordPosition="1275" endWordPosition="1276">this explanation does not account for examples of triply center-embedded sentences that typically do not cause processing problems: SNP that SNP the food that SNP John] ordered] tasted good] pleased him (Gibson 1991). Moreover, the apparent competition between comprehension of center-embedded object relatives and retention of unrelated words in general-purpose memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at least, can be) used to store incomplete constituents during comprehension. This would predict three or four elements of reliable storage, rather than seven (Cowan 2001). The transform-based model described in this article exploits a conception of chunking (Miller 1956) to combine pairs of active and awaited constituents from the Abney and Johnson analysis, connected by recognized structure, in order to operate within estimates of human short-term memory bounds. Because of these counterexamples to the memory-exhaustion explanation of garden path and center-embedding difficulties, recent work has turned to explanations other than memory exhaustion for these phenomena. Lewis and Vasishth (2005) attribute processing errors to activation interference among stored</context>
<context position="27484" citStr="Cowan 2001" startWordPosition="4049" endWordPosition="4050">own to reduce memory requirements for parsing sentences with mainly left- or right-recursive phrase structure to fewer than seven active or awaited constituent categories (Abney and Johnson 1991). This is within Miller’s (1956) estimate of human short-term memory capacity (if memory elements store individual categories), whereas parsing heavily center-embedded sentences (known to be difficult for human readers) would require seven or more elements at the frontier of this capacity. But recent research suggests that human memory capacity may be limited to as few as three or four distinct items (Cowan 2001), with longer estimates of seven or more possibly due to the human capacity to chunk remembered items into associated groups (Miller 1956). The right-corner strategy described in this paper therefore assumes constituent categories can similarly be chunked into incomplete constituents A/B formed by pairing an active category A with an awaited category B somewhere along the active category’s right progeny (so, for example, a transitive verb may become an incomplete constituent VP/NP consisting of an active verb phrase lacking an awaited noun phrase yet to come).7 These chunked incomplete constit</context>
<context position="56780" citStr="Cowan (2001)" startWordPosition="8549" endWordPosition="8550"> on a version of the WSJ corpus with punctuation removed. An analysis (Table 2, column 2) of the Penn Treebank WSJ corpus Sections 2–21 without punctuation, using the right-corner transformed trees just described, shows that 97.66% of trees can be recognized using three hidden levels, and 99.96% can be recognized using four, and again (similar to the Switchboard results), no sentences require more than five remembered incomplete constituents. Table 2, column 3, shows similar results for a left-corner transformed corpus, using left-right reflections of the rewrite rules presented in Section 4. Cowan (2001) documents empirically observed short-term memory limits of about four elements across a wide variety of tasks. It is therefore not surprising to find a similar limit in the memory required to parse the Treebank, assuming elements corresponding to right-corner-transformed incomplete constituents. As the table shows, some quintuply center-embedded constituents were found in both corpora, suggesting that a three- to four-element limit may be soft, and can be relaxed for short durations. Indeed, all quintuply embedded constituents were only a few words long. Interestingly, many of the most heavil</context>
</contexts>
<marker>Cowan, 2001</marker>
<rawString>Cowan, Nelson. 2001. The magical number 4 in short-term memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences, 24:87–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Crain</author>
<author>Mark Steedman</author>
</authors>
<title>On not being led up the garden path: The use of context by the psychological syntax processor.</title>
<date>1985</date>
<booktitle>Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives, number 1 in Studies in Natural Language Processing.</booktitle>
<pages>320--358</pages>
<editor>In D. R. Dowty, L. Karttunen, and A. M. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="6206" citStr="Crain and Steedman 1985" startWordPosition="893" endWordPosition="896">xplanation for human difficulties in processing garden path sentences: for example, the horse raced past the barn fell, with intended interpretation [NP the horse [RC (which was) raced past the barn]] fell (Bever 1970), in which raced seems like the main verb of the sentence until the word fell is encountered. But this explanation due to memory exhaustion is not compatible with observations of unproblematic parsing of sentences such as these when contextual information is provided in advance: for example, two men on horseback had a race; one went by the meadow, and the other went by the barn (Crain and Steedman 1985). 2 Schuler et al. Parsing Using Human-Like Memory Constraints Ades and Steedman (1982) introduce the idea of composing incomplete constituents to reduce storage demands in incremental processing using Combinatorial Categorial Grammar (CCG), avoiding the need to maintain large buffers of complete but unattached constituents. The right-corner transform described in this article composes incomplete constituents in very much the same way, but CCG is essentially a competence model, in that it seeks to unify lexical category representations used in processing with learned generalizations about argu</context>
</contexts>
<marker>Crain, Steedman, 1985</marker>
<rawString>Crain, Stephen and Mark Steedman. 1985. On not being led up the garden path: The use of context by the psychological syntax processor. In D. R. Dowty, L. Karttunen, and A. M. Zwicky, editors, Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives, number 1 in Studies in Natural Language Processing. Cambridge University Press, Cambridge, pages 320–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>CACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="52099" citStr="Earley 1970" startWordPosition="7845" endWordPosition="7846">is experiment showed no unusual branching structure, except in the case of noun sequences in base noun phrases (e.g. [general obligation] bonds or general [obligation bonds]), which were left flat in the Treebank. Correct binarization of these structures would require extensive annotator effort. However, because base noun phrases are often very small, and seldom contain any sub-structure, it seems safe to assume that structural errors in these base noun phrases would not drastically alter coverage results reported in this section. 18 Schuler et al. Parsing Using Human-Like Memory Constraints (Earley 1970). This decomposition therefore does nothing to reduce sparse data effects in statistical parsing. 5.2 Coverage Results Sections 2 and 3 (the standard training set) of the Penn Treebank Switchboard corpus were binarized as described in Section 5.1, then right-corner transformed and mapped to elements in a bounded memory store as described in Section 4. Punctuation added by transcribers was removed. Coverage of this corpus, in sentences, for a recognizer using right-corner transform chunking with one to five levels of stack memory, is shown in Table 1. These results show that a simple syntax-bas</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay. 1970. An efficient context-free parsing algorithm. CACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Distributed representations, simple recurrent networks, and grammatical structure.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>7--195</pages>
<contexts>
<context position="14089" citStr="Elman 1991" startWordPosition="2014" endWordPosition="2015">ven time, rather than about an entire unbounded phrase structure tree.3 2 Probability distributions in entropy-based models like Hale’s are typically assumed to be defined over sets of hypotheses pursued in parallel, but other interpretations (for example, lookahead-based deterministic models) are possible. The model described in this article is also compatible with deterministic parsing frameworks, in which case it models allocation of processing resources among incompletely-recognized constituents within a single non-competing hypothesis. 3 Pure connectionist models of syntactic processing (Elman 1991; Berg 1992; Rohde 2002) attempt to unify storage of constituent structure with that of ambiguous alternative analyses, but the memory demands of systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for using self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen 2003), but are still limited to parsing relatively short travel planning queries with limited syntactic complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up constituents, and use connectionist models for p</context>
</contexts>
<marker>Elman, 1991</marker>
<rawString>Elman, Jeffrey L. 1991. Distributed representations, simple recurrent networks, and grammatical structure. Machine Learning, 7:195–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Anders Ericsson</author>
<author>Walter Kintsch</author>
</authors>
<title>Long-term working memory. Psychological Review,</title>
<date>1995</date>
<pages>102--211</pages>
<contexts>
<context position="1712" citStr="Ericsson and Kintsch 1995" startWordPosition="238" endWordPosition="241">del may be cognitively plausible. 1. Introduction It is an interesting possibility that human syntactic processing may occur entirely within a general-purpose short-term memory. Like other short-term memory processes, syntactic processing is susceptible to degradation if short-term memory capacity is loaded, for example, when readers are asked to retain lists of words while reading (Just and Carpenter 1992); and memory of words and syntax degrades over time within and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse information about referents from other sentences (Ericsson and Kintsch 1995). But short-term memory is known to have severe capacity limitations of perhaps no more than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem * Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455. E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu. ** Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street, Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg. Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for </context>
</contexts>
<marker>Ericsson, Kintsch, 1995</marker>
<rawString>Ericsson, K. Anders and Walter Kintsch. 1995. Long-term working memory. Psychological Review, 102:211–245.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gottlob 1892 Frege</author>
</authors>
<title>Uber sinn und bedeutung.</title>
<booktitle>Zeitschrift fur Philosophie und Philosophischekritik,</booktitle>
<pages>100--25</pages>
<marker>Frege, </marker>
<rawString>Frege, Gottlob.1892. Uber sinn und bedeutung. Zeitschrift fur Philosophie und Philosophischekritik, 100:25–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>A computational theory of human linguistic processing: Memory limitations and processing breakdown.</title>
<date>1991</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="8379" citStr="Gibson 1991" startWordPosition="1221" endWordPosition="1222">entences to within seven or so active or awaited phrase structure constituents. This account may be used to explain human processing difficulties in processing triply center-embedded sentences like the rat that the cat that the dog chased killed ate the malt, with intended interpretation SNP the rat that SNP the cat that SNP the dog] chased] killed] ate the malt (Chomsky and Miller 1963). But this explanation does not account for examples of triply center-embedded sentences that typically do not cause processing problems: SNP that SNP the food that SNP John] ordered] tasted good] pleased him (Gibson 1991). Moreover, the apparent competition between comprehension of center-embedded object relatives and retention of unrelated words in general-purpose memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at least, can be) used to store incomplete constituents during comprehension. This would predict three or four elements of reliable storage, rather than seven (Cowan 2001). The transform-based model described in this article exploits a conception of chunking (Miller 1956) to combine pairs of active and awaited constituents from the Abney and Johnson analysis, connected by r</context>
<context position="34385" citStr="Gibson 1991" startWordPosition="5069" endWordPosition="5070">nt would have to be systematically undone. An interesting possibility might be that the appearance of syntactic restructuring may arise from a collection of hypothesized stores of syntactically fixed incomplete constituents, pursued in parallel. The results presented in this article suggest that this mechanism is possible, but these two possibilities might be very difficult to distinguish empirically. There is also a tradition of defining incomplete constituents as head-driven— introduced in parsing only at the point in incremental recognition at which they can be associated with a head word (Gibson 1991: Pritcher 1991: Gorrell 1995). In typically head-initial languages such as English, incomplete constituents derived from these head-driven models resemble those derived from a right-corner transform. But headdriven incomplete constituents do not appear to obey general-purpose memory bounds in head-final languages such as Japanese, and do not appear to obey attachment prefer10 Schuler et al. Parsing Using Human-Like Memory Constraints ences predicted by a head-driven account (Kamide and Mitchell 1999), favoring a prehead attachment account, as a right-corner transform would predict. 4.1 Tree T</context>
</contexts>
<marker>Gibson, 1991</marker>
<rawString>Gibson, Edward. 1991. A computational theory of human linguistic processing: Memory limitations and processing breakdown. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>517--520</pages>
<location>San Francisco, CA.</location>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>Godfrey, John J., Edward C. Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Proceedings of ICASSP, pages 517–520, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Gordon</author>
<author>D J Salmond</author>
<author>A F M Smith</author>
</authors>
<title>Novel approach to nonlinear/ non-gaussian bayesian state estimation.</title>
<date>1993</date>
<booktitle>IEE Proceedings F (Radar and Signal Processing),</booktitle>
<pages>140--2</pages>
<marker>Gordon, Salmond, Smith, 1993</marker>
<rawString>Gordon, N. J., D. J. Salmond, and A. F. M. Smith. 1993. Novel approach to nonlinear/ non-gaussian bayesian state estimation. IEE Proceedings F (Radar and Signal Processing), 140(2):107–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Gorrell</author>
</authors>
<title>Syntax and Parsing.</title>
<date>1995</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="34415" citStr="Gorrell 1995" startWordPosition="5073" endWordPosition="5074">ically undone. An interesting possibility might be that the appearance of syntactic restructuring may arise from a collection of hypothesized stores of syntactically fixed incomplete constituents, pursued in parallel. The results presented in this article suggest that this mechanism is possible, but these two possibilities might be very difficult to distinguish empirically. There is also a tradition of defining incomplete constituents as head-driven— introduced in parsing only at the point in incremental recognition at which they can be associated with a head word (Gibson 1991: Pritcher 1991: Gorrell 1995). In typically head-initial languages such as English, incomplete constituents derived from these head-driven models resemble those derived from a right-corner transform. But headdriven incomplete constituents do not appear to obey general-purpose memory bounds in head-final languages such as Japanese, and do not appear to obey attachment prefer10 Schuler et al. Parsing Using Human-Like Memory Constraints ences predicted by a head-driven account (Kamide and Mitchell 1999), favoring a prehead attachment account, as a right-corner transform would predict. 4.1 Tree Transforms Using Rewrite Rules </context>
</contexts>
<marker>Gorrell, 1995</marker>
<rawString>Gorrell, Paul. 1995. Syntax and Parsing. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>159--166</pages>
<location>Pittsburgh, PA.</location>
<marker>Hale, 2001</marker>
<rawString>Hale, John. 2001. A probabilistic earley parser as a psycholinguistic model. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, pages 159–166, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>Uncertainty about the rest of the sentence.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<volume>30</volume>
<issue>4</issue>
<marker>Hale, 2006</marker>
<rawString>Hale, John. 2006. Uncertainty about the rest of the sentence. Cognitive Science, 30(4):609–642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marja-Liisa Helasvuo</author>
</authors>
<title>Shared syntax: the grammar of co-constructions.</title>
<date>2004</date>
<journal>Journal of Pragmatics,</journal>
<pages>36--1315</pages>
<contexts>
<context position="23651" citStr="Helasvuo 2004" startWordPosition="3443" endWordPosition="3444"> rather than marginalized, but in any case the intermediate variables do not persist. 5 Also, the fact that this is a generative model, in which observations are conditioned on hypotheses, then flipped using Bayes’ Law (Equation 2)—as opposed to a discriminative or conditional model, in which hypotheses are conditioned directly on observations—is also appealing as a human model, in that it allows the same architecture to be used for both recognition and generation. This is a desirable property for modeling split utterances, in which interlocutors complete one another’s sentences (Lerner 1991; Helasvuo 2004). 7 Computational Linguistics Volume 36, Number 1 variable is a reduction or non-reduction state ftd E GU{1, 0} (indicating, respectively, a reduction of incomplete constituent qd t−1 to a complete right child constituent of some grammatical category from domain G, or a non-reduction of qd t−1 as a unary or left child, as defined in Section 4). An intermediate variable f t d at depth d may indicate reduction or non-reduction according to ΘF-Reduction,d if there is a reduction at the depth level immediately below d, but must indicate non-reduction (ftd = 0) with probability 1 if there is no red</context>
</contexts>
<marker>Helasvuo, 2004</marker>
<rawString>Helasvuo, Marja-Liisa. 2004. Shared syntax: the grammar of co-constructions. Journal of Pragmatics, 36:1315–1336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Lookahead in deterministic left-corner parsing.</title>
<date>2004</date>
<booktitle>In Proceedings Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>26--33</pages>
<location>Barcelona.</location>
<contexts>
<context position="14749" citStr="Henderson 2004" startWordPosition="2104" endWordPosition="2105">rage of constituent structure with that of ambiguous alternative analyses, but the memory demands of systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for using self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen 2003), but are still limited to parsing relatively short travel planning queries with limited syntactic complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up constituents, and use connectionist models for probability estimation over these hypotheses (Henderson 2004) typically achieve better performance in practice. 4 Schuler et al. Parsing Using Human-Like Memory Constraints Previous memory-based explanations of problematic sentences (explaining garden path effects as exceeding a bound of four complete but unattached constituents, or explaining center embedding difficulties as exceeding a bound of seven active or awaited constituents) have been shown to underestimate human sentence processing capacity when equally complex but unproblematic sentences were examined. The hypothesis advanced in this article, that human sentence processing uses general-purpos</context>
<context position="32128" citStr="Henderson 2004" startWordPosition="4736" endWordPosition="4737">uent can be stored in each short-term memory element. Experimental results described in Section 5 suggest that a vast majority of English sentences can be recognized within these human-like memory bounds, even with this strict condition on chunking. If parsing can be performed in bounded memory under such strict conditions, it can reasonably be assumed to operate at least as well under more permissive circumstances, where some amount of syntactically-unrelated referential chunking is allowed. Several existing incremental systems are organized around a left-corner parsing strategy (Roark 2001; Henderson 2004). But these systems generally keep large numbers of constituents open for modifier attachment in each hypothesis. This allows modifiers to be attached as right children of any such open constituent. But if any number of open constituents are allowed, then either the assumption that stored elements have fixed syntactic (and semantic) internal structure will be violated, or the assumption that syntax operates within a bounded memory store will be violated, both of which are psycholinguistically attractive as simplifying assumptions. The HHMM model described in this article upholds both the fixed</context>
<context position="63882" citStr="Henderson (2004)" startWordPosition="9638" endWordPosition="9639"> Cocke-Kasami-Younger (CKY) parser and bounded-memory right-corner HHMM parser were evaluated on the standard Penn Treebank WSJ Section 23 parsing task, using the binarized tree set described in Section 5.2 (WSJ Sections 2–21) as training data. Training examples requiring more 12 It is important to note that neither the right-corner nor left-corner parsing strategy by itself creates this ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as that of Henderson (2004) adopt an arc-standard strategy, essentially always choosing analysis (b), and thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in short-term memory (Resnik 1992). 22 Schuler et al. Parsing Using Human-Like Memory Constraints than four stack elements were excluded from training, in order to avoid generating inconsistent model probabilities (e.g., from expansions that could not be re-composed within the bounded memory store). Most likely sequences</context>
<context position="72532" citStr="Henderson 2004" startWordPosition="10944" endWordPosition="10945">Again the performance is comparable under similar conditions (Table 3, bottom). This system was run with a beam width of 2,000 hypotheses. This beam width was selected in order to compare the performance of the bounded-memory model, which predicts in-element or cross-element composition, with that of conventional broad-coverage parsers, which also maintain large beams. With better modeling and vastly more data from which to learn, it is possible that the human processor may need to maintain far fewer alternative analyses, or perhaps only one, conditioned on a lookahead window of observations (Henderson 2004).15 These experiments used a maximum stack depth of four, and conditioned expansion and transition probabilities for each qdt on only the portion of the parent category following the slash (that is, only A2 of A1/A2), in order to avoid sparse data effects. Examples requiring more than four stack elements were excluded from training. This is because in the basic relative frequency estimation used here, training examples are depth-specific. Because the (unpunctuated) training set contains only about a dozen sentences requiring more than four depth levels, each occupying that level for only a few</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>Henderson, James. 2004. Lookahead in deterministic left-corner parsing. In Proceedings Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 26–33, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Douglas E Appelt</author>
<author>John Bear</author>
<author>David Israel</author>
<author>Megumi Kameyama</author>
<author>Mark Stickel</author>
<author>Mabry Tyson</author>
</authors>
<title>Fastus: A cascaded finite-state transducer for extracting information from naturallanguage text.</title>
<date>1996</date>
<booktitle>Finite State Devices for Natural Language Processing.</booktitle>
<pages>383--406</pages>
<editor>In Yves Schabes, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="28616" citStr="Hobbs et al. 1996" startWordPosition="4218" endWordPosition="4221"> phrase lacking an awaited noun phrase yet to come).7 These chunked incomplete constituent categories A and B are naturally related through fixed contiguous phrase structure between them, established during the course of parsing prior to the beginning of B, and these incomplete constituents can be composed with other incomplete constituents B/C to form similarly related category pairs A/C. These chunks are not only contiguous sections of phrase structure trees, they also have contiguous string yields, so they correspond to the familiar notion of text chunks used in shallow parsing approaches (Hobbs et al. 1996). For example, a hypothesized memory store may contain incomplete constituents S/NP (a sentence without a noun phrase), followed by NP/NN (a noun phrase lacking a common noun), with corresponding string yields demand for bonds propped up and the municipal, respectively, forming a complete contiguous segmentation of a sentence at any point in processing. Although these two chunks could be composed into an incomplete constituent S/NN, doing so at this point would close off the possibility of introducing another constituent between these two, containing the recognized noun phrase as a left child </context>
<context position="46714" citStr="Hobbs et al. 1996" startWordPosition="7036" endWordPosition="7039">n some other grammar (such as in this case the Treebank grammar), rather than a constraint on grammar itself. 10 In fact, one of the original motivations for CCG as a model of language was to minimize stack usage in incremental processing (Ades and Steedman 1982). 16 Schuler et al. Parsing Using Human-Like Memory Constraints 4.6 Comparison with Cascaded FSAs in Information Extraction Hierarchies of weighted finite-state automata (FSA)–equivalent HMMs may also be viewed as probabilistic implementations of cascaded FSAs, used for modeling syntax in information extraction systems such as FASTUS (Hobbs et al. 1996). Indeed, the leftbranching sequences of transformed constituents recognized by this model (as shown in Figure 3) bear a strong resemblance to the flattened phrase structure representations recognized by cascaded FSA systems, in that most phrases are consolidated to flat sequences at one hierarchy level. This flat structure is desirable in cascaded FSA systems because it allows information to be extracted from noun or verb phrases using straightforward pattern matching rules, implemented as FSA-equivalent regular expressions. Like FASTUS, this system produces layers of flat phrases that can be</context>
</contexts>
<marker>Hobbs, Appelt, Bear, Israel, Kameyama, Stickel, Tyson, 1996</marker>
<rawString>Hobbs, Jerry R., Douglas E. Appelt, John Bear, David Israel, Megumi Kameyama, Mark Stickel, and Mabry Tyson. 1996. Fastus: A cascaded finite-state transducer for extracting information from naturallanguage text. In Yves Schabes, editor, Finite State Devices for Natural Language Processing. MIT Press, Cambridge, MA, pages 383–406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert J Jarvella</author>
</authors>
<title>Syntactic processing of connected speech.</title>
<date>1971</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<pages>10--409</pages>
<contexts>
<context position="1603" citStr="Jarvella 1971" startWordPosition="226" endWordPosition="227">ences, and the accuracy of a bounded-memory parsing strategy based on this model, suggest this model may be cognitively plausible. 1. Introduction It is an interesting possibility that human syntactic processing may occur entirely within a general-purpose short-term memory. Like other short-term memory processes, syntactic processing is susceptible to degradation if short-term memory capacity is loaded, for example, when readers are asked to retain lists of words while reading (Just and Carpenter 1992); and memory of words and syntax degrades over time within and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse information about referents from other sentences (Ericsson and Kintsch 1995). But short-term memory is known to have severe capacity limitations of perhaps no more than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem * Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455. E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu. ** Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street, Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-</context>
<context position="74375" citStr="Jarvella 1971" startWordPosition="11226" endWordPosition="11227">re can be mapped to and from conventional phrase structure trees using a reversible right-corner transform. If this syntactic processing model is combined with a bounded-memory interpreter (Schuler, Wu, and Schwartz 2009), however, allowing the contents of this store to be incrementally interpreted within the same bounded memory, it stands to reason that complete, explicit phrase structure trees would not need to be constructed at any time in processing, in keeping with experimental results showing similar lack of retention of words and syntactic structure during human processing (Sachs 1967; Jarvella 1971). Initial results show the use of a memory store consisting of only three to four memory elements within this framework provides nearly complete coverage of the Penn Treebank Switchboard and WSJ corpora, consistent with recent estimates of generalpurpose short-term memory capacity. This suggests that, unlike some earlier models, the hypothesis that human sentence processing uses general-purpose short-term 15 Although, if most competing analyses are unconscious, they would be difficult to detect. Formally, the competing pockets of activation hypothesized in a parallel-processing version of this</context>
</contexts>
<marker>Jarvella, 1971</marker>
<rawString>Jarvella, Robert J. 1971. Syntactic processing of connected speech. Journal of Verbal Learning and Verbal Behavior, 10:409–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Lalit R Bahl</author>
<author>Robert L Mercer</author>
</authors>
<title>Design of a linguistic statistical decoder for the recognition of continuous speech.</title>
<date>1975</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>21--250</pages>
<marker>Jelinek, Bahl, Mercer, 1975</marker>
<rawString>Jelinek, Frederick, Lalit R. Bahl, and Robert L. Mercer. 1975. Design of a linguistic statistical decoder for the recognition of continuous speech. IEEE Transactions on Information Theory, 21:250–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Finite state approximation of constraint-based grammars using left-corner grammar transforms.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>619--623</pages>
<location>Montreal.</location>
<contexts>
<context position="2770" citStr="Johnson (1998" startWordPosition="386" endWordPosition="387">Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg. Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for publication: 27 May 2009. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 too austere to process the rich tree-like phrase structure commonly invoked to explain word-order regularities in natural language. This article aims to show that they are not. The article describes a comprehension model, based on a right-corner transform—a reversible tree transform related to the left-corner transform of Johnson (1998a)—that associates familiar phrase structure trees with the contents of a memory store of three to four partially completed constituents over time. Coverage results on the large syntactically annotated Penn Treebank corpus show a vast majority of naturally occurring sentences can be recognized using a memory store containing a maximum of only three incomplete constituents, and nearly all sentences can be recognized using four, consistent with estimates of human short-term memory capacity. This transform reduces memory usage in incremental (left to right) processing by transforming right-branch</context>
<context position="26347" citStr="Johnson (1998" startWordPosition="3884" endWordPosition="3885">that higher-level HMMs are allowed to transition only when lower-level HMMs terminate. An HHMM therefore behaves like a probabilistic implementation of a shift–reduce parser or pushdown automaton with a bounded stack, where the maximum stack depth is equal to the number of depth levels in the HHMM hierarchy. 4. Right-Corner Transform and Incomplete Constituents The model described in this article recognizes trees in a right-corner transformed representation to minimize usage of a bounded short-term memory store. This rightcorner transform is a variant of the left-corner transform described by Johnson (1998a), but whereas the left-corner transform changes left-branching structure into rightbranching structure, the right-corner transform changes right-branching structure into 6 Here [·] is an indicator function: [φ] = 1 if φ is true, 0 otherwise. 8 Schuler et al. Parsing Using Human-Like Memory Constraints left-branching structure. Recognition using this transformed grammar, extracted from a transformed corpus, is similar to recognition using a left-corner parsing strategy (Aho and Ullman 1972). This kind of strategy was shown to reduce memory requirements for parsing sentences with mainly left- </context>
<context position="48453" citStr="Johnson (1998" startWordPosition="7284" endWordPosition="7285">the Wall Street Journal (Marcus, Santorini, and Marcinkiewicz 1993). These sentences were right-corner transformed and mapped to a time-aligned bounded memory store as described in Section 4 to determine the amount of memory each sentence would require. 5.1 Binary Branching Structure In order to obtain a linguistically plausible right-corner transform representation of incomplete constituents, the corpus is subjected to another pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of right-corner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. First, ordinary phrases and clauses are decomposed into head projections, each consisting of one subordinate head projection and one argument or modifier, for example: A0 A0 ... VB:α1 NP:α2 ... ⇒ ... VB . . . VB:α1 NP:α2 The selection of head constituents is done using rewrite rules similar to the Magerm</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Johnson, Mark. 1998a. Finite state approximation of constraint-based grammars using left-corner grammar transforms. In Proceedings of COLING/ACL, pages 619–623, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--613</pages>
<contexts>
<context position="2770" citStr="Johnson (1998" startWordPosition="386" endWordPosition="387">Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg. Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for publication: 27 May 2009. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 too austere to process the rich tree-like phrase structure commonly invoked to explain word-order regularities in natural language. This article aims to show that they are not. The article describes a comprehension model, based on a right-corner transform—a reversible tree transform related to the left-corner transform of Johnson (1998a)—that associates familiar phrase structure trees with the contents of a memory store of three to four partially completed constituents over time. Coverage results on the large syntactically annotated Penn Treebank corpus show a vast majority of naturally occurring sentences can be recognized using a memory store containing a maximum of only three incomplete constituents, and nearly all sentences can be recognized using four, consistent with estimates of human short-term memory capacity. This transform reduces memory usage in incremental (left to right) processing by transforming right-branch</context>
<context position="26347" citStr="Johnson (1998" startWordPosition="3884" endWordPosition="3885">that higher-level HMMs are allowed to transition only when lower-level HMMs terminate. An HHMM therefore behaves like a probabilistic implementation of a shift–reduce parser or pushdown automaton with a bounded stack, where the maximum stack depth is equal to the number of depth levels in the HHMM hierarchy. 4. Right-Corner Transform and Incomplete Constituents The model described in this article recognizes trees in a right-corner transformed representation to minimize usage of a bounded short-term memory store. This rightcorner transform is a variant of the left-corner transform described by Johnson (1998a), but whereas the left-corner transform changes left-branching structure into rightbranching structure, the right-corner transform changes right-branching structure into 6 Here [·] is an indicator function: [φ] = 1 if φ is true, 0 otherwise. 8 Schuler et al. Parsing Using Human-Like Memory Constraints left-branching structure. Recognition using this transformed grammar, extracted from a transformed corpus, is similar to recognition using a left-corner parsing strategy (Aho and Ullman 1972). This kind of strategy was shown to reduce memory requirements for parsing sentences with mainly left- </context>
<context position="48453" citStr="Johnson (1998" startWordPosition="7284" endWordPosition="7285">the Wall Street Journal (Marcus, Santorini, and Marcinkiewicz 1993). These sentences were right-corner transformed and mapped to a time-aligned bounded memory store as described in Section 4 to determine the amount of memory each sentence would require. 5.1 Binary Branching Structure In order to obtain a linguistically plausible right-corner transform representation of incomplete constituents, the corpus is subjected to another pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of right-corner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. First, ordinary phrases and clauses are decomposed into head projections, each consisting of one subordinate head projection and one argument or modifier, for example: A0 A0 ... VB:α1 NP:α2 ... ⇒ ... VB . . . VB:α1 NP:α2 The selection of head constituents is done using rewrite rules similar to the Magerm</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Johnson, Mark. 1998b. PCFG models of linguistic tree representation. Computational Linguistics, 24:613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P N Johnson-Laird</author>
</authors>
<title>Mental Models: Towards a Cognitive Science of Language, Inference and Consciousness.</title>
<date>1983</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7421" citStr="Johnson-Laird (1983)" startWordPosition="1070" endWordPosition="1071"> argument structure, whereas the model described herein is exclusively a performance model, allowing generalizations about lexical argument structures to be learned in some other representation, then combined with probabilistic information about parsing strategies to yield a set of derived incomplete constituents. As a result, the model described in this article has a freer hand to satisfy strict working memory bounds, which may not permit some of the alternative composition operations proposed in the CCG account, thought to be associated with available prosody and quantifier scope analyses.1 Johnson-Laird (1983) and Abney and Johnson (1991) propose a pure processing account of memory capacity limits in parsing ordinary phrase structure trees. The Johnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, of which the right-corner transform introduced in this article is a variant, in order to bring memory usage for most parsable sentences to within seven or so active or awaited phrase structure constituents. This account may be used to explain human processing difficulties in processing triply center-embedded sentences like the rat that the cat that the dog chased killed ate the </context>
</contexts>
<marker>Johnson-Laird, 1983</marker>
<rawString>Johnson-Laird, P. N. 1983. Mental Models: Towards a Cognitive Science of Language, Inference and Consciousness. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Adam Just</author>
<author>Patricia A Carpenter</author>
</authors>
<title>A capacity theory of comprehension: Individual differences in working memory.</title>
<date>1992</date>
<journal>Psychological Review,</journal>
<pages>99--122</pages>
<contexts>
<context position="1496" citStr="Just and Carpenter 1992" startWordPosition="207" endWordPosition="210"> store over time. Evaluations of the coverage of this model on a large syntactically annotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategy based on this model, suggest this model may be cognitively plausible. 1. Introduction It is an interesting possibility that human syntactic processing may occur entirely within a general-purpose short-term memory. Like other short-term memory processes, syntactic processing is susceptible to degradation if short-term memory capacity is loaded, for example, when readers are asked to retain lists of words while reading (Just and Carpenter 1992); and memory of words and syntax degrades over time within and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse information about referents from other sentences (Ericsson and Kintsch 1995). But short-term memory is known to have severe capacity limitations of perhaps no more than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem * Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455. E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu. ** Computer Science Department, Faculty of Computers and</context>
<context position="8558" citStr="Just and Carpenter 1992" startWordPosition="1241" endWordPosition="1244">center-embedded sentences like the rat that the cat that the dog chased killed ate the malt, with intended interpretation SNP the rat that SNP the cat that SNP the dog] chased] killed] ate the malt (Chomsky and Miller 1963). But this explanation does not account for examples of triply center-embedded sentences that typically do not cause processing problems: SNP that SNP the food that SNP John] ordered] tasted good] pleased him (Gibson 1991). Moreover, the apparent competition between comprehension of center-embedded object relatives and retention of unrelated words in general-purpose memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at least, can be) used to store incomplete constituents during comprehension. This would predict three or four elements of reliable storage, rather than seven (Cowan 2001). The transform-based model described in this article exploits a conception of chunking (Miller 1956) to combine pairs of active and awaited constituents from the Abney and Johnson analysis, connected by recognized structure, in order to operate within estimates of human short-term memory bounds. Because of these counterexamples to the memory-exhaustion explanation of garden path a</context>
</contexts>
<marker>Just, Carpenter, 1992</marker>
<rawString>Just, Marcel Adam and Patricia A. Carpenter. 1992. A capacity theory of comprehension: Individual differences in working memory. Psychological Review, 99:122–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Adam Just</author>
<author>Sashank Varma</author>
</authors>
<title>The organization of thinking: What functional brain imaging reveals about the neuroarchitecture of complex cognition.</title>
<date>2007</date>
<journal>Cognitive, Affective, &amp; Behavioral Neuroscience,</journal>
<pages>7--153</pages>
<contexts>
<context position="10632" citStr="Just and Varma 2007" startWordPosition="1539" endWordPosition="1542"> of support for some of these available scope analyses may not necessarily be problematic for the present model. The complexity of interpreting nested raised quantifiers may place them beyond the capability of human interactive incremental interpretation, but not beyond the capability of post hoc interpretation (understood after the listener has had time to think about it). 3 Computational Linguistics Volume 36, Number 1 compatible with brain imaging evidence of increased cortical activity and recruitment of auxiliary brain areas during periods of increased uncertainty in sentence processing (Just and Varma 2007). But if interference or changing activation is posited as the source of processing difficulty, and delays are not linked to memory exhaustion per se, then these theories do not explain how (or whether) syntactic processing operates within general-purpose short-term memory. Toward this end, this article will specifically evaluate the claim that syntactic processing can be performed entirely within general-purpose short-term memory by using this memory to store unassimilated incomplete syntactic constituents, derived through a right-corner transform from basic properties of phrase structure tre</context>
</contexts>
<marker>Just, Varma, 2007</marker>
<rawString>Just, Marcel Adam and Sashank Varma. 2007. The organization of thinking: What functional brain imaging reveals about the neuroarchitecture of complex cognition. Cognitive, Affective, &amp; Behavioral Neuroscience, 7:153–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuki Kamide</author>
<author>Don C Mitchell</author>
</authors>
<title>Incremental pre-head attachment</title>
<date>1999</date>
<booktitle>in Japanese parsing. Language and Cognitive Processes,</booktitle>
<pages>14--631</pages>
<contexts>
<context position="34891" citStr="Kamide and Mitchell 1999" startWordPosition="5138" endWordPosition="5141">d in parsing only at the point in incremental recognition at which they can be associated with a head word (Gibson 1991: Pritcher 1991: Gorrell 1995). In typically head-initial languages such as English, incomplete constituents derived from these head-driven models resemble those derived from a right-corner transform. But headdriven incomplete constituents do not appear to obey general-purpose memory bounds in head-final languages such as Japanese, and do not appear to obey attachment prefer10 Schuler et al. Parsing Using Human-Like Memory Constraints ences predicted by a head-driven account (Kamide and Mitchell 1999), favoring a prehead attachment account, as a right-corner transform would predict. 4.1 Tree Transforms Using Rewrite Rules The incomplete constituents used in the present model are defined in terms of tree transforms, which consist of recursive operations that change tree structures into other tree structures. These transforms are not cognitive processes—syntax in this model is learned and used entirely as time-series probabilities over random variable values in the memory store. The role of these transforms is as a means to associate sequences of configurations of incomplete constituents in </context>
</contexts>
<marker>Kamide, Mitchell, 1999</marker>
<rawString>Kamide, Yuki and Don C. Mitchell. 1999. Incremental pre-head attachment in Japanese parsing. Language and Cognitive Processes, 14:631–662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo.</location>
<contexts>
<context position="48484" citStr="Klein and Manning (2003)" startWordPosition="7287" endWordPosition="7290">nal (Marcus, Santorini, and Marcinkiewicz 1993). These sentences were right-corner transformed and mapped to a time-aligned bounded memory store as described in Section 4 to determine the amount of memory each sentence would require. 5.1 Binary Branching Structure In order to obtain a linguistically plausible right-corner transform representation of incomplete constituents, the corpus is subjected to another pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of right-corner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. First, ordinary phrases and clauses are decomposed into head projections, each consisting of one subordinate head projection and one argument or modifier, for example: A0 A0 ... VB:α1 NP:α2 ... ⇒ ... VB . . . VB:α1 NP:α2 The selection of head constituents is done using rewrite rules similar to the MagermanBlack head rules (Magerman 19</context>
<context position="66590" citStr="Klein and Manning (2003)" startWordPosition="10038" endWordPosition="10041">this format.13 Both CKY baseline and HHMM test systems were run with a simple part of speech (POS) model using relative frequency estimates from the training set, backed off to a discriminative (decision tree) model conditioned on the last five letters of each word, normalized over unigram POS probabilities. The CKY baseline and HHMM results were obtained by training and evaluating on binarized trees, which is a necessary condition for the right-corner transform. The CKY baseline results appear to be better than those for a baseline probabilistic context-free grammar (PCFG) system reported by Klein and Manning (2003) using no modifications to the corpus, and no parent or sibling conditioning (see Table 3, top) because the binarization process allows the parser to avoid some sparse data effects due to large flat branching structures in the Treebank, resulting in improved parsing accuracy. Klein and Manning note that applying linguistically motivated binarization transforms can yield substantial improvements in accuracy—as much as nine points, in their study (in comparison, binarization only seems to improve accuracy by about seven points above an unmodified baseline in the present study). But the Klein and</context>
<context position="68664" citStr="Klein and Manning 2003" startWordPosition="10357" endWordPosition="10360"> phrase structure brackets at a particular projection or not is something existing parsers learn and exploit to improve their accuracy. But it is not clear that this distinction is linguistically motivated. 23 Computational Linguistics Volume 36, Number 1 Table 3 Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure (% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on unmodified and binarized WSJ Sections 22 (sentences 1–393: “devset”) and 23–24 (all sentences). Results are shown with and without punctuation, compared to Klein and Manning 2003 (KM’03) using baseline and parent+sibling (par+sib) conditioning, and Roark 2001 (R’01) using parent+sibling conditioning. Baseline CKY and test (parent+sibling) cases for the HHMM system start out at a higher accuracy than for the Klein-Manning system because the HHMM system requires binarization of trees, which removes some data sparsity in the raw Treebank annotation, whereas the Klein-Manning results are computed prior to binarization. Because it is incremental, the parser occasionally eliminates all continuable analyses from the beam, and therefore fails to find a parse. HHMM parse failu</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene H Lerner</author>
</authors>
<title>On the syntax of sentences in progress.</title>
<date>1991</date>
<booktitle>Language in Society,</booktitle>
<pages>20--441</pages>
<contexts>
<context position="23635" citStr="Lerner 1991" startWordPosition="3441" endWordPosition="3442"> be maximized rather than marginalized, but in any case the intermediate variables do not persist. 5 Also, the fact that this is a generative model, in which observations are conditioned on hypotheses, then flipped using Bayes’ Law (Equation 2)—as opposed to a discriminative or conditional model, in which hypotheses are conditioned directly on observations—is also appealing as a human model, in that it allows the same architecture to be used for both recognition and generation. This is a desirable property for modeling split utterances, in which interlocutors complete one another’s sentences (Lerner 1991; Helasvuo 2004). 7 Computational Linguistics Volume 36, Number 1 variable is a reduction or non-reduction state ftd E GU{1, 0} (indicating, respectively, a reduction of incomplete constituent qd t−1 to a complete right child constituent of some grammatical category from domain G, or a non-reduction of qd t−1 as a unary or left child, as defined in Section 4). An intermediate variable f t d at depth d may indicate reduction or non-reduction according to ΘF-Reduction,d if there is a reduction at the depth level immediately below d, but must indicate non-reduction (ftd = 0) with probability 1 if</context>
</contexts>
<marker>Lerner, 1991</marker>
<rawString>Lerner, Gene H. 1991. On the syntax of sentences in progress. Language in Society, 20:441–458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Modeling the effects of memory on human online sentence processing with particle filters.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>937--944</pages>
<location>Vancouver.</location>
<contexts>
<context position="12846" citStr="Levy (2008)" startWordPosition="1843" endWordPosition="1844">unded store of short-term memory as described by Miller (1956); the outer disjunctive allocation treats activation as a continuous resource in which like-valued pockets expand and contract as they are reinforced or contradicted by incoming observations. Indeed, it would be surprising if these two dimensions of resource allocation did not exist: the former, because it would contradict years of observations about the behavior of short-term memory; and the latter, because it would require neural activation spreading to be instantaneous and uniform, contradicting most neuropsychological evidence. Levy (2008) compares the allocation of activation in this kind of framework to the distributed allocation of resources in a particle filter (Gordon, Salmond, and Smith 1993), an approximate inference technique for probabilistic time-series models in which particles in a (typically fixed) reservoir are assigned randomly sampled hypotheses from learned transition probabilities, essentially functioning as units of activation. The model described in this paper qualifies this analogy by positing that each individual particle in this reservoir endorses a coherent hypothesis about the contents of a three- to fo</context>
<context position="75267" citStr="Levy (2008)" startWordPosition="11361" endWordPosition="11362">. This suggests that, unlike some earlier models, the hypothesis that human sentence processing uses general-purpose short-term 15 Although, if most competing analyses are unconscious, they would be difficult to detect. Formally, the competing pockets of activation hypothesized in a parallel-processing version of this model could be arbitrarily small and numerous, but it seems unlikely that very small pockets of activation would persist for very long (just as low probability analyses would be unlikely to remain on the HHMM beam). This possibility is discussed in the particle filter account of Levy (2008). 25 Computational Linguistics Volume 36, Number 1 memory to store incomplete constituents, as defined by a right-corner transform, does not seem to substantially underestimate human processing capacity. Moreover, despite additional predictions that must take place within this model to manage parsing in such close quarters, preliminary accuracy results for an unlexicalized, un-smoothed version of this model, using only a four-element memory store, show close to 84% recall and precision on the standard parsing evaluation. This result is comparable to that reported for state-of-the-art cubic-tim</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Levy, Roger. 2008. Modeling the effects of memory on human online sentence processing with particle filters. In Proceedings of NIPS, pages 937–944, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard L Lewis</author>
<author>Shravan Vasishth</author>
</authors>
<title>An activation-based model of sentence processing as skilled memory retrieval.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="9306" citStr="Lewis and Vasishth (2005)" startWordPosition="1347" endWordPosition="1350">This would predict three or four elements of reliable storage, rather than seven (Cowan 2001). The transform-based model described in this article exploits a conception of chunking (Miller 1956) to combine pairs of active and awaited constituents from the Abney and Johnson analysis, connected by recognized structure, in order to operate within estimates of human short-term memory bounds. Because of these counterexamples to the memory-exhaustion explanation of garden path and center-embedding difficulties, recent work has turned to explanations other than memory exhaustion for these phenomena. Lewis and Vasishth (2005) attribute processing errors to activation interference among stored constituents that have similar syntactic and semantic roles. Hale’s surprisal (2001) and entropic model (2006) link human processing difficulties to significant changes in the relative probability of competing hypotheses in incremental parsing, such that if activation is taken to be a mechanism for probability estimation, processing difficulties may be ascribed to the relatively slow speed of activation change within the brain (or to collapsing activation when probabilities grow too small, as in the case of garden path senten</context>
</contexts>
<marker>Lewis, Vasishth, 2005</marker>
<rawString>Lewis, Richard L. and Shravan Vasishth. 2005. An activation-based model of sentence processing as skilled memory retrieval. Cognitive Science, 29(3):375–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Magerman</author>
</authors>
<title>Statistical decisiontree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL’95),</booktitle>
<pages>276--283</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="49087" citStr="Magerman 1995" startWordPosition="7386" endWordPosition="7387">ning (2003). This binarization is done in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of right-corner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. First, ordinary phrases and clauses are decomposed into head projections, each consisting of one subordinate head projection and one argument or modifier, for example: A0 A0 ... VB:α1 NP:α2 ... ⇒ ... VB . . . VB:α1 NP:α2 The selection of head constituents is done using rewrite rules similar to the MagermanBlack head rules (Magerman 1995). Any new constituent created by this process is 17 Computational Linguistics Volume 36, Number 1 assigned the label of the subordinate head projection. The subordinate projection may be the left or complete list of head-projection rewrite rules is provided in Appendix A.11 Conjunctions are decomposed into purely right-branching structures using nonterminals appended with a “-LIST” suffix: A0 ... A1:α1 CC A1:α2 A0 ⇒ ... A1-LIST A1:α1 CC A1:α2 A0 ... A1:α1 A1-LIST:α2 A0 ⇒ ... A1-LIST A1:α1 A1-LIST:α2 This right-branching decomposition of conjoined lists is motivated by the general preference in</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>Magerman, David. 1995. Statistical decisiontree models for parsing. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL’95), pages 276–283, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
</authors>
<title>Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5030" citStr="Marcus (1980)" startWordPosition="710" endWordPosition="711">m and how it relates conventional phrase structure to incomplete constituents in a bounded memory store; Section 5 describes an experiment to estimate the level of coverage of the Penn Treebank corpus that can be achieved using this transform with various memory limits, given a linguistically motivated binarization of this corpus; and Section 6 gives accuracy results of this bounded-memory model trained on this corpus, given that some amount of incremental prediction (as described earlier) must be involved. 2. Bounded-Memory Parsing One of the earliest bounded-memory parsing models is that of Marcus (1980). This model maintains a bounded store of complete but unattached constituents as a buffer, and operates on them using a variety of specialized memory manipulation operations, deferring certain attachment decisions until the contents of this buffer indicate it is safe to do so. (In contrast, the model described in this article maintains a store of incomplete constituents using ordinary stack-like push and pop operations, defined to allow constituents to be composed before being completely recognized.) The Marcus parser provides a bounded-memory explanation for human difficulties in processing </context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, Mitch. 1980. Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marshall R Mayberry</author>
<author>Risto Miikkulainen</author>
</authors>
<title>Incremental nonmonotonic parsing through semantic self-organization.</title>
<date>2003</date>
<booktitle>In Proceedings of the 25th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>798--803</pages>
<contexts>
<context position="14444" citStr="Mayberry and Miikkulainen 2003" startWordPosition="2064" endWordPosition="2067">is article is also compatible with deterministic parsing frameworks, in which case it models allocation of processing resources among incompletely-recognized constituents within a single non-competing hypothesis. 3 Pure connectionist models of syntactic processing (Elman 1991; Berg 1992; Rohde 2002) attempt to unify storage of constituent structure with that of ambiguous alternative analyses, but the memory demands of systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for using self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen 2003), but are still limited to parsing relatively short travel planning queries with limited syntactic complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up constituents, and use connectionist models for probability estimation over these hypotheses (Henderson 2004) typically achieve better performance in practice. 4 Schuler et al. Parsing Using Human-Like Memory Constraints Previous memory-based explanations of problematic sentences (explaining garden path effects as exceeding a bound of four complete but unattached constituents, or explaining center emb</context>
</contexts>
<marker>Mayberry, Miikkulainen, 2003</marker>
<rawString>Mayberry, III, Marshall R. and Risto Miikkulainen. 2003. Incremental nonmonotonic parsing through semantic self-organization. In Proceedings of the 25th Annual Conference of the Cognitive Science Society, pages 798–803,</rawString>
</citation>
<citation valid="false">
<location>Boston, MA.</location>
<marker></marker>
<rawString>Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>The magical number seven, plus or minus two: Some limits on our capacity for processing information.</title>
<date>1956</date>
<journal>Psychological Review,</journal>
<pages>63--81</pages>
<contexts>
<context position="1849" citStr="Miller 1956" startWordPosition="262" endWordPosition="263">urpose short-term memory. Like other short-term memory processes, syntactic processing is susceptible to degradation if short-term memory capacity is loaded, for example, when readers are asked to retain lists of words while reading (Just and Carpenter 1992); and memory of words and syntax degrades over time within and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse information about referents from other sentences (Ericsson and Kintsch 1995). But short-term memory is known to have severe capacity limitations of perhaps no more than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem * Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455. E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu. ** Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street, Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg. Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for publication: 27 May 2009. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 too austere to p</context>
<context position="8875" citStr="Miller 1956" startWordPosition="1289" endWordPosition="1290"> not cause processing problems: SNP that SNP the food that SNP John] ordered] tasted good] pleased him (Gibson 1991). Moreover, the apparent competition between comprehension of center-embedded object relatives and retention of unrelated words in general-purpose memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at least, can be) used to store incomplete constituents during comprehension. This would predict three or four elements of reliable storage, rather than seven (Cowan 2001). The transform-based model described in this article exploits a conception of chunking (Miller 1956) to combine pairs of active and awaited constituents from the Abney and Johnson analysis, connected by recognized structure, in order to operate within estimates of human short-term memory bounds. Because of these counterexamples to the memory-exhaustion explanation of garden path and center-embedding difficulties, recent work has turned to explanations other than memory exhaustion for these phenomena. Lewis and Vasishth (2005) attribute processing errors to activation interference among stored constituents that have similar syntactic and semantic roles. Hale’s surprisal (2001) and entropic mo</context>
<context position="12297" citStr="Miller (1956)" startWordPosition="1766" endWordPosition="1767">el, whereas the framework described here can be taken to model the allocation of processing resources (in this case, memory elements) among conjunctions of incompletely recognized constituents within each competing hypothesis.2 Thus, in this view, there are two ways to simultaneously activate multiple concepts: disjunctively (sharing activation among competing hypotheses) and conjunctively (sharing activation among unassimilated constituents within a hypothesis). But only the inner conjunctive allocation corresponds to the familiar discretely bounded store of short-term memory as described by Miller (1956); the outer disjunctive allocation treats activation as a continuous resource in which like-valued pockets expand and contract as they are reinforced or contradicted by incoming observations. Indeed, it would be surprising if these two dimensions of resource allocation did not exist: the former, because it would contradict years of observations about the behavior of short-term memory; and the latter, because it would require neural activation spreading to be instantaneous and uniform, contradicting most neuropsychological evidence. Levy (2008) compares the allocation of activation in this kind</context>
<context position="27622" citStr="Miller 1956" startWordPosition="4071" endWordPosition="4072">or awaited constituent categories (Abney and Johnson 1991). This is within Miller’s (1956) estimate of human short-term memory capacity (if memory elements store individual categories), whereas parsing heavily center-embedded sentences (known to be difficult for human readers) would require seven or more elements at the frontier of this capacity. But recent research suggests that human memory capacity may be limited to as few as three or four distinct items (Cowan 2001), with longer estimates of seven or more possibly due to the human capacity to chunk remembered items into associated groups (Miller 1956). The right-corner strategy described in this paper therefore assumes constituent categories can similarly be chunked into incomplete constituents A/B formed by pairing an active category A with an awaited category B somewhere along the active category’s right progeny (so, for example, a transitive verb may become an incomplete constituent VP/NP consisting of an active verb phrase lacking an awaited noun phrase yet to come).7 These chunked incomplete constituent categories A and B are naturally related through fixed contiguous phrase structure between them, established during the course of par</context>
</contexts>
<marker>Miller, 1956</marker>
<rawString>Miller, George A. 1956. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 63:81–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
<author>Mark A Paskin</author>
</authors>
<title>Linear time inference in hierarchical HMMs.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>833--840</pages>
<location>Vancouver.</location>
<contexts>
<context position="3920" citStr="Murphy and Paskin 2001" startWordPosition="547" endWordPosition="550">ge in incremental (left to right) processing by transforming right-branching constituent structures into left-branching structures, allowing child constituents to be composed with parent constituents before either have been completely recognized. But because this composition identifies an incomplete child as the awaited portion of an incomplete parent, it implicitly predicts that this child constituent will be the rightmost (i.e., last) child of the parent, before this child has been completely recognized. Parsing accuracy results on the Penn Treebank using a Hierarchical Hidden Markov Model (Murphy and Paskin 2001)—essentially a probabilistic pushdown automaton with a bounded pushdown store—show that this prediction can be reliably learned from training data. The remainder of this article is organized as follows: Section 2 describes some related models of human syntactic processing using a bounded memory store; Section 3 describes a Hierarchical Hidden Markov Model (HHMM) framework for statistical parsing using this bounded store of incomplete constituents; Section 4 describes the right-corner transform and how it relates conventional phrase structure to incomplete constituents in a bounded memory store</context>
<context position="17927" citStr="Murphy and Paskin 2001" startWordPosition="2563" endWordPosition="2566">emory store, and some are transient as results of hypothesized compositions, which are estimated and immediately discarded or folded into the persistent store according to the dependencies in the model. The variables have values or contents (or fillers)—in this case incomplete constituent categories—that change over time, and although these values may be uncertain, the set of hypothesized contents of this memory store at any given point in time are collectively constrained to form a coherent (but possibly incomplete) syntactic analysis of a sentence. The particular model used here is an HHMM (Murphy and Paskin 2001), which mimics a bounded-memory pushdown automaton (PDA), supporting simple push and pop operations on a bounded stack-like memory store. A time-series model is used here instead of an explicit stack machine, first because the probability model is well defined on a bounded memory store, and second because the plasticity of the random variables that mimic stack behavior in this model makes the model cross-linguistically attractive. By evoking additional random variables and dependencies, the model can be defined (or presumably, trained) to mimic other types of automata, such as extended pushdow</context>
</contexts>
<marker>Murphy, Paskin, 2001</marker>
<rawString>Murphy, Kevin P. and Mark A. Paskin. 2001. Linear time inference in hierarchical HMMs. In Proceedings of NIPS, pages 833–840, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar. Chicago: University of Chicago Press and Stanford:</title>
<date>1994</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="50663" citStr="Pollard and Sag 1994" startWordPosition="7630" endWordPosition="7633">eas the words coffee, tea do not.) Empty constituents are removed outright, along with any unary projections that may arise from this removal. In the case of empty constituents representing traces, the extracted category label is annotated onto the lowest nonterminal dominating the trace using the suffix “-extrX,” where “X” is the category of the extracted constituent. To preserve grammaticality, this annotation is then passed up the tree and eliminated when a wh-, topicalized, or other moved constituent is encountered, in a manner similar to that used in Head-driven Phrase Structure Grammar (Pollard and Sag 1994), but this does not affect branching structure. Together these rewrites remove about 65% of super-binary branches from the unprocessed Treebank. All remaining super-binary branches are “nominally” decomposed into right-branching structures by introducing intermediate nodes, each with a label concatenated from the labels of its children, delimited by underscores: A0 A0 ⇒ ... A1 A2 ... A1:α1 A2:α2 A1:α1 A2:α2 This decomposition is “nominal” in that the concatenated labels leave the resulting binary branches just as complex as the original n-ary branches prior to this decomposition. It is equival</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. Chicago: University of Chicago Press and Stanford: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley L Pritchett</author>
</authors>
<title>Head position and parsing ambiguity.</title>
<date>1991</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>20--251</pages>
<marker>Pritchett, 1991</marker>
<rawString>Pritchett, Bradley L. 1991. Head position and parsing ambiguity. Journal of Psycholinguistic Research, 20:251–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Left-corner parsing and psychological plausibility.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>191--197</pages>
<location>Nantes.</location>
<contexts>
<context position="64194" citStr="Resnik 1992" startWordPosition="9687" endWordPosition="9688">r the right-corner nor left-corner parsing strategy by itself creates this ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as that of Henderson (2004) adopt an arc-standard strategy, essentially always choosing analysis (b), and thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in short-term memory (Resnik 1992). 22 Schuler et al. Parsing Using Human-Like Memory Constraints than four stack elements were excluded from training, in order to avoid generating inconsistent model probabilities (e.g., from expansions that could not be re-composed within the bounded memory store). Most likely sequences of HHMM stack configurations are evaluated by reversing the binarization, right-corner, and time-series mapping transforms described in Sections 4 and 5. But some of the binarization rewrites cannot be completely reversed, because they cannot be unambiguously matched to output trees. Automatically derived lexi</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Resnik, Philip. 1992. Left-corner parsing and psychological plausibility. In Proceedings of COLING, pages 191–197, Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="32111" citStr="Roark 2001" startWordPosition="4734" endWordPosition="4735">lete constituent can be stored in each short-term memory element. Experimental results described in Section 5 suggest that a vast majority of English sentences can be recognized within these human-like memory bounds, even with this strict condition on chunking. If parsing can be performed in bounded memory under such strict conditions, it can reasonably be assumed to operate at least as well under more permissive circumstances, where some amount of syntactically-unrelated referential chunking is allowed. Several existing incremental systems are organized around a left-corner parsing strategy (Roark 2001; Henderson 2004). But these systems generally keep large numbers of constituents open for modifier attachment in each hypothesis. This allows modifiers to be attached as right children of any such open constituent. But if any number of open constituents are allowed, then either the assumption that stored elements have fixed syntactic (and semantic) internal structure will be violated, or the assumption that syntax operates within a bounded memory store will be violated, both of which are psycholinguistically attractive as simplifying assumptions. The HHMM model described in this article uphol</context>
<context position="68745" citStr="Roark 2001" startWordPosition="10369" endWordPosition="10370">arn and exploit to improve their accuracy. But it is not clear that this distinction is linguistically motivated. 23 Computational Linguistics Volume 36, Number 1 Table 3 Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure (% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on unmodified and binarized WSJ Sections 22 (sentences 1–393: “devset”) and 23–24 (all sentences). Results are shown with and without punctuation, compared to Klein and Manning 2003 (KM’03) using baseline and parent+sibling (par+sib) conditioning, and Roark 2001 (R’01) using parent+sibling conditioning. Baseline CKY and test (parent+sibling) cases for the HHMM system start out at a higher accuracy than for the Klein-Manning system because the HHMM system requires binarization of trees, which removes some data sparsity in the raw Treebank annotation, whereas the Klein-Manning results are computed prior to binarization. Because it is incremental, the parser occasionally eliminates all continuable analyses from the beam, and therefore fails to find a parse. HHMM parse failures are accounted as zeros in the recall statistics, but are also listed separate</context>
<context position="71746" citStr="Roark (2001)" startWordPosition="10827" endWordPosition="10828">formanceoptimizing effects that are latent in the binarized PCFG, but are brought out in HHMM parsing. Results on Section 23 of this corpus show close to 84% recall and precision, comparable to that reported for state-of-the-art cubic-time parsers (with no constant bounds 14 Without punctuation, the HHMM model has 50,429 free parameters (including both Q and F models), whereas the binarized PCFG has 12,373. 24 Schuler et al. Parsing Using Human-Like Memory Constraints on processing storage) using similar configurations of conditioning information, that is, without lexicalization or smoothing. Roark (2001) describes a similar incremental parser based on left-corner transformed grammars, and also reports results for parsing with and without parent and sibling Markovization. Again the performance is comparable under similar conditions (Table 3, bottom). This system was run with a beam width of 2,000 hypotheses. This beam width was selected in order to compare the performance of the bounded-memory model, which predicts in-element or cross-element composition, with that of conventional broad-coverage parsers, which also maintain large beams. With better modeling and vastly more data from which to l</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Roark, Brian. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L T Rohde</author>
</authors>
<title>A connectionist model of sentence comprehension and production.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, Carnegie Mellon University.</institution>
<contexts>
<context position="14113" citStr="Rohde 2002" startWordPosition="2018" endWordPosition="2019">bout an entire unbounded phrase structure tree.3 2 Probability distributions in entropy-based models like Hale’s are typically assumed to be defined over sets of hypotheses pursued in parallel, but other interpretations (for example, lookahead-based deterministic models) are possible. The model described in this article is also compatible with deterministic parsing frameworks, in which case it models allocation of processing resources among incompletely-recognized constituents within a single non-competing hypothesis. 3 Pure connectionist models of syntactic processing (Elman 1991; Berg 1992; Rohde 2002) attempt to unify storage of constituent structure with that of ambiguous alternative analyses, but the memory demands of systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for using self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen 2003), but are still limited to parsing relatively short travel planning queries with limited syntactic complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up constituents, and use connectionist models for probability estimation ov</context>
</contexts>
<marker>Rohde, 2002</marker>
<rawString>Rohde, Douglas L. T. 2002. A connectionist model of sentence comprehension and production. Ph.D. thesis, Computer Science Department, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacqueline Sachs</author>
</authors>
<title>Recognition memory for syntactic and semantic aspects of connected discourse. Perception and Psychophysics,</title>
<date>1967</date>
<pages>2--437</pages>
<contexts>
<context position="1587" citStr="Sachs 1967" startWordPosition="224" endWordPosition="225">English sentences, and the accuracy of a bounded-memory parsing strategy based on this model, suggest this model may be cognitively plausible. 1. Introduction It is an interesting possibility that human syntactic processing may occur entirely within a general-purpose short-term memory. Like other short-term memory processes, syntactic processing is susceptible to degradation if short-term memory capacity is loaded, for example, when readers are asked to retain lists of words while reading (Just and Carpenter 1992); and memory of words and syntax degrades over time within and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse information about referents from other sentences (Ericsson and Kintsch 1995). But short-term memory is known to have severe capacity limitations of perhaps no more than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem * Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455. E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu. ** Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street, Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.</context>
<context position="74359" citStr="Sachs 1967" startWordPosition="11224" endWordPosition="11225">s memory store can be mapped to and from conventional phrase structure trees using a reversible right-corner transform. If this syntactic processing model is combined with a bounded-memory interpreter (Schuler, Wu, and Schwartz 2009), however, allowing the contents of this store to be incrementally interpreted within the same bounded memory, it stands to reason that complete, explicit phrase structure trees would not need to be constructed at any time in processing, in keeping with experimental results showing similar lack of retention of words and syntactic structure during human processing (Sachs 1967; Jarvella 1971). Initial results show the use of a memory store consisting of only three to four memory elements within this framework provides nearly complete coverage of the Penn Treebank Switchboard and WSJ corpora, consistent with recent estimates of generalpurpose short-term memory capacity. This suggests that, unlike some earlier models, the hypothesis that human sentence processing uses general-purpose short-term 15 Although, if most competing analyses are unconscious, they would be difficult to detect. Formally, the competing pockets of activation hypothesized in a parallel-processing</context>
</contexts>
<marker>Sachs, 1967</marker>
<rawString>Sachs, Jacqueline. 1967. Recognition memory for syntactic and semantic aspects of connected discourse. Perception and Psychophysics, 2:437–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
</authors>
<title>Parsing with a bounded stack using a model-based right-corner transform.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics (NAACL ’09),</booktitle>
<pages>344--352</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="78042" citStr="Schuler 2009" startWordPosition="11743" endWordPosition="11744">rn parsers are lexicalized, incorporating headword dependencies into parsing decisions, and employing finely tuned smoothing and backoff techniques to integrate these potentially sparse head-word dependencies with denser unlexicalized models. The bounded-memory right-corner HHMM described in this article can also be lexicalized in this way, but because head word dependencies are most straightforwardly defined in terms of top-down PCFG-like dependency structures, this lexicalization requires the introduction of additional formal machinery to transform PCFG probabilities into right-corner form (Schuler 2009). In other words, rather than transforming a training set of trees and mapping them to a time series model, it is necessary to transform a consistent probabilistically weighted grammar (in some sense, an infinite set of trees) into appropriately weighted and consistent right-corner PCFG and HHMM models. This requires the introduction of an approximate inference algorithm, similar to that used in value iteration (Bellman 1957), which estimates probabilities of infinite left-recursive or right-recursive chains by exploiting the fact that increasingly longer chains of events contribute exponentia</context>
</contexts>
<marker>Schuler, 2009</marker>
<rawString>Schuler, William. 2009. Parsing with a bounded stack using a model-based right-corner transform. In Proceedings of the North American Association for Computational Linguistics (NAACL ’09), pages 344–352, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Stephen Wu</author>
<author>Lane Schwartz</author>
</authors>
<title>A framework for fast incremental interpretation during speech decoding.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<marker>Schuler, Wu, Schwartz, 2009</marker>
<rawString>Schuler, William, Stephen Wu, and Lane Schwartz. 2009. A framework for fast incremental interpretation during speech decoding. Computational Linguistics, 35(3):313–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>Evidence against the context-freeness of natural language. Linguistics and Philosophy,</title>
<date>1985</date>
<pages>8--333</pages>
<marker>Shieber, 1985</marker>
<rawString>Shieber, Stuart. 1985. Evidence against the context-freeness of natural language. Linguistics and Philosophy, 8:333–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Smolensky</author>
<author>G Legendre</author>
</authors>
<title>The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar.</title>
<date>2006</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Smolensky, Legendre, 2006</marker>
<rawString>Smolensky, P. and G. Legendre. 2006. The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press/Bradford Books,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="44770" citStr="Steedman 2000" startWordPosition="6753" endWordPosition="6754">s in a bounded memory store corresponds to a bottom-up traversal of the tree. If referential semantics are assumed to be calculated in tandem (as suggested by the Tanenhaus et al. [1995] results), a top-down traversal through time requires some effort to reconcile with the traditional compositional semantic notion that the meanings of constituents are composed from the meanings of their parts (Frege 1892). 4.5 Comparison with CCG The incomplete constituent categories generated in the right-corner transform have the same form and much of the same meaning as non-constituent categories in a CCG (Steedman 2000).10 Both CCG operations of forward function application: A1 - A1/A2 A2 and forward function composition: A1/A3 - A1/A2 A2/A3 appear in the branching structure of right-corner transformed trees. Nested operations can also occur in CCG derivations: A1/A2 - A1/A2/A3 A3 as well as in right-corner transformed trees (using underscore delimiters to denote sequences of constituent categories, described in Section 5.1): A1/A2 - A1/A3 A2 A3 There are also correlates of type-raising (unary branches introduced by the right-corner transform operations described in Section 4): A1/A2 - A3 But, importantly, t</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Steedman, Mark. 2000. The Syntactic Process. MIT Press/Bradford Books, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Stevenson</author>
</authors>
<title>Parsing as incremental restructuring.</title>
<date>1998</date>
<booktitle>Reanalysis in Sentence Processing.</booktitle>
<pages>327--363</pages>
<editor>In J. D. Fodor and F. Ferreira, editors,</editor>
<publisher>Kluwer Academic,</publisher>
<location>Boston, MA,</location>
<contexts>
<context position="33505" citStr="Stevenson 1998" startWordPosition="4942" endWordPosition="4943">room for new constituents that may be introduced at a later time. These in-element reductions are defined naturally on phrase structure trees as the result of aligning right-corner transformed constituent structures to sequences of random variables in a factored time-series model. The success of this predictive strategy in corpus-based coverage and accuracy results described in Sections 5 and 6 suggests that it may be plausible as a cognitive model. Other accounts may model reductions in bounded memory as occurring as soon as possible, by maintaining the option of undoing them when necessary (Stevenson 1998). This seems unattractive in the context of an interactive semantic model, however, where syntactic constituents and semantic referents are composed in tandem, because potentially very rich referential constraints introduced by composing a child constituent into a parent would have to be systematically undone. An interesting possibility might be that the appearance of syntactic restructuring may arise from a collection of hypothesized stores of syntactically fixed incomplete constituents, pursued in parallel. The results presented in this article suggest that this mechanism is possible, but th</context>
</contexts>
<marker>Stevenson, 1998</marker>
<rawString>Stevenson, Suzanne. 1998. Parsing as incremental restructuring. In J. D. Fodor and F. Ferreira, editors, Reanalysis in Sentence Processing. Kluwer Academic, Boston, MA, pages 327–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J Spivey-Knowlton</author>
<author>Kathy M Eberhard</author>
<author>Julie E Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>268--1632</pages>
<contexts>
<context position="29754" citStr="Tanenhaus et al. 1995" startWordPosition="4391" endWordPosition="4394">nstituent between these two, containing the recognized noun phrase as a left child (e.g., demand for bonds propped up SNP SNP the municipal bonds]’s prices]). This conception of chunking applied to right-branching progeny in phrase structure trees does not have the power to eliminate the bounds of a memory store, however. In a larger cognitive model, syntactic processing is assumed to occur as part of an interactive semantic interpretation process, in which referents of constituents are calculated as these constituents are recognized, and are used to constrain subsequent processing decisions (Tanenhaus et al. 1995; Brown-Schmidt, Campana, and Tanenhaus 2002).8 The chunked category pairs A and B in these incomplete constituents A/B result from successful compositions of other such constituents earlier in the recognition process, which means that the relationship between the referents of A and B is known and fixed 7 Incomplete constituents may also be defined through a left-corner transform, but left-corner transformed categories are incomplete in the other direction—a goal category yet to come lacking an already-recognized constituent—so stored incomplete constituent categories resulting from a left-cor</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Tanenhaus, Michael K., Michael J. Spivey-Knowlton, Kathy M. Eberhard, and Julie E. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632–1634.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>