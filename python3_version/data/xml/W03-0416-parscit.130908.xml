<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993777">
An Efficient Clustering Algorithm for Class-based Language Models
</title>
<author confidence="0.996018">
Takuya Matsuzakit Yusuke Miyaot Jun’ichi Tsujiit$
</author>
<affiliation confidence="0.999811">
tDepartment of Computer Science, University of Tokyo
</affiliation>
<address confidence="0.795992">
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
$CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
</address>
<email confidence="0.999725">
{matuzaki,yusuke,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963833333333">
This paper defines a general form for class-
based probabilistic language models and pro-
poses an efficient algorithm for clustering
based on this. Our evaluation experiments re-
vealed that our method decreased computation
time drastically, while retaining accuracy.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977408450704">
Clustering algorithms have been extensively studied in
the research area of natural language processing because
many researchers have proved that “classes” obtained by
clustering can improve the performance of various NLP
tasks. Examples have been class-based n-gram models
(Brown et al., 1992; Kneser and Ney, 1993), smooth-
ing techniques for structural disambiguation (Li and Abe,
1998) and word sense disambiguation (Sh¨utze, 1998).
In this paper, we define a general form for class-based
probabilistic language models, and propose an efficient
and model-theoretic algorithm for clustering based on
this. The algorithm involves three operations, CLAS-
SIFY, MERGE, and SPLIT, all of which decreases the
optimization function based on the MDL principle (Ris-
sanen, 1984), and can efficiently find a point near the lo-
cal optimum. The algorithm is applicable to more general
tasks than existing studies (Li and Abe, 1998; Berkhin
and Becher, 2002), and computational costs are signifi-
cantly small, which allows its application to very large
corpora.
Clustering algorithms may be classified into three
types. The first is a type that uses various heuristic mea-
sure of similarity between the elements to be clustered
and has no interpretation as a probability model (Widdow,
2002). The resulting clusters from this type of method
are not guaranteed to work effectively as a component
of a statistical language model, because the similarity
used in clustering is not derived from the criterion in the
learning process of the statistical model, e.g. likelihood.
The second type has clear interpretation as a probability
model, but no criteria to determine the number of clusters
(Brown et al., 1992; Kneser and Ney, 1993). The perfor-
mance of methods of this type depend on the number of
clusters that must be specified before the clustering pro-
cess. It may prove rather troublesome to determine the
proper number of clusters in this type of method. The
third has interpretation as a probability model and uses
some statistically motivated model selection criteria to
determine the proper number of clusters. This type has
a clear advantage compared to the second. AutoClass
(Cheeseman and Stutz, 1996), the Bayesian model merg-
ing method (Stolcke and Omohundro, 1996) and Li’s
method (Li, 2002) are examples of this type. AutoClass
and the Bayesian model merging are based on soft clus-
tering models and Li’s method is based on a hard clus-
tering model. In general, computational costs for hard
clustering models are lower than that for soft clustering
models. However, the time complexity of Li’s method is
of cubic order in the size of the vocabulary. Therefore, it
is not practical to apply it to large corpora.
Our model and clustering algorithm provide a solution
to these problems with existing clustering algorithms.
Since the model has clear interpretation as a probability
model, the clustering algorithm uses MDL as clustering
criteria and using a combination of top-down clustering,
bottom-up clustering, and a K-means style exchange al-
gorithm, the method we propose can perform the cluster-
ing efficiently.
We evaluated the algorithm through experiments on
a disambiguation task of Japanese dependency analysis.
In the experiments, we observed that the proposed algo-
rithm’s computation time is roughly linear to the size of
the vocabulary, and it performed slightly better than the
existing method. Our main intention in the experiments
was to see improvements in terms of computational cost,
not in performance in the test task. We will show, in Sec-
tions 2 and 3, that the proposed method can be applied
to a broader range of tasks than the test task we evalu-
ate in the experiments in Section 4. We need further ex-
periments to determine the performance of the proposed
method with more general tasks.
</bodyText>
<sectionHeader confidence="0.993696" genericHeader="method">
2 Probability model
</sectionHeader>
<subsectionHeader confidence="0.995308">
2.1 Class-based language modeling
</subsectionHeader>
<bodyText confidence="0.999969777777778">
Our probability model is a class-based model and it is an
extension of the model proposed by Li and Abe (1998).
We extend their two-dimensional class model to a multi-
dimensional class model, i.e., we incorporate an arbitrary
number of random variables in our model.
Although our probability model and learning algorithm
are general and not restricted to particular domains, we
mainly intend to use them in natural language process-
ing tasks where large amounts of lexical knowledge are
required. When we incorporate lexical information into
a model, we inevitably face the data-sparseness problem.
The idea of ‘word class’ (Brown et al., 1992) gives a gen-
eral solution to this problem. A word class is a group
of words which performs similarly in some linguistic
phenomena. Part-of-speech are well-known examples of
such classes. Incorporating word classes into linguistic
models yields good smoothing or, hopefully, meaningful
generalization from given samples.
</bodyText>
<subsectionHeader confidence="0.999437">
2.2 Model definition
</subsectionHeader>
<bodyText confidence="0.999515307692308">
Let us introduce some notations to define our model. In
our model, we have considered n kinds of discrete ran-
dom variables X1, X2,. - - , Xn and their joint distribu-
tion. Ak denotes a set ofpossible values for the k-th vari-
able Xk. Our probability model assumes disjunctive par-
titions of each Ak, which are denoted by Tk’s. A disjunc-
tive partition T = {C1, C2, ... , Cm} of A is a subset of
2A, and satisfies Ci n Cj = 0 (i =~ j) and A = Um1Ci.
We call elements in a partition Tk classes of elements in
Ak. Ck, or C. for short, denotes a class in Tk which
contains an element x E Ak.
With these notations, our probability model is ex-
pressed as:
</bodyText>
<equation confidence="0.961902">
P(XJ = X1iX2 = X2i ... iXn =Xn)
= P (CXI, CX2, . . . , CX�)
</equation>
<bodyText confidence="0.998749833333333">
In this paper, we have considered a hard clustering model,
i.e., P(xlC) = 0 for any x V C. Li &amp; Abe’s model
(1998) is an instance of this joint probability model,
where n = 2. Using more than 2 variables the model can
represent the probability for the co-occurrence of triplets,
such as &lt;subject, verb, object&gt;.
</bodyText>
<subsectionHeader confidence="0.999733">
2.3 Clustering criterion
</subsectionHeader>
<bodyText confidence="0.999307454545454">
To determine the proper number of classes in each par-
tition T1, ... , Tn, we need criteria other than the maxi-
mum likelihood criterion, because likelihood always be-
come greater when we use smaller classes. We can see
this class number decision problem as a model selection
problem and apply some statistically motivated model
selection criteria. As mentioned previously (following
Li and Abe (1998)) we used the MDL principle as our
clustering criterion.
Assume that we have N samples of co-occurrence
data:
</bodyText>
<equation confidence="0.650502">
S={xi=(X1i,X2i,•••,Xni) Ii=1,2,...,N}.
</equation>
<bodyText confidence="0.716306666666667">
The objective function in both clustering and parame-
ter estimations in our method is the description length,
I(M, S), which is defined as follows:
</bodyText>
<equation confidence="0.994927">
l(M,S)=— log Lm(S)+l(M), (2)
</equation>
<bodyText confidence="0.9997665">
where M denotes the model and Lm(S) is the likelihood
of samples S under model M:
</bodyText>
<equation confidence="0.981344">
Lm(S) = �� P(X7i)X2i)... )Xni)- (3)
���
</equation>
<bodyText confidence="0.99969225">
The first term in Eq.2, — log Lm(S), is called the data
description length. The second term, l(M), is called the
model description length, and when sample size N is
large, it can be approximated as
</bodyText>
<equation confidence="0.963751">
�
�(�) = 2 log N,
</equation>
<bodyText confidence="0.995858909090909">
where r is the number of free parameters in model M.
We used this approximated form throughout this paper.
Given the number of classes, Tnk = ITkI for each k =
1.... , n, we have l In1 Tnk — 1 free parameters for joint
probabilities P(Cl, .., Cn). Also, for each class C, we
have ICI — 1 free parameters for conditional probabilities
P(xIC), where x E C. Thus, we have
Our learning algorithm tries to minimize l(M, S) by
adjusting the parameters in the model, selecting partition
Tk of each Ak, and choosing the numbers of classes, mk
in each partition Tk.
</bodyText>
<equation confidence="0.9365062">
��
k=1
r=
11
k=1
(10 — 1) +
Tnk-1
=
�
CETI,
n
11
k=1
Tnk-1.
n
k=1
(JAkJ — mk) +
P(Xi I CXJ- (1)
��
���
</equation>
<sectionHeader confidence="0.933729" genericHeader="method">
3 Clustering algorithm
</sectionHeader>
<bodyText confidence="0.999638780487805">
Our clustering algorithm is a combination of three ba-
sic operations: CLASSIFY, SPLIT and MERGE. We it-
eratively invoke these until a terminate condition is met.
Briefly, these three work as follows. The CLASSIFY
takes a partition T in A as input and improves the par-
tition by moving the elements in A from one class to an-
other. This operation is similar to one iteration in the K-
means algorithm. The MERGE takes a partition T as in-
put and successively chooses two classes CZ and Cj from
T and replaces them with their union, CZUCS. The SPLIT
takes a class, C, and tries to find the best division of C
into two new classes, which will decrease the description
length the most.
All of these three basic operations decrease the de-
scription length. Consequently, our overall algorithm
also decreases the description length monotonically and
stops when all three operations cause no decrease in de-
scription length. Strictly, this termination does not guar-
antee the resulting partitions to be even locally opti-
mal, because SPLIT operations do not perform exhaus-
tive searches in all possible divisions of a class. Doing
such an exhaustive search is almost impossible for a class
of modest size, because the time complexity of such an
exhaustive search is of exponential order to the size of the
class. However, by properly selecting the number of tri-
als in SPLIT, we can expect the results to approach some
local optimum.
It is clear that the way the three operations are com-
bined affects the performance of the resulting class-based
model and the computation time required in learning. In
this paper, we basically take a top-down, divisive strat-
egy, but at each stage of division we do CLASSIFY op-
erations on the set of classes at each stage. When we
cannot divide any classes and CLASSIFY cannot move
any elements, we invoke MERGE to merge classes that
are too finely divided. This top-down strategy can drasti-
cally decrease the amount of computation time compared
to the bottom-up approaches used by Brown et al. (1992)
and Li and Abe (1998).
The following is the precise algorithm for our main
procedure:
</bodyText>
<construct confidence="0.572007142857143">
Algorithm 1 MAIN PROCEDURE(J)
INPUT
J : an integer specifying the number of trials in a
SPLIT operation
OUTPUT
Partitions T1, .., Tn and estimated parameters in the
model
</construct>
<sectionHeader confidence="0.589998" genericHeader="method">
PROCEDURE
</sectionHeader>
<bodyText confidence="0.890244">
Step 0 {Tl, ..,TnI �— INITIALIZE({Al, ..An}, J)
</bodyText>
<construct confidence="0.925667857142857">
Step 1 Do Step 2 through Step 3 until no change is made
through one iteration
Step 2 For s = 1,.., n, do Step 2.1 through Step 2.2
Step 2.1 Do Step 2.1.1 until no change occurs through it
Step 2.1.1 For k = 1,.., n , Tk �— CLASSIFY(Tk)
Step 2.2 For each C E Ts, C �— SPLIT(C, J)
Step 3 For k = 1,.., n, Tk �— MERGE(Tk)
</construct>
<bodyText confidence="0.9871635">
Step 4 Return the resulting partitions with the parame-
ters in the model
In the Step 0 of the algorithm, INITIALIZE creates
the initial partitions of Al,.. . , An. It first divides each
Al, ... , An into two classes and then applies CLASSIFY
to each partition T1, ... ,Tn one by one, while any ele-
ments can move.
The following subsections explain the algorithm for
the three basic operations in detail and show that they
decrease l(M, S) monotonically.
</bodyText>
<subsectionHeader confidence="0.995652">
3.1 Iterative classification
</subsectionHeader>
<bodyText confidence="0.999929470588235">
In this subsection, we explain a way of finding a local
optimum in the possible classification of elements in Ak,
given the numbers of classes in partitions Tk.
Given the number of classes, optimization in terms of
the description length (Eq.2) is just the same as optimiz-
ing the likelihood (Eq.3). We used a greedy algorithm
which monotonically increases the likelihood while
updating classification. Our method is a generalized
version of the previously reported K-means/EM-
algorithm-style, iterative-classification methods in
Kneser and Ney (1993), Berkhin and Becher (2002) and
Dhillon et al. (2002). We demonstrate that the method is
applicable to more generic situations than those previ-
ously reported, where the number of random variables is
arbitrary.
To explain the algorithm more fully, we define ‘counter
functions’ f(..) as follows:
</bodyText>
<equation confidence="0.9942638">
f(Xk) = #{X E S I Xk = XkI
f(Ck) = #{X E S I Xk E Ck}
f(Cl, .., Cn) = #{X E S I Xl E Cl, .., Xn E Cnj
f(Cl, .., Ck-1, X, Ck+1, ..,Cn)
= #�� � S � �� � C�(i =� k),~~ = X~
</equation>
<bodyText confidence="0.999571142857143">
where the hatch (#) denotes the cardinality of a set and
Xk is the k-th variable in sample x. We used 0 log 0 = 0,
in this subsection.
Our classification method is variable-wise. That is, to
classify elements in each Al, . . . , A,,, we classified the
elements in each Ak in order. The precise algorithm is as
follows:
</bodyText>
<figure confidence="0.41508475">
Algorithm 2 CLASSIFY(Tk)
INPUT Tk : a partition in Ak
OUTPUT An improved partition in Ak
PROCEDURE
Step 1 Do steps 2.1 through 2.3 until no elements in Ak
can move from their current class to another one.
Step 2.1 For each element x c Ak, choose a class C&apos; G
X
</figure>
<bodyText confidence="0.194343">
Tk which satisfies the following two conditions:
</bodyText>
<equation confidence="0.977419333333333">
1. Q is not empty (C&apos; �4 0), and
X X
2. Q maximizesfollowing quantity g (x, C&apos;):
X X
g(X, CX&apos;) = 1: f(Cl&apos;..&apos; Ck-l, X, Ck+l,.., cn)
C&apos;ETi
~(~l~ ��� ~~~l~ �� �� ~~+l~ ��� ��)
� log �
f(CX,)
When the class containing x now, Cx, maximizes g,
select Cx as Q even ifsome other classes also max-
X
</equation>
<bodyText confidence="0.805113333333333">
imize g.
Step 2.2 Update partition Tk by moving each x c Ak to
the classes which were selected as C&apos; for x in Step
</bodyText>
<equation confidence="0.746729">
X
2.1.
</equation>
<bodyText confidence="0.720141666666667">
from Dx as a set. However, with these notations, it holds
that if Cx&apos; = Cy&apos;, then Dx = Dy. We also use the suffixes
in notations Ci and Di as it holds that, if Q = Ci, then
</bodyText>
<equation confidence="0.925367583333333">
X
x c Di.
Using Eq.4, we can write the change in the log likeli-
hood, A(log L) as follows:
A(log L)
E f(X, C2&apos;..&apos; Cn) log f(Dx, C2&apos;..&apos; Cn)
�(��)
���������l
1: f(X, C2&apos;..&apos; Cn) log f(c, C2, ..&apos; CT&apos;)
�(��) �
���������l
(5)
</equation>
<bodyText confidence="0.999702">
To see the difference is &gt; 0, we insert the intermediate
terms into the right of Eq.5 and transform it as:
</bodyText>
<equation confidence="0.941029083333333">
A(log L)
E f(X, C2&apos;..&apos; Cn) log f(Dx, C2&apos;..&apos; Cn)
�(��)
���������l
� f(Cl, C2&apos;..&apos; Cn)
f(X, C2&apos;..&apos; Cn) log X �(���)
���������l
� f(Cl, C2&apos;..&apos; Cn)
f (X, C2, ..&apos; Cn) log X f(C,X)
CkETk,k,,�l
E f(X, C2&apos;..&apos; Cn) log f(CX&apos; C2&apos;..&apos; Cn)
�(��)
</equation>
<figure confidence="0.933288538461538">
���������l
1:
xEA,
1:
xEA,
1:
xEA,
1:
xEA,
+ 1:
xEA,
1:
xEA,
</figure>
<bodyText confidence="0.919073">
Step 2.3 Update the parameters by maximum likelihood = E 1: f(Di, C2&apos;..&apos; Cn)
estimation according to the updated partition. Dj(,,L0)EUj
Step 3 Return improved partition Tk.
In Step 2.3, the maximum likelihood estimation of the
parameters are given as follows:
</bodyText>
<equation confidence="0.954438">
p(X I CXk) = f(X) P(cl&apos;..&apos; CTI) = PC l, ..&apos; CT&apos;)
f(Cxk)&apos; N
(4)
</equation>
<bodyText confidence="0.985244083333333">
To see why this algorithm monotonically increases the
likelihood (Eq.3), it is sufficient to check that, for vari-
able Xk and any classification before Steps 2 and 3, do-
ing Steps 2 and 3 positively changes the log likelihood
(Eq.3). We can show this as follows.
First, assume k = I without loss of generality. Let
Tl = ICl,..., CT,,I and Ul = IDl,..., DT,,I denote
the partitions before/after Step 2, respectively. Let Cx G
Tl and Dx G Ul denote the classes where an element
x c Al belongs, before and after Step 2, respectively.
Also, let Cx&apos; G Tl denote the class which was chosen for
x in Step 2.1 in the algorithm. Note that C&apos; is different
</bodyText>
<equation confidence="0.9655474">
X
Xflog f(Di, C2&apos;..&apos; Cn) _ log f(C,, C2 &apos; ..&apos; Cn) 1 (6)
f (Dj) f Wi)
+ 1: (g(X, CX&apos;) — g(X, CA)- (7)
xEA,
</equation>
<bodyText confidence="0.947271857142857">
In the last expression, each term in the summation (7)
is &gt; 0 according to the conditions in Step 2 of the al-
gorithm. Then, the summation (7) as a whole is always
&gt; 0 and only equals 0 if no elements are moved. We
can confirm that the summation (6) is positive, through
an optimization problem:
maximize the following quantity
</bodyText>
<equation confidence="0.935629307692308">
1: 1: f(Di, C2&apos;..&apos; Cn) log 7(C2, .., CTjDj)
/10)EU1 CkETh
under the condition:
E 7(C2,.., CT, I Dj) = I
CkETh
for any Di(�4 0) E Ul.
f(Dj) is &gt; 0 because Di =� 0, and f(Di, C2,.., Cn)
is always &gt; 0. Thus, the solution to this problem is given
by:
7(C2,..,
(C2,••, CT,
I Di) = f(Di, C2,.., Cn)
f(Dj)
</equation>
<bodyText confidence="0.997902">
for any DZ(�4 0) E Ul. Through this, we can conclude
that the summation (6) is &gt; 0. Therefore, A(log L) &gt; 0
holds, i.e., CLASSIFY increases log likelihood monoton-
ically.
</bodyText>
<subsectionHeader confidence="0.998491">
3.2 SPLIT operation
</subsectionHeader>
<bodyText confidence="0.999398615384615">
The SPLIT takes a class as input and tries to find a way
to divide it into two sub-classes in such a way as to re-
duce description length. As mentioned earlier, to find the
best division in a class requires computation time that is
exponential to the size of the class. We will first use a
brute-force approach here. Let us simply try J random
divisions, rearrange them with CLASSIFY and use the
best one. If the best division does not reduce the descrip-
tion length, we will not change the class at all. It may
possible to use a more sophisticated initialization scheme,
but this simple method yielded satisfactory results in our
experiment.
The following is the precise algorithm for SPLIT:
</bodyText>
<figure confidence="0.647127071428571">
Algorithm 3 SPLIT(C, J)
INPUT
C : a class to be split
J : an integer specifying the number of trials
OUTPUT
Two new classes Cl and C2 on success, or C with
no modifications on failure
PROCEDURE
Step 1 Do Steps 2.1 through 2.3 J times
Step 2.1 Randomly divide C into two classes
Step 2.2 Apply CLASSIFY to these two classes
Step 2.3 Record the resulting two classes in Step 2.2 with
the reduced description length produced by this split
Step 3 Find the maximum reduction in the records
</figure>
<sectionHeader confidence="0.998094" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.995341">
3.3 MERGE operation
</subsectionHeader>
<bodyText confidence="0.999283235294118">
The MERGE takes partition T as input and successively
chooses two classes CZ and Cj from T and replaces them
with their union CZ U Cj. This operation thus reduces the
number of classes in T and accordingly reduces the num-
ber of parameters in the model. Therefore, if we properly
choose the ‘redundant’ classes in a partition, this merging
reduces the description length by the greater reduction in
the model description length which surpasses the loss in
log-likelihood.
Our MERGE is almost the same procedure as that de-
scribed by Li (2002). We first compute the reduction in
description length for all possible merges and record the
amount of reduction in a table. We then do the merges in
order of reduction, while updating the table.
The following is the precise algorithm for MERGE.
In the pseudo code, S2, denotes the reduction in l(M, S)
which results in the merging of CZ and Cj.
</bodyText>
<figure confidence="0.7857048">
Algorithm 4 MERGE(T)
INPUT T : a partition in A
OUTPUT An improved partition in A on success, or the
same partition as the input on failure
PROCEDURE
</figure>
<bodyText confidence="0.987818384615385">
Step 1 For each pair {C2, Cj} in T compute 62, and
store them in a table.
Step 2 Do Step 3.1 through 3.5 until the termination con-
dition in 3.2 is met
Step 3.1 Find the maximum, Stnax, in all 62,
Step 3.2 If Stnax G 0, return the updated partition, or
else go to Step 3.3.
Step 3.3 Replace the class pair {Ca, Cb} which corre-
sponds to Stnax, with their union C, = Ca U Cb.
Step 3.4 Delete all S2,’s which concern the merged
classes C,,, or Cb from the table.
Step 3.5 For each Ci in T (Ci =� C,), compute 6,j and
store them in the table.
It is clear from the termination condition in Step 3.2
that this operation reduces l(M, S) on success but does
not change it on failure.
Step 4 If this maximum reduction &gt; 0, return the corre-
sponding two classes as output, or return C if the
maximum G 0
Clearly, this operation decreases l(M, S) on success
and does not change it on failure.
This section discusses the results of the evaluation ex-
periment where we compared three clustering methods:
i.e., our method, Li’s agglomerative method described in
Li (2002), and a restricted version of our method that only
uses CLASSIFY.
</bodyText>
<subsectionHeader confidence="0.997862">
4.1 Evaluation task
</subsectionHeader>
<bodyText confidence="0.977727568181818">
We used a simplified version of the dependency analysis
task for Japanese for the evaluation experiment.
In Japanese, a sentence can be thought of as an array of
phrasal units called ‘bunsetsu’ and the dependency struc-
ture of a sentence can be represented by the relationships
between these bunsetsus. A bunsetsu consists of one or
more content words and zero or more function words that
follow these.
For example, the Japanese sentence
Ryoushi-ga kawa-de oyogu nezumi-wo utta.
hunter-SUBJ river-in swim mouse-OBJ shot
(A hunter shot a mouse which swam in the river.)
contains five bunsetsus { Ryoushi-ga, kawa-de, oyogu,
nezumi-wo, utta } and their dependency relations are as
follows:
Ryoushi-ga —� utta kawa-de —� oyogu
oyogu —� nezumi-wo nezumi-wo —� utta
Our task is, given an input bunsetsu, to output the cor-
rect bunsetsu on which the input bunsetsu depends. In
this task, we considered the dependency relations of lim-
ited types. That is the dependency of types: noun-pp —�
pred , where noun is a noun, or the head of a compound
noun, pp is one of 9 postpositions {ga, wo, ni, de, to,
he, made, kara, yori} and pred is a bunsetsu which con-
tains a verb or an adjective as its content word part. We
restricted possible dependee bunsetsus to be those to the
right of the input bunsetsus because in Japanese, basically
all dependency relations are from left to right. Thus, our
test data is in the form
&lt; noun-pp, {predl, .., predn} &gt;, (8)
where {predl,...,predn,I is the set of all candidate de-
pendee bunsetsus that are to the right of the input depen-
dent bunsetsu noun-pp in a sentence. The task is to select
the correct dependee of noun-pp from {pred1,..,pred,,}.
Our training data is in the form &lt;r, noun, pp, pred&gt;.
A sample of this form represents two bunsetsus, noun-
pp and pred within a sentence, in this order, and r E
�+� ��denotes whether they are in a dependency relation
(r = +), or not (r = —). From these types of samples,
we want to estimate probability P (r, noun, pp, pred) and
use these to approximate probability pi, where given the
test data in Eq.8, pred2 is the correct answer, expressed
as
Pi a P(+, noun, pp, pred�)11 P(—, noun, pp, pred�).
</bodyText>
<page confidence="0.988167">
17�2
</page>
<bodyText confidence="0.976306272727273">
We approximated the probability of occurrence for
sample type r = — expressed as
P(—, noun, pp, pred) = P(—, noun)P(—, pp, pred),
and estimated these from the raw frequencies. For the
probability of type r = +, we treated a pair of pp and
pred as one variable, pp:pred, expressed as
P(+, noun, pp, pred) = P+(noun, pp:pred).
and estimated P+(noun, pp:pred) from the training data.
Thus, our decision rule given test data (Eq.8) is, to
select predk where k is the index which maximizes the
value
</bodyText>
<equation confidence="0.9906725">
P+(noun, pp:predk)
P( pp, predk) �
</equation>
<bodyText confidence="0.999971357142857">
We extracted the training samples and the test data
from the EDR Japanese corpus (EDR, 1994). We ex-
tracted all the positive (i.e., r = +) and negative (r = —)
relation samples and divided them into 10 disjunctive sets
for 10-fold cross validation. When we divided the sam-
ples, all the relations extracted from one sentence were
put together in one of 10 sets. When a set was used as
the test data, these relations from one sentence were used
as the test data of the form (Eq.8). Of course, we did not
use samples with only one pred. In the results in the next
subsection, the ‘training data of size s’ means where we
used a subset ofpositive samples that were covered by the
most frequent s nouns and the most frequent s pp:pred
pairs.
</bodyText>
<subsectionHeader confidence="0.647641">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999928222222222">
In this experiments, we compared three methods: ours,
Li’s described in Li (2002), and a restricted version of
our method that only uses CLASSIFY operations. The
last method is simply called ‘the CLASSIFY method’
in this subsection. We used 10 as parameter J in our
method, which specifies the number of trials in initializa-
tion and each SPLIT operation. Li’s method (2002) uses
the MDL principle as clustering criteria and creates word
classes in a bottom-up fashion. Parameters ,,, and „ in
his method, which specify the maximum numbers of suc-
cessive merges in each dimension, were both set to 100.
The CLASSIFY method performs K-means style itera-
tive clustering and requires that the number of clusters be
specified beforehand. We set these to be the same as the
number of clusters created by our method in each train-
ing set. By evaluating the differences in the performance
of ours and the CLASSIFY method, we can see advan-
tages in our top-down approach guided by the MDL prin-
ciple, compared to the K-means style approach that uses a
fixed number of clusters.We expect that these advantages
will remain when compared to other previously reported,
K-means style methods (Kneser and Ney, 1993; Berkhin
and Becher, 2002; Dhillon et al., 2002).
In the results, precision refers to the ratio !l(! + &amp;quot;)
and coverage refers to the ratio !1#, where ! and &amp;quot; denote
the numbers of correct and wrong predictions, and # de-
notes the number of all test data. All the ‘ties cases’ were
</bodyText>
<figure confidence="0.96324">
1000 10000 100000
size of vocabulary
</figure>
<figureCaption confidence="0.999653">
Figure 1: Computation time
</figureCaption>
<figure confidence="0.9948595">
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45
coverage
</figure>
<figureCaption confidence="0.991362">
Figure 3: Coverage-precision plot
</figureCaption>
<figure confidence="0.999635428571429">
our method
Li’s method
CLASSIFY
computation time (sec) 100000
10000
1000
100
10
1
precision 0.81
0.8
0.79
0.78
0.77
0.76
0.75
0.74
our method
Li’s method
CLASSIFY
100000
10000
computation time(sec)
1000
100
10
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45
coverage
</figure>
<figureCaption confidence="0.999908">
Figure 2: Coverage-Cost plot
</figureCaption>
<bodyText confidence="0.985565949152542">
treated as wrong answers (w), where a ‘tie case’ means
a situation where two or more predictions are made with
the same maximum probabilities.
All digits are averages of results for ten training-test
pairs, except for Li’s method where the training sets were
8k or more. The results of the Li’s method on training
set of 8k were the averages over two training-test pairs.
We could not do more trials with Li’s method due to time
constraints. All experiments were done on Pentium III
1.2-GHz computers and the reported computation times
are wall-clock times.
Figure 1 shows the computation time as a function of
the size of the vocabulary, i.e., the number of nouns plus
the number of case frame slots (i.e., pp:pred) in the train-
ing data. We can clearly see the efficiency of our method
in the plot, compared to Li’s method. The log-log plot re-
veals our time complexity is roughly linear to the size of
the vocabulary in these data sets. This is about two orders
lower than that for Li’s method.
There is little relevance in comparing the speed of the
CLASSIFY method to the speed of the other two meth-
ods, because its computation time does not include the
time required to decide the proper number of classes. Of
more interest is to see its seeming speed-up in the largest
data sets. This implies that, in large and sparse training
data, the CLASSIFY method was caught in some bad lo-
cal optima at some early points on the way to better local
optima.
Figure 2 has the computation times as a function of
the coverage which is achieved using that computation
time. From this, we would expect our method to reach
higher coverage within a realistic time if we used larger
quantities of training data. To determine this, we need
other experiments using larger corpora, which we intend
to do in the future.
Table 1 lists the description lengths for training data
from 1 to 32k and Table 2 shows the precision and cov-
erage achieved by each method with this data. In these
tables, we can see that our method works slightly better
than Li’s method as an optimization method which min-
imizes the description length, and also in the evaluation
tasks. Therefore, we can say that our method decreased
computational costs without losing accuracy. We can also
see that ours always performs better than the CLASSIFY
method. Both ours and the CLASSIFY method use ran-
dom initializations, but from the results, it seems that our
top-down, divisive strategy in combination with K-means
like swapping and merging operations avoids the poor lo-
cal optima where the CLASSIFY method was caught.
Figure 3 also presents the results in terms of coverage-
precision trade-off. We can see that our method selected
always better points in the trade-off than Li’s method or
the CLASSIFY method.
From these results, we can conclude that our cluster-
ing algorithm is more efficient and yields slightly better
results than Li’s method, which uses the same cluster-
ing criterion. We can also expect that our combined ap-
our method
Li’s method
</bodyText>
<table confidence="0.7650995">
size of test data 1k 2k 3k 4k 5k 8k 16k 32k
our method 1.15 1.88 2.38 2.76 3.13 3.77 5.03 6.21
Li’s method 1.16 1.89 2.40 2.80 3.17 3.85 N/A N/A
CLASSIFY 1.16 1.89 2.39 2.77 3.14 3.79 5.08 6.31
</table>
<tableCaption confidence="0.996502">
Table 1: Description length in training data sets (unit: 1 x 106)
</tableCaption>
<table confidence="0.972645285714286">
size of training data 1k 2k 3k 4k 5k 8k 16k 32k
our method precision 0.805 0.799 0.798 0.794 0.791 0.797 0.780 0.745
coverage 0.043 0.076 0.109 0.136 0.163 0.245 0.362 0.429
Li’s method precision 0.802 0.795 0.793 0.786 0.784 0.791 N/A N/A
coverage 0.043 0.076 0.109 0.135 0.162 0.242 N/A N/A
CLASSIFY precision 0.797 0.792 0.789 0.785 0.786 0.789 0.768 0.741
coverage 0.042 0.075 0.108 0.135 0.162 0.242 0.356 0.427
</table>
<tableCaption confidence="0.998953">
Table 2: Performance of each method in the evaluation task
</tableCaption>
<bodyText confidence="0.999743666666667">
proach with the MDL principle will have advantages in
large and sparse data compared to existing K-means style
approaches where the number of the clusters is fixed.
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985363636364">
This paper proposed a general, class-based probability
model and described a clustering algorithm for it, which
we evaluated through experiments on a disambiguation
task of Japanese dependency analysis. We obtained the
following results. (1) Our clustering algorithm was much
more efficient than the existing method that uses the same
objective function and the same kind of model. (2) It
worked better as an optimization algorithm for the de-
scription length than the existing method. (3) It per-
formed better in the test task than an existing method and
another method that is similar to other existing methods.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996909375">
Andreas Stolcke and Stephen M. Omohundro. 1994.
Best-first Model Merging for Hidden Markov Model
Induction. Technical Report TR-94-003, Computer
Science Division, University of California at Berkeley
and International Science Institute.
Dominic Widdow and Beate Dorow. 2002. A Graph
Model for Unsupervised Lexical Acquisition. Pro-
ceedings of the 19th International Conference on Com-
putational Linguistics, 1093–1099.
EDR. 1994. EDR (Japanese Electronic Dictionary Re-
search Institute, Ltd) dictionary version 1.5 technical
guide.
Hang Li. 2002. Word Clustering and Disambiguation
based on Co-occurrence Data, Natural Language En-
gineering, 8(1), 25-42.
Hang Li and Naoki Abe. 1998. Word Clustering and
Disambiguation Based on Co-occurrence data. Pro-
ceedings of the 18th International Conference on Com-
putational Linguistics and the 36th Annual Meeting of
Association for Computational Linguistics, 749–755.
Hinrich Sch¨utze. 1998. Automatic Word Sense Discrim-
ination Computational Linguistics, 24(1) 97–124.
Inderjit S. Dhillon, Subramanyam Mallela and Rahul Ku-
mar. 2002. Information Theoretic Feature Clustering
for Text Classification. The Nineteenth International
Conference on Machine Learning, Workshop on Text
Learning.
Jorma Rissanen. 1984. Universal Coding, Information,
Prediction, and Estimation. IEEE Transactions on In-
formation theory, Vol. IT-30(4):629–636
Pavel Berkhin and Jonathan Becher. 2002. Learning
Simple Relations: Theory and Applications. In Pro-
ceedings of the Second SIAMInternational Conference
on Data Mining, 420–436.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai and Robert L. Mercer. 1992. Class-
Based n-gram Models of Natural Language. Compu-
tational Linguistics 18(4):467-479.
Peter Cheeseman and John Stutz. 1996. Bayesian Clas-
sification (AutoClass): Theory and Results. In U.
Fayyad, G. Piatetsky-Shapiro, P. Smyth and R. Uthu-
rusamy (Eds.), Advances in Knowledge Discovery and
Data Mining, 153–180. AAAI Press.
Reinherd Kneser and Hermann Ney. 1993. Improved
Clustering Techniques for Class-Based Statistical Lan-
guage Modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technol-
ogy, 973–976.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.244163">
<title confidence="0.8499455">An Efficient Clustering Algorithm for Class-based Language Models of Computer Science, University of</title>
<author confidence="0.26303">Tokyo Bunkyo-ku</author>
<affiliation confidence="0.971556">JST (Japan Science and Technology</affiliation>
<address confidence="0.946134">Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012</address>
<abstract confidence="0.997463142857143">This paper defines a general form for classbased probabilistic language models and proposes an efficient algorithm for clustering based on this. Our evaluation experiments revealed that our method decreased computation time drastically, while retaining accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Stephen M Omohundro</author>
</authors>
<title>Best-first Model Merging for Hidden Markov Model Induction.</title>
<date>1994</date>
<tech>Technical Report TR-94-003,</tech>
<institution>Computer Science Division, University of California at Berkeley and International Science Institute.</institution>
<marker>Stolcke, Omohundro, 1994</marker>
<rawString>Andreas Stolcke and Stephen M. Omohundro. 1994. Best-first Model Merging for Hidden Markov Model Induction. Technical Report TR-94-003, Computer Science Division, University of California at Berkeley and International Science Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdow</author>
<author>Beate Dorow</author>
</authors>
<title>A Graph Model for Unsupervised Lexical Acquisition.</title>
<date>2002</date>
<booktitle>Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1093--1099</pages>
<marker>Widdow, Dorow, 2002</marker>
<rawString>Dominic Widdow and Beate Dorow. 2002. A Graph Model for Unsupervised Lexical Acquisition. Proceedings of the 19th International Conference on Computational Linguistics, 1093–1099.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EDR</author>
</authors>
<title>EDR (Japanese Electronic Dictionary Research Institute, Ltd) dictionary version 1.5 technical guide.</title>
<date>1994</date>
<contexts>
<context position="22322" citStr="EDR, 1994" startWordPosition="3952" endWordPosition="3953">ted the probability of occurrence for sample type r = — expressed as P(—, noun, pp, pred) = P(—, noun)P(—, pp, pred), and estimated these from the raw frequencies. For the probability of type r = +, we treated a pair of pp and pred as one variable, pp:pred, expressed as P(+, noun, pp, pred) = P+(noun, pp:pred). and estimated P+(noun, pp:pred) from the training data. Thus, our decision rule given test data (Eq.8) is, to select predk where k is the index which maximizes the value P+(noun, pp:predk) P( pp, predk) � We extracted the training samples and the test data from the EDR Japanese corpus (EDR, 1994). We extracted all the positive (i.e., r = +) and negative (r = —) relation samples and divided them into 10 disjunctive sets for 10-fold cross validation. When we divided the samples, all the relations extracted from one sentence were put together in one of 10 sets. When a set was used as the test data, these relations from one sentence were used as the test data of the form (Eq.8). Of course, we did not use samples with only one pred. In the results in the next subsection, the ‘training data of size s’ means where we used a subset ofpositive samples that were covered by the most frequent s n</context>
</contexts>
<marker>EDR, 1994</marker>
<rawString>EDR. 1994. EDR (Japanese Electronic Dictionary Research Institute, Ltd) dictionary version 1.5 technical guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
</authors>
<title>Word Clustering and Disambiguation based on Co-occurrence Data,</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>25--42</pages>
<contexts>
<context position="2937" citStr="Li, 2002" startWordPosition="438" endWordPosition="439">n et al., 1992; Kneser and Ney, 1993). The performance of methods of this type depend on the number of clusters that must be specified before the clustering process. It may prove rather troublesome to determine the proper number of clusters in this type of method. The third has interpretation as a probability model and uses some statistically motivated model selection criteria to determine the proper number of clusters. This type has a clear advantage compared to the second. AutoClass (Cheeseman and Stutz, 1996), the Bayesian model merging method (Stolcke and Omohundro, 1996) and Li’s method (Li, 2002) are examples of this type. AutoClass and the Bayesian model merging are based on soft clustering models and Li’s method is based on a hard clustering model. In general, computational costs for hard clustering models are lower than that for soft clustering models. However, the time complexity of Li’s method is of cubic order in the size of the vocabulary. Therefore, it is not practical to apply it to large corpora. Our model and clustering algorithm provide a solution to these problems with existing clustering algorithms. Since the model has clear interpretation as a probability model, the clu</context>
<context position="17890" citStr="Li (2002)" startWordPosition="3152" endWordPosition="3153">he maximum reduction in the records 4 Evaluation 3.3 MERGE operation The MERGE takes partition T as input and successively chooses two classes CZ and Cj from T and replaces them with their union CZ U Cj. This operation thus reduces the number of classes in T and accordingly reduces the number of parameters in the model. Therefore, if we properly choose the ‘redundant’ classes in a partition, this merging reduces the description length by the greater reduction in the model description length which surpasses the loss in log-likelihood. Our MERGE is almost the same procedure as that described by Li (2002). We first compute the reduction in description length for all possible merges and record the amount of reduction in a table. We then do the merges in order of reduction, while updating the table. The following is the precise algorithm for MERGE. In the pseudo code, S2, denotes the reduction in l(M, S) which results in the merging of CZ and Cj. Algorithm 4 MERGE(T) INPUT T : a partition in A OUTPUT An improved partition in A on success, or the same partition as the input on failure PROCEDURE Step 1 For each pair {C2, Cj} in T compute 62, and store them in a table. Step 2 Do Step 3.1 through 3.</context>
<context position="19441" citStr="Li (2002)" startWordPosition="3445" endWordPosition="3446">om the table. Step 3.5 For each Ci in T (Ci =� C,), compute 6,j and store them in the table. It is clear from the termination condition in Step 3.2 that this operation reduces l(M, S) on success but does not change it on failure. Step 4 If this maximum reduction &gt; 0, return the corresponding two classes as output, or return C if the maximum G 0 Clearly, this operation decreases l(M, S) on success and does not change it on failure. This section discusses the results of the evaluation experiment where we compared three clustering methods: i.e., our method, Li’s agglomerative method described in Li (2002), and a restricted version of our method that only uses CLASSIFY. 4.1 Evaluation task We used a simplified version of the dependency analysis task for Japanese for the evaluation experiment. In Japanese, a sentence can be thought of as an array of phrasal units called ‘bunsetsu’ and the dependency structure of a sentence can be represented by the relationships between these bunsetsus. A bunsetsu consists of one or more content words and zero or more function words that follow these. For example, the Japanese sentence Ryoushi-ga kawa-de oyogu nezumi-wo utta. hunter-SUBJ river-in swim mouse-OBJ </context>
<context position="23059" citStr="Li (2002)" startWordPosition="4088" endWordPosition="4089">r 10-fold cross validation. When we divided the samples, all the relations extracted from one sentence were put together in one of 10 sets. When a set was used as the test data, these relations from one sentence were used as the test data of the form (Eq.8). Of course, we did not use samples with only one pred. In the results in the next subsection, the ‘training data of size s’ means where we used a subset ofpositive samples that were covered by the most frequent s nouns and the most frequent s pp:pred pairs. 4.2 Results In this experiments, we compared three methods: ours, Li’s described in Li (2002), and a restricted version of our method that only uses CLASSIFY operations. The last method is simply called ‘the CLASSIFY method’ in this subsection. We used 10 as parameter J in our method, which specifies the number of trials in initialization and each SPLIT operation. Li’s method (2002) uses the MDL principle as clustering criteria and creates word classes in a bottom-up fashion. Parameters ,,, and „ in his method, which specify the maximum numbers of successive merges in each dimension, were both set to 100. The CLASSIFY method performs K-means style iterative clustering and requires tha</context>
</contexts>
<marker>Li, 2002</marker>
<rawString>Hang Li. 2002. Word Clustering and Disambiguation based on Co-occurrence Data, Natural Language Engineering, 8(1), 25-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Word Clustering and Disambiguation Based on Co-occurrence data.</title>
<date>1998</date>
<booktitle>Proceedings of the 18th International Conference on Computational Linguistics and the 36th Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>749--755</pages>
<contexts>
<context position="1039" citStr="Li and Abe, 1998" startWordPosition="134" endWordPosition="137"> probabilistic language models and proposes an efficient algorithm for clustering based on this. Our evaluation experiments revealed that our method decreased computation time drastically, while retaining accuracy. 1 Introduction Clustering algorithms have been extensively studied in the research area of natural language processing because many researchers have proved that “classes” obtained by clustering can improve the performance of various NLP tasks. Examples have been class-based n-gram models (Brown et al., 1992; Kneser and Ney, 1993), smoothing techniques for structural disambiguation (Li and Abe, 1998) and word sense disambiguation (Sh¨utze, 1998). In this paper, we define a general form for class-based probabilistic language models, and propose an efficient and model-theoretic algorithm for clustering based on this. The algorithm involves three operations, CLASSIFY, MERGE, and SPLIT, all of which decreases the optimization function based on the MDL principle (Rissanen, 1984), and can efficiently find a point near the local optimum. The algorithm is applicable to more general tasks than existing studies (Li and Abe, 1998; Berkhin and Becher, 2002), and computational costs are significantly </context>
<context position="4618" citStr="Li and Abe (1998)" startWordPosition="714" endWordPosition="717">rformed slightly better than the existing method. Our main intention in the experiments was to see improvements in terms of computational cost, not in performance in the test task. We will show, in Sections 2 and 3, that the proposed method can be applied to a broader range of tasks than the test task we evaluate in the experiments in Section 4. We need further experiments to determine the performance of the proposed method with more general tasks. 2 Probability model 2.1 Class-based language modeling Our probability model is a class-based model and it is an extension of the model proposed by Li and Abe (1998). We extend their two-dimensional class model to a multidimensional class model, i.e., we incorporate an arbitrary number of random variables in our model. Although our probability model and learning algorithm are general and not restricted to particular domains, we mainly intend to use them in natural language processing tasks where large amounts of lexical knowledge are required. When we incorporate lexical information into a model, we inevitably face the data-sparseness problem. The idea of ‘word class’ (Brown et al., 1992) gives a general solution to this problem. A word class is a group o</context>
<context position="6931" citStr="Li and Abe (1998)" startWordPosition="1118" endWordPosition="1121">an instance of this joint probability model, where n = 2. Using more than 2 variables the model can represent the probability for the co-occurrence of triplets, such as &lt;subject, verb, object&gt;. 2.3 Clustering criterion To determine the proper number of classes in each partition T1, ... , Tn, we need criteria other than the maximum likelihood criterion, because likelihood always become greater when we use smaller classes. We can see this class number decision problem as a model selection problem and apply some statistically motivated model selection criteria. As mentioned previously (following Li and Abe (1998)) we used the MDL principle as our clustering criterion. Assume that we have N samples of co-occurrence data: S={xi=(X1i,X2i,•••,Xni) Ii=1,2,...,N}. The objective function in both clustering and parameter estimations in our method is the description length, I(M, S), which is defined as follows: l(M,S)=— log Lm(S)+l(M), (2) where M denotes the model and Lm(S) is the likelihood of samples S under model M: Lm(S) = �� P(X7i)X2i)... )Xni)- (3) ��� The first term in Eq.2, — log Lm(S), is called the data description length. The second term, l(M), is called the model description length, and when sampl</context>
<context position="10313" citStr="Li and Abe (1998)" startWordPosition="1719" endWordPosition="1722"> clear that the way the three operations are combined affects the performance of the resulting class-based model and the computation time required in learning. In this paper, we basically take a top-down, divisive strategy, but at each stage of division we do CLASSIFY operations on the set of classes at each stage. When we cannot divide any classes and CLASSIFY cannot move any elements, we invoke MERGE to merge classes that are too finely divided. This top-down strategy can drastically decrease the amount of computation time compared to the bottom-up approaches used by Brown et al. (1992) and Li and Abe (1998). The following is the precise algorithm for our main procedure: Algorithm 1 MAIN PROCEDURE(J) INPUT J : an integer specifying the number of trials in a SPLIT operation OUTPUT Partitions T1, .., Tn and estimated parameters in the model PROCEDURE Step 0 {Tl, ..,TnI �— INITIALIZE({Al, ..An}, J) Step 1 Do Step 2 through Step 3 until no change is made through one iteration Step 2 For s = 1,.., n, do Step 2.1 through Step 2.2 Step 2.1 Do Step 2.1.1 until no change occurs through it Step 2.1.1 For k = 1,.., n , Tk �— CLASSIFY(Tk) Step 2.2 For each C E Ts, C �— SPLIT(C, J) Step 3 For k = 1,.., n, Tk </context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Word Clustering and Disambiguation Based on Co-occurrence data. Proceedings of the 18th International Conference on Computational Linguistics and the 36th Annual Meeting of Association for Computational Linguistics, 749–755.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1998</date>
<journal>Automatic Word Sense Discrimination Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>97--124</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic Word Sense Discrimination Computational Linguistics, 24(1) 97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>Subramanyam Mallela</author>
<author>Rahul Kumar</author>
</authors>
<title>Information Theoretic Feature Clustering for Text Classification.</title>
<date>2002</date>
<booktitle>The Nineteenth International Conference on Machine Learning, Workshop on Text Learning.</booktitle>
<contexts>
<context position="12009" citStr="Dhillon et al. (2002)" startWordPosition="2015" endWordPosition="2018">ive classification In this subsection, we explain a way of finding a local optimum in the possible classification of elements in Ak, given the numbers of classes in partitions Tk. Given the number of classes, optimization in terms of the description length (Eq.2) is just the same as optimizing the likelihood (Eq.3). We used a greedy algorithm which monotonically increases the likelihood while updating classification. Our method is a generalized version of the previously reported K-means/EMalgorithm-style, iterative-classification methods in Kneser and Ney (1993), Berkhin and Becher (2002) and Dhillon et al. (2002). We demonstrate that the method is applicable to more generic situations than those previously reported, where the number of random variables is arbitrary. To explain the algorithm more fully, we define ‘counter functions’ f(..) as follows: f(Xk) = #{X E S I Xk = XkI f(Ck) = #{X E S I Xk E Ck} f(Cl, .., Cn) = #{X E S I Xl E Cl, .., Xn E Cnj f(Cl, .., Ck-1, X, Ck+1, ..,Cn) = #�� � S � �� � C�(i =� k),~~ = X~ where the hatch (#) denotes the cardinality of a set and Xk is the k-th variable in sample x. We used 0 log 0 = 0, in this subsection. Our classification method is variable-wise. That is, </context>
<context position="24221" citStr="Dhillon et al., 2002" startWordPosition="4282" endWordPosition="4285">erforms K-means style iterative clustering and requires that the number of clusters be specified beforehand. We set these to be the same as the number of clusters created by our method in each training set. By evaluating the differences in the performance of ours and the CLASSIFY method, we can see advantages in our top-down approach guided by the MDL principle, compared to the K-means style approach that uses a fixed number of clusters.We expect that these advantages will remain when compared to other previously reported, K-means style methods (Kneser and Ney, 1993; Berkhin and Becher, 2002; Dhillon et al., 2002). In the results, precision refers to the ratio !l(! + &amp;quot;) and coverage refers to the ratio !1#, where ! and &amp;quot; denote the numbers of correct and wrong predictions, and # denotes the number of all test data. All the ‘ties cases’ were 1000 10000 100000 size of vocabulary Figure 1: Computation time 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 coverage Figure 3: Coverage-precision plot our method Li’s method CLASSIFY computation time (sec) 100000 10000 1000 100 10 1 precision 0.81 0.8 0.79 0.78 0.77 0.76 0.75 0.74 our method Li’s method CLASSIFY 100000 10000 computation time(sec) 1000 100 10 0 0.05 0</context>
</contexts>
<marker>Dhillon, Mallela, Kumar, 2002</marker>
<rawString>Inderjit S. Dhillon, Subramanyam Mallela and Rahul Kumar. 2002. Information Theoretic Feature Clustering for Text Classification. The Nineteenth International Conference on Machine Learning, Workshop on Text Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Universal Coding, Information, Prediction, and Estimation.</title>
<date>1984</date>
<journal>IEEE Transactions on Information theory,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1420" citStr="Rissanen, 1984" startWordPosition="191" endWordPosition="193">s” obtained by clustering can improve the performance of various NLP tasks. Examples have been class-based n-gram models (Brown et al., 1992; Kneser and Ney, 1993), smoothing techniques for structural disambiguation (Li and Abe, 1998) and word sense disambiguation (Sh¨utze, 1998). In this paper, we define a general form for class-based probabilistic language models, and propose an efficient and model-theoretic algorithm for clustering based on this. The algorithm involves three operations, CLASSIFY, MERGE, and SPLIT, all of which decreases the optimization function based on the MDL principle (Rissanen, 1984), and can efficiently find a point near the local optimum. The algorithm is applicable to more general tasks than existing studies (Li and Abe, 1998; Berkhin and Becher, 2002), and computational costs are significantly small, which allows its application to very large corpora. Clustering algorithms may be classified into three types. The first is a type that uses various heuristic measure of similarity between the elements to be clustered and has no interpretation as a probability model (Widdow, 2002). The resulting clusters from this type of method are not guaranteed to work effectively as a </context>
</contexts>
<marker>Rissanen, 1984</marker>
<rawString>Jorma Rissanen. 1984. Universal Coding, Information, Prediction, and Estimation. IEEE Transactions on Information theory, Vol. IT-30(4):629–636</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Berkhin</author>
<author>Jonathan Becher</author>
</authors>
<title>Learning Simple Relations: Theory and Applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second SIAMInternational Conference on Data Mining,</booktitle>
<pages>420--436</pages>
<contexts>
<context position="1595" citStr="Berkhin and Becher, 2002" startWordPosition="220" endWordPosition="223">smoothing techniques for structural disambiguation (Li and Abe, 1998) and word sense disambiguation (Sh¨utze, 1998). In this paper, we define a general form for class-based probabilistic language models, and propose an efficient and model-theoretic algorithm for clustering based on this. The algorithm involves three operations, CLASSIFY, MERGE, and SPLIT, all of which decreases the optimization function based on the MDL principle (Rissanen, 1984), and can efficiently find a point near the local optimum. The algorithm is applicable to more general tasks than existing studies (Li and Abe, 1998; Berkhin and Becher, 2002), and computational costs are significantly small, which allows its application to very large corpora. Clustering algorithms may be classified into three types. The first is a type that uses various heuristic measure of similarity between the elements to be clustered and has no interpretation as a probability model (Widdow, 2002). The resulting clusters from this type of method are not guaranteed to work effectively as a component of a statistical language model, because the similarity used in clustering is not derived from the criterion in the learning process of the statistical model, e.g. l</context>
<context position="11983" citStr="Berkhin and Becher (2002)" startWordPosition="2010" endWordPosition="2013">, S) monotonically. 3.1 Iterative classification In this subsection, we explain a way of finding a local optimum in the possible classification of elements in Ak, given the numbers of classes in partitions Tk. Given the number of classes, optimization in terms of the description length (Eq.2) is just the same as optimizing the likelihood (Eq.3). We used a greedy algorithm which monotonically increases the likelihood while updating classification. Our method is a generalized version of the previously reported K-means/EMalgorithm-style, iterative-classification methods in Kneser and Ney (1993), Berkhin and Becher (2002) and Dhillon et al. (2002). We demonstrate that the method is applicable to more generic situations than those previously reported, where the number of random variables is arbitrary. To explain the algorithm more fully, we define ‘counter functions’ f(..) as follows: f(Xk) = #{X E S I Xk = XkI f(Ck) = #{X E S I Xk E Ck} f(Cl, .., Cn) = #{X E S I Xl E Cl, .., Xn E Cnj f(Cl, .., Ck-1, X, Ck+1, ..,Cn) = #�� � S � �� � C�(i =� k),~~ = X~ where the hatch (#) denotes the cardinality of a set and Xk is the k-th variable in sample x. We used 0 log 0 = 0, in this subsection. Our classification method i</context>
<context position="24198" citStr="Berkhin and Becher, 2002" startWordPosition="4278" endWordPosition="4281">100. The CLASSIFY method performs K-means style iterative clustering and requires that the number of clusters be specified beforehand. We set these to be the same as the number of clusters created by our method in each training set. By evaluating the differences in the performance of ours and the CLASSIFY method, we can see advantages in our top-down approach guided by the MDL principle, compared to the K-means style approach that uses a fixed number of clusters.We expect that these advantages will remain when compared to other previously reported, K-means style methods (Kneser and Ney, 1993; Berkhin and Becher, 2002; Dhillon et al., 2002). In the results, precision refers to the ratio !l(! + &amp;quot;) and coverage refers to the ratio !1#, where ! and &amp;quot; denote the numbers of correct and wrong predictions, and # denotes the number of all test data. All the ‘ties cases’ were 1000 10000 100000 size of vocabulary Figure 1: Computation time 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 coverage Figure 3: Coverage-precision plot our method Li’s method CLASSIFY computation time (sec) 100000 10000 1000 100 10 1 precision 0.81 0.8 0.79 0.78 0.77 0.76 0.75 0.74 our method Li’s method CLASSIFY 100000 10000 computation time(se</context>
</contexts>
<marker>Berkhin, Becher, 2002</marker>
<rawString>Pavel Berkhin and Jonathan Becher. 2002. Learning Simple Relations: Theory and Applications. In Proceedings of the Second SIAMInternational Conference on Data Mining, 420–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>ClassBased n-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<pages>18--4</pages>
<contexts>
<context position="945" citStr="Brown et al., 1992" startWordPosition="120" endWordPosition="123">aki,yusuke,tsujii}@is.s.u-tokyo.ac.jp Abstract This paper defines a general form for classbased probabilistic language models and proposes an efficient algorithm for clustering based on this. Our evaluation experiments revealed that our method decreased computation time drastically, while retaining accuracy. 1 Introduction Clustering algorithms have been extensively studied in the research area of natural language processing because many researchers have proved that “classes” obtained by clustering can improve the performance of various NLP tasks. Examples have been class-based n-gram models (Brown et al., 1992; Kneser and Ney, 1993), smoothing techniques for structural disambiguation (Li and Abe, 1998) and word sense disambiguation (Sh¨utze, 1998). In this paper, we define a general form for class-based probabilistic language models, and propose an efficient and model-theoretic algorithm for clustering based on this. The algorithm involves three operations, CLASSIFY, MERGE, and SPLIT, all of which decreases the optimization function based on the MDL principle (Rissanen, 1984), and can efficiently find a point near the local optimum. The algorithm is applicable to more general tasks than existing st</context>
<context position="2342" citStr="Brown et al., 1992" startWordPosition="339" endWordPosition="342">be classified into three types. The first is a type that uses various heuristic measure of similarity between the elements to be clustered and has no interpretation as a probability model (Widdow, 2002). The resulting clusters from this type of method are not guaranteed to work effectively as a component of a statistical language model, because the similarity used in clustering is not derived from the criterion in the learning process of the statistical model, e.g. likelihood. The second type has clear interpretation as a probability model, but no criteria to determine the number of clusters (Brown et al., 1992; Kneser and Ney, 1993). The performance of methods of this type depend on the number of clusters that must be specified before the clustering process. It may prove rather troublesome to determine the proper number of clusters in this type of method. The third has interpretation as a probability model and uses some statistically motivated model selection criteria to determine the proper number of clusters. This type has a clear advantage compared to the second. AutoClass (Cheeseman and Stutz, 1996), the Bayesian model merging method (Stolcke and Omohundro, 1996) and Li’s method (Li, 2002) are </context>
<context position="5150" citStr="Brown et al., 1992" startWordPosition="796" endWordPosition="799">s a class-based model and it is an extension of the model proposed by Li and Abe (1998). We extend their two-dimensional class model to a multidimensional class model, i.e., we incorporate an arbitrary number of random variables in our model. Although our probability model and learning algorithm are general and not restricted to particular domains, we mainly intend to use them in natural language processing tasks where large amounts of lexical knowledge are required. When we incorporate lexical information into a model, we inevitably face the data-sparseness problem. The idea of ‘word class’ (Brown et al., 1992) gives a general solution to this problem. A word class is a group of words which performs similarly in some linguistic phenomena. Part-of-speech are well-known examples of such classes. Incorporating word classes into linguistic models yields good smoothing or, hopefully, meaningful generalization from given samples. 2.2 Model definition Let us introduce some notations to define our model. In our model, we have considered n kinds of discrete random variables X1, X2,. - - , Xn and their joint distribution. Ak denotes a set ofpossible values for the k-th variable Xk. Our probability model assum</context>
<context position="10291" citStr="Brown et al. (1992)" startWordPosition="1714" endWordPosition="1717">ome local optimum. It is clear that the way the three operations are combined affects the performance of the resulting class-based model and the computation time required in learning. In this paper, we basically take a top-down, divisive strategy, but at each stage of division we do CLASSIFY operations on the set of classes at each stage. When we cannot divide any classes and CLASSIFY cannot move any elements, we invoke MERGE to merge classes that are too finely divided. This top-down strategy can drastically decrease the amount of computation time compared to the bottom-up approaches used by Brown et al. (1992) and Li and Abe (1998). The following is the precise algorithm for our main procedure: Algorithm 1 MAIN PROCEDURE(J) INPUT J : an integer specifying the number of trials in a SPLIT operation OUTPUT Partitions T1, .., Tn and estimated parameters in the model PROCEDURE Step 0 {Tl, ..,TnI �— INITIALIZE({Al, ..An}, J) Step 1 Do Step 2 through Step 3 until no change is made through one iteration Step 2 For s = 1,.., n, do Step 2.1 through Step 2.2 Step 2.1 Do Step 2.1.1 until no change occurs through it Step 2.1.1 For k = 1,.., n , Tk �— CLASSIFY(Tk) Step 2.2 For each C E Ts, C �— SPLIT(C, J) Step </context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai and Robert L. Mercer. 1992. ClassBased n-gram Models of Natural Language. Computational Linguistics 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Cheeseman</author>
<author>John Stutz</author>
</authors>
<title>Bayesian Classification (AutoClass): Theory and Results. In</title>
<date>1996</date>
<booktitle>Advances in Knowledge Discovery and Data Mining,</booktitle>
<pages>153--180</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="2845" citStr="Cheeseman and Stutz, 1996" startWordPosition="421" endWordPosition="424">pe has clear interpretation as a probability model, but no criteria to determine the number of clusters (Brown et al., 1992; Kneser and Ney, 1993). The performance of methods of this type depend on the number of clusters that must be specified before the clustering process. It may prove rather troublesome to determine the proper number of clusters in this type of method. The third has interpretation as a probability model and uses some statistically motivated model selection criteria to determine the proper number of clusters. This type has a clear advantage compared to the second. AutoClass (Cheeseman and Stutz, 1996), the Bayesian model merging method (Stolcke and Omohundro, 1996) and Li’s method (Li, 2002) are examples of this type. AutoClass and the Bayesian model merging are based on soft clustering models and Li’s method is based on a hard clustering model. In general, computational costs for hard clustering models are lower than that for soft clustering models. However, the time complexity of Li’s method is of cubic order in the size of the vocabulary. Therefore, it is not practical to apply it to large corpora. Our model and clustering algorithm provide a solution to these problems with existing clu</context>
</contexts>
<marker>Cheeseman, Stutz, 1996</marker>
<rawString>Peter Cheeseman and John Stutz. 1996. Bayesian Classification (AutoClass): Theory and Results. In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth and R. Uthurusamy (Eds.), Advances in Knowledge Discovery and Data Mining, 153–180. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinherd Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Clustering Techniques for Class-Based Statistical Language Modelling.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd European Conference on Speech Communication and Technology,</booktitle>
<pages>973--976</pages>
<contexts>
<context position="968" citStr="Kneser and Ney, 1993" startWordPosition="124" endWordPosition="127">s.s.u-tokyo.ac.jp Abstract This paper defines a general form for classbased probabilistic language models and proposes an efficient algorithm for clustering based on this. Our evaluation experiments revealed that our method decreased computation time drastically, while retaining accuracy. 1 Introduction Clustering algorithms have been extensively studied in the research area of natural language processing because many researchers have proved that “classes” obtained by clustering can improve the performance of various NLP tasks. Examples have been class-based n-gram models (Brown et al., 1992; Kneser and Ney, 1993), smoothing techniques for structural disambiguation (Li and Abe, 1998) and word sense disambiguation (Sh¨utze, 1998). In this paper, we define a general form for class-based probabilistic language models, and propose an efficient and model-theoretic algorithm for clustering based on this. The algorithm involves three operations, CLASSIFY, MERGE, and SPLIT, all of which decreases the optimization function based on the MDL principle (Rissanen, 1984), and can efficiently find a point near the local optimum. The algorithm is applicable to more general tasks than existing studies (Li and Abe, 1998</context>
<context position="2365" citStr="Kneser and Ney, 1993" startWordPosition="343" endWordPosition="346">hree types. The first is a type that uses various heuristic measure of similarity between the elements to be clustered and has no interpretation as a probability model (Widdow, 2002). The resulting clusters from this type of method are not guaranteed to work effectively as a component of a statistical language model, because the similarity used in clustering is not derived from the criterion in the learning process of the statistical model, e.g. likelihood. The second type has clear interpretation as a probability model, but no criteria to determine the number of clusters (Brown et al., 1992; Kneser and Ney, 1993). The performance of methods of this type depend on the number of clusters that must be specified before the clustering process. It may prove rather troublesome to determine the proper number of clusters in this type of method. The third has interpretation as a probability model and uses some statistically motivated model selection criteria to determine the proper number of clusters. This type has a clear advantage compared to the second. AutoClass (Cheeseman and Stutz, 1996), the Bayesian model merging method (Stolcke and Omohundro, 1996) and Li’s method (Li, 2002) are examples of this type. </context>
<context position="11956" citStr="Kneser and Ney (1993)" startWordPosition="2006" endWordPosition="2009"> that they decrease l(M, S) monotonically. 3.1 Iterative classification In this subsection, we explain a way of finding a local optimum in the possible classification of elements in Ak, given the numbers of classes in partitions Tk. Given the number of classes, optimization in terms of the description length (Eq.2) is just the same as optimizing the likelihood (Eq.3). We used a greedy algorithm which monotonically increases the likelihood while updating classification. Our method is a generalized version of the previously reported K-means/EMalgorithm-style, iterative-classification methods in Kneser and Ney (1993), Berkhin and Becher (2002) and Dhillon et al. (2002). We demonstrate that the method is applicable to more generic situations than those previously reported, where the number of random variables is arbitrary. To explain the algorithm more fully, we define ‘counter functions’ f(..) as follows: f(Xk) = #{X E S I Xk = XkI f(Ck) = #{X E S I Xk E Ck} f(Cl, .., Cn) = #{X E S I Xl E Cl, .., Xn E Cnj f(Cl, .., Ck-1, X, Ck+1, ..,Cn) = #�� � S � �� � C�(i =� k),~~ = X~ where the hatch (#) denotes the cardinality of a set and Xk is the k-th variable in sample x. We used 0 log 0 = 0, in this subsection. </context>
<context position="24172" citStr="Kneser and Ney, 1993" startWordPosition="4274" endWordPosition="4277">ion, were both set to 100. The CLASSIFY method performs K-means style iterative clustering and requires that the number of clusters be specified beforehand. We set these to be the same as the number of clusters created by our method in each training set. By evaluating the differences in the performance of ours and the CLASSIFY method, we can see advantages in our top-down approach guided by the MDL principle, compared to the K-means style approach that uses a fixed number of clusters.We expect that these advantages will remain when compared to other previously reported, K-means style methods (Kneser and Ney, 1993; Berkhin and Becher, 2002; Dhillon et al., 2002). In the results, precision refers to the ratio !l(! + &amp;quot;) and coverage refers to the ratio !1#, where ! and &amp;quot; denote the numbers of correct and wrong predictions, and # denotes the number of all test data. All the ‘ties cases’ were 1000 10000 100000 size of vocabulary Figure 1: Computation time 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 coverage Figure 3: Coverage-precision plot our method Li’s method CLASSIFY computation time (sec) 100000 10000 1000 100 10 1 precision 0.81 0.8 0.79 0.78 0.77 0.76 0.75 0.74 our method Li’s method CLASSIFY 100000</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>Reinherd Kneser and Hermann Ney. 1993. Improved Clustering Techniques for Class-Based Statistical Language Modelling. In Proceedings of the 3rd European Conference on Speech Communication and Technology, 973–976.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>