<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.939032">
Fast and Adaptive Online Training of Feature-Rich Translation Models
</title>
<author confidence="0.980843">
Spence Green, Sida Wang, Daniel Cer, and Christopher D. Manning
</author>
<affiliation confidence="0.982172">
Computer Science Department, Stanford University
</affiliation>
<email confidence="0.99007">
{spenceg,sidaw,danielcer,manning}@stanford.edu
</email>
<sectionHeader confidence="0.994583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956090909091">
We present a fast and scalable online
method for tuning statistical machine trans-
lation models with large feature sets. The
standard tuning algorithm—MERT—only
scales to tens of features. Recent discrimi-
native algorithms that accommodate sparse
features have produced smaller than ex-
pected translation quality gains in large
systems. Our method, which is based on
stochastic gradient descent with an adaptive
learning rate, scales to millions of features
and tuning sets with tens of thousands of
sentences, while still converging after only
a few epochs. Large-scale experiments on
Arabic-English and Chinese-English show
that our method produces significant trans-
lation quality gains by exploiting sparse fea-
tures. Equally important is our analysis,
which suggests techniques for mitigating
overfitting and domain mismatch, and ap-
plies to other recent discriminative methods
for machine translation.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933169491526">
Sparse, overlapping features such as words and n-
gram contexts improve many NLP systems such as
parsers and taggers. Adaptation of discriminative
learning methods for these types of features to sta-
tistical machine translation (MT) systems, which
have historically used idiosyncratic learning tech-
niques for a few dense features, has been an active
research area for the past half-decade. However, de-
spite some research successes, feature-rich models
are rarely used in annual MT evaluations. For exam-
ple, among all submissions to the WMT and IWSLT
2012 shared tasks, just one participant tuned more
than 30 features (Hasler et al., 2012a). Slow uptake
of these methods may be due to implementation
complexities, or to practical difficulties of configur-
ing them for specific translation tasks (Gimpel and
Smith, 2012; Simianer et al., 2012, inter alia).
We introduce a new method for training feature-
rich MT systems that is effective yet comparatively
easy to implement. The algorithm scales to millions
of features and large tuning sets. It optimizes a lo-
gistic objective identical to that of PRO (Hopkins
and May, 2011) with stochastic gradient descent, al-
though other objectives are possible. The learning
rate is set adaptively using AdaGrad (Duchi et al.,
2011), which is particularly effective for the mixture
of dense and sparse features present in MT models.
Finally, feature selection is implemented as efficient
Li regularization in the forward-backward splitting
(FOBOS) framework (Duchi and Singer, 2009). Ex-
periments show that our algorithm converges faster
than batch alternatives.
To learn good weights for the sparse features,
most algorithms—including ours—benefit from
more tuning data, and the natural source is the train-
ing bitext. However, the bitext presents two prob-
lems. First, it has a single reference, sometimes of
lower quality than the multiple references in tun-
ing sets from MT competitions. Second, large bi-
texts often comprise many text genres (Haddow and
Koehn, 2012), a virtue for classical dense MT mod-
els but a curse for high dimensional models: bitext
tuning can lead to a significant domain adaptation
problem when evaluating on standard test sets. Our
analysis separates and quantifies these two issues.
We conduct large-scale translation quality exper-
iments on Arabic-English and Chinese-English. As
baselines we use MERT (Och, 2003), PRO, and
the Moses (Koehn et al., 2007) implementation
of k-best MIRA, which Cherry and Foster (2012)
recently showed to work as well as online MIRA
(Chiang, 2012) for feature-rich models. The first
experiment uses standard tuning and test sets from
the NIST OpenMT competitions. The second ex-
periment uses tuning and test sets sampled from the
large bitexts. The new method yields significant
improvements in both experiments. Our code is
included in the Phrasal (Cer et al., 2010) toolkit,
which is freely available.
</bodyText>
<page confidence="0.983814">
311
</page>
<note confidence="0.938904">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.952084" genericHeader="method">
2 Adaptive Online Algorithms
</sectionHeader>
<bodyText confidence="0.999498545454546">
Machine translation is an unusual machine learning
setting because multiple correct translations exist
and decoding is comparatively expensive. When we
have a large feature set and therefore want to tune
on a large data set, batch methods are infeasible.
Online methods can converge faster, and in practice
they often find better solutions (Liang and Klein,
2009; Bottou and Bousquet, 2011, inter alia).
Recall that stochastic gradient descent (SGD),
a fundamental online method, updates weights w
according to
</bodyText>
<equation confidence="0.995058">
wt = wt−1 − q∇ft(wt−1) (1)
</equation>
<bodyText confidence="0.999946916666667">
with loss function1 ft(w) of the tth example,
(sub)gradient of the loss with respect to the param-
eters ∇ft(wt−1), and learning rate q.
SGD is sensitive to the learning rate q, which is
difficult to set in an MT system that mixes frequent
“dense” features (like the language model) with
sparse features (e.g., for translation rules). Further-
more, q applies to each coordinate in the gradient,
an undesirable property in MT where good sparse
features may fire very infrequently. We would in-
stead like to take larger steps for sparse features and
smaller steps for dense features.
</bodyText>
<subsectionHeader confidence="0.894142">
2.1 AdaGrad
</subsectionHeader>
<bodyText confidence="0.9999714">
AdaGrad is a method for setting an adaptive learn-
ing rate that comes with good theoretical guaran-
tees. The theoretical improvement over SGD is
most significant for high-dimensional, sparse fea-
tures. AdaGrad makes the following update:
</bodyText>
<equation confidence="0.96344725">
112
wt = wt−1 − qΣt, ∇ft(wt−1) (2)
Σ−1
t = Σ−1
t−1 + ∇ft(wt−1)∇ft(wt−1)&gt;
t
� ∇fi(wi−1)∇fi(wi−1)&gt; (3)
i=1
</equation>
<bodyText confidence="0.956334166666667">
A diagonal approximation to Σ can be used for a
high-dimensional vector wt. In this case, AdaGrad
is simple to implement and computationally cheap.
Consider a single dimension j, and let scalars vt =
wt,j, gt = ∇jft(wt−1), Gt = Eti=1 g2i , then the
update rule is
</bodyText>
<equation confidence="0.973637285714286">
vt = vt−1 − q G−1/2
t gt (4)
Gt = Gt−1 + g2 (5)
t
Compared to SGD, we just need to store Gt = Σ−1
t,jj
for each dimension j.
</equation>
<footnote confidence="0.960485">
1We specify the loss function for MT in section 3.1.
</footnote>
<subsectionHeader confidence="0.987793">
2.2 Prior Online Algorithms in MT
</subsectionHeader>
<bodyText confidence="0.9935728">
AdaGrad is related to two previous online learning
methods for MT.
MIRA Chiang et al. (2008) described an adaption
of MIRA (Crammer et al., 2006) to MT. MIRA
makes the following update:
</bodyText>
<equation confidence="0.469173">
kw − wt−1k2 2 + ft(w) (6)
2q
</equation>
<bodyText confidence="0.998007307692308">
The first term expresses conservativity: the weight
should change as little as possible based on a sin-
gle example, ensuring that it is never beneficial to
overshoot the minimum.
The relationship to SGD can be seen by lineariz-
ing the loss function ft(w) ≈ ft(wt−1) + (w −
wt−1)&gt;∇ft(wt−1) and taking the derivative of (6).
The result is exactly (1).
AROW Chiang (2012) adapted AROW (Cram-
mer et al., 2009) to MT. AROW models the current
weight as a Gaussian centered at wt−1 with covari-
ance Σt−1, and does the following update upon
seeing training example xt:
</bodyText>
<equation confidence="0.99159">
wt, Σt =
arg min
w,Σ q
1 DKL(N(w, Σ)||N(wt−1, Σt−1))
+ ft (w) + 2q x&gt;t Σxt (7)
</equation>
<bodyText confidence="0.9911908">
The KL-divergence term expresses a more general,
directionally sensitive conservativity. Ignoring the
third term, the Σ that minimizes the KL is actu-
ally Σt−1. As a result, the first two terms of (7)
generalize MIRA so that we may be more conser-
vative in some directions specified by Σ. To see
this, we can write out the KL-divergence between
two Gaussians in closed form, and observe that the
terms involving w do not interact with the terms
involving Σ:
</bodyText>
<equation confidence="0.993383625">
wt = arg min
w 2q (w − wt−1)&gt;Σ−1
1 t−1(w − wt−1)
+ ft(w) (8)
logI  |IΣ−|1 |I + 2q ti1Σ)
1
+ x&gt; t Σxt (9)
2q
</equation>
<bodyText confidence="0.989507333333333">
The third term in (7), called the confidence term,
gives us adaptivity, the notion that we should have
smaller variance in the direction v as more data xt
</bodyText>
<equation confidence="0.949794714285714">
wt = arg min
w
1
Σt = arg min
Σ
1
2q
</equation>
<page confidence="0.993843">
312
</page>
<bodyText confidence="0.999558142857143">
is seen in direction v. For example, if E is diagonal
and xt are indicator features, the confidence term
then says that the weight for a rarer feature should
have more variance and vice-versa. Recall that for
generalized linear models V`t(w) a xt; if we sub-
stitute xt = αtV`t(w) into (9), differentiate and
solve, we get:
</bodyText>
<equation confidence="0.918346833333333">
E−1
t = E−1
t−1 + xtx&gt;t
α2i V`i(wi−1)V`i(wi−1)&gt;
(10)
The precision E−1
</equation>
<bodyText confidence="0.998051571428571">
t generally grows as more data
is seen. Frequently updated features receive an espe-
cially high precision, whereas the model maintains
large variance for rarely seen features.
If we substitute (10) into (8), linearize the loss
`t(w) as before, and solve, then we have the lin-
earized AROW update
</bodyText>
<equation confidence="0.78777275">
wt = wt−1 − ηEtV`t(wt−1) (11)
which is also an adaptive update with per-coordinate
learning rates specified by Et (as opposed to E1/2
t
</equation>
<bodyText confidence="0.615406">
in AdaGrad).
</bodyText>
<subsectionHeader confidence="0.952455">
2.3 Comparing AdaGrad, MIRA, AROW
</subsectionHeader>
<bodyText confidence="0.695479">
Compare (3) to (10) and observe that if we set
</bodyText>
<equation confidence="0.9412125">
E−1
0 = 0 and αt = 1, then the only difference
</equation>
<bodyText confidence="0.9913888">
between the AROW update (11) and the AdaGrad
update (2) is a square root. Under a constant gradi-
ent, AROW decays the step size more aggressively
(1/t) compared to AdaGrad (1/Vt), and it is sensi-
tive to the specification of E−1
</bodyText>
<equation confidence="0.483766">
0 .
</equation>
<bodyText confidence="0.999000666666667">
Informally, SGD can be improved in the conser-
vativity direction using MIRA so the updates do
not overshoot. Second, SGD can be improved in
the adaptivity direction using AdaGrad where the
decaying stepsize is more robust and the adaptive
stepsize allows better weight updates to features
differing in sparsity and scale. Finally, AROW com-
bines both adaptivity and conservativity. For MT,
adaptivity allows us to deal with mixed dense/sparse
features effectively without specific normalization.
Why do we choose AdaGrad over AROW?
MIRA/AROW requires selecting the loss function
`(w) so that wt can be solved in closed-form, by
a quadratic program (QP), or in some other way
that is better than linearizing. This usually means
choosing a hinge loss. On the other hand, Ada-
Grad/linearized AROW only requires that the gradi-
ent of the loss function can be computed efficiently.
</bodyText>
<construct confidence="0.241242">
Algorithm 1 Adaptive online tuning for MT.
Require: Tuning set {fi, e1:k
i }i=1:M
</construct>
<listItem confidence="0.845915444444444">
1: Set w0 = 0
2: Set t = 1
3: repeat
4: for i in 1 ... M in random order do
5: Decode n-best list Ni for fi
6: Sample pairs {dj,+, dj,−}j=1:s from Ni
7: Compute Dt = {φ(dj,+) − φ(dj,−)}j=1:s
8: Set gt = ∇f(Dt; wt−1)}
9: Set Σ−1
</listItem>
<equation confidence="0.988327">
t = Σ−1
t−1 + gtg&gt; &gt; Eq. (3)
t
10: Update wt = wt−1 − ηΣ1/2
t gt &gt; Eq. (2)
</equation>
<listItem confidence="0.6389415">
11: Regularize wt &gt; Eq. (15)
12: Set t = t + 1
13: end for
14: until convergence
</listItem>
<bodyText confidence="0.9990318">
Linearized AROW, however, is less robust than Ada-
Grad empirically2 and lacks known theoretical guar-
antees. Finally, by using AdaGrad, we separate
adaptivity from conservativity. Our experiments
suggest that adaptivity is actually more important.
</bodyText>
<sectionHeader confidence="0.992459" genericHeader="method">
3 Adaptive Online MT
</sectionHeader>
<bodyText confidence="0.9997618">
Algorithm 1 shows the full algorithm introduced in
this paper. AdaGrad (lines 9–10) is a crucial piece,
but the loss function, regularization technique, and
parallelization strategy described in this section are
equally important in the MT setting.
</bodyText>
<subsectionHeader confidence="0.99953">
3.1 Pairwise Logistic Loss Function
</subsectionHeader>
<bodyText confidence="0.999952176470588">
Algorithm 1 lines 5–8 describe the gradient com-
putation. We cast MT tuning as pairwise ranking
(Herbrich et al., 1999, inter alia), which Hopkins
and May (2011) applied to MT. The pairwise ap-
proach results in simple, convex loss functions suit-
able for online learning. The idea is that for any
two derivations, the ranking predicted by the model
should be consistent with the ranking predicted by
a gold sentence-level metric G like BLEU+1 (Lin
and Och, 2004).
Consider a single source sentence f with asso-
ciated references e1:k. Let d be a derivation in an
n-best list of f that has the target e = e(d) and the
feature map φ(d). Let M(d) = w · φ(d) be the
model score. For any derivation d+ that is better
than d− under G, we desire pairwise agreement
such that
</bodyText>
<equation confidence="0.96835575">
G (e(d+)� e1:k) &gt; G (e(d−), e1:k)
�
l J1 M(d+) &gt; M(d−)
2According to experiments not reported in this paper.
t
i=1
= E−1
0 +
</equation>
<page confidence="0.984382">
313
</page>
<bodyText confidence="0.895305666666667">
Ensuring pairwise agreement is the same as ensur-
ing w · [φ(d+) − φ(d_)] &gt; 0.
For learning, we need to select derivation pairs
(d+, d_) to compute difference vectors x+ =
φ(d+) − φ(d_). Then we have a 1-class separa-
tion problem trying to ensure w · x+ &gt; 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011).
We compute difference vectors Dt = {x1:s
+ } (Al-
gorithm 1 line 7) from s pairs (d+, d_) for source
sentence ft. We use the familiar logistic loss:
</bodyText>
<equation confidence="0.97379525">
1
log
1 + e_w&apos;x+
(12)
</equation>
<bodyText confidence="0.999164454545455">
Choosing the hinge loss instead of the logistic
loss results in the 1-class SVM problem. The 1-
class separation problem is equivalent to the binary
classification problem with x+ = φ(d+) − φ(d_)
as positive data and x_ = −x+ as negative data,
which may be plugged into an existing logistic re-
gression solver.
We find that Algorithm 1 works best with mini-
batches instead of single examples. In line 4 we
simply partition the tuning set so that i becomes a
mini-batch of examples.
</bodyText>
<subsectionHeader confidence="0.999877">
3.2 Updating and Regularization
</subsectionHeader>
<bodyText confidence="0.995902">
Algorithm 1 lines 9–11 compute the adaptive learn-
ing rate, update the weights, and apply regulariza-
tion. Section 2.1 explained the AdaGrad learn-
ing rate computation. To update and regularize
the weights we apply the Forward-Backward Split-
ting (FOBOS) (Duchi and Singer, 2009) framework,
which separates the two operations. The two-step
FOBOS update is
</bodyText>
<equation confidence="0.998867666666667">
wt_1 = wt_1 − ηt_1V`t_1 (wt_1) (13)
2
(14)
</equation>
<bodyText confidence="0.8932598">
where (13) is just an unregularized gradient descent
step and (14) balances the regularization term r(w)
with staying close to the gradient step.
Equation (14) permits efficient L1 regulariza-
tion, which is well-suited for selecting good features
from exponentially many irrelevant features (Ng,
2004). It is well-known that feature selection is very
important for feature-rich MT. For example, sim-
ple indicator features like lexicalized re-ordering
classes are potentially useful yet bloat the the fea-
ture set and, in the worst case, can negatively impact
Algorithm 2 “Stale gradient” parallelization
method for Algorithm 1.
Require: Tuning set {fi, e1:k
i }i=1:M
</bodyText>
<listItem confidence="0.94296425">
1: Initialize threadpool p1, ... ,pj
2: Set t = 1
3: repeat
4: for i in 1 ... M in random order do
5: Wait until any thread p is idle
6: Send (fi, e1:k i, t) to p &gt; Alg. 1 lines 5–8
7: while ∃ p� done with gradient gt, do &gt; t&apos; ≤ t
8: Update wt = wt−1 − ηgt, &gt; Alg. 1 lines 9–11
9: Sett = t + 1
10: end while
11: end for
12: until convergence
</listItem>
<bodyText confidence="0.990439571428571">
search. Some of the features generalize, but many
do not. This was well understood in previous work,
so heuristic filtering was usually applied (Chiang
et al., 2009, inter alia). In contrast, we need only
select an appropriate regularization strength λ.
Specifically, when r(w) = λ11w111, the closed-
form solution to (14) is
</bodyText>
<equation confidence="0.99909">
[ ]
wt = sign(wt_2 1 ) |wt_ 2 1  |− ηt_1λ + (15)
</equation>
<bodyText confidence="0.999877846153846">
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls
below the threshold ηt_1λ. It is straightforward to
adapt this to AdaGrad with diagonal E by setting
each dimension of ηt_1,j = ηE
element-wise products.
We find that V`t_1(wt_1) only involves several
hundred active features for the current example
(or mini-batch). However, naively following the
FOBOS framework requires updating millions of
weights. But a practical benefit of FOBOS is that
we can do lazy updates on just the active dimensions
without any approximations.
</bodyText>
<subsectionHeader confidence="0.995576">
3.3 Parallelization
</subsectionHeader>
<bodyText confidence="0.999706076923077">
Algorithm 1 is inherently sequential like standard
online learning. This is undesirable in MT where
decoding is costly. We therefore parallelize the algo-
rithm with the “stale gradient” method of Langford
et al. (2009) (Algorithm 2). A fixed threadpool of
workers computes gradients in parallel and sends
them to a master thread, which updates a central
weight vector. Crucially, the weight updates need
not be applied in order, so synchronization is unnec-
essary; the workers only idle at the end of an epoch.
The consequence is that the update in line 8 of Al-
gorithm 2 is with respect to gradient gt� with t&apos; &lt; t.
Langford et al. (2009) gave convergence results for
</bodyText>
<equation confidence="0.8906915">
2 11w − wt_211122 + ηt_1r(w)
wt = arg min
1 w
`t(w) = `(Dt, w) = − �
x+EDt
1
t,jj and by taking
2
</equation>
<page confidence="0.989455">
314
</page>
<bodyText confidence="0.999979222222222">
stale updating, but the bounds do not apply to our
setting since we use L1 regularization. Neverthe-
less, Gimpel et al. (2010) applied this framework
to other non-convex objectives and obtained good
empirical results.
Our asynchronous, stochastic method has practi-
cal appeal for MT. During a tuning run, the online
method decodes the tuning set under many more
weight vectors than a MERT-style batch method.
This characteristic may result in broader exploration
of the search space, and make the learner more ro-
bust to local optima local optima (Liang and Klein,
2009; Bottou and Bousquet, 2011, inter alia). The
adaptive algorithm identifies appropriate learning
rates for the mixture of dense and sparse features.
Finally, large data structures such as the language
model (LM) and phrase table exist in shared mem-
ory, obviating the need for remote queries.
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999937533333333">
We built Arabic-English and Chinese-English MT
systems with Phrasal (Cer et al., 2010), a phrase-
based system based on alignment templates (Och
and Ney, 2004). The corpora3 in our experiments
(Table 1) derive from several LDC sources from
2012 and earlier. We de-duplicated each bitext ac-
cording to exact string match, and ensured that no
overlap existed with the test sets. We produced
alignments with the Berkeley aligner (Liang et al.,
2006b) with standard settings and symmetrized via
the grow-diag heuristic.
For each language we used SRILM (Stolcke,
2002) to estimate 5-gram LMs with modified
Kneser-Ney smoothing. We included the monolin-
gual English data and the respective target bitexts.
</bodyText>
<subsectionHeader confidence="0.98971">
4.1 Feature Templates
</subsectionHeader>
<bodyText confidence="0.999750142857143">
The baseline “dense” model contains 19 features:
the nine Moses baseline features, the hierarchical
lexicalized re-ordering model of Galley and Man-
ning (2008), the (log) count of each rule, and an
indicator for unique rules.
To the dense features we add three high di-
mensional “sparse” feature sets. Discrimina-
</bodyText>
<footnote confidence="0.97357225">
3We tokenized the English with packages from the Stan-
ford Parser (Klein and Manning, 2003) according to the Penn
Treebank standard (Marcus et al., 1993), the Arabic with the
Stanford Arabic segmenter (Green and DeNero, 2012) accord-
ing to the Penn Arabic Treebank standard (Maamouri et al.,
2008), and the Chinese with the Stanford Chinese segmenter
(Chang et al., 2008) according to the Penn Chinese Treebank
standard (Xue et al., 2005).
</footnote>
<table confidence="0.9676235">
Bilingual Monolingual
Sentences Tokens Tokens
Ar-En 6.6M 375M 990M
Zh-En 9.3M 538M
</table>
<tableCaption confidence="0.990878">
Table 1: Bilingual and monolingual corpora used
</tableCaption>
<bodyText confidence="0.997568">
in these experiments. The monolingual English
data comes from the AFP and Xinhua sections of
English Gigaword 4 (LDC2009T13).
tive phrase table (PT): indicators for each rule
in the phrase table. Alignments (AL): indica-
tors for phrase-internal alignments and deleted
(unaligned) source words. Discriminative re-
ordering (LO): indicators for eight lexicalized re-
ordering classes, including the six standard mono-
tone/swap/discontinuous classes plus the two sim-
pler Moses monotone/non-monotone classes.
</bodyText>
<subsectionHeader confidence="0.988953">
4.2 Tuning Algorithms
</subsectionHeader>
<bodyText confidence="0.9999475">
The primary baseline is the dense feature set tuned
with MERT (Och, 2003). The Phrasal implemen-
tation uses the line search algorithm of Cer et al.
(2008), uniform initialization, and 20 random start-
ing points.4 We tuned according to BLEU-4 (Pap-
ineni et al., 2002).
We built high dimensional baselines with two dif-
ferent algorithms. First, we tuned with batch PRO
using the default settings in Phrasal (L2 regulariza-
tion with u=0.1). Second, we ran the k-best batch
MIRA (kb-MIRA) (Cherry and Foster, 2012) imple-
mentation in Moses. We did implement an online
version of MIRA, and in small-scale experiments
found that the batch variant worked just as well.
Cherry and Foster (2012) reported the same result,
and their implementation is available in Moses. We
ran their code with standard settings.
Moses5 also contains the discriminative phrase
table implementation of (Hasler et al., 2012b),
which is identical to our implementation using
Phrasal. Moses and Phrasal accept the same phrase
table and LM formats, so we kept those data struc-
tures in common. The two decoders also use the
same multi-stack beam search (Och and Ney, 2004).
For our method, we used uniform initialization,
16 threads, and a mini-batch size of 20. We found
that η=0.02 and A=0.1 worked well on development
sets for both languages. To compute the gradients
</bodyText>
<footnote confidence="0.997985">
4Other system settings for all experiments: distortion limit
of 5, a maximum phrase length of 7, and an n-best size of 200.
5v1.0 (28 January 2013)
</footnote>
<page confidence="0.996109">
315
</page>
<table confidence="0.999687692307692">
Model #features Algorithm Tuning Set MT02 MT03 MT04 MT09
Dense 19 MERT MT06 45.08 51.32 52.26 51.42 48.44
Dense 19 This paper MT06 44.19 51.42 52.52 50.16 48.13
+PT 151k kb-MIRA MT06 42.08 47.25 48.98 47.08 45.64
+PT 23k PRO MT06 44.31 51.06 52.18 50.23 47.52
+PT 50k This paper MT06 50.61 51.71 52.89 50.42 48.74
+PT+AL+LO 109k PRO MT06 44.87 51.25 52.43 50.05 47.76
+PT+AL+LO 242k This paper MT06 57.84 52.45 53.18 51.38 49.37
Dense 19 MERT MT05/6/8 49.63 51.60 52.29 51.73 48.68
+PT+AL+LO 390k This paper MT05/6/8 58.20 53.61 54.99 52.79 49.94
(Chiang, 2012)* 10-20k MIRA MT04/6 – – – – 45.90
(Chiang, 2012)* 10-20k AROW MT04/6 – – – – 47.60
#sentences 728 663 1,075 1,313
</table>
<tableCaption confidence="0.9915538">
Table 2: Ar-En results [BLEU-4 % uncased] for the NIST tuning experiment. The tuning and test sets
each have four references. MT06 has 1,717 sentences, while the concatenated MT05/6/8 set has 4,213
sentences. Bold indicates statistical significance relative to the best baseline in each block at p &lt; 0.001;
bold-italic at p &lt; 0.05. We assessed significance with the permutation test of Riezler and Maxwell (2005).
(*) Chiang (2012) used a similar-sized bitext, but two LMs trained on twice as much monolingual data.
</tableCaption>
<table confidence="0.999723454545455">
Model #features Algorithm Tuning Set MT02 MT03 MT04
Dense 19 MERT MT06 33.90 35.72 33.71 34.26
Dense 19 This paper MT06 32.60 36.23 35.14 34.78
+PT 105k kb-MIRA MT06 29.46 30.67 28.96 30.05
+PT 26k PRO MT06 33.70 36.87 34.62 34.80
+PT 66k This paper MT06 33.90 36.09 34.86 34.73
+PT+AL+LO 148k PRO MT06 34.81 36.31 33.81 34.41
+PT+AL+LO 344k This paper MT06 38.99 36.40 35.07 34.84
Dense 19 MERT MT05/6/8 32.36 35.69 33.83 34.33
+PT+AL+LO 487k This paper MT05/6/8 37.64 37.81 36.26 36.15
#sentences 878 919 1,597
</table>
<tableCaption confidence="0.801766">
Table 3: Zh-En results [BLEU-4 % uncased] for the NIST tuning experiment. MT05/6/8 has 4,103
sentences. OpenMT 2009 did not include Zh-En, hence the asymmetry with Table 2.
we sampled 15 derivation pairs for each tuning ex-
ample and scored them with BLEU+1.
</tableCaption>
<subsectionHeader confidence="0.956312">
4.3 NIST OpenMT Experiment
</subsectionHeader>
<bodyText confidence="0.999552363636364">
The first experiment evaluates our algorithm when
tuning and testing on standard test sets, each with
four references. When we add features, our algo-
rithm tends to overfit to a standard-sized tuning set
like MT06. We thus concatenated MT05, MT06,
and MT08 to create a larger tuning set.
Table 2 shows the Ar-En results. Our algorithm
is competitive with MERT in the low dimensional
“dense” setting, and compares favorably to PRO
with the PT feature set. PRO does not benefit
from additional features, whereas our algorithm im-
proves with both additional features and data. The
underperformance of kb-MIRA may result from
a difference between Moses and Phrasal: Moses
MERT achieves only 45.62 on MT09. Moses PRO
with the PT feature set is slightly worse, e.g., 44.52
on MT09. Nevertheless, kb-MIRA does not im-
prove significantly over MERT, and also selects an
unnecessarily large model.
The full feature set PT+AL+LO does help. With
the PT feature set alone, our algorithm tuned on
MT05/6/8 scores well below the best model, e.g.
</bodyText>
<page confidence="0.998108">
316
</page>
<table confidence="0.9981366">
Model #features Algorithm Tuning Set #refs bitext5k-test MT04
Dense 19 MERT MT06 45.08 4 39.28 51.42
+PT 72k This paper MT05/6/8 51.29 4 39.50 50.60
+PT 79k This paper bitext5k 44.79 1 43.85 45.73
+PT+AL+LO 647k This paper bitext15k 45.68 1 43.93 45.24
</table>
<tableCaption confidence="0.9902895">
Table 4: Ar-En results [BLEU-4 % uncased] for the bitext tuning experiment. Statistical significance is
relative to the Dense baseline. We include MT04 for comparison to the NIST genre.
</tableCaption>
<table confidence="0.9993754">
Model #features Algorithm Tuning Set #refs bitext5k-test MT04
Dense 19 MERT MT06 33.90 4 33.44 34.26
+PT 97k This paper MT05/6/8 34.45 4 35.08 35.19
+PT 67k This paper bitext5k 36.26 1 36.01 33.76
+PT+AL+LO 536k This paper bitext15k 37.57 1 36.30 34.05
</table>
<tableCaption confidence="0.999477">
Table 5: Zh-En results [BLEU-4 % uncased] for the bitext tuning experiment.
</tableCaption>
<bodyText confidence="0.999820647058823">
48.56 BLEU on MT09. For Ar-En, our algorithm
thus has the desirable property of benefiting from
more and better features, and more data.
Table 3 shows Zh-En results. Somewhat sur-
prisingly our algorithm improves over MERT in
the dense setting. When we add the discrimina-
tive phrase table, our algorithm improves over kb-
MIRA, and over batch PRO on two evaluation sets.
With all features and the MT05/6/8 tuning set, we
improve significantly over all other models. PRO
learns a smaller model with the PT+AL+LO fea-
ture set which is surprising given that it applies L2
regularization (AdaGrad uses L1). We speculate
that this may be an consequence of stochastic learn-
ing. Our algorithm decodes each example with
a new weight vector, thus exploring more of the
search space for the same tuning set.
</bodyText>
<subsectionHeader confidence="0.997464">
4.4 Bitext Tuning Experiment
</subsectionHeader>
<bodyText confidence="0.999965642857143">
Tables 2 and 3 show that adding tuning examples
improves translation quality. Nevertheless, even
the larger tuning set is small relative to the bitext
from which rules were extracted. He and Deng
(2012) and Simianer et al. (2012) showed significant
translation quality gains by tuning on the bitext.
However, their bitexts matched the genre of their
test sets. Our bitexts, like those of most large-scale
systems, do not. Domain mismatch matters for the
dense feature set (Haddow and Koehn, 2012). We
show that it also matters for feature-rich MT.
Before aligning each bitext, we randomly sam-
pled and sequestered 5k and 15k sentence tuning
sets, and a 5k test set. We prevented overlap be-
</bodyText>
<table confidence="0.999262285714286">
DA DB |A ||B ||A n B|
MT04 MT06 70k 72k 5.9k
MT04 MT568 70k 96k 7.6k
MT04 bitext5k 70k 67k 4.4k
MT04 bitext15k 70k 310k 10.5k
5ktest bitext5k 82k 67k 5.6k
5ktest bitext15k 82k 310k 14k
</table>
<tableCaption confidence="0.981128">
Table 6: Number of overlapping phrase table (+PT)
features on various Zh-En dataset pairs.
</tableCaption>
<bodyText confidence="0.9987511">
tween the tuning sets and the test set. We then
tuned a dense model with MERT on MT06, and
feature-rich models on both MT05/6/8 and the bi-
text tuning set. Table 4 shows the Ar-En results.
When tuned on bitext5k the translation quality gains
are significant for bitext5k-test relative to tuning on
MT05/6/8, which has multiple references. However,
the bitext5k models do not generalize as well to the
NIST evaluation sets as represented by the MT04
result. Table 5 shows similar trends for Zh-En.
</bodyText>
<sectionHeader confidence="0.996233" genericHeader="method">
5 Analysis
</sectionHeader>
<subsectionHeader confidence="0.999671">
5.1 Feature Overlap Analysis
</subsectionHeader>
<bodyText confidence="0.999969714285714">
How many sparse features appear in both the tun-
ing and test sets? In Table 6, A is the set of phrase
table features that received a non-zero weight when
tuned on dataset DA (same for B). Column DA lists
several Zh-En test sets used and column DB lists
tuning sets. Our experiments showed that tuning
on MT06 generalizes better to MT04 than tuning
</bodyText>
<page confidence="0.996114">
317
</page>
<bodyText confidence="0.999959272727273">
on bitext5k, whereas tuning on bitext5k general-
izes better to bitext5k-test than tuning on MT06.
These trends are consistent with the level of fea-
ture overlap. Phrase table features in A ∩ B are
overwhelmingly short, simple, and correct phrases,
suggesting Li regularization is effective for feature
selection. It is also important to balance the number
of features with how well weights can be learned
for those features, as tuning on bitext15k produced
higher coverage for MT04 but worse generalization
than tuning on MT06.
</bodyText>
<subsectionHeader confidence="0.999257">
5.2 Domain Adaptation Analysis
</subsectionHeader>
<bodyText confidence="0.994279733333333">
To understand the domain adaptation issue we com-
pared the non-zero weights in the discriminative
phrase table (PT) for Ar-En models tuned on bi-
text5k and MT05/6/8. Table 7 illustrates a statisti-
cal idiosyncrasy in the data for the American and
British spellings of program/programme. The mass
is concentrated along the diagonal, probably be-
cause MT05/6/8 was prepared by NIST, an Amer-
ican agency, while the bitext was collected from
many sources including Agence France Presse.
Of course, this discrepancy is consequential for
both dense and feature-rich models. However, we
observe that the feature-rich models fit the tuning
data more closely. For example, the MT05/6/8
model learns rules like l.×A�KQK. �Ò .e�JK� → program
includes, l.×A�KQK. → program of, and l.×A�KQ�.Ë@ K kiA�K →
program window. Crucially, it does not learn the
basic rule l.×A�KQK. → program.
In contrast, the bitext5k model contains ba-
sic rules such l.×A�KQK. → programme, l.×A�KQ�.Ë@ @AA
→ this programme, and l.×A�KQ�.Ë@ 1/2Ë X� → that pro-
gramme. It also contains more elaborate rules such
as l.×A�KQ�.Ë@ :,A�® �K → programme expenses
were and aËñë�AÖÏ@ �éJ
KA �’�®Ë@ uCgQË@ l.×@QK. → manned
space flight programmes. We observed similar
trends for ‘defense/defence’, ‘analyze/analyse’, etc.
This particular genre problem could be addressed
with language-specific pre-processing, but our sys-
tem solves it in a data-driven manner.
</bodyText>
<subsectionHeader confidence="0.998635">
5.3 Re-ordering Analysis
</subsectionHeader>
<bodyText confidence="0.839645037037037">
We also analyzed re-ordering differences. Arabic
matrix clauses tend to be verb-initial, meaning that
the subject and verb must be swapped when translat-
ing to English. To assess re-ordering differences—
if any—between the dense and feature-rich models,
we selected all MT09 segments that began with one
programme
program
PT rules w/ programme
PT rules w/ program
Table 7: Top: comparison of token counts in two
Ar-En tuning sets for programme and program. Bot-
tom: rule counts in the discriminative phrase table
(PT) for models tuned on the two tuning sets. Both
spellings correspond to the Arabic l.×A KQK..
of seven common verbs: ÈA�¯ qaal ‘said’, hQå• SrH
`declared&apos;,PA �ƒ �@ ashaar ‘indicated’, JA¿kaan `was&apos;,
�
Q»� dhkr ‘commented’, ¬A�o�@ aDaaf ‘added’, �Ê«@
acln ‘announced’. We compared the output of the
MERT Dense model to our method with the full
feature set, both tuned on MT06. Of the 208 source
segments, 32 of the translation pairs contained dif-
ferent word order in the matrix clause. Our feature-
rich model was correct 18 times (56.3%), Dense
was correct 4 times (12.5%), and neither method
was correct 10 times (31.3%).
</bodyText>
<listItem confidence="0.987033444444445">
(1) ref: lebanese prime minister , fuad siniora ,
announced
a. and lebanese prime minister fuad siniora
that
b. the lebanese prime minister fouad siniora
announced
(2) ref: the newspaper and television reported
a. she said the newspaper and television
b. television and newspaper said
</listItem>
<bodyText confidence="0.999820166666667">
In (1) the dense model (1a) drops the verb while the
feature-rich model correctly re-orders and inserts
it after the subject (1b). The coordinated subject
in (2) becomes an embedded subject in the dense
output (2a). The feature-rich model (2b) performs
the correct re-ordering.
</bodyText>
<subsectionHeader confidence="0.99412">
5.4 Runtime Comparison
</subsectionHeader>
<bodyText confidence="0.999936833333333">
Table 8 compares our method to standard implemen-
tations of the other algorithms. MERT parallelizes
easily but runtime increases quadratically with n-
best list size. PRO runs (single-threaded) L-BFGS
to convergence on every epoch, a potentially slow
procedure for the larger feature set. Moreover, both
</bodyText>
<figure confidence="0.8582194">
# bitext5k # MT05/6/8
185 0
19 449
353 79
9 31
</figure>
<page confidence="0.888283">
318
</page>
<table confidence="0.997951857142857">
epochs min.
MERT Dense 22 180
PRO +PT 25 35
kb-MIRA* +PT 26 25
This paper +PT 10 10
PRO +PT+AL+LO 13 150
This paper +PT+AL+LO 5 15
</table>
<tableCaption confidence="0.980052">
Table 8: Epochs to convergence (“epochs”) and
</tableCaption>
<bodyText confidence="0.9910317">
approximate runtime per epoch in minutes (“min.”)
for selected Zh-En experiments tuned on MT06.
All runs executed on the same dedicated system
with the same number of threads. (*) Moses and
kb-MIRA are written in C++, while all other rows
refer to Java implementations in Phrasal.
the Phrasal and Moses PRO implementations use
L2 regularization, which regularizes every weight
on every update. kb-MIRA makes multiple passes
through the n-best lists during each epoch. The
Moses implementation parallelizes decoding but
weight updating is sequential.
The core of our method is an inner product be-
tween the adaptive learning rate vector and the gra-
dient. This is easy to implement and is very fast
even for large feature sets. Since we applied lazy
regularization, this inner product usually involves
hundred-dimensional vectors. Finally, our method
does not need to accumulate n-best lists, a practice
that slows down the other algorithms.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999916510638298">
Our work relates most closely to that of Hasler et al.
(2012b), who tuned models containing both sparse
and dense features with Moses. A discriminative
phrase table helped them improve slightly over a
dense, online MIRA baseline, but their best results
required initialization with MERT-tuned weights
and re-tuning a single, shared weight for the dis-
criminative phrase table with MERT. In contrast,
our algorithm learned good high dimensional mod-
els from a uniform starting point.
Chiang (2012) adapted AROW to MT and ex-
tended previous work on online MIRA (Chiang et
al., 2008; Watanabe et al., 2007). It was not clear if
his improvements came from the novel Hope/Fear
search, the conservativity gain from MIRA/AROW
by solving the QP exactly, adaptivity, or sophis-
ticated parallelization. In contrast, we show that
AdaGrad, which ignores conservativity and only
capturing adaptivity, is sufficient.
Simianer et al. (2012) investigated SGD with a
pairwise perceptron objective. Their best algorithm
used iterative parameter mixing (McDonald et al.,
2010), which we found to be slower than the stale
gradient method in section 3.3. They regularized
once at the end of each epoch, whereas we regular-
ized each weight update. An empirical comparison
of these two strategies would be an interesting fu-
ture contribution.
Watanabe (2012) investigated SGD and even ran-
domly selected pairwise samples as we did. He
considered both softmax and hinge losses, observ-
ing better results with the latter, which solves a QP.
Their parallelization strategy required a line search
at the end of each epoch.
Many other discriminative techniques have been
proposed based on: ramp loss (Gimpel, 2012);
hinge loss (Cherry and Foster, 2012; Haddow et
al., 2011; Arun and Koehn, 2007); maximum en-
tropy (Xiang and Ittycheriah, 2011; Ittycheriah and
Roukos, 2007; Och and Ney, 2002); perceptron
(Liang et al., 2006a); and structured SVM (Till-
mann and Zhang, 2006). These works use radically
different experimental setups, and to our knowl-
edge only (Cherry and Foster, 2012) and this work
compare to at least two high dimensional baselines.
Broader comparisons, though time-intensive, could
help differentiate these methods.
</bodyText>
<sectionHeader confidence="0.983906" genericHeader="conclusions">
7 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.977929714285714">
We introduced a new online method for tuning
feature-rich translation models. The method is
faster per epoch than MERT, scales to millions of
features, and converges quickly. We used efficient
L1 regularization for feature selection, obviating
the need for the feature scaling and heuristic filter-
ing common in prior work. Those comfortable with
implementing vanilla SGD should find our method
easy to implement. Even basic discriminative fea-
tures were effective, so we believe that our work
enables fresh approaches to more sophisticated MT
feature engineering.
Acknowledgments We thank John DeNero for helpful com-
ments on an earlier draft. The first author is supported by a
National Science Foundation Graduate Research Fellowship.
We also acknowledge the support of the Defense Advanced
Research Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM. Any opin-
ions, findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily
reflect the view of the DARPA or the US government.
</bodyText>
<page confidence="0.998822">
319
</page>
<sectionHeader confidence="0.982212" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995250989583334">
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In MT Summit XI.
L. Bottou and O. Bousquet. 2011. The tradeoffs of
large scale learning. In Optimization for Machine
Learning, pages 351–368. MIT Press.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regu-
larization and search for minimum error rate training.
In WMT.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
P-C. Chang, M. Galley, and C. D. Manning. 2008.
Optimizing Chinese word segmentation for machine
translation performance. In WMT.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In EMNLP.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
HLT-NAACL.
D. Chiang. 2012. Hope and fear for discrimina-
tive training of statistical translation models. JMLR,
13:1159–1187.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551–585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adap-
tive regularization of weight vectors. In NIPS.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899–2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121–2159.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In HLT-
NAACL.
K. Gimpel, D. Das, and N. A. Smith. 2010. Distributed
asynchronous online learning for natural language
processing. In CoNLL.
K. Gimpel. 2012. Discriminative Feature-Rich Mod-
eling for Syntax-Based Machine Translation. Ph.D.
thesis, Language Technologies Institute, Carnegie
Mellon University.
S. Green and J. DeNero. 2012. A class-based agree-
ment model for generating accurately inflected trans-
lations. In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect
of out-of-domain data on SMT systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleR-
ank training for phrase-based machine translation. In
WMT.
E. Hasler, P. Bell, A. Ghoshal, B. Haddow, P. Koehn,
F. McInnes, et al. 2012a. The UEDIN systems for
the IWSLT 2012 evaluation. In IWSLT.
E. Hasler, B. Haddow, and P. Koehn. 2012b. Sparse
lexicalised features and topic adaptation for SMT. In
IWSLT.
X. He and L. Deng. 2012. Maximum expected BLEU
training of phrase and lexicon translation models. In
ACL.
R. Herbrich, T. Graepel, and K. Obermayer. 1999.
Support vector learning for ordinal regression. In
ICANN.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In HLT-NAACL.
D. Klein and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
J. Langford, A. J. Smola, and M. Zinkevich. 2009.
Slow learners are fast. In NIPS.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In HLT-NAACL.
P. Liang, A. Bouchard-Côté, D. Klein, and B. Taskar.
2006a. An end-to-end discriminative approach to
machine translation. In ACL.
P. Liang, B. Taskar, and D. Klein. 2006b. Alignment
by agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanc-
ing the Arabic Treebank: A collaborative effort to-
ward new annotation guidelines. In LREC.
</reference>
<page confidence="0.973462">
320
</page>
<reference confidence="0.999945826086957">
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313–330.
R. McDonald, K. Hall, and G. Mann. 2010. Distributed
training strategies for the structured perceptron. In
NAACL-HLT.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-
ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417–449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization (MTSE).
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-
scale discriminative training in SMT. In ACL.
A Stolcke. 2002. SRILM—an extensible language
modeling toolkit. In ICSLP.
C. Tillmann and T. Zhang. 2006. A discriminative
global training algorithm for statistical MT. In ACL-
COLING.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In EMNLP-CoNLL.
T. Watanabe. 2012. Optimized online rank learning
for machine translation. In HLT-NAACL. Associa-
tion for Computational Linguistics.
B. Xiang and A. Ittycheriah. 2011. Discriminative
feature-tied mixture modeling for statistical machine
translation. In ACL-HLT.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207–238.
</reference>
<page confidence="0.998763">
321
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975483">
<title confidence="0.999705">Fast and Adaptive Online Training of Feature-Rich Translation Models</title>
<author confidence="0.99912">Sida Wang Green</author>
<author confidence="0.99912">Daniel Cer</author>
<author confidence="0.99912">D Manning</author>
<affiliation confidence="0.999926">Computer Science Department, Stanford University</affiliation>
<email confidence="0.999064">spenceg@stanford.edu</email>
<email confidence="0.999064">sidaw@stanford.edu</email>
<email confidence="0.999064">danielcer@stanford.edu</email>
<email confidence="0.999064">manning@stanford.edu</email>
<abstract confidence="0.998851347826087">We present a fast and scalable online method for tuning statistical machine translation models with large feature sets. The standard tuning algorithm—MERT—only scales to tens of features. Recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant translation quality gains by exploiting sparse features. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arun</author>
<author>P Koehn</author>
</authors>
<title>Online learning methods for discriminative training of phrase based statistical machine translation.</title>
<date>2007</date>
<booktitle>In MT Summit XI.</booktitle>
<contexts>
<context position="33836" citStr="Arun and Koehn, 2007" startWordPosition="5632" endWordPosition="5635">ularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than MERT, scales to million</context>
</contexts>
<marker>Arun, Koehn, 2007</marker>
<rawString>A. Arun and P. Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. In MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bottou</author>
<author>O Bousquet</author>
</authors>
<title>The tradeoffs of large scale learning.</title>
<date>2011</date>
<booktitle>In Optimization for Machine Learning,</booktitle>
<pages>351--368</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4656" citStr="Bottou and Bousquet, 2011" startWordPosition="695" endWordPosition="698">eely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Adaptive Online Algorithms Machine translation is an unusual machine learning setting because multiple correct translations exist and decoding is comparatively expensive. When we have a large feature set and therefore want to tune on a large data set, batch methods are infeasible. Online methods can converge faster, and in practice they often find better solutions (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). Recall that stochastic gradient descent (SGD), a fundamental online method, updates weights w according to wt = wt−1 − q∇ft(wt−1) (1) with loss function1 ft(w) of the tth example, (sub)gradient of the loss with respect to the parameters ∇ft(wt−1), and learning rate q. SGD is sensitive to the learning rate q, which is difficult to set in an MT system that mixes frequent “dense” features (like the language model) with sparse features (e.g., for translation rules). Furthermore, q applies to each coordinate in the gradient, an undesirable property in MT where good sparse features ma</context>
<context position="16542" citStr="Bottou and Bousquet, 2011" startWordPosition="2819" endWordPosition="2822">+EDt 1 t,jj and by taking 2 314 stale updating, but the bounds do not apply to our setting since we use L1 regularization. Nevertheless, Gimpel et al. (2010) applied this framework to other non-convex objectives and obtained good empirical results. Our asynchronous, stochastic method has practical appeal for MT. During a tuning run, the online method decodes the tuning set under many more weight vectors than a MERT-style batch method. This characteristic may result in broader exploration of the search space, and make the learner more robust to local optima local optima (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). The adaptive algorithm identifies appropriate learning rates for the mixture of dense and sparse features. Finally, large data structures such as the language model (LM) and phrase table exist in shared memory, obviating the need for remote queries. 4 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and</context>
</contexts>
<marker>Bottou, Bousquet, 2011</marker>
<rawString>L. Bottou and O. Bousquet. 2011. The tradeoffs of large scale learning. In Optimization for Machine Learning, pages 351–368. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Regularization and search for minimum error rate training.</title>
<date>2008</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="19092" citStr="Cer et al. (2008)" startWordPosition="3212" endWordPosition="3215">from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments and deleted (unaligned) source words. Discriminative reordering (LO): indicators for eight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4.2 Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with u=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses</context>
</contexts>
<marker>Cer, Jurafsky, Manning, 2008</marker>
<rawString>D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regularization and search for minimum error rate training. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>M Galley</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Phrasal: A statistical machine translation toolkit for exploring new model features. In HLTNAACL, Demonstration Session.</title>
<date>2010</date>
<contexts>
<context position="4010" citStr="Cer et al., 2010" startWordPosition="601" endWordPosition="604">s. We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Adaptive Online Algorithms Machine translation is an unusual machine learning setting because multiple correct translations exist and decoding is comparatively expensive. When we have a large feature set and therefore want to tune on a large data set, batch methods are infeasible. Online methods can converge faster, and in practice they often find better solutions (Li</context>
<context position="16907" citStr="Cer et al., 2010" startWordPosition="2875" endWordPosition="2878">et under many more weight vectors than a MERT-style batch method. This characteristic may result in broader exploration of the search space, and make the learner more robust to local optima local optima (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). The adaptive algorithm identifies appropriate learning rates for the mixture of dense and sparse features. Finally, large data structures such as the language model (LM) and phrase table exist in shared memory, obviating the need for remote queries. 4 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target</context>
</contexts>
<marker>Cer, Galley, Jurafsky, Manning, 2010</marker>
<rawString>D. Cer, M. Galley, D. Jurafsky, and C. D. Manning. 2010. Phrasal: A statistical machine translation toolkit for exploring new model features. In HLTNAACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-C Chang</author>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="18218" citStr="Chang et al., 2008" startWordPosition="3083" endWordPosition="3086">ses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Bilingual Monolingual Sentences Tokens Tokens Ar-En 6.6M 375M 990M Zh-En 9.3M 538M Table 1: Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments and deleted (unaligned) source words. Discriminative reordering (LO): indicators for eight lexicalized reordering classes, including the six stan</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>P-C. Chang, M. Galley, and C. D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>G Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="3629" citStr="Cherry and Foster (2012)" startWordPosition="539" endWordPosition="542">ity than the multiple references in tuning sets from MT competitions. Second, large bitexts often comprise many text genres (Haddow and Koehn, 2012), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets. Our analysis separates and quantifies these two issues. We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Lin</context>
<context position="19444" citStr="Cherry and Foster, 2012" startWordPosition="3269" endWordPosition="3272">rd monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4.2 Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with u=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. We ran their code with standard settings. Moses5 also contains the discriminative phrase table implementation of (Hasler et al., 2012b), which is identical to our implementation using Phrasal. Moses and Phrasal accept the same phrase table and LM formats, so we kept those data structures in common. The two decoders also use the same multi-stack bea</context>
<context position="33792" citStr="Cherry and Foster, 2012" startWordPosition="5624" endWordPosition="5627">stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is f</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>C. Cherry and G. Foster. 2012. Batch tuning strategies for statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6303" citStr="Chiang et al. (2008)" startWordPosition="985" endWordPosition="988">∇ft(wt−1)∇ft(wt−1)&gt; t � ∇fi(wi−1)∇fi(wi−1)&gt; (3) i=1 A diagonal approximation to Σ can be used for a high-dimensional vector wt. In this case, AdaGrad is simple to implement and computationally cheap. Consider a single dimension j, and let scalars vt = wt,j, gt = ∇jft(wt−1), Gt = Eti=1 g2i , then the update rule is vt = vt−1 − q G−1/2 t gt (4) Gt = Gt−1 + g2 (5) t Compared to SGD, we just need to store Gt = Σ−1 t,jj for each dimension j. 1We specify the loss function for MT in section 3.1. 2.2 Prior Online Algorithms in MT AdaGrad is related to two previous online learning methods for MT. MIRA Chiang et al. (2008) described an adaption of MIRA (Crammer et al., 2006) to MT. MIRA makes the following update: kw − wt−1k2 2 + ft(w) (6) 2q The first term expresses conservativity: the weight should change as little as possible based on a single example, ensuring that it is never beneficial to overshoot the minimum. The relationship to SGD can be seen by linearizing the loss function ft(w) ≈ ft(wt−1) + (w − wt−1)&gt;∇ft(wt−1) and taking the derivative of (6). The result is exactly (1). AROW Chiang (2012) adapted AROW (Crammer et al., 2009) to MT. AROW models the current weight as a Gaussian centered at wt−1 with </context>
<context position="32653" citStr="Chiang et al., 2008" startWordPosition="5447" endWordPosition="5450"> down the other algorithms. 6 Related Work Our work relates most closely to that of Hasler et al. (2012b), who tuned models containing both sparse and dense features with Moses. A discriminative phrase table helped them improve slightly over a dense, online MIRA baseline, but their best results required initialization with MERT-tuned weights and re-tuning a single, shared weight for the discriminative phrase table with MERT. In contrast, our algorithm learned good high dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-margin training of syntactic and structural translation features. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="14380" citStr="Chiang et al., 2009" startWordPosition="2447" endWordPosition="2450">pact Algorithm 2 “Stale gradient” parallelization method for Algorithm 1. Require: Tuning set {fi, e1:k i }i=1:M 1: Initialize threadpool p1, ... ,pj 2: Set t = 1 3: repeat 4: for i in 1 ... M in random order do 5: Wait until any thread p is idle 6: Send (fi, e1:k i, t) to p &gt; Alg. 1 lines 5–8 7: while ∃ p� done with gradient gt, do &gt; t&apos; ≤ t 8: Update wt = wt−1 − ηgt, &gt; Alg. 1 lines 9–11 9: Sett = t + 1 10: end while 11: end for 12: until convergence search. Some of the features generalize, but many do not. This was well understood in previous work, so heuristic filtering was usually applied (Chiang et al., 2009, inter alia). In contrast, we need only select an appropriate regularization strength λ. Specifically, when r(w) = λ11w111, the closedform solution to (14) is [ ] wt = sign(wt_2 1 ) |wt_ 2 1 |− ηt_1λ + (15) where [x]+ = max(x, 0) is the clipping function that in this case sets a weight to 0 when it falls below the threshold ηt_1λ. It is straightforward to adapt this to AdaGrad with diagonal E by setting each dimension of ηt_1,j = ηE element-wise products. We find that V`t_1(wt_1) only involves several hundred active features for the current example (or mini-batch). However, naively following </context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>JMLR,</journal>
<pages>13--1159</pages>
<contexts>
<context position="3691" citStr="Chiang, 2012" startWordPosition="552" endWordPosition="553">ond, large bitexts often comprise many text genres (Haddow and Koehn, 2012), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets. Our analysis separates and quantifies these two issues. We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Adaptive Online Algorithms Machine translation is a</context>
<context position="6792" citStr="Chiang (2012)" startWordPosition="1074" endWordPosition="1075">2 Prior Online Algorithms in MT AdaGrad is related to two previous online learning methods for MT. MIRA Chiang et al. (2008) described an adaption of MIRA (Crammer et al., 2006) to MT. MIRA makes the following update: kw − wt−1k2 2 + ft(w) (6) 2q The first term expresses conservativity: the weight should change as little as possible based on a single example, ensuring that it is never beneficial to overshoot the minimum. The relationship to SGD can be seen by linearizing the loss function ft(w) ≈ ft(wt−1) + (w − wt−1)&gt;∇ft(wt−1) and taking the derivative of (6). The result is exactly (1). AROW Chiang (2012) adapted AROW (Crammer et al., 2009) to MT. AROW models the current weight as a Gaussian centered at wt−1 with covariance Σt−1, and does the following update upon seeing training example xt: wt, Σt = arg min w,Σ q 1 DKL(N(w, Σ)||N(wt−1, Σt−1)) + ft (w) + 2q x&gt;t Σxt (7) The KL-divergence term expresses a more general, directionally sensitive conservativity. Ignoring the third term, the Σ that minimizes the KL is actually Σt−1. As a result, the first two terms of (7) generalize MIRA so that we may be more conservative in some directions specified by Σ. To see this, we can write out the KL-diverg</context>
<context position="20984" citStr="Chiang, 2012" startWordPosition="3529" endWordPosition="3530"> an n-best size of 200. 5v1.0 (28 January 2013) 315 Model #features Algorithm Tuning Set MT02 MT03 MT04 MT09 Dense 19 MERT MT06 45.08 51.32 52.26 51.42 48.44 Dense 19 This paper MT06 44.19 51.42 52.52 50.16 48.13 +PT 151k kb-MIRA MT06 42.08 47.25 48.98 47.08 45.64 +PT 23k PRO MT06 44.31 51.06 52.18 50.23 47.52 +PT 50k This paper MT06 50.61 51.71 52.89 50.42 48.74 +PT+AL+LO 109k PRO MT06 44.87 51.25 52.43 50.05 47.76 +PT+AL+LO 242k This paper MT06 57.84 52.45 53.18 51.38 49.37 Dense 19 MERT MT05/6/8 49.63 51.60 52.29 51.73 48.68 +PT+AL+LO 390k This paper MT05/6/8 58.20 53.61 54.99 52.79 49.94 (Chiang, 2012)* 10-20k MIRA MT04/6 – – – – 45.90 (Chiang, 2012)* 10-20k AROW MT04/6 – – – – 47.60 #sentences 728 663 1,075 1,313 Table 2: Ar-En results [BLEU-4 % uncased] for the NIST tuning experiment. The tuning and test sets each have four references. MT06 has 1,717 sentences, while the concatenated MT05/6/8 set has 4,213 sentences. Bold indicates statistical significance relative to the best baseline in each block at p &lt; 0.001; bold-italic at p &lt; 0.05. We assessed significance with the permutation test of Riezler and Maxwell (2005). (*) Chiang (2012) used a similar-sized bitext, but two LMs trained on t</context>
<context position="32571" citStr="Chiang (2012)" startWordPosition="5433" endWordPosition="5434">, our method does not need to accumulate n-best lists, a practice that slows down the other algorithms. 6 Related Work Our work relates most closely to that of Hasler et al. (2012b), who tuned models containing both sparse and dense features with Moses. A discriminative phrase table helped them improve slightly over a dense, online MIRA baseline, but their best results required initialization with MERT-tuned weights and re-tuning a single, shared weight for the discriminative phrase table with MERT. In contrast, our algorithm learned good high dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the st</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>D. Chiang. 2012. Hope and fear for discriminative training of statistical translation models. JMLR, 13:1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>JMLR,</journal>
<pages>7--551</pages>
<contexts>
<context position="6356" citStr="Crammer et al., 2006" startWordPosition="994" endWordPosition="997">A diagonal approximation to Σ can be used for a high-dimensional vector wt. In this case, AdaGrad is simple to implement and computationally cheap. Consider a single dimension j, and let scalars vt = wt,j, gt = ∇jft(wt−1), Gt = Eti=1 g2i , then the update rule is vt = vt−1 − q G−1/2 t gt (4) Gt = Gt−1 + g2 (5) t Compared to SGD, we just need to store Gt = Σ−1 t,jj for each dimension j. 1We specify the loss function for MT in section 3.1. 2.2 Prior Online Algorithms in MT AdaGrad is related to two previous online learning methods for MT. MIRA Chiang et al. (2008) described an adaption of MIRA (Crammer et al., 2006) to MT. MIRA makes the following update: kw − wt−1k2 2 + ft(w) (6) 2q The first term expresses conservativity: the weight should change as little as possible based on a single example, ensuring that it is never beneficial to overshoot the minimum. The relationship to SGD can be seen by linearizing the loss function ft(w) ≈ ft(wt−1) + (w − wt−1)&gt;∇ft(wt−1) and taking the derivative of (6). The result is exactly (1). AROW Chiang (2012) adapted AROW (Crammer et al., 2009) to MT. AROW models the current weight as a Gaussian centered at wt−1 with covariance Σt−1, and does the following update upon s</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. JMLR, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>A Kulesza</author>
<author>M Dredze</author>
</authors>
<title>Adaptive regularization of weight vectors.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6828" citStr="Crammer et al., 2009" startWordPosition="1078" endWordPosition="1082"> MT AdaGrad is related to two previous online learning methods for MT. MIRA Chiang et al. (2008) described an adaption of MIRA (Crammer et al., 2006) to MT. MIRA makes the following update: kw − wt−1k2 2 + ft(w) (6) 2q The first term expresses conservativity: the weight should change as little as possible based on a single example, ensuring that it is never beneficial to overshoot the minimum. The relationship to SGD can be seen by linearizing the loss function ft(w) ≈ ft(wt−1) + (w − wt−1)&gt;∇ft(wt−1) and taking the derivative of (6). The result is exactly (1). AROW Chiang (2012) adapted AROW (Crammer et al., 2009) to MT. AROW models the current weight as a Gaussian centered at wt−1 with covariance Σt−1, and does the following update upon seeing training example xt: wt, Σt = arg min w,Σ q 1 DKL(N(w, Σ)||N(wt−1, Σt−1)) + ft (w) + 2q x&gt;t Σxt (7) The KL-divergence term expresses a more general, directionally sensitive conservativity. Ignoring the third term, the Σ that minimizes the KL is actually Σt−1. As a result, the first two terms of (7) generalize MIRA so that we may be more conservative in some directions specified by Σ. To see this, we can write out the KL-divergence between two Gaussians in closed</context>
</contexts>
<marker>Crammer, Kulesza, Dredze, 2009</marker>
<rawString>K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive regularization of weight vectors. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>Y Singer</author>
</authors>
<title>Efficient online and batch learning using forward backward splitting.</title>
<date>2009</date>
<journal>JMLR,</journal>
<pages>10--2899</pages>
<contexts>
<context position="2669" citStr="Duchi and Singer, 2009" startWordPosition="389" endWordPosition="392">featurerich MT systems that is effective yet comparatively easy to implement. The algorithm scales to millions of features and large tuning sets. It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. The learning rate is set adaptively using AdaGrad (Duchi et al., 2011), which is particularly effective for the mixture of dense and sparse features present in MT models. Finally, feature selection is implemented as efficient Li regularization in the forward-backward splitting (FOBOS) framework (Duchi and Singer, 2009). Experiments show that our algorithm converges faster than batch alternatives. To learn good weights for the sparse features, most algorithms—including ours—benefit from more tuning data, and the natural source is the training bitext. However, the bitext presents two problems. First, it has a single reference, sometimes of lower quality than the multiple references in tuning sets from MT competitions. Second, large bitexts often comprise many text genres (Haddow and Koehn, 2012), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a signif</context>
<context position="13090" citStr="Duchi and Singer, 2009" startWordPosition="2215" endWordPosition="2218">problem with x+ = φ(d+) − φ(d_) as positive data and x_ = −x+ as negative data, which may be plugged into an existing logistic regression solver. We find that Algorithm 1 works best with minibatches instead of single examples. In line 4 we simply partition the tuning set so that i becomes a mini-batch of examples. 3.2 Updating and Regularization Algorithm 1 lines 9–11 compute the adaptive learning rate, update the weights, and apply regularization. Section 2.1 explained the AdaGrad learning rate computation. To update and regularize the weights we apply the Forward-Backward Splitting (FOBOS) (Duchi and Singer, 2009) framework, which separates the two operations. The two-step FOBOS update is wt_1 = wt_1 − ηt_1V`t_1 (wt_1) (13) 2 (14) where (13) is just an unregularized gradient descent step and (14) balances the regularization term r(w) with staying close to the gradient step. Equation (14) permits efficient L1 regularization, which is well-suited for selecting good features from exponentially many irrelevant features (Ng, 2004). It is well-known that feature selection is very important for feature-rich MT. For example, simple indicator features like lexicalized re-ordering classes are potentially useful </context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>J. Duchi and Y. Singer. 2009. Efficient online and batch learning using forward backward splitting. JMLR, 10:2899–2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<pages>12--2121</pages>
<contexts>
<context position="2419" citStr="Duchi et al., 2011" startWordPosition="354" endWordPosition="357">ptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We introduce a new method for training featurerich MT systems that is effective yet comparatively easy to implement. The algorithm scales to millions of features and large tuning sets. It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. The learning rate is set adaptively using AdaGrad (Duchi et al., 2011), which is particularly effective for the mixture of dense and sparse features present in MT models. Finally, feature selection is implemented as efficient Li regularization in the forward-backward splitting (FOBOS) framework (Duchi and Singer, 2009). Experiments show that our algorithm converges faster than batch alternatives. To learn good weights for the sparse features, most algorithms—including ours—benefit from more tuning data, and the natural source is the training bitext. However, the bitext presents two problems. First, it has a single reference, sometimes of lower quality than the m</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="17697" citStr="Galley and Manning (2008)" startWordPosition="2996" endWordPosition="3000">ier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Bilingual </context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>M. Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="1969" citStr="Gimpel and Smith, 2012" startWordPosition="281" endWordPosition="284"> statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We introduce a new method for training featurerich MT systems that is effective yet comparatively easy to implement. The algorithm scales to millions of features and large tuning sets. It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. The learning rate is set adaptively using AdaGrad (Duchi et al., 2011), which is particularly effective for the mixture of dense and sparse features present in MT models. Finally, feature selection is implemented as effi</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>K. Gimpel and N. A. Smith. 2012. Structured ramp loss minimization for machine translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Distributed asynchronous online learning for natural language processing.</title>
<date>2010</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="16074" citStr="Gimpel et al. (2010)" startWordPosition="2745" endWordPosition="2748">radients in parallel and sends them to a master thread, which updates a central weight vector. Crucially, the weight updates need not be applied in order, so synchronization is unnecessary; the workers only idle at the end of an epoch. The consequence is that the update in line 8 of Algorithm 2 is with respect to gradient gt� with t&apos; &lt; t. Langford et al. (2009) gave convergence results for 2 11w − wt_211122 + ηt_1r(w) wt = arg min 1 w `t(w) = `(Dt, w) = − � x+EDt 1 t,jj and by taking 2 314 stale updating, but the bounds do not apply to our setting since we use L1 regularization. Nevertheless, Gimpel et al. (2010) applied this framework to other non-convex objectives and obtained good empirical results. Our asynchronous, stochastic method has practical appeal for MT. During a tuning run, the online method decodes the tuning set under many more weight vectors than a MERT-style batch method. This characteristic may result in broader exploration of the search space, and make the learner more robust to local optima local optima (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). The adaptive algorithm identifies appropriate learning rates for the mixture of dense and sparse features. Finally, l</context>
</contexts>
<marker>Gimpel, Das, Smith, 2010</marker>
<rawString>K. Gimpel, D. Das, and N. A. Smith. 2010. Distributed asynchronous online learning for natural language processing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
</authors>
<title>Discriminative Feature-Rich Modeling for Syntax-Based Machine Translation.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Language Technologies Institute, Carnegie Mellon University.</institution>
<contexts>
<context position="33755" citStr="Gimpel, 2012" startWordPosition="5620" endWordPosition="5621">ound to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-ric</context>
</contexts>
<marker>Gimpel, 2012</marker>
<rawString>K. Gimpel. 2012. Discriminative Feature-Rich Modeling for Syntax-Based Machine Translation. Ph.D. thesis, Language Technologies Institute, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>J DeNero</author>
</authors>
<title>A class-based agreement model for generating accurately inflected translations.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="18073" citStr="Green and DeNero, 2012" startWordPosition="3059" endWordPosition="3062">ed the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Bilingual Monolingual Sentences Tokens Tokens Ar-En 6.6M 375M 990M Zh-En 9.3M 538M Table 1: Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments </context>
</contexts>
<marker>Green, DeNero, 2012</marker>
<rawString>S. Green and J. DeNero. 2012. A class-based agreement model for generating accurately inflected translations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Haddow</author>
<author>P Koehn</author>
</authors>
<title>Analysing the effect of out-of-domain data on SMT systems.</title>
<date>2012</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="3153" citStr="Haddow and Koehn, 2012" startWordPosition="465" endWordPosition="468">eature selection is implemented as efficient Li regularization in the forward-backward splitting (FOBOS) framework (Duchi and Singer, 2009). Experiments show that our algorithm converges faster than batch alternatives. To learn good weights for the sparse features, most algorithms—including ours—benefit from more tuning data, and the natural source is the training bitext. However, the bitext presents two problems. First, it has a single reference, sometimes of lower quality than the multiple references in tuning sets from MT competitions. Second, large bitexts often comprise many text genres (Haddow and Koehn, 2012), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets. Our analysis separates and quantifies these two issues. We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard t</context>
<context position="25530" citStr="Haddow and Koehn, 2012" startWordPosition="4283" endWordPosition="4286">xample with a new weight vector, thus exploring more of the search space for the same tuning set. 4.4 Bitext Tuning Experiment Tables 2 and 3 show that adding tuning examples improves translation quality. Nevertheless, even the larger tuning set is small relative to the bitext from which rules were extracted. He and Deng (2012) and Simianer et al. (2012) showed significant translation quality gains by tuning on the bitext. However, their bitexts matched the genre of their test sets. Our bitexts, like those of most large-scale systems, do not. Domain mismatch matters for the dense feature set (Haddow and Koehn, 2012). We show that it also matters for feature-rich MT. Before aligning each bitext, we randomly sampled and sequestered 5k and 15k sentence tuning sets, and a 5k test set. We prevented overlap beDA DB |A ||B ||A n B| MT04 MT06 70k 72k 5.9k MT04 MT568 70k 96k 7.6k MT04 bitext5k 70k 67k 4.4k MT04 bitext15k 70k 310k 10.5k 5ktest bitext5k 82k 67k 5.6k 5ktest bitext15k 82k 310k 14k Table 6: Number of overlapping phrase table (+PT) features on various Zh-En dataset pairs. tween the tuning sets and the test set. We then tuned a dense model with MERT on MT06, and feature-rich models on both MT05/6/8 and </context>
</contexts>
<marker>Haddow, Koehn, 2012</marker>
<rawString>B. Haddow and P. Koehn. 2012. Analysing the effect of out-of-domain data on SMT systems. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Haddow</author>
<author>A Arun</author>
<author>P Koehn</author>
</authors>
<title>SampleRank training for phrase-based machine translation.</title>
<date>2011</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="33813" citStr="Haddow et al., 2011" startWordPosition="5628" endWordPosition="5631">section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than </context>
</contexts>
<marker>Haddow, Arun, Koehn, 2011</marker>
<rawString>B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank training for phrase-based machine translation. In WMT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Hasler</author>
<author>P Bell</author>
<author>A Ghoshal</author>
<author>B Haddow</author>
<author>P Koehn</author>
</authors>
<marker>Hasler, Bell, Ghoshal, Haddow, Koehn, </marker>
<rawString>E. Hasler, P. Bell, A. Ghoshal, B. Haddow, P. Koehn,</rawString>
</citation>
<citation valid="true">
<authors>
<author>F McInnes</author>
</authors>
<title>evaluation.</title>
<date>2012</date>
<booktitle>2012a. The UEDIN systems for the IWSLT</booktitle>
<marker>McInnes, 2012</marker>
<rawString>F. McInnes, et al. 2012a. The UEDIN systems for the IWSLT 2012 evaluation. In IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hasler</author>
<author>B Haddow</author>
<author>P Koehn</author>
</authors>
<title>Sparse lexicalised features and topic adaptation for SMT.</title>
<date>2012</date>
<booktitle>In IWSLT.</booktitle>
<contexts>
<context position="1790" citStr="Hasler et al., 2012" startWordPosition="254" endWordPosition="257">ing features such as words and ngram contexts improve many NLP systems such as parsers and taggers. Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We introduce a new method for training featurerich MT systems that is effective yet comparatively easy to implement. The algorithm scales to millions of features and large tuning sets. It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. The learning rate is set adaptively using</context>
<context position="19827" citStr="Hasler et al., 2012" startWordPosition="3329" endWordPosition="3332">uilt high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with u=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. We ran their code with standard settings. Moses5 also contains the discriminative phrase table implementation of (Hasler et al., 2012b), which is identical to our implementation using Phrasal. Moses and Phrasal accept the same phrase table and LM formats, so we kept those data structures in common. The two decoders also use the same multi-stack beam search (Och and Ney, 2004). For our method, we used uniform initialization, 16 threads, and a mini-batch size of 20. We found that η=0.02 and A=0.1 worked well on development sets for both languages. To compute the gradients 4Other system settings for all experiments: distortion limit of 5, a maximum phrase length of 7, and an n-best size of 200. 5v1.0 (28 January 2013) 315 Mode</context>
<context position="32137" citStr="Hasler et al. (2012" startWordPosition="5366" endWordPosition="5369">b-MIRA makes multiple passes through the n-best lists during each epoch. The Moses implementation parallelizes decoding but weight updating is sequential. The core of our method is an inner product between the adaptive learning rate vector and the gradient. This is easy to implement and is very fast even for large feature sets. Since we applied lazy regularization, this inner product usually involves hundred-dimensional vectors. Finally, our method does not need to accumulate n-best lists, a practice that slows down the other algorithms. 6 Related Work Our work relates most closely to that of Hasler et al. (2012b), who tuned models containing both sparse and dense features with Moses. A discriminative phrase table helped them improve slightly over a dense, online MIRA baseline, but their best results required initialization with MERT-tuned weights and re-tuning a single, shared weight for the discriminative phrase table with MERT. In contrast, our algorithm learned good high dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel H</context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2012</marker>
<rawString>E. Hasler, B. Haddow, and P. Koehn. 2012b. Sparse lexicalised features and topic adaptation for SMT. In IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X He</author>
<author>L Deng</author>
</authors>
<title>Maximum expected BLEU training of phrase and lexicon translation models.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="25236" citStr="He and Deng (2012)" startWordPosition="4236" endWordPosition="4239">ng set, we improve significantly over all other models. PRO learns a smaller model with the PT+AL+LO feature set which is surprising given that it applies L2 regularization (AdaGrad uses L1). We speculate that this may be an consequence of stochastic learning. Our algorithm decodes each example with a new weight vector, thus exploring more of the search space for the same tuning set. 4.4 Bitext Tuning Experiment Tables 2 and 3 show that adding tuning examples improves translation quality. Nevertheless, even the larger tuning set is small relative to the bitext from which rules were extracted. He and Deng (2012) and Simianer et al. (2012) showed significant translation quality gains by tuning on the bitext. However, their bitexts matched the genre of their test sets. Our bitexts, like those of most large-scale systems, do not. Domain mismatch matters for the dense feature set (Haddow and Koehn, 2012). We show that it also matters for feature-rich MT. Before aligning each bitext, we randomly sampled and sequestered 5k and 15k sentence tuning sets, and a 5k test set. We prevented overlap beDA DB |A ||B ||A n B| MT04 MT06 70k 72k 5.9k MT04 MT568 70k 96k 7.6k MT04 bitext5k 70k 67k 4.4k MT04 bitext15k 70k</context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>X. He and L. Deng. 2012. Maximum expected BLEU training of phrase and lexicon translation models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Herbrich</author>
<author>T Graepel</author>
<author>K Obermayer</author>
</authors>
<title>Support vector learning for ordinal regression.</title>
<date>1999</date>
<booktitle>In ICANN.</booktitle>
<contexts>
<context position="11024" citStr="Herbrich et al., 1999" startWordPosition="1837" endWordPosition="1840">aGrad empirically2 and lacks known theoretical guarantees. Finally, by using AdaGrad, we separate adaptivity from conservativity. Our experiments suggest that adaptivity is actually more important. 3 Adaptive Online MT Algorithm 1 shows the full algorithm introduced in this paper. AdaGrad (lines 9–10) is a crucial piece, but the loss function, regularization technique, and parallelization strategy described in this section are equally important in the MT setting. 3.1 Pairwise Logistic Loss Function Algorithm 1 lines 5–8 describe the gradient computation. We cast MT tuning as pairwise ranking (Herbrich et al., 1999, inter alia), which Hopkins and May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suitable for online learning. The idea is that for any two derivations, the ranking predicted by the model should be consistent with the ranking predicted by a gold sentence-level metric G like BLEU+1 (Lin and Och, 2004). Consider a single source sentence f with associated references e1:k. Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Let M(d) = w · φ(d) be the model score. For any derivation d+ that is better than d− un</context>
</contexts>
<marker>Herbrich, Graepel, Obermayer, 1999</marker>
<rawString>R. Herbrich, T. Graepel, and K. Obermayer. 1999. Support vector learning for ordinal regression. In ICANN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hopkins</author>
<author>J May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2274" citStr="Hopkins and May, 2011" startWordPosition="332" endWordPosition="335">ample, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We introduce a new method for training featurerich MT systems that is effective yet comparatively easy to implement. The algorithm scales to millions of features and large tuning sets. It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. The learning rate is set adaptively using AdaGrad (Duchi et al., 2011), which is particularly effective for the mixture of dense and sparse features present in MT models. Finally, feature selection is implemented as efficient Li regularization in the forward-backward splitting (FOBOS) framework (Duchi and Singer, 2009). Experiments show that our algorithm converges faster than batch alternatives. To learn good weights for the sparse features, most algorithms—including ours—benefit from more tuning data, and the natural </context>
<context position="11067" citStr="Hopkins and May (2011)" startWordPosition="1844" endWordPosition="1847">ical guarantees. Finally, by using AdaGrad, we separate adaptivity from conservativity. Our experiments suggest that adaptivity is actually more important. 3 Adaptive Online MT Algorithm 1 shows the full algorithm introduced in this paper. AdaGrad (lines 9–10) is a crucial piece, but the loss function, regularization technique, and parallelization strategy described in this section are equally important in the MT setting. 3.1 Pairwise Logistic Loss Function Algorithm 1 lines 5–8 describe the gradient computation. We cast MT tuning as pairwise ranking (Herbrich et al., 1999, inter alia), which Hopkins and May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suitable for online learning. The idea is that for any two derivations, the ranking predicted by the model should be consistent with the ranking predicted by a gold sentence-level metric G like BLEU+1 (Lin and Och, 2004). Consider a single source sentence f with associated references e1:k. Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Let M(d) = w · φ(d) be the model score. For any derivation d+ that is better than d− under G, we desire pairwise agreement such th</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>M. Hopkins and J. May. 2011. Tuning as ranking. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>Direct translation model 2. In HLT-NAACL.</title>
<date>2007</date>
<contexts>
<context position="33912" citStr="Ittycheriah and Roukos, 2007" startWordPosition="5643" endWordPosition="5646">eight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than MERT, scales to millions of features, and converges quickly. We used efficient L1 regularization fo</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>A. Ittycheriah and S. Roukos. 2007. Direct translation model 2. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="17939" citStr="Klein and Manning, 2003" startWordPosition="3038" endWordPosition="3041">ow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Bilingual Monolingual Sentences Tokens Tokens Ar-En 6.6M 375M 990M Zh-En 9.3M 538M Table 1: Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL, Demonstration Session.</booktitle>
<contexts>
<context position="3567" citStr="Koehn et al., 2007" startWordPosition="530" endWordPosition="533">First, it has a single reference, sometimes of lower quality than the multiple references in tuning sets from MT competitions. Second, large bitexts often comprise many text genres (Haddow and Koehn, 2012), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets. Our analysis separates and quantifies these two issues. We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, Sofia, Bulga</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, et al. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Langford</author>
<author>A J Smola</author>
<author>M Zinkevich</author>
</authors>
<title>Slow learners are fast.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="15398" citStr="Langford et al. (2009)" startWordPosition="2616" endWordPosition="2619"> by setting each dimension of ηt_1,j = ηE element-wise products. We find that V`t_1(wt_1) only involves several hundred active features for the current example (or mini-batch). However, naively following the FOBOS framework requires updating millions of weights. But a practical benefit of FOBOS is that we can do lazy updates on just the active dimensions without any approximations. 3.3 Parallelization Algorithm 1 is inherently sequential like standard online learning. This is undesirable in MT where decoding is costly. We therefore parallelize the algorithm with the “stale gradient” method of Langford et al. (2009) (Algorithm 2). A fixed threadpool of workers computes gradients in parallel and sends them to a master thread, which updates a central weight vector. Crucially, the weight updates need not be applied in order, so synchronization is unnecessary; the workers only idle at the end of an epoch. The consequence is that the update in line 8 of Algorithm 2 is with respect to gradient gt� with t&apos; &lt; t. Langford et al. (2009) gave convergence results for 2 11w − wt_211122 + ηt_1r(w) wt = arg min 1 w `t(w) = `(Dt, w) = − � x+EDt 1 t,jj and by taking 2 314 stale updating, but the bounds do not apply to ou</context>
</contexts>
<marker>Langford, Smola, Zinkevich, 2009</marker>
<rawString>J. Langford, A. J. Smola, and M. Zinkevich. 2009. Slow learners are fast. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Online EM for unsupervised models.</title>
<date>2009</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="4629" citStr="Liang and Klein, 2009" startWordPosition="691" endWordPosition="694">0) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Adaptive Online Algorithms Machine translation is an unusual machine learning setting because multiple correct translations exist and decoding is comparatively expensive. When we have a large feature set and therefore want to tune on a large data set, batch methods are infeasible. Online methods can converge faster, and in practice they often find better solutions (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). Recall that stochastic gradient descent (SGD), a fundamental online method, updates weights w according to wt = wt−1 − q∇ft(wt−1) (1) with loss function1 ft(w) of the tth example, (sub)gradient of the loss with respect to the parameters ∇ft(wt−1), and learning rate q. SGD is sensitive to the learning rate q, which is difficult to set in an MT system that mixes frequent “dense” features (like the language model) with sparse features (e.g., for translation rules). Furthermore, q applies to each coordinate in the gradient, an undesirable property in MT wh</context>
<context position="16515" citStr="Liang and Klein, 2009" startWordPosition="2815" endWordPosition="2818">t(w) = `(Dt, w) = − � x+EDt 1 t,jj and by taking 2 314 stale updating, but the bounds do not apply to our setting since we use L1 regularization. Nevertheless, Gimpel et al. (2010) applied this framework to other non-convex objectives and obtained good empirical results. Our asynchronous, stochastic method has practical appeal for MT. During a tuning run, the online method decodes the tuning set under many more weight vectors than a MERT-style batch method. This characteristic may result in broader exploration of the search space, and make the learner more robust to local optima local optima (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). The adaptive algorithm identifies appropriate learning rates for the mixture of dense and sparse features. Finally, large data structures such as the language model (LM) and phrase table exist in shared memory, obviating the need for remote queries. 4 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according</context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>P. Liang and D. Klein. 2009. Online EM for unsupervised models. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Côté</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="17263" citStr="Liang et al., 2006" startWordPosition="2934" endWordPosition="2937">d sparse features. Finally, large data structures such as the language model (LM) and phrase table exist in shared memory, obviating the need for remote queries. 4 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized </context>
<context position="33964" citStr="Liang et al., 2006" startWordPosition="5652" endWordPosition="5655">would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than MERT, scales to millions of features, and converges quickly. We used efficient L1 regularization for feature selection, obviating the need for the feat</context>
</contexts>
<marker>Liang, Bouchard-Côté, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Côté, D. Klein, and B. Taskar. 2006a. An end-to-end discriminative approach to machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="17263" citStr="Liang et al., 2006" startWordPosition="2934" endWordPosition="2937">d sparse features. Finally, large data structures such as the language model (LM) and phrase table exist in shared memory, obviating the need for remote queries. 4 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized </context>
<context position="33964" citStr="Liang et al., 2006" startWordPosition="5652" endWordPosition="5655">would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than MERT, scales to millions of features, and converges quickly. We used efficient L1 regularization for feature selection, obviating the need for the feat</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006b. Alignment by agreement. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>F J Och</author>
</authors>
<title>ORANGE: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="11366" citStr="Lin and Och, 2004" startWordPosition="1895" endWordPosition="1898"> regularization technique, and parallelization strategy described in this section are equally important in the MT setting. 3.1 Pairwise Logistic Loss Function Algorithm 1 lines 5–8 describe the gradient computation. We cast MT tuning as pairwise ranking (Herbrich et al., 1999, inter alia), which Hopkins and May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suitable for online learning. The idea is that for any two derivations, the ranking predicted by the model should be consistent with the ranking predicted by a gold sentence-level metric G like BLEU+1 (Lin and Och, 2004). Consider a single source sentence f with associated references e1:k. Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Let M(d) = w · φ(d) be the model score. For any derivation d+ that is better than d− under G, we desire pairwise agreement such that G (e(d+)� e1:k) &gt; G (e(d−), e1:k) � l J1 M(d+) &gt; M(d−) 2According to experiments not reported in this paper. t i=1 = E−1 0 + 313 Ensuring pairwise agreement is the same as ensuring w · [φ(d+) − φ(d_)] &gt; 0. For learning, we need to select derivation pairs (d+, d_) to compute difference vectors x+</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>S Kulick</author>
</authors>
<title>Enhancing the Arabic Treebank: A collaborative effort toward new annotation guidelines.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="18144" citStr="Maamouri et al., 2008" startWordPosition="3071" endWordPosition="3074">eature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Bilingual Monolingual Sentences Tokens Tokens Ar-En 6.6M 375M 990M Zh-En 9.3M 538M Table 1: Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments and deleted (unaligned) source words. Discriminative reordering (LO): i</context>
</contexts>
<marker>Maamouri, Bies, Kulick, 2008</marker>
<rawString>M. Maamouri, A. Bies, and S. Kulick. 2008. Enhancing the Arabic Treebank: A collaborative effort toward new annotation guidelines. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="18001" citStr="Marcus et al., 1993" startWordPosition="3048" endWordPosition="3051"> to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Bilingual Monolingual Sentences Tokens Tokens Ar-En 6.6M 375M 990M Zh-En 9.3M 538M Table 1: Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). tive phrase table (PT): indicators for each rule in the p</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Hall</author>
<author>G Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="33130" citStr="McDonald et al., 2010" startWordPosition="5516" endWordPosition="5519">dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on:</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>R. McDonald, K. Hall, and G. Mann. 2010. Distributed training strategies for the structured perceptron. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Y Ng</author>
</authors>
<title>Feature selection, L1 vs. L2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="13510" citStr="Ng, 2004" startWordPosition="2281" endWordPosition="2282">ly regularization. Section 2.1 explained the AdaGrad learning rate computation. To update and regularize the weights we apply the Forward-Backward Splitting (FOBOS) (Duchi and Singer, 2009) framework, which separates the two operations. The two-step FOBOS update is wt_1 = wt_1 − ηt_1V`t_1 (wt_1) (13) 2 (14) where (13) is just an unregularized gradient descent step and (14) balances the regularization term r(w) with staying close to the gradient step. Equation (14) permits efficient L1 regularization, which is well-suited for selecting good features from exponentially many irrelevant features (Ng, 2004). It is well-known that feature selection is very important for feature-rich MT. For example, simple indicator features like lexicalized re-ordering classes are potentially useful yet bloat the the feature set and, in the worst case, can negatively impact Algorithm 2 “Stale gradient” parallelization method for Algorithm 1. Require: Tuning set {fi, e1:k i }i=1:M 1: Initialize threadpool p1, ... ,pj 2: Set t = 1 3: repeat 4: for i in 1 ... M in random order do 5: Wait until any thread p is idle 6: Send (fi, e1:k i, t) to p &gt; Alg. 1 lines 5–8 7: while ∃ p� done with gradient gt, do &gt; t&apos; ≤ t 8: Up</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>A. Y. Ng. 2004. Feature selection, L1 vs. L2 regularization, and rotational invariance. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="33932" citStr="Och and Ney, 2002" startWordPosition="5647" endWordPosition="5650">parison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than MERT, scales to millions of features, and converges quickly. We used efficient L1 regularization for feature selection,</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="16978" citStr="Och and Ney, 2004" startWordPosition="2887" endWordPosition="2890"> characteristic may result in broader exploration of the search space, and make the learner more robust to local optima local optima (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). The adaptive algorithm identifies appropriate learning rates for the mixture of dense and sparse features. Finally, large data structures such as the language model (LM) and phrase table exist in shared memory, obviating the need for remote queries. 4 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 </context>
<context position="20072" citStr="Och and Ney, 2004" startWordPosition="3371" endWordPosition="3374">ation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. We ran their code with standard settings. Moses5 also contains the discriminative phrase table implementation of (Hasler et al., 2012b), which is identical to our implementation using Phrasal. Moses and Phrasal accept the same phrase table and LM formats, so we kept those data structures in common. The two decoders also use the same multi-stack beam search (Och and Ney, 2004). For our method, we used uniform initialization, 16 threads, and a mini-batch size of 20. We found that η=0.02 and A=0.1 worked well on development sets for both languages. To compute the gradients 4Other system settings for all experiments: distortion limit of 5, a maximum phrase length of 7, and an n-best size of 200. 5v1.0 (28 January 2013) 315 Model #features Algorithm Tuning Set MT02 MT03 MT04 MT09 Dense 19 MERT MT06 45.08 51.32 52.26 51.42 48.44 Dense 19 This paper MT06 44.19 51.42 52.52 50.16 48.13 +PT 151k kb-MIRA MT06 42.08 47.25 48.98 47.08 45.64 +PT 23k PRO MT06 44.31 51.06 52.18 5</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3526" citStr="Och, 2003" startWordPosition="524" endWordPosition="525">e bitext presents two problems. First, it has a single reference, sometimes of lower quality than the multiple references in tuning sets from MT competitions. Second, large bitexts often comprise many text genres (Haddow and Koehn, 2012), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets. Our analysis separates and quantifies these two issues. We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational</context>
<context position="19012" citStr="Och, 2003" startWordPosition="3200" endWordPosition="3201">al corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments and deleted (unaligned) source words. Discriminative reordering (LO): indicators for eight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4.2 Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with u=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19202" citStr="Papineni et al., 2002" startWordPosition="3229" endWordPosition="3233">or each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments and deleted (unaligned) source words. Discriminative reordering (LO): indicators for eight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4.2 Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with u=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. We ran their code with standard settings. Moses5 also contains the discriminative phrase table implementatio</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing in MT.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization (MTSE).</booktitle>
<contexts>
<context position="21511" citStr="Riezler and Maxwell (2005)" startWordPosition="3616" endWordPosition="3619">0 52.29 51.73 48.68 +PT+AL+LO 390k This paper MT05/6/8 58.20 53.61 54.99 52.79 49.94 (Chiang, 2012)* 10-20k MIRA MT04/6 – – – – 45.90 (Chiang, 2012)* 10-20k AROW MT04/6 – – – – 47.60 #sentences 728 663 1,075 1,313 Table 2: Ar-En results [BLEU-4 % uncased] for the NIST tuning experiment. The tuning and test sets each have four references. MT06 has 1,717 sentences, while the concatenated MT05/6/8 set has 4,213 sentences. Bold indicates statistical significance relative to the best baseline in each block at p &lt; 0.001; bold-italic at p &lt; 0.05. We assessed significance with the permutation test of Riezler and Maxwell (2005). (*) Chiang (2012) used a similar-sized bitext, but two LMs trained on twice as much monolingual data. Model #features Algorithm Tuning Set MT02 MT03 MT04 Dense 19 MERT MT06 33.90 35.72 33.71 34.26 Dense 19 This paper MT06 32.60 36.23 35.14 34.78 +PT 105k kb-MIRA MT06 29.46 30.67 28.96 30.05 +PT 26k PRO MT06 33.70 36.87 34.62 34.80 +PT 66k This paper MT06 33.90 36.09 34.86 34.73 +PT+AL+LO 148k PRO MT06 34.81 36.31 33.81 34.41 +PT+AL+LO 344k This paper MT06 38.99 36.40 35.07 34.84 Dense 19 MERT MT05/6/8 32.36 35.69 33.83 34.33 +PT+AL+LO 487k This paper MT05/6/8 37.64 37.81 36.26 36.15 #sentenc</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>S. Riezler and J. T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing in MT. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization (MTSE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Simianer</author>
<author>S Riezler</author>
<author>C Dyer</author>
</authors>
<title>Joint feature selection in distributed stochastic learning for largescale discriminative training in SMT.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1992" citStr="Simianer et al., 2012" startWordPosition="285" endWordPosition="288">nslation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We introduce a new method for training featurerich MT systems that is effective yet comparatively easy to implement. The algorithm scales to millions of features and large tuning sets. It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. The learning rate is set adaptively using AdaGrad (Duchi et al., 2011), which is particularly effective for the mixture of dense and sparse features present in MT models. Finally, feature selection is implemented as efficient Li regularization</context>
<context position="25263" citStr="Simianer et al. (2012)" startWordPosition="4241" endWordPosition="4244">ificantly over all other models. PRO learns a smaller model with the PT+AL+LO feature set which is surprising given that it applies L2 regularization (AdaGrad uses L1). We speculate that this may be an consequence of stochastic learning. Our algorithm decodes each example with a new weight vector, thus exploring more of the search space for the same tuning set. 4.4 Bitext Tuning Experiment Tables 2 and 3 show that adding tuning examples improves translation quality. Nevertheless, even the larger tuning set is small relative to the bitext from which rules were extracted. He and Deng (2012) and Simianer et al. (2012) showed significant translation quality gains by tuning on the bitext. However, their bitexts matched the genre of their test sets. Our bitexts, like those of most large-scale systems, do not. Domain mismatch matters for the dense feature set (Haddow and Koehn, 2012). We show that it also matters for feature-rich MT. Before aligning each bitext, we randomly sampled and sequestered 5k and 15k sentence tuning sets, and a 5k test set. We prevented overlap beDA DB |A ||B ||A n B| MT04 MT06 70k 72k 5.9k MT04 MT568 70k 96k 7.6k MT04 bitext5k 70k 67k 4.4k MT04 bitext15k 70k 310k 10.5k 5ktest bitext5k</context>
<context position="32998" citStr="Simianer et al. (2012)" startWordPosition="5498" endWordPosition="5501">s and re-tuning a single, shared weight for the discriminative phrase table with MERT. In contrast, our algorithm learned good high dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their paralle</context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature selection in distributed stochastic learning for largescale discriminative training in SMT. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="17381" citStr="Stolcke, 2002" startWordPosition="2953" endWordPosition="2954"> obviating the need for remote queries. 4 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A Stolcke. 2002. SRILM—an extensible language modeling toolkit. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>T Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical MT.</title>
<date>2006</date>
<booktitle>In ACLCOLING.</booktitle>
<contexts>
<context position="34013" citStr="Tillmann and Zhang, 2006" startWordPosition="5659" endWordPosition="5663">n. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than MERT, scales to millions of features, and converges quickly. We used efficient L1 regularization for feature selection, obviating the need for the feature scaling and heuristic filtering common in pri</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>C. Tillmann and T. Zhang. 2006. A discriminative global training algorithm for statistical MT. In ACLCOLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
<author>J Suzuki</author>
<author>H Tsukada</author>
<author>H Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="32677" citStr="Watanabe et al., 2007" startWordPosition="5451" endWordPosition="5454">ithms. 6 Related Work Our work relates most closely to that of Hasler et al. (2012b), who tuned models containing both sparse and dense features with Moses. A discriminative phrase table helped them improve slightly over a dense, online MIRA baseline, but their best results required initialization with MERT-tuned weights and re-tuning a single, shared weight for the discriminative phrase table with MERT. In contrast, our algorithm learned good high dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
</authors>
<title>Optimized online rank learning for machine translation.</title>
<date>2012</date>
<booktitle>In HLT-NAACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33406" citStr="Watanabe (2012)" startWordPosition="5564" endWordPosition="5565">W by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang</context>
</contexts>
<marker>Watanabe, 2012</marker>
<rawString>T. Watanabe. 2012. Optimized online rank learning for machine translation. In HLT-NAACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Xiang</author>
<author>A Ittycheriah</author>
</authors>
<title>Discriminative feature-tied mixture modeling for statistical machine translation.</title>
<date>2011</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="33882" citStr="Xiang and Ittycheriah, 2011" startWordPosition="5639" endWordPosition="5642">whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss (Gimpel, 2012); hinge loss (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007); maximum entropy (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002); perceptron (Liang et al., 2006a); and structured SVM (Tillmann and Zhang, 2006). These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 7 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than MERT, scales to millions of features, and converges quickly. We used </context>
</contexts>
<marker>Xiang, Ittycheriah, 2011</marker>
<rawString>B. Xiang and A. Ittycheriah. 2011. Discriminative feature-tied mixture modeling for statistical machine translation. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F Chiou</author>
<author>M Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="18285" citStr="Xue et al., 2005" startWordPosition="3094" endWordPosition="3097"> of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Bilingual Monolingual Sentences Tokens Tokens Ar-En 6.6M 375M 990M Zh-En 9.3M 538M Table 1: Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments and deleted (unaligned) source words. Discriminative reordering (LO): indicators for eight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>