<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002259">
<title confidence="0.6570035">
Deriving lexical and syntactic expectation-based measures
for psycholinguistic modeling via incremental top-down parsing
</title>
<author confidence="0.72345">
Brian Roark† Asaf Bachrach‡ Carlos Cardenas◦ and Christophe Pallier‡
</author>
<affiliation confidence="0.662119">
†Center for Spoken Language Understanding, Oregon Health &amp; Science University
</affiliation>
<address confidence="0.528175">
‡ INSERM-CEA Cognitive Neuroimaging Unit, Gif sur Yvette, France ◦MIT
</address>
<email confidence="0.990767">
roark@cslu.ogi.edu asafbac@gmail.com cardenas@mit.edu christophe@pallier.org
</email>
<sectionHeader confidence="0.99457" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973">
A number of recent publications have
made use of the incremental output of
stochastic parsers to derive measures of
high utility for psycholinguistic modeling,
following the work of Hale (2001; 2003;
2006). In this paper, we present novel
methods for calculating separate lexical
and syntactic surprisal measures from a
single incremental parser using a lexical-
ized PCFG. We also present an approx-
imation to entropy measures that would
otherwise be intractable to calculate for a
grammar of that size. Empirical results
demonstrate the utility of our methods in
predicting human reading times.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907953846154">
Assessment of linguistic complexity has played
an important role in psycholinguistics and neu-
rolinguistics for a long time, from the use of
mean length of utterance and related scores in
child language development (Klee and Fitzgerald,
1985), to complexity scores related to reading dif-
ficulty in human sentence processing studies (Yn-
gve, 1960; Frazier, 1985; Gibson, 1998). Opera-
tionally, such linguistic complexity scores are de-
rived via deterministic manual (human) annotation
and scoring algorithms of language samples. Nat-
ural language processing has been employed to
automate the extraction of such measures (Sagae
et al., 2005; Roark et al., 2007), which can have
high utility in terms of reduction of time required
to annotate and score samples. More interest-
ingly, however, novel data driven methods are be-
ing increasingly employed in this sphere, yield-
ing language sample characterizations that require
NLP in their derivation. For example, scores
derived from variously estimated language mod-
els have been used to evaluate and classify lan-
guage samples associated with neurodevelopmen-
tal or neurodegenerative disorders (Roark et al.,
2007; Solorio and Liu, 2008; Gabani et al., 2009),
as well as within general studies of human sen-
tence processing (Hale, 2001; 2003; 2006). These
scores cannot feasibly be derived by hand, but
rather rely on large-scale statistical models and
structured inference algorithms to be derived. This
is quickly becoming an important application of
NLP, making possible new methods in the study
of human language processing in both typical and
impaired populations.
The use of broad-coverage parsing for psy-
cholinguistic modeling has become very popular
recently. Hale (2001) suggested a measure (sur-
prisal) derived from an Earley (1970) parser us-
ing a probabilistic context-free grammar (PCFG)
for psycholinguistic modeling; and in later work
(Hale, 2003; 2006) he suggested an alternate
parser-derived measure (entropy reduction) that
may also account for some human sentence pro-
cessing performance. Recent work continues to
advocate surprisal in particular as a very use-
ful measure for predicting processing difficulty
(Boston et al., 2008a; Boston et al., 2008b; Dem-
berg and Keller, 2008; Levy, 2008), and the mea-
sure has been derived using a variety of incre-
mental (left-to-right) parsing strategies, includ-
ing an Earley parser (Boston et al., 2008a), the
Roark (2001) incremental top-down parser (Dem-
berg and Keller, 2008), and an n-best version of
the Nivre et al. (2007) incremental dependency
parser (Boston et al., 2008a; 2008b). Deriving
such measures by hand, even for a relatively lim-
ited set of stimuli, is not feasible, hence parsing
plays a critical role in this developing psycholin-
guistic enterprise.
There is no single measure that can account for
all of the factors influencing human sentence pro-
cessing performance, and some of the most recent
work on using parser-derived measures for psy-
cholinguistic modeling has looked to try to de-
rive multiple, complementary measures. One of
</bodyText>
<page confidence="0.981739">
324
</page>
<note confidence="0.99661">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324–333,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.992951774647887">
the key distinctions being looked at is syntactic
versus lexical expectations (Gibson, 2006). For
example, in Demberg and Keller (2008), trials
were run deriving surprisal from the Roark (2001)
parser under two different conditions: fully lex-
icalized parsing, and fully unlexicalized parsing
(to pre-terminal part-of-speech tags). Boston et
al. (2008a) capture a similar distinction by mak-
ing use of an unlexicalized PCFG within an Ear-
ley parser and a fully lexicalized unlabeled depen-
dency parser (Nivre et al., 2007). As Demberg and
Keller (2008) point out, fully unlexicalized gram-
mars ignore important lexico-syntactic informa-
tion when deriving the “syntactic” expectations,
such as subcategorization preferences of particular
verbs, which are generally accepted to impact syn-
tactic expectations in human sentence processing
(Garnsey et al., 1997). Demberg and Keller argue,
based on their results, for unlexicalized surprisal
instead of lexicalized surprisal. Here we present a
novel method for deriving separate syntactic and
lexical surprisal measures from a fully lexicalized
incremental parser, to allow for rich probabilistic
grammars to be used to derive either measure, and
demonstrate the utility of this method versus that
of Demberg and Keller in empirical trials.
The use of large-scale lexicalized grammars
presents a problem for using an Earley parser to
derive surprisal or for the calculation of entropy as
Hale (2003; 2006) defines it, because both meth-
ods require matrix inversion of a matrix with di-
mensionality the size of the non-terminal set. With
very large lexicalized PCFGs, the size of the non-
terminal set is too large for tractable matrix in-
version. The use of an incremental, beam-search
parser provides a tractable approximation to both
measures. Incremental top-down and left-corner
parsers have been shown to effectively (and effi-
ciently) make use of non-local features from the
left-context to yield very high accuracy syntactic
parses (Roark, 2001; Henderson, 2003; Collins
and Roark, 2004), and we will use such rich mod-
els to derive our scores.
In addition to teasing apart syntactic and lexical
surprisal (defined explicitly in §3), we present an
approximation to the full entropy that Hale (2003;
2006) used to define the entropy reduction hypoth-
esis. Such an entropy measure is derived via a pre-
dictive step, advancing the parses independently
of the input, as described in §3.3. We also present
syntactic and lexical alternatives for this measure,
and demonstrate the utility of making such a dis-
tinction for entropy as well as surprisal.
The purpose of this paper is threefold. First,
to present a careful and well-motivated decompo-
sition of lexical and syntactic expectation-based
measures from a given lexicalized PCFG. Sec-
ond, to explicitly document methods for calculat-
ing these and other measures from a specific in-
cremental parser. And finally, to present some em-
pirical validation of the novel measures from real
reading time trials. We modified the Roark (2001)
parser to calculate the discussed measures1, and
the empirical results in §4 show several things,
including: 1) using a fully lexicalized parser to
calculate syntactic surprisal and entropy provides
higher predictive utility for reading times than
these measures calculated via unlexicalized pars-
ing (as in Demberg and Keller); and 2) syntactic
entropy is a useful predictor of reading time.
2 Notation and preliminaries
</bodyText>
<listItem confidence="0.61754425">
A probabilistic context-free grammar (PCFG)
G = (V, T, 5†, P, p) consists of a set of non-
terminal variables V ; a set of terminal items
(words) T; a special start non-terminal 5† E V ;
a set of rule productions P of the form A —* α
for A E V , α E (V U T)*; and a function p
that assigns probabilities to each rule in P such
that for any given non-terminal symbol X E V ,
</listItem>
<equation confidence="0.992689">
E
α p(X —* α) = 1.
</equation>
<bodyText confidence="0.997770545454545">
For a given rule A —* α E P, let the func-
tion RHS return the right-hand side of the rule, i.e.,
RHS(A —* α) = α. Without loss of generality, we
will assume that for every rule A —* α E P, one
of two cases holds: either RHS(A —* α) E T or
RHS(A —* α) E V *. That is, the right-hand side
sequences consist of either (1) exactly one termi-
nal item, or (2) zero or more non-terminals.
Let W E Tn be a terminal string of length n,
i.e., W = W1 ... Wn and JWJ = n. Let W [i, j]
denote the substring beginning at word Wi and
ending at word Wj of the string. Then W|W |is the
last word in the string, and W [1, JW J] is the string
as a whole. Adjacent strings represent concate-
nation, i.e., W [1, i]W [i+1, j] = W [1, j]. Thus
W [1, i]w represents the string where Wi+1 = w.
We can define a “derives” relation (denoted ==&gt;&apos;G
for a given PCFG G) as follows: QA-y ==&gt;&apos;G Qα-y
if and only if A —* α E P. A string W E T*
is in the language of a grammar G if and only
if 5† +==&gt;&apos;G W, i.e., a sequence of one or more
derivation steps yields the string from the start
</bodyText>
<footnote confidence="0.989075">
1The parser version will be made publicly available.
</footnote>
<page confidence="0.999007">
325
</page>
<bodyText confidence="0.996576666666666">
non-terminal. A leftmost derivation begins with
5† and each derivation step replaces the leftmost
non-terminal A in the yield with some α such that
A —* α E P. For a leftmost derivation 5† *�G α,
where α E (V U T)*, the sequence of deriva-
tion steps that yield α can be represented as a
tree, with the start symbol 5† at the root, and the
“yield” sequence α at the leaves of the tree. A
complete tree has only terminal items in the yield,
i.e., α E T*; a partial tree has some non-terminal
items in the yield. With a leftmost derivation, the
yield α = Q-y partitions into an initial sequence
of terminals Q E T* followed by a sequence of
non-terminals -y E V *. For a complete derivation,
-y = E; for a partial derivation -y E V +, i.e., one or
more non-terminals. Let T (G, W[1, i]) be the set
of complete trees with W[1, i] as the yield of the
tree, given PCFG G.
A leftmost derivation D consists of a sequence
of |D |steps. Let Di represent the ith step in
the derivation D, and D[i, j] represent the subse-
quence of steps in D beginning with Di and end-
ing with Dj. Note that D|D |is the last step in
the derivation, and D[1, |D|] is the derivation as
a whole. Each step Di in the derivation is a rule
in G, i.e., Di E P for all i. The probability of the
derivation and the corresponding tree is:
</bodyText>
<equation confidence="0.997408333333333">
m
p(D) = p(Di) (1)
i=1
</equation>
<bodyText confidence="0.999323833333333">
Let D(G, W[1, i]) be the set of all possible left-
most derivations D (with respect to G) such that
RHS(D|D|) = Wi. These are the set of partial left-
most derivations whose last step used a production
with terminal Wi on the right-hand side. The pre-
fix probability of W [1, i] with respect to G is
</bodyText>
<equation confidence="0.99911">
PrefixProbG(W [1, i]) = � p(D) (2)
DED(G,W [1,i])
</equation>
<bodyText confidence="0.99706825">
From this prefix probability, we can calculate the
conditional probability of each word w E T in the
terminal vocabulary, given the preceding sequence
W[1, i] as follows:
</bodyText>
<equation confidence="0.9990876">
PrefixProbG(W[1, i]w)
PG(w  |W[1, i]) =
Pw,∈T PrefixProbG(W [1, i]w&apos;)
PrefixProbG(W[1, i]w) =(3)
PrefixProbG(W [1, i])
</equation>
<bodyText confidence="0.9995498125">
This, in fact, is precisely the conditional proba-
bility that is used for language modeling for such
applications as speech recognition and machine
translation, which was the motivation for various
syntactic language modeling approaches (Jelinek
and Lafferty, 1991; Stolcke, 1995; Chelba and Je-
linek, 1998; Roark, 2001).
As with language modeling, it is important to
model the end of the string as well, usually with
an explicit end symbol, e.g., &lt;/s&gt;. For a string
W[1, i], we can calculate its prefix probability as
shown above. To calculate its complete probabil-
ity, we must sum the probabilities over the set of
complete trees T (G, W[1, i]). In such a way, we
can calculate the conditional probability of ending
the string with &lt;/s&gt; given W[1, i] as follows:
</bodyText>
<equation confidence="0.825310333333333">
PG(&lt;/s&gt;  |W[ 1,])
i EDET (G,W[1,i]) p(D) (4)
— PrefixProbG (W [ 1, i] )
</equation>
<subsectionHeader confidence="0.980494">
2.1 Incremental top-down parsing
</subsectionHeader>
<bodyText confidence="0.999994027027027">
In this section, we review relevant details of
the Roark (2001) incremental top-down parser,
as configured for use here. As presented in
Roark (2004), the probabilities in the PCFG are
smoothed so that the parser is guaranteed not to
fail due to garden pathing, despite following a
beam search strategy. Hence there is always a non-
zero prefix probability as defined in Eq. 2.
The parser follows a top-down leftmost deriva-
tion strategy. The grammar is factored so that ev-
ery production has either a single terminal item on
the right-hand side or is of the form A —* B A-B,
where A,B E V and the factored A-B category
can expand to any sequence of children categories
of A that can follow B. This factorization of n-
ary productions continues to nullary factored pro-
ductions, i.e., the end of the original production
A —* B1... B,,, is signaled with an empty produc-
tion A-B1-... -B,,, —* E.
The parser maintains a set of possible connected
derivations, weighted via the PCFG. It uses a beam
search, whereby the highest scoring derivations
are worked on first, and derivations that fall out-
side of the beam are discarded. The reader is re-
ferred to Roark (2001; 2004) for specifics about
the beam search.
The model conditions the probability of each
production on features extracted from the par-
tial tree, including non-local node labels such as
parents, grandparents and siblings from the left-
context, as well as c-commanding lexical items.
Hence this is a lexicalized grammar, though the
incremental nature precludes a general head-first
strategy, rather one that looks to the left-context
for c-commanding lexical items.
To avoid some of the early prediction of struc-
ture, the version of the Roark parser that we used
</bodyText>
<page confidence="0.995062">
326
</page>
<bodyText confidence="0.999952318181818">
performs an additional grammar transformation
beyond the simple factorization already described
– a selective left-corner transform of left-recursive
productions (Johnson and Roark, 2000). In the
transformed structure, slash categories are used to
avoid predicting left-recursive structure until some
explicit indication of modification is present, e.g.,
a preposition.
The final step in parsing, following the last word
in the string, is to “complete” all non-terminals
in the yield of the tree. All of these open non-
terminals are composite factored categories, such
as S-NP-VP, which are “completed” by rewriting
to E. The probability of these E productions is what
allows for the calculation of the conditional prob-
ability of ending the string, shown in Eq. 4.
One final note about the size of the non-terminal
set and the intractability of exact inference for
such a scenario. The non-terminal set not only
includes the original atomic non-terminals of the
grammar, but also any categories created by gram-
mar factorization (S-NP) or the left-corner trans-
form (NP/NP). Additionally, however, to remain
context-free, the non-terminal set must include
categories that incorporate non-local features used
by the statistical model into their label, includ-
ing parents, grandparents and sibling categories in
the left-context, as well as c-commanding lexical
heads. These non-local features must be made lo-
cal by encoding them in the non-terminal labels,
leading to a very large non-terminal set and in-
tractable exact inference. Heavy smoothing is re-
quired when estimating the resulting PCFG. The
benefit of such a non-terminal set is a rich model,
which enables a more peaked statistical distribu-
tion around high quality syntactic structures and
thus more effective pruning of the search space.
The fully connected left-context produced by top-
down derivation strategies provides very rich fea-
tures for the stochastic parsing models. See Roark
(2001; 2004) for discussion of these issues.
We now turn to measures that can be derived
from the parser which may be of use for psycholin-
guistic modeling.
</bodyText>
<sectionHeader confidence="0.945892" genericHeader="introduction">
3 Parser and grammar derived measures
</sectionHeader>
<subsectionHeader confidence="0.988811">
3.1 Surprisal
</subsectionHeader>
<bodyText confidence="0.999739666666667">
The surprisal at word Wi is the negative log prob-
ability of Wi given the preceding words. Using
prefix probabilities, this can be calculated as:
</bodyText>
<equation confidence="0.9611126">
PrefixProbG(W [1, i])
SG(Wi) = −log PrefixProbG(W[1, i − 1]) (5)
Substituting equation 2 into this, we get
SG (Wi) = − log EDED(G,W[1,i]) p(D) (6)
EDED(G,W[1,i−1]) p(D)
</equation>
<bodyText confidence="0.9999615">
If we are using a beam-search parser, some of the
derivations are pruned away. Let B(G, W[1, i]) C_
D(G, W [1, i]) be the set of derivations in the
beam. Then the surprisal can be approximated as
</bodyText>
<equation confidence="0.9967395">
SG (Wi) −log EDEB(G,W[1,i]) p(D) (7)
EDEB(G,W[1,i−1]) p(D)
</equation>
<bodyText confidence="0.997570714285714">
Any pruning in the beam search will result in a de-
ficient probability distribution, i.e., a distribution
that sums to less than 1. Roark’s thesis (2001)
showed that the amount of probability mass lost
for this particular approach is very low, hence this
provides a very tight bound on the actual surprisal
given the model.
</bodyText>
<subsectionHeader confidence="0.999396">
3.2 Lexical and Syntactic surprisal
</subsectionHeader>
<bodyText confidence="0.999994642857143">
High surprisal scores result when the prefix proba-
bility at word Wi is low relative to the prefix prob-
ability at word Wi−1. Sometimes this is due to the
identity of Wi, i.e., it is a surprising word given
the context. Other times, it may not be the lexical
identity of the word so much as the syntactic struc-
ture that must be created to integrate the word into
the derivations. One would like to tease surprisal
apart into “syntactic surprisal” versus “lexical sur-
prisal”, which would capture this intuition of the
lexical versus syntactic dimensions to the score.
Our solution to this has the beneficial property of
producing two scores whose sum equals the origi-
nal surprisal score.
The original surprisal score is calculated via
sets of partial derivations at the point when each
word Wi is integrated into the syntactic structure,
D(G, W [1, i]). We then calculate the ratio from
point to point in sequence. To tease apart the lexi-
cal and syntactic surprisal, we will consider sets of
partial derivations immediately before each word
Wi is integrated into the syntactic structure, i.e.,
D[1, |D|−1] for D E D(G,W[1,i]). Recall that
the last derivation move for every derivation in the
set is from the POS-tag to the lexical item. Hence
the sequence of derivation moves that excludes the
last one includes all structure except the word Wi.
Then the syntactic surprisal is calculated as:
</bodyText>
<equation confidence="0.998819666666667">
SynSG(Wi) = − log
PD∈D(G,W[1,i]) p(D[1, |D|−1]) 8
PD∈D(G,W[1,i−1]) p(D)
</equation>
<page confidence="0.967379">
327
</page>
<bodyText confidence="0.987178">
and the lexical surprisal is calculated as:
</bodyText>
<equation confidence="0.995801">
LexSGWi l0 EDED(G,W[1,i]) ρ(D)
( ) = g EDED(G,W[1,i]) ρ(D[1, |D|−1]) (9)
</equation>
<bodyText confidence="0.984212176470588">
Note that the numerator of SynSG(WZ) is the de-
nominator of LexSG(WZ), hence they sum to form
total surprisal SG(WZ). As with total surprisal,
these measures can be defined either for the full
set D(G, W[1, i]) or for a pruned beam of deriva-
tions B(G, W[1, i]) C D(G, W[1, i]).
Finally, we replicated the Demberg and Keller
(2008) “unlexicalized” surprisal by replacing ev-
ery lexical item in the training corpus with its
POS-tag, and then parsing the POS-tags of the lan-
guage samples rather than the words. This differs
from our syntactic surprisal by having no lexical
conditioning events for rule probabilities, and by
having no ambiguity about the POS-tag of the lex-
ical items in the string. We will refer to the result-
ing surprisal measure as “POS surprisal” to distin-
guish it from our syntactic surprisal measure.
</bodyText>
<subsectionHeader confidence="0.98145">
3.3 Entropy
</subsectionHeader>
<bodyText confidence="0.999435235294118">
Entropy scores of the sort advocated by Hale
(2003; 2006) involve calculation over the set of
complete derivations consistent with the set of par-
tial derivations. Hale performs this calculation
efficiently via matrix inversion, which explains
the use of relatively small-scale grammars with
tractably sized non-terminal sets. Such methods
are not tractable for the kinds of richly condi-
tioned, large-scale PCFGs that we advocate using
here. At each word in the string, the Roark (2001)
top-down parser provides access to the weighted
set of partial analyses in the beam; the set of com-
plete derivations consistent with these is not im-
mediately accessible, hence additional work is re-
quired to calculate such measures.
Let H(D) be the entropy over a set of deriva-
tions D, calculated as follows:
</bodyText>
<equation confidence="0.999569">
H(D) = −
DED
� ED�ED ρ(D&apos;) log ED�ED ρ(D&apos;) (10)
ρ(D) ρ(D)
</equation>
<bodyText confidence="0.9997525">
If the set of derivations D = D(G, W[1, i])
is a set of partial derivations for string W [1, i],
then H(D) is a measure of uncertainty over the
partial derivations, i.e., the uncertainty regarding
the correct analysis of what has already been pro-
cessed. This can be calculated directly from the
existing parser operations. If the set of derivations
are the complete derivations consistent with the set
of partial derivations – complete derivations that
could occur over the set of possible continuations
of the string – then this is a measure of the un-
certainty about what is yet to come. We would
like measures that can capture this distinction be-
tween (a) uncertainty of what has already been
processed (“current ambiguity”) versus (b) uncer-
tainty of what is yet to be processed (“predictive
entropy”). In addition, as with surprisal, we would
like to tease apart the syntactic uncertainty versus
lexical uncertainty.
To calculate the predictive entropy after word
sequence W [1, i], we modify the parser as fol-
lows: the parser extends the set of partial deriva-
tions to include all possible next words (the entire
vocabulary plus &lt;/s&gt;), and calculates the entropy
over that set. This measure is calculated from just
one additional word beyond the current word, and
hence is an approximation to Hale’s conditional
entropy of grammatical continuations, which is
over complete derivations. We will denote this as
H&apos;G(W [1, i]) and calculate it as follows:
</bodyText>
<equation confidence="0.986537">
H&apos; G(W[1,i]) = H( � D(G, W [1, i]w)) (11)
wETuJ&lt;/s&gt;}
</equation>
<bodyText confidence="0.999535357142857">
This is performing a predictive step that the base-
line parser does not perform, extending the parses
to all possible next words.
Unlike surprisal, entropy does not decompose
straightforwardly into syntactic and lexical com-
ponents that sum to the original composite mea-
sure. To tease apart entropy due to syntactic un-
certainty versus that due to lexical uncertainty, we
can define the set of derivations up to the pre-
terminal (POS-tag) non-terminals as follows. Let
S(D) = {D[1, |D|−1] : D E D}, i.e., the set of
derivations achieved by removing the last step of
all derivations in D. Then we can calculate a “syn-
tactic” H&apos;G as follows:
</bodyText>
<equation confidence="0.995563666666667">
U
SynH1 G(W[1, i]) = H( S(D(G, W [1, i]w))) (12)
wETU{&lt;/s&gt;}
</equation>
<bodyText confidence="0.855207333333333">
Finally, “lexical” H&apos;G is defined in terms of the
conditional probabilities derived from prefix prob-
abilities as defined in Eq. 3.
</bodyText>
<equation confidence="0.987007">
LexH1G(W[1, i]) =
�− PG(w  |W[1, i]) log PG(w  |W[1, i]) (13)
wETU{&lt;/s&gt;}
</equation>
<bodyText confidence="0.999974">
As a practical matter, these values are calculated
within the Roark parser as follows. A “dummy”
word is created that can be assigned every POS-
tag, and the parser extends from the current state to
this dummy word. (The beam threshold is greatly
</bodyText>
<page confidence="0.997565">
328
</page>
<bodyText confidence="0.999976733333333">
expanded to allow for many possible extensions.)
Then every word in the vocabulary is substituted
for the word, and the appropriate probabilities cal-
culated over the beam. Finally, the actual next
word is substituted, the beam threshold is reduced
to the actual working threshold, and the requisite
number of analyses are advanced to continue pars-
ing the string. This represents a significant amount
of additional work for the parser – particularly for
vocabulary sizes that we currently use, on the or-
der of tens of thousands of words.
As with surprisal, we can calculate an “unlex-
icalized” version of the measure by training and
parsing just to POS-tags. We will refer to this sort
of entropy as “POS entropy”.
</bodyText>
<sectionHeader confidence="0.996981" genericHeader="method">
4 Empirical validation
</sectionHeader>
<subsectionHeader confidence="0.98415">
4.1 Subjects and stimuli
</subsectionHeader>
<bodyText confidence="0.999996611111111">
In order to test the psycholinguistic relevance of
the different measures produced by the parser, we
conducted a word by word reading experiment.
23 native speakers of English read 4 short texts
(mean length: 883.5 words, 49.25 sentences). The
texts were the written versions of narratives used
in a parallel fMRI experiment making use of the
same parser derived measures and whose results
will be published in a different paper (Bachrach et
al., 2009). The narratives contained a high density
of syntactically complex structures (in the form of
sentential embeddings, relative clauses and other
non-local dependencies) but were constructed so
as to appear highly natural. The modified version
of the Roark parser, trained on the Brown Cor-
pus section of the Penn Treebank (Marcus et al.,
1993), was used to parse the different narratives
and produce the word by word measures.
</bodyText>
<subsectionHeader confidence="0.990222">
4.2 Procedure
</subsectionHeader>
<bodyText confidence="0.999933">
Each narrative was presented line by line (cer-
tain sentences required more than one line) on a
computer screen (Dell Optiplex 755 running Win-
dows XP Professional) using Linger 2.882. Each
line contained 11.5 words on average. Each word
would appear in its relative position on the screen.
The subject would then be required to push a key-
board button to advance to the next word. The
original word would then disappear and the fol-
lowing word appear in the subsequent position on
the screen. After certain sentences a comprehen-
sion question would appear on the screen (10 per
narrative). This was done in order to encourage
</bodyText>
<footnote confidence="0.631794">
2http://tedlab.mit.edu/∼dr/Linger/readme.html
</footnote>
<bodyText confidence="0.999334">
subjects to pay attention and to provide data for a
post-hoc evaluation of comprehension. After each
narrative, subjects were instructed to take a short
break (2 minutes on average).
</bodyText>
<subsectionHeader confidence="0.999942">
4.3 Data analysis
</subsectionHeader>
<bodyText confidence="0.999866613636364">
The log (base 10) of the reaction times were ana-
lyzed using a linear mixed effects regression anal-
ysis implemented in the language R (Bates et al.,
2008). Reaction times longer than 1500 ms and
shorter than 150 ms (raw) were excluded from the
analysis (4.8% of total data). Since button press la-
tencies inferior to 150 ms must have been planned
prior to the presentation of the word, we consid-
ered that they could not reflect stimulus driven ef-
fects. Data from the first and last words on each
line were discarded.
The combined data from the 4 narratives was
first modeled using a model which included or-
der of word in the narrative3, word length, parser-
derived lexical surprisal, unigram frequency, bi-
gram probability, syntactic surprisal, lexical en-
tropy, syntactic entropy and mean number of
parser derivation steps as numeric regressors. We
also included the unlexicalized POS variants of
syntactic surprisal and entropy, along the lines of
Demberg and Keller (2008), as detailed in § 3.
Table 1 presents the correlations between these
mean-centered measures.
In addition, we modeled word class
(open/closed) as a categorical factor in order
to assess interaction between class and the vari-
ables of interest, since such an interaction has
been observed in the case of frequency (Bradley,
1983). Finally, the random effect part of the
model included intercepts for subjects, words and
sentences. We report significant effects at the
threshold p &lt; .05.
Given the presence of significant interactions
between lexical class (open/closed) and a number
of the variables of interests, we decided to split
the data set into open and closed class words and
model these separately (linear mixed effects with
the same numeric variables as in the full model).
In order to evaluate the usefulness of splitting
total surprisal into lexical and syntactic compo-
nents we compared, using a likelihood ratio test,
a model where lexical and syntactic surprisal are
modeled as distinct regressors to a model where a
single regressor equal to their sum (total surprisal)
</bodyText>
<footnote confidence="0.9978865">
3This is a regressor to control for the trend of subjects to
read faster later in the narrative.
</footnote>
<page confidence="0.995362">
329
</page>
<table confidence="0.999917181818182">
Predictor SynH LexH SynS LexS Freq Bgrm PosS PosH Step WLen
Syntactic Entropy (SynH) 1.00 -0.26 0.00 0.24 -0.24 0.20 0.02 0.55 -0.05 0.18
Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29
Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03
Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64
Unigram Frequency (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72
Bigram Probability (Bgrm) 0.20 -0.38 0.18 0.87 -0.69 1.00 0.11 -0.11 -0.16 0.56
POS Surprisal (PosS) 0.02 -0.03 0.77 -0.10 0.02 0.11 1.00 0.22 0.32 0.02
POS Entropy (PosH) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11
Derivation steps (Step) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24
Word Length (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00
</table>
<tableCaption confidence="0.9882825">
Table 1: Correlations between (mean-centered) predictors. Note that unigram frequencies were represented as logs, other
scores as negative logs, hence the sign of the correlations.
</tableCaption>
<bodyText confidence="0.998853428571429">
was included. If the larger model provides a sig-
nificantly better fit than the smaller model, this
provides evidence that distinguishing between lex-
ical and syntactic contributions to surprisal is rel-
evant. Since total entropy is not a sum of syntactic
and lexical entropy, an analogous test would not
be valid in that case.
</bodyText>
<subsectionHeader confidence="0.856167">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999834454545454">
All subjects successfully answered the com-
prehension questions (92.8% correct responses,
S.D.=5.1). In the full model, we observed signifi-
cant main effects of word class as well as of lexical
surprisal, bigram probability, unigram frequency,
syntactic entropy, POS entropy and of order in the
narrative. Syntactic surprisal, lexical entropy and
number of steps had no significant effect. Word
length also had no significant main effect but inter-
acted significantly with word class (open/closed).
Word class also interacted significantly with lexi-
cal surprisal, unigram frequency and syntactic sur-
prisal.
The presence of these interactions led us to
construct models restricted to open and closed
class items respectively. The estimated parame-
ters are reported in Table 2. Reading time for open
class words showed significant effects of unigram
frequency, syntactic surprisal, syntactic entropy,
POS entropy and order within the narrative. The
positive effect of length approached significance.
Reading time for closed class words exhibited sig-
nificant effects of lexical surprisal, bigram prob-
ability, syntactic entropy and order in the narra-
tive. Length had a non-significant negative effect,
thus explaining the interaction observed in the full
model.
The models with separate lexical and syntac-
tic surprisal performed better than models includ-
ing combined surprisal. For open class words, the
Akaike’s information criterion (AIC) was -54810
for the combined model and -54819 for the inde-
pendent model (likelihood ratio test comparing the
</bodyText>
<table confidence="0.999882814814815">
Estimate Std. Error t-value
Open-class
(Intercept) 2.40×10+00 2.39×10−02 100.4*
Lexical Surprisal -1.99×10−04 7.28×10−04 -0.3
Word Length 8.97×10−04 4.62×10−04 1.9
Bigram 4.18×10−04 5.27×10−04 0.8
Unigram Freq -2.43×10−03 1.20×10−03 -2.0*
Derivation Steps -1.17×10−03 9.02×10−04 -1.3
Syntactic Entropy 2.55×10−03 6.19×10−04 4.1*
Lexical Entropy 3.96×10−04 6.68×10−04 0.6
Syntactic Surprisal 3.28×10−03 9.71×10−04 3.4*
Order in narrative -1.43×10−05 4.34×10−06 -3.3*
POS Surprisal -6.84×10−04 8.11×10−04 -0.8
POS Entropy 1.47×10−03 6.05×10−04 2.4*
Closed-class
(Intercept) 2.42×10+00 2.32×10−02 104.3*
Lexical Surprisal 2.02×10−03 7.84×10−04 2.6*
Word Length -1.87×10−03 1.13×10−03 -1.7
Bigram 1.19×10−03 4.94×10−04 2.4*
Unigram Freq 1.69×10−03 2.67×10−03 0.6
Derivation Steps 3.01×10−04 5.09×10−04 0.6
Syntactic Entropy 3.15×10−03 5.05×10−04 6.2*
Lexical Entropy 1.83×10−04 8.63×10−04 0.2
Syntactic Surprisal 3.00×10−04 8.35×10−04 0.4
Order in narrative -1.33×10−05 3.99×10−06 -3.3*
POS Surprisal -6.46×10−04 6.81×10−04 -0.9
POS Entropy 6.63×10−04 5.04×10−04 1.3
</table>
<tableCaption confidence="0.9956255">
Table 2: Estimated effects from mixed effects models on
open and closed items (stars denote significance at p&lt;.05)
</tableCaption>
<bodyText confidence="0.92814625">
two, nested, models: x2(1)=10.7,p&lt;.001). For
closed class items, combined model’s AIC was -
61467 and full model’s AIC was -61469 (likeli-
hood ratio test: x2(1)=3.54,p=0.06).
</bodyText>
<subsectionHeader confidence="0.97969">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999378636363636">
Our results demonstrate the relevance of model-
ing psycholinguistic processes using an incremen-
tal probabilistic parser, and the utility of the novel
measures presented here. Of particular interest
are: the significant effects of our syntactic en-
tropy measure; the independent contributions of
lexical surprisal, bigram probability and unigram
frequency; and the differences between the pre-
dictions of the lexicalized parsing model and the
unlexicalized (POS) parsing model.
The effect of entropy, or uncertainty regarding
</bodyText>
<page confidence="0.994146">
330
</page>
<bodyText confidence="0.999988163265306">
the upcoming input independent of the surprise
of that input, has been observed in non-linguistic
tasks (Hyman, 1953; Bestmann et al., 2008) but
to our knowledge has not been quantified before
in the context of sentence processing. The use-
fulness of computational modeling is particularly
evident in the case of entropy given the absence of
any subjective procedure for its evaluation4. The
results argue in favor of a predictive parsing archi-
tecture (Van Berkum et al., 2005). The approach
to entropy here differs from the one described in
Hale (2006) in a couple of ways. First, as dis-
cussed above, the calculation procedure is differ-
ent – we focus on extending the derivations with
just one word, rather than to all possible complete
derivations. Second, and most importantly, Hale
emphasizes entropy reduction (or the gain in in-
formation, given an input, regarding the rest of the
sentence) as the correlate of cognitive cost while
here we are interested in the amount of entropy it-
self (and not the size of change).
Interestingly, we observed only an effect of syn-
tactic entropy, not lexical entropy. Recent ERP
work has demonstrated that subjects do form spe-
cific lexical predictions in the context of sentence
processing (Van Berkum et al., 2005; DeLong et
al., 2005) and so we suspect that the absence of
lexical entropy effect might be partly due to sparse
data. Lexical surprisal and entropy were calcu-
lated using the internal state of a parser trained
on the relatively small Brown corpus. Lexical en-
tropy showed no significant effect while lexical
surprisal affected only closed class words. This
pattern of results might be due to the sparseness
of the relevant information in such a small corpus
(e.g., verb/object preferences) and the relevance of
extra-textual dimensions (world knowledge, con-
textual information) to lexical-specific prediction.
Closed class words are both more frequent (and
hence better sampled) and are less sensitive to
world knowledge, yet are often determined by the
grammatical context.
Demberg and Keller (2008) made use of the
same parsing architecture used here to compute a
syntactic surprisal measure, but used an unlexical-
ized parser (down to POS-tags rather than words)
for this score. Their “lexicalized” surprisal is
equivalent to our total surprisal (lexical surprisal
+ syntactic surprisal), while their POS surprisal is
</bodyText>
<footnote confidence="0.9934045">
4The Cloze procedure (Taylor, 1953) is one way to derive
probabilities that could be used to calculate entropy, though
this procedure is usually conducted with lexical elicitation,
which would make syntactic entropy calculations difficult.
</footnote>
<bodyText confidence="0.999775840909091">
derived from a completely different model. In con-
trast, our approach achieves lexical and syntactic
measures from the same model. In order to eval-
uate the difference between the two approaches
we added unlexicalized POS surprisal calculated
along the lines of that paper to our model, along
with an unlexicalized POS entropy from the same
model. We found no effect of unlexicalized POS
surprisal5 and a significant (but relatively small)
effect of unlexicalized POS entropy. While syn-
tactic surprisal was correlated with POS surprisal
(see Table 1) and syntactic entropy correlated with
POS entropy, the fact that our syntactic measures
still had a significant effect suggests that lexical
information contributes towards the formation of
syntactic expectations.
While the effect of surprisal calculated by an
incremental top down parser has been already
demonstrated (Demberg and Keller, 2008), our re-
sults argue for a distinction between the effect
of lexical surprisal and that of syntactic surprisal
without requiring unlexicalized parsing of the sort
that Demberg and Keller advocate. It is important
to keep in mind that this distinction between types
of prediction (and as a consequence, prediction er-
ror) is not equivalent to the one drawn in the tradi-
tional cognitive science modularity debate, which
has focused on the source of these predictions. We
found a positive effect of syntactic surprisal in the
case of open class words. The absence of an effect
for closed class words remains to be explained.
We quantified word specific surprisal using 3
sources: the parser’s internal state (lexical sur-
prisal); probability given the preceding word (neg-
ative log bigram probability); and the unigram fre-
quency of the word in a large corpus6. As can
be observed in Table 1, these three measures are
highly correlated7. This is the consequence of the
smoothing in the estimation procedure but also re-
lates to a more general fact about language use:
overall, more frequent words are also words more
expected to appear in a specific context (Anderson
and Schooler, 1991). Despite these strong corre-
lations, the three measures produced independent
</bodyText>
<footnote confidence="0.995494181818182">
5We also ran the model including unlexicalized POS sur-
prisal without our syntactic surprisal or syntactic entropy, and
in this condition the unlexicalized POS surprisal measure had
a nearly significant effect (t = 1.85), which is consistent with
the results in Boston et al. (2008a) and Demberg and Keller
(2008).
6The unigram frequencies came from the HAL corpus
(Lund and Burgess, 1996). All other statistical models were
estimated from the Brown Corpus.
7Unigram frequencies were represented as logs, the others
as negative logs, hence the sign of the correlations.
</footnote>
<page confidence="0.997484">
331
</page>
<bodyText confidence="0.999879076923077">
effects. Unigram frequency had a significant effect
for open class words while bigram probability and
lexical surprisal each had an effect on reading time
of closed class items. Bigram probability has been
often found to affect reading time using eye move-
ment measures. This is the first study to demon-
strate an additional effect of contextual surprisal
given the preceding sentential context (lexical sur-
prisal). Demberg and Keller found no effect for
surprisal once bigram and unigram probabilities
were included in the model but, importantly, they
did not distinguish lexical and syntactic surprisal,
rather “lexicalized” and “unlexicalized” surprisal.
</bodyText>
<sectionHeader confidence="0.998307" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.99998892">
We have presented novel methods for teasing apart
syntactic and lexical surprisal from a fully lexi-
calized parser, as well as for extending the oper-
ation of a predictive parser to capture novel en-
tropy measures that are also shown to be rele-
vant to psycholinguistic modeling. Such auto-
matic methods provide psycholinguistically rele-
vant measures that are intractable to calculate by
hand. The empirical validation presented here
demonstrated that the new measures – particularly
syntactic entropy and syntactic surprisal – have
high utility for modeling human reading time data.
Our approach to calculating syntactic surprisal,
based on fully lexicalized parsing, provided sig-
nificant effects, while the POS-tag based (unlexi-
calized) surprisal – of the sort used in Boston et
al. (2008a) and Demberg and Keller (2008) – did
not provide a significant effect in our trials. Fur-
ther, we showed an effect of lexical surprisal for
closed class words even when combined with uni-
gram and bigram probabilities in the same model.
This work contributes to the important, develop-
ing enterprise of leveraging data-driven NLP ap-
proaches to derive new measures of high utility for
psycholinguistic and neuropsychological studies.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999939571428571">
Thanks to Michael Collins, John Hale and Shravan
Vasishth for valuable discussions about this work.
This research was supported in part by NSF Grant
#BCS-0826654. Any opinions, findings, conclu-
sions or recommendations expressed in this publi-
cation are those of the authors and do not neces-
sarily reflect the views of the NSF.
</bodyText>
<sectionHeader confidence="0.983447" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999318928571429">
J.R. Anderson and L.J. Schooler. 1991. Reflections of
the environment in memory. Psychological Science,
2(6):396–408.
A. Bachrach, B. Roark, A. Marantz, S. Whitfield-
Gabrieli, C. Cardenas, and J.D.E. Gabrieli. 2009.
Incremental prediction in naturalistic language pro-
cessing: An fMRI study. In preparation.
D. Bates, M. Maechler, and B. Dai, 2008. lme4: Linear
mixed-effects models using S4 classes. R package
version 0.999375-20.
S. Bestmann, L.M. Harrison, F. Blankenburg, R.B.
Mars, P. Haggard, and K.J. Friston. 2008. Influence
of uncertainty and surprise on human corticospinal
excitability during preparation for action. Current
Biology, 18:775–780.
M. Ferrara Boston, J.T. Hale, R. Kliegl, U. Patil, and
S. Vasishth. 2008a. Parsing costs as predictors
of reading difficulty: An evaluation using the Pots-
dam sentence corpus. Journal of Eye Movement Re-
search, 2(1):1–12.
M. Ferrara Boston, J.T. Hale, R. Kliegl, and S. Va-
sishth. 2008b. Surprising parser actions and read-
ing difficulty. In Proceedings of ACL-08:HLT, Short
Papers, pages 5–8.
D.C. Bradley. 1983. Computational Distinctions of
Vocabulary Type. Indiana University Linguistics
Club, Bloomington.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic
structure for language modeling. In Proceedings of
ACL-COLING, pages 225–231.
M.J. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proceedings of
ACL, pages 111–118.
K.A. DeLong, T.P. Urbach, and M. Kutas. 2005. Prob-
abilistic word pre-activation during language com-
prehension inferred from electrical brain activity.
Nature Neuroscience, 8(8):1117–1121.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451–455.
L. Frazier. 1985. Syntactic complexity. In D.R.
Dowty, L. Karttunen, and A.M. Zwicky, editors,
Natural Language Parsing. Cambridge University
Press, Cambridge, UK.
K. Gabani, M. Sherman, T. Solorio, and Y. Liu.
2009. A corpus-based approach for the prediction
of language impairment in monolingual English and
Spanish-English bilingual children. In Proceedings
of NAACL-HLT.
S.M. Garnsey, N.J. Pearlmutter, E. Myers, and M.A.
Lotocky. 1997. The contributions of verb bias and
plausibility to the comprehension of temporarily am-
biguous sentences. Journal of Memory and Lan-
guage, 37(1):58–93.
</reference>
<page confidence="0.979484">
332
</page>
<reference confidence="0.99995858974359">
E. Gibson. 1998. Linguistic complexity: locality of
syntactic dependencies. Cognition, 68(1):1–76.
E. Gibson. 2006. The interaction of top-down and
bottom-up statistics in the resolution of syntactic
category ambiguity. Journal of Memory and Lan-
guage, 54(3):363–388.
J.T. Hale. 2001. A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the 2nd
meeting of NAACL.
J.T. Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101–123.
J.T. Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):643–672.
J. Henderson. 2003. Inducing history representations
for broad coverage statistical parsing. In Proceed-
ings of HLT-NAACL, pages 24–31.
R. Hyman. 1953. Stimulus information as a determi-
nant of reaction time. Journal of Experimental Psy-
chology: General, 45(3):188–96.
F. Jelinek and J. Lafferty. 1991. Computation of
the probability of initial substring generation by
stochastic context-free grammars. Computational
Linguistics, 17(3):315–323.
M. Johnson and B. Roark. 2000. Compact non-left-
recursive grammars using the selective left-corner
transform and factoring. In Proceedings of COL-
ING, pages 355–361.
T. Klee and M.D. Fitzgerald. 1985. The relation be-
tween grammatical development and mean length
of utterance in morphemes. Journal of Child Lan-
guage, 12:251–269.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126–1177.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, &amp; Computers, 28:203–208.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95–135.
B. Roark, M. Mitchell, and K. Hollingshead. 2007.
Syntactic complexity measures for detecting mild
cognitive impairment. In Proceedings of BioNLP
Workshop at ACL, pages 1–8.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1–24.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Au-
tomatic measurement of syntactic development in
child langugage. In Proceedings ofACL, pages 197–
204.
T. Solorio and Y. Liu. 2008. Using language models
to identify language impairment in Spanish-English
bilingual children. In Proceedings of BioNLP Work-
shop at ACL, pages 116–117.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational Linguistics, 21(2):165–202.
W.L. Taylor. 1953. Cloze procedure: A new tool
for measuring readability. Journalism Quarterly,
30:415–433.
J.J.A. Van Berkum, C.M. Brown, P. Zwitserlood,
V.Kooijman, and P. Hagoort. 2005. Anticipat-
ing upcoming words in discourse: Evidence from
ERPs and reading times. Learning and Memory,
31(3):443–467.
V.H. Yngve. 1960. A model and an hypothesis for lan-
guage structure. Proceedings of the American Philo-
sophical Society, 104:444–466.
</reference>
<page confidence="0.999411">
333
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.612860">
<title confidence="0.995484">Deriving lexical and syntactic expectation-based for psycholinguistic modeling via incremental top-down parsing</title>
<author confidence="0.972084">Christophe</author>
<affiliation confidence="0.994121">for Spoken Language Understanding, Oregon Health &amp; Science University</affiliation>
<address confidence="0.636137">Cognitive Neuroimaging Unit, Gif sur Yvette, France</address>
<email confidence="0.995233">roark@cslu.ogi.eduasafbac@gmail.comcardenas@mit.educhristophe@pallier.org</email>
<abstract confidence="0.999698">A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006). In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Anderson</author>
<author>L J Schooler</author>
</authors>
<title>Reflections of the environment in memory.</title>
<date>1991</date>
<journal>Psychological Science,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="36845" citStr="Anderson and Schooler, 1991" startWordPosition="6000" endWordPosition="6003"> absence of an effect for closed class words remains to be explained. We quantified word specific surprisal using 3 sources: the parser’s internal state (lexical surprisal); probability given the preceding word (negative log bigram probability); and the unigram frequency of the word in a large corpus6. As can be observed in Table 1, these three measures are highly correlated7. This is the consequence of the smoothing in the estimation procedure but also relates to a more general fact about language use: overall, more frequent words are also words more expected to appear in a specific context (Anderson and Schooler, 1991). Despite these strong correlations, the three measures produced independent 5We also ran the model including unlexicalized POS surprisal without our syntactic surprisal or syntactic entropy, and in this condition the unlexicalized POS surprisal measure had a nearly significant effect (t = 1.85), which is consistent with the results in Boston et al. (2008a) and Demberg and Keller (2008). 6The unigram frequencies came from the HAL corpus (Lund and Burgess, 1996). All other statistical models were estimated from the Brown Corpus. 7Unigram frequencies were represented as logs, the others as negat</context>
</contexts>
<marker>Anderson, Schooler, 1991</marker>
<rawString>J.R. Anderson and L.J. Schooler. 1991. Reflections of the environment in memory. Psychological Science, 2(6):396–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bachrach</author>
<author>B Roark</author>
<author>A Marantz</author>
<author>S WhitfieldGabrieli</author>
<author>C Cardenas</author>
<author>J D E Gabrieli</author>
</authors>
<title>Incremental prediction in naturalistic language processing: An fMRI study.</title>
<date>2009</date>
<booktitle>In preparation.</booktitle>
<contexts>
<context position="23915" citStr="Bachrach et al., 2009" startWordPosition="3996" endWordPosition="3999"> version of the measure by training and parsing just to POS-tags. We will refer to this sort of entropy as “POS entropy”. 4 Empirical validation 4.1 Subjects and stimuli In order to test the psycholinguistic relevance of the different measures produced by the parser, we conducted a word by word reading experiment. 23 native speakers of English read 4 short texts (mean length: 883.5 words, 49.25 sentences). The texts were the written versions of narratives used in a parallel fMRI experiment making use of the same parser derived measures and whose results will be published in a different paper (Bachrach et al., 2009). The narratives contained a high density of syntactically complex structures (in the form of sentential embeddings, relative clauses and other non-local dependencies) but were constructed so as to appear highly natural. The modified version of the Roark parser, trained on the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), was used to parse the different narratives and produce the word by word measures. 4.2 Procedure Each narrative was presented line by line (certain sentences required more than one line) on a computer screen (Dell Optiplex 755 running Windows XP Professional</context>
</contexts>
<marker>Bachrach, Roark, Marantz, WhitfieldGabrieli, Cardenas, Gabrieli, 2009</marker>
<rawString>A. Bachrach, B. Roark, A. Marantz, S. WhitfieldGabrieli, C. Cardenas, and J.D.E. Gabrieli. 2009. Incremental prediction in naturalistic language processing: An fMRI study. In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bates</author>
<author>M Maechler</author>
<author>B Dai</author>
</authors>
<title>lme4: Linear mixed-effects models using S4 classes. R package version 0.999375-20.</title>
<date>2008</date>
<contexts>
<context position="25376" citStr="Bates et al., 2008" startWordPosition="4233" endWordPosition="4236">then disappear and the following word appear in the subsequent position on the screen. After certain sentences a comprehension question would appear on the screen (10 per narrative). This was done in order to encourage 2http://tedlab.mit.edu/∼dr/Linger/readme.html subjects to pay attention and to provide data for a post-hoc evaluation of comprehension. After each narrative, subjects were instructed to take a short break (2 minutes on average). 4.3 Data analysis The log (base 10) of the reaction times were analyzed using a linear mixed effects regression analysis implemented in the language R (Bates et al., 2008). Reaction times longer than 1500 ms and shorter than 150 ms (raw) were excluded from the analysis (4.8% of total data). Since button press latencies inferior to 150 ms must have been planned prior to the presentation of the word, we considered that they could not reflect stimulus driven effects. Data from the first and last words on each line were discarded. The combined data from the 4 narratives was first modeled using a model which included order of word in the narrative3, word length, parserderived lexical surprisal, unigram frequency, bigram probability, syntactic surprisal, lexical entr</context>
</contexts>
<marker>Bates, Maechler, Dai, 2008</marker>
<rawString>D. Bates, M. Maechler, and B. Dai, 2008. lme4: Linear mixed-effects models using S4 classes. R package version 0.999375-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bestmann</author>
<author>L M Harrison</author>
<author>F Blankenburg</author>
<author>R B Mars</author>
<author>P Haggard</author>
<author>K J Friston</author>
</authors>
<title>Influence of uncertainty and surprise on human corticospinal excitability during preparation for action. Current Biology,</title>
<date>2008</date>
<pages>18--775</pages>
<contexts>
<context position="32301" citStr="Bestmann et al., 2008" startWordPosition="5276" endWordPosition="5279">eling psycholinguistic processes using an incremental probabilistic parser, and the utility of the novel measures presented here. Of particular interest are: the significant effects of our syntactic entropy measure; the independent contributions of lexical surprisal, bigram probability and unigram frequency; and the differences between the predictions of the lexicalized parsing model and the unlexicalized (POS) parsing model. The effect of entropy, or uncertainty regarding 330 the upcoming input independent of the surprise of that input, has been observed in non-linguistic tasks (Hyman, 1953; Bestmann et al., 2008) but to our knowledge has not been quantified before in the context of sentence processing. The usefulness of computational modeling is particularly evident in the case of entropy given the absence of any subjective procedure for its evaluation4. The results argue in favor of a predictive parsing architecture (Van Berkum et al., 2005). The approach to entropy here differs from the one described in Hale (2006) in a couple of ways. First, as discussed above, the calculation procedure is different – we focus on extending the derivations with just one word, rather than to all possible complete der</context>
</contexts>
<marker>Bestmann, Harrison, Blankenburg, Mars, Haggard, Friston, 2008</marker>
<rawString>S. Bestmann, L.M. Harrison, F. Blankenburg, R.B. Mars, P. Haggard, and K.J. Friston. 2008. Influence of uncertainty and surprise on human corticospinal excitability during preparation for action. Current Biology, 18:775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ferrara Boston</author>
<author>J T Hale</author>
<author>R Kliegl</author>
<author>U Patil</author>
<author>S Vasishth</author>
</authors>
<title>Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam sentence corpus.</title>
<date>2008</date>
<journal>Journal of Eye Movement Research,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="3214" citStr="Boston et al., 2008" startWordPosition="467" endWordPosition="470"> in both typical and impaired populations. The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently. Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and in later work (Hale, 2003; 2006) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance. Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (Boston et al., 2008a; Boston et al., 2008b; Demberg and Keller, 2008; Levy, 2008), and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (Boston et al., 2008a), the Roark (2001) incremental top-down parser (Demberg and Keller, 2008), and an n-best version of the Nivre et al. (2007) incremental dependency parser (Boston et al., 2008a; 2008b). Deriving such measures by hand, even for a relatively limited set of stimuli, is not feasible, hence parsing plays a critical role in this developing psycholinguistic enterprise. There is no single meas</context>
<context position="4580" citStr="Boston et al. (2008" startWordPosition="680" endWordPosition="683">ived measures for psycholinguistic modeling has looked to try to derive multiple, complementary measures. One of 324 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324–333, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP the key distinctions being looked at is syntactic versus lexical expectations (Gibson, 2006). For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags). Boston et al. (2008a) capture a similar distinction by making use of an unlexicalized PCFG within an Earley parser and a fully lexicalized unlabeled dependency parser (Nivre et al., 2007). As Demberg and Keller (2008) point out, fully unlexicalized grammars ignore important lexico-syntactic information when deriving the “syntactic” expectations, such as subcategorization preferences of particular verbs, which are generally accepted to impact syntactic expectations in human sentence processing (Garnsey et al., 1997). Demberg and Keller argue, based on their results, for unlexicalized surprisal instead of lexicali</context>
<context position="37202" citStr="Boston et al. (2008" startWordPosition="6055" endWordPosition="6058"> correlated7. This is the consequence of the smoothing in the estimation procedure but also relates to a more general fact about language use: overall, more frequent words are also words more expected to appear in a specific context (Anderson and Schooler, 1991). Despite these strong correlations, the three measures produced independent 5We also ran the model including unlexicalized POS surprisal without our syntactic surprisal or syntactic entropy, and in this condition the unlexicalized POS surprisal measure had a nearly significant effect (t = 1.85), which is consistent with the results in Boston et al. (2008a) and Demberg and Keller (2008). 6The unigram frequencies came from the HAL corpus (Lund and Burgess, 1996). All other statistical models were estimated from the Brown Corpus. 7Unigram frequencies were represented as logs, the others as negative logs, hence the sign of the correlations. 331 effects. Unigram frequency had a significant effect for open class words while bigram probability and lexical surprisal each had an effect on reading time of closed class items. Bigram probability has been often found to affect reading time using eye movement measures. This is the first study to demonstrat</context>
<context position="38946" citStr="Boston et al. (2008" startWordPosition="6322" endWordPosition="6325">ive parser to capture novel entropy measures that are also shown to be relevant to psycholinguistic modeling. Such automatic methods provide psycholinguistically relevant measures that are intractable to calculate by hand. The empirical validation presented here demonstrated that the new measures – particularly syntactic entropy and syntactic surprisal – have high utility for modeling human reading time data. Our approach to calculating syntactic surprisal, based on fully lexicalized parsing, provided significant effects, while the POS-tag based (unlexicalized) surprisal – of the sort used in Boston et al. (2008a) and Demberg and Keller (2008) – did not provide a significant effect in our trials. Further, we showed an effect of lexical surprisal for closed class words even when combined with unigram and bigram probabilities in the same model. This work contributes to the important, developing enterprise of leveraging data-driven NLP approaches to derive new measures of high utility for psycholinguistic and neuropsychological studies. Acknowledgments Thanks to Michael Collins, John Hale and Shravan Vasishth for valuable discussions about this work. This research was supported in part by NSF Grant #BCS</context>
</contexts>
<marker>Boston, Hale, Kliegl, Patil, Vasishth, 2008</marker>
<rawString>M. Ferrara Boston, J.T. Hale, R. Kliegl, U. Patil, and S. Vasishth. 2008a. Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam sentence corpus. Journal of Eye Movement Research, 2(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ferrara Boston</author>
<author>J T Hale</author>
<author>R Kliegl</author>
<author>S Vasishth</author>
</authors>
<title>Surprising parser actions and reading difficulty.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08:HLT, Short Papers,</booktitle>
<pages>5--8</pages>
<contexts>
<context position="3214" citStr="Boston et al., 2008" startWordPosition="467" endWordPosition="470"> in both typical and impaired populations. The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently. Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and in later work (Hale, 2003; 2006) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance. Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (Boston et al., 2008a; Boston et al., 2008b; Demberg and Keller, 2008; Levy, 2008), and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (Boston et al., 2008a), the Roark (2001) incremental top-down parser (Demberg and Keller, 2008), and an n-best version of the Nivre et al. (2007) incremental dependency parser (Boston et al., 2008a; 2008b). Deriving such measures by hand, even for a relatively limited set of stimuli, is not feasible, hence parsing plays a critical role in this developing psycholinguistic enterprise. There is no single meas</context>
<context position="4580" citStr="Boston et al. (2008" startWordPosition="680" endWordPosition="683">ived measures for psycholinguistic modeling has looked to try to derive multiple, complementary measures. One of 324 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324–333, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP the key distinctions being looked at is syntactic versus lexical expectations (Gibson, 2006). For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags). Boston et al. (2008a) capture a similar distinction by making use of an unlexicalized PCFG within an Earley parser and a fully lexicalized unlabeled dependency parser (Nivre et al., 2007). As Demberg and Keller (2008) point out, fully unlexicalized grammars ignore important lexico-syntactic information when deriving the “syntactic” expectations, such as subcategorization preferences of particular verbs, which are generally accepted to impact syntactic expectations in human sentence processing (Garnsey et al., 1997). Demberg and Keller argue, based on their results, for unlexicalized surprisal instead of lexicali</context>
<context position="37202" citStr="Boston et al. (2008" startWordPosition="6055" endWordPosition="6058"> correlated7. This is the consequence of the smoothing in the estimation procedure but also relates to a more general fact about language use: overall, more frequent words are also words more expected to appear in a specific context (Anderson and Schooler, 1991). Despite these strong correlations, the three measures produced independent 5We also ran the model including unlexicalized POS surprisal without our syntactic surprisal or syntactic entropy, and in this condition the unlexicalized POS surprisal measure had a nearly significant effect (t = 1.85), which is consistent with the results in Boston et al. (2008a) and Demberg and Keller (2008). 6The unigram frequencies came from the HAL corpus (Lund and Burgess, 1996). All other statistical models were estimated from the Brown Corpus. 7Unigram frequencies were represented as logs, the others as negative logs, hence the sign of the correlations. 331 effects. Unigram frequency had a significant effect for open class words while bigram probability and lexical surprisal each had an effect on reading time of closed class items. Bigram probability has been often found to affect reading time using eye movement measures. This is the first study to demonstrat</context>
<context position="38946" citStr="Boston et al. (2008" startWordPosition="6322" endWordPosition="6325">ive parser to capture novel entropy measures that are also shown to be relevant to psycholinguistic modeling. Such automatic methods provide psycholinguistically relevant measures that are intractable to calculate by hand. The empirical validation presented here demonstrated that the new measures – particularly syntactic entropy and syntactic surprisal – have high utility for modeling human reading time data. Our approach to calculating syntactic surprisal, based on fully lexicalized parsing, provided significant effects, while the POS-tag based (unlexicalized) surprisal – of the sort used in Boston et al. (2008a) and Demberg and Keller (2008) – did not provide a significant effect in our trials. Further, we showed an effect of lexical surprisal for closed class words even when combined with unigram and bigram probabilities in the same model. This work contributes to the important, developing enterprise of leveraging data-driven NLP approaches to derive new measures of high utility for psycholinguistic and neuropsychological studies. Acknowledgments Thanks to Michael Collins, John Hale and Shravan Vasishth for valuable discussions about this work. This research was supported in part by NSF Grant #BCS</context>
</contexts>
<marker>Boston, Hale, Kliegl, Vasishth, 2008</marker>
<rawString>M. Ferrara Boston, J.T. Hale, R. Kliegl, and S. Vasishth. 2008b. Surprising parser actions and reading difficulty. In Proceedings of ACL-08:HLT, Short Papers, pages 5–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Bradley</author>
</authors>
<date>1983</date>
<institution>Computational Distinctions of Vocabulary Type. Indiana University Linguistics Club, Bloomington.</institution>
<contexts>
<context position="26520" citStr="Bradley, 1983" startWordPosition="4422" endWordPosition="4423">igram frequency, bigram probability, syntactic surprisal, lexical entropy, syntactic entropy and mean number of parser derivation steps as numeric regressors. We also included the unlexicalized POS variants of syntactic surprisal and entropy, along the lines of Demberg and Keller (2008), as detailed in § 3. Table 1 presents the correlations between these mean-centered measures. In addition, we modeled word class (open/closed) as a categorical factor in order to assess interaction between class and the variables of interest, since such an interaction has been observed in the case of frequency (Bradley, 1983). Finally, the random effect part of the model included intercepts for subjects, words and sentences. We report significant effects at the threshold p &lt; .05. Given the presence of significant interactions between lexical class (open/closed) and a number of the variables of interests, we decided to split the data set into open and closed class words and model these separately (linear mixed effects with the same numeric variables as in the full model). In order to evaluate the usefulness of splitting total surprisal into lexical and syntactic components we compared, using a likelihood ratio test</context>
</contexts>
<marker>Bradley, 1983</marker>
<rawString>D.C. Bradley. 1983. Computational Distinctions of Vocabulary Type. Indiana University Linguistics Club, Bloomington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL-COLING,</booktitle>
<pages>225--231</pages>
<contexts>
<context position="11423" citStr="Chelba and Jelinek, 1998" startWordPosition="1908" endWordPosition="1912">]) = � p(D) (2) DED(G,W [1,i]) From this prefix probability, we can calculate the conditional probability of each word w E T in the terminal vocabulary, given the preceding sequence W[1, i] as follows: PrefixProbG(W[1, i]w) PG(w |W[1, i]) = Pw,∈T PrefixProbG(W [1, i]w&apos;) PrefixProbG(W[1, i]w) =(3) PrefixProbG(W [1, i]) This, in fact, is precisely the conditional probability that is used for language modeling for such applications as speech recognition and machine translation, which was the motivation for various syntactic language modeling approaches (Jelinek and Lafferty, 1991; Stolcke, 1995; Chelba and Jelinek, 1998; Roark, 2001). As with language modeling, it is important to model the end of the string as well, usually with an explicit end symbol, e.g., &lt;/s&gt;. For a string W[1, i], we can calculate its prefix probability as shown above. To calculate its complete probability, we must sum the probabilities over the set of complete trees T (G, W[1, i]). In such a way, we can calculate the conditional probability of ending the string with &lt;/s&gt; given W[1, i] as follows: PG(&lt;/s&gt; |W[ 1,]) i EDET (G,W[1,i]) p(D) (4) — PrefixProbG (W [ 1, i] ) 2.1 Incremental top-down parsing In this section, we review relevant d</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>C. Chelba and F. Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proceedings of ACL-COLING, pages 225–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="6257" citStr="Collins and Roark, 2004" startWordPosition="936" endWordPosition="939">lculation of entropy as Hale (2003; 2006) defines it, because both methods require matrix inversion of a matrix with dimensionality the size of the non-terminal set. With very large lexicalized PCFGs, the size of the nonterminal set is too large for tractable matrix inversion. The use of an incremental, beam-search parser provides a tractable approximation to both measures. Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark, 2001; Henderson, 2003; Collins and Roark, 2004), and we will use such rich models to derive our scores. In addition to teasing apart syntactic and lexical surprisal (defined explicitly in §3), we present an approximation to the full entropy that Hale (2003; 2006) used to define the entropy reduction hypothesis. Such an entropy measure is derived via a predictive step, advancing the parses independently of the input, as described in §3.3. We also present syntactic and lexical alternatives for this measure, and demonstrate the utility of making such a distinction for entropy as well as surprisal. The purpose of this paper is threefold. First</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>M.J. Collins and B. Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of ACL, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A DeLong</author>
<author>T P Urbach</author>
<author>M Kutas</author>
</authors>
<title>Probabilistic word pre-activation during language comprehension inferred from electrical brain activity.</title>
<date>2005</date>
<journal>Nature Neuroscience,</journal>
<volume>8</volume>
<issue>8</issue>
<contexts>
<context position="33435" citStr="DeLong et al., 2005" startWordPosition="5466" endWordPosition="5469">extending the derivations with just one word, rather than to all possible complete derivations. Second, and most importantly, Hale emphasizes entropy reduction (or the gain in information, given an input, regarding the rest of the sentence) as the correlate of cognitive cost while here we are interested in the amount of entropy itself (and not the size of change). Interestingly, we observed only an effect of syntactic entropy, not lexical entropy. Recent ERP work has demonstrated that subjects do form specific lexical predictions in the context of sentence processing (Van Berkum et al., 2005; DeLong et al., 2005) and so we suspect that the absence of lexical entropy effect might be partly due to sparse data. Lexical surprisal and entropy were calculated using the internal state of a parser trained on the relatively small Brown corpus. Lexical entropy showed no significant effect while lexical surprisal affected only closed class words. This pattern of results might be due to the sparseness of the relevant information in such a small corpus (e.g., verb/object preferences) and the relevance of extra-textual dimensions (world knowledge, contextual information) to lexical-specific prediction. Closed class</context>
</contexts>
<marker>DeLong, Urbach, Kutas, 2005</marker>
<rawString>K.A. DeLong, T.P. Urbach, and M. Kutas. 2005. Probabilistic word pre-activation during language comprehension inferred from electrical brain activity. Nature Neuroscience, 8(8):1117–1121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Data from eyetracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>109</volume>
<issue>2</issue>
<contexts>
<context position="3263" citStr="Demberg and Keller, 2008" startWordPosition="475" endWordPosition="479">he use of broad-coverage parsing for psycholinguistic modeling has become very popular recently. Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and in later work (Hale, 2003; 2006) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance. Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (Boston et al., 2008a; Boston et al., 2008b; Demberg and Keller, 2008; Levy, 2008), and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (Boston et al., 2008a), the Roark (2001) incremental top-down parser (Demberg and Keller, 2008), and an n-best version of the Nivre et al. (2007) incremental dependency parser (Boston et al., 2008a; 2008b). Deriving such measures by hand, even for a relatively limited set of stimuli, is not feasible, hence parsing plays a critical role in this developing psycholinguistic enterprise. There is no single measure that can account for all of the factors influ</context>
<context position="4778" citStr="Demberg and Keller (2008)" startWordPosition="714" endWordPosition="717"> Processing, pages 324–333, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP the key distinctions being looked at is syntactic versus lexical expectations (Gibson, 2006). For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags). Boston et al. (2008a) capture a similar distinction by making use of an unlexicalized PCFG within an Earley parser and a fully lexicalized unlabeled dependency parser (Nivre et al., 2007). As Demberg and Keller (2008) point out, fully unlexicalized grammars ignore important lexico-syntactic information when deriving the “syntactic” expectations, such as subcategorization preferences of particular verbs, which are generally accepted to impact syntactic expectations in human sentence processing (Garnsey et al., 1997). Demberg and Keller argue, based on their results, for unlexicalized surprisal instead of lexicalized surprisal. Here we present a novel method for deriving separate syntactic and lexical surprisal measures from a fully lexicalized incremental parser, to allow for rich probabilistic grammars to </context>
<context position="18694" citStr="Demberg and Keller (2008)" startWordPosition="3120" endWordPosition="3123">the last one includes all structure except the word Wi. Then the syntactic surprisal is calculated as: SynSG(Wi) = − log PD∈D(G,W[1,i]) p(D[1, |D|−1]) 8 PD∈D(G,W[1,i−1]) p(D) 327 and the lexical surprisal is calculated as: LexSGWi l0 EDED(G,W[1,i]) ρ(D) ( ) = g EDED(G,W[1,i]) ρ(D[1, |D|−1]) (9) Note that the numerator of SynSG(WZ) is the denominator of LexSG(WZ), hence they sum to form total surprisal SG(WZ). As with total surprisal, these measures can be defined either for the full set D(G, W[1, i]) or for a pruned beam of derivations B(G, W[1, i]) C D(G, W[1, i]). Finally, we replicated the Demberg and Keller (2008) “unlexicalized” surprisal by replacing every lexical item in the training corpus with its POS-tag, and then parsing the POS-tags of the language samples rather than the words. This differs from our syntactic surprisal by having no lexical conditioning events for rule probabilities, and by having no ambiguity about the POS-tag of the lexical items in the string. We will refer to the resulting surprisal measure as “POS surprisal” to distinguish it from our syntactic surprisal measure. 3.3 Entropy Entropy scores of the sort advocated by Hale (2003; 2006) involve calculation over the set of compl</context>
<context position="26193" citStr="Demberg and Keller (2008)" startWordPosition="4368" endWordPosition="4371">prior to the presentation of the word, we considered that they could not reflect stimulus driven effects. Data from the first and last words on each line were discarded. The combined data from the 4 narratives was first modeled using a model which included order of word in the narrative3, word length, parserderived lexical surprisal, unigram frequency, bigram probability, syntactic surprisal, lexical entropy, syntactic entropy and mean number of parser derivation steps as numeric regressors. We also included the unlexicalized POS variants of syntactic surprisal and entropy, along the lines of Demberg and Keller (2008), as detailed in § 3. Table 1 presents the correlations between these mean-centered measures. In addition, we modeled word class (open/closed) as a categorical factor in order to assess interaction between class and the variables of interest, since such an interaction has been observed in the case of frequency (Bradley, 1983). Finally, the random effect part of the model included intercepts for subjects, words and sentences. We report significant effects at the threshold p &lt; .05. Given the presence of significant interactions between lexical class (open/closed) and a number of the variables of</context>
<context position="34213" citStr="Demberg and Keller (2008)" startWordPosition="5586" endWordPosition="5589"> internal state of a parser trained on the relatively small Brown corpus. Lexical entropy showed no significant effect while lexical surprisal affected only closed class words. This pattern of results might be due to the sparseness of the relevant information in such a small corpus (e.g., verb/object preferences) and the relevance of extra-textual dimensions (world knowledge, contextual information) to lexical-specific prediction. Closed class words are both more frequent (and hence better sampled) and are less sensitive to world knowledge, yet are often determined by the grammatical context. Demberg and Keller (2008) made use of the same parsing architecture used here to compute a syntactic surprisal measure, but used an unlexicalized parser (down to POS-tags rather than words) for this score. Their “lexicalized” surprisal is equivalent to our total surprisal (lexical surprisal + syntactic surprisal), while their POS surprisal is 4The Cloze procedure (Taylor, 1953) is one way to derive probabilities that could be used to calculate entropy, though this procedure is usually conducted with lexical elicitation, which would make syntactic entropy calculations difficult. derived from a completely different mode</context>
<context position="35667" citStr="Demberg and Keller, 2008" startWordPosition="5805" endWordPosition="5808">r to our model, along with an unlexicalized POS entropy from the same model. We found no effect of unlexicalized POS surprisal5 and a significant (but relatively small) effect of unlexicalized POS entropy. While syntactic surprisal was correlated with POS surprisal (see Table 1) and syntactic entropy correlated with POS entropy, the fact that our syntactic measures still had a significant effect suggests that lexical information contributes towards the formation of syntactic expectations. While the effect of surprisal calculated by an incremental top down parser has been already demonstrated (Demberg and Keller, 2008), our results argue for a distinction between the effect of lexical surprisal and that of syntactic surprisal without requiring unlexicalized parsing of the sort that Demberg and Keller advocate. It is important to keep in mind that this distinction between types of prediction (and as a consequence, prediction error) is not equivalent to the one drawn in the traditional cognitive science modularity debate, which has focused on the source of these predictions. We found a positive effect of syntactic surprisal in the case of open class words. The absence of an effect for closed class words remai</context>
<context position="37234" citStr="Demberg and Keller (2008)" startWordPosition="6060" endWordPosition="6063">consequence of the smoothing in the estimation procedure but also relates to a more general fact about language use: overall, more frequent words are also words more expected to appear in a specific context (Anderson and Schooler, 1991). Despite these strong correlations, the three measures produced independent 5We also ran the model including unlexicalized POS surprisal without our syntactic surprisal or syntactic entropy, and in this condition the unlexicalized POS surprisal measure had a nearly significant effect (t = 1.85), which is consistent with the results in Boston et al. (2008a) and Demberg and Keller (2008). 6The unigram frequencies came from the HAL corpus (Lund and Burgess, 1996). All other statistical models were estimated from the Brown Corpus. 7Unigram frequencies were represented as logs, the others as negative logs, hence the sign of the correlations. 331 effects. Unigram frequency had a significant effect for open class words while bigram probability and lexical surprisal each had an effect on reading time of closed class items. Bigram probability has been often found to affect reading time using eye movement measures. This is the first study to demonstrate an additional effect of contex</context>
<context position="38978" citStr="Demberg and Keller (2008)" startWordPosition="6327" endWordPosition="6330">l entropy measures that are also shown to be relevant to psycholinguistic modeling. Such automatic methods provide psycholinguistically relevant measures that are intractable to calculate by hand. The empirical validation presented here demonstrated that the new measures – particularly syntactic entropy and syntactic surprisal – have high utility for modeling human reading time data. Our approach to calculating syntactic surprisal, based on fully lexicalized parsing, provided significant effects, while the POS-tag based (unlexicalized) surprisal – of the sort used in Boston et al. (2008a) and Demberg and Keller (2008) – did not provide a significant effect in our trials. Further, we showed an effect of lexical surprisal for closed class words even when combined with unigram and bigram probabilities in the same model. This work contributes to the important, developing enterprise of leveraging data-driven NLP approaches to derive new measures of high utility for psycholinguistic and neuropsychological studies. Acknowledgments Thanks to Michael Collins, John Hale and Shravan Vasishth for valuable discussions about this work. This research was supported in part by NSF Grant #BCS-0826654. Any opinions, findings</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>V. Demberg and F. Keller. 2008. Data from eyetracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>6</volume>
<issue>8</issue>
<contexts>
<context position="2809" citStr="Earley (1970)" startWordPosition="410" endWordPosition="411">Liu, 2008; Gabani et al., 2009), as well as within general studies of human sentence processing (Hale, 2001; 2003; 2006). These scores cannot feasibly be derived by hand, but rather rely on large-scale statistical models and structured inference algorithms to be derived. This is quickly becoming an important application of NLP, making possible new methods in the study of human language processing in both typical and impaired populations. The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently. Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and in later work (Hale, 2003; 2006) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance. Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (Boston et al., 2008a; Boston et al., 2008b; Demberg and Keller, 2008; Levy, 2008), and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (Bos</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 6(8):451–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
</authors>
<title>Syntactic complexity.</title>
<date>1985</date>
<editor>In D.R. Dowty, L. Karttunen, and A.M. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="1391" citStr="Frazier, 1985" startWordPosition="194" endWordPosition="195">lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times. 1 Introduction Assessment of linguistic complexity has played an important role in psycholinguistics and neurolinguistics for a long time, from the use of mean length of utterance and related scores in child language development (Klee and Fitzgerald, 1985), to complexity scores related to reading difficulty in human sentence processing studies (Yngve, 1960; Frazier, 1985; Gibson, 1998). Operationally, such linguistic complexity scores are derived via deterministic manual (human) annotation and scoring algorithms of language samples. Natural language processing has been employed to automate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples. More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation. For example, scores </context>
</contexts>
<marker>Frazier, 1985</marker>
<rawString>L. Frazier. 1985. Syntactic complexity. In D.R. Dowty, L. Karttunen, and A.M. Zwicky, editors, Natural Language Parsing. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gabani</author>
<author>M Sherman</author>
<author>T Solorio</author>
<author>Y Liu</author>
</authors>
<title>A corpus-based approach for the prediction of language impairment in monolingual English and Spanish-English bilingual children.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2227" citStr="Gabani et al., 2009" startWordPosition="319" endWordPosition="322">utomate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples. More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation. For example, scores derived from variously estimated language models have been used to evaluate and classify language samples associated with neurodevelopmental or neurodegenerative disorders (Roark et al., 2007; Solorio and Liu, 2008; Gabani et al., 2009), as well as within general studies of human sentence processing (Hale, 2001; 2003; 2006). These scores cannot feasibly be derived by hand, but rather rely on large-scale statistical models and structured inference algorithms to be derived. This is quickly becoming an important application of NLP, making possible new methods in the study of human language processing in both typical and impaired populations. The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently. Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a pr</context>
</contexts>
<marker>Gabani, Sherman, Solorio, Liu, 2009</marker>
<rawString>K. Gabani, M. Sherman, T. Solorio, and Y. Liu. 2009. A corpus-based approach for the prediction of language impairment in monolingual English and Spanish-English bilingual children. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Garnsey</author>
<author>N J Pearlmutter</author>
<author>E Myers</author>
<author>M A Lotocky</author>
</authors>
<title>The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences.</title>
<date>1997</date>
<journal>Journal of Memory and Language,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="5081" citStr="Garnsey et al., 1997" startWordPosition="754" endWordPosition="757">s: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags). Boston et al. (2008a) capture a similar distinction by making use of an unlexicalized PCFG within an Earley parser and a fully lexicalized unlabeled dependency parser (Nivre et al., 2007). As Demberg and Keller (2008) point out, fully unlexicalized grammars ignore important lexico-syntactic information when deriving the “syntactic” expectations, such as subcategorization preferences of particular verbs, which are generally accepted to impact syntactic expectations in human sentence processing (Garnsey et al., 1997). Demberg and Keller argue, based on their results, for unlexicalized surprisal instead of lexicalized surprisal. Here we present a novel method for deriving separate syntactic and lexical surprisal measures from a fully lexicalized incremental parser, to allow for rich probabilistic grammars to be used to derive either measure, and demonstrate the utility of this method versus that of Demberg and Keller in empirical trials. The use of large-scale lexicalized grammars presents a problem for using an Earley parser to derive surprisal or for the calculation of entropy as Hale (2003; 2006) define</context>
</contexts>
<marker>Garnsey, Pearlmutter, Myers, Lotocky, 1997</marker>
<rawString>S.M. Garnsey, N.J. Pearlmutter, E. Myers, and M.A. Lotocky. 1997. The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences. Journal of Memory and Language, 37(1):58–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
</authors>
<title>Linguistic complexity: locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<volume>68</volume>
<issue>1</issue>
<contexts>
<context position="1406" citStr="Gibson, 1998" startWordPosition="196" endWordPosition="197">G. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times. 1 Introduction Assessment of linguistic complexity has played an important role in psycholinguistics and neurolinguistics for a long time, from the use of mean length of utterance and related scores in child language development (Klee and Fitzgerald, 1985), to complexity scores related to reading difficulty in human sentence processing studies (Yngve, 1960; Frazier, 1985; Gibson, 1998). Operationally, such linguistic complexity scores are derived via deterministic manual (human) annotation and scoring algorithms of language samples. Natural language processing has been employed to automate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples. More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation. For example, scores derived from va</context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>E. Gibson. 1998. Linguistic complexity: locality of syntactic dependencies. Cognition, 68(1):1–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
</authors>
<title>The interaction of top-down and bottom-up statistics in the resolution of syntactic category ambiguity.</title>
<date>2006</date>
<journal>Journal of Memory and Language,</journal>
<volume>54</volume>
<issue>3</issue>
<contexts>
<context position="4322" citStr="Gibson, 2006" startWordPosition="645" endWordPosition="646">nce parsing plays a critical role in this developing psycholinguistic enterprise. There is no single measure that can account for all of the factors influencing human sentence processing performance, and some of the most recent work on using parser-derived measures for psycholinguistic modeling has looked to try to derive multiple, complementary measures. One of 324 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324–333, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP the key distinctions being looked at is syntactic versus lexical expectations (Gibson, 2006). For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags). Boston et al. (2008a) capture a similar distinction by making use of an unlexicalized PCFG within an Earley parser and a fully lexicalized unlabeled dependency parser (Nivre et al., 2007). As Demberg and Keller (2008) point out, fully unlexicalized grammars ignore important lexico-syntactic information when deriving the “syntactic” expectations, such as subca</context>
</contexts>
<marker>Gibson, 2006</marker>
<rawString>E. Gibson. 2006. The interaction of top-down and bottom-up statistics in the resolution of syntactic category ambiguity. Journal of Memory and Language, 54(3):363–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd meeting of NAACL.</booktitle>
<contexts>
<context position="616" citStr="Hale (2001" startWordPosition="75" endWordPosition="76">ng lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing Brian Roark† Asaf Bachrach‡ Carlos Cardenas◦ and Christophe Pallier‡ †Center for Spoken Language Understanding, Oregon Health &amp; Science University ‡ INSERM-CEA Cognitive Neuroimaging Unit, Gif sur Yvette, France ◦MIT roark@cslu.ogi.edu asafbac@gmail.com cardenas@mit.edu christophe@pallier.org Abstract A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006). In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times. 1 Introduction Assessment of linguistic complexity has played an important role in psycholinguistics and neurolinguistics for a long time, from the use of mean length of utterance and related scores</context>
<context position="2303" citStr="Hale, 2001" startWordPosition="334" endWordPosition="335">h can have high utility in terms of reduction of time required to annotate and score samples. More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation. For example, scores derived from variously estimated language models have been used to evaluate and classify language samples associated with neurodevelopmental or neurodegenerative disorders (Roark et al., 2007; Solorio and Liu, 2008; Gabani et al., 2009), as well as within general studies of human sentence processing (Hale, 2001; 2003; 2006). These scores cannot feasibly be derived by hand, but rather rely on large-scale statistical models and structured inference algorithms to be derived. This is quickly becoming an important application of NLP, making possible new methods in the study of human language processing in both typical and impaired populations. The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently. Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and i</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>J.T. Hale. 2001. A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the 2nd meeting of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Hale</author>
</authors>
<title>The information conveyed by words in sentences.</title>
<date>2003</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="2927" citStr="Hale, 2003" startWordPosition="427" endWordPosition="428"> These scores cannot feasibly be derived by hand, but rather rely on large-scale statistical models and structured inference algorithms to be derived. This is quickly becoming an important application of NLP, making possible new methods in the study of human language processing in both typical and impaired populations. The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently. Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and in later work (Hale, 2003; 2006) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance. Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (Boston et al., 2008a; Boston et al., 2008b; Demberg and Keller, 2008; Levy, 2008), and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (Boston et al., 2008a), the Roark (2001) incremental top-down parser (Demberg and Keller, 2008), and an n-best version of </context>
<context position="5667" citStr="Hale (2003" startWordPosition="846" endWordPosition="847">g (Garnsey et al., 1997). Demberg and Keller argue, based on their results, for unlexicalized surprisal instead of lexicalized surprisal. Here we present a novel method for deriving separate syntactic and lexical surprisal measures from a fully lexicalized incremental parser, to allow for rich probabilistic grammars to be used to derive either measure, and demonstrate the utility of this method versus that of Demberg and Keller in empirical trials. The use of large-scale lexicalized grammars presents a problem for using an Earley parser to derive surprisal or for the calculation of entropy as Hale (2003; 2006) defines it, because both methods require matrix inversion of a matrix with dimensionality the size of the non-terminal set. With very large lexicalized PCFGs, the size of the nonterminal set is too large for tractable matrix inversion. The use of an incremental, beam-search parser provides a tractable approximation to both measures. Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark, 2001; Henderson, 2003; Collins and Roark, 2004), and we w</context>
<context position="19245" citStr="Hale (2003" startWordPosition="3214" endWordPosition="3215">, i]). Finally, we replicated the Demberg and Keller (2008) “unlexicalized” surprisal by replacing every lexical item in the training corpus with its POS-tag, and then parsing the POS-tags of the language samples rather than the words. This differs from our syntactic surprisal by having no lexical conditioning events for rule probabilities, and by having no ambiguity about the POS-tag of the lexical items in the string. We will refer to the resulting surprisal measure as “POS surprisal” to distinguish it from our syntactic surprisal measure. 3.3 Entropy Entropy scores of the sort advocated by Hale (2003; 2006) involve calculation over the set of complete derivations consistent with the set of partial derivations. Hale performs this calculation efficiently via matrix inversion, which explains the use of relatively small-scale grammars with tractably sized non-terminal sets. Such methods are not tractable for the kinds of richly conditioned, large-scale PCFGs that we advocate using here. At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately access</context>
</contexts>
<marker>Hale, 2003</marker>
<rawString>J.T. Hale. 2003. The information conveyed by words in sentences. Journal of Psycholinguistic Research, 32(2):101–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Hale</author>
</authors>
<title>Uncertainty about the rest of the sentence.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="32713" citStr="Hale (2006)" startWordPosition="5346" endWordPosition="5347">model. The effect of entropy, or uncertainty regarding 330 the upcoming input independent of the surprise of that input, has been observed in non-linguistic tasks (Hyman, 1953; Bestmann et al., 2008) but to our knowledge has not been quantified before in the context of sentence processing. The usefulness of computational modeling is particularly evident in the case of entropy given the absence of any subjective procedure for its evaluation4. The results argue in favor of a predictive parsing architecture (Van Berkum et al., 2005). The approach to entropy here differs from the one described in Hale (2006) in a couple of ways. First, as discussed above, the calculation procedure is different – we focus on extending the derivations with just one word, rather than to all possible complete derivations. Second, and most importantly, Hale emphasizes entropy reduction (or the gain in information, given an input, regarding the rest of the sentence) as the correlate of cognitive cost while here we are interested in the amount of entropy itself (and not the size of change). Interestingly, we observed only an effect of syntactic entropy, not lexical entropy. Recent ERP work has demonstrated that subjects</context>
</contexts>
<marker>Hale, 2006</marker>
<rawString>J.T. Hale. 2006. Uncertainty about the rest of the sentence. Cognitive Science, 30(4):643–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>24--31</pages>
<contexts>
<context position="6231" citStr="Henderson, 2003" startWordPosition="934" endWordPosition="935">sal or for the calculation of entropy as Hale (2003; 2006) defines it, because both methods require matrix inversion of a matrix with dimensionality the size of the non-terminal set. With very large lexicalized PCFGs, the size of the nonterminal set is too large for tractable matrix inversion. The use of an incremental, beam-search parser provides a tractable approximation to both measures. Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark, 2001; Henderson, 2003; Collins and Roark, 2004), and we will use such rich models to derive our scores. In addition to teasing apart syntactic and lexical surprisal (defined explicitly in §3), we present an approximation to the full entropy that Hale (2003; 2006) used to define the entropy reduction hypothesis. Such an entropy measure is derived via a predictive step, advancing the parses independently of the input, as described in §3.3. We also present syntactic and lexical alternatives for this measure, and demonstrate the utility of making such a distinction for entropy as well as surprisal. The purpose of this</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>J. Henderson. 2003. Inducing history representations for broad coverage statistical parsing. In Proceedings of HLT-NAACL, pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hyman</author>
</authors>
<title>Stimulus information as a determinant of reaction time.</title>
<date>1953</date>
<journal>Journal of Experimental Psychology: General,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="32277" citStr="Hyman, 1953" startWordPosition="5274" endWordPosition="5275">evance of modeling psycholinguistic processes using an incremental probabilistic parser, and the utility of the novel measures presented here. Of particular interest are: the significant effects of our syntactic entropy measure; the independent contributions of lexical surprisal, bigram probability and unigram frequency; and the differences between the predictions of the lexicalized parsing model and the unlexicalized (POS) parsing model. The effect of entropy, or uncertainty regarding 330 the upcoming input independent of the surprise of that input, has been observed in non-linguistic tasks (Hyman, 1953; Bestmann et al., 2008) but to our knowledge has not been quantified before in the context of sentence processing. The usefulness of computational modeling is particularly evident in the case of entropy given the absence of any subjective procedure for its evaluation4. The results argue in favor of a predictive parsing architecture (Van Berkum et al., 2005). The approach to entropy here differs from the one described in Hale (2006) in a couple of ways. First, as discussed above, the calculation procedure is different – we focus on extending the derivations with just one word, rather than to a</context>
</contexts>
<marker>Hyman, 1953</marker>
<rawString>R. Hyman. 1953. Stimulus information as a determinant of reaction time. Journal of Experimental Psychology: General, 45(3):188–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="11382" citStr="Jelinek and Lafferty, 1991" startWordPosition="1902" endWordPosition="1905">i] with respect to G is PrefixProbG(W [1, i]) = � p(D) (2) DED(G,W [1,i]) From this prefix probability, we can calculate the conditional probability of each word w E T in the terminal vocabulary, given the preceding sequence W[1, i] as follows: PrefixProbG(W[1, i]w) PG(w |W[1, i]) = Pw,∈T PrefixProbG(W [1, i]w&apos;) PrefixProbG(W[1, i]w) =(3) PrefixProbG(W [1, i]) This, in fact, is precisely the conditional probability that is used for language modeling for such applications as speech recognition and machine translation, which was the motivation for various syntactic language modeling approaches (Jelinek and Lafferty, 1991; Stolcke, 1995; Chelba and Jelinek, 1998; Roark, 2001). As with language modeling, it is important to model the end of the string as well, usually with an explicit end symbol, e.g., &lt;/s&gt;. For a string W[1, i], we can calculate its prefix probability as shown above. To calculate its complete probability, we must sum the probabilities over the set of complete trees T (G, W[1, i]). In such a way, we can calculate the conditional probability of ending the string with &lt;/s&gt; given W[1, i] as follows: PG(&lt;/s&gt; |W[ 1,]) i EDET (G,W[1,i]) p(D) (4) — PrefixProbG (W [ 1, i] ) 2.1 Incremental top-down pars</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>F. Jelinek and J. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>B Roark</author>
</authors>
<title>Compact non-leftrecursive grammars using the selective left-corner transform and factoring.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>355--361</pages>
<contexts>
<context position="13892" citStr="Johnson and Roark, 2000" startWordPosition="2326" endWordPosition="2329">the partial tree, including non-local node labels such as parents, grandparents and siblings from the leftcontext, as well as c-commanding lexical items. Hence this is a lexicalized grammar, though the incremental nature precludes a general head-first strategy, rather one that looks to the left-context for c-commanding lexical items. To avoid some of the early prediction of structure, the version of the Roark parser that we used 326 performs an additional grammar transformation beyond the simple factorization already described – a selective left-corner transform of left-recursive productions (Johnson and Roark, 2000). In the transformed structure, slash categories are used to avoid predicting left-recursive structure until some explicit indication of modification is present, e.g., a preposition. The final step in parsing, following the last word in the string, is to “complete” all non-terminals in the yield of the tree. All of these open nonterminals are composite factored categories, such as S-NP-VP, which are “completed” by rewriting to E. The probability of these E productions is what allows for the calculation of the conditional probability of ending the string, shown in Eq. 4. One final note about th</context>
</contexts>
<marker>Johnson, Roark, 2000</marker>
<rawString>M. Johnson and B. Roark. 2000. Compact non-leftrecursive grammars using the selective left-corner transform and factoring. In Proceedings of COLING, pages 355–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Klee</author>
<author>M D Fitzgerald</author>
</authors>
<title>The relation between grammatical development and mean length of utterance in morphemes.</title>
<date>1985</date>
<journal>Journal of Child Language,</journal>
<pages>12--251</pages>
<contexts>
<context position="1274" citStr="Klee and Fitzgerald, 1985" startWordPosition="174" endWordPosition="177"> present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times. 1 Introduction Assessment of linguistic complexity has played an important role in psycholinguistics and neurolinguistics for a long time, from the use of mean length of utterance and related scores in child language development (Klee and Fitzgerald, 1985), to complexity scores related to reading difficulty in human sentence processing studies (Yngve, 1960; Frazier, 1985; Gibson, 1998). Operationally, such linguistic complexity scores are derived via deterministic manual (human) annotation and scoring algorithms of language samples. Natural language processing has been employed to automate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples. More interestingly, however, novel data driven methods are being increasingly employed </context>
</contexts>
<marker>Klee, Fitzgerald, 1985</marker>
<rawString>T. Klee and M.D. Fitzgerald. 1985. The relation between grammatical development and mean length of utterance in morphemes. Journal of Child Language, 12:251–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="3276" citStr="Levy, 2008" startWordPosition="480" endWordPosition="481">arsing for psycholinguistic modeling has become very popular recently. Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and in later work (Hale, 2003; 2006) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance. Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (Boston et al., 2008a; Boston et al., 2008b; Demberg and Keller, 2008; Levy, 2008), and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (Boston et al., 2008a), the Roark (2001) incremental top-down parser (Demberg and Keller, 2008), and an n-best version of the Nivre et al. (2007) incremental dependency parser (Boston et al., 2008a; 2008b). Deriving such measures by hand, even for a relatively limited set of stimuli, is not feasible, hence parsing plays a critical role in this developing psycholinguistic enterprise. There is no single measure that can account for all of the factors influencing human </context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>R. Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="37310" citStr="Lund and Burgess, 1996" startWordPosition="6072" endWordPosition="6075"> more general fact about language use: overall, more frequent words are also words more expected to appear in a specific context (Anderson and Schooler, 1991). Despite these strong correlations, the three measures produced independent 5We also ran the model including unlexicalized POS surprisal without our syntactic surprisal or syntactic entropy, and in this condition the unlexicalized POS surprisal measure had a nearly significant effect (t = 1.85), which is consistent with the results in Boston et al. (2008a) and Demberg and Keller (2008). 6The unigram frequencies came from the HAL corpus (Lund and Burgess, 1996). All other statistical models were estimated from the Brown Corpus. 7Unigram frequencies were represented as logs, the others as negative logs, hence the sign of the correlations. 331 effects. Unigram frequency had a significant effect for open class words while bigram probability and lexical surprisal each had an effect on reading time of closed class items. Bigram probability has been often found to affect reading time using eye movement measures. This is the first study to demonstrate an additional effect of contextual surprisal given the preceding sentential context (lexical surprisal). D</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K. Lund and C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, &amp; Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="24256" citStr="Marcus et al., 1993" startWordPosition="4049" endWordPosition="4052">sh read 4 short texts (mean length: 883.5 words, 49.25 sentences). The texts were the written versions of narratives used in a parallel fMRI experiment making use of the same parser derived measures and whose results will be published in a different paper (Bachrach et al., 2009). The narratives contained a high density of syntactically complex structures (in the form of sentential embeddings, relative clauses and other non-local dependencies) but were constructed so as to appear highly natural. The modified version of the Roark parser, trained on the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), was used to parse the different narratives and produce the word by word measures. 4.2 Procedure Each narrative was presented line by line (certain sentences required more than one line) on a computer screen (Dell Optiplex 755 running Windows XP Professional) using Linger 2.882. Each line contained 11.5 words on average. Each word would appear in its relative position on the screen. The subject would then be required to push a keyboard button to advance to the next word. The original word would then disappear and the following word appear in the subsequent position on the screen. After certai</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>Maltparser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K¨ubler, S. Marinov, and E. Marsi. 2007. Maltparser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>M Mitchell</author>
<author>K Hollingshead</author>
</authors>
<title>Syntactic complexity measures for detecting mild cognitive impairment.</title>
<date>2007</date>
<booktitle>In Proceedings of BioNLP Workshop at ACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1687" citStr="Roark et al., 2007" startWordPosition="236" endWordPosition="239"> has played an important role in psycholinguistics and neurolinguistics for a long time, from the use of mean length of utterance and related scores in child language development (Klee and Fitzgerald, 1985), to complexity scores related to reading difficulty in human sentence processing studies (Yngve, 1960; Frazier, 1985; Gibson, 1998). Operationally, such linguistic complexity scores are derived via deterministic manual (human) annotation and scoring algorithms of language samples. Natural language processing has been employed to automate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples. More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation. For example, scores derived from variously estimated language models have been used to evaluate and classify language samples associated with neurodevelopmental or neurodegenerative disorders (Roark et al., 2007; Solorio and Liu, 2008; Gabani et al., 2009), as well as within general studies of human sentence proces</context>
</contexts>
<marker>Roark, Mitchell, Hollingshead, 2007</marker>
<rawString>B. Roark, M. Mitchell, and K. Hollingshead. 2007. Syntactic complexity measures for detecting mild cognitive impairment. In Proceedings of BioNLP Workshop at ACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="3445" citStr="Roark (2001)" startWordPosition="508" endWordPosition="509">istic context-free grammar (PCFG) for psycholinguistic modeling; and in later work (Hale, 2003; 2006) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance. Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (Boston et al., 2008a; Boston et al., 2008b; Demberg and Keller, 2008; Levy, 2008), and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (Boston et al., 2008a), the Roark (2001) incremental top-down parser (Demberg and Keller, 2008), and an n-best version of the Nivre et al. (2007) incremental dependency parser (Boston et al., 2008a; 2008b). Deriving such measures by hand, even for a relatively limited set of stimuli, is not feasible, hence parsing plays a critical role in this developing psycholinguistic enterprise. There is no single measure that can account for all of the factors influencing human sentence processing performance, and some of the most recent work on using parser-derived measures for psycholinguistic modeling has looked to try to derive multiple, co</context>
<context position="6214" citStr="Roark, 2001" startWordPosition="932" endWordPosition="933">derive surprisal or for the calculation of entropy as Hale (2003; 2006) defines it, because both methods require matrix inversion of a matrix with dimensionality the size of the non-terminal set. With very large lexicalized PCFGs, the size of the nonterminal set is too large for tractable matrix inversion. The use of an incremental, beam-search parser provides a tractable approximation to both measures. Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark, 2001; Henderson, 2003; Collins and Roark, 2004), and we will use such rich models to derive our scores. In addition to teasing apart syntactic and lexical surprisal (defined explicitly in §3), we present an approximation to the full entropy that Hale (2003; 2006) used to define the entropy reduction hypothesis. Such an entropy measure is derived via a predictive step, advancing the parses independently of the input, as described in §3.3. We also present syntactic and lexical alternatives for this measure, and demonstrate the utility of making such a distinction for entropy as well as surprisal. Th</context>
<context position="11437" citStr="Roark, 2001" startWordPosition="1913" endWordPosition="1914">,i]) From this prefix probability, we can calculate the conditional probability of each word w E T in the terminal vocabulary, given the preceding sequence W[1, i] as follows: PrefixProbG(W[1, i]w) PG(w |W[1, i]) = Pw,∈T PrefixProbG(W [1, i]w&apos;) PrefixProbG(W[1, i]w) =(3) PrefixProbG(W [1, i]) This, in fact, is precisely the conditional probability that is used for language modeling for such applications as speech recognition and machine translation, which was the motivation for various syntactic language modeling approaches (Jelinek and Lafferty, 1991; Stolcke, 1995; Chelba and Jelinek, 1998; Roark, 2001). As with language modeling, it is important to model the end of the string as well, usually with an explicit end symbol, e.g., &lt;/s&gt;. For a string W[1, i], we can calculate its prefix probability as shown above. To calculate its complete probability, we must sum the probabilities over the set of complete trees T (G, W[1, i]). In such a way, we can calculate the conditional probability of ending the string with &lt;/s&gt; given W[1, i] as follows: PG(&lt;/s&gt; |W[ 1,]) i EDET (G,W[1,i]) p(D) (4) — PrefixProbG (W [ 1, i] ) 2.1 Incremental top-down parsing In this section, we review relevant details of the </context>
<context position="13140" citStr="Roark (2001" startWordPosition="2216" endWordPosition="2217">r is of the form A —* B A-B, where A,B E V and the factored A-B category can expand to any sequence of children categories of A that can follow B. This factorization of nary productions continues to nullary factored productions, i.e., the end of the original production A —* B1... B,,, is signaled with an empty production A-B1-... -B,,, —* E. The parser maintains a set of possible connected derivations, weighted via the PCFG. It uses a beam search, whereby the highest scoring derivations are worked on first, and derivations that fall outside of the beam are discarded. The reader is referred to Roark (2001; 2004) for specifics about the beam search. The model conditions the probability of each production on features extracted from the partial tree, including non-local node labels such as parents, grandparents and siblings from the leftcontext, as well as c-commanding lexical items. Hence this is a lexicalized grammar, though the incremental nature precludes a general head-first strategy, rather one that looks to the left-context for c-commanding lexical items. To avoid some of the early prediction of structure, the version of the Roark parser that we used 326 performs an additional grammar tran</context>
<context position="15651" citStr="Roark (2001" startWordPosition="2601" endWordPosition="2602">c-commanding lexical heads. These non-local features must be made local by encoding them in the non-terminal labels, leading to a very large non-terminal set and intractable exact inference. Heavy smoothing is required when estimating the resulting PCFG. The benefit of such a non-terminal set is a rich model, which enables a more peaked statistical distribution around high quality syntactic structures and thus more effective pruning of the search space. The fully connected left-context produced by topdown derivation strategies provides very rich features for the stochastic parsing models. See Roark (2001; 2004) for discussion of these issues. We now turn to measures that can be derived from the parser which may be of use for psycholinguistic modeling. 3 Parser and grammar derived measures 3.1 Surprisal The surprisal at word Wi is the negative log probability of Wi given the preceding words. Using prefix probabilities, this can be calculated as: PrefixProbG(W [1, i]) SG(Wi) = −log PrefixProbG(W[1, i − 1]) (5) Substituting equation 2 into this, we get SG (Wi) = − log EDED(G,W[1,i]) p(D) (6) EDED(G,W[1,i−1]) p(D) If we are using a beam-search parser, some of the derivations are pruned away. Let </context>
<context position="19680" citStr="Roark (2001)" startWordPosition="3280" endWordPosition="3281">efer to the resulting surprisal measure as “POS surprisal” to distinguish it from our syntactic surprisal measure. 3.3 Entropy Entropy scores of the sort advocated by Hale (2003; 2006) involve calculation over the set of complete derivations consistent with the set of partial derivations. Hale performs this calculation efficiently via matrix inversion, which explains the use of relatively small-scale grammars with tractably sized non-terminal sets. Such methods are not tractable for the kinds of richly conditioned, large-scale PCFGs that we advocate using here. At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures. Let H(D) be the entropy over a set of derivations D, calculated as follows: H(D) = − DED � ED�ED ρ(D&apos;) log ED�ED ρ(D&apos;) (10) ρ(D) ρ(D) If the set of derivations D = D(G, W[1, i]) is a set of partial derivations for string W [1, i], then H(D) is a measure of uncertainty over the partial derivations, i.e., the uncertainty regarding the correct analysis of what has alr</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Robust garden path parsing.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="6257" citStr="Roark, 2004" startWordPosition="938" endWordPosition="939"> entropy as Hale (2003; 2006) defines it, because both methods require matrix inversion of a matrix with dimensionality the size of the non-terminal set. With very large lexicalized PCFGs, the size of the nonterminal set is too large for tractable matrix inversion. The use of an incremental, beam-search parser provides a tractable approximation to both measures. Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark, 2001; Henderson, 2003; Collins and Roark, 2004), and we will use such rich models to derive our scores. In addition to teasing apart syntactic and lexical surprisal (defined explicitly in §3), we present an approximation to the full entropy that Hale (2003; 2006) used to define the entropy reduction hypothesis. Such an entropy measure is derived via a predictive step, advancing the parses independently of the input, as described in §3.3. We also present syntactic and lexical alternatives for this measure, and demonstrate the utility of making such a distinction for entropy as well as surprisal. The purpose of this paper is threefold. First</context>
<context position="12135" citStr="Roark (2004)" startWordPosition="2036" endWordPosition="2037">sually with an explicit end symbol, e.g., &lt;/s&gt;. For a string W[1, i], we can calculate its prefix probability as shown above. To calculate its complete probability, we must sum the probabilities over the set of complete trees T (G, W[1, i]). In such a way, we can calculate the conditional probability of ending the string with &lt;/s&gt; given W[1, i] as follows: PG(&lt;/s&gt; |W[ 1,]) i EDET (G,W[1,i]) p(D) (4) — PrefixProbG (W [ 1, i] ) 2.1 Incremental top-down parsing In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here. As presented in Roark (2004), the probabilities in the PCFG are smoothed so that the parser is guaranteed not to fail due to garden pathing, despite following a beam search strategy. Hence there is always a nonzero prefix probability as defined in Eq. 2. The parser follows a top-down leftmost derivation strategy. The grammar is factored so that every production has either a single terminal item on the right-hand side or is of the form A —* B A-B, where A,B E V and the factored A-B category can expand to any sequence of children categories of A that can follow B. This factorization of nary productions continues to nullary</context>
</contexts>
<marker>Roark, 2004</marker>
<rawString>B. Roark. 2004. Robust garden path parsing. Natural Language Engineering, 10(1):1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
<author>B MacWhinney</author>
</authors>
<title>Automatic measurement of syntactic development in child langugage.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>197--204</pages>
<contexts>
<context position="1666" citStr="Sagae et al., 2005" startWordPosition="232" endWordPosition="235">inguistic complexity has played an important role in psycholinguistics and neurolinguistics for a long time, from the use of mean length of utterance and related scores in child language development (Klee and Fitzgerald, 1985), to complexity scores related to reading difficulty in human sentence processing studies (Yngve, 1960; Frazier, 1985; Gibson, 1998). Operationally, such linguistic complexity scores are derived via deterministic manual (human) annotation and scoring algorithms of language samples. Natural language processing has been employed to automate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples. More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation. For example, scores derived from variously estimated language models have been used to evaluate and classify language samples associated with neurodevelopmental or neurodegenerative disorders (Roark et al., 2007; Solorio and Liu, 2008; Gabani et al., 2009), as well as within general studies of </context>
</contexts>
<marker>Sagae, Lavie, MacWhinney, 2005</marker>
<rawString>K. Sagae, A. Lavie, and B. MacWhinney. 2005. Automatic measurement of syntactic development in child langugage. In Proceedings ofACL, pages 197– 204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Solorio</author>
<author>Y Liu</author>
</authors>
<title>Using language models to identify language impairment in Spanish-English bilingual children.</title>
<date>2008</date>
<booktitle>In Proceedings of BioNLP Workshop at ACL,</booktitle>
<pages>116--117</pages>
<contexts>
<context position="2205" citStr="Solorio and Liu, 2008" startWordPosition="315" endWordPosition="318"> has been employed to automate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples. More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation. For example, scores derived from variously estimated language models have been used to evaluate and classify language samples associated with neurodevelopmental or neurodegenerative disorders (Roark et al., 2007; Solorio and Liu, 2008; Gabani et al., 2009), as well as within general studies of human sentence processing (Hale, 2001; 2003; 2006). These scores cannot feasibly be derived by hand, but rather rely on large-scale statistical models and structured inference algorithms to be derived. This is quickly becoming an important application of NLP, making possible new methods in the study of human language processing in both typical and impaired populations. The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently. Hale (2001) suggested a measure (surprisal) derived from an Earley (1</context>
</contexts>
<marker>Solorio, Liu, 2008</marker>
<rawString>T. Solorio and Y. Liu. 2008. Using language models to identify language impairment in Spanish-English bilingual children. In Proceedings of BioNLP Workshop at ACL, pages 116–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic contextfree parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="11397" citStr="Stolcke, 1995" startWordPosition="1906" endWordPosition="1907">ixProbG(W [1, i]) = � p(D) (2) DED(G,W [1,i]) From this prefix probability, we can calculate the conditional probability of each word w E T in the terminal vocabulary, given the preceding sequence W[1, i] as follows: PrefixProbG(W[1, i]w) PG(w |W[1, i]) = Pw,∈T PrefixProbG(W [1, i]w&apos;) PrefixProbG(W[1, i]w) =(3) PrefixProbG(W [1, i]) This, in fact, is precisely the conditional probability that is used for language modeling for such applications as speech recognition and machine translation, which was the motivation for various syntactic language modeling approaches (Jelinek and Lafferty, 1991; Stolcke, 1995; Chelba and Jelinek, 1998; Roark, 2001). As with language modeling, it is important to model the end of the string as well, usually with an explicit end symbol, e.g., &lt;/s&gt;. For a string W[1, i], we can calculate its prefix probability as shown above. To calculate its complete probability, we must sum the probabilities over the set of complete trees T (G, W[1, i]). In such a way, we can calculate the conditional probability of ending the string with &lt;/s&gt; given W[1, i] as follows: PG(&lt;/s&gt; |W[ 1,]) i EDET (G,W[1,i]) p(D) (4) — PrefixProbG (W [ 1, i] ) 2.1 Incremental top-down parsing In this sec</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>A. Stolcke. 1995. An efficient probabilistic contextfree parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Taylor</author>
</authors>
<title>Cloze procedure: A new tool for measuring readability.</title>
<date>1953</date>
<journal>Journalism Quarterly,</journal>
<pages>30--415</pages>
<contexts>
<context position="34568" citStr="Taylor, 1953" startWordPosition="5642" endWordPosition="5643"> knowledge, contextual information) to lexical-specific prediction. Closed class words are both more frequent (and hence better sampled) and are less sensitive to world knowledge, yet are often determined by the grammatical context. Demberg and Keller (2008) made use of the same parsing architecture used here to compute a syntactic surprisal measure, but used an unlexicalized parser (down to POS-tags rather than words) for this score. Their “lexicalized” surprisal is equivalent to our total surprisal (lexical surprisal + syntactic surprisal), while their POS surprisal is 4The Cloze procedure (Taylor, 1953) is one way to derive probabilities that could be used to calculate entropy, though this procedure is usually conducted with lexical elicitation, which would make syntactic entropy calculations difficult. derived from a completely different model. In contrast, our approach achieves lexical and syntactic measures from the same model. In order to evaluate the difference between the two approaches we added unlexicalized POS surprisal calculated along the lines of that paper to our model, along with an unlexicalized POS entropy from the same model. We found no effect of unlexicalized POS surprisal</context>
</contexts>
<marker>Taylor, 1953</marker>
<rawString>W.L. Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Quarterly, 30:415–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J A Van Berkum</author>
<author>C M Brown</author>
<author>P Zwitserlood</author>
<author>V Kooijman</author>
<author>P Hagoort</author>
</authors>
<title>Anticipating upcoming words in discourse: Evidence from ERPs and reading times.</title>
<date>2005</date>
<journal>Learning and Memory,</journal>
<volume>31</volume>
<issue>3</issue>
<marker>Van Berkum, Brown, Zwitserlood, Kooijman, Hagoort, 2005</marker>
<rawString>J.J.A. Van Berkum, C.M. Brown, P. Zwitserlood, V.Kooijman, and P. Hagoort. 2005. Anticipating upcoming words in discourse: Evidence from ERPs and reading times. Learning and Memory, 31(3):443–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V H Yngve</author>
</authors>
<title>A model and an hypothesis for language structure.</title>
<date>1960</date>
<booktitle>Proceedings of the American Philosophical Society,</booktitle>
<pages>104--444</pages>
<contexts>
<context position="1376" citStr="Yngve, 1960" startWordPosition="191" endWordPosition="193">rser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times. 1 Introduction Assessment of linguistic complexity has played an important role in psycholinguistics and neurolinguistics for a long time, from the use of mean length of utterance and related scores in child language development (Klee and Fitzgerald, 1985), to complexity scores related to reading difficulty in human sentence processing studies (Yngve, 1960; Frazier, 1985; Gibson, 1998). Operationally, such linguistic complexity scores are derived via deterministic manual (human) annotation and scoring algorithms of language samples. Natural language processing has been employed to automate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples. More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation. For e</context>
</contexts>
<marker>Yngve, 1960</marker>
<rawString>V.H. Yngve. 1960. A model and an hypothesis for language structure. Proceedings of the American Philosophical Society, 104:444–466.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>