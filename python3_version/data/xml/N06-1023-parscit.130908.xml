<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9963615">
A Fully-Lexicalized Probabilistic Model
for Japanese Syntactic and Case Structure Analysis
</title>
<author confidence="0.577301">
Daisuke Kawahara∗ and Sadao Kurohashi†
</author>
<affiliation confidence="0.544813">
Graduate School of Information Science and Technology, University of Tokyo
</affiliation>
<address confidence="0.820606">
7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656, Japan
</address>
<email confidence="0.998982">
{kawahara,kuro}@kc.t.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999794533333333">
We present an integrated probabilistic
model for Japanese syntactic and case
structure analysis. Syntactic and case
structure are simultaneously analyzed
based on wide-coverage case frames that
are constructed from a huge raw corpus in
an unsupervised manner. This model se-
lects the syntactic and case structure that
has the highest generative probability. We
evaluate both syntactic structure and case
structure. In particular, the experimen-
tal results for syntactic analysis on web
sentences show that the proposed model
significantly outperforms known syntactic
analyzers.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999276181818182">
Case structure (predicate-argument structure or log-
ical form) represents what arguments are related to
a predicate, and forms a basic unit for conveying the
meaning of natural language text. Identifying such
case structure plays an important role in natural lan-
guage understanding.
In English, syntactic case structure can be mostly
derived from word order. For example, the left ar-
gument of the predicate is the subject, and the right
argument of the predicate is the object in most cases.
Blaheta and Charniak proposed a statistical method
</bodyText>
<affiliation confidence="0.955747">
∗Currently, National Institute of Information and Communi-
cations Technology, JAPAN, dk@nict.go.jp
†Currently, Graduate School of Informatics, Kyoto Univer-
</affiliation>
<email confidence="0.926953">
sity, kuro@i.kyoto-u.ac.jp
</email>
<bodyText confidence="0.96803906060606">
for analyzing function tags in Penn Treebank, and
achieved a really high accuracy of 95.7% for syn-
tactic roles, such as SBJ (subject) and DTV (da-
tive) (Blaheta and Charniak, 2000). In recent years,
there have been many studies on semantic structure
analysis (semantic role labeling) based on PropBank
(Kingsbury et al., 2002) and FrameNet (Baker et al.,
1998). These studies classify syntactic roles into se-
mantic ones such as agent, experiencer and instru-
ment.
Case structure analysis of Japanese is very differ-
ent from that of English. In Japanese, postpositions
are used to mark cases. Frequently used postposi-
tions are “ga”, “wo” and “ni”, which usually mean
nominative, accusative and dative. However, when
an argument is followed by the topic-marking post-
position “wa”, its case marker is hidden. In addi-
tion, case-marking postpositions are often omitted in
Japanese. These troublesome characteristics make
Japanese case structure analysis very difficult.
To address these problems and realize Japanese
case structure analysis, wide-coverage case frames
are required. For example, let us describe how to
apply case structure analysis to the following sen-
tence:
bentou-wa taberu
lunchbox-TM eat
(eat lunchbox)
In this sentence, taberu (eat) is a verb, and bentou-
wa (lunchbox-TM) is a case component (i.e. argu-
ment) of taberu. The case marker of “bentou-wa”
is hidden by the topic marker (TM) “wa”. The an-
alyzer matches “bentou” (lunchbox) with the most
</bodyText>
<page confidence="0.972588">
176
</page>
<note confidence="0.9297325">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 176–183,
New York, June 2006. c�2006 Association for Computational Linguistics
suitable case slot (CS) in the following case frame
of “taberu” (eat).
</note>
<tableCaption confidence="0.960283">
Table 1: Case frame examples (examples are ex-
</tableCaption>
<note confidence="0.73768525">
pressed only in English for space limitation.).
examples
person, child, boy, · · ·
lunch, lunchbox, dinner, · · ·
</note>
<bodyText confidence="0.999898945945946">
Since “bentou” (lunchbox) is included in “wo” ex-
amples, its case is analyzed as “wo”. As a result, we
obtain the case structure “φ:ga bentou:wo taberu”,
which means that “ga” (nominative) argument is
omitted, and “wo” (accusative) argument is “bentou”
(lunchbox). In this paper, we run such case structure
analysis based on example-based case frames that
are constructed from a huge raw corpus in an unsu-
pervised manner.
Let us consider syntactic analysis, into which our
method of case structure analysis is integrated. Re-
cently, many accurate statistical parsers have been
proposed (e.g., (Collins, 1999; Charniak, 2000) for
English, (Uchimoto et al., 2000; Kudo and Mat-
sumoto, 2002) for Japanese). Since they somehow
use lexical information in the tagged corpus, they are
called “lexicalized parsers”. On the other hand, un-
lexicalized parsers achieved an almost equivalent ac-
curacy to such lexicalized parsers (Klein and Man-
ning, 2003; Kurohashi and Nagao, 1994). Accord-
ingly, we can say that the state-of-the-art lexicalized
parsers are mainly based on unlexical (grammatical)
information due to the sparse data problem. Bikel
also indicated that Collins’ parser can use bilexical
dependencies only 1.49% of the time; the rest of
the time, it backs off to condition one word on just
phrasal and part-of-speech categories (Bikel, 2004).
This paper aims at exploiting much more lexical
information, and proposes a fully-lexicalized proba-
bilistic model for Japanese syntactic and case struc-
ture analysis. Lexical information is extracted not
from a small tagged corpus, but from a huge raw cor-
pus as case frames. This model performs case struc-
ture analysis by a generative probabilistic model
based on the case frames, and selects the syntactic
structure that has the highest case structure proba-
bility.
</bodyText>
<sectionHeader confidence="0.940619" genericHeader="introduction">
2 Automatically Constructed Case Frames
</sectionHeader>
<bodyText confidence="0.978997">
We employ automatically constructed case frames
(Kawahara and Kurohashi, 2002) for our model of
</bodyText>
<table confidence="0.988199611111111">
CS examples
youritsu (1) ga &lt;agent&gt;, group, party, · · ·
(support) wo &lt;agent&gt;, candidate, applicant
ni &lt;agent&gt;, district, election, · · ·
youritsu (2) ga &lt;agent&gt;
(support) wo &lt;agent&gt;, member, minister, · · ·
ni &lt;agent&gt;, candidate, successor
. ...
. ...
.
itadaku (1) ga &lt;agent&gt;
(have) wo soup
itadaku (2) ga &lt;agent&gt;
(be given) wo advice, instruction, address
kara &lt;agent&gt;, president, circle, · · ·
. ...
. ...
.
</table>
<bodyText confidence="0.992885225806452">
case structure analysis. This section outlines the
method for constructing the case frames.
A large corpus is automatically parsed, and case
frames are constructed from modifier-head exam-
ples in the resulting parses. The problems of auto-
matic case frame construction are syntactic and se-
mantic ambiguities. That is to say, the parsing re-
sults inevitably contain errors, and verb senses are
intrinsically ambiguous. To cope with these prob-
lems, case frames are gradually constructed from re-
liable modifier-head examples.
First, modifier-head examples that have no syn-
tactic ambiguity are extracted, and they are dis-
ambiguated by a couple of a verb and its closest
case component. Such couples are explicitly ex-
pressed on the surface of text, and can be consid-
ered to play an important role in sentence mean-
ings. For instance, examples are distinguished not
by verbs (e.g., “tsumu” (load/accumulate)), but by
couples (e.g., “nimotsu-wo tsumu” (load baggage)
and “keiken-wo tsumu” (accumulate experience)).
Modifier-head examples are aggregated in this way,
and yield basic case frames.
Thereafter, the basic case frames are clustered
to merge similar case frames. For example, since
“nimotsu-wo tsumu” (load baggage) and “busshi-wo
tsumu” (load supply) are similar, they are clustered.
The similarity is measured using a thesaurus (Ike-
hara et al., 1997).
Using this gradual procedure, we constructed case
frames from the web corpus (Kawahara and Kuro-
</bodyText>
<figure confidence="0.897987333333333">
taberu CS
ga
wo
</figure>
<page confidence="0.749845">
177
</page>
<figureCaption confidence="0.999902">
Figure 1: An Example of Probability Calculation.
</figureCaption>
<bodyText confidence="0.999927166666667">
hashi, 2006). The case frames were obtained from
approximately 470M sentences extracted from the
web. They consisted of 90,000 verbs, and the aver-
age number of case frames for a verb was 34.3.
In Figure 1, some examples of the resulting case
frames are shown. In this table, ‘CS’ means a case
slot. &lt;agent&gt; in the table is a generalized example,
which is given to the case slot where half of the ex-
amples belong to &lt;agent&gt; in a thesaurus (Ikehara
et al., 1997). &lt;agent&gt; is also given to “ga” case
slot that has no examples, because “ga” case com-
ponents are usually agentive and often omitted.
</bodyText>
<sectionHeader confidence="0.9467005" genericHeader="method">
3 Integrated Probabilistic Model for
Syntactic and Case Structure Analysis
</sectionHeader>
<bodyText confidence="0.999754571428571">
The proposed method gives a probability to each
possible syntactic structure T and case structure L
of the input sentence S, and outputs the syntactic
and case structure that have the highest probability.
That is to say, the system selects the syntactic struc-
ture Tbest and the case structure Lbest that maximize
the probability P(T, L|S):
</bodyText>
<equation confidence="0.9843324">
(Tbest, Lbest) = argmax P(T, L|S)
(T,L)
P(T, L, S)
P(S)
P(T, L, S) (1)
</equation>
<bodyText confidence="0.9952195">
The last equation is derived because P(S) is con-
stant.
</bodyText>
<subsectionHeader confidence="0.9883035">
3.1 Generative Model for Syntactic and Case
Structure Analysis
</subsectionHeader>
<bodyText confidence="0.999841166666667">
We propose a generative probabilistic model based
on the dependency formalism. This model considers
a clause as a unit of generation, and generates the
input sentence from the end of the sentence in turn.
P(T, L, S) is defined as the product of a probability
for generating a clause Ci as follows:
</bodyText>
<equation confidence="0.96669">
P(T, L, S) = � P(Ci|bhi) (2)
i=1..n
</equation>
<bodyText confidence="0.996829571428571">
where n is the number of clauses in S, and bhi is Ci’s
modifying bunsetsu1. The main clause Cn at the end
1In Japanese, bunsetsu is a basic unit of dependency, con-
sisting of one or more content words and the following zero or
more function words. It corresponds to a base phrase in English,
and “eojeol” in Korean.
of a sentence does not have a modifying head, but
we handle it by assuming bhn = EOS (End Of Sen-
tence).
For example, consider the sentence in Figure 1.
There are two possible dependency structures, and
for each structure the product of probabilities indi-
cated below of the tree is calculated. Finally, the
model chooses the highest-probability structure (in
this case the left one).
Ci is decomposed into its predicate type fi (in-
cluding the predicate’s inflection) and the rest case
structure CSi. This means that the predicate in-
cluded in CSi is lemmatized. Bunsetsu bhi is also
decomposed into the content part whi and the type
fhi.
</bodyText>
<equation confidence="0.900741">
P (Ci|bhi) = P(CSi, fi|whi, fhi)
= P(CSi|fi, whi, fhi)P(fi|whi, fhi)
Pz P(CSi|fi, whi)P(fi|fhi) (3)
</equation>
<bodyText confidence="0.974818833333333">
The last equation is derived because the content part
in CSi is independent of the type of its modifying
head (fhi), and in most cases, the type fi is indepen-
dent of the content part of its modifying head (whi).
For example, P(bentou-wa tabete|syuppatsu-shita)
is calculated as follows:
</bodyText>
<equation confidence="0.607158">
P (CS(bentou-wa taberu)|te, syuppatsu-suru)P (te|ta.)
</equation>
<bodyText confidence="0.9993735">
We call P(CSi|fi, whi) generative model for case
structure and P(fi|fhi) generative model for predi-
cate type. The following two sections describe these
models.
</bodyText>
<subsectionHeader confidence="0.900915">
3.2 Generative Model for Case Structure
</subsectionHeader>
<bodyText confidence="0.999824">
We propose a generative probabilistic model of case
structure. This model selects a case frame that
</bodyText>
<equation confidence="0.710856">
= argmax
(T,L)
= argmax
(T,L)
</equation>
<page confidence="0.931767">
178
</page>
<figureCaption confidence="0.996142">
Figure 2: An example of case assignment CAk.
</figureCaption>
<bodyText confidence="0.984328571428571">
matches the input case components, and makes cor-
respondences between input case components and
case slots.
A case structure CSi consists of a predicate vi,
a case frame CFl and a case assignment CAk.
Case assignment CAk represents correspondences
between input case components and case slots as
shown in Figure 2. Note that there are various pos-
sibilities of case assignment in addition to that of
Figure 2, such as corresponding “bentou” (lunch-
box) with “ga” case. Accordingly, the index k of
CAk ranges up to the number of possible case as-
signments. By splitting CSi into vi, CFl and CAk,
P(CSi|fi, whi) is rewritten as follows:
</bodyText>
<equation confidence="0.996974857142857">
P(CSi|fi, whi) = P(vi, CFl, CAk|fi, whi)
= P(vi|fi, whi)
× P(CFl|fi, whi, vi)
× P(CAk|fi, whi, vi, CFl)
≈ P(vi|whi)
× P(CFl|vi)
× P(CAk|CFl, fi) (4)
</equation>
<bodyText confidence="0.9935607">
The above approximation is given because it is
natural to consider that the predicate vi depends on
its modifying head whi, that the case frame CFl only
depends on the predicate vi, and that the case assign-
ment CAk depends on the case frame CFl and the
predicate type fi.
The probabilities P(vi|whi) and P(CFl|vi) are
estimated from case structure analysis results of a
large raw corpus. The remainder of this section il-
lustrates P(CAk|CFl, fi) in detail.
</bodyText>
<sectionHeader confidence="0.8834785" genericHeader="method">
3.2.1 Generative Probability of Case
Assignment
</sectionHeader>
<bodyText confidence="0.972093166666667">
Let us consider case assignment CAk for each
case slot sj in case frame CFl. P(CAk|CFl, fi)
can be decomposed into the following product de-
pending on whether a case slot sj is filled with an
input case component (content part nj and type fj)
or vacant:
</bodyText>
<equation confidence="0.9989224">
P(CAk|CFl, fi) =
H P(A(sj) = 1, nj, fj|CFl, fi, sj)
sj:A(sj)=1
H× P(A(sj) = 0|CFl, fi, sj)
sj:A(sj)=0
H= {P(A(sj) = 1|CFl, fi, sj)
sj:A(sj)=1
}×P(nj, fj|CFl, fi, A(sj) = 1, sj)
H× P(A(sj) = 0|CFl, fi, sj) (5)
sj:A(sj)=0
</equation>
<bodyText confidence="0.999851">
where the function A(sj) returns 1 if a case slot sj
is filled with an input case component; otherwise 0.
</bodyText>
<equation confidence="0.99895">
P(A(sj) = 1|CFl, fi,sj) and P(A(sj) =
0|CFl, fi, sj) in equation (5) can be rewritten as
P(A(sj) = 1|CFl, sj) and P(A(sj) = 0|CFl, sj),
</equation>
<bodyText confidence="0.998058866666667">
because the evaluation of case slot assignment de-
pends only on the case frame. We call these proba-
bilities generative probability of a case slot, and they
are estimated from case structure analysis results of
a large corpus.
Let us calculate P(CSi|fi,whi) using the ex-
ample in Figure 1. In the sentence, “wa” is
a topic marking (TM) postposition, and hides
the case marker. The generative probability of
case structure varies depending on the case slot
to which the topic marked phrase is assigned.
For example, when a case frame of “taberu”
(eat) CFtaberu1 with “ga” and “wo” case slots is
used, P (CS(bentou-wa taberu)|te, syuppatsu-suru)
is calculated as follows:
</bodyText>
<equation confidence="0.985894384615385">
P1(CS(bentou-wa taberu)|te, syuppatsu-suru) =
P(taberu|syuppatsu-suru)
× P(CFtaberu1|taberu)
× P(bentou, wa|CFtaberu1, te, A(wo) = 1, wo)
× P(A(wo) = 1|CFtaberu1, wo)
× P(A(ga) = 0|CFtaberu1, ga) (6)
179
P2(CS(bentou-wa taberu)|te, syupatsu-suru) =
P(taberu|syuppatsu-suru)
× P(CFtaberu1|taberu)
× P(bentou, wa|CFtaberu1, te, A(ga) = 1, ga)
× P(A(ga) = 1|CFtaberu1, ga)
× P(A(wo) = 0|CFtaberu1, wo) (7)
</equation>
<bodyText confidence="0.987725833333333">
Such probabilities are computed for each case frame
of “taberu” (eat), and the case frame and its cor-
responding case assignment that have the highest
probability are selected.
We describe the generative probability of a case
component P(nj, fj|CFl, fi, A(sj) = 1, sj) below.
</bodyText>
<sectionHeader confidence="0.9244415" genericHeader="method">
3.2.2 Generative Probability of Case
Component
</sectionHeader>
<bodyText confidence="0.999811">
We approximate the generative probability of a
case component, assuming that:
</bodyText>
<listItem confidence="0.9972525">
• a generative probability of content part nj is in-
dependent of that of type fj,
• and the interpretation of the surface case in-
cluded in fj does not depend on case frames.
</listItem>
<bodyText confidence="0.999785333333333">
Taking into account these assumptions, the genera-
tive probability of a case component is approximated
as follows:
</bodyText>
<equation confidence="0.859784333333333">
P(nj, fj|CFl, fi, A(sj) = 1, sj) ≈
P(nj|CFl, A(sj) = 1, sj) P(fj|sj, fi) (8)
P(nj|CFl, A(sj) = 1, sj) is the probability of
</equation>
<bodyText confidence="0.900743833333334">
generating a content part nj from a case slot sj in a
case frame CFl. This probability is estimated from
case frames.
Let us consider P(fj|sj, fi) in equation (8). This
is the probability of generating the type fj of a case
component that has a correspondence with the case
slot sj. Since the type fj consists of a surface case
cj2, a punctuation mark (comma) pj and a topic
marker “wa” tj, P(fj|sj, fi) is rewritten as follows
2A surface case means a postposition sequence at the end of
bunsetsu, such as “ga”, “wo”, “koso” and “demo”.
(using the chain rule):
</bodyText>
<equation confidence="0.999399857142857">
P(fj|sj, fi) = P(cj, tj, pj|sj, fi)
= P(cj|sj, fi)
× P(pj|sj, fi, cj)
× P(tj|sj, fi, cj,pj)
≈ P(cj|sj)
× P(pj|fi)
× P(tj|fi,pj) (9)
</equation>
<bodyText confidence="0.999709375">
This approximation is given by assuming that cj
only depends on sj, pj only depends on fj, and tj
depends on fj and pj. P(cj|sj) is estimated from the
Kyoto Text Corpus (Kawahara et al., 2002), in which
the relationship between a surface case marker and
a case slot is annotated by hand.
In Japanese, a punctuation mark and a topic
marker are likely to be used when their belong-
ing bunsetsu has a long distance dependency. By
considering such tendency, fi can be regarded as
(oi, ui), where oi means whether a dependent bun-
setsu gets over another head candidate before its
modifying head vi, and ui means a clause type of
vi. The value of oi is binary, and ui is one of the
clause types described in (Kawahara and Kurohashi,
1999).
</bodyText>
<equation confidence="0.9999935">
P(pj|fi) = P(pj|oi, ui) (10)
P(tj|fi,pj) = P(tj|oi,ui,pj) (11)
</equation>
<subsectionHeader confidence="0.685547">
3.3 Generative Model for Predicate Type
</subsectionHeader>
<bodyText confidence="0.999847909090909">
Now, consider P(fi|fhi) in the equation (3). This is
the probability of generating the predicate type of a
clause Ci that modifies bhi. This probability varies
depending on the type of bhi.
When bhi is a predicate bunsetsu, Ci is a subor-
dinate clause embedded in the clause of bhi. As for
the types fi and fhi, it is necessary to consider punc-
tuation marks (pi, phi) and clause types (ui, uhi).
To capture a long distance dependency indicated by
punctuation marks, ohi (whether Ci has a possible
head candidate before bhi) is also considered.
</bodyText>
<equation confidence="0.938365">
PV Bmod(fi|fhi) = PV Bmod(pi, ui|phi, uhi, ohi)
(12)
</equation>
<bodyText confidence="0.99462925">
When bhi is a noun bunsetsu, Ci is an embedded
clause in bhi. In this case, clause types and a punc-
tuation mark of the modifiee do not affect the prob-
ability.
</bodyText>
<equation confidence="0.996869">
PNBmod(fi|fhi) = PNBmod(pi|ohi) (13)
</equation>
<page confidence="0.983821">
180
</page>
<tableCaption confidence="0.967595">
Table 2: Data for parameter estimation.
</tableCaption>
<equation confidence="0.940984666666667">
probability
P(pj|oi, uj)
P(tj|oi, ui, pj)
P(pi, ui|phi, uhi, ohi)
P(cj|sj)
P(vi|whi)
P(nj|CFl, A(sj) = 1, sj)
P(CFl|vi)
P(A(sj) = {0, 11 |CFl, sj)
</equation>
<figure confidence="0.615818222222222">
what is generated
punctuation mark
topic marker
predicate type
surface case
predicate
words
case frame
case slot
</figure>
<table confidence="0.955975666666667">
data
Kyoto Text Corpus
Kyoto Text Corpus
Kyoto Text Corpus
Kyoto Text Corpus
parsing results
case frames
case structure analysis results
case structure analysis results
</table>
<tableCaption confidence="0.993373">
Table 3: Experimental results for syntactic analysis.
</tableCaption>
<table confidence="0.999035125">
baseline proposed
all 3,447/3,976 (86.7%) 3,477/3,976 (87.4%)
NB--+VB 1,310/1,547 (84.7%) 1 ,328/1,547 (85.8%)
TM 244/298 (81.9%) 242/298 (81.2%)
others 1,066/1,249 (85.3%) 1 ,086/1,249 (86.9%)
NB--+NB 525/556 (94.4%) 526/556 (94.6%)
VB--+VB 593/760 (78.0%) 601/760 (79.1%)
VB--+NB 453/497 (91.1%) 457/497 (92.0%)
</table>
<sectionHeader confidence="0.997064" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999716875">
We evaluated the syntactic structure and case struc-
ture outputted by our model. Each parameter is es-
timated using maximum likelihood from the data
described in Table 2. All of these data are not
existing or obtainable by a single process, but ac-
quired by applying syntactic analysis, case frame
construction and case structure analysis in turn. The
process of case structure analysis in this table is a
similarity-based method (Kawahara and Kurohashi,
2002). The case frames were automatically con-
structed from the web corpus comprising 470M sen-
tences, and the case structure analysis results were
obtained from 6M sentences in the web corpus.
The rest of this section first describes the exper-
iments for syntactic structure, and then reports the
experiments for case structure.
</bodyText>
<subsectionHeader confidence="0.86755">
4.1 Experiments for Syntactic Structure
</subsectionHeader>
<bodyText confidence="0.976459">
We evaluated syntactic structures analyzed by the
proposed model. Our experiments were run on
hand-annotated 675 web sentences 3. The web sen-
tences were manually annotated using the same cri-
teria as the Kyoto Text Corpus. The system input
was tagged automatically using the JUMAN mor-
phological analyzer (Kurohashi et al., 1994). The
syntactic structures obtained were evaluated with re-
3The test set is not used for case frame construction and
probability estimation.
gard to dependency accuracy — the proportion of
correct dependencies out of all dependencies except
for the last dependency in the sentence end 4.
Table 3 shows the dependency accuracy. In
the table, “baseline” means the rule-based syn-
tactic parser, KNP (Kurohashi and Nagao, 1994),
and “proposed” represents the proposed method.
The proposed method significantly outperformed the
baseline method (McNemar’s test; p &lt; 0.05). The
dependency accuracies are classified into four types
according to the bunsetsu classes (VB: verb bun-
setsu, NB: noun bunsetsu) of a dependent and its
head. The “NB→VB” type is further divided into
two types: “TM” and “others”. The type that is most
related to case structure is “others” in “NB→VB”.
Its accuracy was improved by 1.6%, and the error
rate was reduced by 10.9%. This result indicated
that the proposed method is effective in analyzing
dependencies related to case structure.
Figure 3 shows some analysis results, where the
dotted lines represent the analysis by the baseline
method, and the solid lines represent the analysis by
the proposed method. Sentence (1) and (2) are in-
correctly analyzed by the baseline but correctly ana-
lyzed by the proposed method.
There are two major causes that led to analysis
errors.
Mismatch between analysis results and annota-
tion criteria
In sentence (3) in Figure 3, the baseline
method correctly recognized the head of “iin-wa”
(commissioner-TM) as “hirakimasu” (open). How-
ever, the proposed method incorrectly judged it as
“oujite-imasuga” (offer). Both analysis results can
be considered to be correct semantically, but from
</bodyText>
<footnote confidence="0.979319333333333">
4Since Japanese is head-final, the second last bunsetsu un-
ambiguously depends on the last bunsetsu, and the last bunsetsu
has no dependency.
</footnote>
<page confidence="0.995466">
181
</page>
<figure confidence="0.992942583333333">
❄ ❄
(1) mizu-ga takai tokoro-kara hikui tokoro-he nagareru.
water-nom high ground-abl low ground-all flow
(Water flows from high ground to low ground.)
❄ ❄
(2) ... Kobe shi-ga senmonchishiki-wo motsu volunteer-wo bosyushita ...
Kobe city-nom expert knowledge-acc have volunteer-acc recruited
(Kobe city recruited a volunteer who has expert knowledge,...)
❄ ❄
(3) iin-wa, jitaku-de minasan-karano gosoudan-ni oujite-imasuga, ... soudansyo-wo hirakimasu
commissioner-TM at home all of you consultation-dat offer window open
(the commissioner offers consultation to all of you at home, but opens a window ...)
</figure>
<figureCaption confidence="0.999627">
Figure 3: Examples of analysis results.
</figureCaption>
<tableCaption confidence="0.651576">
Table 4: Experimental results for case structure anal-
ysis.
</tableCaption>
<bodyText confidence="0.995780333333333">
the viewpoint of our annotation criteria, the latter is
not a syntactic relation, but an ellipsis relation. To
address this problem, it is necessary to simultane-
ously evaluate not only syntactic relations but also
indirect relations, such as ellipses and anaphora.
Linear weighting on each probability
We proposed a generative probabilistic model,
and thus cannot optimize the weight of each proba-
bility. Such optimization could be a way to improve
the system performance. In the future, we plan to
employ a machine learning technique for the opti-
mization.
</bodyText>
<sectionHeader confidence="0.642849" genericHeader="method">
4.2 Experiments for Case Structure
</sectionHeader>
<bodyText confidence="0.999836538461539">
We applied case structure analysis to 215 web sen-
tences which are manually annotated with case
structure, and evaluated case markers of TM phrases
and clausal modifiees by comparing them with the
gold standard in the corpus. The experimental re-
sults are shown in table 4, in which the baseline
refers to a similarity-based method (Kawahara and
Kurohashi, 2002). The experimental results were re-
ally good compared to the baseline. It is difficult to
compare the results with the previous work stated in
the next section, because of different experimental
settings (e.g., our evaluation includes parse errors in
incorrect cases).
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999952">
There have been several approaches for syntactic
analysis handling lexical preference on a large scale.
Shirai et al. proposed a PGLR-based syntactic
analysis method using large-scale lexical preference
(Shirai et al., 1998). Their system learned lexical
preference from a large newspaper corpus (articles
of five years), such as P(pielwo, taberu), but did
not deal with verb sense ambiguity. They reported
84.34% accuracy on 500 relatively short sentences
from the Kyoto Text Corpus.
Fujio and Matsumoto presented a syntactic anal-
ysis method based on lexical statistics (Fujio and
Matsumoto, 1998). They made use of a probabilistic
model defined by the product of a probability of hav-
ing a dependency between two cooccurring words
and a distance probability. The model was trained
on the EDR corpus, and performed with 86.89% ac-
curacy on 10,000 sentences from the EDR corpus 5.
On the other hand, there have been a number
of machine learning-based approaches using lexical
preference as their features. Among these, Kudo
and Matsumoto yielded the best performance (Kudo
and Matsumoto, 2002). They proposed a chunking-
based dependency analysis method using Support
Vector Machines. They used two-fold cross valida-
tion on the Kyoto Text Corpus, and achieved 90.46%
</bodyText>
<footnote confidence="0.842829">
5The evaluation includes the last dependencies in the sen-
tence end, which are always correct.
</footnote>
<figure confidence="0.604293">
baseline proposed
TM 72/105 (68.6%) 82/105 (78.1%)
clause 107/155 (69.0%) 121/155 (78.1%)
</figure>
<page confidence="0.99073">
182
</page>
<bodyText confidence="0.999922466666667">
accuracy 5. However, it is very hard to learn suffi-
cient lexical preference from several tens of thou-
sands sentences of a hand-tagged corpus.
There has been some related work analyzing
clausal modifiees and TM phrases. For exam-
ple, Torisawa analyzed TM phrases using predicate-
argument cooccurences and word classifications in-
duced by the EM algorithm (Torisawa, 2001). Its
accuracy was approximately 88% for “wa” and 84%
for “mo”. It is difficult to compare the accuracy
of their system to ours, because the range of tar-
get expressions is different. Unlike related work,
it is promising to utilize the resultant case frames
for subsequent analyzes such as ellipsis or discourse
analysis.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999939222222222">
We have described an integrated probabilistic model
for syntactic and case structure analysis. This model
takes advantage of lexical selectional preference of
large-scale case frames, and performs syntactic and
case analysis simultaneously. The experiments indi-
cated the effectiveness of our model. In the future,
by incorporating ellipsis resolution, we will develop
an integrated model of syntactic, case and ellipsis
analysis.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999593432432432">
Collin Baker, Charles Fillmore, and John Lowe. 1998. The
Berkeley FrameNet Project. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics and
the 36th Annual Meeting of the Association for Computa-
tional Linguistics, pages 86–90.
Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model.
Computational Linguistics, 30(4):479–511.
Don Blaheta and Eugene Charniak. 2000. Assigning function
tags to parsed text. In Proceedings of the 1st Meeting of
the North American Chapter of the Association for Compu-
tational Linguistics, pages 234–240.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Meeting of the North American
Chapter of the Association for Computational Linguistics,
pages 132–139.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Masakazu Fujio and Yuji Matsumoto. 1998. Japanese depen-
dency structure analysis based on lexicalized statistics. In
Proceedings of the 3rd Conference on Empirical Methods in
Natural Language Processing, pages 88–96.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentarou Ogura, Yoshifumi
Oyama, and Yoshihiko Hayashi, editors. 1997. Japanese
Lexicon. Iwanami Publishing.
Daisuke Kawahara and Sadao Kurohashi. 1999. Corpus-based
dependency analysis of Japanese sentences using verb bun-
setsu transitivity. In Proceedings of the 5th Natural Lan-
guage Processing Pacific Rim Symposium, pages 387–391.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of
case frame dictionary for robust Japanese case analysis. In
Proceedings of the 19th International Conference on Com-
putational Linguistics, pages 425–431.
Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame
compilation from the web using high-performance comput-
ing. In Proceedings of the 5th International Conference on
Language Resources and Evaluation.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002.
Construction of a Japanese relevance-tagged corpus. In Pro-
ceedings of the 3rd International Conference on Language
Resources and Evaluation, pages 2008–2013.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the Penn TreeBank. In Pro-
ceedings of the Human Language Technology Conference.
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics, pages
423–430.
Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proceedings of the
Conference on Natural Language Learning, pages 29–35.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long Japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507–534.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and
Makoto Nagao. 1994. Improvements of Japanese morpho-
logical analyzer JUMAN. In Proceedings of the Interna-
tional Workshop on Sharable Natural Language, pages 22–
28.
Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and Hozumi
Tanaka. 1998. An empirical evaluation on statistical parsing
of Japanese sentences using lexical association statistics. In
Proceedings of the 3rd Conference on Empirical Methods in
Natural Language Processing, pages 80–87.
Kentaro Torisawa. 2001. An unsupervised method for canon-
icalization of Japanese postpositions. In Proceedings of the
6th Natural Language Processing Pacific Rim Simposium,
pages 211–218.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and Hi-
toshi Isahara. 2000. Dependency model using posterior
context. In Proceedings of the 6th International Workshop
on Parsing Technology, pages 321–322.
</reference>
<page confidence="0.99916">
183
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.512799">
<title confidence="0.9842725">A Fully-Lexicalized Probabilistic for Japanese Syntactic and Case Structure Analysis</title>
<author confidence="0.583042">Sadao</author>
<affiliation confidence="0.97102">Graduate School of Information Science and Technology, University of</affiliation>
<address confidence="0.907028">7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656,</address>
<abstract confidence="0.997311">We present an integrated probabilistic model for Japanese syntactic and case structure analysis. Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. This model selects the syntactic and case structure that has the highest generative probability. We evaluate both syntactic structure and case structure. In particular, the experimental results for syntactic analysis on web sentences show that the proposed model significantly outperforms known syntactic analyzers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Charles Fillmore</author>
<author>John Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="1968" citStr="Baker et al., 1998" startWordPosition="279" endWordPosition="282">s the object in most cases. Blaheta and Charniak proposed a statistical method ∗Currently, National Institute of Information and Communications Technology, JAPAN, dk@nict.go.jp †Currently, Graduate School of Informatics, Kyoto University, kuro@i.kyoto-u.ac.jp for analyzing function tags in Penn Treebank, and achieved a really high accuracy of 95.7% for syntactic roles, such as SBJ (subject) and DTV (dative) (Blaheta and Charniak, 2000). In recent years, there have been many studies on semantic structure analysis (semantic role labeling) based on PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). These studies classify syntactic roles into semantic ones such as agent, experiencer and instrument. Case structure analysis of Japanese is very different from that of English. In Japanese, postpositions are used to mark cases. Frequently used postpositions are “ga”, “wo” and “ni”, which usually mean nominative, accusative and dative. However, when an argument is followed by the topic-marking postposition “wa”, its case marker is hidden. In addition, case-marking postpositions are often omitted in Japanese. These troublesome characteristics make Japanese case structure analysis very difficul</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin Baker, Charles Fillmore, and John Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics, pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="4822" citStr="Bikel, 2004" startWordPosition="727" endWordPosition="728">xical information in the tagged corpus, they are called “lexicalized parsers”. On the other hand, unlexicalized parsers achieved an almost equivalent accuracy to such lexicalized parsers (Klein and Manning, 2003; Kurohashi and Nagao, 1994). Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem. Bikel also indicated that Collins’ parser can use bilexical dependencies only 1.49% of the time; the rest of the time, it backs off to condition one word on just phrasal and part-of-speech categories (Bikel, 2004). This paper aims at exploiting much more lexical information, and proposes a fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. Lexical information is extracted not from a small tagged corpus, but from a huge raw corpus as case frames. This model performs case structure analysis by a generative probabilistic model based on the case frames, and selects the syntactic structure that has the highest case structure probability. 2 Automatically Constructed Case Frames We employ automatically constructed case frames (Kawahara and Kurohashi, 2002) for our model </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Blaheta</author>
<author>Eugene Charniak</author>
</authors>
<title>Assigning function tags to parsed text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>234--240</pages>
<contexts>
<context position="1788" citStr="Blaheta and Charniak, 2000" startWordPosition="251" endWordPosition="254">nding. In English, syntactic case structure can be mostly derived from word order. For example, the left argument of the predicate is the subject, and the right argument of the predicate is the object in most cases. Blaheta and Charniak proposed a statistical method ∗Currently, National Institute of Information and Communications Technology, JAPAN, dk@nict.go.jp †Currently, Graduate School of Informatics, Kyoto University, kuro@i.kyoto-u.ac.jp for analyzing function tags in Penn Treebank, and achieved a really high accuracy of 95.7% for syntactic roles, such as SBJ (subject) and DTV (dative) (Blaheta and Charniak, 2000). In recent years, there have been many studies on semantic structure analysis (semantic role labeling) based on PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). These studies classify syntactic roles into semantic ones such as agent, experiencer and instrument. Case structure analysis of Japanese is very different from that of English. In Japanese, postpositions are used to mark cases. Frequently used postpositions are “ga”, “wo” and “ni”, which usually mean nominative, accusative and dative. However, when an argument is followed by the topic-marking postposition “wa”, its</context>
</contexts>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>Don Blaheta and Eugene Charniak. 2000. Assigning function tags to parsed text. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 234–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1788" citStr="Charniak, 2000" startWordPosition="253" endWordPosition="254">glish, syntactic case structure can be mostly derived from word order. For example, the left argument of the predicate is the subject, and the right argument of the predicate is the object in most cases. Blaheta and Charniak proposed a statistical method ∗Currently, National Institute of Information and Communications Technology, JAPAN, dk@nict.go.jp †Currently, Graduate School of Informatics, Kyoto University, kuro@i.kyoto-u.ac.jp for analyzing function tags in Penn Treebank, and achieved a really high accuracy of 95.7% for syntactic roles, such as SBJ (subject) and DTV (dative) (Blaheta and Charniak, 2000). In recent years, there have been many studies on semantic structure analysis (semantic role labeling) based on PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). These studies classify syntactic roles into semantic ones such as agent, experiencer and instrument. Case structure analysis of Japanese is very different from that of English. In Japanese, postpositions are used to mark cases. Frequently used postpositions are “ga”, “wo” and “ni”, which usually mean nominative, accusative and dative. However, when an argument is followed by the topic-marking postposition “wa”, its</context>
<context position="4106" citStr="Charniak, 2000" startWordPosition="615" endWordPosition="616">bentou” (lunchbox) is included in “wo” examples, its case is analyzed as “wo”. As a result, we obtain the case structure “φ:ga bentou:wo taberu”, which means that “ga” (nominative) argument is omitted, and “wo” (accusative) argument is “bentou” (lunchbox). In this paper, we run such case structure analysis based on example-based case frames that are constructed from a huge raw corpus in an unsupervised manner. Let us consider syntactic analysis, into which our method of case structure analysis is integrated. Recently, many accurate statistical parsers have been proposed (e.g., (Collins, 1999; Charniak, 2000) for English, (Uchimoto et al., 2000; Kudo and Matsumoto, 2002) for Japanese). Since they somehow use lexical information in the tagged corpus, they are called “lexicalized parsers”. On the other hand, unlexicalized parsers achieved an almost equivalent accuracy to such lexicalized parsers (Klein and Manning, 2003; Kurohashi and Nagao, 1994). Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem. Bikel also indicated that Collins’ parser can use bilexical dependencies only 1.49% of the time; </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4089" citStr="Collins, 1999" startWordPosition="613" endWordPosition="614">, · · · Since “bentou” (lunchbox) is included in “wo” examples, its case is analyzed as “wo”. As a result, we obtain the case structure “φ:ga bentou:wo taberu”, which means that “ga” (nominative) argument is omitted, and “wo” (accusative) argument is “bentou” (lunchbox). In this paper, we run such case structure analysis based on example-based case frames that are constructed from a huge raw corpus in an unsupervised manner. Let us consider syntactic analysis, into which our method of case structure analysis is integrated. Recently, many accurate statistical parsers have been proposed (e.g., (Collins, 1999; Charniak, 2000) for English, (Uchimoto et al., 2000; Kudo and Matsumoto, 2002) for Japanese). Since they somehow use lexical information in the tagged corpus, they are called “lexicalized parsers”. On the other hand, unlexicalized parsers achieved an almost equivalent accuracy to such lexicalized parsers (Klein and Manning, 2003; Kurohashi and Nagao, 1994). Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem. Bikel also indicated that Collins’ parser can use bilexical dependencies only 1.</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masakazu Fujio</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on lexicalized statistics.</title>
<date>1998</date>
<booktitle>In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>88--96</pages>
<contexts>
<context position="23285" citStr="Fujio and Matsumoto, 1998" startWordPosition="3744" endWordPosition="3747">rect cases). 5 Related Work There have been several approaches for syntactic analysis handling lexical preference on a large scale. Shirai et al. proposed a PGLR-based syntactic analysis method using large-scale lexical preference (Shirai et al., 1998). Their system learned lexical preference from a large newspaper corpus (articles of five years), such as P(pielwo, taberu), but did not deal with verb sense ambiguity. They reported 84.34% accuracy on 500 relatively short sentences from the Kyoto Text Corpus. Fujio and Matsumoto presented a syntactic analysis method based on lexical statistics (Fujio and Matsumoto, 1998). They made use of a probabilistic model defined by the product of a probability of having a dependency between two cooccurring words and a distance probability. The model was trained on the EDR corpus, and performed with 86.89% accuracy on 10,000 sentences from the EDR corpus 5. On the other hand, there have been a number of machine learning-based approaches using lexical preference as their features. Among these, Kudo and Matsumoto yielded the best performance (Kudo and Matsumoto, 2002). They proposed a chunkingbased dependency analysis method using Support Vector Machines. They used two-fol</context>
</contexts>
<marker>Fujio, Matsumoto, 1998</marker>
<rawString>Masakazu Fujio and Yuji Matsumoto. 1998. Japanese dependency structure analysis based on lexicalized statistics. In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing, pages 88–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
</authors>
<title>Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentarou Ogura, Yoshifumi Oyama,</title>
<date>1997</date>
<editor>and Yoshihiko Hayashi, editors.</editor>
<publisher>Iwanami Publishing.</publisher>
<marker>Ikehara, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentarou Ogura, Yoshifumi Oyama, and Yoshihiko Hayashi, editors. 1997. Japanese Lexicon. Iwanami Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Corpus-based dependency analysis of Japanese sentences using verb bunsetsu transitivity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>387--391</pages>
<contexts>
<context position="15959" citStr="Kawahara and Kurohashi, 1999" startWordPosition="2604" endWordPosition="2607">ends on fj and pj. P(cj|sj) is estimated from the Kyoto Text Corpus (Kawahara et al., 2002), in which the relationship between a surface case marker and a case slot is annotated by hand. In Japanese, a punctuation mark and a topic marker are likely to be used when their belonging bunsetsu has a long distance dependency. By considering such tendency, fi can be regarded as (oi, ui), where oi means whether a dependent bunsetsu gets over another head candidate before its modifying head vi, and ui means a clause type of vi. The value of oi is binary, and ui is one of the clause types described in (Kawahara and Kurohashi, 1999). P(pj|fi) = P(pj|oi, ui) (10) P(tj|fi,pj) = P(tj|oi,ui,pj) (11) 3.3 Generative Model for Predicate Type Now, consider P(fi|fhi) in the equation (3). This is the probability of generating the predicate type of a clause Ci that modifies bhi. This probability varies depending on the type of bhi. When bhi is a predicate bunsetsu, Ci is a subordinate clause embedded in the clause of bhi. As for the types fi and fhi, it is necessary to consider punctuation marks (pi, phi) and clause types (ui, uhi). To capture a long distance dependency indicated by punctuation marks, ohi (whether Ci has a possible</context>
</contexts>
<marker>Kawahara, Kurohashi, 1999</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 1999. Corpus-based dependency analysis of Japanese sentences using verb bunsetsu transitivity. In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium, pages 387–391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Fertilization of case frame dictionary for robust Japanese case analysis.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>425--431</pages>
<contexts>
<context position="5407" citStr="Kawahara and Kurohashi, 2002" startWordPosition="814" endWordPosition="817"> and part-of-speech categories (Bikel, 2004). This paper aims at exploiting much more lexical information, and proposes a fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. Lexical information is extracted not from a small tagged corpus, but from a huge raw corpus as case frames. This model performs case structure analysis by a generative probabilistic model based on the case frames, and selects the syntactic structure that has the highest case structure probability. 2 Automatically Constructed Case Frames We employ automatically constructed case frames (Kawahara and Kurohashi, 2002) for our model of CS examples youritsu (1) ga &lt;agent&gt;, group, party, · · · (support) wo &lt;agent&gt;, candidate, applicant ni &lt;agent&gt;, district, election, · · · youritsu (2) ga &lt;agent&gt; (support) wo &lt;agent&gt;, member, minister, · · · ni &lt;agent&gt;, candidate, successor . ... . ... . itadaku (1) ga &lt;agent&gt; (have) wo soup itadaku (2) ga &lt;agent&gt; (be given) wo advice, instruction, address kara &lt;agent&gt;, president, circle, · · · . ... . ... . case structure analysis. This section outlines the method for constructing the case frames. A large corpus is automatically parsed, and case frames are constructed from m</context>
<context position="18168" citStr="Kawahara and Kurohashi, 2002" startWordPosition="2954" endWordPosition="2957">66/1,249 (85.3%) 1 ,086/1,249 (86.9%) NB--+NB 525/556 (94.4%) 526/556 (94.6%) VB--+VB 593/760 (78.0%) 601/760 (79.1%) VB--+NB 453/497 (91.1%) 457/497 (92.0%) 4 Experiments We evaluated the syntactic structure and case structure outputted by our model. Each parameter is estimated using maximum likelihood from the data described in Table 2. All of these data are not existing or obtainable by a single process, but acquired by applying syntactic analysis, case frame construction and case structure analysis in turn. The process of case structure analysis in this table is a similarity-based method (Kawahara and Kurohashi, 2002). The case frames were automatically constructed from the web corpus comprising 470M sentences, and the case structure analysis results were obtained from 6M sentences in the web corpus. The rest of this section first describes the experiments for syntactic structure, and then reports the experiments for case structure. 4.1 Experiments for Syntactic Structure We evaluated syntactic structures analyzed by the proposed model. Our experiments were run on hand-annotated 675 web sentences 3. The web sentences were manually annotated using the same criteria as the Kyoto Text Corpus. The system input</context>
<context position="22404" citStr="Kawahara and Kurohashi, 2002" startWordPosition="3611" endWordPosition="3614">ive probabilistic model, and thus cannot optimize the weight of each probability. Such optimization could be a way to improve the system performance. In the future, we plan to employ a machine learning technique for the optimization. 4.2 Experiments for Case Structure We applied case structure analysis to 215 web sentences which are manually annotated with case structure, and evaluated case markers of TM phrases and clausal modifiees by comparing them with the gold standard in the corpus. The experimental results are shown in table 4, in which the baseline refers to a similarity-based method (Kawahara and Kurohashi, 2002). The experimental results were really good compared to the baseline. It is difficult to compare the results with the previous work stated in the next section, because of different experimental settings (e.g., our evaluation includes parse errors in incorrect cases). 5 Related Work There have been several approaches for syntactic analysis handling lexical preference on a large scale. Shirai et al. proposed a PGLR-based syntactic analysis method using large-scale lexical preference (Shirai et al., 1998). Their system learned lexical preference from a large newspaper corpus (articles of five yea</context>
</contexts>
<marker>Kawahara, Kurohashi, 2002</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of case frame dictionary for robust Japanese case analysis. In Proceedings of the 19th International Conference on Computational Linguistics, pages 425–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Case frame compilation from the web using high-performance computing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation.</booktitle>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame compilation from the web using high-performance computing. In Proceedings of the 5th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>2008--2013</pages>
<contexts>
<context position="15421" citStr="Kawahara et al., 2002" startWordPosition="2506" endWordPosition="2509">h the case slot sj. Since the type fj consists of a surface case cj2, a punctuation mark (comma) pj and a topic marker “wa” tj, P(fj|sj, fi) is rewritten as follows 2A surface case means a postposition sequence at the end of bunsetsu, such as “ga”, “wo”, “koso” and “demo”. (using the chain rule): P(fj|sj, fi) = P(cj, tj, pj|sj, fi) = P(cj|sj, fi) × P(pj|sj, fi, cj) × P(tj|sj, fi, cj,pj) ≈ P(cj|sj) × P(pj|fi) × P(tj|fi,pj) (9) This approximation is given by assuming that cj only depends on sj, pj only depends on fj, and tj depends on fj and pj. P(cj|sj) is estimated from the Kyoto Text Corpus (Kawahara et al., 2002), in which the relationship between a surface case marker and a case slot is annotated by hand. In Japanese, a punctuation mark and a topic marker are likely to be used when their belonging bunsetsu has a long distance dependency. By considering such tendency, fi can be regarded as (oi, ui), where oi means whether a dependent bunsetsu gets over another head candidate before its modifying head vi, and ui means a clause type of vi. The value of oi is binary, and ui is one of the clause types described in (Kawahara and Kurohashi, 1999). P(pj|fi) = P(pj|oi, ui) (10) P(tj|fi,pj) = P(tj|oi,ui,pj) (1</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 2008–2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
<author>Mitch Marcus</author>
</authors>
<title>Adding semantic annotation to the Penn TreeBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference.</booktitle>
<contexts>
<context position="1934" citStr="Kingsbury et al., 2002" startWordPosition="273" endWordPosition="276"> the right argument of the predicate is the object in most cases. Blaheta and Charniak proposed a statistical method ∗Currently, National Institute of Information and Communications Technology, JAPAN, dk@nict.go.jp †Currently, Graduate School of Informatics, Kyoto University, kuro@i.kyoto-u.ac.jp for analyzing function tags in Penn Treebank, and achieved a really high accuracy of 95.7% for syntactic roles, such as SBJ (subject) and DTV (dative) (Blaheta and Charniak, 2000). In recent years, there have been many studies on semantic structure analysis (semantic role labeling) based on PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). These studies classify syntactic roles into semantic ones such as agent, experiencer and instrument. Case structure analysis of Japanese is very different from that of English. In Japanese, postpositions are used to mark cases. Frequently used postpositions are “ga”, “wo” and “ni”, which usually mean nominative, accusative and dative. However, when an argument is followed by the topic-marking postposition “wa”, its case marker is hidden. In addition, case-marking postpositions are often omitted in Japanese. These troublesome characteristics make Japanese cas</context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002. Adding semantic annotation to the Penn TreeBank. In Proceedings of the Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="4421" citStr="Klein and Manning, 2003" startWordPosition="662" endWordPosition="666"> based on example-based case frames that are constructed from a huge raw corpus in an unsupervised manner. Let us consider syntactic analysis, into which our method of case structure analysis is integrated. Recently, many accurate statistical parsers have been proposed (e.g., (Collins, 1999; Charniak, 2000) for English, (Uchimoto et al., 2000; Kudo and Matsumoto, 2002) for Japanese). Since they somehow use lexical information in the tagged corpus, they are called “lexicalized parsers”. On the other hand, unlexicalized parsers achieved an almost equivalent accuracy to such lexicalized parsers (Klein and Manning, 2003; Kurohashi and Nagao, 1994). Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem. Bikel also indicated that Collins’ parser can use bilexical dependencies only 1.49% of the time; the rest of the time, it backs off to condition one word on just phrasal and part-of-speech categories (Bikel, 2004). This paper aims at exploiting much more lexical information, and proposes a fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. Lexical information is extracte</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning,</booktitle>
<pages>29--35</pages>
<contexts>
<context position="4169" citStr="Kudo and Matsumoto, 2002" startWordPosition="623" endWordPosition="627">case is analyzed as “wo”. As a result, we obtain the case structure “φ:ga bentou:wo taberu”, which means that “ga” (nominative) argument is omitted, and “wo” (accusative) argument is “bentou” (lunchbox). In this paper, we run such case structure analysis based on example-based case frames that are constructed from a huge raw corpus in an unsupervised manner. Let us consider syntactic analysis, into which our method of case structure analysis is integrated. Recently, many accurate statistical parsers have been proposed (e.g., (Collins, 1999; Charniak, 2000) for English, (Uchimoto et al., 2000; Kudo and Matsumoto, 2002) for Japanese). Since they somehow use lexical information in the tagged corpus, they are called “lexicalized parsers”. On the other hand, unlexicalized parsers achieved an almost equivalent accuracy to such lexicalized parsers (Klein and Manning, 2003; Kurohashi and Nagao, 1994). Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem. Bikel also indicated that Collins’ parser can use bilexical dependencies only 1.49% of the time; the rest of the time, it backs off to condition one word on jus</context>
<context position="23778" citStr="Kudo and Matsumoto, 2002" startWordPosition="3825" endWordPosition="3828">Kyoto Text Corpus. Fujio and Matsumoto presented a syntactic analysis method based on lexical statistics (Fujio and Matsumoto, 1998). They made use of a probabilistic model defined by the product of a probability of having a dependency between two cooccurring words and a distance probability. The model was trained on the EDR corpus, and performed with 86.89% accuracy on 10,000 sentences from the EDR corpus 5. On the other hand, there have been a number of machine learning-based approaches using lexical preference as their features. Among these, Kudo and Matsumoto yielded the best performance (Kudo and Matsumoto, 2002). They proposed a chunkingbased dependency analysis method using Support Vector Machines. They used two-fold cross validation on the Kyoto Text Corpus, and achieved 90.46% 5The evaluation includes the last dependencies in the sentence end, which are always correct. baseline proposed TM 72/105 (68.6%) 82/105 (78.1%) clause 107/155 (69.0%) 121/155 (78.1%) 182 accuracy 5. However, it is very hard to learn sufficient lexical preference from several tens of thousands sentences of a hand-tagged corpus. There has been some related work analyzing clausal modifiees and TM phrases. For example, Torisawa</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proceedings of the Conference on Natural Language Learning, pages 29–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="4449" citStr="Kurohashi and Nagao, 1994" startWordPosition="667" endWordPosition="670">ase frames that are constructed from a huge raw corpus in an unsupervised manner. Let us consider syntactic analysis, into which our method of case structure analysis is integrated. Recently, many accurate statistical parsers have been proposed (e.g., (Collins, 1999; Charniak, 2000) for English, (Uchimoto et al., 2000; Kudo and Matsumoto, 2002) for Japanese). Since they somehow use lexical information in the tagged corpus, they are called “lexicalized parsers”. On the other hand, unlexicalized parsers achieved an almost equivalent accuracy to such lexicalized parsers (Klein and Manning, 2003; Kurohashi and Nagao, 1994). Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem. Bikel also indicated that Collins’ parser can use bilexical dependencies only 1.49% of the time; the rest of the time, it backs off to condition one word on just phrasal and part-of-speech categories (Bikel, 2004). This paper aims at exploiting much more lexical information, and proposes a fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. Lexical information is extracted not from a small tagged co</context>
<context position="19278" citStr="Kurohashi and Nagao, 1994" startWordPosition="3128" endWordPosition="3131">nces 3. The web sentences were manually annotated using the same criteria as the Kyoto Text Corpus. The system input was tagged automatically using the JUMAN morphological analyzer (Kurohashi et al., 1994). The syntactic structures obtained were evaluated with re3The test set is not used for case frame construction and probability estimation. gard to dependency accuracy — the proportion of correct dependencies out of all dependencies except for the last dependency in the sentence end 4. Table 3 shows the dependency accuracy. In the table, “baseline” means the rule-based syntactic parser, KNP (Kurohashi and Nagao, 1994), and “proposed” represents the proposed method. The proposed method significantly outperformed the baseline method (McNemar’s test; p &lt; 0.05). The dependency accuracies are classified into four types according to the bunsetsu classes (VB: verb bunsetsu, NB: noun bunsetsu) of a dependent and its head. The “NB→VB” type is further divided into two types: “TM” and “others”. The type that is most related to case structure is “others” in “NB→VB”. Its accuracy was improved by 1.6%, and the error rate was reduced by 10.9%. This result indicated that the proposed method is effective in analyzing depen</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures. Computational Linguistics, 20(4):507–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Toshihisa Nakamura</author>
<author>Yuji Matsumoto</author>
<author>Makoto Nagao</author>
</authors>
<title>Improvements of Japanese morphological analyzer JUMAN.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Workshop on Sharable Natural Language,</booktitle>
<pages>22--28</pages>
<contexts>
<context position="18857" citStr="Kurohashi et al., 1994" startWordPosition="3062" endWordPosition="3065">us comprising 470M sentences, and the case structure analysis results were obtained from 6M sentences in the web corpus. The rest of this section first describes the experiments for syntactic structure, and then reports the experiments for case structure. 4.1 Experiments for Syntactic Structure We evaluated syntactic structures analyzed by the proposed model. Our experiments were run on hand-annotated 675 web sentences 3. The web sentences were manually annotated using the same criteria as the Kyoto Text Corpus. The system input was tagged automatically using the JUMAN morphological analyzer (Kurohashi et al., 1994). The syntactic structures obtained were evaluated with re3The test set is not used for case frame construction and probability estimation. gard to dependency accuracy — the proportion of correct dependencies out of all dependencies except for the last dependency in the sentence end 4. Table 3 shows the dependency accuracy. In the table, “baseline” means the rule-based syntactic parser, KNP (Kurohashi and Nagao, 1994), and “proposed” represents the proposed method. The proposed method significantly outperformed the baseline method (McNemar’s test; p &lt; 0.05). The dependency accuracies are class</context>
</contexts>
<marker>Kurohashi, Nakamura, Matsumoto, Nagao, 1994</marker>
<rawString>Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. In Proceedings of the International Workshop on Sharable Natural Language, pages 22– 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
<author>Kentaro Inui</author>
<author>Takenobu Tokunaga</author>
<author>Hozumi Tanaka</author>
</authors>
<title>An empirical evaluation on statistical parsing of Japanese sentences using lexical association statistics.</title>
<date>1998</date>
<booktitle>In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="22911" citStr="Shirai et al., 1998" startWordPosition="3687" endWordPosition="3690">ts are shown in table 4, in which the baseline refers to a similarity-based method (Kawahara and Kurohashi, 2002). The experimental results were really good compared to the baseline. It is difficult to compare the results with the previous work stated in the next section, because of different experimental settings (e.g., our evaluation includes parse errors in incorrect cases). 5 Related Work There have been several approaches for syntactic analysis handling lexical preference on a large scale. Shirai et al. proposed a PGLR-based syntactic analysis method using large-scale lexical preference (Shirai et al., 1998). Their system learned lexical preference from a large newspaper corpus (articles of five years), such as P(pielwo, taberu), but did not deal with verb sense ambiguity. They reported 84.34% accuracy on 500 relatively short sentences from the Kyoto Text Corpus. Fujio and Matsumoto presented a syntactic analysis method based on lexical statistics (Fujio and Matsumoto, 1998). They made use of a probabilistic model defined by the product of a probability of having a dependency between two cooccurring words and a distance probability. The model was trained on the EDR corpus, and performed with 86.8</context>
</contexts>
<marker>Shirai, Inui, Tokunaga, Tanaka, 1998</marker>
<rawString>Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1998. An empirical evaluation on statistical parsing of Japanese sentences using lexical association statistics. In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
</authors>
<title>An unsupervised method for canonicalization of Japanese postpositions.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Natural Language Processing Pacific Rim Simposium,</booktitle>
<pages>211--218</pages>
<contexts>
<context position="24505" citStr="Torisawa, 2001" startWordPosition="3939" endWordPosition="3940">ss validation on the Kyoto Text Corpus, and achieved 90.46% 5The evaluation includes the last dependencies in the sentence end, which are always correct. baseline proposed TM 72/105 (68.6%) 82/105 (78.1%) clause 107/155 (69.0%) 121/155 (78.1%) 182 accuracy 5. However, it is very hard to learn sufficient lexical preference from several tens of thousands sentences of a hand-tagged corpus. There has been some related work analyzing clausal modifiees and TM phrases. For example, Torisawa analyzed TM phrases using predicateargument cooccurences and word classifications induced by the EM algorithm (Torisawa, 2001). Its accuracy was approximately 88% for “wa” and 84% for “mo”. It is difficult to compare the accuracy of their system to ours, because the range of target expressions is different. Unlike related work, it is promising to utilize the resultant case frames for subsequent analyzes such as ellipsis or discourse analysis. 6 Conclusion We have described an integrated probabilistic model for syntactic and case structure analysis. This model takes advantage of lexical selectional preference of large-scale case frames, and performs syntactic and case analysis simultaneously. The experiments indicated</context>
</contexts>
<marker>Torisawa, 2001</marker>
<rawString>Kentaro Torisawa. 2001. An unsupervised method for canonicalization of Japanese postpositions. In Proceedings of the 6th Natural Language Processing Pacific Rim Simposium, pages 211–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Masaki Murata</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Dependency model using posterior context.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Workshop on Parsing Technology,</booktitle>
<pages>321--322</pages>
<contexts>
<context position="4142" citStr="Uchimoto et al., 2000" startWordPosition="619" endWordPosition="622"> in “wo” examples, its case is analyzed as “wo”. As a result, we obtain the case structure “φ:ga bentou:wo taberu”, which means that “ga” (nominative) argument is omitted, and “wo” (accusative) argument is “bentou” (lunchbox). In this paper, we run such case structure analysis based on example-based case frames that are constructed from a huge raw corpus in an unsupervised manner. Let us consider syntactic analysis, into which our method of case structure analysis is integrated. Recently, many accurate statistical parsers have been proposed (e.g., (Collins, 1999; Charniak, 2000) for English, (Uchimoto et al., 2000; Kudo and Matsumoto, 2002) for Japanese). Since they somehow use lexical information in the tagged corpus, they are called “lexicalized parsers”. On the other hand, unlexicalized parsers achieved an almost equivalent accuracy to such lexicalized parsers (Klein and Manning, 2003; Kurohashi and Nagao, 1994). Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem. Bikel also indicated that Collins’ parser can use bilexical dependencies only 1.49% of the time; the rest of the time, it backs off t</context>
</contexts>
<marker>Uchimoto, Murata, Sekine, Isahara, 2000</marker>
<rawString>Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and Hitoshi Isahara. 2000. Dependency model using posterior context. In Proceedings of the 6th International Workshop on Parsing Technology, pages 321–322.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>