<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002318">
<title confidence="0.994342">
An Information Retrieval Approach to Sense Ranking
</title>
<author confidence="0.993502">
Mirella Lapata and Frank Keller
</author>
<affiliation confidence="0.9299875">
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
</affiliation>
<email confidence="0.988316">
{mlap,keller}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997289" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796">
In word sense disambiguation, choosing
the most frequent sense for an ambigu-
ous word is a powerful heuristic. However,
its usefulness is restricted by the availabil-
ity of sense-annotated data. In this paper,
we propose an information retrieval-based
method for sense ranking that does not re-
quire annotated data. The method queries
an information retrieval engine to estimate
the degree of association between a word
and its sense descriptions. Experiments on
the Senseval test materials yield state-of-
the-art performance. We also show that the
estimated sense frequencies correlate reli-
ably with native speakers’ intuitions.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935661538462">
Word sense disambiguation (WSD), the ability to
identify the intended meanings (senses) of words
in context, is crucial for accomplishing many NLP
tasks that require semantic processing. Examples in-
clude paraphrase acquisition, discourse parsing, or
metonymy resolution. Applications such as machine
translation (Vickrey et al., 2005) and information re-
trieval (Stokoe, 2005) have also been shown to ben-
efit from WSD.
Given the importance of WSD for basic NLP
tasks and multilingual applications, much work has
focused on the computational treatment of sense
ambiguity, primarily using data-driven methods.
Most accurate WSD systems to date are super-
vised and rely on the availability of training data
(see Yarowsky and Florian 2002; Mihalcea and Ed-
monds 2004 and the references therein). Although
supervised methods typically achieve better perfor-
mance than unsupervised alternatives, their appli-
cability is limited to those words for which sense
labeled data exists, and their accuracy is strongly
correlated with the amount of labeled data avail-
able. Furthermore, current supervised approaches
rarely outperform the simple heuristic of choosing
the most common or dominant sense in the train-
ing data (henceforth “the first sense heuristic”), de-
spite taking local context into account. One reason
for this is the highly skewed distribution of word
senses (McCarthy et al., 2004a). A large number of
frequent content words is often associated with only
one dominant sense.
Obtaining the first sense via annotation is ob-
viously costly and time consuming. Sense anno-
tated corpora are not readily available for different
languages or indeed sense inventories. Moreover,
a word’s dominant sense will vary across domains
and text genres (the word court in legal documents
will most likely mean tribunal rather than yard).
It is therefore not surprising that recent work (Mc-
Carthy et al., 2004a; Mohammad and Hirst, 2006;
Brody et al., 2006) attempts to alleviate the anno-
tation bottleneck by inferring the first sense auto-
matically from raw text. Automatically acquired first
senses will undoubtedly be noisy when compared to
human annotations. Nevertheless, they can be use-
fully employed in two important tasks: (a) to create
preliminary annotations, thus supporting the “anno-
tate automatically, correct manually” methodology
used to provide high volume annotation in the Penn
Treebank project; and (b) in combination with super-
vised WSD methods that take context into account;
for instance, such methods could default to the dom-
inant sense for unseen words or words with uninfor-
mative contexts.
This paper focuses on a knowledge-lean sense
ranking method that exploits a sense inventory like
WordNet and corpus data to automatically induce
dominant senses. The proposed method infers the
associations between words and sense descriptions
automatically by querying an IR engine whose in-
dex terms have been compiled from the corpus
of interest. The approach is inexpensive, language-
independent, requires minimal supervision, and uses
no additional knowledge other than the word senses
proper and morphological query expansions. We
</bodyText>
<page confidence="0.975485">
348
</page>
<note confidence="0.7984785">
Proceedings of NAACL HLT 2007, pages 348–355,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999295875">
evaluate our method on two tasks. First, we use
the acquired dominant senses to disambiguate the
meanings of words in the Senseval-2 (Palmer et al.,
2001) and Senseval-3 (Snyder and Palmer, 2004)
data sets. Second, we simulate native speakers’ intu-
itions about the salience of word meanings and ex-
amine whether the estimated sense frequencies cor-
relate with sense production data. In all cases our ap-
proach outperforms a naive baseline and yields per-
formances comparable to state of the art.
In the following section, we provide an overview
of existing work on sense ranking. In Section 3, we
introduce our IR-based method, and describe several
sense ranking models. In Section 4, we present our
results. Discussion of our results and future work
conclude the paper (Section 5).
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999932887096774">
McCarthy et al. (2004a) were the first to pro-
pose a computational model for acquiring dominant
senses from text corpora. Key in their approach is
the observation that distributionally similar neigh-
bors often provide cues about a word’s senses. The
model quantifies the degree of similarity between
a word’s sense descriptions and its closest neigh-
bors, thus delivering a ranking over senses where the
most similar sense is intuitively the dominant sense.
Their method exploits two notions of similarity,
distributional and semantic. Distributionally similar
words are acquired from the British National Cor-
pus using an information-theoretic similarity mea-
sure (Lin, 1998) operating over dependency re-
lations (e.g., verb-subject, verb-object). The latter
are obtained from the output of Briscoe and Car-
roll’s (2002) parser. The semantic similarity between
neighbors and senses is measured using a manually
crafted taxonomy such as WordNet (see Budanitsky
and Hirst 2001 for an overview of WordNet-based
similarity measures).
Mohammad and Hirst (2006) propose an algo-
rithm for inferring dominant senses without rely-
ing on distributionally similar neighbors. Their ap-
proach capitalizes on the collocational nature of
semantically related words. Assuming a coarse-
grained sense inventory (e.g., the Macquarie The-
saurus), it first creates a matrix whose columns rep-
resent all categories (senses) c1 ...cn in the inven-
tory and rows the ambiguous target words w1 ...wm;
the matrix cells record the number of times a tar-
get word ti co-occurs with category cj within a win-
dow of size s. Using an appropriate statistical test,
they estimate the relative strength of association be-
tween an ambiguous word and each of its senses.
The sense with the highest association is the pre-
dominant sense.
Our work shares with McCarthy et al. (2004a) and
Mohammad and Hirst (2006) the objective of infer-
ring dominant senses automatically. We propose a
knowledge-lean method that relies on word associa-
tion and requires no syntactic annotation. The latter
may be unavailable when working with languages
other than English for which state-of-the-art parsers
or taggers have not been developed. Mohammad and
Hirst (2006) estimate the co-occurrence frequency
of a word and its sense descriptors by considering
small window sizes of up to five words. These esti-
mates will be less reliable for moderately frequent
words or for sense inventories with many senses.
Our approach is more robust to sparse data – we
work with document-based frequencies – and thus
suitable for both coarse and fine grained sense in-
ventories. Furthermore, it is computationally inex-
pensive; in contrast to McCarthy et al. (2004a) we
do not rely on the structure of the sense inventory
for measuring the similarity between synonyms and
their senses. Moreover, unlike Mohammad and Hirst
(2006), our algorithm only requires co-occurrence
frequencies for the target word and its senses, with-
out considering all senses in the inventory and all
words in the corpus simultaneously.
</bodyText>
<sectionHeader confidence="0.976882" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.998738">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999965533333333">
Central in our approach is the assumption that con-
text provides important cues regarding a word’s
meaning. The idea dates back at least to Firth (1957)
(“You shall know a word by the company it keeps”)
and underlies most WSD work to date. Another ob-
servation that has found wide application in WSD is
that words tend to exhibit only one sense in a given
discourse or document (Gale et al., 1992). Further-
more, documents are typically written with certain
topics in mind which are often indicated by word
distributional patterns (Harris, 1982).
For example, documents talking about congres-
sional tenure are likely to contain words such as term
of office or incumbency, whereas documents talking
about legal tenure (i.e., the right to hold property)
</bodyText>
<page confidence="0.998645">
349
</page>
<bodyText confidence="0.999952">
are likely to include the words right or land. Now,
we could estimate which sense of tenure is most
prevalent simply by comparing whether tenure co-
occurs more often with term of office than with land
provided we knew that both of these terms are se-
mantically related to tenure. Fortunately, senses in
WordNet (and related taxonomies) are represented
by synonym terms. So, all we need to do for esti-
mating a word’s sense frequencies is to count how
often it co-occurs with its synonyms. We adopt here
a fairly broad definition of co-occurrence, two words
co-occur if they are attested in the same document.
We could obtain such counts from any document
collection; however, to facilitate comparisons with
prior work (e.g., McCarthy et al. 2004a), all our ex-
periments use the British National Corpus (BNC). In
what follows we describe in detail how we retrieve
co-occurrence counts from the BNC and how we ac-
quire dominant senses.
</bodyText>
<subsectionHeader confidence="0.999717">
3.2 Dominant Sense Acquisition
</subsectionHeader>
<bodyText confidence="0.999820714285714">
Throughout the paper we use the term frequency as a
shorthand for document frequency, i.e., the number
of documents that contain a word or a set of words
which may or may not be adjacent. The method
we propose here exploits document frequencies of
words and their sense definitions. We base our dis-
cussion below on the WordNet sense inventory and
its representation of senses in terms of synonym
sets (synsets). However, our approach is not lim-
ited to this particular lexicon; any dictionary with
synonym-based sense definitions could serve our
purposes.
As an example consider the noun tenure, which
has the following senses in WordNet:
</bodyText>
<listItem confidence="0.403129">
(1) Sense 1
</listItem>
<bodyText confidence="0.94458575">
tenure, term of office, incumbency
=&gt; term
Sense 2
tenure, land tenure
=&gt; legal right
The senses are represented by the two synsets
{tenure, term of office, incumbency} and
{tenure, land tenure}. (The hypernyms for each
sense are also listed; indicated by the arrows.) We
can now approximate the frequency with which a
word w1 occurs with the sense s by computing its
synonym frequencies: for each word w2 E syns(s),
the set of synonyms of s, we field a query of the form
w1 AND w2. These synonym frequencies can then be
used to determine the most frequent sense of w1 in a
variety of ways (to be detailed below).
The synsets for the two senses in (1) give rise to
the queries in (2) and (3). Note that two queries are
generated for the first synset, as it contains two syn-
onyms of the target word tenure.
</bodyText>
<listItem confidence="0.999430333333333">
(2) a. &amp;quot;tenure&amp;quot; AND &amp;quot;term of office&amp;quot;
b. &amp;quot;tenure&amp;quot; AND &amp;quot;incumbency&amp;quot;
(3) &amp;quot;tenure&amp;quot; AND &amp;quot;land tenure&amp;quot;
</listItem>
<bodyText confidence="0.999920125">
For example, query (2-a) will return the number of
documents in which tenure and term of office co-
occur. Presumably, tenure is mainly used in its dom-
inant sense in these documents. In the same way,
query (3) will return documents in which tenure is
used in the sense of land tenure. Note that this way
of approximating synonym frequencies as document
frequencies crucially relies on the “one sense per
discourse” hypothesis (Gale et al., 1992), under the
assumption that a document counts as a discourse
for word sense disambiguation purposes.
Apart from synonym frequencies, we also gener-
ate hypernym frequencies by submitting queries of
the form w1 AND w2, for each w2 E hype(s), the set of
immediate hypernyms of the sense s. The hypernym
queries for the two senses of tenure are:
</bodyText>
<listItem confidence="0.9980045">
(4) &amp;quot;tenure&amp;quot; AND &amp;quot;term&amp;quot;
(5) &amp;quot;tenure&amp;quot; AND &amp;quot;legal right&amp;quot;
</listItem>
<bodyText confidence="0.999560875">
Hypernym queries are particularly useful for synsets
of size one, i.e., where a word in a given sense has
no synonyms, and is only differentiated from other
senses by its hypernyms.
Before submitting queries such as the ones in
(2) and (3) to an IR engine, we perform query
expansion to make sure that all relevant in-
flected forms are included. For example the query
term &amp;quot;tenure&amp;quot; is expanded to (&amp;quot;tenure&amp;quot; OR
&amp;quot;tenures&amp;quot;), i.e., both singular and plural noun
forms are generated. Similarly, all inflected verb
forms are generated, e.g., &amp;quot;keep up&amp;quot; gives rise to
the query term (&amp;quot;keep up&amp;quot; OR &amp;quot;keeps up&amp;quot; OR
&amp;quot;keeping up&amp;quot; OR &amp;quot;kept up&amp;quot;). John Carroll’s
suite of morphological tools (morpha and morphg)
is used to generate inflected forms for verbs and
</bodyText>
<page confidence="0.978034">
350
</page>
<bodyText confidence="0.9962164">
nouns.1
The queries generated this way are then submitted
to an IR engine to obtain document counts. Specifi-
cally, we indexed the BNC using GLIMPSE (Global
Implicit Search) a fast and flexible indexing and
query system2 (Manber and Wu, 1994). GLIMPSE
supports approximate and exact matching, Boolean
queries, wild cards, regular expressions, and many
other options. The text is divided into equal size
blocks and an inverted index is created containing
the words and the block numbers in which they oc-
cur. Given a query, GLIMPSE will retrieve the rele-
vant documents using a two-level search method. It
will first locate the query in the inverted index and
then use sequential search to find an exact answer.
Once synonym frequencies and hypernym fre-
quencies are in place, we can compute a word’s pre-
dominant sense in a number of ways. First, we can
vary the way the frequency of a given sense is esti-
mated based on synonym frequencies:
</bodyText>
<listItem confidence="0.997184833333333">
• Sum: The frequency of a given synset is com-
puted as the sum of the synonym frequen-
cies. For example, the frequency of the dom-
inant sense of tenure would be computed by
adding up the document frequencies returned
by queries (2-a) and (2-b).
• Average (Avg): The frequency of a synset is
computed by taking the average of synonym
frequencies.
• Highest (High): The frequency of a synset is
determined by the synonym with the highest
frequency.
</listItem>
<bodyText confidence="0.940387">
Secondly, we can vary whether or not hypernyms are
taken into account:
</bodyText>
<listItem confidence="0.999064714285714">
• No hypernyms (−Hyp): Only the synonym
frequencies are included when computing the
frequency of a synset. For example, only the
queries of (2-a) and (2-b) are relevant for esti-
mating the dominant sense of tenure.
• Hypernyms (+Hyp): Both synonym and hy-
pernym frequencies are taken into account
</listItem>
<footnote confidence="0.9399426">
1The tools can be downloaded from http://www.
informatics.susx.ac.uk/research/nlp/carroll/
morph.html.
2The software can be downloaded from http:
//webglimpse.net/download.php
</footnote>
<bodyText confidence="0.999779625">
when computing sense frequency. For example,
the frequency for the senses of tenure would
be computed based on the document frequen-
cies returned by queries (2-a), (2-b), and (4)
(by summing, averaging, or taking the highest
value, as before).
The third option relates to whether the sense fre-
quencies are used in raw or in normalized form:
</bodyText>
<listItem confidence="0.858566769230769">
• Non-normalized (−Norm): The raw synonym
frequencies are used as estimates of sense fre-
quencies.
• Normalized (+Norm): Sense frequencies are
computed by dividing the word-synonym fre-
quency by the frequency of the synonym in
isolation. For example, the normalized fre-
quency for (2-a) is computed by dividing
the document frequency for &amp;quot;tenure&amp;quot; AND
&amp;quot;term of office&amp;quot; by the document fre-
quency for &amp;quot;term of office&amp;quot;. Normalizing
takes into account the fact that the members of
the synset of a sense may differ in frequency.
</listItem>
<bodyText confidence="0.99982975">
The combination of the above parameters yields 12
sense ranking models. We explore the parameter
space exhaustively on the Senseval-2 benchmark
data set. The best performing model on this data set
is then used in all our subsequent experiments. We
use Senseval-2 as a development set, but we also
demonstrate that a far smaller manually annotated
sample is sufficient for selecting the best model.
</bodyText>
<sectionHeader confidence="0.999816" genericHeader="conclusions">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999539933333333">
Our experiments were driven by three questions:
(1) Is WSD feasible at all with a model that does
not employ any syntactic or semantic knowledge?
Recall that McCarthy et al. (2004a) propose a model
that crucially relies on a robust parser for estimat-
ing dominant senses. (2) What is the best parameter
setting for our model? (3) Do the acquired dominant
senses correlate with human judgments? If our sense
frequencies exhibit no such correlation, it is unlikely
that they will be useful in practical applications.
To address the first two questions we use the in-
duced first senses to perform WSD on the Senseval-
2 and Senseval-3 data sets. For our third question we
compare native speakers’ semantic intuitions against
the BNC sense frequencies.
</bodyText>
<page confidence="0.992043">
351
</page>
<table confidence="0.997305833333333">
−Norm +Norm
+Hyp −Hyp +Hyp −Hyp
P R P R P R P R
Sum 42.3 40.8 46.3 44.6 45.9 44.3 48.6 46.8
High 51.6 49.8 51.1 49.3 57.2 55.1 59.7 57.6
Avg 44.1 42.6 48.5 46.8 49.6 47.8 51.5 49.6
</table>
<tableCaption confidence="0.940378">
Table 1: Results for Senseval-2 data by model in-
stantiation
</tableCaption>
<subsectionHeader confidence="0.998234">
4.1 Model Selection
</subsectionHeader>
<bodyText confidence="0.999981457142857">
The goal of our first experiment is to establish which
model configuration (see Section 3.2) is best suited
for the WSD task. We thus varied how the overall
frequency is computed (Sum, High, Avg), whether
hyponyms are included (+Hyp), and whether the
frequencies are normalized (+Norm). To explore the
parameter space, we used the Senseval-2 all-words
test data as our development set. This data set con-
sists of three documents from the Wall Street Jour-
nal containing approximately 2,400 content words.
Following McCarthy et al. (2004a), we first use our
method to find the dominant sense for all word types
in the corpus and then use that sense to disambiguate
tokens without taking contextual information into
account. We used WordNet 1.7.1 (Fellbaum, 1998)
senses.3
We compared our results to a baseline that se-
lects for each word type a random sense, assumes
it is the dominant one, and uses it to disambiguate
all instances of the target word (McCarthy et al.,
2004a). We also report the WSD performance of a
more competitive baseline that always chooses the
sense with the largest synset as the dominant sense.
Consider again the word tenure from Section 3.2.
According to this baseline, the dominant sense for
tenure is the first one since it is represented by the
largest synset (three members).
Our results on Senseval-2 are summarized in Ta-
ble 1. We observe that models that do not include
hypernyms yield consistently better precision and
recall than models that include them. On the one
hand, hypernyms render the estimated sense distri-
butions less sparse. On the other hand, they intro-
duce considerable noise; the resulting sense frequen-
cies are often similar – the same hypernyms can be
</bodyText>
<footnote confidence="0.682925666666667">
3Senseval-2 is annotated with WordNet 1.7 senses which
we converted to 1.7.1 using a publicly available mapping (see
http://www.cs.unt.edu/˜rada/downloads.html).
</footnote>
<table confidence="0.999265571428571">
BaseR BaseS Model N
P R P R P R
Noun 26.8 25.4 45.8 43.4 53.1*# 50.2*# 1,063
Verb 11.2 11.1 19.9 19.5 48.2*# 47.3*# 569
Adj 22.1 21.4 56.5 56.0 56.7* 56.2* 451
Adv 48.0 45.9 66.4 62.9 86.4*# 81.8*# 301
All 26.3 25.4 42.2 40.7 59.7*# 57.6*# 2,384
</table>
<tableCaption confidence="0.8496075">
Table 2: Results of best model (High, +Norm,
−Hyp) for Senseval-2 data by part of speech
</tableCaption>
<bodyText confidence="0.995450605263158">
(*: sig. diff. from BaseR, #: sig. diff. from BaseS;
p &lt; 0.01 using x2 test)
shared among several senses – and selecting one pre-
dominant sense over the other can be due to very
small frequency differences. We also find that mod-
els with normalized document counts outperform
models without normalization. This is not surpris-
ing, there is ample evidence in the literature (Mo-
hammad and Hirst, 2006; Turney, 2001) that associ-
ation measures (e.g., conditional probability, mutual
information) are better indicators of lexical similar-
ity than raw frequency. Finally, selecting the syn-
onym with the highest frequency (and defaulting to
its sense) achieves better results in comparison to av-
eraging or summing over all synsets.
In sum, the best performing model is High,
+Norm, −Hyp, achieving a precision of 59.7% and
a recall of 57.9%. The results for this model are bro-
ken down by part of speech in Table 2. Here, we
also include a comparison with the random base-
line (BaseR) and a baseline that selects the dominant
sense by synset size (BaseS). We observe that the
optimal model significantly outperforms both base-
lines on the complete data set (see row All in Ta-
ble 2) and on most individual parts of speech (perfor-
mances are comparable for our model and BaseS on
adjectives). BaseS is far better than BaseR and gen-
erally harder to beat. Defaulting to synset size in the
absence of any other information is a good heuristic;
large synsets often describe frequent senses. Vari-
ants of our model that select a dominant sense by
summing over synset members are closest to this
baseline. Note that our best performing model does
not rely on synset size; it simply selects the synonym
with the highest frequency, despite the fact that it
might belong to a large or small synset. We con-
jecture that its superior performance is due to the
collocational nature of semantic similarity (Turney,
</bodyText>
<page confidence="0.993463">
352
</page>
<table confidence="0.927233">
−Norm +Norm
+Hyp −Hyp +Hyp −Hyp
P R P R P R P R
Sum 42.3 40.8 46.3 44.6 45.2 44.7 44.6 44.0
High 51.6 49.8 51.1 49.3 55.0 54.3 61.3 60.5
Avg 44.1 42.6 48.5 46.8 51.5 50.8 50.4 49.8
</table>
<tableCaption confidence="0.9803125">
Table 3: Results for 10% of Senseval-2 data by
model instantiation
</tableCaption>
<bodyText confidence="0.991451875">
2001).
In order to establish that High, +Norm, −Hyp is
the optimal model, we utilized the whole Senseval-
2 data set. Using such a large dataset is more likely
to yield a stable parameter setting, but it also raises
the question whether parameter optimization could
take place on a smaller dataset which is less costly
to produce. Table 3 explores the parameter space on
a sample randomly drawn from Senseval-2 that con-
tains only 240 tokens (i.e., one tenth of the original
data set). The behavior of our models on this smaller
sample is comparable to that on the entire Senseval-
2 data. Importantly, both sets yield the same best
model, i.e., High, +Norm, −Hyp. In the remainder
of this paper we will use this model for further ex-
periments without additional parameter tuning.
</bodyText>
<subsectionHeader confidence="0.999035">
4.2 Application to Senseval-3 Data
</subsectionHeader>
<bodyText confidence="0.999931888888889">
We next evaluate our best model the on the
Senseval-3 English all-words data set. Senseval-3
consists of two Wall Street Journal articles and
one excerpt from the Brown corpus (approximately
5,000 content words in total). Similarly to the ex-
periments reported in the previous section, we used
WordNet 1.7.1. We calculate recall and precision
with the Senseval-3 scorer.
Our results are given in Table 4. Besides the
two baselines (BaseR and BaseS), we also com-
pare our model to McCarthy et al. (2004b)4 and
the best unsupervised (IRST-DDD) and supervised
(GAMBLE) systems that participated in Senseval-3.
IRST-DDD was developed by Strapparava et al.
(2004) and performs domain driven disambiguation.
Specifically, the approach compares the domain of
the context surrounding the target word with the do-
mains of its senses and uses a version of WordNet
</bodyText>
<footnote confidence="0.99402225">
4Comparison against Mohammad and Hirst (2006) was not
possible since they use a sense inventory other than WordNet
(i.e., Roget’s thesaurus) and evaluate their model on artificially
generated sense-tagged data.
</footnote>
<table confidence="0.999637625">
P R
BaseR 23.1#†$t 22.7#†$t
BaseS 36.6*†$t 35.9*†$t
McCarthy 49.0*#$t 43.0*#$t
IR-Model 58.0*#†t 57.0*#†t
IRST-DDD 58.3*#†t 58.2*#†t
Semcor 62.4*#†$ 62.4*#†$
GAMBLE 65.1*#†$t 65.2*#†$t
</table>
<tableCaption confidence="0.90927225">
Table 4: Comparison of results on Senseval-3 data
(*: sig. diff. from BaseR, #: sig. diff. from BaseS,
†: sig. diff. from McCarthy, $: sig. diff. from IR-
Model, t: sig. diff. from SemCor; p &lt; 0.01 using
</tableCaption>
<table confidence="0.937266">
x2 test)
BaseR BaseS Model N
P R P R P R
Noun 27.8 12.2 41.1 41.0 58.1*# 58.0*# 900
Verb 12.8 4.6 20.0 19.9 61.0*# 60.8*# 732
Adj 29.2 5.2 56.5 56.5 50.3* 50.3* 363
Adv 100.0 0.6 100.0 81.2 100.0 81.2 16
All 23.1 22.7 36.6 35.9 58.0*# 57.0*# 2,011
</table>
<tableCaption confidence="0.822711">
Table 5: Results of best model (High, +Norm,
−Hyp) for Senseval-3 data by part of speech
</tableCaption>
<bodyText confidence="0.998327869565217">
(*: sig. diff. from BaseR, #: sig. diff. from BaseS;
p &lt; 0.01 using x2 test)
augmented with domain labels (e.g., economy, ge-
ography). GAMBL (Decadt et al., 2004) is a super-
vised system: a classifier is trained for each ambigu-
ous word using memory-based learning. We also re-
port the performance achieved by defaulting to the
first WordNet entry for a given word and part of
speech. Entries in WordNet are ranked according
to the sense frequency estimates obtained from the
manually annotated SemCor corpus. First senses ob-
tained from SemCor will be naturally less noisy than
those computed by our method which does not make
use of manual annotation in any way. We therefore
consider the WSD performance achieved with Sem-
Cor first senses as an upper bound for automatically
acquired first senses.
Our model significantly outperforms the two
baselines and McCarthy et al. (2004b). Its precision
and recall according to individual parts of speech is
shown in Table 5. The model performs comparably
to IRST-DDD and significantly worse than GAM-
BLE. This is not entirely surprising given that GAM-
</bodyText>
<page confidence="0.985965">
353
</page>
<bodyText confidence="0.993937243697479">
BLE is a supervised system trained on a variety act Freq answer Freq
of manually annotated resources including SemCor, pretense/performance 37 response 81
data from previous Senseval workshops and the ex- to perform 30 solution 18
ample sentences in WordNet 1.7.1. GAMBLE is the to take action 16
only system that significantly outperforms the Sem- division 12
Cor upper bound. Finally, note that our model is a deed 3
conceptually simpler than McCarthy et al. (2004b)
and IRST-DDD. It neither requires a parser (for ob-
taining distributionally similar neighbors) nor any
knowledge other than WordNet (e.g., domain la-
bels). This makes our method portable to languages
for which syntactic analysis tools and elaborate se-
mantic resources are not available.
4.3 Modeling Human Data
Research in psycholinguistics has shown that the
meanings of ambiguous words are not perceived as
equally salient in the absence of a biasing context
(Durkin and Manning, 1989; Twilley et al., 1994).
Rather, language users often ascribe dominant and
subordinate meanings to polysemous words. Previ-
ous studies have elicited intuitions with regard to
word senses using a free association task. For ex-
ample, Durkin and Manning (1989) collected asso-
ciation norms from native speakers for 175 ambigu-
ous words. They asked subjects to read each word
and write down the first meaning that came to mind.
The words were presented out of context. From the
subjects’ responses, they computed sense frequen-
cies, which revealed that most words were attributed
a particular meaning with a markedly higher fre-
quency than other meanings.
In this experiment, we examine whether our
model agrees with human intuitions regarding the
prevalence of word senses. We inferred the dominant
meanings for the polysemous words used in Durkin
and Manning (1989). These exhibit a relatively high
degree of ambiguity (the average number of senses
per word is three) and cover a wide variety of parts
of speech (for the full set of words and elicited
sense frequencies see their Appendix A, pp. 501–
609). One stumbling block to using this data are
the meanings associated with the ambiguous words.
These were provided by native English speakers and
may not necessarily correspond to senses described
by trained lexicographers. Fortunately, we were able
to map most of them (except for six which we dis-
carded) on WordNet synsets (version 1.6); two an-
notators performed the mapping by comparing the
sense descriptions provided by Durkin and Manning
354
Table 6: Meaning frequencies for act and answer;
normative data from Durkin and Manning (1989)
to WordNet synsets. The annotators agreed in their
assignments 81% of the time. Disagreements were
resolved through mediation.
Examples of Durkin and Manning’s (1989)
normative data are given in Table 6. The sense
response for answer was mapped to the WordNet
synset {answer, reply, response} (Sense 1),
the sense solution was mapped to the synset
{solution, answer, result, resolution,
solvent} (Sense 2), etc. Durkin and Manning did
not take part of speech ambiguity into account, as
Table 6 shows, subjects came up with meanings
relating to the verb and noun part of speech of act.
We explored the relationship between the sense
frequencies provided by human subjects and those
estimated by our model by computing the Spearman
rank correlation coefficient p. We obtained sense
frequencies from the BNC using the best model
from Section 4.1 (High, +Norm, −Hyp). We found
that the resulting sense frequencies were signifi-
cantly correlated with the human sense frequencies
(p = 0.384, p &lt; 0.01). We performed the same ex-
periment using McCarthy et al.’s (2004a) model,
which also achieved a significant correlation (p =
0.316, p &lt; 0.01). This result provides an additional
validation of our model as it demonstrates that the
sense frequencies it generates can capture the sense
preferences of naive human subjects (rather than
trained lexicographers).
5 Discussion
In this paper we proposed an IR-based approach
for inducing dominant senses automatically. Our
method estimates the degree of association between
words and their sense descriptions (represented by
synsets in WordNet) simply by querying an IR en-
gine. Evaluation on the Senseval data sets showed
that our model significantly outperformed a naive
random sense baseline and a more competitive one
based on synset size. Our method was significantly
better than McCarthy et al. (2004b) on Senseval-2
and Senseval-3. On the latter data set, its perfor-
mance was comparable to that of the best unsuper-
vised system (Strapparava et al., 2004).
An important future direction lies in evaluating
the disambiguation potential of our models across
domains and languages. Furthermore, our experi-
ments have relied on WordNet for providing the
appropriate sense descriptions. Future work must
assess whether the models presented in this pa-
per can be extended to alternative sense invento-
ries (e.g., dictionary definitions) that may differ in
granularity and structure. We will also experiment
with a wider range of lexical association measures
for quantifying the similarity of a word and its
synonyms. Examples include odds ratio (Moham-
mad and Hirst, 2006) and Turney’s (2001) IR-based
pointwise mutual information (PMI-IR).
Our experiments revealed that the IR-based model
is particularly good at disambiguating certain parts
of speech (e.g., verbs, see Tables 2 and 5). A promis-
ing direction is the combination of different ranking
models (Brody et al., 2006) and the integration of
dominant sense models with supervised WSD.
Acknowledgments We are grateful to Diana Mc-
Carthy for her help with this work. The au-
thors acknowledge the support of EPSRC (grant
EP/C538447/1).
</bodyText>
<sectionHeader confidence="0.999544" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999796747126437">
Briscoe, Ted and John Carroll. 2002. Robust accurate statistical
annotation of general text. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evaluation.
Las Palmas, Gran Canaria, pages 1499–1504.
Brody, Samuel, Roberto Navigli, and Mirella Lapata. 2006. En-
semble methods for unsupervised WSD. In Proceedings of
the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics. Sydney, Australia, pages 97–
104.
Budanitsky, Alexander and Graeme Hirst. 2001. Semantic
distance in WordNet: An experimental, application-oriented
evaluation of five measures. In Proceedings of the NAACL
Workshop on WordNet and Other Lexical Resources. Pitts-
burgh, PA.
Decadt, Bart, V´eronique Hoste, Walter Daelemans, and Antal
van den Bosch. 2004. GAMBL, genetic algorithm optimiza-
tion of memory-based WSD. In Mihalcea and Edmonds
(2004), pages 108–112.
Durkin, Kevin and Jocelyn Manning. 1989. Polysemy and
the subjective lexicon: Semantic relatedness and the salience
of intraword senses. Journal of Psycholinguistic Research
18(6):577–612.
Fellbaum, Christiane, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Firth, J. R. 1957. A Synopsis of Linguistic Theory 1930-1955.
Oxford: Philological Society.
Gale, William A., Kenneth W. Church, and David Yarowsky.
1992. A method for disambiguating word senses in a large
corpus. Computers and the Humanities 26(5–6):415–439.
Harris, Zellig. 1982. Discourse and sublanguage. In R. Kit-
tredge and J. Lehrberger, editors, Language in Restricted
Semantic Domains, Walter de Gruyter, Berlin; New York,
pages 231–236.
Lin, Dekang. 1998. An information-theoretic definition of sim-
ilarity. In Proceedings of the 15th International Conference
on Machine Learning. Madison, WI, pages 296–304.
Manber, Udi and Sun Wu. 1994. GLIMPSE: a tool to search
through entire file systems. In Proceedings of USENIX Win-
ter 1994 Technical Conference. San Francisco, CA, pages
23–32.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll.
2004a. Finding predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the Association
for Computational Linguistics. Barcelona, pages 279–286.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll.
2004b. Using automatically acquired predominant senses
for word sense disambiguation. In Mihalcea and Edmonds
(2004), pages 151–154.
Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceed-
ings of Senseval-3: The 3rd International Workshop on the
Evaluation of Systems for the Semantic Analysis of Text.
Barcelona.
Mohammad, Saif and Graeme Hirst. 2006. Determining word
sense dominance using a thesaurus. In Proceedings of the
11th Conference of the European Chapter of the Association
for Computational Linguistics. Trento, Italy, pages 121–128.
Palmer, Martha, Christiane Fellbaum, Scott Cotton, Lauren
Delfs, and Hoa Trang Dang. 2001. English tasks: All words
and verb lexical sample. In Proceedings of Senseval-2: The
3rd International Workshop on the Evaluation of Systems for
the Semantic Analysis of Text. Toulouse.
Snyder, Benjamin and Martha Palmer. 2004. The English all-
words task. In Mihalcea and Edmonds (2004).
Stokoe, Christopher. 2005. Differentiating homonymy and pol-
ysemy in information retrieval. In Proceedings of the Human
Language Technology Conference and the Conference on
Empirical Methods in Natural Language Processing. Van-
couver, pages 403–410.
Strapparava, Carlo, Alfio Gliozzo, and Claudio Giuliano. 2004.
Word-sense disambiguation for machine translation. In Mi-
halcea and Edmonds (2004), pages 229–234.
Turney, Peter D. 2001. Mining the web for synonyms: PMI-IR
versus LSA on TOEFL. In Proceedings of the 12th European
Conference on Machine Learning. Freiburg, Germany, pages
491–502.
Twilley, L. C., P. Dixon, D. Taylor, and K. Clark. 1994. Univer-
sity of Alberta norms of relative meaning frequency for 566
homographs. Memory and Cognition 22(1):111–126.
Vickrey, David, Luke Biewald, Marc Teyssier, and Daphne
Koller. 2005. Word-sense disambiguation for machine trans-
lation. In Proceedings of the Human Language Technology
Conference and the Conference on Empirical Methods in
Natural Language Processing. Vancouver, pages 771–778.
Yarowsky, David and Radu Florian. 2002. Evaluating sense dis-
ambiguation across diverse parameter spaces. Natural Lan-
guage Engineering 9(4):293–310.
</reference>
<page confidence="0.999002">
355
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.686546">
<title confidence="0.998271">An Information Retrieval Approach to Sense Ranking</title>
<author confidence="0.770186">Lapata</author>
<affiliation confidence="0.951831">School of Informatics, University of 2 Buccleuch Place, Edinburgh EH8 9LW,</affiliation>
<email confidence="0.996499">mlap@inf.ed.ac.uk</email>
<email confidence="0.996499">keller@inf.ed.ac.uk</email>
<abstract confidence="0.9990553125">In word sense disambiguation, choosing the most frequent sense for an ambiguous word is a powerful heuristic. However, its usefulness is restricted by the availability of sense-annotated data. In this paper, we propose an information retrieval-based method for sense ranking that does not require annotated data. The method queries an information retrieval engine to estimate the degree of association between a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers’ intuitions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation. Las Palmas, Gran Canaria,</booktitle>
<pages>1499--1504</pages>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Briscoe, Ted and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation. Las Palmas, Gran Canaria, pages 1499–1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>Ensemble methods for unsupervised WSD.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>97--104</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2784" citStr="Brody et al., 2006" startWordPosition="417" endWordPosition="420">ghly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottleneck by inferring the first sense automatically from raw text. Automatically acquired first senses will undoubtedly be noisy when compared to human annotations. Nevertheless, they can be usefully employed in two important tasks: (a) to create preliminary annotations, thus supporting the “annotate automatically, correct manually” methodology used to provide high volume annotation in the Penn Treebank project; and (b) in combination with supervised WSD methods that take context into account; for instance, such methods could default to the dominant sens</context>
</contexts>
<marker>Brody, Navigli, Lapata, 2006</marker>
<rawString>Brody, Samuel, Roberto Navigli, and Mirella Lapata. 2006. Ensemble methods for unsupervised WSD. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics. Sydney, Australia, pages 97– 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources.</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="5863" citStr="Budanitsky and Hirst 2001" startWordPosition="888" endWordPosition="891">eighbors, thus delivering a ranking over senses where the most similar sense is intuitively the dominant sense. Their method exploits two notions of similarity, distributional and semantic. Distributionally similar words are acquired from the British National Corpus using an information-theoretic similarity measure (Lin, 1998) operating over dependency relations (e.g., verb-subject, verb-object). The latter are obtained from the output of Briscoe and Carroll’s (2002) parser. The semantic similarity between neighbors and senses is measured using a manually crafted taxonomy such as WordNet (see Budanitsky and Hirst 2001 for an overview of WordNet-based similarity measures). Mohammad and Hirst (2006) propose an algorithm for inferring dominant senses without relying on distributionally similar neighbors. Their approach capitalizes on the collocational nature of semantically related words. Assuming a coarsegrained sense inventory (e.g., the Macquarie Thesaurus), it first creates a matrix whose columns represent all categories (senses) c1 ...cn in the inventory and rows the ambiguous target words w1 ...wm; the matrix cells record the number of times a target word ti co-occurs with category cj within a window of</context>
</contexts>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>Budanitsky, Alexander and Graeme Hirst. 2001. Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures. In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Decadt</author>
<author>V´eronique Hoste</author>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<title>GAMBL, genetic algorithm optimization of memory-based WSD.</title>
<date>2004</date>
<booktitle>In Mihalcea and Edmonds</booktitle>
<pages>108--112</pages>
<marker>Decadt, Hoste, Daelemans, van den Bosch, 2004</marker>
<rawString>Decadt, Bart, V´eronique Hoste, Walter Daelemans, and Antal van den Bosch. 2004. GAMBL, genetic algorithm optimization of memory-based WSD. In Mihalcea and Edmonds (2004), pages 108–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Durkin</author>
<author>Jocelyn Manning</author>
</authors>
<title>Polysemy and the subjective lexicon: Semantic relatedness and the salience of intraword senses.</title>
<date>1989</date>
<journal>Journal of Psycholinguistic Research</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="25972" citStr="Durkin and Manning, 1989" startWordPosition="4266" endWordPosition="4269">utperforms the Sem- division 12 Cor upper bound. Finally, note that our model is a deed 3 conceptually simpler than McCarthy et al. (2004b) and IRST-DDD. It neither requires a parser (for obtaining distributionally similar neighbors) nor any knowledge other than WordNet (e.g., domain labels). This makes our method portable to languages for which syntactic analysis tools and elaborate semantic resources are not available. 4.3 Modeling Human Data Research in psycholinguistics has shown that the meanings of ambiguous words are not perceived as equally salient in the absence of a biasing context (Durkin and Manning, 1989; Twilley et al., 1994). Rather, language users often ascribe dominant and subordinate meanings to polysemous words. Previous studies have elicited intuitions with regard to word senses using a free association task. For example, Durkin and Manning (1989) collected association norms from native speakers for 175 ambiguous words. They asked subjects to read each word and write down the first meaning that came to mind. The words were presented out of context. From the subjects’ responses, they computed sense frequencies, which revealed that most words were attributed a particular meaning with a m</context>
<context position="27611" citStr="Durkin and Manning (1989)" startWordPosition="4531" endWordPosition="4534">ull set of words and elicited sense frequencies see their Appendix A, pp. 501– 609). One stumbling block to using this data are the meanings associated with the ambiguous words. These were provided by native English speakers and may not necessarily correspond to senses described by trained lexicographers. Fortunately, we were able to map most of them (except for six which we discarded) on WordNet synsets (version 1.6); two annotators performed the mapping by comparing the sense descriptions provided by Durkin and Manning 354 Table 6: Meaning frequencies for act and answer; normative data from Durkin and Manning (1989) to WordNet synsets. The annotators agreed in their assignments 81% of the time. Disagreements were resolved through mediation. Examples of Durkin and Manning’s (1989) normative data are given in Table 6. The sense response for answer was mapped to the WordNet synset {answer, reply, response} (Sense 1), the sense solution was mapped to the synset {solution, answer, result, resolution, solvent} (Sense 2), etc. Durkin and Manning did not take part of speech ambiguity into account, as Table 6 shows, subjects came up with meanings relating to the verb and noun part of speech of act. We explored th</context>
</contexts>
<marker>Durkin, Manning, 1989</marker>
<rawString>Durkin, Kevin and Jocelyn Manning. 1989. Polysemy and the subjective lexicon: Semantic relatedness and the salience of intraword senses. Journal of Psycholinguistic Research 18(6):577–612.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Fellbaum, Christiane, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Fellbaum, Christiane, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A Synopsis of Linguistic Theory 1930-1955.</title>
<date>1957</date>
<publisher>Philological Society.</publisher>
<location>Oxford:</location>
<contexts>
<context position="8092" citStr="Firth (1957)" startWordPosition="1248" endWordPosition="1249">ies. Furthermore, it is computationally inexpensive; in contrast to McCarthy et al. (2004a) we do not rely on the structure of the sense inventory for measuring the similarity between synonyms and their senses. Moreover, unlike Mohammad and Hirst (2006), our algorithm only requires co-occurrence frequencies for the target word and its senses, without considering all senses in the inventory and all words in the corpus simultaneously. 3 Method 3.1 Motivation Central in our approach is the assumption that context provides important cues regarding a word’s meaning. The idea dates back at least to Firth (1957) (“You shall know a word by the company it keeps”) and underlies most WSD work to date. Another observation that has found wide application in WSD is that words tend to exhibit only one sense in a given discourse or document (Gale et al., 1992). Furthermore, documents are typically written with certain topics in mind which are often indicated by word distributional patterns (Harris, 1982). For example, documents talking about congressional tenure are likely to contain words such as term of office or incumbency, whereas documents talking about legal tenure (i.e., the right to hold property) 349</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, J. R. 1957. A Synopsis of Linguistic Theory 1930-1955. Oxford: Philological Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities 26(5–6):415–439.</title>
<date>1992</date>
<contexts>
<context position="8336" citStr="Gale et al., 1992" startWordPosition="1292" endWordPosition="1295">nd Hirst (2006), our algorithm only requires co-occurrence frequencies for the target word and its senses, without considering all senses in the inventory and all words in the corpus simultaneously. 3 Method 3.1 Motivation Central in our approach is the assumption that context provides important cues regarding a word’s meaning. The idea dates back at least to Firth (1957) (“You shall know a word by the company it keeps”) and underlies most WSD work to date. Another observation that has found wide application in WSD is that words tend to exhibit only one sense in a given discourse or document (Gale et al., 1992). Furthermore, documents are typically written with certain topics in mind which are often indicated by word distributional patterns (Harris, 1982). For example, documents talking about congressional tenure are likely to contain words such as term of office or incumbency, whereas documents talking about legal tenure (i.e., the right to hold property) 349 are likely to include the words right or land. Now, we could estimate which sense of tenure is most prevalent simply by comparing whether tenure cooccurs more often with term of office than with land provided we knew that both of these terms a</context>
<context position="11649" citStr="Gale et al., 1992" startWordPosition="1858" endWordPosition="1861"> for the first synset, as it contains two synonyms of the target word tenure. (2) a. &amp;quot;tenure&amp;quot; AND &amp;quot;term of office&amp;quot; b. &amp;quot;tenure&amp;quot; AND &amp;quot;incumbency&amp;quot; (3) &amp;quot;tenure&amp;quot; AND &amp;quot;land tenure&amp;quot; For example, query (2-a) will return the number of documents in which tenure and term of office cooccur. Presumably, tenure is mainly used in its dominant sense in these documents. In the same way, query (3) will return documents in which tenure is used in the sense of land tenure. Note that this way of approximating synonym frequencies as document frequencies crucially relies on the “one sense per discourse” hypothesis (Gale et al., 1992), under the assumption that a document counts as a discourse for word sense disambiguation purposes. Apart from synonym frequencies, we also generate hypernym frequencies by submitting queries of the form w1 AND w2, for each w2 E hype(s), the set of immediate hypernyms of the sense s. The hypernym queries for the two senses of tenure are: (4) &amp;quot;tenure&amp;quot; AND &amp;quot;term&amp;quot; (5) &amp;quot;tenure&amp;quot; AND &amp;quot;legal right&amp;quot; Hypernym queries are particularly useful for synsets of size one, i.e., where a word in a given sense has no synonyms, and is only differentiated from other senses by its hypernyms. Before submitting quer</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William A., Kenneth W. Church, and David Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities 26(5–6):415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Discourse and sublanguage. In</title>
<date>1982</date>
<booktitle>Language in Restricted Semantic Domains, Walter de Gruyter,</booktitle>
<pages>231--236</pages>
<editor>R. Kittredge and J. Lehrberger, editors,</editor>
<location>Berlin; New York,</location>
<contexts>
<context position="8483" citStr="Harris, 1982" startWordPosition="1315" endWordPosition="1316">ry and all words in the corpus simultaneously. 3 Method 3.1 Motivation Central in our approach is the assumption that context provides important cues regarding a word’s meaning. The idea dates back at least to Firth (1957) (“You shall know a word by the company it keeps”) and underlies most WSD work to date. Another observation that has found wide application in WSD is that words tend to exhibit only one sense in a given discourse or document (Gale et al., 1992). Furthermore, documents are typically written with certain topics in mind which are often indicated by word distributional patterns (Harris, 1982). For example, documents talking about congressional tenure are likely to contain words such as term of office or incumbency, whereas documents talking about legal tenure (i.e., the right to hold property) 349 are likely to include the words right or land. Now, we could estimate which sense of tenure is most prevalent simply by comparing whether tenure cooccurs more often with term of office than with land provided we knew that both of these terms are semantically related to tenure. Fortunately, senses in WordNet (and related taxonomies) are represented by synonym terms. So, all we need to do </context>
</contexts>
<marker>Harris, 1982</marker>
<rawString>Harris, Zellig. 1982. Discourse and sublanguage. In R. Kittredge and J. Lehrberger, editors, Language in Restricted Semantic Domains, Walter de Gruyter, Berlin; New York, pages 231–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning.</booktitle>
<pages>296--304</pages>
<location>Madison, WI,</location>
<contexts>
<context position="5566" citStr="Lin, 1998" startWordPosition="846" endWordPosition="847">onal model for acquiring dominant senses from text corpora. Key in their approach is the observation that distributionally similar neighbors often provide cues about a word’s senses. The model quantifies the degree of similarity between a word’s sense descriptions and its closest neighbors, thus delivering a ranking over senses where the most similar sense is intuitively the dominant sense. Their method exploits two notions of similarity, distributional and semantic. Distributionally similar words are acquired from the British National Corpus using an information-theoretic similarity measure (Lin, 1998) operating over dependency relations (e.g., verb-subject, verb-object). The latter are obtained from the output of Briscoe and Carroll’s (2002) parser. The semantic similarity between neighbors and senses is measured using a manually crafted taxonomy such as WordNet (see Budanitsky and Hirst 2001 for an overview of WordNet-based similarity measures). Mohammad and Hirst (2006) propose an algorithm for inferring dominant senses without relying on distributionally similar neighbors. Their approach capitalizes on the collocational nature of semantically related words. Assuming a coarsegrained sens</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, Dekang. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning. Madison, WI, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udi Manber</author>
<author>Sun Wu</author>
</authors>
<title>GLIMPSE: a tool to search through entire file systems.</title>
<date>1994</date>
<booktitle>In Proceedings of USENIX Winter 1994 Technical Conference.</booktitle>
<pages>23--32</pages>
<location>San Francisco, CA,</location>
<contexts>
<context position="13034" citStr="Manber and Wu, 1994" startWordPosition="2093" endWordPosition="2096">rm &amp;quot;tenure&amp;quot; is expanded to (&amp;quot;tenure&amp;quot; OR &amp;quot;tenures&amp;quot;), i.e., both singular and plural noun forms are generated. Similarly, all inflected verb forms are generated, e.g., &amp;quot;keep up&amp;quot; gives rise to the query term (&amp;quot;keep up&amp;quot; OR &amp;quot;keeps up&amp;quot; OR &amp;quot;keeping up&amp;quot; OR &amp;quot;kept up&amp;quot;). John Carroll’s suite of morphological tools (morpha and morphg) is used to generate inflected forms for verbs and 350 nouns.1 The queries generated this way are then submitted to an IR engine to obtain document counts. Specifically, we indexed the BNC using GLIMPSE (Global Implicit Search) a fast and flexible indexing and query system2 (Manber and Wu, 1994). GLIMPSE supports approximate and exact matching, Boolean queries, wild cards, regular expressions, and many other options. The text is divided into equal size blocks and an inverted index is created containing the words and the block numbers in which they occur. Given a query, GLIMPSE will retrieve the relevant documents using a two-level search method. It will first locate the query in the inverted index and then use sequential search to find an exact answer. Once synonym frequencies and hypernym frequencies are in place, we can compute a word’s predominant sense in a number of ways. First,</context>
</contexts>
<marker>Manber, Wu, 1994</marker>
<rawString>Manber, Udi and Sun Wu. 1994. GLIMPSE: a tool to search through entire file systems. In Proceedings of USENIX Winter 1994 Technical Conference. San Francisco, CA, pages 23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>279--286</pages>
<location>Barcelona,</location>
<contexts>
<context position="2227" citStr="McCarthy et al., 2004" startWordPosition="327" endWordPosition="330">4 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available. Furthermore, current supervised approaches rarely outperform the simple heuristic of choosing the most common or dominant sense in the training data (henceforth “the first sense heuristic”), despite taking local context into account. One reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottl</context>
<context position="4916" citStr="McCarthy et al. (2004" startWordPosition="747" endWordPosition="750">) data sets. Second, we simulate native speakers’ intuitions about the salience of word meanings and examine whether the estimated sense frequencies correlate with sense production data. In all cases our approach outperforms a naive baseline and yields performances comparable to state of the art. In the following section, we provide an overview of existing work on sense ranking. In Section 3, we introduce our IR-based method, and describe several sense ranking models. In Section 4, we present our results. Discussion of our results and future work conclude the paper (Section 5). 2 Related Work McCarthy et al. (2004a) were the first to propose a computational model for acquiring dominant senses from text corpora. Key in their approach is the observation that distributionally similar neighbors often provide cues about a word’s senses. The model quantifies the degree of similarity between a word’s sense descriptions and its closest neighbors, thus delivering a ranking over senses where the most similar sense is intuitively the dominant sense. Their method exploits two notions of similarity, distributional and semantic. Distributionally similar words are acquired from the British National Corpus using an in</context>
<context position="6719" citStr="McCarthy et al. (2004" startWordPosition="1028" endWordPosition="1031"> nature of semantically related words. Assuming a coarsegrained sense inventory (e.g., the Macquarie Thesaurus), it first creates a matrix whose columns represent all categories (senses) c1 ...cn in the inventory and rows the ambiguous target words w1 ...wm; the matrix cells record the number of times a target word ti co-occurs with category cj within a window of size s. Using an appropriate statistical test, they estimate the relative strength of association between an ambiguous word and each of its senses. The sense with the highest association is the predominant sense. Our work shares with McCarthy et al. (2004a) and Mohammad and Hirst (2006) the objective of inferring dominant senses automatically. We propose a knowledge-lean method that relies on word association and requires no syntactic annotation. The latter may be unavailable when working with languages other than English for which state-of-the-art parsers or taggers have not been developed. Mohammad and Hirst (2006) estimate the co-occurrence frequency of a word and its sense descriptors by considering small window sizes of up to five words. These estimates will be less reliable for moderately frequent words or for sense inventories with many</context>
<context position="9435" citStr="McCarthy et al. 2004" startWordPosition="1472" endWordPosition="1475">mparing whether tenure cooccurs more often with term of office than with land provided we knew that both of these terms are semantically related to tenure. Fortunately, senses in WordNet (and related taxonomies) are represented by synonym terms. So, all we need to do for estimating a word’s sense frequencies is to count how often it co-occurs with its synonyms. We adopt here a fairly broad definition of co-occurrence, two words co-occur if they are attested in the same document. We could obtain such counts from any document collection; however, to facilitate comparisons with prior work (e.g., McCarthy et al. 2004a), all our experiments use the British National Corpus (BNC). In what follows we describe in detail how we retrieve co-occurrence counts from the BNC and how we acquire dominant senses. 3.2 Dominant Sense Acquisition Throughout the paper we use the term frequency as a shorthand for document frequency, i.e., the number of documents that contain a word or a set of words which may or may not be adjacent. The method we propose here exploits document frequencies of words and their sense definitions. We base our discussion below on the WordNet sense inventory and its representation of senses in ter</context>
<context position="16166" citStr="McCarthy et al. (2004" startWordPosition="2608" endWordPosition="2611">ense may differ in frequency. The combination of the above parameters yields 12 sense ranking models. We explore the parameter space exhaustively on the Senseval-2 benchmark data set. The best performing model on this data set is then used in all our subsequent experiments. We use Senseval-2 as a development set, but we also demonstrate that a far smaller manually annotated sample is sufficient for selecting the best model. 4 Experiments Our experiments were driven by three questions: (1) Is WSD feasible at all with a model that does not employ any syntactic or semantic knowledge? Recall that McCarthy et al. (2004a) propose a model that crucially relies on a robust parser for estimating dominant senses. (2) What is the best parameter setting for our model? (3) Do the acquired dominant senses correlate with human judgments? If our sense frequencies exhibit no such correlation, it is unlikely that they will be useful in practical applications. To address the first two questions we use the induced first senses to perform WSD on the Senseval2 and Senseval-3 data sets. For our third question we compare native speakers’ semantic intuitions against the BNC sense frequencies. 351 −Norm +Norm +Hyp −Hyp +Hyp −Hy</context>
<context position="17531" citStr="McCarthy et al. (2004" startWordPosition="2842" endWordPosition="2845">5 49.6 Table 1: Results for Senseval-2 data by model instantiation 4.1 Model Selection The goal of our first experiment is to establish which model configuration (see Section 3.2) is best suited for the WSD task. We thus varied how the overall frequency is computed (Sum, High, Avg), whether hyponyms are included (+Hyp), and whether the frequencies are normalized (+Norm). To explore the parameter space, we used the Senseval-2 all-words test data as our development set. This data set consists of three documents from the Wall Street Journal containing approximately 2,400 content words. Following McCarthy et al. (2004a), we first use our method to find the dominant sense for all word types in the corpus and then use that sense to disambiguate tokens without taking contextual information into account. We used WordNet 1.7.1 (Fellbaum, 1998) senses.3 We compared our results to a baseline that selects for each word type a random sense, assumes it is the dominant one, and uses it to disambiguate all instances of the target word (McCarthy et al., 2004a). We also report the WSD performance of a more competitive baseline that always chooses the sense with the largest synset as the dominant sense. Consider again th</context>
<context position="22639" citStr="McCarthy et al. (2004" startWordPosition="3721" endWordPosition="3724">er we will use this model for further experiments without additional parameter tuning. 4.2 Application to Senseval-3 Data We next evaluate our best model the on the Senseval-3 English all-words data set. Senseval-3 consists of two Wall Street Journal articles and one excerpt from the Brown corpus (approximately 5,000 content words in total). Similarly to the experiments reported in the previous section, we used WordNet 1.7.1. We calculate recall and precision with the Senseval-3 scorer. Our results are given in Table 4. Besides the two baselines (BaseR and BaseS), we also compare our model to McCarthy et al. (2004b)4 and the best unsupervised (IRST-DDD) and supervised (GAMBLE) systems that participated in Senseval-3. IRST-DDD was developed by Strapparava et al. (2004) and performs domain driven disambiguation. Specifically, the approach compares the domain of the context surrounding the target word with the domains of its senses and uses a version of WordNet 4Comparison against Mohammad and Hirst (2006) was not possible since they use a sense inventory other than WordNet (i.e., Roget’s thesaurus) and evaluate their model on artificially generated sense-tagged data. P R BaseR 23.1#†$t 22.7#†$t BaseS 36.</context>
<context position="24800" citStr="McCarthy et al. (2004" startWordPosition="4079" endWordPosition="4082">learning. We also report the performance achieved by defaulting to the first WordNet entry for a given word and part of speech. Entries in WordNet are ranked according to the sense frequency estimates obtained from the manually annotated SemCor corpus. First senses obtained from SemCor will be naturally less noisy than those computed by our method which does not make use of manual annotation in any way. We therefore consider the WSD performance achieved with SemCor first senses as an upper bound for automatically acquired first senses. Our model significantly outperforms the two baselines and McCarthy et al. (2004b). Its precision and recall according to individual parts of speech is shown in Table 5. The model performs comparably to IRST-DDD and significantly worse than GAMBLE. This is not entirely surprising given that GAM353 BLE is a supervised system trained on a variety act Freq answer Freq of manually annotated resources including SemCor, pretense/performance 37 response 81 data from previous Senseval workshops and the ex- to perform 30 solution 18 ample sentences in WordNet 1.7.1. GAMBLE is the to take action 16 only system that significantly outperforms the Sem- division 12 Cor upper bound. Fin</context>
<context position="29453" citStr="McCarthy et al. (2004" startWordPosition="4822" endWordPosition="4825">the sense frequencies it generates can capture the sense preferences of naive human subjects (rather than trained lexicographers). 5 Discussion In this paper we proposed an IR-based approach for inducing dominant senses automatically. Our method estimates the degree of association between words and their sense descriptions (represented by synsets in WordNet) simply by querying an IR engine. Evaluation on the Senseval data sets showed that our model significantly outperformed a naive random sense baseline and a more competitive one based on synset size. Our method was significantly better than McCarthy et al. (2004b) on Senseval-2 and Senseval-3. On the latter data set, its performance was comparable to that of the best unsupervised system (Strapparava et al., 2004). An important future direction lies in evaluating the disambiguation potential of our models across domains and languages. Furthermore, our experiments have relied on WordNet for providing the appropriate sense descriptions. Future work must assess whether the models presented in this paper can be extended to alternative sense inventories (e.g., dictionary definitions) that may differ in granularity and structure. We will also experiment wit</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll. 2004a. Finding predominant senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics. Barcelona, pages 279–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Using automatically acquired predominant senses for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Mihalcea and Edmonds</booktitle>
<pages>151--154</pages>
<contexts>
<context position="2227" citStr="McCarthy et al., 2004" startWordPosition="327" endWordPosition="330">4 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available. Furthermore, current supervised approaches rarely outperform the simple heuristic of choosing the most common or dominant sense in the training data (henceforth “the first sense heuristic”), despite taking local context into account. One reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottl</context>
<context position="4916" citStr="McCarthy et al. (2004" startWordPosition="747" endWordPosition="750">) data sets. Second, we simulate native speakers’ intuitions about the salience of word meanings and examine whether the estimated sense frequencies correlate with sense production data. In all cases our approach outperforms a naive baseline and yields performances comparable to state of the art. In the following section, we provide an overview of existing work on sense ranking. In Section 3, we introduce our IR-based method, and describe several sense ranking models. In Section 4, we present our results. Discussion of our results and future work conclude the paper (Section 5). 2 Related Work McCarthy et al. (2004a) were the first to propose a computational model for acquiring dominant senses from text corpora. Key in their approach is the observation that distributionally similar neighbors often provide cues about a word’s senses. The model quantifies the degree of similarity between a word’s sense descriptions and its closest neighbors, thus delivering a ranking over senses where the most similar sense is intuitively the dominant sense. Their method exploits two notions of similarity, distributional and semantic. Distributionally similar words are acquired from the British National Corpus using an in</context>
<context position="6719" citStr="McCarthy et al. (2004" startWordPosition="1028" endWordPosition="1031"> nature of semantically related words. Assuming a coarsegrained sense inventory (e.g., the Macquarie Thesaurus), it first creates a matrix whose columns represent all categories (senses) c1 ...cn in the inventory and rows the ambiguous target words w1 ...wm; the matrix cells record the number of times a target word ti co-occurs with category cj within a window of size s. Using an appropriate statistical test, they estimate the relative strength of association between an ambiguous word and each of its senses. The sense with the highest association is the predominant sense. Our work shares with McCarthy et al. (2004a) and Mohammad and Hirst (2006) the objective of inferring dominant senses automatically. We propose a knowledge-lean method that relies on word association and requires no syntactic annotation. The latter may be unavailable when working with languages other than English for which state-of-the-art parsers or taggers have not been developed. Mohammad and Hirst (2006) estimate the co-occurrence frequency of a word and its sense descriptors by considering small window sizes of up to five words. These estimates will be less reliable for moderately frequent words or for sense inventories with many</context>
<context position="9435" citStr="McCarthy et al. 2004" startWordPosition="1472" endWordPosition="1475">mparing whether tenure cooccurs more often with term of office than with land provided we knew that both of these terms are semantically related to tenure. Fortunately, senses in WordNet (and related taxonomies) are represented by synonym terms. So, all we need to do for estimating a word’s sense frequencies is to count how often it co-occurs with its synonyms. We adopt here a fairly broad definition of co-occurrence, two words co-occur if they are attested in the same document. We could obtain such counts from any document collection; however, to facilitate comparisons with prior work (e.g., McCarthy et al. 2004a), all our experiments use the British National Corpus (BNC). In what follows we describe in detail how we retrieve co-occurrence counts from the BNC and how we acquire dominant senses. 3.2 Dominant Sense Acquisition Throughout the paper we use the term frequency as a shorthand for document frequency, i.e., the number of documents that contain a word or a set of words which may or may not be adjacent. The method we propose here exploits document frequencies of words and their sense definitions. We base our discussion below on the WordNet sense inventory and its representation of senses in ter</context>
<context position="16166" citStr="McCarthy et al. (2004" startWordPosition="2608" endWordPosition="2611">ense may differ in frequency. The combination of the above parameters yields 12 sense ranking models. We explore the parameter space exhaustively on the Senseval-2 benchmark data set. The best performing model on this data set is then used in all our subsequent experiments. We use Senseval-2 as a development set, but we also demonstrate that a far smaller manually annotated sample is sufficient for selecting the best model. 4 Experiments Our experiments were driven by three questions: (1) Is WSD feasible at all with a model that does not employ any syntactic or semantic knowledge? Recall that McCarthy et al. (2004a) propose a model that crucially relies on a robust parser for estimating dominant senses. (2) What is the best parameter setting for our model? (3) Do the acquired dominant senses correlate with human judgments? If our sense frequencies exhibit no such correlation, it is unlikely that they will be useful in practical applications. To address the first two questions we use the induced first senses to perform WSD on the Senseval2 and Senseval-3 data sets. For our third question we compare native speakers’ semantic intuitions against the BNC sense frequencies. 351 −Norm +Norm +Hyp −Hyp +Hyp −Hy</context>
<context position="17531" citStr="McCarthy et al. (2004" startWordPosition="2842" endWordPosition="2845">5 49.6 Table 1: Results for Senseval-2 data by model instantiation 4.1 Model Selection The goal of our first experiment is to establish which model configuration (see Section 3.2) is best suited for the WSD task. We thus varied how the overall frequency is computed (Sum, High, Avg), whether hyponyms are included (+Hyp), and whether the frequencies are normalized (+Norm). To explore the parameter space, we used the Senseval-2 all-words test data as our development set. This data set consists of three documents from the Wall Street Journal containing approximately 2,400 content words. Following McCarthy et al. (2004a), we first use our method to find the dominant sense for all word types in the corpus and then use that sense to disambiguate tokens without taking contextual information into account. We used WordNet 1.7.1 (Fellbaum, 1998) senses.3 We compared our results to a baseline that selects for each word type a random sense, assumes it is the dominant one, and uses it to disambiguate all instances of the target word (McCarthy et al., 2004a). We also report the WSD performance of a more competitive baseline that always chooses the sense with the largest synset as the dominant sense. Consider again th</context>
<context position="22639" citStr="McCarthy et al. (2004" startWordPosition="3721" endWordPosition="3724">er we will use this model for further experiments without additional parameter tuning. 4.2 Application to Senseval-3 Data We next evaluate our best model the on the Senseval-3 English all-words data set. Senseval-3 consists of two Wall Street Journal articles and one excerpt from the Brown corpus (approximately 5,000 content words in total). Similarly to the experiments reported in the previous section, we used WordNet 1.7.1. We calculate recall and precision with the Senseval-3 scorer. Our results are given in Table 4. Besides the two baselines (BaseR and BaseS), we also compare our model to McCarthy et al. (2004b)4 and the best unsupervised (IRST-DDD) and supervised (GAMBLE) systems that participated in Senseval-3. IRST-DDD was developed by Strapparava et al. (2004) and performs domain driven disambiguation. Specifically, the approach compares the domain of the context surrounding the target word with the domains of its senses and uses a version of WordNet 4Comparison against Mohammad and Hirst (2006) was not possible since they use a sense inventory other than WordNet (i.e., Roget’s thesaurus) and evaluate their model on artificially generated sense-tagged data. P R BaseR 23.1#†$t 22.7#†$t BaseS 36.</context>
<context position="24800" citStr="McCarthy et al. (2004" startWordPosition="4079" endWordPosition="4082">learning. We also report the performance achieved by defaulting to the first WordNet entry for a given word and part of speech. Entries in WordNet are ranked according to the sense frequency estimates obtained from the manually annotated SemCor corpus. First senses obtained from SemCor will be naturally less noisy than those computed by our method which does not make use of manual annotation in any way. We therefore consider the WSD performance achieved with SemCor first senses as an upper bound for automatically acquired first senses. Our model significantly outperforms the two baselines and McCarthy et al. (2004b). Its precision and recall according to individual parts of speech is shown in Table 5. The model performs comparably to IRST-DDD and significantly worse than GAMBLE. This is not entirely surprising given that GAM353 BLE is a supervised system trained on a variety act Freq answer Freq of manually annotated resources including SemCor, pretense/performance 37 response 81 data from previous Senseval workshops and the ex- to perform 30 solution 18 ample sentences in WordNet 1.7.1. GAMBLE is the to take action 16 only system that significantly outperforms the Sem- division 12 Cor upper bound. Fin</context>
<context position="29453" citStr="McCarthy et al. (2004" startWordPosition="4822" endWordPosition="4825">the sense frequencies it generates can capture the sense preferences of naive human subjects (rather than trained lexicographers). 5 Discussion In this paper we proposed an IR-based approach for inducing dominant senses automatically. Our method estimates the degree of association between words and their sense descriptions (represented by synsets in WordNet) simply by querying an IR engine. Evaluation on the Senseval data sets showed that our model significantly outperformed a naive random sense baseline and a more competitive one based on synset size. Our method was significantly better than McCarthy et al. (2004b) on Senseval-2 and Senseval-3. On the latter data set, its performance was comparable to that of the best unsupervised system (Strapparava et al., 2004). An important future direction lies in evaluating the disambiguation potential of our models across domains and languages. Furthermore, our experiments have relied on WordNet for providing the appropriate sense descriptions. Future work must assess whether the models presented in this paper can be extended to alternative sense inventories (e.g., dictionary definitions) that may differ in granularity and structure. We will also experiment wit</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll. 2004b. Using automatically acquired predominant senses for word sense disambiguation. In Mihalcea and Edmonds (2004), pages 151–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Phil Edmonds</author>
<author>editors</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of Senseval-3: The 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<location>Barcelona.</location>
<marker>Mihalcea, Edmonds, editors, 2004</marker>
<rawString>Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceedings of Senseval-3: The 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Graeme Hirst</author>
</authors>
<title>Determining word sense dominance using a thesaurus.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>121--128</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2763" citStr="Mohammad and Hirst, 2006" startWordPosition="413" endWordPosition="416"> reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottleneck by inferring the first sense automatically from raw text. Automatically acquired first senses will undoubtedly be noisy when compared to human annotations. Nevertheless, they can be usefully employed in two important tasks: (a) to create preliminary annotations, thus supporting the “annotate automatically, correct manually” methodology used to provide high volume annotation in the Penn Treebank project; and (b) in combination with supervised WSD methods that take context into account; for instance, such methods could default</context>
<context position="5944" citStr="Mohammad and Hirst (2006)" startWordPosition="899" endWordPosition="902">ntuitively the dominant sense. Their method exploits two notions of similarity, distributional and semantic. Distributionally similar words are acquired from the British National Corpus using an information-theoretic similarity measure (Lin, 1998) operating over dependency relations (e.g., verb-subject, verb-object). The latter are obtained from the output of Briscoe and Carroll’s (2002) parser. The semantic similarity between neighbors and senses is measured using a manually crafted taxonomy such as WordNet (see Budanitsky and Hirst 2001 for an overview of WordNet-based similarity measures). Mohammad and Hirst (2006) propose an algorithm for inferring dominant senses without relying on distributionally similar neighbors. Their approach capitalizes on the collocational nature of semantically related words. Assuming a coarsegrained sense inventory (e.g., the Macquarie Thesaurus), it first creates a matrix whose columns represent all categories (senses) c1 ...cn in the inventory and rows the ambiguous target words w1 ...wm; the matrix cells record the number of times a target word ti co-occurs with category cj within a window of size s. Using an appropriate statistical test, they estimate the relative streng</context>
<context position="7733" citStr="Mohammad and Hirst (2006)" startWordPosition="1188" endWordPosition="1191">urrence frequency of a word and its sense descriptors by considering small window sizes of up to five words. These estimates will be less reliable for moderately frequent words or for sense inventories with many senses. Our approach is more robust to sparse data – we work with document-based frequencies – and thus suitable for both coarse and fine grained sense inventories. Furthermore, it is computationally inexpensive; in contrast to McCarthy et al. (2004a) we do not rely on the structure of the sense inventory for measuring the similarity between synonyms and their senses. Moreover, unlike Mohammad and Hirst (2006), our algorithm only requires co-occurrence frequencies for the target word and its senses, without considering all senses in the inventory and all words in the corpus simultaneously. 3 Method 3.1 Motivation Central in our approach is the assumption that context provides important cues regarding a word’s meaning. The idea dates back at least to Firth (1957) (“You shall know a word by the company it keeps”) and underlies most WSD work to date. Another observation that has found wide application in WSD is that words tend to exhibit only one sense in a given discourse or document (Gale et al., 19</context>
<context position="19590" citStr="Mohammad and Hirst, 2006" startWordPosition="3192" endWordPosition="3196">*# 47.3*# 569 Adj 22.1 21.4 56.5 56.0 56.7* 56.2* 451 Adv 48.0 45.9 66.4 62.9 86.4*# 81.8*# 301 All 26.3 25.4 42.2 40.7 59.7*# 57.6*# 2,384 Table 2: Results of best model (High, +Norm, −Hyp) for Senseval-2 data by part of speech (*: sig. diff. from BaseR, #: sig. diff. from BaseS; p &lt; 0.01 using x2 test) shared among several senses – and selecting one predominant sense over the other can be due to very small frequency differences. We also find that models with normalized document counts outperform models without normalization. This is not surprising, there is ample evidence in the literature (Mohammad and Hirst, 2006; Turney, 2001) that association measures (e.g., conditional probability, mutual information) are better indicators of lexical similarity than raw frequency. Finally, selecting the synonym with the highest frequency (and defaulting to its sense) achieves better results in comparison to averaging or summing over all synsets. In sum, the best performing model is High, +Norm, −Hyp, achieving a precision of 59.7% and a recall of 57.9%. The results for this model are broken down by part of speech in Table 2. Here, we also include a comparison with the random baseline (BaseR) and a baseline that sel</context>
<context position="23036" citStr="Mohammad and Hirst (2006)" startWordPosition="3779" endWordPosition="3782">ious section, we used WordNet 1.7.1. We calculate recall and precision with the Senseval-3 scorer. Our results are given in Table 4. Besides the two baselines (BaseR and BaseS), we also compare our model to McCarthy et al. (2004b)4 and the best unsupervised (IRST-DDD) and supervised (GAMBLE) systems that participated in Senseval-3. IRST-DDD was developed by Strapparava et al. (2004) and performs domain driven disambiguation. Specifically, the approach compares the domain of the context surrounding the target word with the domains of its senses and uses a version of WordNet 4Comparison against Mohammad and Hirst (2006) was not possible since they use a sense inventory other than WordNet (i.e., Roget’s thesaurus) and evaluate their model on artificially generated sense-tagged data. P R BaseR 23.1#†$t 22.7#†$t BaseS 36.6*†$t 35.9*†$t McCarthy 49.0*#$t 43.0*#$t IR-Model 58.0*#†t 57.0*#†t IRST-DDD 58.3*#†t 58.2*#†t Semcor 62.4*#†$ 62.4*#†$ GAMBLE 65.1*#†$t 65.2*#†$t Table 4: Comparison of results on Senseval-3 data (*: sig. diff. from BaseR, #: sig. diff. from BaseS, †: sig. diff. from McCarthy, $: sig. diff. from IRModel, t: sig. diff. from SemCor; p &lt; 0.01 using x2 test) BaseR BaseS Model N P R P R P R Noun 2</context>
</contexts>
<marker>Mohammad, Hirst, 2006</marker>
<rawString>Mohammad, Saif and Graeme Hirst. 2006. Determining word sense dominance using a thesaurus. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics. Trento, Italy, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Christiane Fellbaum</author>
<author>Scott Cotton</author>
<author>Lauren Delfs</author>
<author>Hoa Trang Dang</author>
</authors>
<title>English tasks: All words and verb lexical sample.</title>
<date>2001</date>
<booktitle>In Proceedings of Senseval-2: The 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<location>Toulouse.</location>
<contexts>
<context position="4255" citStr="Palmer et al., 2001" startWordPosition="638" endWordPosition="641">ssociations between words and sense descriptions automatically by querying an IR engine whose index terms have been compiled from the corpus of interest. The approach is inexpensive, languageindependent, requires minimal supervision, and uses no additional knowledge other than the word senses proper and morphological query expansions. We 348 Proceedings of NAACL HLT 2007, pages 348–355, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics evaluate our method on two tasks. First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) data sets. Second, we simulate native speakers’ intuitions about the salience of word meanings and examine whether the estimated sense frequencies correlate with sense production data. In all cases our approach outperforms a naive baseline and yields performances comparable to state of the art. In the following section, we provide an overview of existing work on sense ranking. In Section 3, we introduce our IR-based method, and describe several sense ranking models. In Section 4, we present our results. Discussion of our results and future work conclud</context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, Dang, 2001</marker>
<rawString>Palmer, Martha, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang Dang. 2001. English tasks: All words and verb lexical sample. In Proceedings of Senseval-2: The 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English allwords task.</title>
<date>2004</date>
<booktitle>In Mihalcea and Edmonds</booktitle>
<contexts>
<context position="4296" citStr="Snyder and Palmer, 2004" startWordPosition="644" endWordPosition="647">escriptions automatically by querying an IR engine whose index terms have been compiled from the corpus of interest. The approach is inexpensive, languageindependent, requires minimal supervision, and uses no additional knowledge other than the word senses proper and morphological query expansions. We 348 Proceedings of NAACL HLT 2007, pages 348–355, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics evaluate our method on two tasks. First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) data sets. Second, we simulate native speakers’ intuitions about the salience of word meanings and examine whether the estimated sense frequencies correlate with sense production data. In all cases our approach outperforms a naive baseline and yields performances comparable to state of the art. In the following section, we provide an overview of existing work on sense ranking. In Section 3, we introduce our IR-based method, and describe several sense ranking models. In Section 4, we present our results. Discussion of our results and future work conclude the paper (Section 5). 2 Related Work M</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Snyder, Benjamin and Martha Palmer. 2004. The English allwords task. In Mihalcea and Edmonds (2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Stokoe</author>
</authors>
<title>Differentiating homonymy and polysemy in information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing. Vancouver,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="1223" citStr="Stokoe, 2005" startWordPosition="173" endWordPosition="174">etween a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers’ intuitions. 1 Introduction Word sense disambiguation (WSD), the ability to identify the intended meanings (senses) of words in context, is crucial for accomplishing many NLP tasks that require semantic processing. Examples include paraphrase acquisition, discourse parsing, or metonymy resolution. Applications such as machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005) have also been shown to benefit from WSD. Given the importance of WSD for basic NLP tasks and multilingual applications, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data (see Yarowsky and Florian 2002; Mihalcea and Edmonds 2004 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and th</context>
</contexts>
<marker>Stokoe, 2005</marker>
<rawString>Stokoe, Christopher. 2005. Differentiating homonymy and polysemy in information retrieval. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing. Vancouver, pages 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
</authors>
<title>Word-sense disambiguation for machine translation. In Mihalcea and Edmonds</title>
<date>2004</date>
<pages>229--234</pages>
<contexts>
<context position="22796" citStr="Strapparava et al. (2004)" startWordPosition="3742" endWordPosition="3745"> the on the Senseval-3 English all-words data set. Senseval-3 consists of two Wall Street Journal articles and one excerpt from the Brown corpus (approximately 5,000 content words in total). Similarly to the experiments reported in the previous section, we used WordNet 1.7.1. We calculate recall and precision with the Senseval-3 scorer. Our results are given in Table 4. Besides the two baselines (BaseR and BaseS), we also compare our model to McCarthy et al. (2004b)4 and the best unsupervised (IRST-DDD) and supervised (GAMBLE) systems that participated in Senseval-3. IRST-DDD was developed by Strapparava et al. (2004) and performs domain driven disambiguation. Specifically, the approach compares the domain of the context surrounding the target word with the domains of its senses and uses a version of WordNet 4Comparison against Mohammad and Hirst (2006) was not possible since they use a sense inventory other than WordNet (i.e., Roget’s thesaurus) and evaluate their model on artificially generated sense-tagged data. P R BaseR 23.1#†$t 22.7#†$t BaseS 36.6*†$t 35.9*†$t McCarthy 49.0*#$t 43.0*#$t IR-Model 58.0*#†t 57.0*#†t IRST-DDD 58.3*#†t 58.2*#†t Semcor 62.4*#†$ 62.4*#†$ GAMBLE 65.1*#†$t 65.2*#†$t Table 4: </context>
<context position="29607" citStr="Strapparava et al., 2004" startWordPosition="4848" endWordPosition="4851">s paper we proposed an IR-based approach for inducing dominant senses automatically. Our method estimates the degree of association between words and their sense descriptions (represented by synsets in WordNet) simply by querying an IR engine. Evaluation on the Senseval data sets showed that our model significantly outperformed a naive random sense baseline and a more competitive one based on synset size. Our method was significantly better than McCarthy et al. (2004b) on Senseval-2 and Senseval-3. On the latter data set, its performance was comparable to that of the best unsupervised system (Strapparava et al., 2004). An important future direction lies in evaluating the disambiguation potential of our models across domains and languages. Furthermore, our experiments have relied on WordNet for providing the appropriate sense descriptions. Future work must assess whether the models presented in this paper can be extended to alternative sense inventories (e.g., dictionary definitions) that may differ in granularity and structure. We will also experiment with a wider range of lexical association measures for quantifying the similarity of a word and its synonyms. Examples include odds ratio (Mohammad and Hirst</context>
</contexts>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>Strapparava, Carlo, Alfio Gliozzo, and Claudio Giuliano. 2004. Word-sense disambiguation for machine translation. In Mihalcea and Edmonds (2004), pages 229–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning.</booktitle>
<pages>491--502</pages>
<location>Freiburg, Germany,</location>
<contexts>
<context position="19605" citStr="Turney, 2001" startWordPosition="3197" endWordPosition="3198">4 56.5 56.0 56.7* 56.2* 451 Adv 48.0 45.9 66.4 62.9 86.4*# 81.8*# 301 All 26.3 25.4 42.2 40.7 59.7*# 57.6*# 2,384 Table 2: Results of best model (High, +Norm, −Hyp) for Senseval-2 data by part of speech (*: sig. diff. from BaseR, #: sig. diff. from BaseS; p &lt; 0.01 using x2 test) shared among several senses – and selecting one predominant sense over the other can be due to very small frequency differences. We also find that models with normalized document counts outperform models without normalization. This is not surprising, there is ample evidence in the literature (Mohammad and Hirst, 2006; Turney, 2001) that association measures (e.g., conditional probability, mutual information) are better indicators of lexical similarity than raw frequency. Finally, selecting the synonym with the highest frequency (and defaulting to its sense) achieves better results in comparison to averaging or summing over all synsets. In sum, the best performing model is High, +Norm, −Hyp, achieving a precision of 59.7% and a recall of 57.9%. The results for this model are broken down by part of speech in Table 2. Here, we also include a comparison with the random baseline (BaseR) and a baseline that selects the domina</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Turney, Peter D. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the 12th European Conference on Machine Learning. Freiburg, Germany, pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L C Twilley</author>
<author>P Dixon</author>
<author>D Taylor</author>
<author>K Clark</author>
</authors>
<title>University of Alberta norms of relative meaning frequency for 566 homographs.</title>
<date>1994</date>
<journal>Memory and Cognition</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="25995" citStr="Twilley et al., 1994" startWordPosition="4270" endWordPosition="4273">on 12 Cor upper bound. Finally, note that our model is a deed 3 conceptually simpler than McCarthy et al. (2004b) and IRST-DDD. It neither requires a parser (for obtaining distributionally similar neighbors) nor any knowledge other than WordNet (e.g., domain labels). This makes our method portable to languages for which syntactic analysis tools and elaborate semantic resources are not available. 4.3 Modeling Human Data Research in psycholinguistics has shown that the meanings of ambiguous words are not perceived as equally salient in the absence of a biasing context (Durkin and Manning, 1989; Twilley et al., 1994). Rather, language users often ascribe dominant and subordinate meanings to polysemous words. Previous studies have elicited intuitions with regard to word senses using a free association task. For example, Durkin and Manning (1989) collected association norms from native speakers for 175 ambiguous words. They asked subjects to read each word and write down the first meaning that came to mind. The words were presented out of context. From the subjects’ responses, they computed sense frequencies, which revealed that most words were attributed a particular meaning with a markedly higher frequenc</context>
</contexts>
<marker>Twilley, Dixon, Taylor, Clark, 1994</marker>
<rawString>Twilley, L. C., P. Dixon, D. Taylor, and K. Clark. 1994. University of Alberta norms of relative meaning frequency for 566 homographs. Memory and Cognition 22(1):111–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Luke Biewald</author>
<author>Marc Teyssier</author>
<author>Daphne Koller</author>
</authors>
<title>Word-sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing. Vancouver,</booktitle>
<pages>771--778</pages>
<contexts>
<context position="1182" citStr="Vickrey et al., 2005" startWordPosition="165" endWordPosition="168">al engine to estimate the degree of association between a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers’ intuitions. 1 Introduction Word sense disambiguation (WSD), the ability to identify the intended meanings (senses) of words in context, is crucial for accomplishing many NLP tasks that require semantic processing. Examples include paraphrase acquisition, discourse parsing, or metonymy resolution. Applications such as machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005) have also been shown to benefit from WSD. Given the importance of WSD for basic NLP tasks and multilingual applications, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data (see Yarowsky and Florian 2002; Mihalcea and Edmonds 2004 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words fo</context>
</contexts>
<marker>Vickrey, Biewald, Teyssier, Koller, 2005</marker>
<rawString>Vickrey, David, Luke Biewald, Marc Teyssier, and Daphne Koller. 2005. Word-sense disambiguation for machine translation. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing. Vancouver, pages 771–778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
</authors>
<title>Evaluating sense disambiguation across diverse parameter spaces.</title>
<date>2002</date>
<journal>Natural Language Engineering</journal>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="1580" citStr="Yarowsky and Florian 2002" startWordPosition="229" endWordPosition="232">, is crucial for accomplishing many NLP tasks that require semantic processing. Examples include paraphrase acquisition, discourse parsing, or metonymy resolution. Applications such as machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005) have also been shown to benefit from WSD. Given the importance of WSD for basic NLP tasks and multilingual applications, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data (see Yarowsky and Florian 2002; Mihalcea and Edmonds 2004 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available. Furthermore, current supervised approaches rarely outperform the simple heuristic of choosing the most common or dominant sense in the training data (henceforth “the first sense heuristic”), despite taking local context into account. One reason for this is the highly skewed dis</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>Yarowsky, David and Radu Florian. 2002. Evaluating sense disambiguation across diverse parameter spaces. Natural Language Engineering 9(4):293–310.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>