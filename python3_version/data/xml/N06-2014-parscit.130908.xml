<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.052675">
<title confidence="0.986167">
Agreement/Disagreement Classification:
Exploiting Unlabeled Data using Contrast Classifiers
</title>
<author confidence="0.992358">
Sangyun Hahn Richard Ladner Mari Ostendorf
</author>
<affiliation confidence="0.998304">
Dept. of Computer Science and Engineering Dept. of Electrical Engineering
University of Washington, Seattle, WA University of Washington, Seattle, WA
</affiliation>
<email confidence="0.999306">
{syhahn,ladner}@cs.washington.edu mo@ee.washington.edu
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999306">
Several semi-supervised learning methods
have been proposed to leverage unlabeled
data, but imbalanced class distributions in
the data set can hurt the performance of
most algorithms. In this paper, we adapt
the new approach of contrast classifiers for
semi-supervised learning. This enables us
to exploit large amounts of unlabeled data
with a skewed distribution. In experiments
on a speech act (agreement/disagreement)
classification problem, we achieve better
results than other semi-supervised meth-
ods. We also obtain performance com-
parable to the best results reported so far
on this task and outperform systems with
equivalent feature sets.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999638076923077">
In natural language understanding research with
data-driven techniques, data labeling is an essential
but time-consuming and costly process. To allevi-
ate this effort, various semi-supervised learning al-
gorithms such as self-training (Yarowsky, 1995), co-
training (Blum and Mitchell, 1998; Goldman and
Zhou, 2000), transductive SVM (Joachims, 1999)
and many others have been proposed and success-
fully applied under different assumptions and set-
tings. They all aim to improve classification accu-
racy by exploiting more readily available unlabeled
data as well as labeled examples. However, these
iterative training methods have shortcomings when
</bodyText>
<page confidence="0.988817">
53
</page>
<bodyText confidence="0.999952060606061">
trained on data with imbalanced class distributions.
One reason is that most classifiers underlying these
methods assume a balanced training set, and thus
when one of the classes has a much larger number of
examples than the other classes, the trained classifier
will be biased toward the majority class. The imbal-
ance will propagate through subsequent iterations,
resulting in a more skewed data set upon which a
further biased classifier will be trained. To exploit
unlabeled data in learning an inherently skewed data
distribution, we introduce a semi-supervised classi-
fication method using contrast classifiers, first pro-
posed by Peng et al. (Peng et al., 2003). It approx-
imates the posterior class probability given an ob-
servation using class-specific contrast classifiers that
implicitly model the difference between the distrib-
ution of labeled data for that class and the unlabeled
data.
In this paper, we will explore the applicabil-
ity of contrast classifiers to the problem of semi-
supervised learning for identifying agreements and
disagreements in multi-party conversational speech.
These labels represent a simple type of “speech act”
that can be important for understanding the interac-
tion between speakers, or for automatically summa-
rizing or browsing the contents of a meeting. This
problem was previously studied (Hillard et al., 2003;
Galley et al., 2004), using a subset of ICSI meet-
ing recording corpus (Janin et al., 2003). In semi-
supervised learning, there is a challenge due to an
imbalanced class distribution: over 60% of the data
are associated with the default class and only 5% are
with disagreements.
</bodyText>
<note confidence="0.813912">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 53–56,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.889223" genericHeader="method">
2 Contrast Classifier
</sectionHeader>
<bodyText confidence="0.999732181818182">
The contrast classifier approach was developed by
Peng et al and successfully applied to the problem
of identifying protein disorder in a protein struc-
ture database (outlier detection) and to finding arti-
cles about them (single-class detection) (Peng et al.,
2003). A contrast classifier discriminates between
the labeled and unlabeled data, and can be used
to approximate the posterior class probability of a
given data instance as follows. Taking a Bayesian
approach, a contrast classifier for the j-th class is
defined as:
</bodyText>
<equation confidence="0.998542">
rjg(x)
ccj(x) = (1 − rj)hj(x) + rjg(x) (1)
</equation>
<bodyText confidence="0.9663916">
where hj(x) is the likelihood of x generated by
class j in the labeled data, g(x) is the distribution
of unlabeled data, and rj is the relative proportion
of unlabeled data compared to the labeled data for
class j. This discriminates the class j in the la-
beled data from the unlabeled data. Here, we con-
strain rj = 0.5 for all j, using resampling to address
class distribution skew, as described below. Rewrit-
ing equation 1, hj(x) can be expressed in terms of
ccj(x) as:
</bodyText>
<equation confidence="0.998127">
1 − ccj(x)
hj(x) = ccj(x)
</equation>
<bodyText confidence="0.999372">
Then, the posterior probability of an input x for class
j, p(j|x), can be approximated as:
</bodyText>
<equation confidence="0.925921333333333">
hj(x)qj
p(j|x) = (3)
&amp; hi(x)qi
</equation>
<bodyText confidence="0.9997205">
where qj is the prior class probability which can
be approximated by the fraction of instances in the
class j among the labeled data. By substituting eq. 2
into eq. 3, we obtain:
</bodyText>
<equation confidence="0.932537">
qj · (1 − ccj(x))/ccj(x)
p(j|x ) = &amp; qi · (1 − cci(x))/cci(x). (4)
</equation>
<bodyText confidence="0.999707333333333">
Notice that we do not have to explicitly estimate
g(x). Eq. 4 can be used to construct the MAP clas-
sifier:
</bodyText>
<equation confidence="0.869316">
max
1 − ccj (x)
ccj (x)
</equation>
<bodyText confidence="0.999918642857143">
To approximate the class-specific contrast classifier,
ccj(x), we can choose any classifier that outputs a
probability, such as a neural net, logistic regression,
or an SVM with outputs calibrated to produce a rea-
sonable probability.
Typically a lot more unlabeled data are avail-
able than labeled data, which causes class imbalance
when training a contrast classifier. In a supervised
setting, a resampling technique is often used to re-
duce the effect of imbalanced data. Here, we use a
committee of classifiers, each of which is trained on
a balanced training set sampled from each class. To
compute the final output of the classifier, we imple-
mented four different strategies.
</bodyText>
<listItem confidence="0.979871666666667">
• For each class, average the outputs of the con-
trast classifiers in the committee, and use the
average as ccj(x) in eq. 5.
• Average only the outputs of contrast classifiers
smaller than their corresponding threshold, and
the fraction of the included classifiers is used
as the strength of the probability output for the
class.
• Use a meta classifier whose inputs are the out-
puts of the contrast classifiers in the commit-
tee for a class, and whose output is modeled by
training it from a separate, randomly sampled
data set. The output of the meta classifier is
used as ccj(x).
• Classify an input as the majority class only
when the outputs of the meta classifiers for
the other classes are all larger than their cor-
responding thresholds.
</listItem>
<bodyText confidence="0.999475071428572">
Another benefit of the contrast classifier approach
is that it is less affected by imbalanced data. When
training the contrast classifier for each class, it uses
the instances in only one class in the labeled data,
and implicitly models the data distribution within
that class independently of other classes. That is,
given a data instance, the distribution within a class,
hj(x), determines the output of the contrast classi-
fier for the class (eq. 1), which in turn determines
the posterior probability (eq. 4). Thus it will not be
as highly biased toward the majority class as a clas-
sifier trained with a collection of data from imbal-
anced classes. Our experimental results presented in
the next section confirm this benefit.
</bodyText>
<equation confidence="0.9974094">
r
· ·g(x). (2)
1 − r
cˆ = arg
· qj (5)
</equation>
<page confidence="0.996137">
54
</page>
<sectionHeader confidence="0.999356" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999961">
We conducted experiments to answer the following
questions. First, is the contrast classifier approach
applicable to language processing problems, which
often involve large amounts of unlabeled data? Sec-
ond, does it outperform other semi-supervised learn-
ing methods on a skewed data set?
</bodyText>
<subsectionHeader confidence="0.997112">
3.1 Features and data sets
</subsectionHeader>
<bodyText confidence="0.999966083333333">
The data set used consists of seven transcripts out of
75 meeting transcripts included in the ICSI meet-
ing corpus (Janin et al., 2003). For the study, 7
meetings were segmented into spurts, defined as a
chunk of speech of a speaker containing no longer
than 0.5 second pause. The first 450 spurts in each
of four meetings were hand-labeled as either posi-
tive (agreement, 9%), negative (disagreement, 6%),
backchannel (23%) or other (62%).
To approximate ccs(x) we use a Support Vector
Machine (SVM) that outputs the probability of the
positive class given an instance (Lin et al., 2003).
We use only word-based features similar to those
used in (Hillard et al., 2003), which include the num-
ber of words in a spurt, the number of keywords
associated with the positive and negative classes,
and classification based on keywords. We also ob-
tain word and class-based bigram language models
for each class from the training data, and compute
such language model features as the perplexity of a
spurt, probability of the spurt, and the probability of
the first two words in a spurt, using each language
model. We also include the most likely class by the
language models as features.
</bodyText>
<subsectionHeader confidence="0.868791">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.996929583333333">
First, we performed the same experiment as in
(Hillard et al., 2003) and (Galley et al., 2004), using
the contrast classifier (CC) method. Among the four
meetings, the data from one meeting was set aside
for testing. Table 1 compares the 3-class accuracy
of the contrast classifier with previous results, merg-
ing positive and backchannel class together into one
class as in the other work. When only lexical fea-
tures are used (the first three entries), the SVM-
based contrast classifier using meta-classifiers gives
the best performance, outperforming the decision
tree in (Hillard et al., 2003) and the maximum en-
</bodyText>
<tableCaption confidence="0.953346">
Table 1: Comparison of 3-way classification accu-
racy on lexical (lex) vs. expanded (exp) features
sets.
</tableCaption>
<table confidence="0.999567166666667">
Accuracy
Hillard-lex 82
Galley-lex 85.0
SVM-lex 86.3
CC-lex 86.7
Galley-exp 86.9
</table>
<tableCaption confidence="0.978961">
Table 2: Comparison of the classification perfor-
mance
</tableCaption>
<table confidence="0.998715857142857">
Method 3-way A/D A/D
Acc confusion recovery
unsupervised 79 8 83
cc 81.4 4 82.4
cc-threshold 76.7 6 85.2
cc-meta 86.7 5 81.3
cc-meta-thres 87.1 5 82.4
</table>
<bodyText confidence="0.9992136">
tropy model in (Galley et al., 2004). It also outper-
formed the SVM trained using the labeled data only.
The contrast classifier is also competitive with the
best case result in (Galley et al., 2004) (last entry),
which adds speaker change, segment duration, and
adjacency pair sequence dependency features using
a dynamic Bayesian network.
In table 2, we report the performance of the four
classification strategies described in section 2. For
comparison, we include a result from Hillard, ob-
tained by training a decision tree on the labels pro-
duced by their unsupervised clustering technique.
Meta classifiers usually obtained higher accuracy,
but averaging often achieved higher recovery of
agreement/disagreement (A/D) spurts. The use of
thresholds increases A/D recovery, with a decrease
in accuracy. We obtained the best accuracy using
both meta classifiers and thresholds together here,
but we more often obtained higher accuracy using
meta classifiers only.
Next, we performed experiments on the entire
ICSI meeting data. Only 1,318 spurts were labeled,
and 62,944 spurts were unlabeled. Again, one of the
labeled meeting transcripts was set aside as a test set.
We compared the SVM trained only on labeled data
</bodyText>
<page confidence="0.999426">
55
</page>
<tableCaption confidence="0.999919">
Table 3: Classification performance, training on the
</tableCaption>
<bodyText confidence="0.689905">
entire ICSI data set. F is defined as ���
��� where p is
macro precision and r is the macro recall.
</bodyText>
<table confidence="0.9974102">
Method Acc F Neg recall
SVM 85.4 72.6 21.1
self-training 80.4 65.3 5.2
cotraining 85.1 73.8 47.4
cc 83.0 75.5 68.5
</table>
<bodyText confidence="0.997765814814815">
with three semi-supervised methods: self-training,
co-training, and the contrast classifier with a meta-
classifier. The self-training iteratively trained an
SVM with additional data labeled with confidence
by the previously trained SVM. For the co-training,
each of an SVM and a multilayer backpropagation
network was trained on the labeled data and the un-
labeled data classified with high confidence (99%)
by one classifier were used as labeled data for fur-
ther training the other classifier. We used two differ-
ent classifiers, instead of two independent view of
the input features as in (Goldman and Zhou, 2000).
Table 3 shows that the SVM obtained high accu-
racy, but the F measure and the recall of the smallest
class, negative, is quite low. The bias toward the ma-
jority class propagates through each iteration in self-
training, so that only 5% of the negative tokens were
detected after 30 iterations. We observed the same
pattern in co-training; its accuracy peaked after two
iterations (85.1%) and then performance degraded
drastically (68% after five iterations) due in part to
an increase in mislabeled data in the training set (as
previously observed in (Pierce and Cardie, 2001))
and in part because the data skew is not controlled
for. The contrast classifier performs better than the
others in both F measure and negative class recall,
retaining reasonably good accuracy.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999751947368421">
In summary, our experiments on agree-
ment/disagreement detection show that semi-
supervised learning using contrast classifiers is an
effective method for taking advantage of a large
unlabeled data set for a problem with imbalanced
classes. The contrast classifier approach outper-
forms co-training and self-training in detecting
the infrequent classes. We also obtain good per-
formance relative to other methods using simple
lexical features and performance comparable to the
best result reported.
The experiments here kept the feature set fixed,
but results of (Galley et al., 2004) suggest that
further gains can be achieved by augmenting the
feature set. In addition, it is important to assess
the impact of semi-supervised training with recog-
nizer output, where gains from using unlabeled data
may be greater than with reference transcripts as in
(Hillard et al., 2003).
</bodyText>
<sectionHeader confidence="0.999146" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903857142857">
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proc. Conference
on Computational Learning Theory (COLT-98), pages
92–100.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: use of Bayesian networks to model
dependencies. In Proc. ACL.
S. Goldman and Y. Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proc. the 17th ICML,
pages 327–334.
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. Detec-
tion of agreement vs. disagreement in meetings: train-
ing with unlabeled data. In Proc. HLT-NAACL.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In ICASSP-03.
T. Joachims. 1999. Transductive inference for text clas-
sification using support vector machines. In Proc.
ICML, pages 200–209.
H. T. Lin, C. J. Lin, and R. C. Weng. 2003. A note
on platt’s probabilistic outputs for support vector ma-
chines. Technical report, Dept. of Computer Science,
National Taiwan University.
K. Peng, S. Vucetic, B. Han, H. Xie, and Z. Obradovic.
2003. Exploiting unlabeled data for improving accu-
racy of predictive data mining. In ICDM, pages 267–
274.
D. Pierce and C. Cardie. 2001. Limitations of co-training
for natural language learning from large datasets. In
Proc. EMNLP-2001).
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. ACL,
pages 189–196.
</reference>
<page confidence="0.998421">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987236">
<title confidence="0.996316">Agreement/Disagreement Exploiting Unlabeled Data using Contrast Classifiers</title>
<author confidence="0.99994">Sangyun Hahn Richard Ladner Mari Ostendorf</author>
<affiliation confidence="0.998986">Dept. of Computer Science and Engineering Dept. of Electrical University of Washington, Seattle, WA University of Washington, Seattle, WA</affiliation>
<email confidence="0.999812">mo@ee.washington.edu</email>
<abstract confidence="0.999811058823529">Several semi-supervised learning methods have been proposed to leverage unlabeled data, but imbalanced class distributions in the data set can hurt the performance of most algorithms. In this paper, we adapt the new approach of contrast classifiers for semi-supervised learning. This enables us to exploit large amounts of unlabeled data with a skewed distribution. In experiments on a speech act (agreement/disagreement) classification problem, we achieve better results than other semi-supervised methods. We also obtain performance comparable to the best results reported so far on this task and outperform systems with equivalent feature sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proc. Conference on Computational Learning Theory (COLT-98),</booktitle>
<pages>92--100</pages>
<contexts>
<context position="1298" citStr="Blum and Mitchell, 1998" startWordPosition="169" endWordPosition="172">abeled data with a skewed distribution. In experiments on a speech act (agreement/disagreement) classification problem, we achieve better results than other semi-supervised methods. We also obtain performance comparable to the best results reported so far on this task and outperform systems with equivalent feature sets. 1 Introduction In natural language understanding research with data-driven techniques, data labeling is an essential but time-consuming and costly process. To alleviate this effort, various semi-supervised learning algorithms such as self-training (Yarowsky, 1995), cotraining (Blum and Mitchell, 1998; Goldman and Zhou, 2000), transductive SVM (Joachims, 1999) and many others have been proposed and successfully applied under different assumptions and settings. They all aim to improve classification accuracy by exploiting more readily available unlabeled data as well as labeled examples. However, these iterative training methods have shortcomings when 53 trained on data with imbalanced class distributions. One reason is that most classifiers underlying these methods assume a balanced training set, and thus when one of the classes has a much larger number of examples than the other classes, </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proc. Conference on Computational Learning Theory (COLT-98), pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model dependencies. In</title>
<date>2004</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="3029" citStr="Galley et al., 2004" startWordPosition="434" endWordPosition="437">cific contrast classifiers that implicitly model the difference between the distribution of labeled data for that class and the unlabeled data. In this paper, we will explore the applicability of contrast classifiers to the problem of semisupervised learning for identifying agreements and disagreements in multi-party conversational speech. These labels represent a simple type of “speech act” that can be important for understanding the interaction between speakers, or for automatically summarizing or browsing the contents of a meeting. This problem was previously studied (Hillard et al., 2003; Galley et al., 2004), using a subset of ICSI meeting recording corpus (Janin et al., 2003). In semisupervised learning, there is a challenge due to an imbalanced class distribution: over 60% of the data are associated with the default class and only 5% are with disagreements. Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 53–56, New York, June 2006. c�2006 Association for Computational Linguistics 2 Contrast Classifier The contrast classifier approach was developed by Peng et al and successfully applied to the problem of identifying protein disorder in a pr</context>
<context position="8858" citStr="Galley et al., 2004" startWordPosition="1430" endWordPosition="1433">03), which include the number of words in a spurt, the number of keywords associated with the positive and negative classes, and classification based on keywords. We also obtain word and class-based bigram language models for each class from the training data, and compute such language model features as the perplexity of a spurt, probability of the spurt, and the probability of the first two words in a spurt, using each language model. We also include the most likely class by the language models as features. 3.2 Results First, we performed the same experiment as in (Hillard et al., 2003) and (Galley et al., 2004), using the contrast classifier (CC) method. Among the four meetings, the data from one meeting was set aside for testing. Table 1 compares the 3-class accuracy of the contrast classifier with previous results, merging positive and backchannel class together into one class as in the other work. When only lexical features are used (the first three entries), the SVMbased contrast classifier using meta-classifiers gives the best performance, outperforming the decision tree in (Hillard et al., 2003) and the maximum enTable 1: Comparison of 3-way classification accuracy on lexical (lex) vs. expande</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model dependencies. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldman</author>
<author>Y Zhou</author>
</authors>
<title>Enhancing supervised learning with unlabeled data.</title>
<date>2000</date>
<booktitle>In Proc. the 17th ICML,</booktitle>
<pages>327--334</pages>
<contexts>
<context position="1323" citStr="Goldman and Zhou, 2000" startWordPosition="173" endWordPosition="176"> distribution. In experiments on a speech act (agreement/disagreement) classification problem, we achieve better results than other semi-supervised methods. We also obtain performance comparable to the best results reported so far on this task and outperform systems with equivalent feature sets. 1 Introduction In natural language understanding research with data-driven techniques, data labeling is an essential but time-consuming and costly process. To alleviate this effort, various semi-supervised learning algorithms such as self-training (Yarowsky, 1995), cotraining (Blum and Mitchell, 1998; Goldman and Zhou, 2000), transductive SVM (Joachims, 1999) and many others have been proposed and successfully applied under different assumptions and settings. They all aim to improve classification accuracy by exploiting more readily available unlabeled data as well as labeled examples. However, these iterative training methods have shortcomings when 53 trained on data with imbalanced class distributions. One reason is that most classifiers underlying these methods assume a balanced training set, and thus when one of the classes has a much larger number of examples than the other classes, the trained classifier wi</context>
<context position="11869" citStr="Goldman and Zhou, 2000" startWordPosition="1910" endWordPosition="1913"> 75.5 68.5 with three semi-supervised methods: self-training, co-training, and the contrast classifier with a metaclassifier. The self-training iteratively trained an SVM with additional data labeled with confidence by the previously trained SVM. For the co-training, each of an SVM and a multilayer backpropagation network was trained on the labeled data and the unlabeled data classified with high confidence (99%) by one classifier were used as labeled data for further training the other classifier. We used two different classifiers, instead of two independent view of the input features as in (Goldman and Zhou, 2000). Table 3 shows that the SVM obtained high accuracy, but the F measure and the recall of the smallest class, negative, is quite low. The bias toward the majority class propagates through each iteration in selftraining, so that only 5% of the negative tokens were detected after 30 iterations. We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the</context>
</contexts>
<marker>Goldman, Zhou, 2000</marker>
<rawString>S. Goldman and Y. Zhou. 2000. Enhancing supervised learning with unlabeled data. In Proc. the 17th ICML, pages 327–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>E Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: training with unlabeled data. In</title>
<date>2003</date>
<booktitle>Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="3007" citStr="Hillard et al., 2003" startWordPosition="430" endWordPosition="433">vation using class-specific contrast classifiers that implicitly model the difference between the distribution of labeled data for that class and the unlabeled data. In this paper, we will explore the applicability of contrast classifiers to the problem of semisupervised learning for identifying agreements and disagreements in multi-party conversational speech. These labels represent a simple type of “speech act” that can be important for understanding the interaction between speakers, or for automatically summarizing or browsing the contents of a meeting. This problem was previously studied (Hillard et al., 2003; Galley et al., 2004), using a subset of ICSI meeting recording corpus (Janin et al., 2003). In semisupervised learning, there is a challenge due to an imbalanced class distribution: over 60% of the data are associated with the default class and only 5% are with disagreements. Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 53–56, New York, June 2006. c�2006 Association for Computational Linguistics 2 Contrast Classifier The contrast classifier approach was developed by Peng et al and successfully applied to the problem of identifying pr</context>
<context position="8241" citStr="Hillard et al., 2003" startWordPosition="1324" endWordPosition="1327">s out of 75 meeting transcripts included in the ICSI meeting corpus (Janin et al., 2003). For the study, 7 meetings were segmented into spurts, defined as a chunk of speech of a speaker containing no longer than 0.5 second pause. The first 450 spurts in each of four meetings were hand-labeled as either positive (agreement, 9%), negative (disagreement, 6%), backchannel (23%) or other (62%). To approximate ccs(x) we use a Support Vector Machine (SVM) that outputs the probability of the positive class given an instance (Lin et al., 2003). We use only word-based features similar to those used in (Hillard et al., 2003), which include the number of words in a spurt, the number of keywords associated with the positive and negative classes, and classification based on keywords. We also obtain word and class-based bigram language models for each class from the training data, and compute such language model features as the perplexity of a spurt, probability of the spurt, and the probability of the first two words in a spurt, using each language model. We also include the most likely class by the language models as features. 3.2 Results First, we performed the same experiment as in (Hillard et al., 2003) and (Gal</context>
</contexts>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>D. Hillard, M. Ostendorf, and E. Shriberg. 2003. Detection of agreement vs. disagreement in meetings: training with unlabeled data. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In ICASSP-03.</booktitle>
<contexts>
<context position="3099" citStr="Janin et al., 2003" startWordPosition="447" endWordPosition="450"> the distribution of labeled data for that class and the unlabeled data. In this paper, we will explore the applicability of contrast classifiers to the problem of semisupervised learning for identifying agreements and disagreements in multi-party conversational speech. These labels represent a simple type of “speech act” that can be important for understanding the interaction between speakers, or for automatically summarizing or browsing the contents of a meeting. This problem was previously studied (Hillard et al., 2003; Galley et al., 2004), using a subset of ICSI meeting recording corpus (Janin et al., 2003). In semisupervised learning, there is a challenge due to an imbalanced class distribution: over 60% of the data are associated with the default class and only 5% are with disagreements. Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 53–56, New York, June 2006. c�2006 Association for Computational Linguistics 2 Contrast Classifier The contrast classifier approach was developed by Peng et al and successfully applied to the problem of identifying protein disorder in a protein structure database (outlier detection) and to finding articles a</context>
<context position="7708" citStr="Janin et al., 2003" startWordPosition="1235" endWordPosition="1238">ion of data from imbalanced classes. Our experimental results presented in the next section confirm this benefit. r · ·g(x). (2) 1 − r cˆ = arg · qj (5) 54 3 Experiments We conducted experiments to answer the following questions. First, is the contrast classifier approach applicable to language processing problems, which often involve large amounts of unlabeled data? Second, does it outperform other semi-supervised learning methods on a skewed data set? 3.1 Features and data sets The data set used consists of seven transcripts out of 75 meeting transcripts included in the ICSI meeting corpus (Janin et al., 2003). For the study, 7 meetings were segmented into spurts, defined as a chunk of speech of a speaker containing no longer than 0.5 second pause. The first 450 spurts in each of four meetings were hand-labeled as either positive (agreement, 9%), negative (disagreement, 6%), backchannel (23%) or other (62%). To approximate ccs(x) we use a Support Vector Machine (SVM) that outputs the probability of the positive class given an instance (Lin et al., 2003). We use only word-based features similar to those used in (Hillard et al., 2003), which include the number of words in a spurt, the number of keywo</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI meeting corpus. In ICASSP-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>200--209</pages>
<contexts>
<context position="1358" citStr="Joachims, 1999" startWordPosition="179" endWordPosition="180">ct (agreement/disagreement) classification problem, we achieve better results than other semi-supervised methods. We also obtain performance comparable to the best results reported so far on this task and outperform systems with equivalent feature sets. 1 Introduction In natural language understanding research with data-driven techniques, data labeling is an essential but time-consuming and costly process. To alleviate this effort, various semi-supervised learning algorithms such as self-training (Yarowsky, 1995), cotraining (Blum and Mitchell, 1998; Goldman and Zhou, 2000), transductive SVM (Joachims, 1999) and many others have been proposed and successfully applied under different assumptions and settings. They all aim to improve classification accuracy by exploiting more readily available unlabeled data as well as labeled examples. However, these iterative training methods have shortcomings when 53 trained on data with imbalanced class distributions. One reason is that most classifiers underlying these methods assume a balanced training set, and thus when one of the classes has a much larger number of examples than the other classes, the trained classifier will be biased toward the majority cl</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Transductive inference for text classification using support vector machines. In Proc. ICML, pages 200–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Lin</author>
<author>C J Lin</author>
<author>R C Weng</author>
</authors>
<title>A note on platt’s probabilistic outputs for support vector machines.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Dept. of Computer Science, National Taiwan University.</institution>
<contexts>
<context position="8160" citStr="Lin et al., 2003" startWordPosition="1310" endWordPosition="1313">et? 3.1 Features and data sets The data set used consists of seven transcripts out of 75 meeting transcripts included in the ICSI meeting corpus (Janin et al., 2003). For the study, 7 meetings were segmented into spurts, defined as a chunk of speech of a speaker containing no longer than 0.5 second pause. The first 450 spurts in each of four meetings were hand-labeled as either positive (agreement, 9%), negative (disagreement, 6%), backchannel (23%) or other (62%). To approximate ccs(x) we use a Support Vector Machine (SVM) that outputs the probability of the positive class given an instance (Lin et al., 2003). We use only word-based features similar to those used in (Hillard et al., 2003), which include the number of words in a spurt, the number of keywords associated with the positive and negative classes, and classification based on keywords. We also obtain word and class-based bigram language models for each class from the training data, and compute such language model features as the perplexity of a spurt, probability of the spurt, and the probability of the first two words in a spurt, using each language model. We also include the most likely class by the language models as features. 3.2 Resu</context>
</contexts>
<marker>Lin, Lin, Weng, 2003</marker>
<rawString>H. T. Lin, C. J. Lin, and R. C. Weng. 2003. A note on platt’s probabilistic outputs for support vector machines. Technical report, Dept. of Computer Science, National Taiwan University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Peng</author>
<author>S Vucetic</author>
<author>B Han</author>
<author>H Xie</author>
<author>Z Obradovic</author>
</authors>
<title>Exploiting unlabeled data for improving accuracy of predictive data mining.</title>
<date>2003</date>
<booktitle>In ICDM,</booktitle>
<pages>267--274</pages>
<contexts>
<context position="2323" citStr="Peng et al., 2003" startWordPosition="327" endWordPosition="330">ions. One reason is that most classifiers underlying these methods assume a balanced training set, and thus when one of the classes has a much larger number of examples than the other classes, the trained classifier will be biased toward the majority class. The imbalance will propagate through subsequent iterations, resulting in a more skewed data set upon which a further biased classifier will be trained. To exploit unlabeled data in learning an inherently skewed data distribution, we introduce a semi-supervised classification method using contrast classifiers, first proposed by Peng et al. (Peng et al., 2003). It approximates the posterior class probability given an observation using class-specific contrast classifiers that implicitly model the difference between the distribution of labeled data for that class and the unlabeled data. In this paper, we will explore the applicability of contrast classifiers to the problem of semisupervised learning for identifying agreements and disagreements in multi-party conversational speech. These labels represent a simple type of “speech act” that can be important for understanding the interaction between speakers, or for automatically summarizing or browsing </context>
<context position="3753" citStr="Peng et al., 2003" startWordPosition="548" endWordPosition="551">is a challenge due to an imbalanced class distribution: over 60% of the data are associated with the default class and only 5% are with disagreements. Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 53–56, New York, June 2006. c�2006 Association for Computational Linguistics 2 Contrast Classifier The contrast classifier approach was developed by Peng et al and successfully applied to the problem of identifying protein disorder in a protein structure database (outlier detection) and to finding articles about them (single-class detection) (Peng et al., 2003). A contrast classifier discriminates between the labeled and unlabeled data, and can be used to approximate the posterior class probability of a given data instance as follows. Taking a Bayesian approach, a contrast classifier for the j-th class is defined as: rjg(x) ccj(x) = (1 − rj)hj(x) + rjg(x) (1) where hj(x) is the likelihood of x generated by class j in the labeled data, g(x) is the distribution of unlabeled data, and rj is the relative proportion of unlabeled data compared to the labeled data for class j. This discriminates the class j in the labeled data from the unlabeled data. Here</context>
</contexts>
<marker>Peng, Vucetic, Han, Xie, Obradovic, 2003</marker>
<rawString>K. Peng, S. Vucetic, B. Han, H. Xie, and Z. Obradovic. 2003. Exploiting unlabeled data for improving accuracy of predictive data mining. In ICDM, pages 267– 274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pierce</author>
<author>C Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proc. EMNLP-2001).</booktitle>
<contexts>
<context position="12444" citStr="Pierce and Cardie, 2001" startWordPosition="2006" endWordPosition="2009">the input features as in (Goldman and Zhou, 2000). Table 3 shows that the SVM obtained high accuracy, but the F measure and the recall of the smallest class, negative, is quite low. The bias toward the majority class propagates through each iteration in selftraining, so that only 5% of the negative tokens were detected after 30 iterations. We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for. The contrast classifier performs better than the others in both F measure and negative class recall, retaining reasonably good accuracy. 4 Conclusion In summary, our experiments on agreement/disagreement detection show that semisupervised learning using contrast classifiers is an effective method for taking advantage of a large unlabeled data set for a problem with imbalanced classes. The contrast classifier approach outperforms co-training and self-training in detecting the infrequent classes. We also obtain good performance relative </context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>D. Pierce and C. Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proc. EMNLP-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="1261" citStr="Yarowsky, 1995" startWordPosition="165" endWordPosition="166"> exploit large amounts of unlabeled data with a skewed distribution. In experiments on a speech act (agreement/disagreement) classification problem, we achieve better results than other semi-supervised methods. We also obtain performance comparable to the best results reported so far on this task and outperform systems with equivalent feature sets. 1 Introduction In natural language understanding research with data-driven techniques, data labeling is an essential but time-consuming and costly process. To alleviate this effort, various semi-supervised learning algorithms such as self-training (Yarowsky, 1995), cotraining (Blum and Mitchell, 1998; Goldman and Zhou, 2000), transductive SVM (Joachims, 1999) and many others have been proposed and successfully applied under different assumptions and settings. They all aim to improve classification accuracy by exploiting more readily available unlabeled data as well as labeled examples. However, these iterative training methods have shortcomings when 53 trained on data with imbalanced class distributions. One reason is that most classifiers underlying these methods assume a balanced training set, and thus when one of the classes has a much larger number</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. ACL, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>