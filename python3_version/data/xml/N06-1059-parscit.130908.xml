<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9788735">
An Information-Theoretic Approach to
Automatic Evaluation of Summaries
</title>
<author confidence="0.9963">
Chin-Yew Lin+, Guihong Cao *, Jianfeng Gao#, and Jian-Yun Nie
</author>
<affiliation confidence="0.76858775">
Information Sciences Institute+ University de Montr6al&apos; Microsoft Corporation#
University of Southern California Montr6al, Canada One Microsoft Way
Marina del Rey, CA 90292 {caogui,nie}@iro .umontreal .ca Redmond, WA 98052
USA USA
</affiliation>
<email confidence="0.997099">
cyl@isi.edu jfgao@microsoft.com
</email>
<sectionHeader confidence="0.994732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957684210526">
Until recently there are no common, con-
venient, and repeatable evaluation methods
that could be easily applied to sup-
port fast turn-around development of auto-
matic text summarization systems. In this
paper, we introduce an information-
theoretic approach to automatic evaluation
of summaries based on the Jensen-Shannon
divergence of distributions between an
automatic summary and a set of reference
summaries. Several variants of the ap-
proach are also considered and compared.
The results indicate that JS divergence-
based evaluation method achieves compa-
rable performance with the common auto-
matic evaluation method ROUGE in single
documents summarization task; while
achieves better performance than ROUGE
in multiple document summarization task.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999826090909091">
Most previous automatic evaluation methods in
summarization use co-occurrence statistics (Lin
and Hovy 2003) to measure the content overlap
between an automatic summary and a set of refer-
ence summaries. Among them, ROUGE (Lin
2004) has been used by the annual summarization
evaluation conference, Document Understanding
Conference1 (DUC), sponsored by NIST since
2001 . Content and quality of a summary are the
two main aspects of summarization measured in
the past DUCs. Using a manual evaluation inter-
</bodyText>
<note confidence="0.655722">
1 Please see: http://duc.nist.gov for more information
</note>
<bodyText confidence="0.999815230769231">
face called SEE2, NIST assessors compared the
content overlap between a system summary and a
reference summary and assigned a coverage score
to indicate the extent of the overlap between sys-
tem and reference summaries. The overall system
content coverage score was then the average of
coverage scores over a set of test topics. NIST as-
sessors also judged the quality of a peer summary
by answering a set of quality assessment questions
related to grammaticality, coherence, and organiza-
tion for each system summary. However, we only
focus on automatic evaluation of content coverage
in this paper and aim at establishing a statistical
framework that can perform at least as good as the
current state-of-the-art automatic summarization
evaluation methods such as ROUGE.
We start with a brief description of our statistical
summary generation model and how to estimate its
parameters in the next section. We then describe
experimental setups and criterion of success in
Section 3 . The results of the experiments are
shown and analyzed in Section 4 . We discuss re-
lated work and recent advances in statistical lan-
guage models for information retrieval in Section
5 . Finally, we conclude and suggest future direc-
tions in Section 6.
</bodyText>
<sectionHeader confidence="0.98745" genericHeader="introduction">
2 Summarization Evaluation Using In-
formation-Theoretic Measures
</sectionHeader>
<bodyText confidence="0.999855">
Given a set of documents D = {d,, d2, ..., di13, i
= 1 to n, we assume there exists a probabilistic
distribution with parameters specified by BR that
generates reference summaries from D . The task of
summarization is to estimate BR . Similarly, we as-
</bodyText>
<footnote confidence="0.317978">
2 SEE can be downloaded at: http://www.isi.edu/—cyl/SEE.
3 n = 1 for a single document summarization task; n &gt; 1 for a
multi-document summarization task.
</footnote>
<page confidence="0.9891">
463
</page>
<note confidence="0.995389">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 463–470,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999400136363636">
sume every system summary is generated from a
probabilistic distribution with parameters specified
by o,. Therefore, a good summarizer should have
its oA very close to oR and the process of summary
evaluation could be viewed as a task of estimating
the distance between oA and oR .
For example, if we use Kullback-Leibler (KL) di-
vergence as the distance function, then the sum-
mary evaluation task could be viewed as finding
the KL divergence between oA and oR . However,
KL divergence is unspecified when a value is de-
fined in oR but not in oA (Lin 1991, Dagan et al .
1999) . Usually smoothing has to be applied to ad-
dress this missing data problem (unseen word in
this case).
Another problem is that KL divergence is not
symmetric, i .e . KL(oR IIBA) # KL((BA IIBR), except
when oR = oA. This is counter-intuitive in our appli-
cation scenario. We therefore use generalized Jen-
sen-Shannon (JS) divergence proposed by Lin
(1991) . The JS divergence can be written as fol-
lows:
</bodyText>
<equation confidence="0.996696">
4 (P1, P2,..., P.) =
� �
� �
� �� ��
� � � ���(�� )
� ,
� � �
��1 ��1
</equation>
<bodyText confidence="0.934194">
where pi is a probability distribution with weight
�
��, � ,
�� � 1 and H() is Shannon entropy
Since the Jensen-Shannon divergence is a distance
measure, we take its negative value to indicate the
similarity between two distributions as follows:
</bodyText>
<equation confidence="0.321821">
������������
�� (�� I ��) � ���1�2(�(�� I )[i �(�� I ��)) .(3)
</equation>
<bodyText confidence="0.993011">
Equation (3) suggests that the problem of summary
evaluation could be cast as ranking system summa-
ries according to their negative Jensen-Shannon
divergence with respect to the estimated posterior
distribution of reference summaries. The question
now is how to estimate these distributions.
</bodyText>
<subsectionHeader confidence="0.903285">
2.1 Estimation of Posterior and Prior Sys-
tem Summary Distributions
</subsectionHeader>
<bodyText confidence="0.9966885">
BA is estimated via maximum a posterior (MAP)
as:
</bodyText>
<equation confidence="0.9284105">
o&apos; = arg max p(oA I SA)
BA
By Bayes&apos; rule, the posterior probability of oA
given SA, p(oA I SA), can be written as:
P(RA I SA) = P(SA I0,)P(BA)
P(SA)
</equation>
<bodyText confidence="0.923648666666667">
Assuming a multinomial generation model
(Zaragoza et al . 2oo3) for each summary, param-
eterized by:
</bodyText>
<equation confidence="0.986135333333333">
m
0A = (BA,1,01,2, ...,BA,m) G 1o,11_, � ��,� � 1.
��1
</equation>
<bodyText confidence="0.875546333333333">
oA,; is the parameter of generating word i in sum-
mary SA and in is the total number of words in the
vocabulary. Assuming a bag-of-words unigram
model, the system summary likelihood can be ex-
pressed as/follows:
�m
PISA IBA) =Za1(BA,i)a&apos; , (5)
where ai is the number of word i occurring in
summary SA, ao = E mi=1 � a. , and Zao is a constant as:
</bodyText>
<equation confidence="0.9999125">
17(ao + 1)
17(ai + 1)
</equation>
<bodyText confidence="0.952149857142857">
where r is the gamma function, i .e .
r(n + 1) = nF(n), r(o) =1, n is an integer and n &gt;_ o .
In a MAP estimate, we usually choose the conju-
ate distribution of the generation distribution for a
prior. In our case, we assume a Dirichlet prior dis-
tribution (the conjugate distribution of the multi-
nomial distribution) as follows: (L
</bodyText>
<equation confidence="0.933112">
P(RA) =Zao��m1(0&amp;quot;,)a,-1, (6)
</equation>
<bodyText confidence="0.986098">
where ai is hyperparameter related to word i,
m and Z¢o is:
</bodyText>
<equation confidence="0.994499">
a =�. a� .,
o i=1 �. a � &gt;o
r(ao)
r(a)
</equation>
<bodyText confidence="0.999344">
By the theory of total probability, the system
summary probability can be computed as follows:
</bodyText>
<equation confidence="0.997939109375">
P(SA) = �(�� I )~( )~
= Z Zao� ® m=1 ( F1 (BA i ) doA
��
Qo i a,+a,-1 (7)
= Z� ao
� ��1
�m �(a. + a)
h(ao + ao)
�
�
�
�
�
�
�
� �
E
i=1
For = 2 and equal weight, we have the following:
JS1�2
[i r2) =
1
2
(P1
B,
�
�
~1 log �1
��1
� 2
�
~2 log �1
��1
� 2
�
�
�1 1 1 (2)
� �
� �
� � �
1 �
� �
� �
� � �
� � �
� ~2
1
2
�2
� ~2
1
2
(1)
, (4)
�
Z�
m
�
i=1
,
�
Zao�
m
i=1
</equation>
<page confidence="0.9100525">
11
464
</page>
<bodyText confidence="0.892457">
Substituting (5), (6), and (7) into (4), we have the
posterior distribution p(BA I SA) as below:
</bodyText>
<equation confidence="0.893974555555555">
P(BA ISA)/ /
PISA I /BA)PIBA)
PISA)
�
I&apos; (ai + ai)
ap 11m1(BA,i ),,+a, -1
fj
m �/ a,+a,_1
Zap +ap&apos; i=1(OA,i)
</equation>
<bodyText confidence="0.946711571428572">
We now turn to discuss different ways of estimat-
ing BA,i and ai and their implications as described
by Zaragoza et al . (2pp3) .
According to Equation (8), the posterior distribu-
tion of BA given SA is also a Dirichlet distribution.
Its maximum posterior estimation has the follow-
ing form (Gelman et al . 2pp3):
</bodyText>
<equation confidence="0.9894235">
�m ai+a,-1
BA,` ap + a p — m
</equation>
<bodyText confidence="0.998871666666667">
an (d the posterior distribution (8) c(an be written as:
pIBA SA) &apos;„ p(BA SA) —Zap +ap � j= AT )a,+a,-1. (1p)
If we set ai =1, then BA,~ does not depend on a,,
i .e . all possible BA&apos;s have equal prior. In this case,
equation (9) becomes the maximum likelihood es-
timation as follows:
</bodyText>
<equation confidence="0.813955">
��,� �� � ��
ap
</equation>
<bodyText confidence="0.87087">
and( the posterior distribution (8) can be written as:
</bodyText>
<subsectionHeader confidence="0.452873">
pIBA SA) &apos;&amp;quot; p(BA SA) = Zap +m1m1(OA,i )a�. (12)
</subsectionHeader>
<bodyText confidence="0.999761904761905">
The problem with using maximum likelihood esti-
mation is when ai equal to zero. If zero occur-
rence happens for word i, then its maximum
likelihood estimation, B� , would be zero and the
whole posterior distribution would be zero. To
tackle this problem, we need to redistribute some
probability mass to zero occurrence events or un-
seen word events. The process of redistribution is
called smoot!in in the language modeling litera-
tures. For an in-depth discussion of this topic,
please see Chen and Goodman (1996) .
By choosing different value for ai , we could de-
rive different smoothing methods as discussed in
Zaragoza et al . (2pp3) . For example, we could es-
timate ai using topic collection frequency by set-
ting ai = ,up(&amp;quot;i I #) + 1, where p is a scaling factor
and p(&amp;quot;i I #) is the probability of word i occurring
in topic #. This is called Bayes-smoothing and has
been used in language modeling for information
retrieval (Zhai and Lafferty 2pp4) . The Bayes-
smoothing can be written as:
</bodyText>
<figure confidence="0.74744675">
$ OA
= ai +,up(&amp;quot;i I #)
A,
ap +p
Using Equation (13), Equation (8)becomes:
P(RA I SA) &apos;&amp;quot; P(BA$S I SA) — Za,+p+Jj&amp;quot;(OA,�
$S
)W +-(&amp;quot;j# ). (14)
</figure>
<bodyText confidence="0.993175">
We now turn to estimating the posterior distribu-
tion of BR given a reference summary SR .
</bodyText>
<subsectionHeader confidence="0.9952845">
2.2 Estimation of Reference Summary
Distributions
</subsectionHeader>
<bodyText confidence="0.999739666666667">
Given a reference summary SR, we could esti-
mate posterior distribution BR in the same way that
we estimate posterior distribution BA as follows:
</bodyText>
<equation confidence="0.95043025">
P(ER I SR) = P(SR IBR)P(ER )
ASO
m �a,+a, _1
Zap +ap&apos; i=1(BR,i) ,
</equation>
<bodyText confidence="0.999831333333333">
where ai is the number of occurrence of word i in
reference summary SR .
Given another reference summary SR&apos; , i .e ., when
multiple reference summaries are available, the
posterior distribution can be updated using Bayes-
ian( inference as follows:
</bodyText>
<equation confidence="0.960471833333333">
PIER I SR, S.&apos; )
�(��,�� &apos; I ��)
�
P(SR &apos; I SR)
�(�� &apos; I~,~)~(~ I ��)
�
P(SR&apos;I SR)
�(�� &apos; I~)~(~ I ��)
�
P(SR )
� &apos;
a,
Z°�lli=1(BR,i) Zap +ap�i=1(BR,i)
�
�m
Zap&apos; Zap +ap
i-1I&apos;(a, + ai + a,)
I&apos;(ap&apos; + ap + ap)
_ F(ap&apos; +ap +ap) m (B )a,+a,+a,-1
fjm �i=1 T,i
i-11,(a,: + ai +a,)
�
�Z&apos;(~,~)~~&apos;~~~1,
ap&apos; +ap +ap � ��1
</equation>
<bodyText confidence="0.999934333333333">
where p(BR I SR) is the posterior distribution from
equation (15), SR &apos; is independent of SR, and p(SR )
is computed using equation (7) but with the poste-
rior distribution p(BR I SR) as prior. In a more gen-
eral case, given multiple (L) reference summaries,
SR,1, &amp;quot;&apos;,SR,L , the posterior distribution of BR could
</bodyText>
<equation confidence="0.922618833333333">
(8)
, (9)
(11)
(13)
(15)
(16)
</equation>
<page confidence="0.990025">
465
</page>
<bodyText confidence="0.994676">
be written as follows by repeat application of
Bayesian inference with equation (16):
</bodyText>
<equation confidence="0.9627494">
�
P(ER I SR, 1, ... , SR,L) = Z L f1 m1 (o.,, )
o0 + °0J
�
��1
</equation>
<bodyText confidence="0.999159052631579">
where a,,, is the number of occurrence of word i in
reference summary SR, ;, and
Equation (18) is the total number of words in ref-
erence summary s,,; . The total number of words in
the topic collection could be computed as follows:
Equation (17) indicates that estimation of posterior
distribution given multiple summaries is the same
as estimation of posterior distribution given a sin-
gle summary that contains all the reference sum-
maries . This is reasonable since we assume a bag-
of-word unigram likelihood model for generating
summaries4 . It also bodes well with the consensus-
oriented manual summarization evaluation ap-
proaches proposed by van Halteren and Teufel
(2003) and Nenkova and Passonneau (2004) . With
equations (8) and (17), the summary score of sys-
tem summary, SA, can be computed using Jensen-
Shannon divergence from equation (3) as follows:
SCOPe On,~(SA SRL) =-&apos;JS1/2(P(O&amp;quot; IS&amp;quot;)
</bodyText>
<sectionHeader confidence="0.611294" genericHeader="method">
S&amp;quot;)OP(BR I SR1,L)), (20)
</sectionHeader>
<bodyText confidence="0.998314">
where SRL is a shorthand for SR,1,...,SR,L .
</bodyText>
<sectionHeader confidence="0.998521" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999933153846154">
We used data from DUC 2002 100-word single
and multi-document tasks as our testing corpus.
DUC 2002 data includes 59 topic sets. Each topic
set contains about 10 news article pertaining to
some news topic, for example, topic D061 is about
&amp;quot;Hurricane %i&amp;&apos;ert&amp;quot; . Two human written summa-
ries per topic are provided as reference summaries.
14 sets of system summaries and 1 simple lead
baseline summary are included for the single
document summarization task (total 15 runs);
while 8 sets of system summaries, 1 lead baseline,
and 1 latest news baseline are included for the
multi-document summarization task (total 12 runs).
</bodyText>
<sectionHeader confidence="0.789379" genericHeader="method">
4 Please refer to equation (5)
</sectionHeader>
<bodyText confidence="0.999947322580645">
All summaries are about 100 words5. Manually
evaluation results in average coverage6 scores are
also included in the DUC 2002 data set.
The commonly used criterion of success to evalu-
ate an automatic evaluation method is to compute
the correlation between the ranking of systems ac-
cording to human assigned scores and the ranking
according to automatic scores (Papineni et al .
2002; Lin &amp; Hovy 2003) . We followed the same
convention and computed Pearson&apos;s product mo-
ment correlation coefficient and Spearman&apos;s rank
correlation coefficient as indicators of success.
Besides evaluating the performance of the auto-
matic evaluation measure based on Jensen-
Shannon (IS) divergence as defined in equation
(2), we also compared it with measures based on
KL-divergence and simple log likelihood. The ef-
fect of smoothing and the difference of using sin-
gle and multiple reference summaries were also
investigated. To examine the effect of using longer
n-grams (n &gt; 1), we also used bag-of-bigram and
bag-of-trigram models by simply replace unigrams
in the model proposed in Section 2 with bigrams
and trigrams and treat them as unigrams. Lemur
toolkit version 4 .07 was used to estimate models
with modification to speedup computation of bi-
gram and trigram models. We also ran standard
ROUGE v1 .5 .5 with ROUGE1 to 4 as baselines.
All experiments were run with common words ex-
cluded and Porter stemmer applied. We summarize
these experiments in the following sections.
</bodyText>
<subsectionHeader confidence="0.99854">
3.1 Jensen-Shannon Divergence (JSD)
</subsectionHeader>
<bodyText confidence="0.9999842">
We use equation (22) to compute summary
score and apply maximum likelihood estimation
(a&apos; ) of the parameters according to equation (11) .
Using a unigram model and single reference sum-
mary, we rewrite equation (22) as follows:
</bodyText>
<construct confidence="0.580396461538461">
5 There were also 10-, 50-, and 200-word summary tasks in
DUC 2002 multi-document summarization evaluation. How-
ever, we only used the data of 100-word summarization sub-
task for this experiment.
6 Coverage is a weighted recall metric measuring the overlap
of content between a system summary and a reference sum-
mary. For example, if 4 elementary discourse units (EDU,
Marcu 1998) in a system summary partially match EDUs in a
reference summary of total 8 EDUs, then its coverage score is
R*4/8 . R is the ratio of the partial match. R is 20%, 40%,
60%, 80%, or 100% in DUC 2002 . A single human assigned
the ratio using only one reference. The other reference was not
used in the manual evaluation.
</construct>
<page confidence="0.523762">
7 The Lemur project: http://www.lemurproject.org.
</page>
<figure confidence="0.930578">
a0,; = Ea,, ;.(18)
~=0
m
L m (19)
.
~=1 ~=1 ~=1
L
� �
~0, � EE a,,,
(17)
,
� � �1� ��, �
�
��1
</figure>
<page confidence="0.741705">
466
</page>
<figure confidence="0.992328328571428">
1
�
�
�
�
�
�
�
� �
Score�����r, (SAI SR&apos;) _
( 1
� P(Or I M log &apos; P(O � I SA) mrp &apos; &apos;
2 P(Bi I SA)+ 2 P(Bi I SR, )
�
where &apos;,&apos;)
�(��$� I ��) and �(�� $� I �� are estimated as
follows:
P(B�$S ISA) = aA,i + FUP(&amp;quot;i I ()
aA,0 +A&apos;
((&amp;quot;i,SA) + PP(&amp;quot;i I ()
)
� +p
�
�
,
�
�
l
((&amp;quot;i,SA)
Z
&amp;quot;,
�
�
�
�
� �
�
�
l
&apos;
2
�� ��
( 1
P(Or I sl&amp;quot;) log PW,I SR&apos;)
2 P(Or I SA)+ 2P( ���I SR � �
�
�
�
�
� �
+
where &apos;,&apos;)
�(�� �� I ��) and �(�� �� I �� are estimated as
follows:
aA,i
P(Or ISA) _
aA,0
_
((&amp;quot;i ,SA)
ZCI
r /Wi,SA) ,
&amp;quot;,
&apos;,&apos;) P(Or I SR
aR,i
aR,0
� ((&amp;quot;i,SR&apos;)
((&amp;quot;i,SR&apos;)
&apos;,&apos;)
((&amp;quot;�,��) and ((&amp;quot;�,�� are the counts of word &amp;quot;i in
&apos;,&apos;
</figure>
<bodyText confidence="0.9532845">
system summary �� and reference summary ��
respectively. When multiple reference summaries
</bodyText>
<equation confidence="0.915328454545454">
are used, &apos;,�)
�(�� �� I �� is estimated as follows:
&amp;quot;i((� &apos;,L
P(or I SRL) = aRi
a,1 L
,S~,~
�, � )
i
z ((&amp;quot;�,��,�
�, � )
�
</equation>
<subsectionHeader confidence="0.9991565">
3.2 Jensen-Shannon Divergence with
Smoothing (JSDS)
</subsectionHeader>
<bodyText confidence="0.9999818">
To examine the effect of smoothing when we
compute summary score using equation (22), we
apply Bayes-smoothing as shown in equation (&apos;5) .
Using a unigram model and single reference sum-
mary, we rewrite equation (22) as follows:
</bodyText>
<equation confidence="0.915868">
score=s, A I SR&apos;) _
( 1
� �(�� $� I ��)
�(�� $� I )log �
2P(B�$SISA)+2&apos;P(B�$S SR,&apos;)
�
( 1
P(Bi I SR, )log
$S P(B�$S SR,&apos; )
�2WSPISA)+2&apos;P(B�$S SR&apos;) � �
�
�
&apos;,&apos;) � ��,� � ��(&amp;quot;� I ()
�(�� $� I ��
aR,0 + P
((&amp;quot;i,SR&apos;)+ PP(&amp;quot;i I ()
� )
� � E ((&amp;quot;i,SR,&apos;) � +,u
&amp;quot;, �
&apos;,&apos;)
((&amp;quot;�,��) and ((&amp;quot;�,�� are the counts of word &amp;quot;i in
&apos;,&apos;
</equation>
<bodyText confidence="0.9474394">
system summary �� and reference summary ��
respectively. The Bayes-smoothing probability or
Bayesian prior p(&amp;quot;i I () is estimated from a general
English corpus instead of the topic collection as we
described in section 2 .&apos; . In our experiments, we
used TREC AP88-90 collection that contained
more than 200,000 news articles. When multiple
reference summaries are used, &apos;,�)
�(�� $� I �� is esti-
mated as follows:
</bodyText>
<equation confidence="0.982528">
&apos;,L /
p(B�$S SR L) = aR,i + Pp(wi I ()
~~,0
&apos;,� +P
1 �/
+ PP(Wi I ()
�
</equation>
<bodyText confidence="0.997937666666667">
The value of �could be determined empirically.
In this experiment we set p to 2,000 following
Zhai and Lafferty (2004) .
</bodyText>
<subsectionHeader confidence="0.998991">
3.3 Kullback-Leibler Divergence with
Smoothing (KLDS)
</subsectionHeader>
<bodyText confidence="0.921756833333333">
To compare the performance of .ISD and JSDS
scoring methods with other alternative distance
measure, we also compute summary scores using
KL divergence with Bayes-smoothing as follows:
�������������
� (&apos;,L / I�I / B~$S SA)
SA I SR) = _E p(0�1 I SA)loglP(B$S SRL)
�� $� 1
)
The Bayes-smoothing factor y is also set to 2,000
and e�$s is estimated by the same way that we
compute JSDS.
</bodyText>
<figure confidence="0.996654431818182">
�
E
E ((&amp;quot;i,SR,i �, �)
�
1
� � �
�
�
� �
&amp;quot;,
Z
&amp;quot;,
�
z
&amp;quot;,
�
�
�
�
E �
�
�
�
� �
&apos;
2
�� $�
�
�
� �
+
1
�
�
�
�
�
�
�
� �
E ((&amp;quot;i,SR,i �, �)
i
I�
l
</figure>
<page confidence="0.999028">
467
</page>
<tableCaption confidence="0.990141125">
Table 1 . DUC 2002 single (SD) and multi-
document summarization (MD) tasks&apos; Pearson&apos;s
(P) and Spearman&apos;s (S) correlations of automatic
measures (JSD, JSDS, KLDS, and LLS) using
single (SR) or multiple (MR) reference
summaries. (Unigram: bag-of-unigram model,
Bigram: bag-of-bigram model, and Trigram: bag-
of-trigram model)
</tableCaption>
<subsectionHeader confidence="0.994886">
3.4 Log Likelihood with Smoothing (LLS)
</subsectionHeader>
<bodyText confidence="0.999973166666667">
As a baseline measure, we also compute the log
likelihood score of an automatic summary given a
reference summary or a set of reference summaries
as follows:
where )SA) is the length of SA and p(g�$s I SRL) is es-
timated as before.
</bodyText>
<sectionHeader confidence="0.999825" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999637285714286">
Table 1 shows the results of all runs. According
to Table 1, automatic evaluation measure based on
Jensen-Shannon Divergence without Bayes-
smoothing (.ISD) performed the best among all
measures. Among them, .ISD over the bag-of-
unigram model achieved the best results in the sin-
gle document summarization task (P-SD-MR:
0 .97, S-SD-MR: 0 .91); while the bag-of-bigram
model achieved the best results in the multiple
document summarization task (P-MD-MR: 0 .96,
S-MD-MR: 0 .94) . Although the bag-of-bigram
model did not perform as well as the bag-of-
unigram model in the single document summariza-
tion task, its Pearson (SD-MR: 0 .94) and Spearman
</bodyText>
<tableCaption confidence="0.9907708">
Table 2 . DUC 2002 single (SD) and multi-
document (MD) summarization tasks&apos; Pear-
son&apos;s (P) and Spearman&apos;s (S) correlations of
automatic measures (ROUGE1-4) using mul-
tiple (MR) reference summaries.
</tableCaption>
<bodyText confidence="0.998338">
(SD-MR: 0 .90) correlation values were still over
90% regardless of single or multiple references
were used.
We also observed that using multiple references
outperformed using only single reference. This is
reasonable since we expect to estimate models bet-
ter when more reference summaries are available.
Smoothed measures did not perform well. This is
not a surprise due to the nature of summarization
evaluation. Intuitively, only information presented
in system and reference summaries should be con-
sidered for evaluation.
The .ISD-based measure was also compared fa-
vorably to ROUGE in the multiple document
summarization task as shown in Table 2 . In par-
ticular, the .ISD-based measure over bag-of-bigram
model using multiple references achieved much
better results in both Pearson&apos;s and Spearman&apos;s
correlations than all versions of ROUGE. For sin-
gle document summarization task, the .ISD-based
measure still achieved high correlations (90%+)
though it was not as high as ROUGE2, 3, and 4 .
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999645166666667">
The approach described in this paper is most
similar to the Bayesian extension in information
retrieval (IR) work by Zaragoza et al . (2003) . In
their work, query likelihood model was presented
as Bayesian inference. Other earlier language
modeling (Rosenfeld 2002) work in information
retrieval, especially the idea of modeling a docu-
ment using bag-of-word unigram model, also in-
spire this work (Berger and Lafferty 1999, Lafferty
and Zhai 2001
Statistical language models such as document lan-
guage model (Ponte and Croft 1998, Zhai and
Lafferty 2004), relevance-based language models
(Lavrenko and Croft 2001), and dependency-based
language models (Gao et al . 2004) have been ap-
plied successfully in information retrieval. It has
also been applied to topic detection and tracking
(Lavrenko et al . 2002, Larkey et al . 2004) . Ex-
</bodyText>
<figure confidence="0.925803545454546">
]SD
Trigram
]SD
Unigram
P S P
]SD ]SDS KLDS LLS
Bigram
P S P S P S P S
MD
MD
MD
</figure>
<table confidence="0.911368533333333">
SD
SD
SD
SR 0.97 0.91
SR 0.80 0.83
SR 0.92 0.90
SR 0.91 0.88 -0.17 -0.19
SR 0.92 0.90
SR 0.87 0.82 -0.39 -0.33 -0.11 -0.10
MR 0.97 0.91
MR 0.88 0.89
MR 0.94 0.90
MR 0.96 0.94
MR 0.94 0.90
MR 0.93 0.89 -0.30 -0.26 -0.11 -0.10
</table>
<figure confidence="0.990242666666667">
P S P S P S P S
0.61
0.62
0.44
0.76
0.64
0.64
0.17
0.68
0.68
]SDS KLDS
]SDS KLDS LLS
0.25
0.65
0.64
0.81
0.60 0.50 0.20 -0.81 0.05
0.62
0.36
0.44 0.53 0.11 -0.72 0.03
0.58
S
0.59
0.61
0.34
0.61
0.53
0.01
0.12
0.55
P
0.23 -0.54 0.16
0.25 -0.60 0.11
0.54
0.71
0.26 -0.80 0.06
0.14
0.22
0.20 -0.71 0.01
S P S
0.21 0.36
0.47 0.60
0.82 0.87
0.85 0.89
0.50 0.54
0.54 0.54
LLS
Score
��
(SA �SR1,L) = log ~(g~
F $� I SRL)
ISA I
,
ROUGE-4
SD 0.99 0.84
1.00 0.96 1.00 0.98
1.00 0.99
MR
MD 0.70 0.59
0.89 0.84 0.92 0.85
0.90 0.78
ROUGE ROUGE-1 ROUGE-2 ROUGE-3
P S P S P S P S
</figure>
<page confidence="0.999019">
468
</page>
<bodyText confidence="0.999905384615385">
tended models also have been developed to deal
with vocabulary mismatch and query expansion
problems (Berger and Lafferty 1999, Hofmann
1999, Lafferty and Zhai 2001) . However, it has not
been applied in automatic evaluation of summari-
zation . Hori et al . (2004) also considered using
&amp;quot;posterior probability&amp;quot; derived from consensus
among human summaries as weighting factor to
improve evaluations of speech summarization. But
their notion of &amp;quot;posterior probability&amp;quot; was not true
probability and was not presented as an integral
part of the Bayesian inference framework as we
have described in this paper.
</bodyText>
<sectionHeader confidence="0.998688" genericHeader="evaluation">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.98756778125">
The research proposed in this paper aims at pro-
viding a pilot study of applying information-
theoretic measures in automatic evaluation of
summaries. With the initial success of this study,
we would like to: (1) verify the results with other
set of data, for example, DUC 2003 data, (2) tune
the Bayesian smoothing parameter p to further ex-
amine the effect of smoothing, (3) develop better
content generation model and (4) add synonym and
paraphrase matching capability in the future . To
address (3), for example, we would like to explore
mutual information-based dependency language
modeling as proposed by Gao et al . (2004) .
For (4), manual evaluation methods recently pro-
posed separately by van Halteren and Teufel
(2003), the factoid method, and Nenkova and Pas-
souneau (2004), the pyramid method, tried to take
advantage of the availability of multiple refer-
ences. Both methods assume that the more impor-
tant a piece of information is, the more reference
summaries it appears in . These manual evaluation
methods can identify semantic equivalents. For
example, a summary content unit (SCU) &amp;quot;The
diamonds were replaced by fake replicas8&amp;quot; created
as defined in Nenkova and Passouneau (2004)
from the following four contributing clauses (1a —
d):
1 . Authorities, responding to a tip, [switched the
diamonds with fakes]1a and were waiting inside
the building dressed as cleaners when the
thieves burst in with a bulldozer and sledge-
hammers.
</bodyText>
<footnote confidence="0.716443">
8 Example is taken from Multilingual Summarization Evalua-
tion 2005 (MSE2005), topic number 33003 .
</footnote>
<bodyText confidence="0.679299857142857">
2 . However, authorities were tipped off and
[switched the diamonds with fakes] 1b .
3 . They disguised themselves as cleaners at the
Millennium Dome, [switched the diamonds
with worthless glass] 1c, and waited for the rob-
bers, who planned to get away in a speedboat
down the Thames River.
</bodyText>
<sectionHeader confidence="0.278059" genericHeader="conclusions">
4 . [The diamonds had been swapped with glass
</sectionHeader>
<subsectionHeader confidence="0.193598">
replicas] 1d.
</subsectionHeader>
<bodyText confidence="0.999197333333333">
Contributors (1a — d) from 4 reference summaries
to the SCU are underlined. The manual pyramid
method can identify these semantic equivalents. It
is obvious that automatic evaluation methods rely-
ing on strict n-gram or lexical matching would
only find two out of four possible matches, i .e .
&amp;quot;switched the diamonds with fakes&amp;quot; from (1a) and
(1b) while leave &amp;quot;switched the diamonds with
worthless glass&amp;quot; (1c) and &amp;quot;The diamonds had been
swapped with glass replicas&amp;quot; (1d) unmatched. Al-
lowing near synonyms such as fakes, worthless
glass, and glass replicas to match might help, but
how to acquire these equivalents and how to assign
appropriate weights to reflect their subtle differ-
ences remain open questions. To find semantic
equivalents automatically, we would like to try
query expansion techniques (Hofmann 1999,
Lafferty and Zhai 2001, Bai et al . 2005, Cao et al .
2005) commonly used in IR . Proper query expan-
sion boosts IR system performance. We suspect
that these techniques would help a little but we
probably would need to develop much better para-
phrase expansion and matching techniques to see
significant boost in overall performance.
</bodyText>
<sectionHeader confidence="0.998233" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9954465">
Part of this work was conducted while the first au-
thor visited Microsoft Research Asia (MSRA) in
the summer of 2005 . He would like to thank Ming
Zhou, Hsiao-Wuen Hon, and other staffs at MSRA
for providing an excellent research environment
and exciting intellectual exchanges during his visit.
</bodyText>
<sectionHeader confidence="0.993797" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.998862">
Bai, Jing, Dawei Song, Peter Bruza, Jian-Yun Nie, and
Guihong Cao. 2005 . Query Expansion Using Term
Relationships in Language Models for Information
Retrieval. Proceedings of International Conference
on Information and Knowledge Management (CIKM)
2005 . October 31 — November 5, 2005, Bremen,
Germany.
</reference>
<page confidence="0.995226">
469
</page>
<reference confidence="0.999671714285714">
Berger, Adam and John Lafftery . 1999 . Information
Retrieval as Statistical Translation. Proceedings of
ACM-SIGIR 1999 . August 15-19, 1999, Berkeley,
CA, USA.
Cao, Guihong, Jian-Yun Nie, and Jing Bai. 2005 . Inte-
grating Word Relationships into Language Models.
Proceedings of SIGIR 2005 . August 15-19, 2005,
Salvador, Brazil.
Chen, Stanley and Joshua Goodman. 1996 . An Empiri-
cal Study of Smoothing Techniques for Language
Modeling. Proceedings of 34th Annual Meeting on
Association for Computational Linguistics, page 310-
318, June 23-28, Santa Cruz, California, USA.
Dagan, Ido, Lillian Lee, and Fernado C . N . Pereira.
1999 . Similarity-Based Models of Word Cooccur-
rence Probability. Machine Learning. Vol 34, page
43-69, 1999 .
Gao, Jianfeng, Jian-Yun Nie, Guangyuan Wu, and Gui-
hong Cao. 2004 . Dependence Language Model for
Information Retrieval. Proceedings of SIGIR 2004 .
July 25-29, 2004, Sheffield, UK.
Gelman, Andrew, John B . Carlin, Hal S . Stern, and
Donald B . Rubin. 2003 . Bayesian Data Analysis. 2nd
Edition. Chapman &amp; Hall/CRC.
Hofmann, Thomas. 1999 . Probabilistic Latent Semantic
Indexing. Proceedings of ACM-SIGIR 1999 . August
15-19, 1999, Berkeley, CA, USA.
Hori, Chiori, Tsutomu Hirao, and Hideki Isozaki. 2004 .
Evaluation Measures Considering Sentence Concate-
nation for Automatic Summarization by Sentence or
Word Extraction. Proceedings of Workshop on Text
Summarization Branches Out. July 25, 2004, Barce-
lona, Spain.
Kraaij, Wessel, Martijn Spitters, and Martine van der
Heijden. 2001 . Combining a Mixture Language
Model and Naive Bayes for Multi-Document Sum-
marisation. Proceedings of DUC 2001 .
Lafferty, John and Chiangxiang Zhai. 2001 . Document
Language Models, Query Models, and Risk Minimi-
zation for Information Retrieval. Proceedings of
ACM-SIGIR 2001 . September 9-13, 2001, New Or-
leans, LA, USA.
Larkey, Leah S ., Fangfang Feng, Margaret Connell, and
Victor Larvrenko. 2004 . Language-specific Models
in Multilingual Topic Tracking. Proceedings of
ACM-SIGIR 2004 . July 25-29, 2004 . Sheffield, UK.
Lavrenko, Victor, James Allan, Edward DeGuzman,
Daniel LaFlamme, Veera Pollard, and Stephen Tho-
mas. 2002 . Relevance Models for Topic Detection
and Tracking. Proceedings of HLT 2002 . March 24-
27, 2002, San Diego, CA, USA.
Lavrenko, Victor and W . Bruce Croft. 2001 . Relevance-
Based Language Models. Proceedings of ACM-
SIGIR 2001 . September 9-13, 2001, New Orleans,
LA, USA.
Lin, Chin-Yew and Eduard Hovy. 2003 . Automatic
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. Proceedings of HLT-NAACL-
2003. May 27-June 1, 2003, Edmonton, Canada.
Lin, Chin-Yew. 2004 . ROUGE: a Package for Auto-
matic Evaluation of Summaries. Proceedings of
Workshop on Text Summarization 2004 . July 21-26,
2004, Barcelona, Spain.
Lin, Jianhua. 1991 . Divergence Measures Based on the
Shannon Entropy. IEEE Transactions on Information
Theory, 37(1), page 145-151, 1991 .
Nenkova, Ani and Rebecca Passonneau. 2004 . Evaluat-
ing Content Selection in Summarization: the Pyramid
Method. Proceedings of NAACL-HLT 2004 . May 2-
7, 2004, Boston, MA, USA.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002 . BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002), Philadelphia,
USA, July 2002, page 311-318 .
Ponte, Jay M . and W . Bruce Croft. 1998 . A language
modeling approach to information retrieval. Proceed-
ings ofACM-SIGIR 1998, pages 275-281 . August 24-
28, 1998, Melbourne, Australia.
Rosenfeld, Ronald. 2002 . Two Decades of Statistical
Language Modeling: Where do We Go from Here?
Proceedings ofIEEE.
Van Halteren, Hans and Simone Teufel. 2003 . Examin-
ing the Consensus between Human Summaries: Ini-
tial Experiments with Factoid Analysis. Proceedings
of Workshop on Text Summarization 2003 . May 27-
June 1, 2003, Edmonton, Canada.
Zaragoza, Hugo, Djoerd Hiemstra, and Michael Tip-
ping. 2003 . Bayesian Extension to the Language
Model for Ad Hoc Information Retrieval. Proceed-
ings of ACM-SIGIR 2003 . July 28-August 1, 2003,
Toronto, Canada.
Zhai, Chengxiang and John Lafferty. 2004 . A Study of
Smoothing Methods for Language Models Applied to
Information Retrieval. ACM Transactions on Infor-
mation Systems, Vol 22, No . 2, April 2004, pages
179-214 .
</reference>
<page confidence="0.998192">
470
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.539233">
<title confidence="0.999509">An Information-Theoretic Approach Automatic Evaluation of Summaries</title>
<author confidence="0.992293">Guihong Cao Jianfeng</author>
<author confidence="0.992293">Jian-Yun Nie</author>
<affiliation confidence="0.8158675">Sciences de One Microsoft University of Southern Montr6al, Redmond, WA</affiliation>
<author confidence="0.865643">Marina del Rey</author>
<author confidence="0.865643">CA caogui</author>
<author confidence="0.865643">nieiro umontreal ca jfgaomicrosoft com</author>
<email confidence="0.997137">cyl@isi.edu</email>
<abstract confidence="0.99972575">Until recently there are no common, convenient, and repeatable evaluation methods that could be easily applied to support fast turn-around development of automatic text summarization systems. In this paper, we introduce an informationtheoretic approach to automatic evaluation of summaries based on the Jensen-Shannon divergence of distributions between an automatic summary and a set of reference summaries. Several variants of the approach are also considered and compared. The results indicate that JS divergencebased evaluation method achieves comparable performance with the common automatic evaluation method ROUGE in single documents summarization task; while achieves better performance than ROUGE in multiple document summarization task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jing Bai</author>
<author>Dawei Song</author>
<author>Peter Bruza</author>
<author>Jian-Yun Nie</author>
<author>Guihong Cao</author>
</authors>
<title>Query Expansion Using Term Relationships in Language Models for Information Retrieval.</title>
<date>2005</date>
<booktitle>Proceedings of International Conference on Information and Knowledge Management (CIKM) 2005 . October 31 —</booktitle>
<location>Bremen, Germany.</location>
<marker>Bai, Song, Bruza, Nie, Cao, 2005</marker>
<rawString>Bai, Jing, Dawei Song, Peter Bruza, Jian-Yun Nie, and Guihong Cao. 2005 . Query Expansion Using Term Relationships in Language Models for Information Retrieval. Proceedings of International Conference on Information and Knowledge Management (CIKM) 2005 . October 31 — November 5, 2005, Bremen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>John Lafftery</author>
</authors>
<title>Information Retrieval as Statistical Translation.</title>
<date>1999</date>
<booktitle>Proceedings of ACM-SIGIR</booktitle>
<pages>.</pages>
<location>Berkeley, CA, USA.</location>
<marker>Berger, Lafftery, 1999</marker>
<rawString>Berger, Adam and John Lafftery . 1999 . Information Retrieval as Statistical Translation. Proceedings of ACM-SIGIR 1999 . August 15-19, 1999, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihong Cao</author>
<author>Jian-Yun Nie</author>
<author>Jing Bai</author>
</authors>
<title>Integrating Word Relationships into Language Models.</title>
<date>2005</date>
<booktitle>Proceedings of SIGIR</booktitle>
<pages>.</pages>
<location>Salvador, Brazil.</location>
<marker>Cao, Nie, Bai, 2005</marker>
<rawString>Cao, Guihong, Jian-Yun Nie, and Jing Bai. 2005 . Integrating Word Relationships into Language Models. Proceedings of SIGIR 2005 . August 15-19, 2005, Salvador, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1996</date>
<booktitle>Proceedings of 34th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<location>Santa Cruz, California, USA.</location>
<contexts>
<context position="8431" citStr="Chen and Goodman (1996)" startWordPosition="1501" endWordPosition="1504"> � �� ap and( the posterior distribution (8) can be written as: pIBA SA) &apos;&amp;quot; p(BA SA) = Zap +m1m1(OA,i )a�. (12) The problem with using maximum likelihood estimation is when ai equal to zero. If zero occurrence happens for word i, then its maximum likelihood estimation, B� , would be zero and the whole posterior distribution would be zero. To tackle this problem, we need to redistribute some probability mass to zero occurrence events or unseen word events. The process of redistribution is called smoot!in in the language modeling literatures. For an in-depth discussion of this topic, please see Chen and Goodman (1996) . By choosing different value for ai , we could derive different smoothing methods as discussed in Zaragoza et al . (2pp3) . For example, we could estimate ai using topic collection frequency by setting ai = ,up(&amp;quot;i I #) + 1, where p is a scaling factor and p(&amp;quot;i I #) is the probability of word i occurring in topic #. This is called Bayes-smoothing and has been used in language modeling for information retrieval (Zhai and Lafferty 2pp4) . The Bayessmoothing can be written as: $ OA = ai +,up(&amp;quot;i I #) A, ap +p Using Equation (13), Equation (8)becomes: P(RA I SA) &apos;&amp;quot; P(BA$S I SA) — Za,+p+Jj&amp;quot;(OA,� $S</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Chen, Stanley and Joshua Goodman. 1996 . An Empirical Study of Smoothing Techniques for Language Modeling. Proceedings of 34th Annual Meeting on Association for Computational Linguistics, page 310-318, June 23-28, Santa Cruz, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>C Fernado</author>
</authors>
<title>Similarity-Based Models of Word Cooccurrence Probability.</title>
<date>1999</date>
<journal>Machine Learning. Vol</journal>
<volume>34</volume>
<pages>43--69</pages>
<marker>Dagan, Lee, Fernado, 1999</marker>
<rawString>Dagan, Ido, Lillian Lee, and Fernado C . N . Pereira. 1999 . Similarity-Based Models of Word Cooccurrence Probability. Machine Learning. Vol 34, page 43-69, 1999 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
<author>Guangyuan Wu</author>
<author>Guihong Cao</author>
</authors>
<title>Dependence Language Model for Information Retrieval.</title>
<date>2004</date>
<booktitle>Proceedings of SIGIR</booktitle>
<pages>.</pages>
<location>Sheffield, UK.</location>
<marker>Gao, Nie, Wu, Cao, 2004</marker>
<rawString>Gao, Jianfeng, Jian-Yun Nie, Guangyuan Wu, and Guihong Cao. 2004 . Dependence Language Model for Information Retrieval. Proceedings of SIGIR 2004 . July 25-29, 2004, Sheffield, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal S Carlin</author>
</authors>
<title>Bayesian Data Analysis. 2nd Edition.</title>
<date>2003</date>
<publisher>Chapman &amp; Hall/CRC.</publisher>
<marker>Carlin, 2003</marker>
<rawString>Gelman, Andrew, John B . Carlin, Hal S . Stern, and Donald B . Rubin. 2003 . Bayesian Data Analysis. 2nd Edition. Chapman &amp; Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>Proceedings of ACM-SIGIR</booktitle>
<pages>.</pages>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="21573" citStr="Hofmann 1999" startWordPosition="3920" endWordPosition="3921">64 0.81 0.60 0.50 0.20 -0.81 0.05 0.62 0.36 0.44 0.53 0.11 -0.72 0.03 0.58 S 0.59 0.61 0.34 0.61 0.53 0.01 0.12 0.55 P 0.23 -0.54 0.16 0.25 -0.60 0.11 0.54 0.71 0.26 -0.80 0.06 0.14 0.22 0.20 -0.71 0.01 S P S 0.21 0.36 0.47 0.60 0.82 0.87 0.85 0.89 0.50 0.54 0.54 0.54 LLS Score �� (SA �SR1,L) = log ~(g~ F $� I SRL) ISA I , ROUGE-4 SD 0.99 0.84 1.00 0.96 1.00 0.98 1.00 0.99 MR MD 0.70 0.59 0.89 0.84 0.92 0.85 0.90 0.78 ROUGE ROUGE-1 ROUGE-2 ROUGE-3 P S P S P S P S 468 tended models also have been developed to deal with vocabulary mismatch and query expansion problems (Berger and Lafferty 1999, Hofmann 1999, Lafferty and Zhai 2001) . However, it has not been applied in automatic evaluation of summarization . Hori et al . (2004) also considered using &amp;quot;posterior probability&amp;quot; derived from consensus among human summaries as weighting factor to improve evaluations of speech summarization. But their notion of &amp;quot;posterior probability&amp;quot; was not true probability and was not presented as an integral part of the Bayesian inference framework as we have described in this paper. 6 Conclusions and Future Work The research proposed in this paper aims at providing a pilot study of applying informationtheoretic mea</context>
<context position="24776" citStr="Hofmann 1999" startWordPosition="4440" endWordPosition="4441">trict n-gram or lexical matching would only find two out of four possible matches, i .e . &amp;quot;switched the diamonds with fakes&amp;quot; from (1a) and (1b) while leave &amp;quot;switched the diamonds with worthless glass&amp;quot; (1c) and &amp;quot;The diamonds had been swapped with glass replicas&amp;quot; (1d) unmatched. Allowing near synonyms such as fakes, worthless glass, and glass replicas to match might help, but how to acquire these equivalents and how to assign appropriate weights to reflect their subtle differences remain open questions. To find semantic equivalents automatically, we would like to try query expansion techniques (Hofmann 1999, Lafferty and Zhai 2001, Bai et al . 2005, Cao et al . 2005) commonly used in IR . Proper query expansion boosts IR system performance. We suspect that these techniques would help a little but we probably would need to develop much better paraphrase expansion and matching techniques to see significant boost in overall performance. 7 Acknowledgement Part of this work was conducted while the first author visited Microsoft Research Asia (MSRA) in the summer of 2005 . He would like to thank Ming Zhou, Hsiao-Wuen Hon, and other staffs at MSRA for providing an excellent research environment and exc</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, Thomas. 1999 . Probabilistic Latent Semantic Indexing. Proceedings of ACM-SIGIR 1999 . August 15-19, 1999, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiori Hori</author>
<author>Tsutomu Hirao</author>
<author>Hideki Isozaki</author>
</authors>
<title>Evaluation Measures Considering Sentence Concatenation for Automatic Summarization by Sentence or Word Extraction.</title>
<date>2004</date>
<booktitle>Proceedings of Workshop on Text Summarization Branches Out.</booktitle>
<location>Barcelona,</location>
<marker>Hori, Hirao, Isozaki, 2004</marker>
<rawString>Hori, Chiori, Tsutomu Hirao, and Hideki Isozaki. 2004 . Evaluation Measures Considering Sentence Concatenation for Automatic Summarization by Sentence or Word Extraction. Proceedings of Workshop on Text Summarization Branches Out. July 25, 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wessel Kraaij</author>
<author>Martijn Spitters</author>
<author>Martine van der Heijden</author>
</authors>
<title>Combining a Mixture Language Model and Naive Bayes for Multi-Document Summarisation.</title>
<date>2001</date>
<booktitle>Proceedings of DUC</booktitle>
<pages>.</pages>
<marker>Kraaij, Spitters, van der Heijden, 2001</marker>
<rawString>Kraaij, Wessel, Martijn Spitters, and Martine van der Heijden. 2001 . Combining a Mixture Language Model and Naive Bayes for Multi-Document Summarisation. Proceedings of DUC 2001 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Chiangxiang Zhai</author>
</authors>
<title>Document Language Models, Query Models, and Risk Minimization for Information Retrieval.</title>
<date>2001</date>
<booktitle>Proceedings of ACM-SIGIR</booktitle>
<pages>.</pages>
<location>New Orleans, LA, USA.</location>
<contexts>
<context position="20172" citStr="Lafferty and Zhai 2001" startWordPosition="3634" endWordPosition="3637">of ROUGE. For single document summarization task, the .ISD-based measure still achieved high correlations (90%+) though it was not as high as ROUGE2, 3, and 4 . 5 Related Work The approach described in this paper is most similar to the Bayesian extension in information retrieval (IR) work by Zaragoza et al . (2003) . In their work, query likelihood model was presented as Bayesian inference. Other earlier language modeling (Rosenfeld 2002) work in information retrieval, especially the idea of modeling a document using bag-of-word unigram model, also inspire this work (Berger and Lafferty 1999, Lafferty and Zhai 2001 Statistical language models such as document language model (Ponte and Croft 1998, Zhai and Lafferty 2004), relevance-based language models (Lavrenko and Croft 2001), and dependency-based language models (Gao et al . 2004) have been applied successfully in information retrieval. It has also been applied to topic detection and tracking (Lavrenko et al . 2002, Larkey et al . 2004) . Ex]SD Trigram ]SD Unigram P S P ]SD ]SDS KLDS LLS Bigram P S P S P S P S MD MD MD SD SD SD SR 0.97 0.91 SR 0.80 0.83 SR 0.92 0.90 SR 0.91 0.88 -0.17 -0.19 SR 0.92 0.90 SR 0.87 0.82 -0.39 -0.33 -0.11 -0.10 MR 0.97 0.</context>
<context position="21598" citStr="Lafferty and Zhai 2001" startWordPosition="3922" endWordPosition="3925">.50 0.20 -0.81 0.05 0.62 0.36 0.44 0.53 0.11 -0.72 0.03 0.58 S 0.59 0.61 0.34 0.61 0.53 0.01 0.12 0.55 P 0.23 -0.54 0.16 0.25 -0.60 0.11 0.54 0.71 0.26 -0.80 0.06 0.14 0.22 0.20 -0.71 0.01 S P S 0.21 0.36 0.47 0.60 0.82 0.87 0.85 0.89 0.50 0.54 0.54 0.54 LLS Score �� (SA �SR1,L) = log ~(g~ F $� I SRL) ISA I , ROUGE-4 SD 0.99 0.84 1.00 0.96 1.00 0.98 1.00 0.99 MR MD 0.70 0.59 0.89 0.84 0.92 0.85 0.90 0.78 ROUGE ROUGE-1 ROUGE-2 ROUGE-3 P S P S P S P S 468 tended models also have been developed to deal with vocabulary mismatch and query expansion problems (Berger and Lafferty 1999, Hofmann 1999, Lafferty and Zhai 2001) . However, it has not been applied in automatic evaluation of summarization . Hori et al . (2004) also considered using &amp;quot;posterior probability&amp;quot; derived from consensus among human summaries as weighting factor to improve evaluations of speech summarization. But their notion of &amp;quot;posterior probability&amp;quot; was not true probability and was not presented as an integral part of the Bayesian inference framework as we have described in this paper. 6 Conclusions and Future Work The research proposed in this paper aims at providing a pilot study of applying informationtheoretic measures in automatic evalua</context>
<context position="24800" citStr="Lafferty and Zhai 2001" startWordPosition="4442" endWordPosition="4445">r lexical matching would only find two out of four possible matches, i .e . &amp;quot;switched the diamonds with fakes&amp;quot; from (1a) and (1b) while leave &amp;quot;switched the diamonds with worthless glass&amp;quot; (1c) and &amp;quot;The diamonds had been swapped with glass replicas&amp;quot; (1d) unmatched. Allowing near synonyms such as fakes, worthless glass, and glass replicas to match might help, but how to acquire these equivalents and how to assign appropriate weights to reflect their subtle differences remain open questions. To find semantic equivalents automatically, we would like to try query expansion techniques (Hofmann 1999, Lafferty and Zhai 2001, Bai et al . 2005, Cao et al . 2005) commonly used in IR . Proper query expansion boosts IR system performance. We suspect that these techniques would help a little but we probably would need to develop much better paraphrase expansion and matching techniques to see significant boost in overall performance. 7 Acknowledgement Part of this work was conducted while the first author visited Microsoft Research Asia (MSRA) in the summer of 2005 . He would like to thank Ming Zhou, Hsiao-Wuen Hon, and other staffs at MSRA for providing an excellent research environment and exciting intellectual excha</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>Lafferty, John and Chiangxiang Zhai. 2001 . Document Language Models, Query Models, and Risk Minimization for Information Retrieval. Proceedings of ACM-SIGIR 2001 . September 9-13, 2001, New Orleans, LA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangfang Feng</author>
<author>Margaret Connell</author>
<author>Victor Larvrenko</author>
</authors>
<title>Language-specific Models in Multilingual Topic Tracking.</title>
<date>2004</date>
<booktitle>Proceedings of ACM-SIGIR</booktitle>
<pages>.</pages>
<publisher>Sheffield, UK.</publisher>
<marker>Feng, Connell, Larvrenko, 2004</marker>
<rawString>Larkey, Leah S ., Fangfang Feng, Margaret Connell, and Victor Larvrenko. 2004 . Language-specific Models in Multilingual Topic Tracking. Proceedings of ACM-SIGIR 2004 . July 25-29, 2004 . Sheffield, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>James Allan</author>
<author>Edward DeGuzman</author>
<author>Daniel LaFlamme</author>
<author>Veera Pollard</author>
<author>Stephen Thomas</author>
</authors>
<title>Relevance Models for Topic Detection and Tracking.</title>
<date>2002</date>
<booktitle>Proceedings of HLT</booktitle>
<pages>.</pages>
<location>San Diego, CA, USA.</location>
<marker>Lavrenko, Allan, DeGuzman, LaFlamme, Pollard, Thomas, 2002</marker>
<rawString>Lavrenko, Victor, James Allan, Edward DeGuzman, Daniel LaFlamme, Veera Pollard, and Stephen Thomas. 2002 . Relevance Models for Topic Detection and Tracking. Proceedings of HLT 2002 . March 24-27, 2002, San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W</author>
</authors>
<title>RelevanceBased Language Models.</title>
<date>2001</date>
<booktitle>Proceedings of ACMSIGIR</booktitle>
<pages>.</pages>
<location>New Orleans, LA, USA.</location>
<marker>Lavrenko, W, 2001</marker>
<rawString>Lavrenko, Victor and W . Bruce Croft. 2001 . RelevanceBased Language Models. Proceedings of ACMSIGIR 2001 . September 9-13, 2001, New Orleans, LA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram Cooccurrence Statistics.</title>
<date>2003</date>
<booktitle>Proceedings of HLT-NAACL2003. May 27-June 1,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1275" citStr="Lin and Hovy 2003" startWordPosition="171" endWordPosition="174"> to automatic evaluation of summaries based on the Jensen-Shannon divergence of distributions between an automatic summary and a set of reference summaries. Several variants of the approach are also considered and compared. The results indicate that JS divergencebased evaluation method achieves comparable performance with the common automatic evaluation method ROUGE in single documents summarization task; while achieves better performance than ROUGE in multiple document summarization task. 1 Introduction Most previous automatic evaluation methods in summarization use co-occurrence statistics (Lin and Hovy 2003) to measure the content overlap between an automatic summary and a set of reference summaries. Among them, ROUGE (Lin 2004) has been used by the annual summarization evaluation conference, Document Understanding Conference1 (DUC), sponsored by NIST since 2001 . Content and quality of a summary are the two main aspects of summarization measured in the past DUCs. Using a manual evaluation inter1 Please see: http://duc.nist.gov for more information face called SEE2, NIST assessors compared the content overlap between a system summary and a reference summary and assigned a coverage score to indica</context>
<context position="12496" citStr="Lin &amp; Hovy 2003" startWordPosition="2232" endWordPosition="2235">ummarization task (total 15 runs); while 8 sets of system summaries, 1 lead baseline, and 1 latest news baseline are included for the multi-document summarization task (total 12 runs). 4 Please refer to equation (5) All summaries are about 100 words5. Manually evaluation results in average coverage6 scores are also included in the DUC 2002 data set. The commonly used criterion of success to evaluate an automatic evaluation method is to compute the correlation between the ranking of systems according to human assigned scores and the ranking according to automatic scores (Papineni et al . 2002; Lin &amp; Hovy 2003) . We followed the same convention and computed Pearson&apos;s product moment correlation coefficient and Spearman&apos;s rank correlation coefficient as indicators of success. Besides evaluating the performance of the automatic evaluation measure based on JensenShannon (IS) divergence as defined in equation (2), we also compared it with measures based on KL-divergence and simple log likelihood. The effect of smoothing and the difference of using single and multiple reference summaries were also investigated. To examine the effect of using longer n-grams (n &gt; 1), we also used bag-of-bigram and bag-of-tr</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, Chin-Yew and Eduard Hovy. 2003 . Automatic Evaluation of Summaries Using N-gram Cooccurrence Statistics. Proceedings of HLT-NAACL2003. May 27-June 1, 2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>Proceedings of Workshop on Text Summarization</booktitle>
<pages>.</pages>
<location>Barcelona,</location>
<contexts>
<context position="1398" citStr="Lin 2004" startWordPosition="194" endWordPosition="195"> of reference summaries. Several variants of the approach are also considered and compared. The results indicate that JS divergencebased evaluation method achieves comparable performance with the common automatic evaluation method ROUGE in single documents summarization task; while achieves better performance than ROUGE in multiple document summarization task. 1 Introduction Most previous automatic evaluation methods in summarization use co-occurrence statistics (Lin and Hovy 2003) to measure the content overlap between an automatic summary and a set of reference summaries. Among them, ROUGE (Lin 2004) has been used by the annual summarization evaluation conference, Document Understanding Conference1 (DUC), sponsored by NIST since 2001 . Content and quality of a summary are the two main aspects of summarization measured in the past DUCs. Using a manual evaluation inter1 Please see: http://duc.nist.gov for more information face called SEE2, NIST assessors compared the content overlap between a system summary and a reference summary and assigned a coverage score to indicate the extent of the overlap between system and reference summaries. The overall system content coverage score was then the</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Lin, Chin-Yew. 2004 . ROUGE: a Package for Automatic Evaluation of Summaries. Proceedings of Workshop on Text Summarization 2004 . July 21-26, 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence Measures Based on the Shannon Entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>145--151</pages>
<contexts>
<context position="4164" citStr="Lin 1991" startWordPosition="649" endWordPosition="650">2006. c�2006 Association for Computational Linguistics sume every system summary is generated from a probabilistic distribution with parameters specified by o,. Therefore, a good summarizer should have its oA very close to oR and the process of summary evaluation could be viewed as a task of estimating the distance between oA and oR . For example, if we use Kullback-Leibler (KL) divergence as the distance function, then the summary evaluation task could be viewed as finding the KL divergence between oA and oR . However, KL divergence is unspecified when a value is defined in oR but not in oA (Lin 1991, Dagan et al . 1999) . Usually smoothing has to be applied to address this missing data problem (unseen word in this case). Another problem is that KL divergence is not symmetric, i .e . KL(oR IIBA) # KL((BA IIBR), except when oR = oA. This is counter-intuitive in our application scenario. We therefore use generalized Jensen-Shannon (JS) divergence proposed by Lin (1991) . The JS divergence can be written as follows: 4 (P1, P2,..., P.) = � � � � � �� �� � � � ���(�� ) � , � � � ��1 ��1 where pi is a probability distribution with weight � ��, � , �� � 1 and H() is Shannon entropy Since the Jen</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Lin, Jianhua. 1991 . Divergence Measures Based on the Shannon Entropy. IEEE Transactions on Information Theory, 37(1), page 145-151, 1991 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating Content Selection in Summarization: the Pyramid Method.</title>
<date>2004</date>
<booktitle>Proceedings of NAACL-HLT</booktitle>
<pages>.</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="11164" citStr="Nenkova and Passonneau (2004)" startWordPosition="2011" endWordPosition="2014">d Equation (18) is the total number of words in reference summary s,,; . The total number of words in the topic collection could be computed as follows: Equation (17) indicates that estimation of posterior distribution given multiple summaries is the same as estimation of posterior distribution given a single summary that contains all the reference summaries . This is reasonable since we assume a bagof-word unigram likelihood model for generating summaries4 . It also bodes well with the consensusoriented manual summarization evaluation approaches proposed by van Halteren and Teufel (2003) and Nenkova and Passonneau (2004) . With equations (8) and (17), the summary score of system summary, SA, can be computed using JensenShannon divergence from equation (3) as follows: SCOPe On,~(SA SRL) =-&apos;JS1/2(P(O&amp;quot; IS&amp;quot;) S&amp;quot;)OP(BR I SR1,L)), (20) where SRL is a shorthand for SR,1,...,SR,L . 3 Experimental Setup We used data from DUC 2002 100-word single and multi-document tasks as our testing corpus. DUC 2002 data includes 59 topic sets. Each topic set contains about 10 news article pertaining to some news topic, for example, topic D061 is about &amp;quot;Hurricane %i&amp;&apos;ert&amp;quot; . Two human written summaries per topic are provided as refere</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Nenkova, Ani and Rebecca Passonneau. 2004 . Evaluating Content Selection in Summarization: the Pyramid Method. Proceedings of NAACL-HLT 2004 . May 2-7, 2004, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA,</location>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002 . BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), Philadelphia, USA, July 2002, page 311-318 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>W</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>Proceedings ofACM-SIGIR</booktitle>
<pages>275--281</pages>
<location>Melbourne, Australia.</location>
<marker>W, 1998</marker>
<rawString>Ponte, Jay M . and W . Bruce Croft. 1998 . A language modeling approach to information retrieval. Proceedings ofACM-SIGIR 1998, pages 275-281 . August 24-28, 1998, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Two Decades of Statistical Language Modeling: Where do We Go from Here?</title>
<date>2002</date>
<booktitle>Proceedings ofIEEE.</booktitle>
<contexts>
<context position="19992" citStr="Rosenfeld 2002" startWordPosition="3607" endWordPosition="3608">ular, the .ISD-based measure over bag-of-bigram model using multiple references achieved much better results in both Pearson&apos;s and Spearman&apos;s correlations than all versions of ROUGE. For single document summarization task, the .ISD-based measure still achieved high correlations (90%+) though it was not as high as ROUGE2, 3, and 4 . 5 Related Work The approach described in this paper is most similar to the Bayesian extension in information retrieval (IR) work by Zaragoza et al . (2003) . In their work, query likelihood model was presented as Bayesian inference. Other earlier language modeling (Rosenfeld 2002) work in information retrieval, especially the idea of modeling a document using bag-of-word unigram model, also inspire this work (Berger and Lafferty 1999, Lafferty and Zhai 2001 Statistical language models such as document language model (Ponte and Croft 1998, Zhai and Lafferty 2004), relevance-based language models (Lavrenko and Croft 2001), and dependency-based language models (Gao et al . 2004) have been applied successfully in information retrieval. It has also been applied to topic detection and tracking (Lavrenko et al . 2002, Larkey et al . 2004) . Ex]SD Trigram ]SD Unigram P S P ]SD</context>
</contexts>
<marker>Rosenfeld, 2002</marker>
<rawString>Rosenfeld, Ronald. 2002 . Two Decades of Statistical Language Modeling: Where do We Go from Here? Proceedings ofIEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Van Halteren</author>
<author>Simone Teufel</author>
</authors>
<title>Examining the Consensus between Human Summaries: Initial Experiments with Factoid Analysis.</title>
<date>2003</date>
<booktitle>Proceedings of Workshop on Text Summarization</booktitle>
<volume>27</volume>
<pages>.</pages>
<location>Edmonton, Canada.</location>
<marker>Van Halteren, Teufel, 2003</marker>
<rawString>Van Halteren, Hans and Simone Teufel. 2003 . Examining the Consensus between Human Summaries: Initial Experiments with Factoid Analysis. Proceedings of Workshop on Text Summarization 2003 . May 27-June 1, 2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Zaragoza</author>
<author>Djoerd Hiemstra</author>
<author>Michael Tipping</author>
</authors>
<title>Bayesian Extension to the Language Model for Ad Hoc Information Retrieval.</title>
<date>2003</date>
<booktitle>Proceedings of ACM-SIGIR</booktitle>
<pages>.</pages>
<location>Toronto, Canada.</location>
<marker>Zaragoza, Hiemstra, Tipping, 2003</marker>
<rawString>Zaragoza, Hugo, Djoerd Hiemstra, and Michael Tipping. 2003 . Bayesian Extension to the Language Model for Ad Hoc Information Retrieval. Proceedings of ACM-SIGIR 2003 . July 28-August 1, 2003, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A Study of Smoothing Methods for Language Models Applied to Information Retrieval.</title>
<date>2004</date>
<journal>ACM Transactions on Information Systems, Vol</journal>
<volume>22</volume>
<pages>179--214</pages>
<contexts>
<context position="16689" citStr="Zhai and Lafferty (2004)" startWordPosition="3041" endWordPosition="3044">nts of word &amp;quot;i in &apos;,&apos; system summary �� and reference summary �� respectively. The Bayes-smoothing probability or Bayesian prior p(&amp;quot;i I () is estimated from a general English corpus instead of the topic collection as we described in section 2 .&apos; . In our experiments, we used TREC AP88-90 collection that contained more than 200,000 news articles. When multiple reference summaries are used, &apos;,�) �(�� $� I �� is estimated as follows: &apos;,L / p(B�$S SR L) = aR,i + Pp(wi I () ~~,0 &apos;,� +P 1 �/ + PP(Wi I () � The value of �could be determined empirically. In this experiment we set p to 2,000 following Zhai and Lafferty (2004) . 3.3 Kullback-Leibler Divergence with Smoothing (KLDS) To compare the performance of .ISD and JSDS scoring methods with other alternative distance measure, we also compute summary scores using KL divergence with Bayes-smoothing as follows: ������������� � (&apos;,L / I�I / B~$S SA) SA I SR) = _E p(0�1 I SA)loglP(B$S SRL) �� $� 1 ) The Bayes-smoothing factor y is also set to 2,000 and e�$s is estimated by the same way that we compute JSDS. � E E ((&amp;quot;i,SR,i �, �) � 1 � � � � � � � &amp;quot;, Z &amp;quot;, � z &amp;quot;, � � � � E � � � � � � &apos; 2 �� $� � � � � + 1 � � � � � � � � � E ((&amp;quot;i,SR,i �, �) i I� l 467 Table 1 . DUC </context>
<context position="20279" citStr="Zhai and Lafferty 2004" startWordPosition="3651" endWordPosition="3654">(90%+) though it was not as high as ROUGE2, 3, and 4 . 5 Related Work The approach described in this paper is most similar to the Bayesian extension in information retrieval (IR) work by Zaragoza et al . (2003) . In their work, query likelihood model was presented as Bayesian inference. Other earlier language modeling (Rosenfeld 2002) work in information retrieval, especially the idea of modeling a document using bag-of-word unigram model, also inspire this work (Berger and Lafferty 1999, Lafferty and Zhai 2001 Statistical language models such as document language model (Ponte and Croft 1998, Zhai and Lafferty 2004), relevance-based language models (Lavrenko and Croft 2001), and dependency-based language models (Gao et al . 2004) have been applied successfully in information retrieval. It has also been applied to topic detection and tracking (Lavrenko et al . 2002, Larkey et al . 2004) . Ex]SD Trigram ]SD Unigram P S P ]SD ]SDS KLDS LLS Bigram P S P S P S P S MD MD MD SD SD SD SR 0.97 0.91 SR 0.80 0.83 SR 0.92 0.90 SR 0.91 0.88 -0.17 -0.19 SR 0.92 0.90 SR 0.87 0.82 -0.39 -0.33 -0.11 -0.10 MR 0.97 0.91 MR 0.88 0.89 MR 0.94 0.90 MR 0.96 0.94 MR 0.94 0.90 MR 0.93 0.89 -0.30 -0.26 -0.11 -0.10 P S P S P S P S</context>
</contexts>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Zhai, Chengxiang and John Lafferty. 2004 . A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Transactions on Information Systems, Vol 22, No . 2, April 2004, pages 179-214 .</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>