<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997837">
Efficient Generation in Primitive Optimality Theory
</title>
<author confidence="0.999049">
Jason Eisner
</author>
<affiliation confidence="0.9987155">
Dept. of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.844916">
200 S. 33rd St., Philadelphia, PA 19104-6389, USA
</address>
<email confidence="0.9994">
jeisner@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934869565218">
This paper introduces primitive Optimal-
ity Theory (OTP), a linguistically moti-
vated formalization of OT. OTP specifies
the class of autosegmental representations,
the universal generator Gen, and the two
simple families of permissible constraints.
In contrast to less restricted theories us-
ing Generalized Alignment, OTP&apos;s opti-
mal surface forms can be generated with
finite-state methods adapted from (Ellison,
1994). Unfortunately these methods take
time exponential on the size of the gram-
mar. Indeed the generation problem is
shown NP-complete in this sense. How-
ever, techniques are discussed for making
Ellison&apos;s approach fast in the typical case,
including a simple trick that alone provides
a 100-fold speedup on a grammar fragment
of moderate size. One avenue for future
improvements is a new finite-state notion,
&amp;quot;factored automata,&amp;quot; where regular lan-
guages are represented compactly via for-
mal intersections nLi Ai of FSAs.
</bodyText>
<sectionHeader confidence="0.886961" genericHeader="method">
1 Why formalize OT?
</sectionHeader>
<bodyText confidence="0.998840269230769">
Phonology has recently undergone a paradigm shift.
Since the seminal work of (Prince St Smolensky,
1993), phonologists have published literally hun-
dreds of analyses in the new constraint-based frame-
work of Optimality Theory, or OT. Old-style deriva-
tional analyses have all but vanished from the lin-
guistics conferences.
The price of this creative ferment has been a cer-
tain lack of rigor. The claim for 0:T as Universal
Grammar is not substantive or falsifiable without
formal definitions of the putative Universal Gram-
mar objects Repns, Con, and Gen (see below).
Formalizing OT is necessary not only to flesh it out
as a linguistic theory, but also for the sake of compu-
tational phonology. Without knowing what classes
of constraints may appear in grammars, we can say
only so much about the properties of the system,
or about algorithms for generation, comprehension,
and learning.
The central claim of OT is that the phonology of
any language can be naturally described as succes-
sive filtering. In OT, a phonological grammar for
a language consists of a vector C1, C2, ... Cn of soft
constraints drawn from a universal fixed set Con.
Each constraint in the vector is a function that scores
possible output representations (surface forms):
</bodyText>
<listItem confidence="0.605528">
(1) : Repns {0, I, 2, (Ci E Con)
If Ci(R) = 0, the output representation R is said to
satisfy the ith constraint of the language. Other-
wise it is said to violate that constraint, where the
value of Ci(R) specifies the degree of violation. Each
constraint yields a filter that permits only minimal
violation of the constraint:
(2) Filteri(Set)= E Set : Ci(R) is minimal}
</listItem>
<bodyText confidence="0.945762">
Given an underlying phonological input, its set of
legal surface forms under the grammar—typically of
size 1—is just
</bodyText>
<listItem confidence="0.991549">
(3) Filter n ( • • .Filter2( Filter ( Gen( input))))
</listItem>
<bodyText confidence="0.997810555555556">
where the function Gen is fixed across languages
and Gen(input) C Repns is a potentially infinite
set of candidate surface forms.
In practice, each surface form in Gen(input) must
contain a silent copy of input, so the constraints
can score it on how closely its pronounced material
matches input. The constraints also score other cri-
teria, such as how easy the material is to pronounce.
If C1 in a given language is violated by just the forms
with coda consonants, then Filteri(Gen( input)) in-
cludes only coda-free candidates—regardless of their
other demerits, such as discrepancies from input
or unusual syllable structure. The remaining con-
straints are satisfied only as well as they can be given
this set of survivors. Thus, when it is impossible
to satisfy all constraints at once, successive filtering
means early constraints take priority.
Questions under the new paradigm include these:
</bodyText>
<listItem confidence="0.9710455">
• Generation. How to implement the input-
output mapping in (3)? A brute-force approach
</listItem>
<page confidence="0.986503">
313
</page>
<listItem confidence="0.807313714285714">
fails to terminate if Gen produces infinitely
many candidates. Speakers must solve this
problem. So must linguists, if they are to know
what their proposed grammars predict.
• Comprehenston. How to invert the input-
output mapping in (3)? Hearers must solve this.
• Learning. How to induce a lexicon and a
</listItem>
<bodyText confidence="0.9949505625">
phonology like (1) for a particular language.
given the kind of evidence available to child lan-
guage learners?
None of these questions is well-posed without restric-
tions on Gen and Con.
In the absence of such restrictions, computational
linguists have assumed convenient ones. Ellison
(1994) solves the generation problem where Gen
produces a regular set of strings and Con admits
all finite state transducers that can map a string to
a number in unary notation. (Thus Ci(R) = 4 if the
Ci transducer outputs the string 1111 on input R.)
Tesar (1995. 1996) extends this result to the case
where Gen( input) is the set of parse trees for input
under some context-free grammar (CFG).1 Tesar&apos;s
constraints are functions on parse trees such that
Ci({A [B1 • • .] [B2 .]]) can be computed from A, B1,
B2, C(Bl), and Ci(B2). The optimal tree can then
be found with a standard dynamic-programming
chart parser for weighted CFGs.
It is an important question whether these for-
malisms are useful in practice. On the one hand, are
they expressive enough to describe real languages?
On the other, are they restrictive enough to admit
good comprehension and unsupervised-learning al-
gorithms?
The present paper sketches primitive Optimal-
ity Theory (OTP)—a new formalization of OT
that is explicitly proposed as a linguistic hypothe-
sis. Representations are autosegmental. Gen is triv-
ial, and only certain simple and phonologically local
constraints are allowed. I then show the following:
</bodyText>
<listItem confidence="0.997017">
1. Good news: Generation in OTP can be solved
attractively with finite-state methods. The so-
lution is given in some detail.
2. Good news: OTP usefully restricts the space of
grammars to be learned. (In particular. Gener-
alized Alignment is outside the scope of finite-
state or indeed context-free methods.)
3. Bad news: While OTP generation is close to lin-
ear on the size of the input form. it is NP-hard
on the size of the grammar. which for human
languages is likely to be quite large.
4. Good news: Ellison&apos;s algorithm can be improved
so that its exponential blowup is often avoided.
</listItem>
<bodyText confidence="0.831568">
&apos;This extension is useful for OT syntax but may have
little application to phonology, since the context-free
case reduces to the regular case (i.e., Ellison) unless the
CFG contains recursive productions.
</bodyText>
<sectionHeader confidence="0.835201" genericHeader="method">
2 Primitive Optimality Theory
</sectionHeader>
<bodyText confidence="0.974418363636364">
Primitive Optimality Theory. or OTP. is a formal-
ization of OT featuring a homogeneous output repre-
sentation. extremely local constraints, and a simple,
unrestricted Gen. Linguistic arguments for OTP&apos;s
constraints and representations are given in I, Eisner.
1997). whereas the present description focuses on its
formal properties and suitability for computational
work. An axiomatic treatment is omitted for rea-
sons of space. Despite its simplicity. OTP appears
capable of capturing virtually all analyses found in
the (phonological) OT literature.
</bodyText>
<subsectionHeader confidence="0.979">
2.1 Repns: Representations in OTP
</subsectionHeader>
<bodyText confidence="0.999907666666667">
To represent [mpl. OTP uses not the autosegmental
representation in (4a) (Goldsmith. 1976; Goldsmith.
1990) but rather the simplified autosegmental rep-
resentation in (4b). which has no association lines.
Similarly (:5a) is replaced by 5.b). The central rep-
resentational notion is that of a constituent time-
line: an infinitely divisible line along on which con-
stituents are laid out. Every constituent has width
and edges.
</bodyText>
<equation confidence="0.916086">
boll: juot
1
naJ. r&apos;t.as
Cl CAC&apos; jC
1
lab i jlab
--timeltne--
</equation>
<bodyText confidence="0.716727666666667">
For phonetic interpretation: says to end voic-
ing (laryngeal vibration). At the same instant,
says to end nasality (raise velum).
</bodyText>
<equation confidence="0.29161475">
(5) a.
0&apos; 0&apos;
/1\/1
CVCV
</equation>
<bodyText confidence="0.999150117647059">
A timeline can carry the full panoply of phonolog-
ical and morphological constituents—anything that
phonological constraints might have to refer to.
Thus, a timeline bears not only autosegmental fea-
tures like nasal gestures [nasj and prosodic con-
stituents such as syllables [a]. but also stress marks
[x], feature domains such as [ATRdoml (Cole k
Kisseberth, 1994) and morphemes such as [Sterni.
All these constituents are formally identical: each
marks off an interval on the timeline. Let Tiers de-
note the fixed finite set of constituent types. {nns.
a. x, ATRdom. Stern. ...}.
It is always possible to recover the old representa-
tion (4a) from the new one (4b), under the conven-
tion that two constituents on the timeline are linked
if their interiors overlap (Bird &amp; Ellison, 1994). The
interior of a constituent is the open interval that
</bodyText>
<figure confidence="0.646307">
(4) a. voi b.
nas/
1/
C C
\/
lab
</figure>
<page confidence="0.995194">
314
</page>
<bodyText confidence="0.999920833333333">
excludes its edges: Thus, lab is linked to both con-
sonants C in (4b), but the two consonants are not
linked to each other, because their interiors do not
overlap.
By eliminating explicit association lines, OTP
eliminates the need for faithfulness constraints on
them, or for well-formedness constraints against gap-
ping or crossing of associations. In addition, OTP
can refer naturally to the edges of syllables (or mor-
phemes). Such edges are tricky to define in (5a), be-
cause a syllable&apos;s features are scattered across multi-
ple tiers and perhaps shared with adjacent syllables.
In diagrams of timelines, such as (4b) and (5b),
the intent is that only horizontal order matters.
Horizontal spacing and vertical order are irrelevant.
Thus, a timeline may be represented as a finite col-
lection S of labeled edge brackets, equipped with or-
dering relations -&lt; and that indicate which brack-
ets precede each other or fall in the same place.
Valid timelines (those in Repns) also require that
edge brackets come in matching pairs, that con-
stituents have positive width, and that constituents
of the same type do not overlap (i.e., two con-
stituents on the same tier may not be linked).
</bodyText>
<subsectionHeader confidence="0.998487">
2.2 Gen: Input and output in OTP
</subsectionHeader>
<bodyText confidence="0.99996925">
OT&apos;s principle of Containment (Prince &amp; Smolen-
sky, 1993) says that each of the potential outputs in
Repns includes a silent copy of the input, so that
constraints evaluating it can consider the goodness
of match between input and output. Accordingly,
OTP represents both input and output constituents
on the constituent timeline, but on different tiers.
Thus surface nasal autosegments are bracketed with
nas[ and ]r,a, , while underlying nasal autosegments
are bracketed with and Ina, . The underlining
is a notational convention to denote input material.
No connection is required between [nas] and has]
except as enforced by constraints that prefer [rias]
and [nas] or their edges to overlap in some way. (6)
shows a candidate in which underlying [nasi has sur-
faced &amp;quot;in place&amp;quot; but with rightward spreading.
</bodyText>
<equation confidence="0.3442265">
(6) nas[ Inas
nas[ ]nas
</equation>
<bodyText confidence="0.999981947368421">
Here the left edges and interiors overlap, but the
right edges fail to. Such overlap of interiors may be
regarded as featural Input-Output Correspondence
in the sense of (McCarthy &amp; Prince, 1995).
The lexicon and morphology supply to Gen an
underspecified timeline—a partially ordered col-
lection of input edges. The use of a partial ordering
allows the lexicon and morphology to supply float-
ing tones, floating morphemes and templatic mor-
phemes.
Given such an underspecified timeline as lexical
input, Gen outputs the set of all fully specified time-
lines that are consistent with it. No new input con-
stituents may be added. In essence, Gen generates
every way of refining the partial order of input con-
stituents into a total order and decorating it freely
with output constituents. Conditions such as the
prosodic hierarchy (Selkirk, 1980) are enforced by
universally high-ranked constraints, not by Gen.&apos;
</bodyText>
<subsectionHeader confidence="0.998259">
2.3 Con: The primitive constraints
</subsectionHeader>
<bodyText confidence="0.9999875">
Having described the representations used, it is now
possible to describe the constraints that evaluate
them. OTP claims that Con is restricted to the
following two families of primitive constraints:
</bodyText>
<listItem confidence="0.6894126">
0 (&amp;quot;implication&amp;quot;):
&amp;quot;Each a temporally overlaps some g.&amp;quot;
Scoring: Constraint(R) z-- number of a&apos;s in R
that do not overlap any 0.
(8) a 1 0 (&amp;quot;clash&amp;quot;):
</listItem>
<subsectionHeader confidence="0.54178">
&amp;quot;Each a temporally overlaps no ,(3.&amp;quot;
</subsectionHeader>
<bodyText confidence="0.96659495">
Scoring: Constraint(R) = number of (a,/3)
pairs in R such that the a overlaps the 0.
That is, a 0 says that a&apos;s attract 0&apos;s, while
J_ 0 says that a&apos;s repel 0&apos;s. These are simple and
arguably natural constraints; no others are used.
In each primitive constraint, a and 13 each spec-
ify a phonological event. An event is defined to be
either a type of labeled edge, written e.g. , or
the interior (excluding edges) of a type of labeled
constituent, written e.g. or. To express some con-
straints that appear in real phonologies, it is also
necessary to allow, a and 0 to be non-empty con-
junctions and disjunctions of events. However, it
appears possible to limit these cases to the forms in
(9)-(10). Note that other forms, such as those in
(11), can be decomposed into a sequence of two or
2The formalism is complicated slightly by the pos-
sibility of deleting segments (syncope) or inserting seg-
ments (epenthesis), as illustrated by the candidates be-
low.
</bodyText>
<listItem confidence="0.9687995">
(i) Syncope (CVC CC): the V is crushed to zero
width so the C&apos;s can be adjacent.
</listItem>
<equation confidence="0.8927845">
c[ lc ic
c 1c.
vlv
(ii) Epenthesis (CC CVC): the C&apos;s are pushed
apart.
v[ Iv
c[ lc c[ ic
c.[ lc
</equation>
<bodyText confidence="0.975755375">
In order to allow adjacency of the surface consonants in
(i), as expected by assimilation processes (and encour-
aged by a high-ranked constraint), note that the underly-
ing vowel must be allowed to have zero width—an option
available to to input but not output constituents. The
input representation must specify only v[ &lt; ]v, not
v[ ]v. Similarly, to allow (ii), the input representa-
tion must specify only 1c, c.,[, not lci c2[.
</bodyText>
<equation confidence="0.76538">
(7)
</equation>
<page confidence="0.984248">
315
</page>
<bodyText confidence="0.865905">
more constraints.3
</bodyText>
<listItem confidence="0.994434727272727">
(9) ( ai and a, and ... ) (13i or /32 or ... )
Scoring: Constraint(R) = number of sets of
events {A1, A2, ...} of types a1, a2,... respec-
tively that all overlap on the timeline and
whose intersection does not overlap any event
of type /31, /32, ....
(10) ( al and a2 and ... ) 1 (/3i and ,32 and ...
Scoring: Constraint(R) = number of sets
of events {Ai, A2, • • • BI,B2,...} of types
al a2, • • • /31o3,, • • • respectively that all
overlap on the timeline.
</listItem>
<equation confidence="0.7897555">
(Could also be notated:
ai az -L • • • -I- J- ,82 • • ••)
</equation>
<bodyText confidence="0.979343631578947">
(II) a (i31 and /32) [cf. a —&gt; /31 &gt;&gt; a — 02]
( al or a2 ) 13 [cf. al —* &gt;&gt; a2 /3]
The unifying theme is that each primitive con-
straint counts the number of times a candidate gets
into some bad local configuration. This is an inter-
val on the timeline throughout which certain events
(one or more specified edges or interiors) are all
present and certain other events (zero or more spec-
ified edges or interiors) are all absent.
Several examples of phonologically plausible con-
straints, with monikers and descriptions, are given
below. (Eisner, 1997) shows how to rewrite hun-
dreds of constraints from the literature in the primi-
tive constraint notation, and discusses the problem-
atic case of reduplication. (Eisner, in press) gives
a detailed stress typology using only primitive con-
straints; in particular, non-local constraints such
as FTBIN, FooTFoRm, and Generalized Alignment
(McCarthy Sz Prince, 1993) are eliminated.
</bodyText>
<listItem confidence="0.626202714285714">
(12) a. ONSET: 4 C[
&amp;quot;Every syllable starts with a consonant.&amp;quot;
b. NONFINALITY: ]Word
&amp;quot;The end of a word may not be footed.&amp;quot;
c. FTSYL: F[ , IF — icr
&amp;quot;Feet start and end on syllable boundaries.&amp;quot;
d. PACKFEET: 1F F[
</listItem>
<table confidence="0.947510973684211">
&amp;quot;Each foot is followed immediately by an-
other foot; i.e., minimize the number of gaps
between feet. Note that the final foot, if any,
will always violate this constraint.&amp;quot;
e. NoCLASH: ]x x[
&amp;quot;Two stress marks may not be adjacent.&amp;quot;
f. PROGRESSIVEVOICING: ]voiJ C[
&amp;quot;If the segment preceding a consonant is
voiced, voicing may not stop prior to the
3Such a sequence does alter the meaning slightly. To
get the exact original meaning, we would have to de-
compose into so-called &amp;quot;unranked&amp;quot; constraints, whereby
C, (R) is defined as C,, (R)+Cs2(R). But such ties under-
mine OT&apos;s idea of strict ranking: they confer the power
to minimize linear functions such as (C1 -I- Ci
+ C3 + C3 )( R)= 3 Ci (R)+ C2 (R)+2C3(R). For this
reason, OTP currently disallows unranked constraints; I
know of no linguistic data that crucially require them.
consonant but must be spread onto it.&amp;quot;
NAsVoi: nas voi
&amp;quot;Every nasal gesture must be at least partly
voiced.&amp;quot;
h. FuLLNAsVoi: nas 1..vod nas J.. ]voi
&amp;quot;A nasal gesture may not be only partly
voiced.&amp;quot;
i. —Ax
M ( i)
vo., Or PARSE( V01): t/02 voi
&amp;quot;Underlying voicing features surface.&amp;quot;
DEP(voi) or FILL(voi): voi
&amp;quot;Voicing features appear on the surface only
if they are also underlying.&amp;quot;
k. NoSPREADRIGHT(voi): voi 1 1,,
&amp;quot;Underlying voicing may not spread right-
ward as in (6).&amp;quot;
1. NONDEGENERATE: F A
&amp;quot;Every foot must cross at least one mora
boundary „,[
</table>
<listItem confidence="0.943557125">
m. TAUTOMORPHEMICFOOT: F J M orph[
&amp;quot;No foot may cross a morpheme boundary.&amp;quot;
3 Finite-state generation in OTP
3.1 A simple generation algorithm
Recall that the generation problem is to find the
output set Sn, where .
(13) a. So = Gen(input) C Repns
b. Si+1 = Filteri+I(Si) C
</listItem>
<bodyText confidence="0.998174833333334">
Since in OTP, the input is a partial order of edge
brackets, and Sr, is a set of one or more total orders
(timelines), a natural approach is to successively re-
fine a partial order. This has merit. However, not
every Si can be represented as a single partial order,
so the approach is quickly complicated by the need
to encode disjunction.
A simpler approach is to represent Si (as well
as input and Repns) as a finite-state automaton
(FSA), denoting a regular set of strings that encode
timelines. The idea is essentially due to (Ellison,
1994), and can be boiled down to two lines:
</bodyText>
<listItem confidence="0.64064">
(14) Ellison&apos;s algorithm (variant).
</listItem>
<bodyText confidence="0.87109">
So = input n Repns
= all conceivable outputs containing input
Si+i = BestPdths(Si n Ci+i
Each constraint Ci must be formulated as an edge-
weighted FSA that scores candidates: Ci accepts any
string R, on a single path of weight Ci(R).4 Best-
Paths is Dijkstra&apos;s &amp;quot;single-source shortest paths&amp;quot;
algorithm, a dynamic-programming algorithm that
prunes away all but the minimum-weight paths in
an automaton, leaving an unweighted automaton.
OTP is simple enough that it can be described in
this way. The next section gives a nice encoding.
</bodyText>
<tableCaption confidence="0.243413">
&apos;Weighted versions of the state-labeled finite au-
tomata of (Bird &amp; Ellison, 1994) could be used instead.
</tableCaption>
<figure confidence="0.948827">
g.
J.
</figure>
<page confidence="0.994839">
316
</page>
<subsectionHeader confidence="0.999298">
3.2 OTP with automata
</subsectionHeader>
<bodyText confidence="0.999945714285714">
We may encode each timeline as a string over an
enormous alphabet E. If &apos;Tiers&apos; = k, then each
symbol in E is a k-tuple, whose components describe
what is happening on the various tiers at a given
moment. The components are drawn from a smaller
alphabet A = { C, 1. I , +, -}. Thus at any time, the
ith tier may be beginning or ending a constituent ( C,
1) or both at once ( I ), or it may be in a steady state
in the interior or exterior of a constituent (+, -).
At a minimum, the string must record all moments
where there is an edge on some tier. If all tiers are in
a steady state, the string need not use any symbols
to say so. Thus the string encoding is not unique.
(15) gives an expression for all strings that cor-
rectly describe the single tier shown. (16) describes
a two-tier timeline consistent with (15). Note that
the brackets on the two tiers are ordered with re-
spect to each other. Timelines like these could be
assembled morphologically from one or more lexical
entries (Bird &amp; Ellison, 1994), or produced in the
course of algorithm (14).
</bodyText>
<equation confidence="0.9942986">
(15) .[ xlr Ir [4.* I +*]
(16) XIX /X
Y [ IY [
(-,-)*(C,-)(+,-)*(+, 0(+,+)*(1,+)(+,+)*
(+,])(+,-)*(+, [X+,+)*(],])
</equation>
<bodyText confidence="0.999598857142857">
We store timeline expressions like (16) as deter-
ministic FSAs. To reduce the size of these automata,
it is convenient to label arcs not with individual el-
ements of E (which is huge) but with subsets of E,
denoted by predicates. We use conjunctive predi-
cates where each conjunct lists the allowed symbols
on a given tier:
</bodyText>
<listItem confidence="0.637073">
(17) +F, cr, C I +-voi (arc label w/ 3 conjuncts)
</listItem>
<bodyText confidence="0.999962875">
The arc label in (17) is said to mention the tiers
F,a,voi E Tiers. Such a predicate allows any sym-
bol from z. on the tiers it does not mention.
The input FSA constrains only the input tiers. In
(14) we intersect it with Repns, which constrains
only the output tiers. Repns is defined as the inter-
section of many automata exactly like (18), called
tier rules, which ensure that brackets are properly
paired on a given tier such as F (foot).
Like the tier rules, the constraint automata Ci are
small and deterministic and can be built automat-
ically. Every edge has weight 0 or 1. With some
care it is possible to draw each Ci with two or fewer
states, and with a number of arcs proportional to
the number of tiers mentioned by the constraint.
Keeping the constraints small is important for ef-
ficiency, since real languages have many constraints
that must be intersected.
Let us do the hardest case first. An implication
constraint has the general form (9). Suppose that all
the ai are interiors, not edges. Then the constraint
targets intervals of the form a = ai fl a2 n • Each
time such an interval ends without any 1.32 having
occurred during it, one violation is counted:
</bodyText>
<equation confidence="0.957574666666667">
(19) Weight-1 arcs are shown in bold; others are
weight-O.
(other)
</equation>
<bodyText confidence="0.995351714285714">
A candidate that does see a I3 during an a can go
and rest in the right-hand state for the duration of
the a.
Let us fill in the details of (19). How do we detect
the end of an a&apos;? Because one or more of the ai end
(1, I), while all the ai either end or continue (+), so
that we know we are leaving an a.5 Thus:
</bodyText>
<listItem confidence="0.9755026">
(20) (in all ai) - (some bj)
or end a al
- (in all ai)
gma - in
or end all ai) in all ai
</listItem>
<bodyText confidence="0.9616619">
(in all ai) &amp; (some bj)
(in or end all ai) - (in all ai)
An unusually complex example is shown in (21).
Note that to preserve the form of the predicates
in (17) and keep the automaton deterministic, we
need to split some of the arcs above into multi-
ple arcs. Each 13j gets its own arc, and we must
also expand set differences into multiple arcs, using
the scheme W—xAyAz = WV--1(xAyAz)=
(WA-Ix)V(WAxA-,y)V(WAxAyA-1::).
51t is important to take], not +, as our indication that
we have been inside a constituent. This means that the
timeline ( [, -)(+, -)*(+, [)(+, +)*(], +)(-, +)*(_,]) cannot
avoid violating a clash constraint simply by instantiat-
ing the (+,+)* part as e. Furthermore, the ] convention
means that a zero-width input constituent (more pre-
cisely, a sequence of zero-width constituents, represented
as a single I symbol) will often act as if it has an interior.
Thus if V syncopates as in footnote 2, it still violates the
parse constraint V V. This is an explicit property of
OTP: otherwise, nothing that failed to parse would ever
violate PARSE, because it would be gone!
On the other hand, ] does not have this special role
on the right hand side of —■ , which does not quantify
universally over an interval. The consequence for zero-
width consituents is that even if a zero-width V overlaps
(at the edge, say) with a surface V, the latter cannot
claim on this basis alone to satisfy FILL: V — V. This
too seems like the right move linguistically, although fur-
ther study is needed.
</bodyText>
<page confidence="0.993918">
317
</page>
<bodyText confidence="0.776341923076923">
(21) ( p and q) (b or c[ )
+p +q
How about other cases? If the antecedent of
an implication is not an interval, then the con-
straint needs only one state, to penalize mo-
ments when the antecedent holds and the con-
sequent does not. Finally, a clash constraint
al 1 a2 1 • • • is identical to the implication
constraint ( al and 02 and • • • ) FALSE. Clash
FSAs are therefore just degenerate versions of im-
plication FSAs, where the arcs looking for i3j do not
exist because they would accept no symbol. (22)
shows the constraints ( p and k ) b and p J. q.
</bodyText>
<equation confidence="0.857538">
(22) +p +q
01-b + 11q
</equation>
<sectionHeader confidence="0.886439" genericHeader="method">
4 Computational requirements
</sectionHeader>
<subsectionHeader confidence="0.948561">
4.1 Generalized Alignment is not finite-state
</subsectionHeader>
<bodyText confidence="0.999546555555555">
Ellison&apos;s method can succeed only on a restricted
formalism such as OTP, which does not admit such
constraints as the popular Generalized Alignment
(GA) family of (McCarthy &amp; Prince, 1993). A typ-
ical GA constraint is ALIGN(F, L, Word, L), which
sums the number of syllables between each left foot
edge F[ and the left edge of the prosodic word. Min-
imizing this sum achieves a kind of left-to-right it-
erative footing. OTP argues that such non-local,
arithmetic constraints can generally be eliminated
in favor of simpler mechanisms (Eisner, in press).
Ellison&apos;s method cannot directly express the above
GA constraint, even outside OTP, because it cannot
compute a quadratic function 0 + 2 + 4 + • • on a
string like [bo-)F Vroll- olF • • • . Path weights in an
FSA cannot be more than linear on string length.
Perhaps the filtering operation of any GA con-
straint can be simulated with a system of finite-
state constraints? No: GA is simply too powerful.
The proof is suppressed here for reasons of space,
but it relies on a form of the pumping lemma for
weighted FSAs. The key insight is that among can-
didates with a fixed number of syllables and a single
(floating) tone, ALIGN(cr, L, H, L) prefers candidates
where the tone docks at the center. A similar argu-
ment for weighted CFGs (using two tones) shows this
constraint to be too hard even for (Tesar, 1996).
</bodyText>
<subsectionHeader confidence="0.92571">
4.2 Generation is NP-complete even in OTP
</subsectionHeader>
<bodyText confidence="0.996301368421052">
When algorithm (14) is implemented literally and
with moderate care, using an optimizing C compiler
on a 167MHz UltraSPARC, it takes fully 3.5 minutes
(real time) to discover a stress pattern for the syl-
lable sequence The automata
become impractically huge due to intersections.
Much of the explosion in this case is introduced
at the start and can be avoided. Because Repns
has 21Tiersi = 512 states, So, S, and S., each
have about 5000 states and 500,000 to 775,000 arcs.
Thereafter the Si automata become smaller, thanks
to the pruning performed at each step by BestPaths.
This repeated pruning is already an improvement
over Ellison&apos;s original algorithm (which saves prun-
ing till the end, and so continues to grow exponen-
tially with every new constraint). If we modify (14)
further, so that each tier rule from Repns is inter-
sected with the candidate set only when its tier is
first mentioned by a constraint, then the automata
are pruned back as quickly as they grow. They have
about 10 times fewer states and 100 times fewer arcs.
and the generation time drops to 2.2 seconds.
This is a key practical trick. But neither it nor
any other trick can help for all grammars, for in the
worst case, the OTP generation problem is NP-hard
on the number of tiers used by the grammar. The
locality of constraints does not save us here. Many
NP-complete problems, such as graph coloring or
bin packing, attempt to minimize some global count
subject to numerous local restrictions. In the case of
OTP generation, the global count to minimize is the
degree of violation of Ci, and the local restrictions
are imposed by CI, C2, • • • Ci- •
Proof of NP-hardness (by polytime reduction
from Hamilton Path). Given G = (V(G), E(G)),
an n-vertex directed graph. Put Tiers = V(G) U
{Stem, . Consider the following vector of 0(n2)
primitive constraints (ordered as shown):
</bodyText>
<listItem confidence="0.835092">
(23) a. Vv E V(G): vi si
b. Vv E V(G): iv Is
c. Vv E V(G): Stem
d. Stem .1 S
</listItem>
<equation confidence="0.6185865">
e. V u, v E V(G) s.t. uv E(G): b, 1 „[
f • Is s[
</equation>
<bodyText confidence="0.9086215">
6The grammar is taken from the OTP stress typol-
ogy proposed by (Eisner, in press). It has tier rules for 9
tiers, and then spends 26 constraints on obvious univer-
sal properties of moras and syllables, followed by 6 con-
straints for universal properties of feet and stress marks
and finally 6 substantive constraints that can be freely
reranked to yield different stress systems, such as left-to-
right iambs with iambic lengthening.
</bodyText>
<page confidence="0.997686">
318
</page>
<bodyText confidence="0.999912138888889">
Suppose the input is simply [Stem]. Filtering
Gen(input) through constraints (23a—d), we are left
with just those candidates where Stem bears n (dis-
joint) constituents of type S, each coextensive with
a constituent bearing a different label v E V(G).
(These candidates satisfy (23a—c) but violate (23d)
n times.) (23e) says that a chain of abutting con-
stituents [uIvItv] • • is allowed only if it corresponds
to a path in G. Finally, (23f) forces the grammar to
minimize the number of such chains. If the minimum
is 1 (i.e., an arbitrarily selected output candidate vi-
olates (23f) only once), then G has a Hamilton path.
When confronted with this pathological case, the
finite:-state methods respond essentially by enumer-
ating all possible permutations of V(G) (though
with sharing of prefixes). The machine state stores,
among other things, the subset of V(G) that has al-
ready been seen; so there are at least 21Tiersi states.
It must be emphasized that if the grammar is
fixed in advance, algorithm (14) is close to linear
in the size of the input form: it is dominated by
a constant number of calls to Dijkstra&apos;s BestPaths
method, each taking time Oainput arcsI log linput
statesp. There are nonetheless three reasons why
the above result is important. (a) It raises the prac-
tical specter of huge constant factors (&gt; 240) for real
grammars. Even if a fixed grammar can somehow be
compiled into a fast form for use with many inputs,
the compilation itself will have to deal with this con-
stant factor. (b) The result has the interesting im-
plication that candidate sets can arise that cannot
be concisely represented with FSAs. For if all Si
were polynomial-sized in (14), the algorithm would
run in polynomial time. (c) Finally, the grammar
is not fixed in all circumstances: both linguists and
children crucially experiment with different theories.
</bodyText>
<subsectionHeader confidence="0.656426">
4.3 Work in progress: Factored automata
</subsectionHeader>
<bodyText confidence="0.997270947368421">
The previous section gave a useful trick for speeding
up Ellison&apos;s algorithm in the typical case. We are
currently experimenting with additional improve-
ments along the same lines, which attempt to de-
fer intersection by keeping tiers separate as long as
possible.
The idea is to represent the candidate set Si not as
a large unweighted FSA, but rather as a collection A
of preferably small unweighted FSAs, called factors,
each of which mentions as few tiers as possible. This
collection, called a factored automaton, serves as
a compact representation of nA. It usually has far
fewer states than nA would if the intersection were
carried out.
For instance, the natural factors of So are input
and all the tier rules (see 18). This requires only
0(1Tiersi + &apos;input!) states, not 0(21Tiersi • linputp.
Using factored automata helps Ellison&apos;s algorithm
(14) in several ways:
</bodyText>
<listItem confidence="0.9006414">
• The candidate sets Si tend to be represented
more compactly.
• In (14), the constraint Ci+1 needs to be inter-
sected with only certain factors of Si.
• Sometimes Ci+i does not need to be intersected
</listItem>
<bodyText confidence="0.968777092592593">
with the input, because they do not mention
any of the same tiers. Then step i + 1 can be
performed in time independent of input length.
Example: input = , which is
a 43-state automaton, and C1 is F x, which says
that every foot bears a stress mark. Then to find
= BestPaths(So fl C1), we need only consider
So&apos;s tier rules for F and x, which require well-formed
feet and well-formed stress marks, and combine them
with C1 to get a new factor that requires stressed
feet. No other factors need be involved.
The key operation in (14) is to find Bestpaths(A n
C), where A is an unweighted factored automaton
and C is an ordinary weighted FSA (a constraint).
This is the best intersection problem. For con-
creteness let us suppose that C encodes F x, a
two-state constraint.
A naive idea is simply to add F x to A as
a new factor. However, this ignores the BestPaths
step: we wish to keep just the best paths in p[ X[
that are compatible with A. Such paths might be
long and include cycles in F[ x[ . For example,
a weight-1 path would describe a chain of optimal
stressed feet interrupted by a single unstressed one
where A happens to block stress.
A corrected variant is to put I = nA and run
BestPaths on I n C. Let the pruned result be B.
We could add B directly back to to A as a new
factor, but it is large. We would rather add a smaller
factor B&apos; that has the same effect, in that I n B&apos; =
I n B. (B&apos; will look something like the original C,
but with some paths missing, some states split, and
some cycles unrolled.) Observe that each state of B
has the form i x c for some i E / and c E C. We
form B&apos; from B by &amp;quot;re-merging&amp;quot; states i x c and
x c where possible, using an approach similar to
DFA minimization.
Of course, this variant is not very efficient, because
it requires us to find and use I = nA. What we
really want is to follow the above idea but use a
smaller I, one that considers just the relevant factors
in A. We need not consider factors that will not
affect the choice of paths in C above.
Various approaches are possible for choosing such
an I. The following technique is completely general,
though it may or may not be practical.
Observe that for BestPaths to do the correct
thing, I needs to reflect the sum total of A&apos;s con-
straints on F and x, the tiers that C mentions. More
formally, we want I to be the projection of the can-
didate set nA onto just the F and x tiers. Unfortu-
nately, these constraints are not just reflected in the
factors mentioning F or x, since the allowed con-
figurations of F and x may be mediated through
</bodyText>
<page confidence="0.997682">
319
</page>
<bodyText confidence="0.9992972">
additional factors. As an example, there may be a
factor mentioning F and some of whose paths are
incompatible with the input factor, because the lat-
ter allows ik only in certain places or because only
allows paths of length 14.
</bodyText>
<listItem confidence="0.890360866666667">
1. Number the tiers such that F and x are num-
bered 0, and all other tiers have distinct positive
numbers.
2. Partition the factors of A into lists Lo,
L2, Lk, according to the highest-numbered
tier they mention. (Any factor that mentions
no tiers at all goes onto Lo.)
3. If k = 0, then return nLk as our desired I.
4. Otherwise, aLk exhausts tier k&apos;s ability to me-
diate relations among the factors. Modify the
arc labels of ni,k so that they no longer restrict
(mention) k. Then add a determinized, mini-
mized version of the result to to Li, where j is
the highest-numbered tier it now mentions.
5. Decrement k and return to step 3.
</listItem>
<bodyText confidence="0.998974888888889">
If nA has k factors, this technique must per-
form k — 1 intersections, just as if we had put
I = nA. However, it intersperses the intersections
with determinization and minimization operations,
so that the automata being intersected tend not
to be large. In the best case, we will have k —
1 intersection-determinization-minimizations that
cost 0(1) apiece, rather than k —1 intersections that
cost up to 0(2k) apiece.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.961672314285714">
Primitive Optimality Theory, or OTP, is an attempt
to produce a a simple, rigorous, constraint-based
model of phonology that is closely fitted to the needs
of working linguists. I believe it is worth study both
as a hypothesis about Universal Grammar and as a
formal object.
The present paper introduces the OTP formal-
ization to the computational linguistics community.
We have seen two formal results of interest, both
having to do with generation of surface forms:
• OTP&apos;s generative power is low: finite-state
optimization. In particular it is more con-
strained than theories using Generalized Align-
ment. This is good news for comprehension and
learning.
• OTP&apos;s computational complexity, for genera-
tion, is nonetheless high: NP-complete on the
size of the grammar. This is mildly unfortunate
for OTP and for the OT approach in general.
It remains true that for a fixed grammar, the
time to do generation is close to linear on the
size of the input (Ellison, 1994), which is heart-
ening if we intend to optimize long utterances
with respect to a fixed phonology.
Finally, we have considered the prospect of building
a practical tool to generate optimal outputs from
OT theories. We saw above to set up the represen-
tations and constraints efficiently using determinis-
tic finite-state automata, and how to remedy some
hidden inefficiencies in the seminal work of (Elli-
son, 1994), achieving at least a 100-fold observed
speedup. Delayed intersection and aggressive prun-
ing prove to be important. Aggressive minimization
and a more compact. &amp;quot;factored&amp;quot; representation of
automata may also turn out to help.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999716756097561">
Bird, Steven, &amp; T. Mark Ellison. One Level Phonol-
ogy: Autosegmental representations and rules as
finite automata. Comp. Linguistics 20:55-90.
Cole, Jennifer, &amp; Charles Kisseberth. 1994. An op-
timal domains theory of harmony. Studies in the
Linguistic Sciences 24: 2.
Eisner, Jason. In press. Decomposing FootForm:
Primitive constraints in OT. Proceedings of SCIL
8, NYU. Published by MIT Working Papers.
(Available at http://ruccs.rutgers.edu/roa.html.)
Eisner, Jason. What constraints should OT allow?
Handout for talk at LSA, Chicago. (Available at
http://ruccs.rutgers.edu/roa.html.)
Ellison, T. Mark. Phonological derivation in opti-
mality theory. COLING &apos;94, 1007-1013.
Goldsmith, John. 1976. Autosegmental phonology.
Cambridge, Mass: MIT PhD. dissertation. Pub-
lished 1979 by New York: Garland Press.
Goldsmith, John. 1990. Autosegmental and metrical
phonology. Oxford: Blackwell Publishers.
McCarthy, John, &amp; Alan Prince. 1993. General-
ized alignment. Yearbook of Morphology, ed. Geert
Booij &amp; Jaap van Marle, pp. 79-153. Kluwer.
McCarthy, John and Alan Prince. 1995. Faithful-
ness and reduplicative identity. In Jill Beckman
et al., eds., Papers in Optimality Theory. UMass.
Amherst: GLSA. 259-384.
Prince, Alan, &amp; Paul Smolensky. 1993. Optimality
theory: constraint interaction in generative gram-
mar. Technical Reports of the Rutgers University
Center for Cognitive Science.
Selkirk, Elizabeth. 1980. Prosodic domains in
phonology: Sanskrit revisited. In Mark Aranoff
and Mary-Louise Kean, eds., Juncture, pp. 107-
129. Anna Libri, Saratoga, CA.
Tesar, Bruce. 1995. Computational Optimality The-
ory. Ph.D. dissertation, U. of Colorado, Boulder.
Tesar, Bruce. 1996. Computing optimal descriptions
for Optimality Theory: Grammars with context-
free position structures. Proceedings of the 34th
Annual Meeting of the ACL.
</reference>
<page confidence="0.998251">
320
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003070">
<title confidence="0.999916">Efficient Generation in Primitive Optimality Theory</title>
<author confidence="0.999993">Jason Eisner</author>
<affiliation confidence="0.9996285">Dept. of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999903">200 S. 33rd St., Philadelphia, PA 19104-6389, USA</address>
<email confidence="0.999878">jeisner@linc.cis.upenn.edu</email>
<abstract confidence="0.998617296158616">This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP&apos;s optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison&apos;s approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, &amp;quot;factored automata,&amp;quot; where regular languages are represented compactly via forintersections Ai of FSAs. 1 Why formalize OT? Phonology has recently undergone a paradigm shift. Since the seminal work of (Prince St Smolensky, 1993), phonologists have published literally hundreds of analyses in the new constraint-based framework of Optimality Theory, or OT. Old-style derivational analyses have all but vanished from the linguistics conferences. The price of this creative ferment has been a certain lack of rigor. The claim for 0:T as Universal Grammar is not substantive or falsifiable without formal definitions of the putative Universal Grammar objects Repns, Con, and Gen (see below). Formalizing OT is necessary not only to flesh it out as a linguistic theory, but also for the sake of computational phonology. Without knowing what classes of constraints may appear in grammars, we can say only so much about the properties of the system, or about algorithms for generation, comprehension, and learning. The central claim of OT is that the phonology of language can be naturally described as succesfiltering. OT, a phonological grammar for language consists of a vector C1, C2, ... of soft constraints drawn from a universal fixed set Con. Each constraint in the vector is a function that scores possible output representations (surface forms): (1) : Repns {0, I, 2, (Ci E Con) 0, the output representation said to satisfy the ith constraint of the language. Otherit is said to constraint, where the of the degree of violation. Each constraint yields a filter that permits only minimal violation of the constraint: Filteri(Set)= E : Ci(R) minimal} Given an underlying phonological input, its set of legal surface forms under the grammar—typically of size 1—is just Filter n( • • .Filter2( Filter ( Gen( the function fixed across languages Gen(input) C a potentially infinite set of candidate surface forms. In practice, each surface form in Gen(input) must a silent copy of the constraints can score it on how closely its pronounced material constraints also score other criteria, such as how easy the material is to pronounce. in a given language is violated by just the forms coda consonants, then includes only coda-free candidates—regardless of their demerits, such as discrepancies from or unusual syllable structure. The remaining constraints are satisfied only as well as they can be given this set of survivors. Thus, when it is impossible to satisfy all constraints at once, successive filtering means early constraints take priority. Questions under the new paradigm include these: Generation. to implement the inputoutput mapping in (3)? A brute-force approach 313 fails to terminate if Gen produces infinitely many candidates. Speakers must solve this problem. So must linguists, if they are to know what their proposed grammars predict. Comprehenston. to invert the output mapping in (3)? Hearers must solve this. Learning. to induce a lexicon and a phonology like (1) for a particular language. given the kind of evidence available to child language learners? None of these questions is well-posed without restricon and Con. In the absence of such restrictions, computational linguists have assumed convenient ones. Ellison solves the generation problem where a regular set of strings and all finite state transducers that can map a string to number in unary notation. (Thus = if the transducer outputs the string 1111 on input Tesar (1995. 1996) extends this result to the case input) the set of parse trees for some context-free grammar Tesar&apos;s constraints are functions on parse trees such that • • .] .]]) can be computed from A, B1, and Ci(B2). The optimal tree can then be found with a standard dynamic-programming chart parser for weighted CFGs. It is an important question whether these formalisms are useful in practice. On the one hand, are they expressive enough to describe real languages? On the other, are they restrictive enough to admit good comprehension and unsupervised-learning algorithms? present paper sketches Optimal- Theory (OTP)—a formalization of OT that is explicitly proposed as a linguistic hypothe- Representations are autosegmental. trivial, and only certain simple and phonologically local constraints are allowed. I then show the following: Good news: in OTP can be solved attractively with finite-state methods. The solution is given in some detail. Good news: usefully restricts the space of grammars to be learned. (In particular. Generalized Alignment is outside the scope of finitestate or indeed context-free methods.) Bad news: OTP generation is close to linear on the size of the input form. it is NP-hard on the size of the grammar. which for human languages is likely to be quite large. Good news: algorithm can be improved so that its exponential blowup is often avoided. &apos;This extension is useful for OT syntax but may have little application to phonology, since the context-free case reduces to the regular case (i.e., Ellison) unless the CFG contains recursive productions. 2 Primitive Optimality Theory Primitive Optimality Theory. or OTP. is a formalization of OT featuring a homogeneous output representation. extremely local constraints, and a simple, arguments for OTP&apos;s constraints and representations are given in I, Eisner. 1997). whereas the present description focuses on its formal properties and suitability for computational work. An axiomatic treatment is omitted for reasons of space. Despite its simplicity. OTP appears capable of capturing virtually all analyses found in the (phonological) OT literature. 2.1 Repns: Representations in OTP To represent [mpl. OTP uses not the autosegmental representation in (4a) (Goldsmith. 1976; Goldsmith. 1990) but rather the simplified autosegmental representation in (4b). which has no association lines. (:5a) is replaced by The central repnotion is that of a timeinfinitely divisible line along on which constituents are laid out. Every constituent has width and edges. boll: 1 r&apos;t.as Cl CAC&apos; jC 1 lab jlab --timeltne-- For phonetic interpretation: to end voicing (laryngeal vibration). At the same instant, says to end nasality (raise velum). (5) a. /1\/1 CVCV A timeline can carry the full panoply of phonological and morphological constituents—anything that phonological constraints might have to refer to. Thus, a timeline bears not only autosegmental fealike nasal gestures prosodic conas syllables [a]. but also stress marks domains such as [ATRdoml (Cole k Kisseberth, 1994) and morphemes such as [Sterni. All these constituents are formally identical: each off an interval on the timeline. Let dethe fixed finite set of constituent types. ATRdom. ...}. It is always possible to recover the old representation (4a) from the new one (4b), under the convention that two constituents on the timeline are linked if their interiors overlap (Bird &amp; Ellison, 1994). The a constituent is the open interval that (4) a. voi b. nas/ 1/ C C \/ lab 314 its edges: Thus, linked to both con- (4b), but the two consonants are not linked to each other, because their interiors do not overlap. By eliminating explicit association lines, OTP eliminates the need for faithfulness constraints on them, or for well-formedness constraints against gapping or crossing of associations. In addition, OTP can refer naturally to the edges of syllables (or morphemes). Such edges are tricky to define in (5a), because a syllable&apos;s features are scattered across multiple tiers and perhaps shared with adjacent syllables. In diagrams of timelines, such as (4b) and (5b), the intent is that only horizontal order matters. Horizontal spacing and vertical order are irrelevant. Thus, a timeline may be represented as a finite collabeled edge brackets, equipped with ordering relations -&lt; and that indicate which brackets precede each other or fall in the same place. Valid timelines (those in Repns) also require that edge brackets come in matching pairs, that constituents have positive width, and that constituents of the same type do not overlap (i.e., two constituents on the same tier may not be linked). 2.2 Gen: Input and output in OTP OT&apos;s principle of Containment (Prince &amp; Smolensky, 1993) says that each of the potential outputs in a silent copy of the input, so that constraints evaluating it can consider the goodness of match between input and output. Accordingly, OTP represents both input and output constituents on the constituent timeline, but on different tiers. Thus surface nasal autosegments are bracketed with and , while underlying nasal autosegments are bracketed with and . The underlining is a notational convention to denote input material. connection is required between has] except as enforced by constraints that prefer [rias] [nas]or their edges to overlap in some way. (6) a candidate in which underlying [nasihas surfaced &amp;quot;in place&amp;quot; but with rightward spreading. (6) nas[ Inas nas[ ]nas Here the left edges and interiors overlap, but the right edges fail to. Such overlap of interiors may be regarded as featural Input-Output Correspondence in the sense of (McCarthy &amp; Prince, 1995). lexicon and morphology supply to timeline—a ordered colof The use of a allows the lexicon and morphology to supply floating tones, floating morphemes and templatic morphemes. Given such an underspecified timeline as lexical the set of specified timelines that are consistent with it. No new input conmay be added. In essence, every way of refining the partial order of input constituents into a total order and decorating it freely with output constituents. Conditions such as the prosodic hierarchy (Selkirk, 1980) are enforced by high-ranked constraints, not by 2.3 Con: The primitive constraints Having described the representations used, it is now possible to describe the constraints that evaluate OTP claims that restricted to the two families of constraints: 0 (&amp;quot;implication&amp;quot;): a temporally overlaps some z-number of a&apos;s in do not overlap any a 1 a temporally overlaps no = number of (a,/3) in that the a overlaps the 0. That is, a 0 says that a&apos;s attract 0&apos;s, while that a&apos;s repel 0&apos;s. These are simple and arguably natural constraints; no others are used. each primitive constraint, a and specify a phonological event. An event is defined to be either a type of labeled edge, written e.g. , or the interior (excluding edges) of a type of labeled constituent, written e.g. or. To express some constraints that appear in real phonologies, it is also necessary to allow, a and 0 to be non-empty conjunctions and disjunctions of events. However, it appears possible to limit these cases to the forms in (9)-(10). Note that other forms, such as those in (11), can be decomposed into a sequence of two or formalism is complicated slightly by the possibility of deleting segments (syncope) or inserting segments (epenthesis), as illustrated by the candidates below. Syncope (CVCCC): crushed to zero so the be adjacent. 1c. vlv Epenthesis CVC): C&apos;s are pushed apart. v[ Iv c[ lc c[ ic lc In order to allow adjacency of the surface consonants in (i), as expected by assimilation processes (and encouraged by a high-ranked constraint), note that the underlying vowel must be allowed to have zero width—an option available to to input but not output constituents. The input representation must specify only v[ &lt; ]v, not ]v. Similarly, to allow (ii), the input representamust specify only 1c, c.,[, not c2[. 315 ( and a, and ... ) or or ... ) = number of sets of ...} of types respectively that all overlap on the timeline and whose intersection does not overlap any event type .... ( and a2 and ... ) 1 (/3iand and ... = number of sets events {Ai, A2, • • • types a2, • • • • • • respectively that all overlap on the timeline. (Could also be notated: ai az -L • • • -I- J- ,82 • • ••) (i31and/32)[cf. a —&gt; &gt;&gt; a — or a2 ) 13 [cf. —* The unifying theme is that each primitive constraint counts the number of times a candidate gets some bad This is an interval on the timeline throughout which certain events (one or more specified edges or interiors) are all present and certain other events (zero or more specified edges or interiors) are all absent. Several examples of phonologically plausible constraints, with monikers and descriptions, are given below. (Eisner, 1997) shows how to rewrite hundreds of constraints from the literature in the primitive constraint notation, and discusses the problematic case of reduplication. (Eisner, in press) gives a detailed stress typology using only primitive constraints; in particular, non-local constraints such as FTBIN, FooTFoRm, and Generalized Alignment 1993) are eliminated. a. ONSET: syllable with a consonant.&amp;quot; NONFINALITY: &amp;quot;The end of a word may not be footed.&amp;quot; FTSYL: &amp;quot;Feet start and end on syllable boundaries.&amp;quot; PACKFEET: F[ &amp;quot;Each foot is followed immediately by another foot; i.e., minimize the number of gaps between feet. Note that the final foot, if any, will always violate this constraint.&amp;quot; x[ &amp;quot;Two stress marks may not be adjacent.&amp;quot; f. PROGRESSIVEVOICING: ]voiJ C[ &amp;quot;If the segment preceding a consonant is voiced, voicing may not stop prior to the a sequence does alter the meaning slightly. To get the exact original meaning, we would have to decompose into so-called &amp;quot;unranked&amp;quot; constraints, whereby (R) defined as C,, such ties undermine OT&apos;s idea of strict ranking: they confer the power to minimize linear functions such as (C1 -I- Ci C3 + C3 )( 3 Ci reason, OTP currently disallows unranked constraints; I know of no linguistic data that crucially require them. consonant but must be spread onto it.&amp;quot; voi &amp;quot;Every nasal gesture must be at least partly voiced.&amp;quot; FuLLNAsVoi: 1..vod J.. ]voi &amp;quot;A nasal gesture may not be only partly voiced.&amp;quot; i. —Ax M ( i) Or PARSE( V01): &amp;quot;Underlying voicing features surface.&amp;quot; or FILL(voi): &amp;quot;Voicing features appear on the surface only if they are also underlying.&amp;quot; NoSPREADRIGHT(voi): voicing may spread rightward as in (6).&amp;quot; NONDEGENERATE: &amp;quot;Every foot must cross at least one mora boundary „,[ TAUTOMORPHEMICFOOT: J orph[ &amp;quot;No foot may cross a morpheme boundary.&amp;quot; 3 Finite-state generation in OTP 3.1 A simple generation algorithm Recall that the generation problem is to find the set where . a. = Gen(input) Repns = C Since in OTP, the input is a partial order of edge and is a set of one or more total orders (timelines), a natural approach is to successively refine a partial order. This has merit. However, not can be represented as a single partial order, so the approach is quickly complicated by the need to encode disjunction. simpler approach is to represent (as well a finite-state automaton (FSA), denoting a regular set of strings that encode timelines. The idea is essentially due to (Ellison, 1994), and can be boiled down to two lines: (14) Ellison&apos;s algorithm (variant). Repns all conceivable outputs containing = constraint must be formulated as an edge- FSA scores candidates: any a single path of weight Best- Paths is Dijkstra&apos;s &amp;quot;single-source shortest paths&amp;quot; algorithm, a dynamic-programming algorithm that prunes away all but the minimum-weight paths in an automaton, leaving an unweighted automaton. OTP is simple enough that it can be described in this way. The next section gives a nice encoding. &apos;Weighted versions of the state-labeled finite auof (Bird 1994) could be used instead. g. J. 316 3.2 OTP with automata We may encode each timeline as a string over an alphabet E. If &apos;Tiers&apos; = each symbol in E is a k-tuple, whose components describe what is happening on the various tiers at a given moment. The components are drawn from a smaller alphabet A = { C, 1. I , +, -}. Thus at any time, the ith tier may be beginning or ending a constituent ( C, 1) or both at once ( I ), or it may be in a steady state in the interior or exterior of a constituent (+, -). At a minimum, the string must record all moments where there is an edge on some tier. If all tiers are in a steady state, the string need not use any symbols to say so. Thus the string encoding is not unique. gives an expression for that correctly describe the single tier shown. (16) describes a two-tier timeline consistent with (15). Note that the brackets on the two tiers are ordered with respect to each other. Timelines like these could be assembled morphologically from one or more lexical (Bird 1994), or produced in the course of algorithm (14). .[ Ir +*] XIX Y [ IY [ 0(+,+)*(1,+)(+,+)* We store timeline expressions like (16) as deterministic FSAs. To reduce the size of these automata, it is convenient to label arcs not with individual elements of E (which is huge) but with subsets of E, denoted by predicates. We use conjunctive predicates where each conjunct lists the allowed symbols on a given tier: +F, I +-voi label w/ 3 conjuncts) arc label in (17) is said to tiers a predicate allows any symbol from z. on the tiers it does not mention. The input FSA constrains only the input tiers. In we intersect it with constrains the output tiers. defined as the intersection of many automata exactly like (18), called rules, ensure that brackets are properly on a given tier such as Like the tier rules, the constraint automata Ci are small and deterministic and can be built automatically. Every edge has weight 0 or 1. With some it is possible to draw each two or fewer states, and with a number of arcs proportional to the number of tiers mentioned by the constraint. Keeping the constraints small is important for efficiency, since real languages have many constraints that must be intersected. Let us do the hardest case first. An implication constraint has the general form (9). Suppose that all the ai are interiors, not edges. Then the constraint intervals of the form a = fl n • Each such an interval ends without any having occurred during it, one violation is counted: arcs are shown in bold; others are weight-O. (other) A candidate that does see a I3 during an a can go and rest in the right-hand state for the duration of the a. Let us fill in the details of (19). How do we detect the end of an a&apos;? Because one or more of the ai end (1, I), while all the ai either end or continue (+), so we know we are leaving an Thus: (20) (in all ai) - (some bj) or end a al - (in all ai) gma in or end all ai) in all ai (in all ai) &amp; (some bj) (in or end all ai) - (in all ai) An unusually complex example is shown in (21). Note that to preserve the form of the predicates in (17) and keep the automaton deterministic, we need to split some of the arcs above into multiarcs. Each its own arc, and we must also expand set differences into multiple arcs, using scheme = important to take], not +, as our indication that we have been inside a constituent. This means that the timeline ( [, -)(+, -)*(+, [)(+, +)*(], +)(-, +)*(_,]) cannot avoid violating a clash constraint simply by instantiating the (+,+)* part as e. Furthermore, the ] convention means that a zero-width input constituent (more precisely, a sequence of zero-width constituents, represented as a single I symbol) will often act as if it has an interior. Thus if V syncopates as in footnote 2, it still violates the constraint This is an explicit property of OTP: otherwise, nothing that failed to parse would ever violate PARSE, because it would be gone! On the other hand, ] does not have this special role on the right hand side of —■ , which does not quantify universally over an interval. The consequence for zerowidth consituents is that even if a zero-width V overlaps the edge, say) with a surface latter cannot on this basis alone to satisfy FILL: V — too seems like the right move linguistically, although further study is needed. 317 ( (b ) +p +q How about other cases? If the antecedent of implication is interval, then the constraint needs only one state, to penalize mowhen the antecedent holds and the consequent does not. Finally, a clash constraint 1 1 • • • is identical to the implication ( al and 02 and • • • ) Clash FSAs are therefore just degenerate versions of im- FSAs, where the arcs looking for do not exist because they would accept no symbol. (22) the constraints ( ) (22) +p +q 01-b + 11q 4 Computational requirements 4.1 Generalized Alignment is not finite-state method can succeed a restricted formalism such as OTP, which does not admit such constraints as the popular Generalized Alignment (GA) family of (McCarthy &amp; Prince, 1993). A typ- GA constraint is ALIGN(F, L, which sums the number of syllables between each left foot the left edge of the prosodic word. Minimizing this sum achieves a kind of left-to-right iterative footing. OTP argues that such non-local, arithmetic constraints can generally be eliminated in favor of simpler mechanisms (Eisner, in press). Ellison&apos;s method cannot directly express the above GA constraint, even outside OTP, because it cannot compute a quadratic function 0 + 2 + 4 + • • on a like olF • • • . weights in an FSA cannot be more than linear on string length. Perhaps the filtering operation of any GA constraint can be simulated with a system of finitestate constraints? No: GA is simply too powerful. The proof is suppressed here for reasons of space, but it relies on a form of the pumping lemma for weighted FSAs. The key insight is that among candidates with a fixed number of syllables and a single tone, H, L) candidates the tone docks at the similar argufor weighted CFGs (using shows this constraint to be too hard even for (Tesar, 1996). 4.2 Generation is NP-complete even in OTP When algorithm (14) is implemented literally and with moderate care, using an optimizing C compiler on a 167MHz UltraSPARC, it takes fully 3.5 minutes time) to discover a stress pattern for the syllable sequence The automata become impractically huge due to intersections. Much of the explosion in this case is introduced the start and can be avoided. Because 21Tiersi = states, So, S, and have about 5000 states and 500,000 to 775,000 arcs. the automata become smaller, thanks to the pruning performed at each step by BestPaths. This repeated pruning is already an improvement over Ellison&apos;s original algorithm (which saves pruning till the end, and so continues to grow exponentially with every new constraint). If we modify (14) so that each tier rule from intersected with the candidate set only when its tier is first mentioned by a constraint, then the automata are pruned back as quickly as they grow. They have about 10 times fewer states and 100 times fewer arcs. and the generation time drops to 2.2 seconds. This is a key practical trick. But neither it nor any other trick can help for all grammars, for in the worst case, the OTP generation problem is NP-hard on the number of tiers used by the grammar. The locality of constraints does not save us here. Many NP-complete problems, such as graph coloring or bin packing, attempt to minimize some global count subject to numerous local restrictions. In the case of OTP generation, the global count to minimize is the of violation of and the local restrictions are imposed by CI, C2, • • • Ci- • of NP-hardness polytime reduction Hamilton Path). Given = (V(G), E(G)), n-vertex directed graph. Put = {Stem,. the following vector of primitive constraints (ordered as shown): a. Vv Vv E Vv d. Stem.1 S V u, v s.t. uv E(G): b, 1 „[ f • Is s[ grammar is taken from the OTP stress typology proposed by (Eisner, in press). It has tier rules for 9 and then spends constraints obvious univerproperties of moras and syllables, followed by conuniversal properties of feet and stress marks 6 substantive that can be freely reranked to yield different stress systems, such as left-toright iambs with iambic lengthening. 318 the input is simply [Stem].Filtering constraints (23a—d), we are left just those candidates where Stembears n (disconstituents of type coextensive with constituent bearing a different label v E (These candidates satisfy (23a—c) but violate (23d) n times.) (23e) says that a chain of abutting constituents [uIvItv] • • is allowed only if it corresponds a path in (23f) forces the grammar to minimize the number of such chains. If the minimum is 1 (i.e., an arbitrarily selected output candidate vi- (23f) only once), then a Hamilton path. When confronted with this pathological case, the methods respond essentially by enumerall possible permutations of with sharing of prefixes). The machine state stores, other things, the subset of has albeen seen; so there are at least states. It must be emphasized that if the grammar is in advance, (14) is close to linear in the size of the input form: it is dominated by a constant number of calls to Dijkstra&apos;s BestPaths method, each taking time Oainput arcsI log linput statesp. There are nonetheless three reasons why the above result is important. (a) It raises the pracspecter of huge constant factors (&gt; for real grammars. Even if a fixed grammar can somehow be compiled into a fast form for use with many inputs, the compilation itself will have to deal with this constant factor. (b) The result has the interesting implication that candidate sets can arise that cannot concisely represented with FSAs. For if all were polynomial-sized in (14), the algorithm would run in polynomial time. (c) Finally, the grammar in all circumstances: both linguists and children crucially experiment with different theories. 4.3 Work in progress: Factored automata The previous section gave a useful trick for speeding up Ellison&apos;s algorithm in the typical case. We are currently experimenting with additional improvements along the same lines, which attempt to defer intersection by keeping tiers separate as long as possible. idea is to represent the candidate set not as large unweighted FSA, but rather as a collection of preferably small unweighted FSAs, called factors, each of which mentions as few tiers as possible. This collection, called a factored automaton, serves as compact representation of usually has far states than if the intersection were carried out. instance, the natural factors of are and all the tier rules (see 18). This requires only + not • linputp. Using factored automata helps Ellison&apos;s algorithm (14) in several ways: • The candidate sets Si tend to be represented more compactly. In (14), the constraint needs to be interwith only certain factors of • Sometimes Ci+i does not need to be intersected with the input, because they do not mention of the same tiers. Then step + can be performed in time independent of input length. = , is 43-state automaton, and is which says that every foot bears a stress mark. Then to find = BestPaths(So fl C1), we need only consider tier rules for x, which require well-formed feet and well-formed stress marks, and combine them with C1 to get a new factor that requires stressed feet. No other factors need be involved. key operation in (14) is to find Bestpaths(A A is an unweighted factored automaton an ordinary weighted FSA (a constraint). is the best intersection problem. For conlet us suppose that a two-state constraint. naive idea is simply to add to A as a new factor. However, this ignores the BestPaths we wish to keep just the best paths in X[ that are compatible with A. Such paths might be and include cycles in . For example, a weight-1 path would describe a chain of optimal stressed feet interrupted by a single unstressed one to block stress. corrected variant is to put = run on the pruned result be could add back to to a new factor, but it is large. We would rather add a smaller has the same effect, in that n B&apos; = n B. (B&apos; look something like the original but with some paths missing, some states split, and cycles unrolled.) Observe that each state of the form some / and &amp;quot;re-merging&amp;quot; states possible, using an approach similar to DFA minimization. Of course, this variant is not very efficient, because requires us to find and use = we really want is to follow the above idea but use a that considers just the relevant factors need not consider factors that will not the choice of paths in Various approaches are possible for choosing such following technique is completely general, though it may or may not be practical. Observe that for BestPaths to do the correct to reflect the sum total of A&apos;s conon x, the tiers that More we want be the projection of the canset just the x tiers. Unfortunately, these constraints are not just reflected in the mentioning x, since the allowed conof x may be mediated through 319 additional factors. As an example, there may be a mentioning some of whose paths are incompatible with the input factor, because the latter allows ik only in certain places or because only allows paths of length 14. Number the tiers such that x are numbered 0, and all other tiers have distinct positive numbers. Partition the factors of lists Lk, to the highest-numbered tier they mention. (Any factor that mentions no tiers at all goes onto Lo.) If = then return as our desired Otherwise, exhausts tier k&apos;s ability to mediate relations among the factors. Modify the labels of so that they no longer restrict add a determinized, miniversion of the result to to the highest-numbered tier it now mentions. Decrement return to step 3. this technique must per- — intersections, just as if we had put = it intersperses the intersections with determinization and minimization operations, so that the automata being intersected tend not be large. In the best case, we will have — 1 intersection-determinization-minimizations that 0(1) apiece, rather than —1 that up to 5 Conclusions Primitive Optimality Theory, or OTP, is an attempt to produce a a simple, rigorous, constraint-based model of phonology that is closely fitted to the needs of working linguists. I believe it is worth study both as a hypothesis about Universal Grammar and as a formal object. The present paper introduces the OTP formalization to the computational linguistics community. We have seen two formal results of interest, both having to do with generation of surface forms: OTP&apos;s power low: finite-state optimization. In particular it is more constrained than theories using Generalized Alignment. This is good news for comprehension and learning. OTP&apos;s complexity, generation, is nonetheless high: NP-complete on the size of the grammar. This is mildly unfortunate for OTP and for the OT approach in general. It remains true that for a fixed grammar, the time to do generation is close to linear on the size of the input (Ellison, 1994), which is heartening if we intend to optimize long utterances with respect to a fixed phonology. Finally, we have considered the prospect of building a practical tool to generate optimal outputs from OT theories. We saw above to set up the representations and constraints efficiently using deterministic finite-state automata, and how to remedy some hidden inefficiencies in the seminal work of (Ellison, 1994), achieving at least a 100-fold observed speedup. Delayed intersection and aggressive pruning prove to be important. Aggressive minimization and a more compact. &amp;quot;factored&amp;quot; representation of automata may also turn out to help.</abstract>
<note confidence="0.634950230769231">References Bird, Steven, &amp; T. Mark Ellison. One Level Phonology: Autosegmental representations and rules as automata. Linguistics Cole, Jennifer, &amp; Charles Kisseberth. 1994. An opdomains theory of harmony. Sciences Eisner, Jason. In press. Decomposing FootForm: Primitive constraints in OT. Proceedings of SCIL 8, NYU. Published by MIT Working Papers. (Available at http://ruccs.rutgers.edu/roa.html.) Eisner, Jason. What constraints should OT allow? Handout for talk at LSA, Chicago. (Available at</note>
<web confidence="0.960817">http://ruccs.rutgers.edu/roa.html.</web>
<note confidence="0.9329725625">Ellison, T. Mark. Phonological derivation in optimality theory. COLING &apos;94, 1007-1013. Goldsmith, John. 1976. Autosegmental phonology. Cambridge, Mass: MIT PhD. dissertation. Published 1979 by New York: Garland Press. Goldsmith, John. 1990. Autosegmental and metrical phonology. Oxford: Blackwell Publishers. McCarthy, John, &amp; Alan Prince. 1993. Generalalignment. of Morphology, Geert Booij &amp; Jaap van Marle, pp. 79-153. Kluwer. McCarthy, John and Alan Prince. 1995. Faithfulness and reduplicative identity. In Jill Beckman et al., eds., Papers in Optimality Theory. UMass. Amherst: GLSA. 259-384. Alan, &amp; Paul Smolensky. 1993. theory: constraint interaction in generative gram-</note>
<affiliation confidence="0.9808495">Reports of the Rutgers University Center for Cognitive Science.</affiliation>
<address confidence="0.930425">Selkirk, Elizabeth. 1980. Prosodic domains in</address>
<affiliation confidence="0.447171">phonology: Sanskrit revisited. In Mark Aranoff</affiliation>
<address confidence="0.7991402">Mary-Louise Kean, eds., 107- 129. Anna Libri, Saratoga, CA. Tesar, Bruce. 1995. Computational Optimality Theory. Ph.D. dissertation, U. of Colorado, Boulder. Tesar, Bruce. 1996. Computing optimal descriptions</address>
<abstract confidence="0.821037666666667">for Optimality Theory: Grammars with contextfree position structures. Proceedings of the 34th Annual Meeting of the ACL.</abstract>
<intro confidence="0.592242">320</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Steven Bird</author>
<author>T Mark Ellison</author>
</authors>
<title>One Level Phonology: Autosegmental representations and rules as finite automata.</title>
<journal>Comp. Linguistics</journal>
<pages>20--55</pages>
<marker>Bird, Ellison, </marker>
<rawString>Bird, Steven, &amp; T. Mark Ellison. One Level Phonology: Autosegmental representations and rules as finite automata. Comp. Linguistics 20:55-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Cole</author>
<author>Charles Kisseberth</author>
</authors>
<title>An optimal domains theory of harmony.</title>
<date>1994</date>
<journal>Studies in the Linguistic Sciences</journal>
<volume>24</volume>
<pages>2</pages>
<marker>Cole, Kisseberth, 1994</marker>
<rawString>Cole, Jennifer, &amp; Charles Kisseberth. 1994. An optimal domains theory of harmony. Studies in the Linguistic Sciences 24: 2.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jason Eisner</author>
</authors>
<title>In press. Decomposing FootForm: Primitive constraints in OT.</title>
<booktitle>Proceedings of SCIL 8, NYU.</booktitle>
<note>Published by MIT Working Papers. (Available at http://ruccs.rutgers.edu/roa.html.)</note>
<marker>Eisner, </marker>
<rawString>Eisner, Jason. In press. Decomposing FootForm: Primitive constraints in OT. Proceedings of SCIL 8, NYU. Published by MIT Working Papers. (Available at http://ruccs.rutgers.edu/roa.html.)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jason Eisner</author>
</authors>
<title>What constraints should OT allow? Handout for talk at LSA,</title>
<location>Chicago.</location>
<note>(Available at http://ruccs.rutgers.edu/roa.html.)</note>
<marker>Eisner, </marker>
<rawString>Eisner, Jason. What constraints should OT allow? Handout for talk at LSA, Chicago. (Available at http://ruccs.rutgers.edu/roa.html.)</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Mark Ellison</author>
</authors>
<title>Phonological derivation in optimality theory.</title>
<journal>COLING</journal>
<volume>94</volume>
<pages>1007--1013</pages>
<marker>Ellison, </marker>
<rawString>Ellison, T. Mark. Phonological derivation in optimality theory. COLING &apos;94, 1007-1013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Autosegmental phonology.</title>
<date>1976</date>
<publisher>MIT</publisher>
<location>Cambridge, Mass:</location>
<note>PhD. dissertation. Published</note>
<marker>Goldsmith, 1976</marker>
<rawString>Goldsmith, John. 1976. Autosegmental phonology. Cambridge, Mass: MIT PhD. dissertation. Published 1979 by New York: Garland Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Autosegmental and metrical phonology.</title>
<date>1990</date>
<publisher>Blackwell Publishers.</publisher>
<location>Oxford:</location>
<marker>Goldsmith, 1990</marker>
<rawString>Goldsmith, John. 1990. Autosegmental and metrical phonology. Oxford: Blackwell Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John McCarthy</author>
<author>Alan Prince</author>
</authors>
<date>1993</date>
<booktitle>Generalized alignment. Yearbook of Morphology, ed. Geert Booij &amp; Jaap van Marle,</booktitle>
<pages>79--153</pages>
<publisher>Kluwer.</publisher>
<contexts>
<context position="23699" citStr="McCarthy &amp; Prince, 1993" startWordPosition="4084" endWordPosition="4087">quent does not. Finally, a clash constraint al 1 a2 1 • • • is identical to the implication constraint ( al and 02 and • • • ) FALSE. Clash FSAs are therefore just degenerate versions of implication FSAs, where the arcs looking for i3j do not exist because they would accept no symbol. (22) shows the constraints ( p and k ) b and p J. q. (22) +p +q 01-b + 11q 4 Computational requirements 4.1 Generalized Alignment is not finite-state Ellison&apos;s method can succeed only on a restricted formalism such as OTP, which does not admit such constraints as the popular Generalized Alignment (GA) family of (McCarthy &amp; Prince, 1993). A typical GA constraint is ALIGN(F, L, Word, L), which sums the number of syllables between each left foot edge F[ and the left edge of the prosodic word. Minimizing this sum achieves a kind of left-to-right iterative footing. OTP argues that such non-local, arithmetic constraints can generally be eliminated in favor of simpler mechanisms (Eisner, in press). Ellison&apos;s method cannot directly express the above GA constraint, even outside OTP, because it cannot compute a quadratic function 0 + 2 + 4 + • • on a string like [bo-)F Vroll- olF • • • . Path weights in an FSA cannot be more than line</context>
</contexts>
<marker>McCarthy, Prince, 1993</marker>
<rawString>McCarthy, John, &amp; Alan Prince. 1993. Generalized alignment. Yearbook of Morphology, ed. Geert Booij &amp; Jaap van Marle, pp. 79-153. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John McCarthy</author>
<author>Alan Prince</author>
</authors>
<title>Faithfulness and reduplicative identity.</title>
<date>1995</date>
<booktitle>Papers in Optimality Theory. UMass. Amherst: GLSA.</booktitle>
<pages>259--384</pages>
<editor>In Jill Beckman et al., eds.,</editor>
<contexts>
<context position="10872" citStr="McCarthy &amp; Prince, 1995" startWordPosition="1764" endWordPosition="1767">nas[ and ]r,a, , while underlying nasal autosegments are bracketed with and Ina, . The underlining is a notational convention to denote input material. No connection is required between [nas] and has] except as enforced by constraints that prefer [rias] and [nas] or their edges to overlap in some way. (6) shows a candidate in which underlying [nasi has surfaced &amp;quot;in place&amp;quot; but with rightward spreading. (6) nas[ Inas nas[ ]nas Here the left edges and interiors overlap, but the right edges fail to. Such overlap of interiors may be regarded as featural Input-Output Correspondence in the sense of (McCarthy &amp; Prince, 1995). The lexicon and morphology supply to Gen an underspecified timeline—a partially ordered collection of input edges. The use of a partial ordering allows the lexicon and morphology to supply floating tones, floating morphemes and templatic morphemes. Given such an underspecified timeline as lexical input, Gen outputs the set of all fully specified timelines that are consistent with it. No new input constituents may be added. In essence, Gen generates every way of refining the partial order of input constituents into a total order and decorating it freely with output constituents. Conditions su</context>
</contexts>
<marker>McCarthy, Prince, 1995</marker>
<rawString>McCarthy, John and Alan Prince. 1995. Faithfulness and reduplicative identity. In Jill Beckman et al., eds., Papers in Optimality Theory. UMass. Amherst: GLSA. 259-384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Prince</author>
<author>Paul Smolensky</author>
</authors>
<title>Optimality theory: constraint interaction in generative grammar.</title>
<date>1993</date>
<booktitle>Technical Reports of the</booktitle>
<institution>Rutgers University Center for Cognitive Science.</institution>
<contexts>
<context position="9899" citStr="Prince &amp; Smolensky, 1993" startWordPosition="1605" endWordPosition="1609"> that only horizontal order matters. Horizontal spacing and vertical order are irrelevant. Thus, a timeline may be represented as a finite collection S of labeled edge brackets, equipped with ordering relations -&lt; and that indicate which brackets precede each other or fall in the same place. Valid timelines (those in Repns) also require that edge brackets come in matching pairs, that constituents have positive width, and that constituents of the same type do not overlap (i.e., two constituents on the same tier may not be linked). 2.2 Gen: Input and output in OTP OT&apos;s principle of Containment (Prince &amp; Smolensky, 1993) says that each of the potential outputs in Repns includes a silent copy of the input, so that constraints evaluating it can consider the goodness of match between input and output. Accordingly, OTP represents both input and output constituents on the constituent timeline, but on different tiers. Thus surface nasal autosegments are bracketed with nas[ and ]r,a, , while underlying nasal autosegments are bracketed with and Ina, . The underlining is a notational convention to denote input material. No connection is required between [nas] and has] except as enforced by constraints that prefer [ria</context>
</contexts>
<marker>Prince, Smolensky, 1993</marker>
<rawString>Prince, Alan, &amp; Paul Smolensky. 1993. Optimality theory: constraint interaction in generative grammar. Technical Reports of the Rutgers University Center for Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Selkirk</author>
</authors>
<title>Prosodic domains in phonology: Sanskrit revisited.</title>
<date>1980</date>
<booktitle>In Mark Aranoff and Mary-Louise Kean, eds., Juncture,</booktitle>
<pages>107--129</pages>
<location>Anna Libri, Saratoga, CA.</location>
<contexts>
<context position="11516" citStr="Selkirk, 1980" startWordPosition="1870" endWordPosition="1871">upply to Gen an underspecified timeline—a partially ordered collection of input edges. The use of a partial ordering allows the lexicon and morphology to supply floating tones, floating morphemes and templatic morphemes. Given such an underspecified timeline as lexical input, Gen outputs the set of all fully specified timelines that are consistent with it. No new input constituents may be added. In essence, Gen generates every way of refining the partial order of input constituents into a total order and decorating it freely with output constituents. Conditions such as the prosodic hierarchy (Selkirk, 1980) are enforced by universally high-ranked constraints, not by Gen.&apos; 2.3 Con: The primitive constraints Having described the representations used, it is now possible to describe the constraints that evaluate them. OTP claims that Con is restricted to the following two families of primitive constraints: 0 (&amp;quot;implication&amp;quot;): &amp;quot;Each a temporally overlaps some g.&amp;quot; Scoring: Constraint(R) z-- number of a&apos;s in R that do not overlap any 0. (8) a 1 0 (&amp;quot;clash&amp;quot;): &amp;quot;Each a temporally overlaps no ,(3.&amp;quot; Scoring: Constraint(R) = number of (a,/3) pairs in R such that the a overlaps the 0. That is, a 0 says that a&apos;s</context>
</contexts>
<marker>Selkirk, 1980</marker>
<rawString>Selkirk, Elizabeth. 1980. Prosodic domains in phonology: Sanskrit revisited. In Mark Aranoff and Mary-Louise Kean, eds., Juncture, pp. 107-129. Anna Libri, Saratoga, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Tesar</author>
</authors>
<title>Computational Optimality Theory.</title>
<date>1995</date>
<institution>U. of Colorado,</institution>
<location>Boulder.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="4793" citStr="Tesar (1995" startWordPosition="771" endWordPosition="772">ers must solve this. • Learning. How to induce a lexicon and a phonology like (1) for a particular language. given the kind of evidence available to child language learners? None of these questions is well-posed without restrictions on Gen and Con. In the absence of such restrictions, computational linguists have assumed convenient ones. Ellison (1994) solves the generation problem where Gen produces a regular set of strings and Con admits all finite state transducers that can map a string to a number in unary notation. (Thus Ci(R) = 4 if the Ci transducer outputs the string 1111 on input R.) Tesar (1995. 1996) extends this result to the case where Gen( input) is the set of parse trees for input under some context-free grammar (CFG).1 Tesar&apos;s constraints are functions on parse trees such that Ci({A [B1 • • .] [B2 .]]) can be computed from A, B1, B2, C(Bl), and Ci(B2). The optimal tree can then be found with a standard dynamic-programming chart parser for weighted CFGs. It is an important question whether these formalisms are useful in practice. On the one hand, are they expressive enough to describe real languages? On the other, are they restrictive enough to admit good comprehension and unsu</context>
</contexts>
<marker>Tesar, 1995</marker>
<rawString>Tesar, Bruce. 1995. Computational Optimality Theory. Ph.D. dissertation, U. of Colorado, Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Tesar</author>
</authors>
<title>Computing optimal descriptions for Optimality Theory: Grammars with contextfree position structures.</title>
<date>1996</date>
<booktitle>Proceedings of the 34th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="24870" citStr="Tesar, 1996" startWordPosition="4298" endWordPosition="4299">eights in an FSA cannot be more than linear on string length. Perhaps the filtering operation of any GA constraint can be simulated with a system of finitestate constraints? No: GA is simply too powerful. The proof is suppressed here for reasons of space, but it relies on a form of the pumping lemma for weighted FSAs. The key insight is that among candidates with a fixed number of syllables and a single (floating) tone, ALIGN(cr, L, H, L) prefers candidates where the tone docks at the center. A similar argument for weighted CFGs (using two tones) shows this constraint to be too hard even for (Tesar, 1996). 4.2 Generation is NP-complete even in OTP When algorithm (14) is implemented literally and with moderate care, using an optimizing C compiler on a 167MHz UltraSPARC, it takes fully 3.5 minutes (real time) to discover a stress pattern for the syllable sequence The automata become impractically huge due to intersections. Much of the explosion in this case is introduced at the start and can be avoided. Because Repns has 21Tiersi = 512 states, So, S, and S., each have about 5000 states and 500,000 to 775,000 arcs. Thereafter the Si automata become smaller, thanks to the pruning performed at each</context>
</contexts>
<marker>Tesar, 1996</marker>
<rawString>Tesar, Bruce. 1996. Computing optimal descriptions for Optimality Theory: Grammars with contextfree position structures. Proceedings of the 34th Annual Meeting of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>