<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001286">
<title confidence="0.998896">
Modified Distortion Matrices
for Phrase-Based Statistical Machine Translation
</title>
<author confidence="0.809343">
Arianna Bisazza and Marcello Federico
</author>
<affiliation confidence="0.744075">
Fondazione Bruno Kessler
</affiliation>
<address confidence="0.5791">
Trento, Italy
</address>
<email confidence="0.997108">
{bisazza,federico}@fbk.eu
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9988436875">
This paper presents a novel method to suggest
long word reorderings to a phrase-based SMT
decoder. We address language pairs where
long reordering concentrates on few patterns,
and use fuzzy chunk-based rules to predict
likely reorderings for these phenomena. Then
we use reordered n-gram LMs to rank the re-
sulting permutations and select the n-best for
translation. Finally we encode these reorder-
ings by modifying selected entries of the dis-
tortion cost matrix, on a per-sentence basis.
In this way, we expand the search space by a
much finer degree than if we simply raised the
distortion limit. The proposed techniques are
tested on Arabic-English and German-English
using well-known SMT benchmarks.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997114893617">
Despite the large research effort devoted to the mod-
eling of word reordering, this remains one of the
main obstacles to the development of accurate SMT
systems for many language pairs. On one hand, the
phrase-based approach (PSMT) (Och, 2002; Zens et
al., 2002; Koehn et al., 2003), with its shallow and
loose modeling of linguistic equivalences, appears
as the most competitive choice for closely related
language pairs with similar clause structures, both
in terms of quality and of efficiency. On the other,
tree-based approaches (Wu, 1997; Yamada, 2002;
Chiang, 2005) gain advantage, at the cost of higher
complexity and isomorphism assumptions, on lan-
guage pairs with radically different word orders.
Lying between these two extremes are language
pairs where most of the reordering happens locally,
and where long reorderings can be isolated and de-
scribed by a handful of linguistic rules. Notable
examples are the family-unrelated Arabic-English
and the related German-English language pairs. In-
terestingly, on these pairs, PSMT generally pre-
vails over tree-based SMT1, producing overall high-
quality outputs and isolated but critical reordering
errors that undermine the global sentence meaning.
Previous works on this type of language pairs have
mostly focused on source reordering prior to trans-
lation (Xia and McCord, 2004; Collins et al., 2005),
or on sophisticated reordering models integrated into
decoding (Koehn et al., 2005; Al-Onaizan and Pap-
ineni, 2006), achieving mixed results. To merge the
best of both approaches – namely, access to rich con-
text in the former and natural coupling of reorder-
ing and translation decisions in the latter – we intro-
duce modified distortion matrices: a novel method to
seamlessly provide to the decoder a set of likely long
reorderings pre-computed for a given input sentence.
Added to the usual space of local permutations de-
fined by a low distortion limit (DL), this results in a
linguistically informed definition of the search space
that simplifies the task of the in-decoder reordering
model, besides decreasing its complexity.
The paper is organized as follows. After review-
ing a selection of relevant works, we analyze salient
reordering patterns in Arabic-English and German-
English, and describe the corresponding chunk-
based reordering rule sets. In the following sections
we present a reordering selection technique based on
</bodyText>
<footnote confidence="0.992421666666667">
1A good comparison of phrase-based and tree-based ap-
proaches across language pairs with different reordering levels
can be found in (Zollmann et al., 2008).
</footnote>
<page confidence="0.886507">
478
</page>
<note confidence="0.9864015">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 478–487,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998896">
reordered n-gram LMs and, finally, explain the no-
tion of modified distortion matrices. In the last part
of the paper, we evaluate the proposed techniques on
two popular MT tasks.
</bodyText>
<sectionHeader confidence="0.984382" genericHeader="introduction">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999910425287357">
Pre-processing approaches to word reordering aim
at permuting input words in a way that minimizes
the reordering needed for translation: determinis-
tic reordering aims at finding a single optimal re-
ordering for each input sentence, which is then
translated monotonically (Xia and McCord, 2004)
or with a low DL (Collins et al., 2005; Habash,
2007); non-deterministic reordering encodes mul-
tiple alternative reorderings into a word lattice and
lets a monotonic decoder find the best path accord-
ing to its models (Zhang et al., 2007; Crego and
Habash, 2008; Elming and Habash, 2009; Niehues
and Kolss, 2009). The latter approaches are ideally
conceived as alternative to in-decoding reordering,
and therefore require an exhaustive reordering rule
set. Two recent works (Bisazza and Federico, 2010;
Andreas et al., 2011) opt instead for a hybrid way:
rules are used to generate multiple likely reorder-
ings, but only for a specific phenomenon – namely
verb-initial clauses in Arabic. This yields sparse re-
ordering lattices that can be translated with a regular
decoder performing additional reordering.
Reordering rules for pre-processing are either
manually written (Collins et al., 2005) or automat-
ically learned from syntactic parses (Xia and Mc-
Cord, 2004; Habash, 2007; Elming and Habash,
2009), shallow syntax chunks (Zhang et al., 2007;
Crego and Habash, 2008) or part-of-speech labels
(Niehues and Kolss, 2009). Similarly to hybrid ap-
proaches, in this work we use few linguistically in-
formed rules to generate multiple reorderings for se-
lected phenomena but, as a difference, we do not
employ lattices to represent them. We also include a
competitive in-decoding reordering model in all the
systems used to evaluate our methods.
Another large body of work is devoted to the mod-
eling of reordering decisions inside decoding, based
on a decomposition of the problem into a sequence
of basic reordering steps. Existing approaches range
from basic linear distortion to more complex models
that are conditioned on the words being translated.
The linear distortion model (Koehn et al., 2003)
encourages monotonic translations by penalizing
source position jumps proportionally to their length.
If used alone, this model is inadequate for language
pairs with different word orders. Green et al. (2010)
tried to improve it with a future distortion cost es-
timate. Thus they were able to preserve baseline
performance at a very high DL, but not to improve
it. Lexicalized phrase orientation models (Tillmann,
2004; Koehn et al., 2005; Zens and Ney, 2006; Gal-
ley and Manning, 2008) predict the orientation of a
phrase with respect to the last translated one. These
models are known to well handle local reordering
and are widely adopted by the PSMT community.
However, they are unsuitable to model long reorder-
ing as they classify as “discontinuous” every phrase
that does not immediately follow or precede the last
translated one. Lexicalized distortion models pre-
dict the jump from the last translated word to the
next one, with a class for each possible jump length
(Al-Onaizan and Papineni, 2006), or bin of lengths
(Green et al., 2010). These models are conceived to
deal with long reordering, but can easily suffer from
data sparseness, especially for longer jumps occur-
ring less frequently.
Following a typical sequence modeling approach,
Feng et al. (2010) train n-gram language models on
source data previously reordered in accordance to
the target language translation. This method does
not directly model reordering decisions, but rather
word sequences produced by them. Despite their
high perplexities, reordered LMs yield some im-
provements when integrated to a PSMT baseline that
already includes a discriminative phrase orientation
model (Zens and Ney, 2006). In this work we use
similar models to rank sets of chunk permutations.
Attempting to improve the reordering space def-
inition, Yahyaei and Monz (2010) train a classifier
to guess the most likely jump length at each source
position, then use its predictions to dynamically set
the DL. Translation improvements are obtained on a
simple task with mostly short sentences (BTEC).
Modifying the distortion function, as proposed in
this paper, makes it possible to expand the pemuta-
tion search space by a much finer degree than vary-
ing the DL does.
</bodyText>
<page confidence="0.999449">
479
</page>
<sectionHeader confidence="0.94454" genericHeader="method">
3 Long reordering patterns
</sectionHeader>
<bodyText confidence="0.99998998">
Our study focuses on Arabic-English and German-
English: two language pairs characterized by uneven
distributions of word-reordering phenomena, with
long-range movements concentrating on few pat-
terns. In Arabic-English, the internal order of most
noun phrases needs to be reversed during translation,
which is generally well handled by phrase-internal
reordering or local distortion. At the constituent
level, instead, Arabic admits both SV(O) and VS(O)
orders, the latter causing problematic long reorder-
ings. Common errors due to this issue are the ab-
sence of main verb in the English translation, or the
placement of the main verb before its own subject.
In both cases, adequacy is seriously compromised.
In German-English, the noun phrase structure is
similar between source and target languages. How-
ever, at the constituent level, the verb-second order
of German main clauses conflicts with the rigid SVO
structure of English, as does the clause-final verb
position of German subordinate clauses. As a fur-
ther complication, German compound verbs are split
apart so that the non-finite element (main verb) can
appear long after the inflected auxiliary or modal.
Thanks to sophisticated reordering models, state-
of-the-art PSMT systems are generally good at han-
dling local reordering phenomena that are not cap-
tured by phrase-internal reordering. However, they
typically fail to predict long reorderings. We believe
this is mainly not the fault of the reordering mod-
els, but rather of a too coarse definition of the search
space. To have a concrete idea, consider that a small
change of the DL from 5 to 6 words, in a sentence
of 8, makes the number of explorable permutations
increase from about 9,000 to 22,000. Existing mod-
els cannot be powerful enough to deal with such a
rapidly growing search space.
As a result, decoding at very high DLs is not
a good solution for these language pairs. Indeed,
decent performances are obtained within a low or
medium DL, but this obviously comes at the expense
of long reorderings, which are often crucial to pre-
serve the general meaning of a translated sentence.
For instance, taking English as the target language,
it is precisely the relative positioning of predicate ar-
guments that determines their role, in the absence of
case markers. Thus, a wrongly reordered verb with
minor impact on automatic scores, can be judged
very badly by a human evaluator.
We will now describe two rule sets aimed at cap-
turing these reordering phenomena.
</bodyText>
<sectionHeader confidence="0.90219" genericHeader="method">
4 Shallow syntax reordering rules
</sectionHeader>
<bodyText confidence="0.999893035714286">
To compute the source reorderings, we use chunk-
based rules following Bisazza and Federico (2010).
Shallow syntax chunking is indeed a lighter and
simpler task compared to full parsing, and it can
be used to constrain the number of reorderings in
a softer way. While rules based on full parses
are generally deterministic, chunk-based rules are
non-deterministic or fuzzy, as they generate sev-
eral permutations for each matching sequence2. Be-
sides defining a unique segmentation of the sen-
tence, chunk annotation provides other useful infor-
mation that can be used by the rules – namely chunk
type and POS tags3.
For Arabic-English we apply the rules proposed
by Bisazza and Federico (2010) aimed at transform-
ing VS(O) sentences into SV(O). Reorderings are
generated by moving each verb chunk (VC), alone
or with its following chunk, by 1 to 6 chunks to the
right. The maximum movement of each VC is lim-
ited to the position of the next VC, so that neigh-
boring verb-reordering sequences may not overlap.
This rule set was shown to cover most (99.5%) of
the verb reorderings observed in a parallel news cor-
pus, including those where the verb must be moved
along with an adverbial or a complement.
For German-English we propose a set of three
rules4 aimed at arranging the German constituents
in SVO order:
</bodyText>
<listItem confidence="0.998105333333333">
• infinitive: move each infinitive VC right after a
preceding punctuation;
• subordinate: if a VC is immediately followed
by a punctuation, place it after a preceding sub-
ordinating conjunction (KOUS) or substitutive
relative pronoun (PRELS);
</listItem>
<footnote confidence="0.940868428571428">
2Chunk annotation does not identify subject and comple-
ment boundaries, nor the relations among constituents that are
needed to deterministically rearrange a sentence in SVO order.
3We use AMIRA (Diab et al., 2004) to annotate Arabic and
Tree Tagger (Schmid, 1994) to annotate German.
4A similar rule set was previously used to produce chunk
reordering lattices in (Hardmeier et al., 2010).
</footnote>
<page confidence="0.995636">
480
</page>
<figure confidence="0.98263">
(b) German broken verb chunk: three permutations
</figure>
<figureCaption confidence="0.99483">
Figure 1: Examples of chunk permutations generated by shallow syntax reordering rules. Chunk types: CC conjunc-
tion, VC verb (auxiliary/past participle), PC preposition, NC noun, Pct punctuation.
</figureCaption>
<figure confidence="0.997904046875">
w-
and
CC1
Pct6
Pct6
&gt;0
Pct6
CC1
PC3
VC2
NC4
PC5
CC1
CC1
-89
PC3
NC4
VC2
PC5
98:7
PC3
NC4
=&gt;=?
?A&amp;quot;
PC5
4 VC2
=&gt;
CC1 NC4 VC2 PC3 PC5
D0F0734A4LFF =040
Pct6
CC1 NC4 PC5 VC2 PC3 &amp; Pct6
6&amp;quot;N** &amp;quot;FQ M% ORS =38
$Ark
took part
VC2
fy A(tZAhrp E$rAt AlmslHyn mn AlktA}b �
in the march dozens of militants from the Brigades
PC3 NC4 PC5 Pct6
(a) Arabic VS(O) clause: five permutations
M%&amp; 4=&gt;A4F4=040N%%--Q N-T*&amp;
*&amp;+
*&amp;+
&amp;quot;#$%&amp;&apos;
((%&amp;)
*&amp;,
-&amp;.
-/01
*&amp;+
*&amp;,
&amp;quot;#$%&amp;&apos;
-&amp;.
-/01
*&amp;+
*&amp;,
-&amp;.
((%&amp;)
-/01
C=4 5#6&amp;quot;(47048 D0&amp;quot;&amp;quot;0 &amp;quot;&gt;E&amp;quot;F07/3&amp;quot;G
The Budapest Pr.secut.r&apos;s Office
&amp;quot;#$%&amp;&apos; *&amp;, -&amp;. ((%&amp;) -/01
hat =384 H8I=JF#&gt;A4&gt; K#I %98L&amp;quot;FF eingeleitet B
has its investigati.n .n the accident initiated
((%&amp;)
&amp;quot;#$%&amp;&apos;
</figure>
<listItem confidence="0.960111833333333">
• broken verb chunk: join each finite VC (auxil-
iary or modal) with the nearest following non-
finite VC (infinitive or participle). Place the re-
sulting block in any position between the orig-
inal position of the finite verb and that of the
non-finite verb5.
</listItem>
<bodyText confidence="0.999019448275862">
The application of chunk reordering rules is illus-
trated by Fig. 1: in the Arabic sentence (a), the sub-
ject ‘dozens of militants’ is preceded by the main
verb ‘took part’ and its argument ‘to the march’. The
rules generate 5 permutations for one matching se-
quence (chunks 2 to 5), out of which the 5th is the
best for translation. The German sentence (b) con-
tains a broken VC with the inflected auxiliary ‘has’
separated from the past participle ‘initiated’. Here,
the rules generate 3 permutations for the chunk se-
quence 2 to 5, corresponding to likely locations of
the merged verb phrase, the 1st being optimal.
By construction, both rule sets generate a limited
number of permutations per matching sequence: in
5To bound the number of reorderings, we use the follow-
ing heuristics. In ‘infinitive’ at most 3 punctuations preceding
the VC are considered. In ‘subordinate’ 1 to 3 chunks are left
between the conjunction (or pronoun) and the moved VC to ac-
count for the subject. In ‘broken VC’ if the distance between the
finite and non-finite verb is more than 10 chunks, only the first
5 and last 5 positions of the verb-to-verb span are considered.
Arabic at most 12 for each VC; in German at most 3
for each infinitive VC and for each VC-punctuation
sequence, at most 10 for each broken VC. Empiri-
cally, this yields on average 22 reorderings per sen-
tence in the NIST-MT Arabic benchmark dev06-NW
and 3 on the WMT German benchmark test08. Ara-
bic rules are indeed more noisy, which is not surpris-
ing as reordering is triggered by any verb chunk.
</bodyText>
<sectionHeader confidence="0.985911" genericHeader="method">
5 Reordering selection
</sectionHeader>
<bodyText confidence="0.9997343125">
The number of chunk-based reorderings per sen-
tence varies according to the rule set, to the size of
chunks and to the context. A high degree of fuzzi-
ness can complicate the decoding process, leaving
too much work to the in-decoding reordering model.
A solution to this problem is using an external model
to score the rule-generated reorderings and discard
the less probable ones. In such a way, a further part
of reordering complexity is taken out of decoding.
At this end, instead of using a Support Vector Ma-
chine classifier as was done in (Bisazza et al., 2011),
we apply reordered n-gram models that are lighter-
weight and more suitable for a ranking task.
Differently from Feng et al. (2010), we train our
models on partially reordered data and at the level of
chunks. Chunks can be represented simply by their
</bodyText>
<page confidence="0.994905">
481
</page>
<bodyText confidence="0.988100833333333">
type label (such as VC or NC), but also by a com-
bination of the type and head word, to obtain finer
lexicalized distributions. LMs trained on different
chunk representations can also be applied jointly, by
log-linear combination.
We perform reordering selection as follows:
</bodyText>
<listItem confidence="0.9188164">
1. Chunk-based reordering rules are applied de-
terministically to the source side of the parallel
training data, using word alignment to choose
the optimal permutation (“oracle reordering”)6.
2. One or several chunk-level 5-gram LMs are
trained on such reordered data, using different
chunk representation modes.
3. Reordering rules are applied to the test sen-
tences and the resulting sets of rule-matching
sequence permutations are scored by the LMs.
</listItem>
<bodyText confidence="0.994751555555556">
The n-best reorderings of each rule-matching
sequence are selected for translation.
In experiments not reported here, we obtained
accurate rankings by scoring source permutations
with a uniformly weighted combination of two LMs
trained on chunk types and on chunk-type+head-
word, respectively. In particular, 3-best reorderings
of each rule-matching sequence yield reordering re-
calls of 77.2% in Arabic and 89.3% in German.
</bodyText>
<sectionHeader confidence="0.983543" genericHeader="method">
6 Modified distortion matrices
</sectionHeader>
<bodyText confidence="0.9997022">
We present here a novel technique to encode likely
long reorderings of an input sentence, which can be
seamlessly integrated into the PSMT framework.
During decoding, the distance between source po-
sitions is used for two main purposes: (i) generating
a distortion penalty for the current hypothesis and
(ii) determining the set of source positions that can
be covered at the next hypothesis expansion. We can
then tackle the coarseness of both distortion penalty
and reordering constraints, by replacing the distance
function with a function defined ad hoc for each in-
put sentence.
Distortion can be thought of as a matrix assigning
a positive integer to any ordered pair of source posi-
tions (sx, sy). In the linear distortion model this is
</bodyText>
<footnote confidence="0.5388585">
6Following Bisazza and Federico (2010), the optimal re-
ordering for a source sentence is the one that minimizes dis-
tortion in the word alignment to a target translation, measured
by number of swaps and sum of distortion costs.
</footnote>
<equation confidence="0.6489085">
defined as:
DL(sx, sy) = |sy − sx − 11
</equation>
<bodyText confidence="0.996064153846154">
so that moving to the right by 1 position costs 0 and
by 2 positions costs 1. Moving to the left by 1 posi-
tion costs 2 and by 2 positions costs 3, and so on. At
the level of phrases, distortion is computed between
the last word of the last translated phrase and the
first word of the next phrase. We retain this equa-
tion as the core distortion function for our model.
Then, we modify entries in the matrix such that the
distortion cost is minimized for the decoding paths
pre-computed with the reordering rules.
Given a source sentence and its set of rule-
generated permutations, the linear distortion matrix
is modified as follows:
</bodyText>
<listItem confidence="0.718499">
1. non-monotonic jumps (i.e. ordered pairs
(si, si+1) such that si+1 − si =� 1) are extracted
from the permutations;
2. then, for each extracted pair, the corresponding
point in the matrix is assigned the lowest possi-
ble distortion cost, that is 0 if si &lt; si+1 and 2
if si &gt; si+1. We call these points shortcuts.
</listItem>
<bodyText confidence="0.9991601">
Although this technique is approximate and can
overgenerate minimal-distortion decoding paths7, it
practically works when the number of encoded per-
mutations per sequence is limited. This makes mod-
ifed distortion matrices particularly suitable to en-
code just those reorderings that are typically missed
by phrase-based decoders (see Sect. 3).
Since in this work we use chunk-based rules, we
also have to convert chunk-to-chunk jumps into
word-to-word shortcuts. We propose two ways to
do this, given an ordered pair of chunks (cx,cy):
mode AxA : create a shortcut from each word of
cx to each word of cy;
mode LxF : create only one shortcut from the last
word of cx to the first of cy.
The former solution admits more chunk-internal per-
mutations with the same minimal distortion cost,
whereas the latter implies that the first word of a re-
ordered chunk is covered first and the last is covered
last.
</bodyText>
<footnote confidence="0.9974985">
7In fact, any decoding path that includes a jump marked as
shortcut benefits from the same distortion discount in that point.
</footnote>
<page confidence="0.994969">
482
</page>
<bodyText confidence="0.715633">
source word and is naturally compatible with the
PSMT decoder’s standard reordering mechanisms.
</bodyText>
<figure confidence="0.998969764705882">
NC1
Die
Budapester
Staat
anwaltschaft
auxVC2
hat
NC3
ihre
Ermittlungen
PC4
zum
Vorfall
ppVC5
eingeleitet
Pct6
�
Die hat auxVCp ihre zum eingeleitet ppVC5 � Pct6
Budapester NC3 PCq
Staat NC1 Ermittlungen Vorfall
anwaltschaft
0 1 2 3 4 5 6 7 8 9 10
0 1 2 3 4 5 6 7 8 9
2 0 1 2 3 4 5 6 7 8
3 2 0 1 2 3 4 5 6 7
4 3 2 0 1 2 3 4 5 6
5 4 3 2 0 1 2 3 0 5
6 5 4 3 2 0 1 2 3 4
7 6 5 4 3 2 0 1 2 3
8 7 6 5 4 3 2 0 1 0
9 8 7 6 5 4 3 2 0 0
10 9 8 7 6 2 2 3 2 0
11 10 9 8 7 6 5 4 3 2
&lt;S&gt;
</figure>
<figureCaption confidence="0.818588333333333">
orig: NC, auxVC2 NC3 PC4 ppVC5 Punch
reo: NC, auxVC2 ppVC5 NC3 PC4 Punch
Figure 2: Modified distortion matrix (mode AxA) of the
German sentence given in Fig. 1. The chunk reordering
shown on top generates three shortcuts corresponding to
the 0’s and 2’s highlighted in the matrix.
</figureCaption>
<bodyText confidence="0.99813345">
Fig. 2 shows the distortion matrix of the German
sentence of Fig. 1, with starting positions as columns
and landing positions as rows. Suppose we want to
encode the reordering shown on top of Fig. 2, cor-
responding to the merging of the broken VC ‘hat ...
eingeleitet’. This permutation contains three jumps:
(2,5), (5,3) and (4,6). Converted to word-level in
AxA mode, these yield five word shortcuts8: one
for the onward jump (2,5) assigned 0 distortion; two
for the backward jump (5,3), assigned 2; and two for
the onward jump (4,6), also assigned 0. The desired
reordering is now attainable within a DL of 2 words
instead of 5. The same process is then applied to
other permutations of the sentence.
If compared to the word reordering lattices used
by Bisazza and Federico (2010) and Andreas et al.
(2011), modified distortion matrices provide a more
compact, implicit way to encode likely reorderings
in a sentence-specific fashion. Matrix representation
does not require multiplication of nodes for the same
</bodyText>
<footnote confidence="0.989064">
8In LxF mode, instead, each chunk-to-chunk jump would
yield exactly one word shortcut, for a total of three.
</footnote>
<sectionHeader confidence="0.985802" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.9999857">
In this section we evaluate the impact of modified
distortion matrices on two news translation tasks.
Matrices were integrated into the Moses
toolkit (Koehn et al., 2007) using a sentence-
level XML markup. The list of word shortcuts
for each sentence is provided as an XML tag that
is parsed by the decoder to modify the distortion
matrix just before starting the search. As usual, the
distortion matrix is queried by the distortion penalty
generator and by the hypothesis expander9.
</bodyText>
<subsectionHeader confidence="0.962587">
7.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9999774">
For Arabic-English, we use the union of all in-
domain parallel corpora provided for the NIST-MT09
evaluation10 for a total of 986K sentences, 31M En-
glish words. The target LM is trained on the English
side of all available NIST-MT09 parallel data, UN in-
cluded (147M words). For development and test, we
use the newswire sections of the NIST benchmarks,
hereby called dev06-NW, eval08-NW and eval09-
NW: 1033, 813 and 586 sentences, respectively, each
provided with four reference translations.
The German-English system is instead trained
on WMT10 data: namely Europarl (v.5) plus News-
commentary-2010 for a total of 1.6M parallel sen-
tences, 43M English words. The target LM is trained
on the monolingual news data provided for the con-
strained track (1133M words). For development and
test, we use the WMT10 news benchmarks test08,
test09 and test10: 2051, 2525 and 2489 sentences,
respectively, with one reference translation.
Concerning pre-processing, we apply standard to-
kenization to the English data, while for Arabic we
use our in-house tokenizer that removes diacritics
and normalizes special characters. Arabic text is
then segmented with AMIRA (Diab et al., 2004) ac-
cording to the ATB scheme11. German tokenization
</bodyText>
<footnote confidence="0.955872571428571">
9Note that lexicalized reordering models use real word dis-
tances to compute the orientation class of a new hypothesis, thus
they are not affected by changes in the matrix.
10That is everything except the small GALE corpus and the
UN corpus. As reported by Green et al. (2010) the removal of
UN data does not affect baseline performances on news test.
11The Arabic Treebank tokenization scheme isolates con-
</footnote>
<page confidence="0.999285">
483
</page>
<bodyText confidence="0.999976791666667">
and compound splitting is performed with Tree Tag-
ger (Schmid, 1994) and the Gertwol morphological
analyser (Koskenniemi and Haapalainen, 1994)12.
Using Moses we build competitive baselines on
the training data described above. Word alignment
is produced by the Berkeley Aligner (Liang et al.,
2006). The decoder is based on the log-linear com-
bination of a phrase translation model, a lexicalized
reordering model, a 6-gram target language model,
distortion cost, word and phrase penalties. The re-
ordering model is a hierarchical phrase orientation
model (Tillmann, 2004; Koehn et al., 2005; Galley
and Manning, 2008) trained on all the available par-
allel data. We choose the hierarchical variant, as it
was shown by its authors to outperform the default
word-based on an Arabic-English task. Finally, for
German, we enable the Moses option monotone-at-
punctuation which forbids reordering across punc-
tuation marks. The DL is initially set to 5 words
for Arabic-English and to 10 for German-English.
According to our experience, these are the optimal
settings for the evaluated tasks. Feature weights are
optimized by minimum error training (Och, 2003)
on the development sets (dev06-NW and test08).
</bodyText>
<subsectionHeader confidence="0.999053">
7.2 Translation quality and efficiency results
</subsectionHeader>
<bodyText confidence="0.999772647058824">
We evaluate translations with BLEU (Papineni et al.,
2002) and METEOR (Banerjee and Lavie, 2005).
As these scores are only indirectly sensitive to word
order, we also compute KRS or Kendall Reorder-
ing Score (Birch et al., 2010; Bisazza et al., 2011)
which is a positive score based on the Kendall’s
Tau distance between the source-output and source-
reference permutations. To isolate the impact of our
techniques on problematic reordering, we extract
from each test set the sentences that got permuted
by “oracle reordering” (see Sect. 5). These consti-
tute about a half of the Arabic sentences, and about
a third of the German. We refer to the KRS com-
puted on these test subsets as KRS(R). Statistically
significant differences are assessed by approximate
randomization as in (Riezler and Maxwell, 2005)13.
Tab. 1 reports results obtained by varying the DL
</bodyText>
<footnote confidence="0.7055746">
junctions w+ and f+, prepositions l+, k+, b+, future marker
s+, pronominal suffixes, but not the article Al+.
12http://www2.lingsoft.fi/cgi-bin/gertwol
13Translation scores and significance tests are computed with
the tools multeval (Clark et al., 2011) and sigf (Pad´o, 2006).
</footnote>
<bodyText confidence="0.999704042553192">
and modifying the distortion function. To evalu-
ate the reordering selection technique, we also com-
pare the encoding of all rule-generated reorderings
against only the 3 best per rule-matching sequence,
as ranked by our best performing reordered LM (see
end of Sect. 5). We mark the DL with a ‘+’ to denote
that some longer jumps are being allowed by modi-
fied distortion. Run times refer to the translation of
the first 100 sentences of eval08-NW and test09 by
a 4-core processor.
Arabic-English. As anticipated, raising the DL
does not improve, but rather worsen performances.
The decrease in BLEU and METEOR reported with
DL=8 is not significant, but the decrease in KRS is
both significant and large. Efficiency is heavily af-
fected, with a 42% increase of the run time.
Results in the row “allReo” are obtained by encod-
ing all the rule-generated reorderings in LxF chunk-
to-word conversion mode. Except for some gains in
KRS reported on eval08-NW, most of the scores are
lower or equal to the baseline. Such inconsistent be-
haviour is probably due to the low precision of the
Arabic rule set, pointed out in Sect. 4.
Finally, we arrive to the performance of 3-best re-
orderings per sequence. With LxF we obtain sev-
eral improvements, but it’s with AxA that we are
able to beat the baseline according to all metrics.
BLEU and METEOR improvements are rather small
but significant and consistent across test sets, the
best gain being reported on eval09-NW (+.9 BLEU).
Most importantly, substantial word order improve-
ments are achieved on both full test sets (+.7/+.6
KRS) and selected subsets (+.7/+.6 KRS(R)). Ac-
cording to these figures, word order is affected only
in the sentences that contain problematic reordering.
This is good evidence, suggesting that the decoder
does not get “confused” by spurious shortcuts.
Looking at run times, we can say that modified
distortion matrices are a very efficient way to ad-
dress long reordering. Even when all the generated
reorderings are encoded, translation time increases
only by 4%. Reordering selection naturally helps to
further reduce decoding overload. As for conversion
modes, AxA yields slightly higher run times than
LxF because it generates more shortcuts.
German-English. In this task we manage to im-
prove translation quality with a setting that is almost
</bodyText>
<page confidence="0.997767">
484
</page>
<figure confidence="0.740461">
(a) Arabic to English
</figure>
<table confidence="0.992983555555555">
Distortion Function DL eval08-nw eval09-nw runtime
(s)
bleu met krs krs(R) bleu met krs krs(R)
† plain [baseline] 5 44.5 34.9 81.6 82.9 49.9 38.0 84.1 84.4 1038
plain 8 44.20 34.8 80.7• 82.2• 49.8 37.9 83.3• 83.5• 1470
† modified: allReo, LxF 5+ 44.4 34.9 82.2• 83.7• 49.9 37.8• 84.3 84.4 1078
modified: 3bestReo, LxF 5+ 44.5 35.1• 82.3• 83.5• 50.7• 38.1 84.8• 85.0• 1052
† modified: 3bestReo, AxA 5+ 44.80 35.1• 82.3• 83.6• 50.8• 38.2• 84.7• 85.0• 1072
(b) German to English
Distortion Function DL test09 test10 runtime
(s)
bleu met krs krs(R) bleu met krs krs(R)
† plain [baseline] 10 18.8 27.5 65.8 66.7 20.1 29.4 68.7 68.9 629
plain 20 18.4• 27.4• 63.6• 65.2 • 19.8• 29.3• 66.3• 66.6• 792
plain 4 18.4• 27.4• 67.3• 66.9 19.6• 29.1• 70.2• 69.6• 345
† modified: allReo, LxF 4+ 19.1• 27.6• 67.6• 68.1• 20.4• 29.4 70.6• 70.7• 352
modified: 3bestReo, LxF 4+ 19.2• 27.7• 67.4• 68.1• 20.4• 29.4 70.4• 70.6• 351
† modified: 3bestReo, AxA 4+ 19.2• 27.7• 67.4• 68.4• 20.6• 29.50 70.4• 70.7• 357
</table>
<tableCaption confidence="0.912086">
Table 1: Impact of modified distortion matrices on translation quality, measured with BLEU, METEOR and KRS
(all in percentage form, higher scores mean higher quality). The settings used for weight tuning are marked with †.
Statistically significant differences wrt the baseline are marked with • at the p &lt; .05 level and 0 at the p &lt; .10 level.
</tableCaption>
<bodyText confidence="0.999987222222222">
twice as fast as the baseline. As shown by the first
part of the table, the best baseline results are ob-
tained with a rather high DL, that is 10 (only KRS
improves with a lower DL). However, with modified
distortion, the best results according to all metrics
are obtained with a DL of 4.
Looking at the rest of the table, we see that re-
ordering selection is not as crucial as in Arabic-
English. This is in line with the properties of the
more precise German reordering rule set (two rules
out of three generate at most 3 reorderings per se-
quence). Considering all scores, the last setting
(3-best reordering and AxA) appears as the best
one, achieving the following gains over the base-
line: +.4/+.5 BLEU, +.2/+.1 METEOR, +1.6/+1.7
KRS and +1.7/+1.8 KRS(R). The agreement ob-
served among such diverse metrics makes us con-
fident about the goodness of the approach.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999941727272727">
In Arabic-English and German-English, long re-
ordering concentrates on specific patterns describ-
able by a small number of linguistic rules. By
means of non-deterministic chunk reordering rules,
we have generated likely permutations of the test
sentences and ranked them with n-gram LMs trained
on pre-reordered data. We have then introduced the
notion of modified distortion matrices to naturally
encode a set of likely reorderings in the decoder
input. Modified distortion allows for a finer and
more linguistically informed definition of the search
space, which is reflected in better translation outputs
and more efficient decoding.
We expect that further improvements may be
achieved by refining the Arabic reordering rules with
specific POS tags and lexical cues. We also plan
to evaluate modified distortion matrices in conjunc-
tion with a different type of in-decoding reorder-
ing model such as the one proposed by Green et
al. (2010). Finally, we may try to exploit not only
the ranking, but also the scores produced by the re-
ordered LMs, as an additional decoding feature.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999902333333333">
This work was supported by the T4ME network of
excellence (IST-249119) funded by the European
Commission DG INFSO through the 7th Framework
Programme. We thank Christian Hardmeier for
helping us define the German reordering rules, and
the anonymous reviewers for valuable suggestions.
</bodyText>
<page confidence="0.998844">
485
</page>
<sectionHeader confidence="0.989106" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99881519047619">
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 529–
536, Sydney, Australia, July. Association for Computa-
tional Linguistics.
Jacob Andreas, Nizar Habash, and Owen Rambow. 2011.
Fuzzy syntactic reordering for phrase-based statistical
machine translation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 227–
236, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65–72, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15–26.
Arianna Bisazza and Marcello Federico. 2010. Chunk-
based verb reordering in VSO sentences for Arabic-
English statistical machine translation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and Metrics MATR, pages 241–249, Upp-
sala, Sweden, July. Association for Computational Lin-
guistics.
Arianna Bisazza, Daniele Pighin, and Marcello Federico.
2011. Chunk-lattices for verb reordering in Arabic-
English statistical machine translation. Machine Trans-
lation, Published Online.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL’05), pages 263–270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better hypothesis testing for sta-
tistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the Asso-
ciation for Computational Lingustics, ACL 2011,
Portland, Oregon, USA. Association for Com-
putational Linguistics. accepted; available at
http://www.cs.cmu.edu/ jhclark/pubs/significance.pdf.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL’05), pages 531–540, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Josep M. Crego and Nizar Habash. 2008. Using shal-
low syntax information to improve word alignment and
reordering for smt. In StatMT ’08: Proceedings of
the Third Workshop on Statistical Machine Translation,
pages 53–61, Morristown, NJ, USA. Association for
Computational Linguistics.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic Tagging of Arabic Text: From Raw Text to
Base Phrase Chunks. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Short
Papers, pages 149–152, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Jakob Elming and Nizar Habash. 2009. Syntactic
reordering for English-Arabic phrase-based machine
translation. In Proceedings of the EACL 2009 Work-
shop on Computational Approaches to Semitic Lan-
guages, pages 69–77, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statistical
machine translation. In Conference of the Association
for Machine Translation in the Americas (AMTA), Den-
ver, Colorado, USA.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reordering
model. In EMNLP ’08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 848–856, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for sta-
tistical machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 867–875, Los An-
geles, California. Association for Computational Lin-
guistics.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Bente Maegaard, ed-
itor, Proceedings of the Machine Translation Summit
XI, pages 215–222, Copenhagen, Denmark.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. FBK at WMT 2010: Word lattices
for morphological reduction and chunk-based reorder-
ing. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics MATR, pages
88–92, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
</reference>
<page confidence="0.988486">
486
</page>
<reference confidence="0.99987585">
ings of HLT-NAACL 2003, pages 127–133, Edmonton,
Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proc. of the International Workshop on Spoken Lan-
guage Translation, October.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics Companion Volume Proceedings of
the Demo and Poster Sessions, pages 177–180, Prague,
Czech Republic.
Kimmo Koskenniemi and Mariikka Haapalainen, 1994.
GERTWOL – Lingsoft Oy, chapter 11, pages 121–140.
Roland Hausser, Niemeyer, T¨ubingen.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104–111, New York City, USA,
June. Association for Computational Linguistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 206–214, Athens, Greece, March.
Association for Computational Linguistics.
Franz Josef Och. 2002. Statistical Machine Trans-
lation: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160–167.
Sebastian Pad´o, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association of Computa-
tional Linguistics (ACL), pages 311–318, Philadelphia,
PA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL Workshop on In-
trinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 57–64, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Proceed-
ings of the Joint Conference on Human Language Tech-
nologies and the Annual Meeting of the North Ameri-
can Chapter of the Association of Computational Lin-
guistics (HLT-NAACL).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In Proceedings of Coling 2004, pages 508–
514, Geneva, Switzerland, Aug 23–Aug 27. COLING.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic dis-
tortion in a discriminative reordering model for statisti-
cal machine translation. In International Workshop on
Spoken Language Translation (IWSLT), Paris, France.
Kenji Yamada. 2002. A syntax-based translation model.
Ph.D. thesis, Department of Computer Science, Uni-
versity of Southern California, Los Angeles.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Machine
Translation, pages 55–63, New York City, June. Asso-
ciation for Computational Linguistics.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In 25th German Confer-
ence on Artificial Intelligence (KI2002), pages 18–32,
Aachen, Germany. Springer Verlag.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sentences
with automatically learned rules for statistical machine
translation. In Proceedings of SSST, NAACL-HLT 2007
/AMTA Workshop on Syntax and Structure in Statistical
Translation, pages 1–8, Rochester, New York, April.
Association for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 1145–1152, Manchester, UK, August. Coling
2008 Organizing Committee.
</reference>
<page confidence="0.998398">
487
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.293285">
<title confidence="0.999869">Modified Distortion for Phrase-Based Statistical Machine Translation</title>
<author confidence="0.6875535">Bisazza Fondazione Bruno</author>
<affiliation confidence="0.647766">Trento,</affiliation>
<abstract confidence="0.998721352941176">This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>529--536</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2379" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="354" endWordPosition="358">olated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1, producing overall highquality outputs and isolated but critical reordering errors that undermine the global sentence meaning. Previous works on this type of language pairs have mostly focused on source reordering prior to translation (Xia and McCord, 2004; Collins et al., 2005), or on sophisticated reordering models integrated into decoding (Koehn et al., 2005; Al-Onaizan and Papineni, 2006), achieving mixed results. To merge the best of both approaches – namely, access to rich context in the former and natural coupling of reordering and translation decisions in the latter – we introduce modified distortion matrices: a novel method to seamlessly provide to the decoder a set of likely long reorderings pre-computed for a given input sentence. Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a linguistically informed definition of the search space that simplifies the task of the in-decoder reordering model, besides decreasing its</context>
<context position="6934" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="1074" endWordPosition="1077">ed phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle local reordering and are widely adopted by the PSMT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or precede the last translated one. Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al., 2010). These models are conceived to deal with long reordering, but can easily suffer from data sparseness, especially for longer jumps occurring less frequently. Following a typical sequence modeling approach, Feng et al. (2010) train n-gram language models on source data previously reordered in accordance to the target language translation. This method does not directly model reordering decisions, but rather word sequences produced by them. Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 529– 536, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Fuzzy syntactic reordering for phrase-based statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>227--236</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="4659" citStr="Andreas et al., 2011" startWordPosition="710" endWordPosition="713">reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually written (Collins et al., 2005) or automatically learned from syntactic parses (Xia and McCord, 2004; Habash, 2007; Elming and Habash, 2009), shallow syntax chunks (Zhang et al., 2007; Crego and Habash, 2008) or part-of-speech labels (Niehues and Kolss, 2009). Si</context>
<context position="22019" citStr="Andreas et al. (2011)" startWordPosition="3671" endWordPosition="3674">p of Fig. 2, corresponding to the merging of the broken VC ‘hat ... eingeleitet’. This permutation contains three jumps: (2,5), (5,3) and (4,6). Converted to word-level in AxA mode, these yield five word shortcuts8: one for the onward jump (2,5) assigned 0 distortion; two for the backward jump (5,3), assigned 2; and two for the onward jump (4,6), also assigned 0. The desired reordering is now attainable within a DL of 2 words instead of 5. The same process is then applied to other permutations of the sentence. If compared to the word reordering lattices used by Bisazza and Federico (2010) and Andreas et al. (2011), modified distortion matrices provide a more compact, implicit way to encode likely reorderings in a sentence-specific fashion. Matrix representation does not require multiplication of nodes for the same 8In LxF mode, instead, each chunk-to-chunk jump would yield exactly one word shortcut, for a total of three. 7 Evaluation In this section we evaluate the impact of modified distortion matrices on two news translation tasks. Matrices were integrated into the Moses toolkit (Koehn et al., 2007) using a sentencelevel XML markup. The list of word shortcuts for each sentence is provided as an XML t</context>
</contexts>
<marker>Andreas, Habash, Rambow, 2011</marker>
<rawString>Jacob Andreas, Nizar Habash, and Owen Rambow. 2011. Fuzzy syntactic reordering for phrase-based statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 227– 236, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="25825" citStr="Banerjee and Lavie, 2005" startWordPosition="4268" endWordPosition="4271">its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and test08). 7.2 Translation quality and efficiency results We evaluate translations with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). As these scores are only indirectly sensitive to word order, we also compute KRS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on these test subsets as KRS(R). Statist</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Phil Blunsom</author>
</authors>
<title>Metrics for MT evaluation: evaluating reordering.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="25955" citStr="Birch et al., 2010" startWordPosition="4291" endWordPosition="4294">nctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and test08). 7.2 Translation quality and efficiency results We evaluate translations with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). As these scores are only indirectly sensitive to word order, we also compute KRS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on these test subsets as KRS(R). Statistically significant differences are assessed by approximate randomization as in (Riezler and Maxwell, 2005)13. Tab. 1 reports resul</context>
</contexts>
<marker>Birch, Osborne, Blunsom, 2010</marker>
<rawString>Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT evaluation: evaluating reordering. Machine Translation, 24(1):15–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Marcello Federico</author>
</authors>
<title>Chunkbased verb reordering in VSO sentences for ArabicEnglish statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="4636" citStr="Bisazza and Federico, 2010" startWordPosition="706" endWordPosition="709">at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually written (Collins et al., 2005) or automatically learned from syntactic parses (Xia and McCord, 2004; Habash, 2007; Elming and Habash, 2009), shallow syntax chunks (Zhang et al., 2007; Crego and Habash, 2008) or part-of-speech labels (Niehu</context>
<context position="10791" citStr="Bisazza and Federico (2010)" startWordPosition="1691" endWordPosition="1694">es at the expense of long reorderings, which are often crucial to preserve the general meaning of a translated sentence. For instance, taking English as the target language, it is precisely the relative positioning of predicate arguments that determines their role, in the absence of case markers. Thus, a wrongly reordered verb with minor impact on automatic scores, can be judged very badly by a human evaluator. We will now describe two rule sets aimed at capturing these reordering phenomena. 4 Shallow syntax reordering rules To compute the source reorderings, we use chunkbased rules following Bisazza and Federico (2010). Shallow syntax chunking is indeed a lighter and simpler task compared to full parsing, and it can be used to constrain the number of reorderings in a softer way. While rules based on full parses are generally deterministic, chunk-based rules are non-deterministic or fuzzy, as they generate several permutations for each matching sequence2. Besides defining a unique segmentation of the sentence, chunk annotation provides other useful information that can be used by the rules – namely chunk type and POS tags3. For Arabic-English we apply the rules proposed by Bisazza and Federico (2010) aimed a</context>
<context position="18159" citStr="Bisazza and Federico (2010)" startWordPosition="2924" endWordPosition="2927">ring decoding, the distance between source positions is used for two main purposes: (i) generating a distortion penalty for the current hypothesis and (ii) determining the set of source positions that can be covered at the next hypothesis expansion. We can then tackle the coarseness of both distortion penalty and reordering constraints, by replacing the distance function with a function defined ad hoc for each input sentence. Distortion can be thought of as a matrix assigning a positive integer to any ordered pair of source positions (sx, sy). In the linear distortion model this is 6Following Bisazza and Federico (2010), the optimal reordering for a source sentence is the one that minimizes distortion in the word alignment to a target translation, measured by number of swaps and sum of distortion costs. defined as: DL(sx, sy) = |sy − sx − 11 so that moving to the right by 1 position costs 0 and by 2 positions costs 1. Moving to the left by 1 position costs 2 and by 2 positions costs 3, and so on. At the level of phrases, distortion is computed between the last word of the last translated phrase and the first word of the next phrase. We retain this equation as the core distortion function for our model. Then,</context>
<context position="21993" citStr="Bisazza and Federico (2010)" startWordPosition="3666" endWordPosition="3669">ncode the reordering shown on top of Fig. 2, corresponding to the merging of the broken VC ‘hat ... eingeleitet’. This permutation contains three jumps: (2,5), (5,3) and (4,6). Converted to word-level in AxA mode, these yield five word shortcuts8: one for the onward jump (2,5) assigned 0 distortion; two for the backward jump (5,3), assigned 2; and two for the onward jump (4,6), also assigned 0. The desired reordering is now attainable within a DL of 2 words instead of 5. The same process is then applied to other permutations of the sentence. If compared to the word reordering lattices used by Bisazza and Federico (2010) and Andreas et al. (2011), modified distortion matrices provide a more compact, implicit way to encode likely reorderings in a sentence-specific fashion. Matrix representation does not require multiplication of nodes for the same 8In LxF mode, instead, each chunk-to-chunk jump would yield exactly one word shortcut, for a total of three. 7 Evaluation In this section we evaluate the impact of modified distortion matrices on two news translation tasks. Matrices were integrated into the Moses toolkit (Koehn et al., 2007) using a sentencelevel XML markup. The list of word shortcuts for each senten</context>
</contexts>
<marker>Bisazza, Federico, 2010</marker>
<rawString>Arianna Bisazza and Marcello Federico. 2010. Chunkbased verb reordering in VSO sentences for ArabicEnglish statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 241–249, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Daniele Pighin</author>
<author>Marcello Federico</author>
</authors>
<title>Chunk-lattices for verb reordering in ArabicEnglish statistical machine translation. Machine Translation,</title>
<date>2011</date>
<publisher>Published Online.</publisher>
<contexts>
<context position="15946" citStr="Bisazza et al., 2011" startWordPosition="2576" endWordPosition="2579">g is triggered by any verb chunk. 5 Reordering selection The number of chunk-based reorderings per sentence varies according to the rule set, to the size of chunks and to the context. A high degree of fuzziness can complicate the decoding process, leaving too much work to the in-decoding reordering model. A solution to this problem is using an external model to score the rule-generated reorderings and discard the less probable ones. In such a way, a further part of reordering complexity is taken out of decoding. At this end, instead of using a Support Vector Machine classifier as was done in (Bisazza et al., 2011), we apply reordered n-gram models that are lighterweight and more suitable for a ranking task. Differently from Feng et al. (2010), we train our models on partially reordered data and at the level of chunks. Chunks can be represented simply by their 481 type label (such as VC or NC), but also by a combination of the type and head word, to obtain finer lexicalized distributions. LMs trained on different chunk representations can also be applied jointly, by log-linear combination. We perform reordering selection as follows: 1. Chunk-based reordering rules are applied deterministically to the so</context>
<context position="25978" citStr="Bisazza et al., 2011" startWordPosition="4295" endWordPosition="4298">ids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and test08). 7.2 Translation quality and efficiency results We evaluate translations with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). As these scores are only indirectly sensitive to word order, we also compute KRS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on these test subsets as KRS(R). Statistically significant differences are assessed by approximate randomization as in (Riezler and Maxwell, 2005)13. Tab. 1 reports results obtained by varying </context>
</contexts>
<marker>Bisazza, Pighin, Federico, 2011</marker>
<rawString>Arianna Bisazza, Daniele Pighin, and Marcello Federico. 2011. Chunk-lattices for verb reordering in ArabicEnglish statistical machine translation. Machine Translation, Published Online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1480" citStr="Chiang, 2005" startWordPosition="222" endWordPosition="223">well-known SMT benchmarks. 1 Introduction Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs. On one hand, the phrase-based approach (PSMT) (Och, 2002; Zens et al., 2002; Koehn et al., 2003), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency. On the other, tree-based approaches (Wu, 1997; Yamada, 2002; Chiang, 2005) gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders. Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1, producing overall highquality outputs and isolated but critical reordering errors that undermin</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jonathan Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<journal>Michael Collins, Philipp Koehn, and Ivona</journal>
<booktitle>In Proceedings of the Association for Computational Lingustics, ACL 2011,</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="26838" citStr="Clark et al., 2011" startWordPosition="4427" endWordPosition="4430">t permuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on these test subsets as KRS(R). Statistically significant differences are assessed by approximate randomization as in (Riezler and Maxwell, 2005)13. Tab. 1 reports results obtained by varying the DL junctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. 12http://www2.lingsoft.fi/cgi-bin/gertwol 13Translation scores and significance tests are computed with the tools multeval (Clark et al., 2011) and sigf (Pad´o, 2006). and modifying the distortion function. To evaluate the reordering selection technique, we also compare the encoding of all rule-generated reorderings against only the 3 best per rule-matching sequence, as ranked by our best performing reordered LM (see end of Sect. 5). We mark the DL with a ‘+’ to denote that some longer jumps are being allowed by modified distortion. Run times refer to the translation of the first 100 sentences of eval08-NW and test09 by a 4-core processor. Arabic-English. As anticipated, raising the DL does not improve, but rather worsen performances</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the Association for Computational Lingustics, ACL 2011, Portland, Oregon, USA. Association for Computational Linguistics. accepted; available at http://www.cs.cmu.edu/ jhclark/pubs/significance.pdf. Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 531–540, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Nizar Habash</author>
</authors>
<title>Using shallow syntax information to improve word alignment and reordering for smt.</title>
<date>2008</date>
<booktitle>In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>53--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4396" citStr="Crego and Habash, 2008" startWordPosition="672" endWordPosition="675">ate the proposed techniques on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually</context>
</contexts>
<marker>Crego, Habash, 2008</marker>
<rawString>Josep M. Crego and Nizar Habash. 2008. Using shallow syntax information to improve word alignment and reordering for smt. In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation, pages 53–61, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Kadri Hacioglu</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Tagging of Arabic Text: From Raw Text to Base Phrase Chunks.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers,</booktitle>
<volume>2</volume>
<pages>149--152</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="12452" citStr="Diab et al., 2004" startWordPosition="1966" endWordPosition="1969">ust be moved along with an adverbial or a complement. For German-English we propose a set of three rules4 aimed at arranging the German constituents in SVO order: • infinitive: move each infinitive VC right after a preceding punctuation; • subordinate: if a VC is immediately followed by a punctuation, place it after a preceding subordinating conjunction (KOUS) or substitutive relative pronoun (PRELS); 2Chunk annotation does not identify subject and complement boundaries, nor the relations among constituents that are needed to deterministically rearrange a sentence in SVO order. 3We use AMIRA (Diab et al., 2004) to annotate Arabic and Tree Tagger (Schmid, 1994) to annotate German. 4A similar rule set was previously used to produce chunk reordering lattices in (Hardmeier et al., 2010). 480 (b) German broken verb chunk: three permutations Figure 1: Examples of chunk permutations generated by shallow syntax reordering rules. Chunk types: CC conjunction, VC verb (auxiliary/past participle), PC preposition, NC noun, Pct punctuation. wand CC1 Pct6 Pct6 &gt;0 Pct6 CC1 PC3 VC2 NC4 PC5 CC1 CC1 -89 PC3 NC4 VC2 PC5 98:7 PC3 NC4 =&gt;=? ?A&amp;quot; PC5 4 VC2 =&gt; CC1 NC4 VC2 PC3 PC5 D0F0734A4LFF =040 Pct6 CC1 NC4 PC5 VC2 PC3 &amp; </context>
<context position="24023" citStr="Diab et al., 2004" startWordPosition="3986" endWordPosition="3989">namely Europarl (v.5) plus Newscommentary-2010 for a total of 1.6M parallel sentences, 43M English words. The target LM is trained on the monolingual news data provided for the constrained track (1133M words). For development and test, we use the WMT10 news benchmarks test08, test09 and test10: 2051, 2525 and 2489 sentences, respectively, with one reference translation. Concerning pre-processing, we apply standard tokenization to the English data, while for Arabic we use our in-house tokenizer that removes diacritics and normalizes special characters. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme11. German tokenization 9Note that lexicalized reordering models use real word distances to compute the orientation class of a new hypothesis, thus they are not affected by changes in the matrix. 10That is everything except the small GALE corpus and the UN corpus. As reported by Green et al. (2010) the removal of UN data does not affect baseline performances on news test. 11The Arabic Treebank tokenization scheme isolates con483 and compound splitting is performed with Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 19</context>
</contexts>
<marker>Diab, Hacioglu, Jurafsky, 2004</marker>
<rawString>Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004. Automatic Tagging of Arabic Text: From Raw Text to Base Phrase Chunks. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers, pages 149–152, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Nizar Habash</author>
</authors>
<title>Syntactic reordering for English-Arabic phrase-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>69--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="4421" citStr="Elming and Habash, 2009" startWordPosition="676" endWordPosition="679">ues on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually written (Collins et al.,</context>
</contexts>
<marker>Elming, Habash, 2009</marker>
<rawString>Jakob Elming and Nizar Habash. 2009. Syntactic reordering for English-Arabic phrase-based machine translation. In Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwei Feng</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>A source-side decoding sequence model for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="7198" citStr="Feng et al. (2010)" startWordPosition="1116" endWordPosition="1119">MT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or precede the last translated one. Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al., 2010). These models are conceived to deal with long reordering, but can easily suffer from data sparseness, especially for longer jumps occurring less frequently. Following a typical sequence modeling approach, Feng et al. (2010) train n-gram language models on source data previously reordered in accordance to the target language translation. This method does not directly model reordering decisions, but rather word sequences produced by them. Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006). In this work we use similar models to rank sets of chunk permutations. Attempting to improve the reordering space definition, Yahyaei and Monz (2010) train a classifier to guess the most lik</context>
<context position="16077" citStr="Feng et al. (2010)" startWordPosition="2598" endWordPosition="2601">le set, to the size of chunks and to the context. A high degree of fuzziness can complicate the decoding process, leaving too much work to the in-decoding reordering model. A solution to this problem is using an external model to score the rule-generated reorderings and discard the less probable ones. In such a way, a further part of reordering complexity is taken out of decoding. At this end, instead of using a Support Vector Machine classifier as was done in (Bisazza et al., 2011), we apply reordered n-gram models that are lighterweight and more suitable for a ranking task. Differently from Feng et al. (2010), we train our models on partially reordered data and at the level of chunks. Chunks can be represented simply by their 481 type label (such as VC or NC), but also by a combination of the type and head word, to obtain finer lexicalized distributions. LMs trained on different chunk representations can also be applied jointly, by log-linear combination. We perform reordering selection as follows: 1. Chunk-based reordering rules are applied deterministically to the source side of the parallel training data, using word alignment to choose the optimal permutation (“oracle reordering”)6. 2. One or s</context>
</contexts>
<marker>Feng, Mauser, Ney, 2010</marker>
<rawString>Minwei Feng, Arne Mauser, and Hermann Ney. 2010. A source-side decoding sequence model for statistical machine translation. In Conference of the Association for Machine Translation in the Americas (AMTA), Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6415" citStr="Galley and Manning, 2008" startWordPosition="988" endWordPosition="992">inear distortion to more complex models that are conditioned on the words being translated. The linear distortion model (Koehn et al., 2003) encourages monotonic translations by penalizing source position jumps proportionally to their length. If used alone, this model is inadequate for language pairs with different word orders. Green et al. (2010) tried to improve it with a future distortion cost estimate. Thus they were able to preserve baseline performance at a very high DL, but not to improve it. Lexicalized phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle local reordering and are widely adopted by the PSMT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or precede the last translated one. Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al., 2010). These models are conceived to deal with</context>
<context position="25100" citStr="Galley and Manning, 2008" startWordPosition="4156" endWordPosition="4159">s con483 and compound splitting is performed with Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994)12. Using Moses we build competitive baselines on the training data described above. Word alignment is produced by the Berkeley Aligner (Liang et al., 2006). The decoder is based on the log-linear combination of a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, word and phrase penalties. The reordering model is a hierarchical phrase orientation model (Tillmann, 2004; Koehn et al., 2005; Galley and Manning, 2008) trained on all the available parallel data. We choose the hierarchical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and test08). 7.2 Translation qu</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 848–856, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved models of distortion cost for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>867--875</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California.</location>
<contexts>
<context position="6139" citStr="Green et al. (2010)" startWordPosition="941" endWordPosition="944">odel in all the systems used to evaluate our methods. Another large body of work is devoted to the modeling of reordering decisions inside decoding, based on a decomposition of the problem into a sequence of basic reordering steps. Existing approaches range from basic linear distortion to more complex models that are conditioned on the words being translated. The linear distortion model (Koehn et al., 2003) encourages monotonic translations by penalizing source position jumps proportionally to their length. If used alone, this model is inadequate for language pairs with different word orders. Green et al. (2010) tried to improve it with a future distortion cost estimate. Thus they were able to preserve baseline performance at a very high DL, but not to improve it. Lexicalized phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle local reordering and are widely adopted by the PSMT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or preced</context>
<context position="24350" citStr="Green et al. (2010)" startWordPosition="4042" endWordPosition="4045"> respectively, with one reference translation. Concerning pre-processing, we apply standard tokenization to the English data, while for Arabic we use our in-house tokenizer that removes diacritics and normalizes special characters. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme11. German tokenization 9Note that lexicalized reordering models use real word distances to compute the orientation class of a new hypothesis, thus they are not affected by changes in the matrix. 10That is everything except the small GALE corpus and the UN corpus. As reported by Green et al. (2010) the removal of UN data does not affect baseline performances on news test. 11The Arabic Treebank tokenization scheme isolates con483 and compound splitting is performed with Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994)12. Using Moses we build competitive baselines on the training data described above. Word alignment is produced by the Berkeley Aligner (Liang et al., 2006). The decoder is based on the log-linear combination of a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, wor</context>
</contexts>
<marker>Green, Galley, Manning, 2010</marker>
<rawString>Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 867–875, Los Angeles, California. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Syntactic preprocessing for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Bente Maegaard, editor, Proceedings of the Machine Translation Summit XI,</booktitle>
<pages>215--222</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="4189" citStr="Habash, 2007" startWordPosition="641" endWordPosition="642">Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics reordered n-gram LMs and, finally, explain the notion of modified distortion matrices. In the last part of the paper, we evaluate the proposed techniques on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely</context>
</contexts>
<marker>Habash, 2007</marker>
<rawString>Nizar Habash. 2007. Syntactic preprocessing for statistical machine translation. In Bente Maegaard, editor, Proceedings of the Machine Translation Summit XI, pages 215–222, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Arianna Bisazza</author>
<author>Marcello Federico</author>
</authors>
<title>FBK at WMT 2010: Word lattices for morphological reduction and chunk-based reordering.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>88--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="12627" citStr="Hardmeier et al., 2010" startWordPosition="1994" endWordPosition="1997">itive: move each infinitive VC right after a preceding punctuation; • subordinate: if a VC is immediately followed by a punctuation, place it after a preceding subordinating conjunction (KOUS) or substitutive relative pronoun (PRELS); 2Chunk annotation does not identify subject and complement boundaries, nor the relations among constituents that are needed to deterministically rearrange a sentence in SVO order. 3We use AMIRA (Diab et al., 2004) to annotate Arabic and Tree Tagger (Schmid, 1994) to annotate German. 4A similar rule set was previously used to produce chunk reordering lattices in (Hardmeier et al., 2010). 480 (b) German broken verb chunk: three permutations Figure 1: Examples of chunk permutations generated by shallow syntax reordering rules. Chunk types: CC conjunction, VC verb (auxiliary/past participle), PC preposition, NC noun, Pct punctuation. wand CC1 Pct6 Pct6 &gt;0 Pct6 CC1 PC3 VC2 NC4 PC5 CC1 CC1 -89 PC3 NC4 VC2 PC5 98:7 PC3 NC4 =&gt;=? ?A&amp;quot; PC5 4 VC2 =&gt; CC1 NC4 VC2 PC3 PC5 D0F0734A4LFF =040 Pct6 CC1 NC4 PC5 VC2 PC3 &amp; Pct6 6&amp;quot;N** &amp;quot;FQ M% ORS =38 $Ark took part VC2 fy A(tZAhrp E$rAt AlmslHyn mn AlktA}b � in the march dozens of militants from the Brigades PC3 NC4 PC5 Pct6 (a) Arabic VS(O) claus</context>
</contexts>
<marker>Hardmeier, Bisazza, Federico, 2010</marker>
<rawString>Christian Hardmeier, Arianna Bisazza, and Marcello Federico. 2010. FBK at WMT 2010: Word lattices for morphological reduction and chunk-based reordering. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 88–92, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1190" citStr="Koehn et al., 2003" startWordPosition="177" endWordPosition="180">ode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks. 1 Introduction Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs. On one hand, the phrase-based approach (PSMT) (Och, 2002; Zens et al., 2002; Koehn et al., 2003), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency. On the other, tree-based approaches (Wu, 1997; Yamada, 2002; Chiang, 2005) gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders. Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of ling</context>
<context position="5930" citStr="Koehn et al., 2003" startWordPosition="911" endWordPosition="914">w linguistically informed rules to generate multiple reorderings for selected phenomena but, as a difference, we do not employ lattices to represent them. We also include a competitive in-decoding reordering model in all the systems used to evaluate our methods. Another large body of work is devoted to the modeling of reordering decisions inside decoding, based on a decomposition of the problem into a sequence of basic reordering steps. Existing approaches range from basic linear distortion to more complex models that are conditioned on the words being translated. The linear distortion model (Koehn et al., 2003) encourages monotonic translations by penalizing source position jumps proportionally to their length. If used alone, this model is inadequate for language pairs with different word orders. Green et al. (2010) tried to improve it with a future distortion cost estimate. Thus they were able to preserve baseline performance at a very high DL, but not to improve it. Lexicalized phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127–133, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation,</booktitle>
<contexts>
<context position="2347" citStr="Koehn et al., 2005" startWordPosition="350" endWordPosition="353">eorderings can be isolated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1, producing overall highquality outputs and isolated but critical reordering errors that undermine the global sentence meaning. Previous works on this type of language pairs have mostly focused on source reordering prior to translation (Xia and McCord, 2004; Collins et al., 2005), or on sophisticated reordering models integrated into decoding (Koehn et al., 2005; Al-Onaizan and Papineni, 2006), achieving mixed results. To merge the best of both approaches – namely, access to rich context in the former and natural coupling of reordering and translation decisions in the latter – we introduce modified distortion matrices: a novel method to seamlessly provide to the decoder a set of likely long reorderings pre-computed for a given input sentence. Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a linguistically informed definition of the search space that simplifies the task of the in-decoder reorderi</context>
<context position="6368" citStr="Koehn et al., 2005" startWordPosition="980" endWordPosition="983">. Existing approaches range from basic linear distortion to more complex models that are conditioned on the words being translated. The linear distortion model (Koehn et al., 2003) encourages monotonic translations by penalizing source position jumps proportionally to their length. If used alone, this model is inadequate for language pairs with different word orders. Green et al. (2010) tried to improve it with a future distortion cost estimate. Thus they were able to preserve baseline performance at a very high DL, but not to improve it. Lexicalized phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle local reordering and are widely adopted by the PSMT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or precede the last translated one. Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al.,</context>
<context position="25073" citStr="Koehn et al., 2005" startWordPosition="4152" endWordPosition="4155">ation scheme isolates con483 and compound splitting is performed with Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994)12. Using Moses we build competitive baselines on the training data described above. Word alignment is produced by the Berkeley Aligner (Liang et al., 2006). The decoder is based on the log-linear combination of a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, word and phrase penalties. The reordering model is a hierarchical phrase orientation model (Tillmann, 2004; Koehn et al., 2005; Galley and Manning, 2008) trained on all the available parallel data. We choose the hierarchical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and </context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proc. of the International Workshop on Spoken Language Translation, October.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="22516" citStr="Koehn et al., 2007" startWordPosition="3746" endWordPosition="3749">of the sentence. If compared to the word reordering lattices used by Bisazza and Federico (2010) and Andreas et al. (2011), modified distortion matrices provide a more compact, implicit way to encode likely reorderings in a sentence-specific fashion. Matrix representation does not require multiplication of nodes for the same 8In LxF mode, instead, each chunk-to-chunk jump would yield exactly one word shortcut, for a total of three. 7 Evaluation In this section we evaluate the impact of modified distortion matrices on two news translation tasks. Matrices were integrated into the Moses toolkit (Koehn et al., 2007) using a sentencelevel XML markup. The list of word shortcuts for each sentence is provided as an XML tag that is parsed by the decoder to modify the distortion matrix just before starting the search. As usual, the distortion matrix is queried by the distortion penalty generator and by the hypothesis expander9. 7.1 Experimental setup For Arabic-English, we use the union of all indomain parallel corpora provided for the NIST-MT09 evaluation10 for a total of 986K sentences, 31M English words. The target LM is trained on the English side of all available NIST-MT09 parallel data, UN included (147M</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
<author>Mariikka Haapalainen</author>
</authors>
<date>1994</date>
<journal>GERTWOL – Lingsoft Oy, chapter</journal>
<volume>11</volume>
<pages>121--140</pages>
<location>Roland Hausser, Niemeyer, T¨ubingen.</location>
<contexts>
<context position="24626" citStr="Koskenniemi and Haapalainen, 1994" startWordPosition="4083" endWordPosition="4086"> with AMIRA (Diab et al., 2004) according to the ATB scheme11. German tokenization 9Note that lexicalized reordering models use real word distances to compute the orientation class of a new hypothesis, thus they are not affected by changes in the matrix. 10That is everything except the small GALE corpus and the UN corpus. As reported by Green et al. (2010) the removal of UN data does not affect baseline performances on news test. 11The Arabic Treebank tokenization scheme isolates con483 and compound splitting is performed with Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994)12. Using Moses we build competitive baselines on the training data described above. Word alignment is produced by the Berkeley Aligner (Liang et al., 2006). The decoder is based on the log-linear combination of a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, word and phrase penalties. The reordering model is a hierarchical phrase orientation model (Tillmann, 2004; Koehn et al., 2005; Galley and Manning, 2008) trained on all the available parallel data. We choose the hierarchical variant, as it was shown by its authors to outperform </context>
</contexts>
<marker>Koskenniemi, Haapalainen, 1994</marker>
<rawString>Kimmo Koskenniemi and Mariikka Haapalainen, 1994. GERTWOL – Lingsoft Oy, chapter 11, pages 121–140. Roland Hausser, Niemeyer, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="24782" citStr="Liang et al., 2006" startWordPosition="4107" endWordPosition="4110">ion class of a new hypothesis, thus they are not affected by changes in the matrix. 10That is everything except the small GALE corpus and the UN corpus. As reported by Green et al. (2010) the removal of UN data does not affect baseline performances on news test. 11The Arabic Treebank tokenization scheme isolates con483 and compound splitting is performed with Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994)12. Using Moses we build competitive baselines on the training data described above. Word alignment is produced by the Berkeley Aligner (Liang et al., 2006). The decoder is based on the log-linear combination of a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, word and phrase penalties. The reordering model is a hierarchical phrase orientation model (Tillmann, 2004; Koehn et al., 2005; Galley and Manning, 2008) trained on all the available parallel data. We choose the hierarchical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across pun</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 104–111, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Muntsin Kolss</author>
</authors>
<title>A POS-based model for long-range reorderings in SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>206--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="4447" citStr="Niehues and Kolss, 2009" startWordPosition="680" endWordPosition="683">ks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually written (Collins et al., 2005) or automatically le</context>
</contexts>
<marker>Niehues, Kolss, 2009</marker>
<rawString>Jan Niehues and Muntsin Kolss. 2009. A POS-based model for long-range reorderings in SMT. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 206–214, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen,</institution>
<contexts>
<context position="1150" citStr="Och, 2002" startWordPosition="171" endWordPosition="172">or translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks. 1 Introduction Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs. On one hand, the phrase-based approach (PSMT) (Och, 2002; Zens et al., 2002; Koehn et al., 2003), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency. On the other, tree-based approaches (Wu, 1997; Yamada, 2002; Chiang, 2005) gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders. Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be iso</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz Josef Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<contexts>
<context position="25634" citStr="Och, 2003" startWordPosition="4242" endWordPosition="4243">ntation model (Tillmann, 2004; Koehn et al., 2005; Galley and Manning, 2008) trained on all the available parallel data. We choose the hierarchical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and test08). 7.2 Translation quality and efficiency results We evaluate translations with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). As these scores are only indirectly sensitive to word order, we also compute KRS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Erhard Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>User’s guide to sigf: Significance testing by approximate randomisation.</title>
<date>2006</date>
<marker>Pad´o, 2006</marker>
<rawString>Sebastian Pad´o, 2006. User’s guide to sigf: Significance testing by approximate randomisation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="25787" citStr="Papineni et al., 2002" startWordPosition="4262" endWordPosition="4265">chical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and test08). 7.2 Translation quality and efficiency results We evaluate translations with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). As these scores are only indirectly sensitive to word order, we also compute KRS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL), pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="26531" citStr="Riezler and Maxwell, 2005" startWordPosition="4384" endWordPosition="4387">RS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on these test subsets as KRS(R). Statistically significant differences are assessed by approximate randomization as in (Riezler and Maxwell, 2005)13. Tab. 1 reports results obtained by varying the DL junctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. 12http://www2.lingsoft.fi/cgi-bin/gertwol 13Translation scores and significance tests are computed with the tools multeval (Clark et al., 2011) and sigf (Pad´o, 2006). and modifying the distortion function. To evaluate the reordering selection technique, we also compare the encoding of all rule-generated reorderings against only the 3 best per rule-matching sequence, as ranked by our best performing reordered LM (see end of Sect. 5)</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57–64, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="12502" citStr="Schmid, 1994" startWordPosition="1976" endWordPosition="1977">or German-English we propose a set of three rules4 aimed at arranging the German constituents in SVO order: • infinitive: move each infinitive VC right after a preceding punctuation; • subordinate: if a VC is immediately followed by a punctuation, place it after a preceding subordinating conjunction (KOUS) or substitutive relative pronoun (PRELS); 2Chunk annotation does not identify subject and complement boundaries, nor the relations among constituents that are needed to deterministically rearrange a sentence in SVO order. 3We use AMIRA (Diab et al., 2004) to annotate Arabic and Tree Tagger (Schmid, 1994) to annotate German. 4A similar rule set was previously used to produce chunk reordering lattices in (Hardmeier et al., 2010). 480 (b) German broken verb chunk: three permutations Figure 1: Examples of chunk permutations generated by shallow syntax reordering rules. Chunk types: CC conjunction, VC verb (auxiliary/past participle), PC preposition, NC noun, Pct punctuation. wand CC1 Pct6 Pct6 &gt;0 Pct6 CC1 PC3 VC2 NC4 PC5 CC1 CC1 -89 PC3 NC4 VC2 PC5 98:7 PC3 NC4 =&gt;=? ?A&amp;quot; PC5 4 VC2 =&gt; CC1 NC4 VC2 PC3 PC5 D0F0734A4LFF =040 Pct6 CC1 NC4 PC5 VC2 PC3 &amp; Pct6 6&amp;quot;N** &amp;quot;FQ M% ORS =38 $Ark took part VC2 fy A(</context>
<context position="24551" citStr="Schmid, 1994" startWordPosition="4076" endWordPosition="4077">izes special characters. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme11. German tokenization 9Note that lexicalized reordering models use real word distances to compute the orientation class of a new hypothesis, thus they are not affected by changes in the matrix. 10That is everything except the small GALE corpus and the UN corpus. As reported by Green et al. (2010) the removal of UN data does not affect baseline performances on news test. 11The Arabic Treebank tokenization scheme isolates con483 and compound splitting is performed with Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994)12. Using Moses we build competitive baselines on the training data described above. Word alignment is produced by the Berkeley Aligner (Liang et al., 2006). The decoder is based on the log-linear combination of a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, word and phrase penalties. The reordering model is a hierarchical phrase orientation model (Tillmann, 2004; Koehn et al., 2005; Galley and Manning, 2008) trained on all the available parallel data. We cho</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A Unigram Orientation Model for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="6348" citStr="Tillmann, 2004" startWordPosition="978" endWordPosition="979">reordering steps. Existing approaches range from basic linear distortion to more complex models that are conditioned on the words being translated. The linear distortion model (Koehn et al., 2003) encourages monotonic translations by penalizing source position jumps proportionally to their length. If used alone, this model is inadequate for language pairs with different word orders. Green et al. (2010) tried to improve it with a future distortion cost estimate. Thus they were able to preserve baseline performance at a very high DL, but not to improve it. Lexicalized phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle local reordering and are widely adopted by the PSMT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or precede the last translated one. Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of le</context>
<context position="25053" citStr="Tillmann, 2004" startWordPosition="4150" endWordPosition="4151">Treebank tokenization scheme isolates con483 and compound splitting is performed with Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994)12. Using Moses we build competitive baselines on the training data described above. Word alignment is produced by the Berkeley Aligner (Liang et al., 2006). The decoder is based on the log-linear combination of a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, word and phrase penalties. The reordering model is a hierarchical phrase orientation model (Tillmann, 2004; Koehn et al., 2005; Galley and Manning, 2008) trained on all the available parallel data. We choose the hierarchical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1451" citStr="Wu, 1997" startWordPosition="218" endWordPosition="219">nd German-English using well-known SMT benchmarks. 1 Introduction Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs. On one hand, the phrase-based approach (PSMT) (Och, 2002; Zens et al., 2002; Koehn et al., 2003), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency. On the other, tree-based approaches (Wu, 1997; Yamada, 2002; Chiang, 2005) gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders. Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1, producing overall highquality outputs and isolated but critical re</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<tech>Ph.D. thesis,</tech>
<pages>508--514</pages>
<institution>Department of Computer Science, University of Southern</institution>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="2240" citStr="Xia and McCord, 2004" startWordPosition="334" endWordPosition="337"> between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1, producing overall highquality outputs and isolated but critical reordering errors that undermine the global sentence meaning. Previous works on this type of language pairs have mostly focused on source reordering prior to translation (Xia and McCord, 2004; Collins et al., 2005), or on sophisticated reordering models integrated into decoding (Koehn et al., 2005; Al-Onaizan and Papineni, 2006), achieving mixed results. To merge the best of both approaches – namely, access to rich context in the former and natural coupling of reordering and translation decisions in the latter – we introduce modified distortion matrices: a novel method to seamlessly provide to the decoder a set of likely long reorderings pre-computed for a given input sentence. Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a</context>
<context position="4135" citStr="Xia and McCord, 2004" startWordPosition="628" endWordPosition="631">r Computational Linguistics, pages 478–487, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics reordered n-gram LMs and, finally, explain the notion of modified distortion matrices. In the last part of the paper, we evaluate the proposed techniques on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely re</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proceedings of Coling 2004, pages 508– 514, Geneva, Switzerland, Aug 23–Aug 27. COLING. Sirvan Yahyaei and Christof Monz. 2010. Dynamic distortion in a discriminative reordering model for statistical machine translation. In International Workshop on Spoken Language Translation (IWSLT), Paris, France. Kenji Yamada. 2002. A syntax-based translation model. Ph.D. thesis, Department of Computer Science, University of Southern California, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative reordering models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="6388" citStr="Zens and Ney, 2006" startWordPosition="984" endWordPosition="987">s range from basic linear distortion to more complex models that are conditioned on the words being translated. The linear distortion model (Koehn et al., 2003) encourages monotonic translations by penalizing source position jumps proportionally to their length. If used alone, this model is inadequate for language pairs with different word orders. Green et al. (2010) tried to improve it with a future distortion cost estimate. Thus they were able to preserve baseline performance at a very high DL, but not to improve it. Lexicalized phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle local reordering and are widely adopted by the PSMT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or precede the last translated one. Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al., 2010). These models</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative reordering models for statistical machine translation. In Proceedings on the Workshop on Statistical Machine Translation, pages 55–63, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>In 25th German Conference on Artificial Intelligence (KI2002),</booktitle>
<pages>18--32</pages>
<publisher>Springer Verlag.</publisher>
<location>Aachen, Germany.</location>
<contexts>
<context position="1169" citStr="Zens et al., 2002" startWordPosition="173" endWordPosition="176">ion. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks. 1 Introduction Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs. On one hand, the phrase-based approach (PSMT) (Och, 2002; Zens et al., 2002; Koehn et al., 2003), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency. On the other, tree-based approaches (Wu, 1997; Yamada, 2002; Chiang, 2005) gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders. Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based statistical machine translation. In 25th German Conference on Artificial Intelligence (KI2002), pages 18–32, Aachen, Germany. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqi Zhang</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT 2007 /AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="4372" citStr="Zhang et al., 2007" startWordPosition="668" endWordPosition="671"> the paper, we evaluate the proposed techniques on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-proces</context>
</contexts>
<marker>Zhang, Zens, Ney, 2007</marker>
<rawString>Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation. In Proceedings of SSST, NAACL-HLT 2007 /AMTA Workshop on Syntax and Structure in Statistical Translation, pages 1–8, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Franz Och</author>
<author>Jay Ponte</author>
</authors>
<title>A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1145--1152</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="3448" citStr="Zollmann et al., 2008" startWordPosition="524" endWordPosition="527">s results in a linguistically informed definition of the search space that simplifies the task of the in-decoder reordering model, besides decreasing its complexity. The paper is organized as follows. After reviewing a selection of relevant works, we analyze salient reordering patterns in Arabic-English and GermanEnglish, and describe the corresponding chunkbased reordering rule sets. In the following sections we present a reordering selection technique based on 1A good comparison of phrase-based and tree-based approaches across language pairs with different reordering levels can be found in (Zollmann et al., 2008). 478 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 478–487, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics reordered n-gram LMs and, finally, explain the notion of modified distortion matrices. In the last part of the paper, we evaluate the proposed techniques on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Franz Och, and Jay Ponte. 2008. A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145–1152, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>