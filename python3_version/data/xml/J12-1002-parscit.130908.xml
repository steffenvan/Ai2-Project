<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999608">
A Context-Theoretic Framework for
Compositionality in Distributional Semantics
</title>
<author confidence="0.998256">
Daoud Clarke*
</author>
<affiliation confidence="0.980129">
University of Hertfordshire
</affiliation>
<bodyText confidence="0.989677111111111">
Formalizing “meaning as context” mathematically leads to a new, algebraic theory of meaning,
in which composition is bilinear and associative. These properties are shared by other methods
that have been proposed in the literature, including the tensor product, vector addition, point-
wise multiplication, and matrix multiplication.
Entailment can be represented by a vector lattice ordering, inspired by a strengthened
form of the distributional hypothesis, and a degree of entailment is defined in the form of a
conditional probability. Approaches to the task of recognizing textual entailment, including the
use of subsequence matching, lexical entailment probability, and latent Dirichlet allocation, can
be described within our framework.
</bodyText>
<sectionHeader confidence="0.995197" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999973523809524">
This article presents the thesis that defining meaning as context leads naturally to
a model in which meanings of strings are represented as elements of an associative
algebra over the real numbers, and entailment is described by a vector lattice ordering.
This model is general enough to encompass several proposed methods of composition
in vector-based representations of meaning.
In recent years, the abundance of text corpora and computing power has allowed
the development of techniques to analyze statistical properties of words. For example
techniques such as latent semantic analysis (Deerwester et al. 1990) and its variants,
and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspects
of the meanings of words by statistical analysis, and statistical information is often
used when parsing to determine sentence structure (Collins 1997). These techniques
have proved useful in many applications within computational linguistics and natural
language processing (Grefenstette 1994; Sch¨utze 1998; Bellegarda 2000; Choi, Wiemer-
Hastings, and Moore 2001; Lin 2003; McCarthy et al. 2004), arguably providing evidence
that they capture something about the nature of words that should be included in
representations of their meaning. However, it is very difficult to reconcile these tech-
niques with existing theories of meaning in language, which revolve around logical
and ontological representations. The new techniques, almost without exception, can be
viewed as dealing with vector-based representations of meaning, placing meaning (at
least at the word level) within the realm of mathematics and algebra; conversely the
older theories of meaning dwell in the realm of logic and ontology. It seems there is
</bodyText>
<note confidence="0.651986">
* Gorkana Group, Discovery House, 28–48 Banner Street, London EC1Y8QE.
</note>
<email confidence="0.828292">
E-mail: daoud.clarke@gorkana.com.
</email>
<note confidence="0.9245735">
Submission received: 29 October 2008; revised submission received: 28 April 2011; accepted for publication:
29 May 2011.
© 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.974833673469388">
no unifying theory of meaning to provide guidance to those making use of the new
techniques.
The problem appears to be a fundamental one in computational linguistics because
the whole foundation of meaning seems to be in question. The older, logical theories
often subscribe to a model-theoretic philosophy of meaning (Kamp and Reyle 1993;
Blackburn and Bos 2005). According to this approach, sentences should be translated to
a logical form that can be interpreted as a description of the state of the world. The new
vector-based techniques, on the other hand, are often closer in spirit to the philosophy
of meaning as context, the idea that the meaning of an expression is determined by how
it is used. This is an old idea with origins in the philosophy of Wittgenstein (1953), who
said that “meaning just is use,” Firth’s (1968) “You shall know a word by the company
it keeps,” and the distributional hypothesis of Harris (1968), that words will occur in
similar contexts if and only if they have similar meanings. This hypothesis is justified
by the success of techniques such as latent semantic analysis as well as experimental
evidence (Miller and Charles 1991). Although the two philosophies are not obviously
incompatible—especially because the former applies mainly at the sentence level and
the latter mainly at the word level—it is not clear how they relate to each other.
The problem of how to compose vector representations of meanings of words has
recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and
Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara
2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier
work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998;
Kintsch 2001). A solution to this problem would have practical as well as philosophical
benefits. Current techniques such as latent semantic analysis work well at the word
level, but we cannot extend them much beyond this, to the phrase or sentence level,
without quickly encountering the data-sparseness problem: There are not enough occur-
rences of strings of words to determine what their vectors should be merely by looking
in corpora. If we knew how such vectors should compose then we would be able to
extend the benefits of the vector based techniques to the many applications that require
reasoning about the meaning of phrases and sentences.
This article describes the results of our own efforts to identify a theory that can unite
these two paradigms, introduced in the author’s DPhil thesis (Clarke 2007). In addition,
we also discuss the relationship between this theory and methods of composition that
have recently been proposed in the literature, showing that many of them can be
considered as falling within our framework.
Our approach in identifying the framework is summarized in Figure 1:
• Inspired by the philosophy of meaning as context and vector-based
techniques we developed a mathematical model of meaning as context,
in which the meaning of a string is a vector representing contexts in
which that string occurs in a hypothetical infinite corpus.
• The theory on its own is not useful when applied to real-world corpora
because of the problem of data sparseness. Instead we examine the
mathematical properties of the model, and abstract them to form a
framework which contains many of the properties of the model.
Implementations of the framework are called context theories because
they can be viewed as theories about the contexts in which strings
occur. By analogy with the term “model-theoretic” we use the term
“context-theoretic” for concepts relating to context theories, thus we
call our framework the context-theoretic framework.
</bodyText>
<page confidence="0.996844">
42
</page>
<figure confidence="0.600233">
Clarke A Context-Theoretic Framework for Distributional Semantics
</figure>
<figureCaption confidence="0.99046">
Figure 1
</figureCaption>
<bodyText confidence="0.922417">
Our approach in developing the context-theoretic framework.
</bodyText>
<listItem confidence="0.6165545">
• In order to ensure that the framework was practically useful, context
theories were developed in parallel with the framework itself. The aim
was to be able to describe existing approaches to representing meaning
within the framework as fully as possible.
</listItem>
<bodyText confidence="0.8069915">
In developing the framework we were looking for specific properties; namely, we
wanted it to:
</bodyText>
<listItem confidence="0.944373363636364">
• provide some guidelines describing in what way the representation of a
phrase or sentence should relate to the representations of the individual
words as vectors;
• require information about the probability of a string of words to be
incorporated into the representation;
• provide a way to measure the degree of entailment between strings based
on the particular meaning representation;
• be general enough to encompass logical representations of meaning; and
• be able to incorporate the representation of ambiguity and uncertainty,
including statistical information such as the probability of a parse or the
probability that a word takes a particular sense.
</listItem>
<bodyText confidence="0.999938333333333">
The framework we present is abstract, and hence does not subscribe to a particular
method for obtaining word vectors: They may be raw frequency counts, or vectors ob-
tained by a method such as latent semantic analysis. Nor does the framework provide a
recipe for how to represent meaning in natural language; instead it provides restrictions
on the set of possibilities. The advantage of the framework is in ensuring that techniques
are used in a way that is well-founded in a theory of meaning. For example, given vector
representations of words, there is not one single way of combining these to give vector
representations of phrases and sentences, but in order to fit within the framework there
are certain properties of the representation that need to hold. Any method of combining
</bodyText>
<page confidence="0.999447">
43
</page>
<note confidence="0.798698">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.9929428">
these vectors in which these properties hold can be considered within the framework
and is thus justified according to the underlying theory; in addition the framework
instructs us as to how to measure the degree of entailment between strings according to
that particular method.
The contribution of this article is as follows:
</bodyText>
<listItem confidence="0.962735857142857">
• We define the context-theoretic framework and introduce the mathematics
necessary to understand it. The description presented here is cleaner than
that of Clarke (2007), and in addition we provide examples that should
provide intuition for the concepts we describe.
• We relate the framework to methods of composition that have been
proposed in the literature, namely:
– vector addition (Landauer and Dumais 1997; Foltz, Kintsch,
and Landauer 1998);
– the tensor product (Smolensky 1990; Clark and Pulman 2007;
Widdows 2008);
– the multiplicative models of Mitchell and Lapata (2008);
– matrix multiplication (Baroni and Zamparelli 2010; Rudolph
and Giesbrecht 2010); and
– the approach of Clark, Coecke, and Sadrzadeh (2008).
</listItem>
<bodyText confidence="0.999984538461538">
It is important to note that the purpose of describing related work in terms of our
framework is not merely to demonstrate the generality of our framework: In doing
so, we identify previously ignored features of this work such as the lattice structure
within the vector space. This allows any one of these approaches to be endowed with an
entailment property defined by this lattice structure, based on a philosophy of meaning
as context.
Although the examples described here show that existing approaches can be de-
scribed within the framework and show some of its potential, they cannot demonstrate
its full power. The mathematical structures we make use of are extremely general, and
we hope that in the future many interesting discoveries will be made by exploring the
realm we identify here.
Our approach in defining the framework may be perceived as overly abstract;
however, we believe this approach has many potential benefits, because approaches to
composition which may have been considered unrelated (such as the tensor product
and vector addition) are now shown to be related. This means that when studying
such constructions, work can be avoided by considering the general case, for the same
reason that class inheritance aids code reuse. For example, definitions given in terms
of the framework can be applied to all instances, such as our definition of a degree
of entailment. We also hope to motivate people to prove theorems in terms of the
framework, having demonstrated its wide applicability.
The remainder of the article is as follows: In Section 2 we define our framework,
introducing the necessary definitions, and showing how related work fits into the
framework. In Section 3 we introduce our motivating example, showing that a simple
mathematical definition of the notions of “corpus” and “context” leads to an instance
of our framework. In Section 4, we describe specific instances of our framework in
application to the task of recognizing textual entailment. In Section 5 we show how the
</bodyText>
<page confidence="0.998057">
44
</page>
<note confidence="0.716028">
Clarke A Context-Theoretic Framework for Distributional Semantics
</note>
<bodyText confidence="0.724">
sophisticated approach of Clark, Coecke, and Sadrzadeh (2008) can be described within
our framework. Finally, in Section 6 we present our conclusions and plans for further
work.
</bodyText>
<sectionHeader confidence="0.963534" genericHeader="method">
2. Context Theory
</sectionHeader>
<bodyText confidence="0.9999405">
In this section, we define the fundamental concept of our concern, a context theory,
and discuss its properties. The definition is an abstraction of both the more commonly
used methods of defining composition in vector-based semantics and our motivating
example of meaning as context, described in the next section. Because of its relation to
this motivating example, a context theory can be thought of as a hypothesis describing
in what contexts all strings occur.
</bodyText>
<sectionHeader confidence="0.420216" genericHeader="method">
Definition 1 (Context Theory)
</sectionHeader>
<bodyText confidence="0.999966666666667">
A context theory is a tuple (A, A, &amp;, V, *), where A is a set (the alphabet), A is a unital
algebra over the real numbers, &amp; is a function from A to A, V is an abstract Lebesgue
space, and * is an injective linear map from A to V.
We will explain each part of this definition, introducing the necessary mathematics
as we proceed. We assume the reader is familiar with linear algebra; see Halmos (1974)
for definitions that are not included here.
</bodyText>
<subsectionHeader confidence="0.9986">
2.1 Algebra over a Field
</subsectionHeader>
<bodyText confidence="0.932692285714286">
We have identified an algebra over a field (or simply algebra when there is no am-
biguity) as an important construction because it generalizes nearly all the methods of
vector-based composition that have been proposed. An algebra adds a multiplication
operation to a vector space; the vector space is intended to describe meaning, and it
is this multiplication operation that defines the composition of meaning in context-
theoretic semantics.
Definition 2 (Algebra over a Field)
</bodyText>
<equation confidence="0.84190475">
An algebra over a field is a vector space A over a field K together with a binary
operation (a, b) �-+ ab on A that is bilinear,
a(ocb + Pc) = ocab + Pac (1)
(oca + Pb)c = ocac + Pbc (2)
</equation>
<bodyText confidence="0.945000444444445">
and associative, (ab)c = a(bc) for all a, b, c E A and all oc, P E K. Some authors do not
place the requirement that an algebra is associative, in which case our definition would
refer to an associative algebra. An algebra is called unital if it has a distinguished unity
element 1 satisfying 1x = x1 = x for all x E A. We are generally only interested in real
algebras, where K is the field of real numbers, R.
Example 1
The square real-valued matrices of order n form a real unital associative algebra under
standard matrix multiplication. The vector operations are defined entry-wise. The unity
element of the algebra is the identity matrix.
</bodyText>
<page confidence="0.997268">
45
</page>
<note confidence="0.296995">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.996394666666667">
This means that our proposal is more general than that of Rudolph and Giesbrecht
(2010), who suggest using matrix multiplication as a framework for distributional
semantic composition. The main differences in our proposal are as follows.
</bodyText>
<listItem confidence="0.977294285714286">
• We allow dimensionality to be infinite, instead of restricting ourselves to
finite-dimensional matrices.
• Matrix algebras form a ∗-algebra, whereas we do not currently impose this
requirement.
• Many of the vector spaces used in computational linguistics have an
implicit lattice structure; we emphasize the importance of this structure
and use the associated partial ordering to define entailment.
</listItem>
<bodyText confidence="0.786286">
The purpose of ξ in the context theory is to associate elements of the algebra with
strings of words. Considering only the multiplication of A (and ignoring the vector
operations), A is a monoid, because we assumed that the multiplication on A is asso-
ciative. Then ξ induces a monoid homomorphism a �-+ aˆ from A* to A. We denote the
mapped value of a ∈ A* by aˆ ∈ A, which is defined as follows:
</bodyText>
<equation confidence="0.999149">
aˆ = ξ(a1)ξ(a2) ... ξ(an) (3)
</equation>
<bodyText confidence="0.999983181818182">
where a = a1a2 ... an for ai ∈ A, and we define e = 1, where c is the empty string. Thus,
the mapping defined by ˆ allows us to associate an element of the algebra with every
string of words.
The algebra is what tells us how meanings compose. A crucial part of our thesis
is that meanings can be represented by elements of an algebra, and that the type of
composition that can be defined using an algebra is general enough to describe the
composition of meaning in natural language. To go some way towards justifying this,
we give several examples of algebras that describe methods of composition that have
been proposed in the literature: namely, point-wise multiplication (Mitchell and Lapata
2008), vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998),
and the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008).
</bodyText>
<subsectionHeader confidence="0.832335">
Example 2 (Point-wise Multiplication)
</subsectionHeader>
<bodyText confidence="0.999923666666667">
Consider the n-dimensional real vector space Rn. We describe a vector u ∈ Rn in terms
of its components as (u1, u2, ... un) with each ui ∈ R. We can define a multiplication · on
this space by
</bodyText>
<equation confidence="0.984615">
(u1, u2, ...,un) · (v1, v2, ... , vn) = (u1v1, u2v2, ... unvn) (4)
</equation>
<bodyText confidence="0.9994348">
It is easy to see that this satisfies the requirements for an algebra as specified earlier.
Table 1 shows a simple example of possible occurrences for three terms in three different
contexts, d1, d2, and d3, which may, for example, represent documents. We use this to
define the mapping ξ from terms to vectors. Thus, in this example, we have ξ(cat) =
(0, 2,3) and ξ(big) = (1, 3, 0). Under point-wise multiplication, we would have
</bodyText>
<equation confidence="0.6978345">
�
big cat = ξ(big) · ξ(cat) = (1, 3,0) · (0, 2, 3) = (0, 6, 0) (5)
</equation>
<page confidence="0.996955">
46
</page>
<table confidence="0.461104">
Clarke A Context-Theoretic Framework for Distributional Semantics
</table>
<tableCaption confidence="0.995197">
Table 1
</tableCaption>
<table confidence="0.671936">
Example of possible occurrences for three terms in three different contexts.
d1 d2 d3
cat 0 2 3
animal 2 1 2
big 1 3 0
</table>
<bodyText confidence="0.81438275">
One commonly used operation for composing vector-based representations of
meaning is vector addition. As noted by Rudolph and Giesbrecht (2010), this can be
described using matrix multiplication, by embedding an n-dimensional vector u into a
matrix of order n + 1:
</bodyText>
<equation confidence="0.9969042">
α u1 u2 ··· un
0 α 0 ··· 0
0 0 α ··· 0
...
0 0 0 ··· α
</equation>
<bodyText confidence="0.933400285714286">
where α = 1. The set of all such matrices, for all real values of α, forms a subalgebra of
the algebra of matrices of order n + 1. A subalgebra of an algebra A is a sub-vector space
of A which is closed under the multiplication of A. This subalgebra can be equivalently
described as follows:
Example 3 (Additive Algebra)
For two vectors u = (α, u1, u2, ... un) and v = (β, v1, v2 ... vn) in Rn+1, we define the
additive product ® by
</bodyText>
<equation confidence="0.855968">
u ® v = (αβ, αv1 + βu1, αv2 + βu2, ... αvn + βun) (7)
</equation>
<bodyText confidence="0.999632">
To verify that this multiplication makes Rn+1 an algebra, we can directly verify the
bilinear and associativity requirements, or check that it is isomorphic to the subalgebra
of matrices discussed previously.
Using Table 1, we define ξ+ so that it maps n-dimensional context vectors to Rn+1,
where the first component is 1, so ξ+(big) = (1, 1, 3, 0) and ξ+(cat) = (1, 0, 2,3) and
</bodyText>
<equation confidence="0.9645">
�
big cat = ξ+(big) ® ξ+(cat) = (1, 1, 5,3) (8)
</equation>
<bodyText confidence="0.9996267">
Point-wise multiplication and addition are not ideal as methods for composing
meaning in natural language because they are commutative; although it is often useful
to consider the simpler, commutative case, natural language itself is inherently non-
commutative. One obvious method of composing vectors that is not commutative is the
tensor product. This method of composition can be viewed as a product in an algebra by
considering the tensor algebra, which is formed from direct sums of all tensor powers
of a base vector space.
We assume the reader is familiar with the tensor product and direct sum (see
Halmos [1974] for definitions); we recall their basic properties here. Let Vn denote a
vector space of dimensionality n (note that all vector spaces of a fixed dimensionality
</bodyText>
<equation confidence="0.98754375">
�
� � � � � �
... ... ...
1 (6)
</equation>
<page confidence="0.94691">
47
</page>
<note confidence="0.248823">
Computational Linguistics Volume 38, Number 1
</note>
<equation confidence="0.7007856">
are isomorphic). Then the tensor product space Vn ⊗ Vm is isomorphic to a space Vnm of
dimensionality nm; moreover, given orthonormal bases B = {b1, b2, ... , bn} for Vn and
C = {c1, c2,.. . , cm} for Vm there is an orthonormal basis for Vnm defined by
{bi ⊗ cj : 1 ≤ i ≤ n and 1 ≤ j ≤ m} (9)
Example 4
</equation>
<bodyText confidence="0.998933666666667">
The multiplicative models of Mitchell and Lapata (2008) correspond to the class of finite
dimensional algebras. Let A be a finite-dimensional vector space. Then every associative
bilinear product on A can be described by a linear function T from A ⊗ A to A, as
required in Mitchell and Lapata’s model. To see this, consider the action of the product ·
on two orthonormal basis vectors a and b of A. This is a vector in A, thus we can define
T(a ⊗ b) = a · b. By considering all basis vectors, we can define the linear function T.
If the tensor product can loosely be viewed as “multiplying” vector spaces, then the
direct sum is like adding them; the space Vn ⊕ Vm has dimensionality n + m and has
basis vectors
</bodyText>
<equation confidence="0.9646895">
{bi ⊕ 0 : 1 ≤ i ≤ n} ∪ {0 ⊕ cj : 1 ≤ j ≤ m}; (10)
it is usual to write b ⊕ 0 as b and 0 ⊕ c as c.
Example 5 (Tensor Algebra)
If V is a vector space, then we define T(V), the free algebra of tensor algebra generated
by V, as:
T(V) = R ⊕ V ⊕ (V ⊗ V) ⊕ (V ⊗ V ⊗ V) ⊕ ··· (11)
</equation>
<bodyText confidence="0.999987181818182">
where we assume that the direct sum is commutative. We can think of it as the direct
sum of all tensor powers of V, with R representing the zeroth power. In order to make
this space an algebra, we define the product on elements of these tensor powers, viewed
as subspaces of the tensor algebra, as their tensor product. This is enough to define the
product on the whole space, because every element can be written as a sum of tensor
powers of elements of V. There is a natural embedding from V to T(V), where each
element maps to an element in the first tensor power. Thus for example we can think of
u, u ⊗ v, and u ⊗ v + w as elements of T(V), for all u, v, w ∈ V.
This product defines an algebra because the tensor product is a bilinear operation.
Taking V = R3 and using ξ as the natural embedding from the context vector of a string
T(V), our previous example becomes
</bodyText>
<equation confidence="0.9964505">
big cat = ξ(big) ⊗ ξ(cat) (12)
= (1, 3,0) ⊗ (0, 2, 3) (13)
∼= (1(0,2,3),3(0,2,3),0(0,2,3)) (14)
∼= (0, 2, 3, 0, 6, 9, 0, 0, 0) (15)
</equation>
<bodyText confidence="0.9989055">
where the last two lines demonstrate how a vector in R3 ⊗ R3 can be described in the
isomorphic space R9.
</bodyText>
<page confidence="0.997982">
48
</page>
<figure confidence="0.336684">
Clarke A Context-Theoretic Framework for Distributional Semantics
</figure>
<subsectionHeader confidence="0.985281">
2.2 Vector Lattices
</subsectionHeader>
<bodyText confidence="0.9981358">
The next part of the definition specifies an abstract Lebesgue space. This is a special
kind of vector lattice, or even more generally, a partially ordered vector space. This
lattice structure is implicit in most vector spaces used in computational linguistics, and
an important part of our thesis is that the partial ordering can be interpreted as an
entailment relation.
</bodyText>
<subsectionHeader confidence="0.487902">
Definition 3 (Partially Ordered Vector Space)
</subsectionHeader>
<bodyText confidence="0.746711">
A partially ordered vector space V is a real vector space together with a partial ordering
&lt; such that:
</bodyText>
<construct confidence="0.790901428571429">
if x &lt; y then x + z &lt; y + z
if x &lt; y then αx &lt; αy
for all x, y, z E V, and for all α &gt; 0. Such a partial ordering is called a vector space order
on V. An element u of V satisfying u &gt; 0 is called a positive element; the set of all
positive elements of V is denoted V+. If &lt; defines a lattice on V then the space is called
a vector lattice or Riesz space.
Example 6 (Lattice Operations on Rn)
</construct>
<bodyText confidence="0.99039025">
A vector lattice captures many properties that are inherent in real vector spaces when
there is a distinguished basis. In Rn, given a specific basis, we can write two vectors u and
v as sequences of numbers: u = (u1, u2, ... un) and v = (v1, v2, ... vn). This allows us to
define the lattice operations of meet n and join V as
</bodyText>
<equation confidence="0.9874115">
u n v = (min(u1, v1), min(u2, v2),... min(un, vn)) (16)
u V v = (max(u1,v1),max(u2,v2),...max(un,vn)) (17)
</equation>
<bodyText confidence="0.999577">
These are the component-wise minimum and maximum, respectively. The partial
ordering is then given by u &lt; v if and only if u n v = u, or equivalently un &lt; vn for all
n. A graphical depiction of the meet operation is shown in Figure 2.
The vector operations of addition and multiplication by scalar, which can be defined
in a similar component-wise fashion, are nevertheless independent of the particular
</bodyText>
<figureCaption confidence="0.686836">
Figure 2
</figureCaption>
<bodyText confidence="0.7255725">
Vector representations of the terms orange and fruit based on hypothetical occurrences in six
documents and their vector lattice meet (the darker shaded area).
</bodyText>
<page confidence="0.992106">
49
</page>
<note confidence="0.285139">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.999448270833333">
basis chosen. Conversely, the lattice operations depend on the choice of basis, so the
operations as defined herein would behave differently if the components were written
using a different basis. We argue that it makes sense for us to consider these properties
of vectors in the context of computational linguistics because we can often have a distin-
guished basis: namely, the one defined by the contexts in which terms occur. Of course it
is true that techniques such as latent semantic analysis introduce a new basis which does
not have a clear interpretation in relation to contexts; nevertheless they nearly always
identify a distinguished basis which we can use to define the lattice operations. Because
our aim is a theory of meaning as context, we should include in our theory a description
of the lattice structure which arises out of consideration of these contexts.
We argue that the mere association of words with vectors is not enough to constitute
a theory of meaning—a theory of meaning must allow us to interpret these vectors. In
particular it should be able to tell us whether one meaning entails or implies another;
indeed this is one meaning of the verb to mean. Entailment is an asymmetric relation: “x
entails y” does not have the same meaning as “y entails x”. Vector representations allow
the measurement of similarity or distance, through an inner product or metric; this is a
symmetric relation, however, and so cannot be suitable for describing entailment.
In propositional and first order logic, the entailment relation is a partial ordering;
in fact it is a Boolean algebra, which is a special kind of lattice. It seems natural to
consider whether the lattice structure that is inherent in the vector representations used
in computational linguistics can be used to model entailment.
We believe our framework is suited to all vector-based representations of natural
language meaning, however the vectors are obtained. Given this assumption, we can
only justify our assumption that the partial order structure of the vector space is suitable
to represent the entailment relation by observing that it has the right kind of properties
we would expect from this relation.
There may be more justification for this assumption, however, based on the case
where the vectors for terms are simply their frequencies of occurrences in n different
contexts, so that they are vectors in Rn. In this case, the relation ξ(x) &lt; ξ(y) means
that y occurs at least as frequently as x in every context. This means that y occurs in
at least as wide a range of contexts as x, and occurs as least as frequently as x. Thus the
statement “x entails y if and only if ξ(x) &lt; ξ(y)” can be viewed as a stronger form of the
distributional hypothesis of Harris (1968).
In fact, this idea can be related to the notion of distributional generality, introduced
by Weeds, Weir, and McCarthy (2004) and developed by Geffet and Dagan (2005). A
term x is distributionally more general than another term y if x occurs in a subset of the
contexts that y occurs in. The idea is that distributional generality may be connected to
semantic generality. An example of this is the hypernymy or is-a relation that is used to
express generality of concepts in ontologies; for example, the term animal is a hypernym
of dog because a dog is an animal. Weeds, Weir, and McCarthy (2004, p. 1019) explain
the connection to distributional generality as follows:
Although one can obviously think of counter-examples, we would generally expect that
the more specific term dog can only be used in contexts where animal can be used and
that the more general term animal might be used in all of the contexts where dog is used
and possibly others. Thus, we might expect that distributional generality is correlated
with semantic generality...
Our proposal, in the case where words are represented by frequency vectors, can
be considered a stronger version of distributional generality, where the additional
</bodyText>
<page confidence="0.941513">
50
</page>
<figure confidence="0.210247">
Clarke A Context-Theoretic Framework for Distributional Semantics
</figure>
<bodyText confidence="0.954339619047619">
requirement is on the frequency of occurrences. In practice, this assumption is unlikely
to be compatible with the ontological view of entailment. For example the term entity
is semantically more general than the term animal; however, entity is unlikely to occur
more frequently in each context, because it is a rarer word. A more realistic foundation
for this assumption might be if we were to consider the components for a word to
represent the plausibility of observing the word in each context. The question then,
of course, is how such vectors might be obtained. Another possibility is to attempt to
weight components in such a way that entailment becomes a plausible interpretation
for the partial ordering relation.
Even if we allow for such alternatives, however, in general it is unlikely that the
relation will hold between any two strings, because u &lt; v iff ui &lt; vi for each component,
ui, vi, of the two vectors. Instead, we propose to allow for degrees of entailment. We take
a Bayesian perspective on this, and suggest that the degree of entailment should take
the form of a conditional probability. In order to define this, however, we need some
additional structure on the vector lattice that allows it to be viewed as a description of
probability, by requiring it to be an abstract Lebesgue space.
Definition 4 (Banach Lattice)
A Banach lattice V is a vector lattice together with a norm II · II such that V is complete
with respect to II · II.
Definition 5 (Abstract Lebesgue Space)
An abstract Lebesgue (or AL) space is a Banach lattice V such that
</bodyText>
<equation confidence="0.827515">
IIu + vII = IIuII + IIvII (18)
</equation>
<figureCaption confidence="0.5510985">
for all u,v in V with u &gt; 0, v &gt; 0 and u ∧ v = 0.
Example 7 (fp Spaces)
</figureCaption>
<bodyText confidence="0.99995175">
Let u = (u1, u2, ...) be an infinite sequence of real numbers. We can view ui as compo-
nents of the infinite-dimensional vector u. We call the set of all such vectors the sequence
space; it is a vector space where the operations are defined component-wise. We define
a set of norms, the fp-norms, on the space of all such vectors by
</bodyText>
<equation confidence="0.968016">
1/p
IIuIIp = |ui|p (19)
i&gt;0
</equation>
<bodyText confidence="0.999668">
The space of all vectors u for which IIuIIp is finite is called the fp space. Considered
as vector spaces, these are Banach spaces, because they are complete with respect to
the associated norm, and under the component-wise lattice operations, they are Banach
lattices. In particular, the f1 space is an abstract Lebesgue space under the f1 norm.
The finite-dimensional real vector spaces Rn can be considered as special cases of
the sequence spaces (consisting of vectors in which all but n components are zero) and,
because they are finite-dimensional, we can use any of the fp norms. Thus, our previous
examples, in which &amp; mapped terms to vectors in Rn, can be considered as mapping to
abstract Lebesgue spaces if we adopt the f1 norm.
</bodyText>
<page confidence="0.982831">
51
</page>
<figure confidence="0.368417">
Computational Linguistics Volume 38, Number 1
</figure>
<subsectionHeader confidence="0.997345">
2.3 Degrees of Entailment
</subsectionHeader>
<bodyText confidence="0.973647133333333">
We propose that in vector-based semantics, a degree of entailment is more appropriate
than a black-and-white observation of whether or not entailment holds. If we think of
the vectors as describing “degrees of meaning,” it makes sense that we should then look
for degrees of entailment.
Conditional probability is closely connected to entailment: If A entails B, then
P(B|A) = 1. Moreover, if A and B are mutually exclusive, then P(A|B) = P(B|A) = 0. It
is thus natural to think of conditional probability as a degree of entailment.
An abstract Lebesgue space has many of the properties of a probability space, where
the set operations of a probability space are replaced by the lattice operations of the
vector space. This means that we can think of an abstract Lebesgue space as a vector-
based probability space. Here, events correspond to positive elements with the norm
less than or equal to 1; the probability of an event u is given by the norm (which we shall
always assume is the f1 norm), and the joint probability of two events u and v is ~u ∧ v~1.
Definition 6 (Degree of Entailment)
We define the degree to which u entails v in the form of a conditional probability:
</bodyText>
<equation confidence="0.992634">
Ent(u,v) = IIu ∧ v~1 (20)
~u~1
</equation>
<bodyText confidence="0.9627505">
If we are only interested in degrees of entailment (i.e., conditional probabilities) and
not probabilities, then we can drop the requirement that the norm should be less than
or equal to one, because conditional probabilities are automatically normalized. This
definition, together with the multiplication of the algebra, allows us to compute the
degree of entailment between any two strings according to the context theory.
Example 8
The vectors given in Table 1 give the following calculation for the degree to which cat
entails animal:
</bodyText>
<equation confidence="0.99976175">
,(cat) = (0,2,3)
,(animal) = (2,1,2)
,(cat) ∧ ,(animal) = (0,1,2)
Ent(,(cat), ,(animal)) = II,(cat) ∧ ,(animal)II1/II,(cat)II1 = 3/5
</equation>
<bodyText confidence="0.999152">
An important question is how this context-theoretic definition of the degree of
entailment relates to more familiar notions of entailment. There are three main ways
in which the term entailment is used:
</bodyText>
<listItem confidence="0.891699714285714">
• the model-theoretic sense of entailment in which a theory A entails a
theory B if every model of A is also a model of B. It was shown in Clarke
(2007) that this type of entailment can be described using context theories,
where sentences are represented as projections on a vector space.
• entailment between terms (as expressed for example in the WordNet
hierarchy), for example the hypernymy relation between the terms cat and
animal encodes the fact that a cat is an animal. In Clarke (2007) we showed
</listItem>
<page confidence="0.834933">
52
</page>
<listItem confidence="0.829487666666667">
Clarke A Context-Theoretic Framework for Distributional Semantics
that such relations can be encoded in the partial order structure of a vector
lattice.
• Human common-sense judgments as to whether one sentence entails or
implies another sentence, as used in the Recognising Textual Entailment
Challenges (Dagan, Glickman, and Magnini 2005).
</listItem>
<bodyText confidence="0.9991221875">
Our context-theoretic notion of entailment is thus intended to generalize both the first
two senses of entailment given here. In addition, we hope that context theories will be
useful in the practical application of recognizing textual entailment. Capturing this type
of entailment is not our initial aim because we are interested in foundational issues,
and doing well at this task poses major engineering challenges beyond the scope of
our work. Nevertheless, we believe the ability to represent the preceding two types of
entailment as well as standard distributional methods of composition bodes well for the
possibility of using our framework for this task. In Section 4 we describe several basic
approaches to textual entailment within the framework.
Our definition is more general than the model-theoretic and hypernymy notions of
entailment, however, as it allows the measurement of a degree of entailment between
any two strings: As an extreme example, one may measure the degree to which not a
entails in the. Although this may not be useful or philosophically meaningful, we view
it as a practical consequence of the fact that every string has a vector representation in
our model, which coincides with the current practice in vector-based compositionality
techniques (Clark, Coecke, and Sadrzadeh 2008; Widdows 2008).
</bodyText>
<subsectionHeader confidence="0.998739">
2.4 Lattice Ordered Algebras
</subsectionHeader>
<bodyText confidence="0.999960857142857">
A lattice ordered algebra merges the lattice ordering of the vector space V with the
product of A. This structure encapsulates the ordering properties that are familiar from
multiplication in matrices and elementary arithmetic. For this reason, many proposed
methods of composing vector-based representations of meaning can be viewed as lattice
ordered algebras. The only reason we have not included it as a requirement of the
framework is because our motivating example (described in the next section) is not
guaranteed to have this property, although it does give us a partially ordered algebra.
</bodyText>
<subsectionHeader confidence="0.820882">
Definition 7 (Partially Ordered Algebra)
</subsectionHeader>
<bodyText confidence="0.999985333333333">
A partially ordered algebra A is an algebra which is also a partially ordered vector
space, which satisfies u · v &gt; 0 for all u, v E A+. If the partial ordering is a lattice, then
A is called a lattice-ordered algebra.
</bodyText>
<subsectionHeader confidence="0.856956">
Example 9 (Lattice-Ordered Algebra of Matrices)
</subsectionHeader>
<bodyText confidence="0.859100166666667">
The matrices of order n form a lattice-ordered algebra under normal matrix multi-
plication, where the lattice operations are defined as the entry-wise minimum and
maximum.
Example 10 (Operators on V Spaces)
Matrices can be viewed as operators on finite-dimensional vector spaces; in fact this lat-
tice property extends to operators on certain infinite-dimensional spaces, the &amp; spaces,
</bodyText>
<page confidence="0.984707">
53
</page>
<note confidence="0.278199">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.9630945">
by the Riesz-Kantorovich theorem (Abramovich and Aliprantis 2002). The operations
are defined by:
</bodyText>
<equation confidence="0.999183">
(S V T)(u) = sup{S(v) + T(w) : v,w E U+ and v + w = u} (25)
(S n T)(u) = inf{S(v) + T(w) : v, w E U+ and v + w = u} (26)
</equation>
<bodyText confidence="0.82879375">
If A is a lattice-ordered algebra which is also an abstract Lebesgue space, then
(A, A, ξ, A, 1) is a context theory. In this simplified situation, A plays the role of the
vector lattice as well as the algebra; ξ maps from A to A as before, and 1 indicates the
identity map on A. Many of the examples we discuss will be of this form, so we will
use the shorthand notation, (A, A, ξ). It is tempting to adopt this as the definition of
context theory; as we will see in the next section, however, this is not supported by our
prototypical example of a context theory as in this case the algebra is not necessarily
lattice-ordered.
</bodyText>
<sectionHeader confidence="0.965481" genericHeader="method">
3. Context Algebras
</sectionHeader>
<bodyText confidence="0.9656125">
In this section we describe the prototypical examples of a context theory, the context
algebras. The definition of a context algebra originates in the idea that the notion of
“meaning as context” can be extended beyond the word level to strings of arbitrary
length. In fact, the notion of context algebra can be thought of as a generalization of the
syntactic monoid of a formal language: Instead of a set of strings defining the language,
we have a fuzzy set of strings, or more generally, a real-valued function on a free monoid.
We call such functions real-valued languages and they take the place of formal
languages in our theory. We attach a real number to each string which is intended as an
indication of its importance or likelihood of being observed; for example, those with a
value of zero are considered not to occur.
Definition 8 (Real-Valued Language)
Let A be a finite set of symbols. A real-valued language (or simply a language when
there is no ambiguity) L on A is a function from A* to R. If the range of L is a subset of
R+ then L is called a positive language. If the range of L is a subset of [0, 1] then L is
called a fuzzy language. If L is a positive language such that ExEA∗ L(x) = 1 then L is a
probability distribution over A*, a distributional language.
One possible interpretation for L when it is a distributional language is that L(x) is the
probability of observing the string x when selecting a document at random from an
infinite collection of documents.
The following inclusion relation applies among these classes of language:
distributional ==&gt; fuzzy ==&gt; positive ==&gt; real-valued (27)
Because A* is a countable set, the set RA∗ of functions from A* to R is isomorphic
to the sequence space, and we shall treat them equivalently. We denote by fp(A*) the
set of functions with a finite F norm when considered as sequences. There is another
hierarchy of spaces given by the inclusion of the F spaces: fp(A*) C Qq(A*) if p &lt; q. In
particular,
</bodyText>
<equation confidence="0.936491">
~1(A*) C f2(A*) C f°°(A*) C RA∗ (28)
</equation>
<page confidence="0.948558">
54
</page>
<note confidence="0.355392">
Clarke A Context-Theoretic Framework for Distributional Semantics
</note>
<bodyText confidence="0.9993467">
where the f&apos; norm gives the maximum value of the function and f&apos;(A*) is the space of
all bounded real-valued functions on A*. Recall that a linear operator T from one vector
space U to another V is called bounded if there exists some α &gt; 0 such that 11Tu11 ≤ α~u11
for all u ∈ U, where the norm on the left hand side is the norm in V, and that on the right
hand side is in U.
Note that probability distributions are in f1(A*) and fuzzy languages are in f&apos;(A*).
If L ∈ f1(A*)+ (the space of positive functions on A* such that the sum of all values
of the function is finite) then we can define a probability distribution pL over A* by
pL(x) = L(x)/IILJJ1. Similarly, if L ∈ f&apos;(A*)+ (the space of bounded positive functions
on A*) then we can define a fuzzy language fL by fL(x) = L(x)/~L~&apos;.
</bodyText>
<subsectionHeader confidence="0.29413">
Example 11
</subsectionHeader>
<bodyText confidence="0.917109625">
Given a finite set of strings C ⊂ A*, which we may imagine to be a corpus of documents,
define L(x) = 1/|C |if x ∈ C, or 0 otherwise. Then L is a probability distribution over A*.
In general, we think of a real-valued language as an abstraction of a corpus; in par-
ticular, we think of a corpus as a finite sample of a distributional language representing
all possible documents that could ever be written.
Example 12
Let L be a language such that L(x) = 0 for all but a finite subset of A*. Then L ∈ fp(A*)
for all p.
</bodyText>
<subsectionHeader confidence="0.404714">
Example 13
</subsectionHeader>
<bodyText confidence="0.991223333333333">
Let L be the language defined by L(x) = |x |where x is the length of (i.e., number of
symbols in) string x. Then L is a positive language which is not bounded: For any string
y there exists a z such that L(z) &gt; L(y), for example z = ay for a ∈ A.
</bodyText>
<subsectionHeader confidence="0.317081">
Example 14
</subsectionHeader>
<bodyText confidence="0.955421333333333">
Let L be the language defined by L(x) = 1/2 for all x. Then L is a fuzzy language but
L ∈/ f1(A*)
We will assume now that L is fixed, and consider the properties of contexts of strings
with respect to this language. As in a syntactic monoid, we consider the context to be
everything surrounding the string, although in this case instead of a set of pairs of
strings we have a function from pairs of strings to the real numbers. We emphasize
the vector nature of these real-valued functions by calling them “context vectors.” Our
thesis is centered around these vectors, and it is their properties that form the inspiration
for the context-theoretic framework.
</bodyText>
<equation confidence="0.854795">
Definition 9 (Context Vectors)
Let L be a language on A. For x ∈ A*, we define the context of x as a vector xˆ ∈ RA∗xA∗:
ˆx(y,z) = L(yxz) (29)
</equation>
<bodyText confidence="0.999624571428572">
In other words, xˆ is a function from pairs of strings to the real numbers, and the value
of ˆx(y,z) is the value of x in the context (y, z), which is L(yxz).
The question we are addressing is: Does there exist some algebra A containing
the context vectors of strings in A* such that xˆ · yˆ = zy where x, y ∈ A* and · indicates
multiplication in the algebra? As a first try, consider the vector space L&apos;(A* × A*) in
which the context vectors live. Is it possible to define multiplication on the whole vector
space such that the condition just specified holds?
</bodyText>
<page confidence="0.980449">
55
</page>
<figure confidence="0.5157335">
Computational Linguistics Volume 38, Number 1
Example 15
</figure>
<bodyText confidence="0.97672">
Consider the language C on the alphabet A = {a, b, c, d, e,f } defined by C(abcd) =
C(aecd) = C(abfd) = 13 and C(x) = 0 for all other x ∈ A*. Now if we take the shorthand
notation of writing the basis vector in L&apos;(A* × A*) corresponding to a pair of strings
as the pair of strings itself then
</bodyText>
<equation confidence="0.999159714285714">
bˆ = 13 (a, cd) + 13 (a, fd) (30)
cˆ = 13(ab,d)+ 13(ae,d) (31)
be = 13 (a, d) (32)
It would thus seem sensible to define multiplication of contexts so that 13 (a, cd) ·
3 (ab, d) = 1
1 3 (a, d). However we then find
eˆ · fˆ = 13 (a, cd) · 13 (ab, d) =� �ef = 0 (33)
</equation>
<bodyText confidence="0.995176714285714">
showing that this definition of multiplication doesn’t provide us with what we are
looking for. In fact, if there did exist a way to define multiplication on contexts in
a satisfactory manner it would necessarily be far from intuitive, as, in this example,
we would have to define (a, cd) · (ab, d) = 0 meaning the product bˆ · cˆ would have to
have a non-zero component derived from the products of context vectors (a,fd) and
(ae, d) which don’t relate at all to the contexts of bc. This leads us to instead define
multiplication on a subspace of L&apos;(A* × A*).
</bodyText>
<table confidence="0.269593">
Definition 10 (Generated Subspace A)
The subspace A of L&apos;(A* × A*) is the set defined by
</table>
<equation confidence="0.8633895">
EA = {a : a = αxxˆ for some αx ∈ R} (34)
xcA∗
</equation>
<bodyText confidence="0.998195777777778">
In other words, it is the space of all vectors formed from linear combinations of context
vectors.
Because of the way we define the subspace, there will always exist some basis B =
{uˆ : u ∈ B} where B ⊆ A*, and we can define multiplication on this basis by uˆ · vˆ = uv
where u, v ∈ B. Defining multiplication on this basis defines it for the whole vector
subspace, because we define multiplication to be linear, making A an algebra.
There are potentially many different bases we could choose, however, each corre-
sponding to a different subset of A*, and each giving rise to a different definition of
multiplication. Remarkably, this isn’t a problem.
</bodyText>
<subsectionHeader confidence="0.394676">
Proposition 1(Context Algebra)
</subsectionHeader>
<bodyText confidence="0.7343276">
Multiplication on A is the same irrespective of the choice of basis B.
Proof
We say B ⊆ A* defines a basis B for A when B is a basis such that B = {ˆx : x ∈ B}.
Assume there are two sets B1, B2 ⊆ A* that define corresponding bases B1 and B2 for
A. We will show that multiplication in basis B1 is the same as in the basis B2.
</bodyText>
<page confidence="0.961231">
56
</page>
<note confidence="0.245724">
Clarke A Context-Theoretic Framework for Distributional Semantics
</note>
<bodyText confidence="0.623991">
We represent two basis elements ˆu1 and ˆu2 of 131 in terms of basis elements of 132:
</bodyText>
<equation confidence="0.7645415">
�ˆu1 = �αiˆvi and ˆu2 = βjˆvj (35)
i j
</equation>
<bodyText confidence="0.988972">
for some ui E B1, vj E B2 and αi, βj E R. First consider multiplication in the basis 131.
Note that ˆu1 = Ei αiˆvi means that L(xu1y) = Ei αiL(xviy) for all x, y E A*. This includes
the special case where y = u2y&apos; so
</bodyText>
<equation confidence="0.993252">
�L(xu1u2y&apos;) = αiL(xviu2y&apos;) (36)
i
</equation>
<bodyText confidence="0.994644">
for all x,y&apos; E A*. Similarly, we have L(xu2y) = Ej βjL(xvjy) for all x,y E A* which in-
cludes the special case x = x&apos;vi, so L(x&apos;viu2y) = Ej βjL(x&apos;vivjy) for all x&apos;,y E A*. Insert-
ing this into Equation (36) yields
</bodyText>
<equation confidence="0.99036075">
�L(xu1u2y) = αiβjL(xvivjy) (37)
i,j
for all x, y E A* which we can rewrite as
ˆu1 · ˆu2 = ��u1u2 = �αiβj(ˆvi · ˆvj) = αiβj�vivj (38)
i,j i,j
Conversely, the product of u1 and u2 using the basis 132 is
�ˆu1 · ˆu2 = �αi ˆvi · �βj ˆvj = αiβj(ˆvi · ˆvj) (39)
i j i,j
</equation>
<bodyText confidence="0.770913166666667">
thus showing that multiplication is defined independently of what we choose as the
basis. ■
Example 16
Returning to the previous example, we can see that in this case multiplication is in fact
defined on L&apos;(A* x A*) because we can describe each basis vector in terms of context
vectors:
</bodyText>
<equation confidence="0.99933025">
(a,fd) · (ae, d) = 3(ˆb − ˆe) · 3(ˆc − fˆ) = −3(a, d) (40)
(a, cd) · (ae, d) = 3ˆe · 3(ˆc − fˆ) = 3(a, d) (41)
(a,fd) · (ab, d) = 3(ˆb − ˆe) · 3fˆ = 3(a, d) (42)
(a, cd) · (ab, d) = 3ˆe · 3fˆ = 0, (43)
</equation>
<bodyText confidence="0.854062125">
thus confirming what we predicted about the product of bˆ and ˆc: The value is only
correct because of the negative correction from (a,fd) · (ae, d). This example also serves
to demonstrate an important property of context algebras: They do not satisfy the
positivity condition; it is possible for positive vectors (those with all components
greater than or equal to zero) to have a non-positive product. This means they are not
necessarily partially ordered algebras under the normal partial order. Compare this
to the case of matrix multiplication, for example, where the product of two positive
matrices is always positive.
</bodyText>
<page confidence="0.979413">
57
</page>
<note confidence="0.478632">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.999913333333333">
The notion of a context theory is founded on the prototypical example given by
context vectors. So far we have shown that multiplication can be defined on the vector
space A generated by context vectors of strings; we have not discussed the lattice
properties of the vector space, however. In fact, A does not come with a natural lattice
ordering that makes sense for our purposes, although the original space RA∗ xA∗ does—
it is isomorphic to the sequence space. Thus (A, A, ξ,RA∗ xA , ψ) will form our context
theory, where ξ(a) = aˆ for a E A and ψ is the canonical map that simply maps elements
of A to themselves, but are considered as elements of RA∗xA∗. There is an important
caveat here, however: We required that the vector lattice be an abstract Lebesgue space,
which means we need to be able to define a norm on it. The f1 norm on RA∗xA∗ is an
obvious candidate, although it is not guaranteed to be finite. This is where the nature of
the underlying language L becomes important.
We might hope that the most restrictive class of the languages we discussed, the
distributional languages, would guarantee that the norm is finite. Unfortunately, this is
not the case, as the following example demonstrates.
</bodyText>
<equation confidence="0.974102">
Example 17
Let L be the language defined by
L(a2n) = 1/2n+1 (44)
</equation>
<bodyText confidence="0.998977375">
for integer n &gt; 0, and zero otherwise, where by an we mean n repetitions of a, so
for example, L(a) = 12, L(aa) = 14, L(aaa) = 0, and L(aaaa) = 18. Then L is a probability
distribution over A*, because L is positive and 11L111 = 1. However, 11ˆa111 is infinite,
because each string x for which L(x) &gt; 0 contributes 1/2 to the value of the norm, and
there are an infinite number of such strings.
The problem in the previous example is that the average string length is infinite. If
we restrict ourselves to distributional languages in which the average string length is
finite, then the problem goes away.
</bodyText>
<equation confidence="0.692722363636364">
Proposition 2
Let L be a probability distribution over A* such that
L¯ = � L(x)|x |(45)
xcA∗
is finite, where |x |is the number of symbols in string x; we will call such languages finite
average length. Then 11ˆy111 is finite for each y E A*.
Proof
Denote the number of occurrences of string y as a substring of string x by |x|y. Clearly
|x|y &lt; |x |for all x,y E A*. Moreover,
11 ˆy111 = � L(x)|x|y &lt; � L(x)|x |(46)
xcA∗ xcA∗
</equation>
<bodyText confidence="0.952506">
and so 11ˆy111 &lt; L¯ is finite for all y E A*. ■
If L is finite average length, then A C 81(A* x A*), and so (A, A, ξ, f1(A* x A*), ψ)
is a context theory, where ψ is the canonical map from A to f1(A* x A*). Thus context
</bodyText>
<page confidence="0.993371">
58
</page>
<note confidence="0.352335">
Clarke A Context-Theoretic Framework for Distributional Semantics
</note>
<bodyText confidence="0.995211">
algebras of finite average length languages provide our prototypical examples of
context theories.
</bodyText>
<subsectionHeader confidence="0.974042">
3.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999971">
The benefit of the context-theoretic framework is in providing a space of exploration for
models of meaning in language. Our effort has been in finding principles by which to
define the boundaries of this space. Each of the key boundaries, namely, bilinearity and
associativity of multiplication and entailment through vector lattice structure, can also
be viewed as limitations of the model.
Bilinearity is a strong requirement to place, and has wide-ranging implications for
the way meaning is represented in the model. It can be interpreted loosely as follows:
Components of meaning persist or diminish but do not spontaneously appear. This is
particularly counterintuitive in the case of idiom and metaphor in language. It means
that, for example, both red and herring must contain some components relating to the
meaning of red herring which only come into play when these two words are combined
in this particular order. Any other combination would give a zero product for these
components. It is easy to see how this requirement arises from a context-theoretic
perspective, nevertheless from a linguistic perspective it is arguably undesirable.
One potential limitation of the model is that it does not explicitly model syntax, but
rather syntactic restrictions are encoded into the vector space and product itself. For
example, we may assume the word square has some component of meaning in common
with the word shape. Then we would expect this component to be preserved in the
sentences He drew a square and He drew a shape. However, in the case of the two sentences
The box is square and *The box is shape we would expect the second to be represented by
the zero vector because it is not grammatical; square can be a noun and an adjective,
whereas shape cannot. Distributivity of meaning means that the component of meaning
that square has in common with shape must be disjoint with the adjectival component of
the meaning of square.
Associativity is also a very strong requirement to place; indeed Lambek (1961)
introduced non-associativity into his calculus precisely to deal with examples that were
not satisfactorily dealt with by his associative model (Lambek 1958).
Our framework provides answers to someone considering the use of algebra for
natural language semantics. What field should be used? The real numbers. Need the
algebra be finite-dimensional? No. Should the algebra by unital? Yes. Some of these
answers impose restrictions on what is possible within the framework. The full impli-
cation of these restrictions for linguistics is beyond the scope of this article, and indeed
is not yet known.
Although we hope that these features or boundaries are useful in their current form,
it may be that with time, or for certain applications, there is a reason to expand or
contract certain of them, perhaps because of theoretical discoveries relating to the model
of meaning as context, or for practical or linguistic reasons, if, for example, the model is
found to be too restrictive to model certain linguistic phenomena.
</bodyText>
<sectionHeader confidence="0.942611" genericHeader="method">
4. Applications to Textual Entailment
</sectionHeader>
<bodyText confidence="0.99994275">
In this section we analyze approaches to the problem of recognizing textual entailment,
showing how they can be related to the context-theoretic framework, and discussing
potential new approaches that are suggested by looking at them within the framework.
We first discuss some simple approaches to textual entailment based on subsequence
</bodyText>
<page confidence="0.994815">
59
</page>
<note confidence="0.489663">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.9994009">
matching and measuring lexical overlap. We then look at the approach of Glickman
and Dagan (2005), showing that it can be considered as a context theory in which words
are represented as projections on the vector space of documents. This leads us to an
implementation of our own in which we used latent Dirichlet allocation as an alternative
approach to overcoming the problem of data sparseness.
A fair amount of effort is required to describe these approaches within our frame-
work. Although there is no immediate practical benefit to be gained from this, our
main purpose in doing this is to demonstrate the generality of the framework. We also
hope that insight into these approaches may be gleaned by viewing them from a new
perspective.
</bodyText>
<subsectionHeader confidence="0.999366">
4.1 Subsequence Matching and Lexical Overlap
</subsectionHeader>
<bodyText confidence="0.995366857142857">
A sequence x E A* is a subsequence of y E A* if each element of x occurs in y in the
same order, but with the possibility of other elements occurring in between, so for
example abba is a subsequence of acabcba in {a, b, c}*. Subsequence matching compares
the subsequences of two sequences: The more subsequences they have in common
the more similar they are assumed to be. This idea has been used successfully in text
classification (Lodhi et al. 2002) and also formed the basis of the author’s entry to the
second Recognising Textual Entailment Challenge (Clarke 2006).
</bodyText>
<equation confidence="0.88766925">
If S is a semigroup, f1(S) is a lattice-ordered algebra under the multiplication of
convolution:
(f � g)(x) = 1: f (y)g(z) (47)
yz=x
</equation>
<bodyText confidence="0.976958">
where x,y,z E S, f,g E $1(S).
</bodyText>
<subsectionHeader confidence="0.539966">
Example 18 (Subsequence Matching)
</subsectionHeader>
<bodyText confidence="0.999777625">
Consider the algebra f1(A*) for some alphabet A. This has a basis consisting of elements
ex for x E A*, where ex the function that is 1 on x and 0 elsewhere. In particular eE is
a unity for the algebra. Define ξ(a) = 12 (ea + e,); then (A, V(A*), ξ) is a context theory.
Under this context theory, a sequence x completely entails y if and only if it is a sub-
sequence of y. In our experiments, we have shown that this type of context theory can
perform significantly better than straightforward lexical overlap (Clarke 2006). Many
variations on this idea are possible: for example, using more complex mappings from
A* to V(A*).
</bodyText>
<subsectionHeader confidence="0.546884">
Example 19 (Lexical Overlap)
</subsectionHeader>
<bodyText confidence="0.999880777777778">
The simplest approach to textual entailment is to measure the degree of lexical over-
lap: the proportion of words in the hypothesis sentence that are contained in the text
sentence (Dagan, Glickman, and Magnini 2005). This approach can be described as
a context theory in terms of a free commutative semigroup on a set A, defined by
A*/ - where x - y in A* if the symbols making up x can be reordered to make y. Then
define ξ&apos; by ξ&apos;(a) = 12 (e[a] + e[E]) where [a] is the equivalence class of a in A*/ -. Then
(A, f1(A*/ =4 ξ&apos;) is a context theory in which entailment is defined by lexical overlap.
More complex definitions of xˆ can be used, for example, to weight different words by
their probabilities.
</bodyText>
<page confidence="0.980717">
60
</page>
<figure confidence="0.371537">
Clarke A Context-Theoretic Framework for Distributional Semantics
</figure>
<subsectionHeader confidence="0.965954">
4.2 Document Projections
</subsectionHeader>
<bodyText confidence="0.9996455">
Glickman and Dagan (2005) give a probabilistic definition of entailment in terms of
“possible worlds” which they use to justify their lexical entailment model based on oc-
currences of words in Web documents. They estimate the lexical entailment probability
LEP(u, v) to be
</bodyText>
<equation confidence="0.7655185">
LEP(u,v) ^&apos; nu&apos;
vv (48)
</equation>
<bodyText confidence="0.999077">
where nv and nu,v denote the number of documents in which the word v occurs and in
which the words u and v both occur, respectively. From the context-theoretic perspec-
tive, we view the set of documents in which the word occurs as its context vector. To
describe this situation in terms of a context theory, consider the vector space f∞(D)
where D is the set of documents. With each word u in some set A we associate an
operator Pu on this vector space by
</bodyText>
<equation confidence="0.930278333333333">
� ed if u occurs in document d
Pued = (49)
0 otherwise.
</equation>
<bodyText confidence="0.957945333333333">
where ed is the basis element associated with document d E D. Pu is a projection, that is,
PuPu = Pu; it projects onto the space of documents that u occurs in. These projections are
clearly commutative (they are in fact band projections): PuPv = PvPu = Pu n Pv projects
onto the space of documents in which both u and v occur.
In their paper, Glickman and Dagan (2005) assume that probabilities can be attached
to individual words, as we do, although they interpret these as the probability that a
word is “true” in a possible world. In their interpretation, a document corresponds to a
possible world, and a word is true in that world if it occurs in the document.
They do not, however, determine these probabilities directly; instead they make
assumptions about how the entailment probability of a sentence depends on lexical
entailment probability. Although they do not state this, the reason for this is presumably
data sparseness: They assume that a sentence is true if all its lexical components are true;
this will only happen if all the words occur in the same document. For any sizeable
sentence this is extremely unlikely, hence their alternative approach.
It is nevertheless useful to consider this idea from a context-theoretic perspective.
We define a context theory (A, B(f∞(D)), &amp;, f∞(D), ¯p), where:
• We denote by B(U) the set of bounded operators on the vector space U;
in this case we are considering the bounded operators on the vector space
indexed by the set of documents D. Because D is finite, all operators on
this space are in fact bounded; this property will be needed when we
generalize D to an infinite set, however.
</bodyText>
<listItem confidence="0.9964064">
• &amp; : A -+ B(f∞(D)) is defined by &amp;(u) = Pu; it maps words to document
projections.
• p¯: B(f∞(D)) -+ f∞(D) is a map defined by ¯p(T) = Tp, where
• p E f∞(D) is defined by p(d) = 1/|D |for all d E D. This is defined such
that I 1PupI 11 is the probability of the term u.
</listItem>
<page confidence="0.995459">
61
</page>
<note confidence="0.473757">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.87592625">
The degree to which x entails y is then given by I1Pxp n PypI11/I1PxI11 =
I1PxPypI11/I1PxI11. This corresponds directly to Glickman and Dagan’s (2005) entailment
“confidence”; it is simply the proportion of documents that contain all the terms of x
which also contain all the terms of y.
</bodyText>
<subsectionHeader confidence="0.99869">
4.3 Latent Dirichlet Projections
</subsectionHeader>
<bodyText confidence="0.9999555">
The formulation in the previous section suggests an alternative approach to that of
Glickman and Dagan (2005) to cope with the data sparseness problem. We consider the
finite data available D as a sample from a distributional language D&apos;; the vector p then
becomes a probability distribution over the documents in D&apos;. In our own experiments,
we used latent Dirichlet allocation (Blei, Ng, and Jordan 2003) to build a model of the
corpus as a probabilistic language based on a subset of around 380,000 documents from
the Gigaword corpus. Having this model allows us to consider an infinite array of
possible documents, and thus we can use our context-theoretic definition of entailment
because there is no problem of data sparseness.
Latent Dirichlet allocation (LDA) follows the same vein as latent semantic analy-
sis (LSA; Deerwester et al. 1990) and probabilistic latent semantic analysis (PLSA;
Hofmann 1999) in that it can be used to build models of corpora in which words within
a document are considered to be exchangeable, so that a document is treated as a bag
of words. LSA performs a singular value decomposition on the matrix of words and
documents which brings out hidden “latent” similarities in meaning between words,
even though they may not occur together.
In contrast, PLSA and LDA provide probabilistic models of corpora using Bayesian
methods. LDA differs from PLSA in that, whereas the latter assumes a fixed number
of documents, LDA assumes that the data at hand are a sample from an infinite set of
documents, allowing new documents to be assigned probabilities in a straightforward
manner.
Figure 3 shows a graphical representation of the latent Dirichlet allocation genera-
tive model, and Figure 4 shows how the model generates a document of length N. In
this model, the probability of occurrence of a word w in a document is considered to be
a multinomial variable conditioned on a k-dimensional “topic” variable z. The number
of topics k is generally chosen to be much fewer than the number of possible words,
so that topics provide a “bottleneck” through which the latent similarity in meaning
between words becomes exposed.
The topic variable is assumed to follow a multinomial distribution parameterized
by a k-dimensional variable 0, satisfying
</bodyText>
<equation confidence="0.996528666666667">
k
0i = 1 (50)
i=1
</equation>
<bodyText confidence="0.996249666666667">
and which is in turn assumed to follow a Dirichlet distribution. The Dirichlet distribu-
tion is itself parameterized by a k-dimensional vector o . The components of this vector
can be viewed as determining the marginal probabilities of topics, because
</bodyText>
<equation confidence="0.996427">
�p(zi) = p(zi|0)p(0)d0 (51)
�= 0ip(0)d0 (52)
</equation>
<page confidence="0.996269">
62
</page>
<figure confidence="0.787899">
Clarke A Context-Theoretic Framework for Distributional Semantics
</figure>
<figureCaption confidence="0.944017">
Figure 3
</figureCaption>
<bodyText confidence="0.989829">
Graphical representation of the Dirichlet model. The inner box shows the choices that are
repeated for each word in the document; the outer box shows the choice that is made for each
document; the parameters outside the boxes are constant for the model.
</bodyText>
<equation confidence="0.889697">
This is just the expected value of 0i, which is given by
p(zi) _ �oci (53)
j ocj
</equation>
<bodyText confidence="0.999210666666667">
The model is thus entirely specified by oc and the conditional probabilities p(w|z)
that we can assume are specified in a k × V matrix R where V is the number of words in
the vocabulary. The parameters oc and R can be estimated from a corpus of documents
by a variational expectation maximization algorithm, as described by Blei, Ng, and
Jordan (2003).
LDA was applied by Blei, Ng, and Jordan (2003) to the tasks of document model-
ing, document classification, and collaborative filtering. They compare LDA to several
techniques including PLSA; LDA outperforms these on all of the applications. LDA has
been applied to the task of word sense disambiguation (Boyd-Graber, Blei, and Zhu
2007; Cai, Lee, and Teh 2007) with significant success.
Consider the vector space f∞(A∗) for some alphabet A, the space of all bounded
functions on possible documents. In this approach, we define the representation of a
</bodyText>
<figureCaption confidence="0.635419">
Figure 4
</figureCaption>
<bodyText confidence="0.724274">
Generative process assumed in the Dirichlet model.
</bodyText>
<page confidence="0.996836">
63
</page>
<note confidence="0.593803">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.998069">
string x to be a projection Px on the subspace representing the (infinite) set of documents
in which all the words in string x occur. We define a vector q(x) for x E A* where q(x) is
the probability of string x in the probabilistic language.
Our context theory is then given by (A, B($°°(A*)), &amp;, $°°(A*), ¯q), where &amp; is defined
as before and q¯ is defined as p¯ earlier. In this case, we are considering an infinite set
of possible documents, A*, so the boundedness property becomes important. IIPxqII1
is thus the probability that a document chosen at random contains all the words that
occur in string x. In order to estimate this value we have to integrate over the Dirichlet
parameter 0:
</bodyText>
<equation confidence="0.953612">
IIPxqII1 =f 1 jpe(a) p(0)d0 (54)
6 (aEx
</equation>
<bodyText confidence="0.999914">
where by a E x we mean that the word a occurs in string x, and pe(a) is the probability
of observing word a in a document generated by the parameter 0. We estimate this by
</bodyText>
<equation confidence="0.988664">
( N
E
pe(a) � 1 − 1 − p(a|z)p(z|0)
z
</equation>
<bodyText confidence="0.9159808">
We built a latent Dirichlet allocation model using Blei, Ng, and
(2003)
implementation on documents from the British National Corpus, using 100 topics. We
evaluated this model on the 800 entailment pairs from the first Recognizing Textual
Entailment Challenge test
Results were comparable to those obtained by Glickman
and Dagan (2005) (see Table 2). In this table, Accuracy is the accuracy on the test set,
consisting of 800 entailment pairs, and CWS is the confidence weighted score; see
Dagan, Glickman, and Magnini (2005) for the definition. The differences between the
accuracy values in the table are not statistically significant because of the small data
set, although all accuracies in the table are significantly better than chance at the 1%
level. The accuracy of the model is considerably lower than the state of the art, which
is around 75% (Bar-Haim et al. 2006). We experimented with various document lengths
and found very long documents (N = 106 and N = 107) to work best.
It is important to note that because the LDA model is commutative, the resulting
context algebra must also be commutative, which is clearly far from ideal in modeli
Jordan’s
set.1
ng
natural language.
</bodyText>
<sectionHeader confidence="0.971984" genericHeader="method">
5. The Model of Clark, Coecke, and Sadrzadeh
</sectionHeader>
<bodyText confidence="0.927694375">
where we have assumed a fixed document length N. This formula is an estimate of the
probability of a word occurring at least once in a document of length N, and the sum
over the topic variable z is the probability that the word a occurs at any one point in
a document given the parameter 0. We approximated the integral using Monte Carlo
sampling to generate values of 0 according to the Dirichlet distribution.
One of the most sophisticated proposals for a method of composition is that of Clark,
Coecke, and Sadrzadeh (2008) and the more recent implementation of Grefenstett
e et al.
</bodyText>
<footnote confidence="0.971904333333333">
1 We have so far only used data from the first challenge, because we performed the experiment before the
enges had taken place.
other chall
</footnote>
<equation confidence="0.323144">
(55)
</equation>
<page confidence="0.949409">
64
</page>
<table confidence="0.439715">
Clarke A Context-Theoretic Framework for Distributional Semantics
</table>
<tableCaption confidence="0.990339">
Table 2
</tableCaption>
<table confidence="0.9996445">
Results obtained with our latent Dirichlet projection model on the data from the first
Recognizing Textual Entailment Challenge for two document lengths N = 106 and N = 107
using a cut-off for the degree of entailment of 0.5 at which entailment was regarded as holding.
Model Accuracy CWS
Dirichlet (106) 0.584 0.630
Dirichlet (107) 0.576 0.642
Bayer (MITRE) 0.586 0.617
Glickman (Bar Ilan) 0.586 0.572
Jijkoun (Amsterdam) 0.552 0.559
Newman (Dublin) 0.565 0.6
</table>
<bodyText confidence="0.837283666666667">
(2011). In this section, we will show how their model can be described as a context
theory.
The authors describe the syntactic element of their construction using pregroups
(Lambek 2001), a formalism which simplifies the syntactic calculus of Lambek (1958).
These can be described in terms of partially ordered monoids, a monoid G with a partial
ordering &lt; satisfying x &lt; y implies xz &lt; yz and zx &lt; zy for all x, y, z E G.
</bodyText>
<subsectionHeader confidence="0.759417">
Definition 11 (Pregroup)
</subsectionHeader>
<bodyText confidence="0.927755">
Let G be a partially ordered monoid. Then G is called a pregroup if for each x E G there
are elements xl and xr in G such that
</bodyText>
<table confidence="0.9169505">
xlx &lt; 1
xxr &lt; 1
1 &lt; xxl
1 &lt; xrx
</table>
<bodyText confidence="0.8669088">
If x, y E G, we call y a reduction of x if y can be obtained from x using only Rules (56)
and (57).
Pregroup grammars are defined by freely generating a pregroup on a set of basic
grammatical types. Words are then represented as elements formed from these basic
types, for example:
</bodyText>
<subsectionHeader confidence="0.5090925">
John likes Mary
π πrsol o
</subsectionHeader>
<bodyText confidence="0.999712333333333">
where π, s, and o are the basic types for first person singular, statement, and object,
respectively. It is easy to see that this sentence reduces to type s under the pregroup
reductions.
As Clark, Coecke, and Sadrzadeh (2008) note, their construction can be generalized
by endowing the grammatical type of a word with a vector nature, in addition to its
semantics. We use this slightly more general construction to allow us to formulate it
in the context-theoretic framework. We define an elementary meaning space to be the
tensor product space V = S ® P where S is a vector space representing meanings of
words and P is a vector space with an orthonormal basis corresponding to the basic
</bodyText>
<page confidence="0.998484">
65
</page>
<note confidence="0.594219">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.9984075">
grammatical types in a pregroup grammar and their adjoints. We assume that meanings
of words live in the tensor algebra space T(V), defined by
</bodyText>
<equation confidence="0.968832">
T(V) = R ⊕ V ⊕ (V ⊗ V) ⊕ (V ⊗ V ⊗ V) ⊕ ···
</equation>
<bodyText confidence="0.991283428571429">
For an element v in a particular tensor power of V, such that v = (s1 ⊗ p1) ⊗ (s2 ⊗ p2) ⊗
· · · ⊗ (sn ⊗ pn), where the pi are basis vectors of P, then we can recover a complex gram-
matical type for v as the product γ(v) = γ1γ2 · · · γn, where γi is the basic grammatical
type corresponding to pi. We will call the vectors such as this which have a single
complex type (i.e., they are not formed from a weighted sum of more than one type)
unambiguous.
We also assume that words are represented by vectors whose grammatical type is
irreducible: There is no pregroup reduction possible on the type. We define Γ(T(V)) as
the vector space generated by all such vectors.
We will now define a product · on Γ(T(V)) that will make it an algebra. To do this,
it suffices to define the product between two elements u1, u2 which are unambiguous
and whose grammatical type is basic, so that they can be viewed as elements of V.
The definition of the product on the rest of the space follows from the assumption of
distributivity. We define
</bodyText>
<equation confidence="0.9979055">
u1 · u2 = { u1 ⊗ u2 if γ(u1 ⊗ u2) is irreducible (61)
(u1, u2) otherwise.
</equation>
<bodyText confidence="0.9997054">
This product is bilinear, because for a particular pair of basis elements, only one of these
two conditions will apply, and both the tensor and inner products are bilinear functions.
Moreover, it corresponds to composed and reduced word vectors, as defined in Clark,
Coecke, and Sadrzadeh (2008).
To see how this works on our example sentence, we assume we have vectors for
the meanings of the three words, which we write as vword. We assume for the purpose
of this example that the word like is represented as a product state composed of three
vectors, one for each basic grammatical type. This removes any potentially interesting
semantics, but allows us to demonstrate the product in a simple manner. We write this
as follows:
</bodyText>
<equation confidence="0.6679655">
John likes Mary (62)
(vJohn ⊗ est) · (vlikes,1 ⊗ estr) · (vlikes,2 ⊗ es) · (vlikes,3 ⊗ eol) · (vMary ⊗ eo )
</equation>
<bodyText confidence="0.999631666666667">
where ey is the orthonormal basis vector corresponding to basic grammatical type γ.
More interesting representations of like would consist of sums over similar vectors.
Computing this product from left to right:
</bodyText>
<equation confidence="0.5989208">
(vJohn ⊗ est) · (vlikes,1 ⊗ estr)· (vlikes,2 ⊗ es) · (vlikes,3 ⊗ eol) · (vMary ⊗ eo)
= (vJohn, vlikes,1) (vlikes,2 ⊗ es) · (vlikes,3 ⊗ eol) · (vMary ⊗ eo)
= (vJohn, vlikes,1) (vlikes,2 ⊗ es) ⊗ (vlikes,3 ⊗ eol) · (vMary ⊗ eo)
= (vJohn, vlikes,1)(vlikes,3, vMary) (vlikes,2 ⊗ es)
(63)
</equation>
<page confidence="0.74401">
66
</page>
<note confidence="0.332552">
Clarke A Context-Theoretic Framework for Distributional Semantics
</note>
<bodyText confidence="0.999703166666667">
As we would expect in this simplified example the product is a scalar multiple of the
second vector for like, with the type of a statement.
This construction thus allows us to represent complex grammatical types, similar
to Clark, Coecke, and Sadrzadeh (2008), although it also allows us to take weighted
sums of these complex types, giving us a powerful method of expressing syntactic and
semantic ambiguity.
</bodyText>
<sectionHeader confidence="0.99586" genericHeader="conclusions">
6. Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999933166666667">
We have presented a context-theoretic framework for natural language semantics. The
framework is founded on the idea that meaning in natural language can be determined
by context, and is inspired by techniques that make use of statistical properties of
language by analyzing large text corpora. Such techniques can generally be viewed as
representing language in terms of vectors. These techniques are currently used in appli-
cations such as textual entailment recognition, although the lack of a theory of meaning
that incorporates these techniques means that they are often used in a somewhat ad hoc
manner. The purpose behind the framework is to provide a unified theoretical founda-
tion for such techniques so that they may be used in a principled manner.
By formalizing the notion of “meaning as context” we have been able to build a
mathematical model that informs us about the nature of meaning under this paradigm.
Specifically, it gives us a theory about how to represent words and phrases using
vectors, and tells us that the product of two meanings should be distributive and
associative. It also gives us an interpretation of the inherent lattice structure on these
vector spaces as defining the relation of entailment. It tells us how to measure the size
of the vector representation of a string in such a way that the size corresponds to the
probability of the string.
We have demonstrated that the framework encompasses several related approaches
to compositional distributional semantics, including those based on a predefined
composition operation such as addition (Landauer and Dumais 1997; Foltz, Kintsch,
and Landauer 1998; Mitchell and Lapata 2008) or the tensor product (Smolensky
1990; Clark and Pulman 2007; Widdows 2008), matrix multiplication (Rudolph
and Giesbrecht 2010), and the more sophisticated construction of Clark, Coecke, and
Sadrzadeh (2008).
</bodyText>
<subsectionHeader confidence="0.990359">
6.1 Practical Investigations
</subsectionHeader>
<bodyText confidence="0.9999875">
Section 4 raises many possibilities for the design of systems to recognize textual entail-
ment within the framework.
</bodyText>
<listItem confidence="0.9994754">
• Variations on substring matching: experiments with different weighting
schemes for substrings, allowing partial commutativity of words or
phrases, and replacing words with vectors representing their context,
using tensor products of these vectors instead of concatenation.
• Extensions of Glickman and Dagan’s approach and our own
</listItem>
<bodyText confidence="0.99735875">
context-theoretic approach using LDA, perhaps using other distributional
languages based on n-grams or other models in which words do not
commute, or a combination of context theories based on commutative
and non-commutative models.
</bodyText>
<page confidence="0.996125">
67
</page>
<note confidence="0.529489">
Computational Linguistics Volume 38, Number 1
</note>
<bodyText confidence="0.87597475">
• The LDA model we used is a commutative one. This is a considerable
simplification of what is possible within the context-theoretic framework;
it would be interesting to investigate methods of incorporating
non-commutativity into the model.
</bodyText>
<listItem confidence="0.5808355">
• Implementations based on the approach to representing uncertainty in
logical semantics similar to those described in Clarke (2007).
</listItem>
<bodyText confidence="0.998735571428572">
All of these ideas could be evaluated using the data sets from the Recognising Textual
Entailment Challenges.
There are many approaches to textual entailment that we have not considered here;
we conjecture that variations of many of them could be described within our frame-
work. We leave the task of investigating the relationship between these approaches and
our framework to further work.
Other areas that we are investigating, together with researchers at the University
of Sussex, is the possibility of learning finite-dimensional algebras directly from corpus
data, along the lines of Guevara (2011) and Baroni and Zamparelli (2010).
One question we have not addressed in this article is the feasibility of computing
with algebraic representations. Although this question is highly dependent on the
particular context theory chosen, it is possible that general algorithms for computation
within this framework could be found; this is another area that we intend to address in
further work.
</bodyText>
<subsectionHeader confidence="0.993975">
6.2 Theoretical Investigations
</subsectionHeader>
<bodyText confidence="0.99999352631579">
Although the context-theoretic framework is an abstraction of the model of meaning as
context, it would be good to have a complete understanding of the model and the types
of context theories that it allows. Tying down these properties would allow us to define
algebras that could truly be called “context theories.”
The context-theoretic framework shares a lot of properties with the study of free
probability (Voiculescu 1997). It would be interesting to investigate whether ideas from
free probability would carry over to context-theoretic semantics.
Although we have related our model to many techniques described in the literature,
we still have to investigate its relationship with other models such as that of Song and
Bruza (2003) and Guevara (2011).
We have not given much consideration here to the issue of multi-word expres-
sions and non-compositionality. What predictions does the context-theoretic framework
make about non-compositionality? Answering this may lead us to new techniques for
recognizing and handling multi-word expressions and non-compositionality.
Of course it is hard to predict the benefits that may result from what we have
presented, because we have given a way of thinking about meaning in natural language
that in many respects is new. This new way of thinking opens the door to the unification
of logic-based and vector-based methods in computational linguistics, and the potential
fruits of this union are many.
</bodyText>
<sectionHeader confidence="0.998209" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.414946454545455">
The ideas presented here have benefitted
enormously from the input and support of
my DPhil supervisor, David Weir, without
whom this work would not exist; Rudi Lutz;
and Stephen Clark, who really grokked this
and made many excellent suggestions for
improvements. I am also grateful for the
advice and encouragement of Bill Keller,
John Carroll, Peter Williams, Mark
W. Hopkins, Peter Lane, Paul Hender, and
Peter Hines. I am indebted to the anonymous
</bodyText>
<page confidence="0.997191">
68
</page>
<note confidence="0.412754">
Clarke A Context-Theoretic Framework for Distributional Semantics
</note>
<bodyText confidence="0.9588128">
reviewers; their suggestions have
undoubtedly improved this article beyond
measure; the paragraph on the three uses of
the term entailment was derived directly from
one of their suggestions.
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999622663636364">
Abramovich, Yuri A. and Charalambos D.
Aliprantis. 2002. An Invitation to Operator
Theory. American Mathematical Society,
Providence, RI.
Bar-Haim, Roy, Ido Dagan, Bill Dolan, Lisa
Ferro, Danilo Giampiccolo, Bernardo
Magnini, and Idan Szpektor. 2006.
The second pascal recognising textual
entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1–9,
Venice.
Baroni, Marco and Roberto Zamparelli.
2010. Nouns are vectors, adjectives are
matrices: Representing adjective-noun
constructions in semantic space. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP 2010), pages 1183–1193, East
Stroudsburg PA.
Bellegarda, Jerome R. 2000. Exploiting
latent semantic information in statistical
language modeling. Proceedings of the
IEEE, 88(8):1279–1296.
Blackburn, Patrick and Johan Bos.
2005. Representation and Inference for
Natural Language. CSLI Publications,
Stanford, CA.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
3:993–1022.
Boyd-Graber, Jordan, David Blei, and
Xiaojin Zhu. 2007. A topic model for
word sense disambiguation. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024–1033,
Prague.
Cai, Junfu, Wee Sun Lee, and Yee Whye
Teh. 2007. Improving word sense
disambiguation using topic features.
In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL),
pages 1015–1023, Prague.
Choi, Freddy, Peter Wiemer-Hastings,
and Johanna Moore. 2001. Latent
Semantic Analysis for text segmentation.
In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language
Processing, pages 109–117, Ithaca, NY.
Clark, Stephen, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional
distributional model of meaning.
In Proceedings of the Second Quantum
Interaction Symposium (QI-2008),
pages 133–140, Oxford.
Clark, Stephen and Stephen Pulman. 2007.
Combining symbolic and distributional
models of meaning. In Proceedings
of the AAAI Spring Symposium on
Quantum Interaction, pages 52–55,
Stanford, CA.
Clarke, Daoud. 2006. Meaning as context
and subsequence analysis for textual
entailment. In Proceedings of the Second
PASCAL Recognising Textual Entailment
Challenge, pages 134–139, Venice.
Clarke, Daoud. 2007. Context-theoretic
Semantics for Natural Language:
An Algebraic Framework. Ph.D. thesis,
Department of Informatics, University
of Sussex.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting
of the Association for Computational
Linguistics and Eighth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 16–23,
Madrid.
Dagan, Ido, Oren Glickman, and Bernardo
Magnini. 2005. The PASCAL recognising
textual entailment challenge. In Proceedings
of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1–8,
Southampton, UK.
Deerwester, Scott, Susan Dumais,
George Furnas, Thomas Landauer, and
Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society for Information Science,
41(6):391–407.
Erk, Katrin and Sebastian Pad´o. 2009.
Paraphrase assessment in structured
vector space: Exploring parameters and
datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language
Semantics, pages 57–65, Athens.
Firth, John R. 1968. A synopsis of linguistic
theory, 1930–1955. In John R. Firth, editor,
Selected Papers of JR Firth, 1952–59.
Indiana University Press, Bloomington,
pages 168–205.
Foltz, Peter W., Walter Kintsch, and
Thomas K. Landauer. 1998. The
measurement of textual coherence
</reference>
<page confidence="0.961114">
69
</page>
<reference confidence="0.994662067226891">
Computational Linguistics Volume 38, Number 1
with latent semantic analysis. Discourse
Process, 15:285–307.
Geffet, M. and I. Dagan. 2005. The
distributional inclusion hypotheses
and lexical entailment. In Proceedings
of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 107–114,
Ann Arbor, MI.
Glickman, O. and I. Dagan. 2005.
A probabilistic setting and lexical
cooccurrence model for textual
entailment. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 43–48,
Ann Arbor, MI.
Grefenstette, Edward, Mehrnoosh
Sadrzadeh, Stephen Clark,
Bob Coecke, and Stephen Pulman.
2011. Concrete sentence spaces for
compositional distributional models
of meaning. Proceedings of the 9th
International Conference on Computational
Semantics (IWCS 2011), pages 125–134,
Oxford.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Dordrecht,
Netherlands.
Guevara, Emiliano. 2011. Computing
semantic compositionality in distributional
semantics. In Proceedings of the 9th
International Conference on Computational
Semantics (IWCS 2011), pages 135–144,
Oxford.
Halmos, Paul. 1974. Finite Dimensional Vector
Spaces. Springer, Berlin.
Harris, Zellig. 1968. Mathematical Structures
of Language. Wiley, New York.
Hofmann, Thomas. 1999. Probabilistic
latent semantic analysis. In Proceedings of
the 15th Conference on Uncertainty in AI,
pages 289–296, Stockholm.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic: Introduction to
Model-theoretic Semantics of Natural
Language, Formal Logic and Discourse
Representation Theory, volume 42 of
Studies in Linguistics and Philosophy.
Kluwer, Dordrecht.
Kintsch, Walter. 2001. Predication. Cognitive
Science, 25:173–202.
Lambek, Joachim. 1958. The mathematics of
sentence structure. American Mathematical
Monthly, 65:154–169.
Lambek, Joachim. 1961. On the calculus of
syntactic types. In Roman Jakobson, editor,
Structure of Language and Its Mathematical
Aspects, pages 166–178, American
Mathematical Society, Providence, RI.
Lambek, Joachim. 2001. Type grammars as
pregroups. Grammars, 4(1):21–39.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato’s problem:
The latent semantic analysis theory of
acquisition, induction and representation
of knowledge. Psychological Review,
104(2):211–240.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-1999), pages 23–32,
College Park, MD.
Lin, Dekang.1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics
(COLING-ACL ’98), pages 768–774,
Montreal.
Lin, Dekang. 2003. Dependency-based
evaluation of MINIPAR. In Anne Abeill´e,
editor, Treebanks: Building and Using
Parsed Corpora, pages 317–330, Kluwer,
Dordrecht.
Lodhi, Huma, Craig Saunders, John
Shawe-Taylor, Nello Cristianini, and
Chris Watkins. 2002. Text classification
using string kernels. Journal of Machine
Learning Research, 2:419–444.
McCarthy, Diana, Rob Koeling, Julie
Weeds, and John Carroll. 2004. Finding
predominant word senses in untagged
text. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics
(ACL’04), pages 279–286, Barcelona.
Miller, George A. and Walter G. Charles.
1991. Contextual correlates of semantic
similarity. Language and Cognitive
Processes, 6(1):1–28.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL-08:
HLT, pages 236–244, Columbus, OH.
Preller, Anne and Mehrnoosh Sadrzadeh.
2011. Bell states and negative sentences
in the distributed model of meaning.
Electronic Notes in Theoretical Computer
Science, 270(2):141–153.
Rudolph, Sebastian and Eugenie Giesbrecht.
2010. Compositional matrix-space models
of language. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 907–916,
Uppsala.
Sch¨utze, Heinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97–123.
</reference>
<page confidence="0.897675">
70
</page>
<reference confidence="0.983899592592592">
Clarke A Context-Theoretic Framework for Distributional Semantics
Smolensky, Paul. 1990. Tensor product
variable binding and the representation
of symbolic structures in connectionist
systems. Artificial Intelligence,
46(1-2):159–216.
Song, Dawei and Peter D. Bruza. 2003.
Towards context-sensitive information
inference. Journal of the American Society
for Information Science and Technology
(JASIST), 54:321–334.
Voiculescu, Dan-Virgil. 1997. Free Probability
Theory. American Mathematical Society,
Providence, RI.
Weeds, Julie, David Weir, and Diana
McCarthy. 2004. Characterising measures
of lexical distributional similarity.
In Proceedings of CoLING 2004,
pages 1015–1021, Geneva.
Widdows, Dominic. 2008. Semantic vector
products: Some initial investigations.
In Proceedings of the Second Symposium
on Quantum Interaction, pages 1–8,
Oxford.
Wittgenstein, Ludwig. 1953. Philosophical
Investigations. Macmillan, New York. G.
Anscombe, translator.
</reference>
<page confidence="0.99914">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.624055">
<title confidence="0.997203">A Context-Theoretic Framework for</title>
<author confidence="0.796969">Compositionality in Distributional Semantics</author>
<affiliation confidence="0.888075">University of Hertfordshire</affiliation>
<abstract confidence="0.979881666666667">Formalizing “meaning as context” mathematically leads to a new, algebraic theory of meaning, in which composition is bilinear and associative. These properties are shared by other methods that have been proposed in the literature, including the tensor product, vector addition, pointwise multiplication, and matrix multiplication. Entailment can be represented by a vector lattice ordering, inspired by a strengthened form of the distributional hypothesis, and a degree of entailment is defined in the form of a conditional probability. Approaches to the task of recognizing textual entailment, including the use of subsequence matching, lexical entailment probability, and latent Dirichlet allocation, can be described within our framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yuri A Abramovich</author>
<author>Charalambos D Aliprantis</author>
</authors>
<title>An Invitation to Operator Theory.</title>
<date>2002</date>
<publisher>American Mathematical Society,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="36421" citStr="Abramovich and Aliprantis 2002" startWordPosition="6157" endWordPosition="6160">+. If the partial ordering is a lattice, then A is called a lattice-ordered algebra. Example 9 (Lattice-Ordered Algebra of Matrices) The matrices of order n form a lattice-ordered algebra under normal matrix multiplication, where the lattice operations are defined as the entry-wise minimum and maximum. Example 10 (Operators on V Spaces) Matrices can be viewed as operators on finite-dimensional vector spaces; in fact this lattice property extends to operators on certain infinite-dimensional spaces, the &amp; spaces, 53 Computational Linguistics Volume 38, Number 1 by the Riesz-Kantorovich theorem (Abramovich and Aliprantis 2002). The operations are defined by: (S V T)(u) = sup{S(v) + T(w) : v,w E U+ and v + w = u} (25) (S n T)(u) = inf{S(v) + T(w) : v, w E U+ and v + w = u} (26) If A is a lattice-ordered algebra which is also an abstract Lebesgue space, then (A, A, ξ, A, 1) is a context theory. In this simplified situation, A plays the role of the vector lattice as well as the algebra; ξ maps from A to A as before, and 1 indicates the identity map on A. Many of the examples we discuss will be of this form, so we will use the shorthand notation, (A, A, ξ). It is tempting to adopt this as the definition of context theo</context>
</contexts>
<marker>Abramovich, Aliprantis, 2002</marker>
<rawString>Abramovich, Yuri A. and Charalambos D. Aliprantis. 2002. An Invitation to Operator Theory. American Mathematical Society, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<pages>1--9</pages>
<location>Venice.</location>
<contexts>
<context position="64785" citStr="Bar-Haim et al. 2006" startWordPosition="11263" endWordPosition="11266">enge test Results were comparable to those obtained by Glickman and Dagan (2005) (see Table 2). In this table, Accuracy is the accuracy on the test set, consisting of 800 entailment pairs, and CWS is the confidence weighted score; see Dagan, Glickman, and Magnini (2005) for the definition. The differences between the accuracy values in the table are not statistically significant because of the small data set, although all accuracies in the table are significantly better than chance at the 1% level. The accuracy of the model is considerably lower than the state of the art, which is around 75% (Bar-Haim et al. 2006). We experimented with various document lengths and found very long documents (N = 106 and N = 107) to work best. It is important to note that because the LDA model is commutative, the resulting context algebra must also be commutative, which is clearly far from ideal in modeli Jordan’s set.1 ng natural language. 5. The Model of Clark, Coecke, and Sadrzadeh where we have assumed a fixed document length N. This formula is an estimate of the probability of a word occurring at least once in a document of length N, and the sum over the topic variable z is the probability that the word a occurs at </context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Bar-Haim, Roy, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, pages 1–9, Venice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),</booktitle>
<pages>1183--1193</pages>
<location>East Stroudsburg PA.</location>
<contexts>
<context position="4566" citStr="Baroni and Zamparelli 2010" startWordPosition="680" endWordPosition="683">ave similar meanings. This hypothesis is justified by the success of techniques such as latent semantic analysis as well as experimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Kintsch 2001). A solution to this problem would have practical as well as philosophical benefits. Current techniques such as latent semantic analysis work well at the word level, but we cannot extend them much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings of words to determine what their vectors should be merely by lo</context>
<context position="9663" citStr="Baroni and Zamparelli 2010" startWordPosition="1484" endWordPosition="1487">the context-theoretic framework and introduce the mathematics necessary to understand it. The description presented here is cleaner than that of Clarke (2007), and in addition we provide examples that should provide intuition for the concepts we describe. • We relate the framework to methods of composition that have been proposed in the literature, namely: – vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998); – the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008); – the multiplicative models of Mitchell and Lapata (2008); – matrix multiplication (Baroni and Zamparelli 2010; Rudolph and Giesbrecht 2010); and – the approach of Clark, Coecke, and Sadrzadeh (2008). It is important to note that the purpose of describing related work in terms of our framework is not merely to demonstrate the generality of our framework: In doing so, we identify previously ignored features of this work such as the lattice structure within the vector space. This allows any one of these approaches to be endowed with an entailment property defined by this lattice structure, based on a philosophy of meaning as context. Although the examples described here show that existing approaches can</context>
<context position="74848" citStr="Baroni and Zamparelli (2010)" startWordPosition="12974" endWordPosition="12977">ese ideas could be evaluated using the data sets from the Recognising Textual Entailment Challenges. There are many approaches to textual entailment that we have not considered here; we conjecture that variations of many of them could be described within our framework. We leave the task of investigating the relationship between these approaches and our framework to further work. Other areas that we are investigating, together with researchers at the University of Sussex, is the possibility of learning finite-dimensional algebras directly from corpus data, along the lines of Guevara (2011) and Baroni and Zamparelli (2010). One question we have not addressed in this article is the feasibility of computing with algebraic representations. Although this question is highly dependent on the particular context theory chosen, it is possible that general algorithms for computation within this framework could be found; this is another area that we intend to address in further work. 6.2 Theoretical Investigations Although the context-theoretic framework is an abstraction of the model of meaning as context, it would be good to have a complete understanding of the model and the types of context theories that it allows. Tyi</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Baroni, Marco and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 1183–1193, East Stroudsburg PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="1929" citStr="Bellegarda 2000" startWordPosition="270" endWordPosition="271">omputing power has allowed the development of techniques to analyze statistical properties of words. For example techniques such as latent semantic analysis (Deerwester et al. 1990) and its variants, and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspects of the meanings of words by statistical analysis, and statistical information is often used when parsing to determine sentence structure (Collins 1997). These techniques have proved useful in many applications within computational linguistics and natural language processing (Grefenstette 1994; Sch¨utze 1998; Bellegarda 2000; Choi, WiemerHastings, and Moore 2001; Lin 2003; McCarthy et al. 2004), arguably providing evidence that they capture something about the nature of words that should be included in representations of their meaning. However, it is very difficult to reconcile these techniques with existing theories of meaning in language, which revolve around logical and ontological representations. The new techniques, almost without exception, can be viewed as dealing with vector-based representations of meaning, placing meaning (at least at the word level) within the realm of mathematics and algebra; converse</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Bellegarda, Jerome R. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the IEEE, 88(8):1279–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
<author>Johan Bos</author>
</authors>
<title>Representation and Inference for Natural Language.</title>
<date>2005</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="3306" citStr="Blackburn and Bos 2005" startWordPosition="473" endWordPosition="476"> EC1Y8QE. E-mail: daoud.clarke@gorkana.com. Submission received: 29 October 2008; revised submission received: 28 April 2011; accepted for publication: 29 May 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 1 no unifying theory of meaning to provide guidance to those making use of the new techniques. The problem appears to be a fundamental one in computational linguistics because the whole foundation of meaning seems to be in question. The older, logical theories often subscribe to a model-theoretic philosophy of meaning (Kamp and Reyle 1993; Blackburn and Bos 2005). According to this approach, sentences should be translated to a logical form that can be interpreted as a description of the state of the world. The new vector-based techniques, on the other hand, are often closer in spirit to the philosophy of meaning as context, the idea that the meaning of an expression is determined by how it is used. This is an old idea with origins in the philosophy of Wittgenstein (1953), who said that “meaning just is use,” Firth’s (1968) “You shall know a word by the company it keeps,” and the distributional hypothesis of Harris (1968), that words will occur in simi</context>
</contexts>
<marker>Blackburn, Bos, 2005</marker>
<rawString>Blackburn, Patrick and Johan Bos. 2005. Representation and Inference for Natural Language. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1024--1033</pages>
<location>Prague.</location>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1024–1033, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junfu Cai</author>
<author>Wee Sun Lee</author>
<author>Yee Whye Teh</author>
</authors>
<title>Improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1015--1023</pages>
<location>Prague.</location>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Cai, Junfu, Wee Sun Lee, and Yee Whye Teh. 2007. Improving word sense disambiguation using topic features. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1015–1023, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Choi</author>
<author>Peter Wiemer-Hastings</author>
<author>Johanna Moore</author>
</authors>
<title>Latent Semantic Analysis for text segmentation.</title>
<date>2001</date>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>Choi, Freddy, Peter Wiemer-Hastings, and Johanna Moore. 2001. Latent Semantic Analysis for text segmentation.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>109--117</pages>
<location>Ithaca, NY.</location>
<marker></marker>
<rawString>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 109–117, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Quantum Interaction Symposium (QI-2008),</booktitle>
<pages>133--140</pages>
<location>Oxford.</location>
<marker>Clark, Coecke, Sadrzadeh, 2008</marker>
<rawString>Clark, Stephen, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of meaning. In Proceedings of the Second Quantum Interaction Symposium (QI-2008), pages 133–140, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Stephen Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Quantum Interaction,</booktitle>
<pages>52--55</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="9536" citStr="Clark and Pulman 2007" startWordPosition="1466" endWordPosition="1469">tailment between strings according to that particular method. The contribution of this article is as follows: • We define the context-theoretic framework and introduce the mathematics necessary to understand it. The description presented here is cleaner than that of Clarke (2007), and in addition we provide examples that should provide intuition for the concepts we describe. • We relate the framework to methods of composition that have been proposed in the literature, namely: – vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998); – the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008); – the multiplicative models of Mitchell and Lapata (2008); – matrix multiplication (Baroni and Zamparelli 2010; Rudolph and Giesbrecht 2010); and – the approach of Clark, Coecke, and Sadrzadeh (2008). It is important to note that the purpose of describing related work in terms of our framework is not merely to demonstrate the generality of our framework: In doing so, we identify previously ignored features of this work such as the lattice structure within the vector space. This allows any one of these approaches to be endowed with an entailment property defined by this lattice</context>
<context position="16269" citStr="Clark and Pulman 2007" startWordPosition="2599" endWordPosition="2602">how meanings compose. A crucial part of our thesis is that meanings can be represented by elements of an algebra, and that the type of composition that can be defined using an algebra is general enough to describe the composition of meaning in natural language. To go some way towards justifying this, we give several examples of algebras that describe methods of composition that have been proposed in the literature: namely, point-wise multiplication (Mitchell and Lapata 2008), vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998), and the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008). Example 2 (Point-wise Multiplication) Consider the n-dimensional real vector space Rn. We describe a vector u ∈ Rn in terms of its components as (u1, u2, ... un) with each ui ∈ R. We can define a multiplication · on this space by (u1, u2, ...,un) · (v1, v2, ... , vn) = (u1v1, u2v2, ... unvn) (4) It is easy to see that this satisfies the requirements for an algebra as specified earlier. Table 1 shows a simple example of possible occurrences for three terms in three different contexts, d1, d2, and d3, which may, for example, represent documents. We use this to define the mapping</context>
<context position="72925" citStr="Clark and Pulman 2007" startWordPosition="12695" endWordPosition="12698">ives us an interpretation of the inherent lattice structure on these vector spaces as defining the relation of entailment. It tells us how to measure the size of the vector representation of a string in such a way that the size corresponds to the probability of the string. We have demonstrated that the framework encompasses several related approaches to compositional distributional semantics, including those based on a predefined composition operation such as addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Mitchell and Lapata 2008) or the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008), matrix multiplication (Rudolph and Giesbrecht 2010), and the more sophisticated construction of Clark, Coecke, and Sadrzadeh (2008). 6.1 Practical Investigations Section 4 raises many possibilities for the design of systems to recognize textual entailment within the framework. • Variations on substring matching: experiments with different weighting schemes for substrings, allowing partial commutativity of words or phrases, and replacing words with vectors representing their context, using tensor products of these vectors instead of concatenation. • Extensions of Glickman and D</context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>Clark, Stephen and Stephen Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of the AAAI Spring Symposium on Quantum Interaction, pages 52–55, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Meaning as context and subsequence analysis for textual entailment.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Recognising Textual Entailment Challenge,</booktitle>
<pages>134--139</pages>
<location>Venice.</location>
<contexts>
<context position="54205" citStr="Clarke 2006" startWordPosition="9430" endWordPosition="9431">1 Subsequence Matching and Lexical Overlap A sequence x E A* is a subsequence of y E A* if each element of x occurs in y in the same order, but with the possibility of other elements occurring in between, so for example abba is a subsequence of acabcba in {a, b, c}*. Subsequence matching compares the subsequences of two sequences: The more subsequences they have in common the more similar they are assumed to be. This idea has been used successfully in text classification (Lodhi et al. 2002) and also formed the basis of the author’s entry to the second Recognising Textual Entailment Challenge (Clarke 2006). If S is a semigroup, f1(S) is a lattice-ordered algebra under the multiplication of convolution: (f � g)(x) = 1: f (y)g(z) (47) yz=x where x,y,z E S, f,g E $1(S). Example 18 (Subsequence Matching) Consider the algebra f1(A*) for some alphabet A. This has a basis consisting of elements ex for x E A*, where ex the function that is 1 on x and 0 elsewhere. In particular eE is a unity for the algebra. Define ξ(a) = 12 (ea + e,); then (A, V(A*), ξ) is a context theory. Under this context theory, a sequence x completely entails y if and only if it is a subsequence of y. In our experiments, we have </context>
</contexts>
<marker>Clarke, 2006</marker>
<rawString>Clarke, Daoud. 2006. Meaning as context and subsequence analysis for textual entailment. In Proceedings of the Second PASCAL Recognising Textual Entailment Challenge, pages 134–139, Venice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic Semantics for Natural Language: An Algebraic Framework.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Informatics, University of Sussex.</institution>
<contexts>
<context position="5559" citStr="Clarke 2007" startWordPosition="841" endWordPosition="842">em much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings of words to determine what their vectors should be merely by looking in corpora. If we knew how such vectors should compose then we would be able to extend the benefits of the vector based techniques to the many applications that require reasoning about the meaning of phrases and sentences. This article describes the results of our own efforts to identify a theory that can unite these two paradigms, introduced in the author’s DPhil thesis (Clarke 2007). In addition, we also discuss the relationship between this theory and methods of composition that have recently been proposed in the literature, showing that many of them can be considered as falling within our framework. Our approach in identifying the framework is summarized in Figure 1: • Inspired by the philosophy of meaning as context and vector-based techniques we developed a mathematical model of meaning as context, in which the meaning of a string is a vector representing contexts in which that string occurs in a hypothetical infinite corpus. • The theory on its own is not useful whe</context>
<context position="9195" citStr="Clarke (2007)" startWordPosition="1414" endWordPosition="1415"> of the representation that need to hold. Any method of combining 43 Computational Linguistics Volume 38, Number 1 these vectors in which these properties hold can be considered within the framework and is thus justified according to the underlying theory; in addition the framework instructs us as to how to measure the degree of entailment between strings according to that particular method. The contribution of this article is as follows: • We define the context-theoretic framework and introduce the mathematics necessary to understand it. The description presented here is cleaner than that of Clarke (2007), and in addition we provide examples that should provide intuition for the concepts we describe. • We relate the framework to methods of composition that have been proposed in the literature, namely: – vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998); – the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008); – the multiplicative models of Mitchell and Lapata (2008); – matrix multiplication (Baroni and Zamparelli 2010; Rudolph and Giesbrecht 2010); and – the approach of Clark, Coecke, and Sadrzadeh (2008). It is important to note that the purpose </context>
<context position="32959" citStr="Clarke (2007)" startWordPosition="5620" endWordPosition="5621">xt theory. Example 8 The vectors given in Table 1 give the following calculation for the degree to which cat entails animal: ,(cat) = (0,2,3) ,(animal) = (2,1,2) ,(cat) ∧ ,(animal) = (0,1,2) Ent(,(cat), ,(animal)) = II,(cat) ∧ ,(animal)II1/II,(cat)II1 = 3/5 An important question is how this context-theoretic definition of the degree of entailment relates to more familiar notions of entailment. There are three main ways in which the term entailment is used: • the model-theoretic sense of entailment in which a theory A entails a theory B if every model of A is also a model of B. It was shown in Clarke (2007) that this type of entailment can be described using context theories, where sentences are represented as projections on a vector space. • entailment between terms (as expressed for example in the WordNet hierarchy), for example the hypernymy relation between the terms cat and animal encodes the fact that a cat is an animal. In Clarke (2007) we showed 52 Clarke A Context-Theoretic Framework for Distributional Semantics that such relations can be encoded in the partial order structure of a vector lattice. • Human common-sense judgments as to whether one sentence entails or implies another sente</context>
<context position="74209" citStr="Clarke (2007)" startWordPosition="12878" endWordPosition="12879">ps using other distributional languages based on n-grams or other models in which words do not commute, or a combination of context theories based on commutative and non-commutative models. 67 Computational Linguistics Volume 38, Number 1 • The LDA model we used is a commutative one. This is a considerable simplification of what is possible within the context-theoretic framework; it would be interesting to investigate methods of incorporating non-commutativity into the model. • Implementations based on the approach to representing uncertainty in logical semantics similar to those described in Clarke (2007). All of these ideas could be evaluated using the data sets from the Recognising Textual Entailment Challenges. There are many approaches to textual entailment that we have not considered here; we conjecture that variations of many of them could be described within our framework. We leave the task of investigating the relationship between these approaches and our framework to further work. Other areas that we are investigating, together with researchers at the University of Sussex, is the possibility of learning finite-dimensional algebras directly from corpus data, along the lines of Guevara </context>
</contexts>
<marker>Clarke, 2007</marker>
<rawString>Clarke, Daoud. 2007. Context-theoretic Semantics for Natural Language: An Algebraic Framework. Ph.D. thesis, Department of Informatics, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Madrid.</location>
<contexts>
<context position="1755" citStr="Collins 1997" startWordPosition="249" endWordPosition="250">odel is general enough to encompass several proposed methods of composition in vector-based representations of meaning. In recent years, the abundance of text corpora and computing power has allowed the development of techniques to analyze statistical properties of words. For example techniques such as latent semantic analysis (Deerwester et al. 1990) and its variants, and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspects of the meanings of words by statistical analysis, and statistical information is often used when parsing to determine sentence structure (Collins 1997). These techniques have proved useful in many applications within computational linguistics and natural language processing (Grefenstette 1994; Sch¨utze 1998; Bellegarda 2000; Choi, WiemerHastings, and Moore 2001; Lin 2003; McCarthy et al. 2004), arguably providing evidence that they capture something about the nature of words that should be included in representations of their meaning. However, it is very difficult to reconcile these techniques with existing theories of meaning in language, which revolve around logical and ontological representations. The new techniques, almost without except</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 16–23, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<pages>1--8</pages>
<location>Southampton, UK.</location>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Dagan, Ido, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, pages 1–8, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>George Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1495" citStr="Deerwester et al. 1990" startWordPosition="209" endWordPosition="212">uction This article presents the thesis that defining meaning as context leads naturally to a model in which meanings of strings are represented as elements of an associative algebra over the real numbers, and entailment is described by a vector lattice ordering. This model is general enough to encompass several proposed methods of composition in vector-based representations of meaning. In recent years, the abundance of text corpora and computing power has allowed the development of techniques to analyze statistical properties of words. For example techniques such as latent semantic analysis (Deerwester et al. 1990) and its variants, and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspects of the meanings of words by statistical analysis, and statistical information is often used when parsing to determine sentence structure (Collins 1997). These techniques have proved useful in many applications within computational linguistics and natural language processing (Grefenstette 1994; Sch¨utze 1998; Bellegarda 2000; Choi, WiemerHastings, and Moore 2001; Lin 2003; McCarthy et al. 2004), arguably providing evidence that they capture something about the nature of words that should</context>
<context position="59799" citStr="Deerwester et al. 1990" startWordPosition="10417" endWordPosition="10420">al language D&apos;; the vector p then becomes a probability distribution over the documents in D&apos;. In our own experiments, we used latent Dirichlet allocation (Blei, Ng, and Jordan 2003) to build a model of the corpus as a probabilistic language based on a subset of around 380,000 documents from the Gigaword corpus. Having this model allows us to consider an infinite array of possible documents, and thus we can use our context-theoretic definition of entailment because there is no problem of data sparseness. Latent Dirichlet allocation (LDA) follows the same vein as latent semantic analysis (LSA; Deerwester et al. 1990) and probabilistic latent semantic analysis (PLSA; Hofmann 1999) in that it can be used to build models of corpora in which words within a document are considered to be exchangeable, so that a document is treated as a bag of words. LSA performs a singular value decomposition on the matrix of words and documents which brings out hidden “latent” similarities in meaning between words, even though they may not occur together. In contrast, PLSA and LDA provide probabilistic models of corpora using Bayesian methods. LDA differs from PLSA in that, whereas the latter assumes a fixed number of document</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, Scott, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Paraphrase assessment in structured vector space: Exploring parameters and datasets.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>57--65</pages>
<location>Athens.</location>
<marker>Erk, Pad´o, 2009</marker>
<rawString>Erk, Katrin and Sebastian Pad´o. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 57–65, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A synopsis of linguistic theory, 1930–1955. In</title>
<date>1968</date>
<booktitle>Selected Papers of JR Firth,</booktitle>
<pages>1952--59</pages>
<editor>John R. Firth, editor,</editor>
<publisher>Indiana University Press,</publisher>
<location>Bloomington,</location>
<marker>Firth, 1968</marker>
<rawString>Firth, John R. 1968. A synopsis of linguistic theory, 1930–1955. In John R. Firth, editor, Selected Papers of JR Firth, 1952–59. Indiana University Press, Bloomington, pages 168–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas K Landauer</author>
</authors>
<date>1998</date>
<booktitle>The measurement of textual coherence Computational Linguistics Volume 38, Number 1 with latent semantic analysis. Discourse Process,</booktitle>
<pages>15--285</pages>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Foltz, Peter W., Walter Kintsch, and Thomas K. Landauer. 1998. The measurement of textual coherence Computational Linguistics Volume 38, Number 1 with latent semantic analysis. Discourse Process, 15:285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>107--114</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="26765" citStr="Geffet and Dagan (2005)" startWordPosition="4550" endWordPosition="4553"> simply their frequencies of occurrences in n different contexts, so that they are vectors in Rn. In this case, the relation ξ(x) &lt; ξ(y) means that y occurs at least as frequently as x in every context. This means that y occurs in at least as wide a range of contexts as x, and occurs as least as frequently as x. Thus the statement “x entails y if and only if ξ(x) &lt; ξ(y)” can be viewed as a stronger form of the distributional hypothesis of Harris (1968). In fact, this idea can be related to the notion of distributional generality, introduced by Weeds, Weir, and McCarthy (2004) and developed by Geffet and Dagan (2005). A term x is distributionally more general than another term y if x occurs in a subset of the contexts that y occurs in. The idea is that distributional generality may be connected to semantic generality. An example of this is the hypernymy or is-a relation that is used to express generality of concepts in ontologies; for example, the term animal is a hypernym of dog because a dog is an animal. Weeds, Weir, and McCarthy (2004, p. 1019) explain the connection to distributional generality as follows: Although one can obviously think of counter-examples, we would generally expect that the more s</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Geffet, M. and I. Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 107–114, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Glickman</author>
<author>I Dagan</author>
</authors>
<title>A probabilistic setting and lexical cooccurrence model for textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>43--48</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="52946" citStr="Glickman and Dagan (2005)" startWordPosition="9212" endWordPosition="9215">ons, if, for example, the model is found to be too restrictive to model certain linguistic phenomena. 4. Applications to Textual Entailment In this section we analyze approaches to the problem of recognizing textual entailment, showing how they can be related to the context-theoretic framework, and discussing potential new approaches that are suggested by looking at them within the framework. We first discuss some simple approaches to textual entailment based on subsequence 59 Computational Linguistics Volume 38, Number 1 matching and measuring lexical overlap. We then look at the approach of Glickman and Dagan (2005), showing that it can be considered as a context theory in which words are represented as projections on the vector space of documents. This leads us to an implementation of our own in which we used latent Dirichlet allocation as an alternative approach to overcoming the problem of data sparseness. A fair amount of effort is required to describe these approaches within our framework. Although there is no immediate practical benefit to be gained from this, our main purpose in doing this is to demonstrate the generality of the framework. We also hope that insight into these approaches may be gle</context>
<context position="55884" citStr="Glickman and Dagan (2005)" startWordPosition="9730" endWordPosition="9733">nd Magnini 2005). This approach can be described as a context theory in terms of a free commutative semigroup on a set A, defined by A*/ - where x - y in A* if the symbols making up x can be reordered to make y. Then define ξ&apos; by ξ&apos;(a) = 12 (e[a] + e[E]) where [a] is the equivalence class of a in A*/ -. Then (A, f1(A*/ =4 ξ&apos;) is a context theory in which entailment is defined by lexical overlap. More complex definitions of xˆ can be used, for example, to weight different words by their probabilities. 60 Clarke A Context-Theoretic Framework for Distributional Semantics 4.2 Document Projections Glickman and Dagan (2005) give a probabilistic definition of entailment in terms of “possible worlds” which they use to justify their lexical entailment model based on occurrences of words in Web documents. They estimate the lexical entailment probability LEP(u, v) to be LEP(u,v) ^&apos; nu&apos; vv (48) where nv and nu,v denote the number of documents in which the word v occurs and in which the words u and v both occur, respectively. From the context-theoretic perspective, we view the set of documents in which the word occurs as its context vector. To describe this situation in terms of a context theory, consider the vector sp</context>
<context position="59062" citStr="Glickman and Dagan (2005)" startWordPosition="10296" endWordPosition="10299">by ¯p(T) = Tp, where • p E f∞(D) is defined by p(d) = 1/|D |for all d E D. This is defined such that I 1PupI 11 is the probability of the term u. 61 Computational Linguistics Volume 38, Number 1 The degree to which x entails y is then given by I1Pxp n PypI11/I1PxI11 = I1PxPypI11/I1PxI11. This corresponds directly to Glickman and Dagan’s (2005) entailment “confidence”; it is simply the proportion of documents that contain all the terms of x which also contain all the terms of y. 4.3 Latent Dirichlet Projections The formulation in the previous section suggests an alternative approach to that of Glickman and Dagan (2005) to cope with the data sparseness problem. We consider the finite data available D as a sample from a distributional language D&apos;; the vector p then becomes a probability distribution over the documents in D&apos;. In our own experiments, we used latent Dirichlet allocation (Blei, Ng, and Jordan 2003) to build a model of the corpus as a probabilistic language based on a subset of around 380,000 documents from the Gigaword corpus. Having this model allows us to consider an infinite array of possible documents, and thus we can use our context-theoretic definition of entailment because there is no prob</context>
<context position="64244" citStr="Glickman and Dagan (2005)" startWordPosition="11172" endWordPosition="11175">ntegrate over the Dirichlet parameter 0: IIPxqII1 =f 1 jpe(a) p(0)d0 (54) 6 (aEx where by a E x we mean that the word a occurs in string x, and pe(a) is the probability of observing word a in a document generated by the parameter 0. We estimate this by ( N E pe(a) � 1 − 1 − p(a|z)p(z|0) z We built a latent Dirichlet allocation model using Blei, Ng, and (2003) implementation on documents from the British National Corpus, using 100 topics. We evaluated this model on the 800 entailment pairs from the first Recognizing Textual Entailment Challenge test Results were comparable to those obtained by Glickman and Dagan (2005) (see Table 2). In this table, Accuracy is the accuracy on the test set, consisting of 800 entailment pairs, and CWS is the confidence weighted score; see Dagan, Glickman, and Magnini (2005) for the definition. The differences between the accuracy values in the table are not statistically significant because of the small data set, although all accuracies in the table are significantly better than chance at the 1% level. The accuracy of the model is considerably lower than the state of the art, which is around 75% (Bar-Haim et al. 2006). We experimented with various document lengths and found v</context>
</contexts>
<marker>Glickman, Dagan, 2005</marker>
<rawString>Glickman, O. and I. Dagan. 2005. A probabilistic setting and lexical cooccurrence model for textual entailment. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 43–48, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Stephen Pulman</author>
</authors>
<title>Concrete sentence spaces for compositional distributional models of meaning.</title>
<date>2011</date>
<booktitle>Proceedings of the 9th International Conference on Computational Semantics (IWCS 2011),</booktitle>
<pages>125--134</pages>
<location>Oxford.</location>
<marker>Grefenstette, Sadrzadeh, Clark, Coecke, Pulman, 2011</marker>
<rawString>Grefenstette, Edward, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete sentence spaces for compositional distributional models of meaning. Proceedings of the 9th International Conference on Computational Semantics (IWCS 2011), pages 125–134, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, Netherlands.</location>
<contexts>
<context position="1897" citStr="Grefenstette 1994" startWordPosition="266" endWordPosition="267">he abundance of text corpora and computing power has allowed the development of techniques to analyze statistical properties of words. For example techniques such as latent semantic analysis (Deerwester et al. 1990) and its variants, and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspects of the meanings of words by statistical analysis, and statistical information is often used when parsing to determine sentence structure (Collins 1997). These techniques have proved useful in many applications within computational linguistics and natural language processing (Grefenstette 1994; Sch¨utze 1998; Bellegarda 2000; Choi, WiemerHastings, and Moore 2001; Lin 2003; McCarthy et al. 2004), arguably providing evidence that they capture something about the nature of words that should be included in representations of their meaning. However, it is very difficult to reconcile these techniques with existing theories of meaning in language, which revolve around logical and ontological representations. The new techniques, almost without exception, can be viewed as dealing with vector-based representations of meaning, placing meaning (at least at the word level) within the realm of m</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, Gregory. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>Computing semantic compositionality in distributional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Semantics (IWCS</booktitle>
<pages>135--144</pages>
<location>Oxford.</location>
<contexts>
<context position="4580" citStr="Guevara 2011" startWordPosition="684" endWordPosition="685">ypothesis is justified by the success of techniques such as latent semantic analysis as well as experimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Kintsch 2001). A solution to this problem would have practical as well as philosophical benefits. Current techniques such as latent semantic analysis work well at the word level, but we cannot extend them much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings of words to determine what their vectors should be merely by looking in corpo</context>
<context position="74815" citStr="Guevara (2011)" startWordPosition="12971" endWordPosition="12972">e (2007). All of these ideas could be evaluated using the data sets from the Recognising Textual Entailment Challenges. There are many approaches to textual entailment that we have not considered here; we conjecture that variations of many of them could be described within our framework. We leave the task of investigating the relationship between these approaches and our framework to further work. Other areas that we are investigating, together with researchers at the University of Sussex, is the possibility of learning finite-dimensional algebras directly from corpus data, along the lines of Guevara (2011) and Baroni and Zamparelli (2010). One question we have not addressed in this article is the feasibility of computing with algebraic representations. Although this question is highly dependent on the particular context theory chosen, it is possible that general algorithms for computation within this framework could be found; this is another area that we intend to address in further work. 6.2 Theoretical Investigations Although the context-theoretic framework is an abstraction of the model of meaning as context, it would be good to have a complete understanding of the model and the types of con</context>
</contexts>
<marker>Guevara, 2011</marker>
<rawString>Guevara, Emiliano. 2011. Computing semantic compositionality in distributional semantics. In Proceedings of the 9th International Conference on Computational Semantics (IWCS 2011), pages 135–144, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Halmos</author>
</authors>
<title>Finite Dimensional Vector Spaces.</title>
<date>1974</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="12932" citStr="Halmos (1974)" startWordPosition="2023" endWordPosition="2024"> described in the next section. Because of its relation to this motivating example, a context theory can be thought of as a hypothesis describing in what contexts all strings occur. Definition 1 (Context Theory) A context theory is a tuple (A, A, &amp;, V, *), where A is a set (the alphabet), A is a unital algebra over the real numbers, &amp; is a function from A to A, V is an abstract Lebesgue space, and * is an injective linear map from A to V. We will explain each part of this definition, introducing the necessary mathematics as we proceed. We assume the reader is familiar with linear algebra; see Halmos (1974) for definitions that are not included here. 2.1 Algebra over a Field We have identified an algebra over a field (or simply algebra when there is no ambiguity) as an important construction because it generalizes nearly all the methods of vector-based composition that have been proposed. An algebra adds a multiplication operation to a vector space; the vector space is intended to describe meaning, and it is this multiplication operation that defines the composition of meaning in contexttheoretic semantics. Definition 2 (Algebra over a Field) An algebra over a field is a vector space A over a fi</context>
</contexts>
<marker>Halmos, 1974</marker>
<rawString>Halmos, Paul. 1974. Finite Dimensional Vector Spaces. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="3875" citStr="Harris (1968)" startWordPosition="574" endWordPosition="575">Kamp and Reyle 1993; Blackburn and Bos 2005). According to this approach, sentences should be translated to a logical form that can be interpreted as a description of the state of the world. The new vector-based techniques, on the other hand, are often closer in spirit to the philosophy of meaning as context, the idea that the meaning of an expression is determined by how it is used. This is an old idea with origins in the philosophy of Wittgenstein (1953), who said that “meaning just is use,” Firth’s (1968) “You shall know a word by the company it keeps,” and the distributional hypothesis of Harris (1968), that words will occur in similar contexts if and only if they have similar meanings. This hypothesis is justified by the success of techniques such as latent semantic analysis as well as experimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2</context>
<context position="26598" citStr="Harris (1968)" startWordPosition="4525" endWordPosition="4526">roperties we would expect from this relation. There may be more justification for this assumption, however, based on the case where the vectors for terms are simply their frequencies of occurrences in n different contexts, so that they are vectors in Rn. In this case, the relation ξ(x) &lt; ξ(y) means that y occurs at least as frequently as x in every context. This means that y occurs in at least as wide a range of contexts as x, and occurs as least as frequently as x. Thus the statement “x entails y if and only if ξ(x) &lt; ξ(y)” can be viewed as a stronger form of the distributional hypothesis of Harris (1968). In fact, this idea can be related to the notion of distributional generality, introduced by Weeds, Weir, and McCarthy (2004) and developed by Geffet and Dagan (2005). A term x is distributionally more general than another term y if x occurs in a subset of the contexts that y occurs in. The idea is that distributional generality may be connected to semantic generality. An example of this is the hypernymy or is-a relation that is used to express generality of concepts in ontologies; for example, the term animal is a hypernym of dog because a dog is an animal. Weeds, Weir, and McCarthy (2004, p</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Harris, Zellig. 1968. Mathematical Structures of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the 15th Conference on Uncertainty in AI,</booktitle>
<pages>289--296</pages>
<location>Stockholm.</location>
<contexts>
<context position="59863" citStr="Hofmann 1999" startWordPosition="10427" endWordPosition="10428">the documents in D&apos;. In our own experiments, we used latent Dirichlet allocation (Blei, Ng, and Jordan 2003) to build a model of the corpus as a probabilistic language based on a subset of around 380,000 documents from the Gigaword corpus. Having this model allows us to consider an infinite array of possible documents, and thus we can use our context-theoretic definition of entailment because there is no problem of data sparseness. Latent Dirichlet allocation (LDA) follows the same vein as latent semantic analysis (LSA; Deerwester et al. 1990) and probabilistic latent semantic analysis (PLSA; Hofmann 1999) in that it can be used to build models of corpora in which words within a document are considered to be exchangeable, so that a document is treated as a bag of words. LSA performs a singular value decomposition on the matrix of words and documents which brings out hidden “latent” similarities in meaning between words, even though they may not occur together. In contrast, PLSA and LDA provide probabilistic models of corpora using Bayesian methods. LDA differs from PLSA in that, whereas the latter assumes a fixed number of documents, LDA assumes that the data at hand are a sample from an infini</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, Thomas. 1999. Probabilistic latent semantic analysis. In Proceedings of the 15th Conference on Uncertainty in AI, pages 289–296, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic: Introduction to Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory,</title>
<date>1993</date>
<booktitle>of Studies in Linguistics and Philosophy.</booktitle>
<volume>42</volume>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="3281" citStr="Kamp and Reyle 1993" startWordPosition="469" endWordPosition="472">Banner Street, London EC1Y8QE. E-mail: daoud.clarke@gorkana.com. Submission received: 29 October 2008; revised submission received: 28 April 2011; accepted for publication: 29 May 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 1 no unifying theory of meaning to provide guidance to those making use of the new techniques. The problem appears to be a fundamental one in computational linguistics because the whole foundation of meaning seems to be in question. The older, logical theories often subscribe to a model-theoretic philosophy of meaning (Kamp and Reyle 1993; Blackburn and Bos 2005). According to this approach, sentences should be translated to a logical form that can be interpreted as a description of the state of the world. The new vector-based techniques, on the other hand, are often closer in spirit to the philosophy of meaning as context, the idea that the meaning of an expression is determined by how it is used. This is an old idea with origins in the philosophy of Wittgenstein (1953), who said that “meaning just is use,” Firth’s (1968) “You shall know a word by the company it keeps,” and the distributional hypothesis of Harris (1968), that</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Kamp, Hans and Uwe Reyle. 1993. From Discourse to Logic: Introduction to Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory, volume 42 of Studies in Linguistics and Philosophy. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<pages>25--173</pages>
<contexts>
<context position="4758" citStr="Kintsch 2001" startWordPosition="710" endWordPosition="711">e not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Kintsch 2001). A solution to this problem would have practical as well as philosophical benefits. Current techniques such as latent semantic analysis work well at the word level, but we cannot extend them much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings of words to determine what their vectors should be merely by looking in corpora. If we knew how such vectors should compose then we would be able to extend the benefits of the vector based techniques to the many applications that require reasoning about t</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Kintsch, Walter. 2001. Predication. Cognitive Science, 25:173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Lambek</author>
</authors>
<title>The mathematics of sentence structure.</title>
<date>1958</date>
<pages>65--154</pages>
<publisher>American Mathematical Monthly,</publisher>
<contexts>
<context position="51561" citStr="Lambek 1958" startWordPosition="8996" endWordPosition="8997">ntences The box is square and *The box is shape we would expect the second to be represented by the zero vector because it is not grammatical; square can be a noun and an adjective, whereas shape cannot. Distributivity of meaning means that the component of meaning that square has in common with shape must be disjoint with the adjectival component of the meaning of square. Associativity is also a very strong requirement to place; indeed Lambek (1961) introduced non-associativity into his calculus precisely to deal with examples that were not satisfactorily dealt with by his associative model (Lambek 1958). Our framework provides answers to someone considering the use of algebra for natural language semantics. What field should be used? The real numbers. Need the algebra be finite-dimensional? No. Should the algebra by unital? Yes. Some of these answers impose restrictions on what is possible within the framework. The full implication of these restrictions for linguistics is beyond the scope of this article, and indeed is not yet known. Although we hope that these features or boundaries are useful in their current form, it may be that with time, or for certain applications, there is a reason to</context>
<context position="66670" citStr="Lambek (1958)" startWordPosition="11581" endWordPosition="11582">al Entailment Challenge for two document lengths N = 106 and N = 107 using a cut-off for the degree of entailment of 0.5 at which entailment was regarded as holding. Model Accuracy CWS Dirichlet (106) 0.584 0.630 Dirichlet (107) 0.576 0.642 Bayer (MITRE) 0.586 0.617 Glickman (Bar Ilan) 0.586 0.572 Jijkoun (Amsterdam) 0.552 0.559 Newman (Dublin) 0.565 0.6 (2011). In this section, we will show how their model can be described as a context theory. The authors describe the syntactic element of their construction using pregroups (Lambek 2001), a formalism which simplifies the syntactic calculus of Lambek (1958). These can be described in terms of partially ordered monoids, a monoid G with a partial ordering &lt; satisfying x &lt; y implies xz &lt; yz and zx &lt; zy for all x, y, z E G. Definition 11 (Pregroup) Let G be a partially ordered monoid. Then G is called a pregroup if for each x E G there are elements xl and xr in G such that xlx &lt; 1 xxr &lt; 1 1 &lt; xxl 1 &lt; xrx If x, y E G, we call y a reduction of x if y can be obtained from x using only Rules (56) and (57). Pregroup grammars are defined by freely generating a pregroup on a set of basic grammatical types. Words are then represented as elements formed from</context>
</contexts>
<marker>Lambek, 1958</marker>
<rawString>Lambek, Joachim. 1958. The mathematics of sentence structure. American Mathematical Monthly, 65:154–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Lambek</author>
</authors>
<title>On the calculus of syntactic types.</title>
<date>1961</date>
<booktitle>Structure of Language and Its Mathematical Aspects,</booktitle>
<pages>166--178</pages>
<editor>In Roman Jakobson, editor,</editor>
<publisher>American Mathematical Society,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="51403" citStr="Lambek (1961)" startWordPosition="8974" endWordPosition="8975">h the word shape. Then we would expect this component to be preserved in the sentences He drew a square and He drew a shape. However, in the case of the two sentences The box is square and *The box is shape we would expect the second to be represented by the zero vector because it is not grammatical; square can be a noun and an adjective, whereas shape cannot. Distributivity of meaning means that the component of meaning that square has in common with shape must be disjoint with the adjectival component of the meaning of square. Associativity is also a very strong requirement to place; indeed Lambek (1961) introduced non-associativity into his calculus precisely to deal with examples that were not satisfactorily dealt with by his associative model (Lambek 1958). Our framework provides answers to someone considering the use of algebra for natural language semantics. What field should be used? The real numbers. Need the algebra be finite-dimensional? No. Should the algebra by unital? Yes. Some of these answers impose restrictions on what is possible within the framework. The full implication of these restrictions for linguistics is beyond the scope of this article, and indeed is not yet known. Al</context>
</contexts>
<marker>Lambek, 1961</marker>
<rawString>Lambek, Joachim. 1961. On the calculus of syntactic types. In Roman Jakobson, editor, Structure of Language and Its Mathematical Aspects, pages 166–178, American Mathematical Society, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Lambek</author>
</authors>
<title>Type grammars as pregroups.</title>
<date>2001</date>
<journal>Grammars,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="66600" citStr="Lambek 2001" startWordPosition="11571" endWordPosition="11572">richlet projection model on the data from the first Recognizing Textual Entailment Challenge for two document lengths N = 106 and N = 107 using a cut-off for the degree of entailment of 0.5 at which entailment was regarded as holding. Model Accuracy CWS Dirichlet (106) 0.584 0.630 Dirichlet (107) 0.576 0.642 Bayer (MITRE) 0.586 0.617 Glickman (Bar Ilan) 0.586 0.572 Jijkoun (Amsterdam) 0.552 0.559 Newman (Dublin) 0.565 0.6 (2011). In this section, we will show how their model can be described as a context theory. The authors describe the syntactic element of their construction using pregroups (Lambek 2001), a formalism which simplifies the syntactic calculus of Lambek (1958). These can be described in terms of partially ordered monoids, a monoid G with a partial ordering &lt; satisfying x &lt; y implies xz &lt; yz and zx &lt; zy for all x, y, z E G. Definition 11 (Pregroup) Let G be a partially ordered monoid. Then G is called a pregroup if for each x E G there are elements xl and xr in G such that xlx &lt; 1 xxr &lt; 1 1 &lt; xxl 1 &lt; xrx If x, y E G, we call y a reduction of x if y can be obtained from x using only Rules (56) and (57). Pregroup grammars are defined by freely generating a pregroup on a set of basic</context>
</contexts>
<marker>Lambek, 2001</marker>
<rawString>Lambek, Joachim. 2001. Type grammars as pregroups. Grammars, 4(1):21–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="4708" citStr="Landauer and Dumais 1997" startWordPosition="701" endWordPosition="704">e (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Kintsch 2001). A solution to this problem would have practical as well as philosophical benefits. Current techniques such as latent semantic analysis work well at the word level, but we cannot extend them much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings of words to determine what their vectors should be merely by looking in corpora. If we knew how such vectors should compose then we would be able to extend the benefits of the vector based techniques to th</context>
<context position="9439" citStr="Landauer and Dumais 1997" startWordPosition="1451" endWordPosition="1454"> the underlying theory; in addition the framework instructs us as to how to measure the degree of entailment between strings according to that particular method. The contribution of this article is as follows: • We define the context-theoretic framework and introduce the mathematics necessary to understand it. The description presented here is cleaner than that of Clarke (2007), and in addition we provide examples that should provide intuition for the concepts we describe. • We relate the framework to methods of composition that have been proposed in the literature, namely: – vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998); – the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008); – the multiplicative models of Mitchell and Lapata (2008); – matrix multiplication (Baroni and Zamparelli 2010; Rudolph and Giesbrecht 2010); and – the approach of Clark, Coecke, and Sadrzadeh (2008). It is important to note that the purpose of describing related work in terms of our framework is not merely to demonstrate the generality of our framework: In doing so, we identify previously ignored features of this work such as the lattice structure within the vector space. This all</context>
<context position="16170" citStr="Landauer and Dumais 1997" startWordPosition="2584" endWordPosition="2587">ws us to associate an element of the algebra with every string of words. The algebra is what tells us how meanings compose. A crucial part of our thesis is that meanings can be represented by elements of an algebra, and that the type of composition that can be defined using an algebra is general enough to describe the composition of meaning in natural language. To go some way towards justifying this, we give several examples of algebras that describe methods of composition that have been proposed in the literature: namely, point-wise multiplication (Mitchell and Lapata 2008), vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998), and the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008). Example 2 (Point-wise Multiplication) Consider the n-dimensional real vector space Rn. We describe a vector u ∈ Rn in terms of its components as (u1, u2, ... un) with each ui ∈ R. We can define a multiplication · on this space by (u1, u2, ...,un) · (v1, v2, ... , vn) = (u1v1, u2v2, ... unvn) (4) It is easy to see that this satisfies the requirements for an algebra as specified earlier. Table 1 shows a simple example of possible occurrences for three terms in three different conte</context>
<context position="72802" citStr="Landauer and Dumais 1997" startWordPosition="12676" endWordPosition="12679">rds and phrases using vectors, and tells us that the product of two meanings should be distributive and associative. It also gives us an interpretation of the inherent lattice structure on these vector spaces as defining the relation of entailment. It tells us how to measure the size of the vector representation of a string in such a way that the size corresponds to the probability of the string. We have demonstrated that the framework encompasses several related approaches to compositional distributional semantics, including those based on a predefined composition operation such as addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Mitchell and Lapata 2008) or the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008), matrix multiplication (Rudolph and Giesbrecht 2010), and the more sophisticated construction of Clark, Coecke, and Sadrzadeh (2008). 6.1 Practical Investigations Section 4 raises many possibilities for the design of systems to recognize textual entailment within the framework. • Variations on substring matching: experiments with different weighting schemes for substrings, allowing partial commutativity of words or phrases, and replacing words with vectors </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, Thomas K. and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-1999),</booktitle>
<pages>23--32</pages>
<location>College Park, MD.</location>
<contexts>
<context position="1576" citStr="Lee 1999" startWordPosition="223" endWordPosition="224">odel in which meanings of strings are represented as elements of an associative algebra over the real numbers, and entailment is described by a vector lattice ordering. This model is general enough to encompass several proposed methods of composition in vector-based representations of meaning. In recent years, the abundance of text corpora and computing power has allowed the development of techniques to analyze statistical properties of words. For example techniques such as latent semantic analysis (Deerwester et al. 1990) and its variants, and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspects of the meanings of words by statistical analysis, and statistical information is often used when parsing to determine sentence structure (Collins 1997). These techniques have proved useful in many applications within computational linguistics and natural language processing (Grefenstette 1994; Sch¨utze 1998; Bellegarda 2000; Choi, WiemerHastings, and Moore 2001; Lin 2003; McCarthy et al. 2004), arguably providing evidence that they capture something about the nature of words that should be included in representations of their meaning. However, it is very difficult t</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lee, Lillian. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-1999), pages 23–32, College Park, MD.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dekang 1998 Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL ’98),</booktitle>
<pages>768--774</pages>
<location>Montreal.</location>
<marker>Lin, </marker>
<rawString>Lin, Dekang.1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL ’98), pages 768–774, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>2003</date>
<booktitle>Treebanks: Building and Using Parsed Corpora,</booktitle>
<pages>317--330</pages>
<editor>In Anne Abeill´e, editor,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1977" citStr="Lin 2003" startWordPosition="278" endWordPosition="279">s to analyze statistical properties of words. For example techniques such as latent semantic analysis (Deerwester et al. 1990) and its variants, and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspects of the meanings of words by statistical analysis, and statistical information is often used when parsing to determine sentence structure (Collins 1997). These techniques have proved useful in many applications within computational linguistics and natural language processing (Grefenstette 1994; Sch¨utze 1998; Bellegarda 2000; Choi, WiemerHastings, and Moore 2001; Lin 2003; McCarthy et al. 2004), arguably providing evidence that they capture something about the nature of words that should be included in representations of their meaning. However, it is very difficult to reconcile these techniques with existing theories of meaning in language, which revolve around logical and ontological representations. The new techniques, almost without exception, can be viewed as dealing with vector-based representations of meaning, placing meaning (at least at the word level) within the realm of mathematics and algebra; conversely the older theories of meaning dwell in the re</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Lin, Dekang. 2003. Dependency-based evaluation of MINIPAR. In Anne Abeill´e, editor, Treebanks: Building and Using Parsed Corpora, pages 317–330, Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="54088" citStr="Lodhi et al. 2002" startWordPosition="9410" endWordPosition="9413">of the framework. We also hope that insight into these approaches may be gleaned by viewing them from a new perspective. 4.1 Subsequence Matching and Lexical Overlap A sequence x E A* is a subsequence of y E A* if each element of x occurs in y in the same order, but with the possibility of other elements occurring in between, so for example abba is a subsequence of acabcba in {a, b, c}*. Subsequence matching compares the subsequences of two sequences: The more subsequences they have in common the more similar they are assumed to be. This idea has been used successfully in text classification (Lodhi et al. 2002) and also formed the basis of the author’s entry to the second Recognising Textual Entailment Challenge (Clarke 2006). If S is a semigroup, f1(S) is a lattice-ordered algebra under the multiplication of convolution: (f � g)(x) = 1: f (y)g(z) (47) yz=x where x,y,z E S, f,g E $1(S). Example 18 (Subsequence Matching) Consider the algebra f1(A*) for some alphabet A. This has a basis consisting of elements ex for x E A*, where ex the function that is 1 on x and 0 elsewhere. In particular eE is a unity for the algebra. Define ξ(a) = 12 (ea + e,); then (A, V(A*), ξ) is a context theory. Under this co</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Lodhi, Huma, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<pages>279--286</pages>
<location>Barcelona.</location>
<contexts>
<context position="2000" citStr="McCarthy et al. 2004" startWordPosition="280" endWordPosition="283">ze statistical properties of words. For example techniques such as latent semantic analysis (Deerwester et al. 1990) and its variants, and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspects of the meanings of words by statistical analysis, and statistical information is often used when parsing to determine sentence structure (Collins 1997). These techniques have proved useful in many applications within computational linguistics and natural language processing (Grefenstette 1994; Sch¨utze 1998; Bellegarda 2000; Choi, WiemerHastings, and Moore 2001; Lin 2003; McCarthy et al. 2004), arguably providing evidence that they capture something about the nature of words that should be included in representations of their meaning. However, it is very difficult to reconcile these techniques with existing theories of meaning in language, which revolve around logical and ontological representations. The new techniques, almost without exception, can be viewed as dealing with vector-based representations of meaning, placing meaning (at least at the word level) within the realm of mathematics and algebra; conversely the older theories of meaning dwell in the realm of logic and ontolo</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), pages 279–286, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="4111" citStr="Miller and Charles 1991" startWordPosition="610" endWordPosition="613">n the other hand, are often closer in spirit to the philosophy of meaning as context, the idea that the meaning of an expression is determined by how it is used. This is an old idea with origins in the philosophy of Wittgenstein (1953), who said that “meaning just is use,” Firth’s (1968) “You shall know a word by the company it keeps,” and the distributional hypothesis of Harris (1968), that words will occur in similar contexts if and only if they have similar meanings. This hypothesis is justified by the success of techniques such as latent semantic analysis as well as experimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; F</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>Miller, George A. and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="4504" citStr="Mitchell and Lapata 2008" startWordPosition="670" endWordPosition="673">t words will occur in similar contexts if and only if they have similar meanings. This hypothesis is justified by the success of techniques such as latent semantic analysis as well as experimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Kintsch 2001). A solution to this problem would have practical as well as philosophical benefits. Current techniques such as latent semantic analysis work well at the word level, but we cannot extend them much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings o</context>
<context position="9610" citStr="Mitchell and Lapata (2008)" startWordPosition="1477" endWordPosition="1480">tribution of this article is as follows: • We define the context-theoretic framework and introduce the mathematics necessary to understand it. The description presented here is cleaner than that of Clarke (2007), and in addition we provide examples that should provide intuition for the concepts we describe. • We relate the framework to methods of composition that have been proposed in the literature, namely: – vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998); – the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008); – the multiplicative models of Mitchell and Lapata (2008); – matrix multiplication (Baroni and Zamparelli 2010; Rudolph and Giesbrecht 2010); and – the approach of Clark, Coecke, and Sadrzadeh (2008). It is important to note that the purpose of describing related work in terms of our framework is not merely to demonstrate the generality of our framework: In doing so, we identify previously ignored features of this work such as the lattice structure within the vector space. This allows any one of these approaches to be endowed with an entailment property defined by this lattice structure, based on a philosophy of meaning as context. Although the exam</context>
<context position="16127" citStr="Mitchell and Lapata 2008" startWordPosition="2578" endWordPosition="2581"> string. Thus, the mapping defined by ˆ allows us to associate an element of the algebra with every string of words. The algebra is what tells us how meanings compose. A crucial part of our thesis is that meanings can be represented by elements of an algebra, and that the type of composition that can be defined using an algebra is general enough to describe the composition of meaning in natural language. To go some way towards justifying this, we give several examples of algebras that describe methods of composition that have been proposed in the literature: namely, point-wise multiplication (Mitchell and Lapata 2008), vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998), and the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008). Example 2 (Point-wise Multiplication) Consider the n-dimensional real vector space Rn. We describe a vector u ∈ Rn in terms of its components as (u1, u2, ... un) with each ui ∈ R. We can define a multiplication · on this space by (u1, u2, ...,un) · (v1, v2, ... , vn) = (u1v1, u2v2, ... unvn) (4) It is easy to see that this satisfies the requirements for an algebra as specified earlier. Table 1 shows a simple example of possible occurrenc</context>
<context position="19733" citStr="Mitchell and Lapata (2008)" startWordPosition="3256" endWordPosition="3259">ct sum (see Halmos [1974] for definitions); we recall their basic properties here. Let Vn denote a vector space of dimensionality n (note that all vector spaces of a fixed dimensionality � � � � � � � ... ... ... 1 (6) 47 Computational Linguistics Volume 38, Number 1 are isomorphic). Then the tensor product space Vn ⊗ Vm is isomorphic to a space Vnm of dimensionality nm; moreover, given orthonormal bases B = {b1, b2, ... , bn} for Vn and C = {c1, c2,.. . , cm} for Vm there is an orthonormal basis for Vnm defined by {bi ⊗ cj : 1 ≤ i ≤ n and 1 ≤ j ≤ m} (9) Example 4 The multiplicative models of Mitchell and Lapata (2008) correspond to the class of finite dimensional algebras. Let A be a finite-dimensional vector space. Then every associative bilinear product on A can be described by a linear function T from A ⊗ A to A, as required in Mitchell and Lapata’s model. To see this, consider the action of the product · on two orthonormal basis vectors a and b of A. This is a vector in A, thus we can define T(a ⊗ b) = a · b. By considering all basis vectors, we can define the linear function T. If the tensor product can loosely be viewed as “multiplying” vector spaces, then the direct sum is like adding them; the spac</context>
<context position="72864" citStr="Mitchell and Lapata 2008" startWordPosition="12685" endWordPosition="12688">of two meanings should be distributive and associative. It also gives us an interpretation of the inherent lattice structure on these vector spaces as defining the relation of entailment. It tells us how to measure the size of the vector representation of a string in such a way that the size corresponds to the probability of the string. We have demonstrated that the framework encompasses several related approaches to compositional distributional semantics, including those based on a predefined composition operation such as addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Mitchell and Lapata 2008) or the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008), matrix multiplication (Rudolph and Giesbrecht 2010), and the more sophisticated construction of Clark, Coecke, and Sadrzadeh (2008). 6.1 Practical Investigations Section 4 raises many possibilities for the design of systems to recognize textual entailment within the framework. • Variations on substring matching: experiments with different weighting schemes for substrings, allowing partial commutativity of words or phrases, and replacing words with vectors representing their context, using tensor products of these vec</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Mitchell, Jeff and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Preller</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Bell states and negative sentences in the distributed model of meaning.</title>
<date>2011</date>
<journal>Electronic Notes in Theoretical Computer Science,</journal>
<volume>270</volume>
<issue>2</issue>
<contexts>
<context position="4609" citStr="Preller and Sadrzadeh 2011" startWordPosition="686" endWordPosition="689">ustified by the success of techniques such as latent semantic analysis as well as experimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Kintsch 2001). A solution to this problem would have practical as well as philosophical benefits. Current techniques such as latent semantic analysis work well at the word level, but we cannot extend them much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings of words to determine what their vectors should be merely by looking in corpora. If we knew how such vecto</context>
</contexts>
<marker>Preller, Sadrzadeh, 2011</marker>
<rawString>Preller, Anne and Mehrnoosh Sadrzadeh. 2011. Bell states and negative sentences in the distributed model of meaning. Electronic Notes in Theoretical Computer Science, 270(2):141–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Rudolph</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>907--916</pages>
<location>Uppsala.</location>
<contexts>
<context position="9693" citStr="Rudolph and Giesbrecht 2010" startWordPosition="1488" endWordPosition="1491">ork and introduce the mathematics necessary to understand it. The description presented here is cleaner than that of Clarke (2007), and in addition we provide examples that should provide intuition for the concepts we describe. • We relate the framework to methods of composition that have been proposed in the literature, namely: – vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998); – the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008); – the multiplicative models of Mitchell and Lapata (2008); – matrix multiplication (Baroni and Zamparelli 2010; Rudolph and Giesbrecht 2010); and – the approach of Clark, Coecke, and Sadrzadeh (2008). It is important to note that the purpose of describing related work in terms of our framework is not merely to demonstrate the generality of our framework: In doing so, we identify previously ignored features of this work such as the lattice structure within the vector space. This allows any one of these approaches to be endowed with an entailment property defined by this lattice structure, based on a philosophy of meaning as context. Although the examples described here show that existing approaches can be described within the frame</context>
<context position="14447" citStr="Rudolph and Giesbrecht (2010)" startWordPosition="2291" endWordPosition="2294"> case our definition would refer to an associative algebra. An algebra is called unital if it has a distinguished unity element 1 satisfying 1x = x1 = x for all x E A. We are generally only interested in real algebras, where K is the field of real numbers, R. Example 1 The square real-valued matrices of order n form a real unital associative algebra under standard matrix multiplication. The vector operations are defined entry-wise. The unity element of the algebra is the identity matrix. 45 Computational Linguistics Volume 38, Number 1 This means that our proposal is more general than that of Rudolph and Giesbrecht (2010), who suggest using matrix multiplication as a framework for distributional semantic composition. The main differences in our proposal are as follows. • We allow dimensionality to be infinite, instead of restricting ourselves to finite-dimensional matrices. • Matrix algebras form a ∗-algebra, whereas we do not currently impose this requirement. • Many of the vector spaces used in computational linguistics have an implicit lattice structure; we emphasize the importance of this structure and use the associated partial ordering to define entailment. The purpose of ξ in the context theory is to as</context>
<context position="17421" citStr="Rudolph and Giesbrecht (2010)" startWordPosition="2810" endWordPosition="2813">hich may, for example, represent documents. We use this to define the mapping ξ from terms to vectors. Thus, in this example, we have ξ(cat) = (0, 2,3) and ξ(big) = (1, 3, 0). Under point-wise multiplication, we would have � big cat = ξ(big) · ξ(cat) = (1, 3,0) · (0, 2, 3) = (0, 6, 0) (5) 46 Clarke A Context-Theoretic Framework for Distributional Semantics Table 1 Example of possible occurrences for three terms in three different contexts. d1 d2 d3 cat 0 2 3 animal 2 1 2 big 1 3 0 One commonly used operation for composing vector-based representations of meaning is vector addition. As noted by Rudolph and Giesbrecht (2010), this can be described using matrix multiplication, by embedding an n-dimensional vector u into a matrix of order n + 1: α u1 u2 ··· un 0 α 0 ··· 0 0 0 α ··· 0 ... 0 0 0 ··· α where α = 1. The set of all such matrices, for all real values of α, forms a subalgebra of the algebra of matrices of order n + 1. A subalgebra of an algebra A is a sub-vector space of A which is closed under the multiplication of A. This subalgebra can be equivalently described as follows: Example 3 (Additive Algebra) For two vectors u = (α, u1, u2, ... un) and v = (β, v1, v2 ... vn) in Rn+1, we define the additive pro</context>
<context position="72993" citStr="Rudolph and Giesbrecht 2010" startWordPosition="12703" endWordPosition="12706">n these vector spaces as defining the relation of entailment. It tells us how to measure the size of the vector representation of a string in such a way that the size corresponds to the probability of the string. We have demonstrated that the framework encompasses several related approaches to compositional distributional semantics, including those based on a predefined composition operation such as addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Mitchell and Lapata 2008) or the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008), matrix multiplication (Rudolph and Giesbrecht 2010), and the more sophisticated construction of Clark, Coecke, and Sadrzadeh (2008). 6.1 Practical Investigations Section 4 raises many possibilities for the design of systems to recognize textual entailment within the framework. • Variations on substring matching: experiments with different weighting schemes for substrings, allowing partial commutativity of words or phrases, and replacing words with vectors representing their context, using tensor products of these vectors instead of concatenation. • Extensions of Glickman and Dagan’s approach and our own context-theoretic approach using LDA, pe</context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>Rudolph, Sebastian and Eugenie Giesbrecht. 2010. Compositional matrix-space models of language. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 907–916, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Sch¨utze, Heinrich. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="false">
<title>Clarke A Context-Theoretic Framework for Distributional Semantics</title>
<marker></marker>
<rawString>Clarke A Context-Theoretic Framework for Distributional Semantics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--1</pages>
<contexts>
<context position="4682" citStr="Smolensky 1990" startWordPosition="699" endWordPosition="700">rimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Kintsch 2001). A solution to this problem would have practical as well as philosophical benefits. Current techniques such as latent semantic analysis work well at the word level, but we cannot extend them much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings of words to determine what their vectors should be merely by looking in corpora. If we knew how such vectors should compose then we would be able to extend the benefits of the vec</context>
<context position="9513" citStr="Smolensky 1990" startWordPosition="1464" endWordPosition="1465">the degree of entailment between strings according to that particular method. The contribution of this article is as follows: • We define the context-theoretic framework and introduce the mathematics necessary to understand it. The description presented here is cleaner than that of Clarke (2007), and in addition we provide examples that should provide intuition for the concepts we describe. • We relate the framework to methods of composition that have been proposed in the literature, namely: – vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998); – the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008); – the multiplicative models of Mitchell and Lapata (2008); – matrix multiplication (Baroni and Zamparelli 2010; Rudolph and Giesbrecht 2010); and – the approach of Clark, Coecke, and Sadrzadeh (2008). It is important to note that the purpose of describing related work in terms of our framework is not merely to demonstrate the generality of our framework: In doing so, we identify previously ignored features of this work such as the lattice structure within the vector space. This allows any one of these approaches to be endowed with an entailment property </context>
<context position="16246" citStr="Smolensky 1990" startWordPosition="2597" endWordPosition="2598">s what tells us how meanings compose. A crucial part of our thesis is that meanings can be represented by elements of an algebra, and that the type of composition that can be defined using an algebra is general enough to describe the composition of meaning in natural language. To go some way towards justifying this, we give several examples of algebras that describe methods of composition that have been proposed in the literature: namely, point-wise multiplication (Mitchell and Lapata 2008), vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998), and the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008). Example 2 (Point-wise Multiplication) Consider the n-dimensional real vector space Rn. We describe a vector u ∈ Rn in terms of its components as (u1, u2, ... un) with each ui ∈ R. We can define a multiplication · on this space by (u1, u2, ...,un) · (v1, v2, ... , vn) = (u1v1, u2v2, ... unvn) (4) It is easy to see that this satisfies the requirements for an algebra as specified earlier. Table 1 shows a simple example of possible occurrences for three terms in three different contexts, d1, d2, and d3, which may, for example, represent documents. We use thi</context>
<context position="72902" citStr="Smolensky 1990" startWordPosition="12693" endWordPosition="12694">ative. It also gives us an interpretation of the inherent lattice structure on these vector spaces as defining the relation of entailment. It tells us how to measure the size of the vector representation of a string in such a way that the size corresponds to the probability of the string. We have demonstrated that the framework encompasses several related approaches to compositional distributional semantics, including those based on a predefined composition operation such as addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Mitchell and Lapata 2008) or the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008), matrix multiplication (Rudolph and Giesbrecht 2010), and the more sophisticated construction of Clark, Coecke, and Sadrzadeh (2008). 6.1 Practical Investigations Section 4 raises many possibilities for the design of systems to recognize textual entailment within the framework. • Variations on substring matching: experiments with different weighting schemes for substrings, allowing partial commutativity of words or phrases, and replacing words with vectors representing their context, using tensor products of these vectors instead of concatenation. • Exten</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Smolensky, Paul. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46(1-2):159–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dawei Song</author>
<author>Peter D Bruza</author>
</authors>
<title>Towards context-sensitive information inference.</title>
<date>2003</date>
<journal>Journal of the American Society for Information Science and Technology (JASIST),</journal>
<pages>54--321</pages>
<contexts>
<context position="75975" citStr="Song and Bruza (2003)" startWordPosition="13148" endWordPosition="13151">ve a complete understanding of the model and the types of context theories that it allows. Tying down these properties would allow us to define algebras that could truly be called “context theories.” The context-theoretic framework shares a lot of properties with the study of free probability (Voiculescu 1997). It would be interesting to investigate whether ideas from free probability would carry over to context-theoretic semantics. Although we have related our model to many techniques described in the literature, we still have to investigate its relationship with other models such as that of Song and Bruza (2003) and Guevara (2011). We have not given much consideration here to the issue of multi-word expressions and non-compositionality. What predictions does the context-theoretic framework make about non-compositionality? Answering this may lead us to new techniques for recognizing and handling multi-word expressions and non-compositionality. Of course it is hard to predict the benefits that may result from what we have presented, because we have given a way of thinking about meaning in natural language that in many respects is new. This new way of thinking opens the door to the unification of logic-</context>
</contexts>
<marker>Song, Bruza, 2003</marker>
<rawString>Song, Dawei and Peter D. Bruza. 2003. Towards context-sensitive information inference. Journal of the American Society for Information Science and Technology (JASIST), 54:321–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan-Virgil Voiculescu</author>
</authors>
<title>Free Probability Theory.</title>
<date>1997</date>
<publisher>American Mathematical Society,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="75665" citStr="Voiculescu 1997" startWordPosition="13102" endWordPosition="13103">it is possible that general algorithms for computation within this framework could be found; this is another area that we intend to address in further work. 6.2 Theoretical Investigations Although the context-theoretic framework is an abstraction of the model of meaning as context, it would be good to have a complete understanding of the model and the types of context theories that it allows. Tying down these properties would allow us to define algebras that could truly be called “context theories.” The context-theoretic framework shares a lot of properties with the study of free probability (Voiculescu 1997). It would be interesting to investigate whether ideas from free probability would carry over to context-theoretic semantics. Although we have related our model to many techniques described in the literature, we still have to investigate its relationship with other models such as that of Song and Bruza (2003) and Guevara (2011). We have not given much consideration here to the issue of multi-word expressions and non-compositionality. What predictions does the context-theoretic framework make about non-compositionality? Answering this may lead us to new techniques for recognizing and handling m</context>
</contexts>
<marker>Voiculescu, 1997</marker>
<rawString>Voiculescu, Dan-Virgil. 1997. Free Probability Theory. American Mathematical Society, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of CoLING</booktitle>
<pages>1015--1021</pages>
<location>Geneva.</location>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Weeds, Julie, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of CoLING 2004, pages 1015–1021, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Symposium on Quantum Interaction,</booktitle>
<pages>1--8</pages>
<location>Oxford.</location>
<contexts>
<context position="4518" citStr="Widdows 2008" startWordPosition="674" endWordPosition="675">lar contexts if and only if they have similar meanings. This hypothesis is justified by the success of techniques such as latent semantic analysis as well as experimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pad´o 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Kintsch 2001). A solution to this problem would have practical as well as philosophical benefits. Current techniques such as latent semantic analysis work well at the word level, but we cannot extend them much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings of words to det</context>
<context position="9551" citStr="Widdows 2008" startWordPosition="1470" endWordPosition="1471">s according to that particular method. The contribution of this article is as follows: • We define the context-theoretic framework and introduce the mathematics necessary to understand it. The description presented here is cleaner than that of Clarke (2007), and in addition we provide examples that should provide intuition for the concepts we describe. • We relate the framework to methods of composition that have been proposed in the literature, namely: – vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998); – the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008); – the multiplicative models of Mitchell and Lapata (2008); – matrix multiplication (Baroni and Zamparelli 2010; Rudolph and Giesbrecht 2010); and – the approach of Clark, Coecke, and Sadrzadeh (2008). It is important to note that the purpose of describing related work in terms of our framework is not merely to demonstrate the generality of our framework: In doing so, we identify previously ignored features of this work such as the lattice structure within the vector space. This allows any one of these approaches to be endowed with an entailment property defined by this lattice structure, bas</context>
<context position="16284" citStr="Widdows 2008" startWordPosition="2603" endWordPosition="2604"> crucial part of our thesis is that meanings can be represented by elements of an algebra, and that the type of composition that can be defined using an algebra is general enough to describe the composition of meaning in natural language. To go some way towards justifying this, we give several examples of algebras that describe methods of composition that have been proposed in the literature: namely, point-wise multiplication (Mitchell and Lapata 2008), vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998), and the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008). Example 2 (Point-wise Multiplication) Consider the n-dimensional real vector space Rn. We describe a vector u ∈ Rn in terms of its components as (u1, u2, ... un) with each ui ∈ R. We can define a multiplication · on this space by (u1, u2, ...,un) · (v1, v2, ... , vn) = (u1v1, u2v2, ... unvn) (4) It is easy to see that this satisfies the requirements for an algebra as specified earlier. Table 1 shows a simple example of possible occurrences for three terms in three different contexts, d1, d2, and d3, which may, for example, represent documents. We use this to define the mapping ξ from terms t</context>
<context position="34986" citStr="Widdows 2008" startWordPosition="5935" endWordPosition="5936"> textual entailment within the framework. Our definition is more general than the model-theoretic and hypernymy notions of entailment, however, as it allows the measurement of a degree of entailment between any two strings: As an extreme example, one may measure the degree to which not a entails in the. Although this may not be useful or philosophically meaningful, we view it as a practical consequence of the fact that every string has a vector representation in our model, which coincides with the current practice in vector-based compositionality techniques (Clark, Coecke, and Sadrzadeh 2008; Widdows 2008). 2.4 Lattice Ordered Algebras A lattice ordered algebra merges the lattice ordering of the vector space V with the product of A. This structure encapsulates the ordering properties that are familiar from multiplication in matrices and elementary arithmetic. For this reason, many proposed methods of composing vector-based representations of meaning can be viewed as lattice ordered algebras. The only reason we have not included it as a requirement of the framework is because our motivating example (described in the next section) is not guaranteed to have this property, although it does give us </context>
<context position="72940" citStr="Widdows 2008" startWordPosition="12699" endWordPosition="12700">on of the inherent lattice structure on these vector spaces as defining the relation of entailment. It tells us how to measure the size of the vector representation of a string in such a way that the size corresponds to the probability of the string. We have demonstrated that the framework encompasses several related approaches to compositional distributional semantics, including those based on a predefined composition operation such as addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Mitchell and Lapata 2008) or the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008), matrix multiplication (Rudolph and Giesbrecht 2010), and the more sophisticated construction of Clark, Coecke, and Sadrzadeh (2008). 6.1 Practical Investigations Section 4 raises many possibilities for the design of systems to recognize textual entailment within the framework. • Variations on substring matching: experiments with different weighting schemes for substrings, allowing partial commutativity of words or phrases, and replacing words with vectors representing their context, using tensor products of these vectors instead of concatenation. • Extensions of Glickman and Dagan’s approach</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Widdows, Dominic. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second Symposium on Quantum Interaction, pages 1–8, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludwig Wittgenstein</author>
</authors>
<title>Philosophical Investigations.</title>
<date>1953</date>
<publisher>Macmillan,</publisher>
<location>New York. G. Anscombe, translator.</location>
<contexts>
<context position="3722" citStr="Wittgenstein (1953)" startWordPosition="548" endWordPosition="549">stics because the whole foundation of meaning seems to be in question. The older, logical theories often subscribe to a model-theoretic philosophy of meaning (Kamp and Reyle 1993; Blackburn and Bos 2005). According to this approach, sentences should be translated to a logical form that can be interpreted as a description of the state of the world. The new vector-based techniques, on the other hand, are often closer in spirit to the philosophy of meaning as context, the idea that the meaning of an expression is determined by how it is used. This is an old idea with origins in the philosophy of Wittgenstein (1953), who said that “meaning just is use,” Firth’s (1968) “You shall know a word by the company it keeps,” and the distributional hypothesis of Harris (1968), that words will occur in similar contexts if and only if they have similar meanings. This hypothesis is justified by the success of techniques such as latent semantic analysis as well as experimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each o</context>
</contexts>
<marker>Wittgenstein, 1953</marker>
<rawString>Wittgenstein, Ludwig. 1953. Philosophical Investigations. Macmillan, New York. G. Anscombe, translator.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>