<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997484">
Planning Reference Choices for Argumentative Texts
</title>
<author confidence="0.992652">
Xiaorong Huang*
</author>
<affiliation confidence="0.967689">
Techne Knowledge Systems
439 University Avenue
</affiliation>
<address confidence="0.8020995">
Toronto, Ontario M5S 3G4
Canada
</address>
<email confidence="0.865937">
xhOFormalSystems.ca
</email>
<sectionHeader confidence="0.995283" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851714285714">
This paper deals with the reference choices in-
volved in the generation of argumentative text.
Since a natual segmentation of discourse into
attentional spaces is needed to carry out this
task, this paper first proposes an architecture
for natural language generation that combines
hierarchical planning and focus-guided naviga-
tion, a work in its own right. While hierarchi-
cal planning spans out an attentional hierarchy
of the discourse produced, local navigation fills
details into the primitive discourse spaces. The
usefulness of this architecture actually goes be-
yond the particular domain of application for
which it is developed.
A piece of argumentative text such as the proof
of a mathematical theorem conveys a sequence
of derivations. For each step of derivation, the
premises derived in the previous context and
the inference method (such as the application
of a particular theorem or definition) must be
made clear. Although not restricted to nominal
phrases, our reference decisions are similar to
those concerning nominal subsequent referring
expressions. Based on the work of Reichmann,
this paper presents a discourse theory that han-
dles reference choices by taking into account
both textual distance as well as the attentional
hierarchy.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.944750043478261">
This paper describes how reference decisions are
made in PROVERB, a system that verbalizes
machine-found natural deduction (ND) proofs. A
piece of argumentative text such as the proof of a
mathematical theorem can be viewed as a sequence
•Much of this research was carried out while the au-
thor was at Dept. of CS, Univ. of the Saarland, sup-
ported by DFG (German Research Council). This paper
was written while the author was a visitor at Dept. of
CS, Univ. of Toronto, using facilities supported by a
grant from the Natural Sciences and Engineering Re-
search Council of Canada.
of derivations. Each such derivation is realized in
PROVERB by a proof communicative act (PCA),
following the viewpoint that language utterances are
actions. PCAs involve referring phrases that should
help a reader to unambiguously identify an object of
a certain type from a pool of candidates. Concretely,
such references must be made for previously derived
conclusions used as premises and for the inference
method used in the current step.
As an example, let us look at the PCA with the
name Derive below:
</bodyText>
<note confidence="0.506371">
(Derive Derived-Formula: u* lu = u
Reasons: (unit(lu ,U, *), u EU)
Method: Def-Semigroup*unit)
</note>
<bodyText confidence="0.999934">
Here, the slot Derived-Formula is filled by a new
conclusion which this PCA aims to convey. It can be
inferred by applying the filler of Method to the filler
of Reasons as premises. There are alternative ways
of referring to both the Reasons and the Method.
Depending on the discourse history, the following
are two of the possible verbalizations:
</bodyText>
<listItem confidence="0.81802">
1. (inference method omitted):
</listItem>
<bodyText confidence="0.860846">
&amp;quot;Since l is the unit element of U, and u is
an element of U, u * lu =u.&amp;quot;
</bodyText>
<sectionHeader confidence="0.432135" genericHeader="introduction">
2. (reasons omitted):
</sectionHeader>
<bodyText confidence="0.997068625">
&amp;quot;According to the definition of unit element,
u * lu u.&amp;quot;
An explicit reference to a premise or an inference
method is not restricted to a nominal phrase, as
opposed to many of the treatments of subsequent
references found in the literature. Despite this dif-
ference, the choices to be made here have much in
common with the choices of subsequent references
discussed in more general frameworks (Reichman,
1985; Grosz and Sidner, 1986; Dale, 1992): they
depend on the availability of the object to be re-
ferred to in the context and are sensitive to the seg-
mentation of a context into an attentional hierarchy.
Therefore, we have first to devise an architecture for
natural language generation that facilitates a nat-
ural and effective segmentation of discourse. The
</bodyText>
<page confidence="0.995145">
190
</page>
<bodyText confidence="0.999913076923077">
basic idea is to distinguish between language pro-
duction activities that effect the global shift of at-
tention, and language production activities that in-
volve only local attentional movement. Concretely,
PROVERB uses an architecture that models text
generation as a combination of hierarchical planning
and focus-guided navigation. Following (Grosz and
Sidner, 1986) we further assume that every posting
of a new task by the hierarchical planning mecha-
nism creates new attentional spaces. Based on this
segmentation, PROVERB makes reference choices
according to a discourse theory adapted from Reich-
man (Reichman, 1985; Huang, 1990).
</bodyText>
<sectionHeader confidence="0.825221" genericHeader="method">
2 The System PROVERB
</sectionHeader>
<bodyText confidence="0.999894607142857">
PROVERB is a text planner that verbalizes natural
deduction (ND) style proofs (Gentzen, 1935). Sev-
eral similar attempts can be found in previous work.
The system EXPOUND (Chester, 1976) is an exam-
ple of direct translation: Although a sophisticated
linearization is applied on the input ND proofs, the
steps are then translated locally in a template-driven
way. ND proofs were tested as inputs to an early
version of MUMBLE (McDonald, 1983); the main
aim, however, was to show the feasibility of the ar-
chitecture. A more recent attempt can be found in
THINKER (Edgar and Pelletier, 1993), which imple-
ments several interesting but isolated proof presenta-
tion strategies. PROVERB however can be seen as
the first serious attempt for a comprehensive system
that produces adequate argumentative texts from
ND style proofs. Figure 1 shows the architecture
of PROVERB(Huang, 1994a; HuangFiedler, 1997):
the macroplanner produces a sequence of PCAs, the
DRCC (Derive Reference Choices Component) mod-
ule of the microplanner enriches the PCAs with ref-
erence choices. The TSG (Text Structure Genera-
tor) module subsequently produces the text struc-
tures as the output of the microplanner. Finally,
text structures are realized by TAG-GEN (Kilger
and Finkler, 1995), our realization component. In
this paper, we concentrate only on the macroplan-
ner and the DRCC component.
</bodyText>
<subsectionHeader confidence="0.998838">
2.1 Architecture of the Macroplanner
</subsectionHeader>
<bodyText confidence="0.999947076923077">
Most current text planners adopt a hierarchical plan-
ning approach (Hovy, 1988; Moore and Paris, 1989;
Dale, 1992; Reithinger, 1991). Nevertheless there
is psychological evidence that language has an un-
planned, spontaneous aspect as well (Ochs, 1979).
Based on this observation, Sibun (Sibun, 1990) im-
plemented a system for generating descriptions of
objects with a strong domain structure, such as
houses, chips, and families. Her system produces
text using a technique she called local organization.
While a hierarchical planner recursively breaks gen-
eration tasks into subtasks, local organization navi-
gates the domain-object following the local focus of
</bodyText>
<subsectionHeader confidence="0.430204">
Natural Deduction Proof
</subsectionHeader>
<figureCaption confidence="0.999756">
Figure 1: Architecture of PROVERB
</figureCaption>
<bodyText confidence="0.992285034482759">
attention.
PROVERB combines both of these approaches
in a uniform planning framework (Huang, 1994a).
The hierarchical planning splits the task of present-
ing a particular proof into subtasks of presenting
subproofs. While the overall planning mechanism
follows the RST-based planning approach (Hovy,
1988; Moore and Paris, 1989; Reithinger, 1991),
the planning operators more resemble the schemata
in schema-based planning (McKeown, 1985; Paris,
1988) since presentation patterns associated with
specific proof patterns normally contain multiple
RST-relations. PROVERB&apos;s hierarchical planning
is driven by proof patterns that entail or suggest es-
tablished ways of presentation. For trivial proofs
that demonstrate no characteristic patterns, how-
ever, this technology will fail. PROVERB navigates
such relatively small parts of a proof and chooses the
next conclusion to be presented under the guidance
of a local focus mechanism.
While most existing systems follow one of the two
approaches exclusively, PROVERB uses them as
complementary techniques in an integrated frame-
work. In this way, our architecture provides a clear
way of factoring out domain-dependent presenta-
tion knowledge from more general NLG techniques.
While PROVERB&apos;s hierarchical planning operators
encodes accepted format for mathematical text, its
local navigation embodies more generic principles of
</bodyText>
<figure confidence="0.965854666666667">
Tqacroplanner
Microplanner
Text Structure
</figure>
<page confidence="0.99502">
191
</page>
<bodyText confidence="0.999339166666667">
language production.
The two kinds of planning operators are treated ac-
cordingly. Since hierarchical planning operators em-
body explicit communicative norms, they are given
a higher priority. Only when none of them is appli-
cable, will a local navigation operator be chosen.
</bodyText>
<subsectionHeader confidence="0.99974">
2.2 Proof Communicative Acts
</subsectionHeader>
<bodyText confidence="0.9994321">
PCAs are the primitive actions planned by the
macroplanner of PROVERB. Like speech acts, they
can be defined in terms of the communicative goals
they fulfill as well as their possible verbalizations.
The simplest one conveying the derivation of a new
intermediate conclusion is illustrated in the intro-
duction. There are also PCAs that convey a partial
plan for further presentation and thereby update the
reader&apos;s global attentional structure. For instance,
the PCA
</bodyText>
<subsubsectionHeader confidence="0.341752">
(Begin-Cases Goal: Formula
</subsubsectionHeader>
<sectionHeader confidence="0.363487" genericHeader="method">
Assumptions: (A B))
</sectionHeader>
<bodyText confidence="0.535478">
creates two attentional spaces with A and B as the
assumptions, and Formula as the goal by producing
the verbalization:
&amp;quot;To prove Formula, let us consider the two cases
by assuming A and B.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.999507">
2.3 Hierarchical Planning
</subsectionHeader>
<bodyText confidence="0.999972625">
Hierarchical planning operators represent commu-
nicative norms concerning how a proof is to be pre-
sented can be split into subproofs, how the subproofs
can be mapped onto some linear order, and how
primitive subproofs should be conveyed by PCAs.
Let us look at one such operator, which handles
proof by case analysis. The corresponding schema
of such a proof tree&apos; is shown in Figure 2, where
</bodyText>
<equation confidence="0.68823075">
•
?L4 : F V G
71-7 CASE
?Li : F Q
</equation>
<figureCaption confidence="0.986099">
Figure 2: Proof Schema Case
</figureCaption>
<bodyText confidence="0.98511075">
the subproof rooted by ?L4 leads to F V G, while
subproofs rooted by ?L2 and ?L3 are the two cases
proving Q by assuming F or G, respectively. The
applicability encodes the two scenarios of case anal-
ysis, where we do not go into details. In both circum-
stances this operator first presents the part leading
to F V G, and then proceeds with the two cases. It
also inserts certain PCAs to mediate between parts
&apos;We adopt for proof tree the notation of Gentzen.
Each bar represents a step of derivation, where the for-
mula beneath the bar is derived from the premises above
the bar. For the convenience of discussion, some formu-
lae are given an identifying label, such as ?Li.
of proofs. This procedure is captured by the plan-
ning operator below.
Case-Implicit
</bodyText>
<listItem confidence="0.999607">
• Applicability Condition: ((task ?L1) V
(local-focus ?L4)) A (not-conveyed (?L2 ?L3))
• Acts:
1. if ?L4 has not been conveyed, then present ?L4
(subgoal 1)
2. a PCA with the verbalization: &amp;quot;First, let us
consider the first case by assuming F.&amp;quot;
3. present ?L2 (subgoal 2)
4. a PCA with the verbalization: &amp;quot;Next, we con-
sider the second case by assuming G.&amp;quot;
5. present ?L3 (subgoal 3)
6. mark ?L1 as conveyed
• features: (hierarchical-planning compulsory im-
plicit)
</listItem>
<subsectionHeader confidence="0.998824">
2.4 Planning as Navigation
</subsectionHeader>
<bodyText confidence="0.9999776">
The local navigation operators simulate the un-
planned part of proof presentation. Instead of split-
ting presentation goals into subgoals, they follow the
local derivation relation to find a proof step to be
presented next.
</bodyText>
<subsectionHeader confidence="0.869616">
2.4.1 The Local Focus
</subsectionHeader>
<bodyText confidence="0.999959230769231">
The node to be presented next is suggested by the
mechanism of local focus. In PROVERB, our local
focus is the last derived step, while focal centers are
semantic objects mentioned in the local focus. Al-
though logically any proof node that uses the local
focus as a premise could be chosen for the next step,
usually the one with the greatest semantic overlap
with the focal centers is preferred. In other words, if
one has proved a property about some semantic ob-
jects, one will tend to continue to talk about these
particular objects, before turning to new objects.
Let us examine the situation when the proof below
is awaiting presentation.
</bodyText>
<equation confidence="0.668979">
1] : P(a, b) 1 • P a b , [3
2] : Q(a,b)&apos; 4: R b,
[5] : Q(a,b) A R(b, c)
</equation>
<bodyText confidence="0.9996726">
Assume that node [1] is the local focus, {a, b} is the
set of focal centers, [3] is a previously presented node
and node [5] is the root of the proof to be presented.
[2] is chosen as the next node to be presented, since
it does not introduce any new semantic object and
its overlap with the focal centers a a, bp is larger
than the overlap of [4] with the focal centers ({b}).
For local focus mechanisms used in another do-
main of application, readers are referred to (McKe-
own, 1985).
</bodyText>
<sectionHeader confidence="0.990125" genericHeader="method">
3 The Attentional Hierarchy
</sectionHeader>
<bodyText confidence="0.999552">
The distinction between hierarchical planning and
local navigation leads to a very natural segmentation
</bodyText>
<page confidence="0.979583">
192
</page>
<table confidence="0.976018833333333">
NNo S;D
7; I-
7; I-
7; F
7; I-
;11 F
7;11 I-
7;11 I-
7;11 F
7;11 F
7;11 I-
7;11 F-
7;11 I-
7;11 I-
7;11 F
7; I-
Formula
group(F, *) A subgroup(U, F, *) A unit(F,1,*) A
</table>
<equation confidence="0.895369647058823">
unit(U, lu, *)
UCF
lu E U
3sx U
u E U
u* lu =u
u E F
lu E F
semigroup(F, *)
solution(u, u, lu, F,*)
u*1 = u
1 E F
solution(u, u, 1, F,*)
1 = lu
1 = lu
group(F,*) A subgroup(U, F,*) A unit(F,1,*) A
unit(U, lu,*) = 1 = 1u
Reason
(113/13)
(Def-subgroup 7)
(Def-unit 7)
(3 9)
(}13&apos;13)
(Def-unit 7 11)
(Def-subset 8 11)
(Def-subset 8 9)
(Def-group 7)
(Def-solution 12 13 14 15)
(Def-unit 7 13)
(Def-unit 7)
(Def-solution 13 17 18 15)
(Th-solution 17 16 19)
(Choice 10 20)
(Ded 7 21)
</equation>
<figureCaption confidence="0.998452">
Figure 3: Abstracted Proof about Unit Element of Subgroups
</figureCaption>
<bodyText confidence="0.97764106060606">
of a discourse into an attentional hierarchy, since fol-
lowing the theory of Grosz and Sidner (Grosz and
Sidner, 1986), there is a one-to-one correspondence
between the intentional hierarchy and the atten-
tional hierarchy. In this section, we illustrate the
attentional hierarchy with the help of an example,
which will be used to discuss reference choices later.
The input proof in Figure 3 is an ND style proof
for the following theorem2:
Theorem:
Let F be a group and U a subgroup of F. If 1 and
lu are unit elements of F and U respectively, then
1 = lu.
The definitions of semigroup, group, and unit are
obvious. solution (a , b, c, F, *) stands for &amp;quot;c is a so-
lution of the equation a * x = b in F.&amp;quot; Each line in
the proof is of the form:
Label A I- Conclusion (Justification reasons)
where Justification is either an ND inference rule, a
definition or theorem, which justifies the derivation
of the Conclusion using as premises the formulas in
the lines given as reasons. A can be ignored for our
purpose.
We assume a reader will build up a (partial) proof
tree as his model of the ongoing discourse. The
corresponding discourse model after the completion
of the presentation of the proof in Figure 3 is a
proof tree shown in Figure 4. Note that the bars
in Gentzen&apos;s notion (Figure 2) are replaced by links
for clarity. The numbers associated with nodes are
the corresponding line numbers in Figure 4. Chil-
dren of nodes are given in the order they have been
presented. The circles denote nodes which are first
</bodyText>
<footnote confidence="0.9478795">
2The first 6 lines are definitions and theorems used in
this proof, which are omitted.
</footnote>
<bodyText confidence="0.997395888888889">
derived at this place, and nodes in the form of small
boxes are copies of some previously derived nodes
(circled nodes), which are used as premises again.
For nodes in a box, a referring expression must have
been generated in the text. The big boxes represent
attentional spaces (previously called proof units by
the author), created during the presentation process.
The naturalness of this segmentation is largely due
to the naturalness of the hierarchical planning oper-
ators. For example, attentional space U2 has two
subordinate spaces U3 and U4. This reflects a natu-
ral shift of attention between a subproof that de-
rives a formula of the pattern 3x P(x) (node 10,
3„r E U), and the subproof that proceeds after
assuming a new constant u satisfying P (node 11,
U E U). When PROVERB opens a new attentional
space, the reader will be given information to post an
open goal and the corresponding premises. Elemen-
tary attentional spaces are often composed of multi-
ple PCAs produced by consecutive navigation steps,
such as U5 and U6. It is interesting to note that
elementary attentional space cannot contain PCAs
that are produced by consecutive planning operators
in a pure hierarchical planning framework.
Adapting the theory of Reichman for our purpose
(Reichman, 1985), we assume that each attentional
space may have one of the following status:
</bodyText>
<listItem confidence="0.987749">
• an attentional space is said to be open if its root
is still an open goal.
</listItem>
<bodyText confidence="0.86217925">
— The active attentional space is the innermost
attentional space that contains the local focus.
— The controlling attentional space is the inner-
most proof unit that contains the active atten-
tional space.
— precontrol attentional spaces are attentional
spaces that contain the controlling attentional
space.
</bodyText>
<page confidence="0.999191">
193
</page>
<figureCaption confidence="0.999486">
Figure 4: Proof Tree as Discourse Model
</figureCaption>
<figure confidence="0.988554333333333">
OA
175
a6
00 0 a oo
0 0 0
a Oa
F)
16
a
173
1.12
171
</figure>
<listItem confidence="0.850016">
• Closed spaces are attentional spaces without open
goals.
</listItem>
<sectionHeader confidence="0.657367" genericHeader="method">
4 A Classification of Reference
Forms
</sectionHeader>
<bodyText confidence="0.999699333333333">
A referring expression should help a reader to iden-
tify an object from a pool of candidates. This sec-
tion presents a classification of the possible forms
with which mathematicians refer to conclusions pre-
viously proved (called reasons) or to methods of in-
ference available in a domain.
</bodyText>
<subsectionHeader confidence="0.972456">
4.1 Reference Forms for Reasons
</subsectionHeader>
<bodyText confidence="0.982791">
Three reference forms have been identified by the
author for reasons in naturally occurring proofs
(Huang, 1990):
</bodyText>
<listItem confidence="0.785127714285714">
1. The omit form: where a reason is not mentioned
at all.
2. The explicit form: where a reason is literally re-
peated.
3. The implicit form: By an implicit form we mean
that although a reason is not verbalized directly,
a hint is given in the verbalization of either the
</listItem>
<bodyText confidence="0.983603333333333">
inference method, or of the conclusion. For in-
stance, in the verbalization below
&amp;quot;Since u is an element in U, u * lu = u by
the definition of unit.&amp;quot;
the first reason of the PCA in Section 1, &amp;quot;since
1u is the unit element of U&amp;quot; is hinted at by the
inference method which reads &amp;quot;by the definition
of unit&amp;quot;.
Although omit and implicit forms lead to the same
surface structure, the existence of an implicit hint in
the other part of the verbalization affects a reader&apos;s
understanding.
</bodyText>
<subsectionHeader confidence="0.966083">
4.2 Reference Forms for Methods
</subsectionHeader>
<bodyText confidence="0.9869918">
PROVERB must select referring expressions for
methods of inference in PCAs as well. Below are
the three reference forms identified by the author,
which are analogous to the corresponding cases for
reasons:
</bodyText>
<listItem confidence="0.894963285714286">
1. the explicit form: this is the case where a writer
may decide to indicate explicitly which inference
rule he is using. For instance, explicit translations
of a definition may have the pattern: &amp;quot;by the def-
inition of unit element&amp;quot;, or &amp;quot;by the uniqueness of
solution.&amp;quot; ND rules have usually standard verbal-
izations.
2. the omit form: in this case a word such as &amp;quot;thus&amp;quot;
or &amp;quot;therefore&amp;quot; will be used.
3. The implicit form: Similar to the implicit form
for the expression of reasons, an implicit hint to
a domain-specific inference method can be given
either in the verbalization of the reasons, or in
that of the conclusion.
</listItem>
<sectionHeader confidence="0.895075" genericHeader="method">
5 Reference Choices in PROVERB
</sectionHeader>
<subsectionHeader confidence="0.997282">
5.1 Referring to Reasons
</subsectionHeader>
<bodyText confidence="0.9999336">
Because reasons are intermediate conclusions proved
previously in context, their reference choices have
much in common with the problem of choosing
anaphoric referring expressions in general. To ac-
count for this phenomenon, concepts like activat-
</bodyText>
<page confidence="0.997426">
194
</page>
<bodyText confidence="0.999964387096774">
edness, foregroundness and consciousness have been
introduced. More recently, the shift of focus has
been further investigated in the light of a structured
flow of discourse (Reichman, 1985; Grosz and Sid-
ner, 1986; Dale, 1992). The issue of salience is also
studied in a broader framework in (Pattabhiraman
and Cercone, 1993). Apart from salience, it is also
shown that referring expressions are strongly influ-
enced by other aspects of human preference. For ex-
ample, easily perceivable attributes and basic-level
attributes values are preferred (Dale and Haddock,
1991; Dale, 1992; Reiter and Dale, 1992).
In all discourse-based theories, the update of the
focus status is tightly coupled to the factoring of
the flux of text into segments. With the segmenta-
tion problem settled in section 3, the DRCC module
makes reference choices following a discourse theory
adapted from Reichman (Reichman, 1985). Based
on empirical data, Reichman argues that the choice
of referring expressions is constrained both by the
status of the discourse space and by the object&apos;s
level of focus within this space. In her theory, there
are seven status assignments a discourse space may
have. Within a discourse space, four levels of focus
can be assigned to individual objects: high, medium,
low, or zero, since there are four major ways of re-
ferring to an object using English, namely, by using
a pronoun, by name, by a description, or implicitly.
Our theory uses the notions of structural closeness
and textual closeness, and takes both of these factors
into account for argumentative discourse.
</bodyText>
<subsubsectionHeader confidence="0.961189">
5.1.1 Structural Closeness
</subsubsectionHeader>
<bodyText confidence="0.999658736842105">
The structural closeness of a reason reflects the
foreground and background character of the inner-
most attentional space containing it. Reasons that
may still remain in the focus of attention at the cur-
rent point from the structural perspective are con-
sidered as structurally close. Otherwise they are
considered as structurally distant. If a reason, for
instance, is last mentioned or proved in the active
attentional space (the subproof which a reader is
supposed to concentrate on), it is likely that this
reason still remains in his focus of attention. In con-
trast, if a reason is in a closed subproof, but is not
its conclusion, it is likely that the reason has already
been moved out of the reader&apos;s focus of attention.
Although finer differentiation may be needed, our
theory only distinguishes between reasons residing
in attentional spaces that are structurally close or
structurally distant. DRCC assigns the structural
status by applying the following rules.
</bodyText>
<listItem confidence="0.992958583333333">
1. Reasons in the active attentional space are struc-
turally close.
2. Reasons in the controlling attentional space are
structurally close.
3. Reasons in closed attentional spaces:
(a) reasons that are the root of a closed attentional
space immediate subordinate to the active at-
tentional space are structurally close.
(b) Other reasons in a closed attentional space are
structurally distant.
4. Reasons in precontrol attentional spaces are struc-
turally distant.
</listItem>
<bodyText confidence="0.9994726">
Note that the rules are specified with respect to
the innermost proof unit containing a proof node.
Rule 3 means that only the conclusions of closed
subordinated subproofs still remain in the reader&apos;s
focus of attention.
</bodyText>
<subsubsectionHeader confidence="0.973071">
5.1.2 Textual Closeness
</subsubsectionHeader>
<bodyText confidence="0.999996538461538">
The textual closeness is used as a measure of the
level of focus of an individual reason. In general,
the level of focus of an object is established when
it is activated, and decreases with the flow of dis-
course. In Reichman&apos;s theory, although four levels
of focus can be established upon activation, only one
is used in the formulation of the four reference rules.
In other words, it suffices to track the status high
alone. Therefore, we use only two values to denote
the level of focus of individual intermediate conclu-
sions, which is calculated from textual distance be-
tween the last mentioning of a reason and the current
sentence where the reason is referred to.
</bodyText>
<subsubsectionHeader confidence="0.899359">
5.1.3 Reference Rules
</subsubsectionHeader>
<bodyText confidence="0.999991333333333">
We assume that each intermediate conclusion is
put into high focus when it is presented as a newly
derived conclusion or cited as a reason supporting
the derivation of another intermediate result. This
level of focus decreases, either when a attentional
space is moved out of the foreground of discussion,
or with the increase of textual distance. The DRCC
component of PROVERB models this behavior with
the following four reference rules.
</bodyText>
<sectionHeader confidence="0.918898" genericHeader="method">
Referring Expressions for Reasons
</sectionHeader>
<listItem confidence="0.6759518">
1. If a reason is both structurally and textually close,
it will be omitted.
2. If a reason is structurally close but textually dis-
tant, first try to find an implicit form; if impossi-
ble, use an explicit form.
3. If a reason is structurally distant but textually
close, first try to find an implicit form; if impossi-
ble, omit it.
4. An explicit form will be used for reasons that are
both structurally and textually far.
</listItem>
<bodyText confidence="0.9998967">
Note that the result of applying rule 2 and rule
3 depends on the availability of an implicit form,
which often interacts with the verbalization of the
rest of a PCA, in particular with that of the inference
method. Since the reference choice for methods is
handled independent of the discourse segmentation
(Huang, 1996), however, it is not discussed in this
paper.
Fourteen PCAs are generated by the macroplanner
of PROVERB for our example in Figure 3. The
</bodyText>
<page confidence="0.996883">
195
</page>
<bodyText confidence="0.991754435897436">
microplanner and the realizer of PROVERB finally
produces:
Proof:
Let F be a group, U be a subgroup of F, 1
and 1u be unit elements of F and U, respec-
tively. According to the definition of unit ele-
ment, lu E U. Therefore there is an X, X E U.
Now suppose that u is such an X. According
to the definition of unit element, u * lu = u.
Since U is a subgroup of F, U C F. Therefore
lu E F. Similarly u E F, since u E U. Since F
is a group, F is a semigroup. Because u*lu = u,
lu is a solution of the equation u * X = u.
Since 1 is a unit element of F, u *1 = u. Since 1
is a unit element of F, 1 E F. Because u E F,1
is a solution of the equation u * X = u. Since F
is a group, lu = 1 by the uniqueness of solution.
Some explanations are in order. PROVERB&apos;s
microplanner cuts the entire text into three para-
graphs, basically mirroring the larger attentional
spaces U3, U5 and U6 in Figure 4. Since nodes 22
and 21 are omitted in this verbalization, node 20
(the last sentence) is merged into the paragraph for
U6.
Let&apos;s examine the reference choices in the second
last sentence:
Because u E F, 1 is a solution of the equation
U * X = U.
which is actually line 19 in Figure 3 and node 19
in Figure 4. Among the four reason nodes 13, 17,
18, 15, only node 13 is explicitly mentioned, since
it is in a closed attentional space (U5) and is men-
tioned five sentences ago. Node 17 and 18 are in the
current space (U6) and was activated only one or
two sentence ago, they are therefore omitted. Node
15 is also omitted although also in the same closed
space U5, but it was mentioned one sentence after
node 13 and is considered as near concerning textual
distance.
</bodyText>
<sectionHeader confidence="0.998847" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999988484848485">
This paper describes the way in which PROVERB
refers to previously derived results while verbalizing
machine-found proofs. By distinguishing between
hierarchical planning and focus-guided navigation,
PROVERB achieves a natural segmentation of con-
text into an attentional hierarchy. Based on this
segmentation, PROVERB makes reference decisions
according to a discourse theory adapted from Reich-
man for this special application.
PROVERB works in a fully automatic way. The
output texts are close to detailed proofs in text-
books and are basically accepted by the community
of automated reasoning. With the increasing size of
proofs which PROVERB is getting as input, inves-
tigation is needed both for longer proofs as well as
for more concise styles.
Although developed for a specific application, we
believe the main rationales behind of our system ar-
chitecture are useful for natural language generation
in general. Concerning segmentation of discourse, a
natural segmentation can be easily achieved if we
could distinguish between language generation ac-
tivities affecting global structure of attention and
those only moving the local focus. We believe a
global attentional hierarchy plays a crucial role in
choosing reference expressions beyond this particu-
lar domain of application. Furthermore, it turned
out to be also important for other generation deci-
sions, such as paragraph scoping and layout. Finally,
the combination of hierarchical planning with local
navigation needs more research as a topic in its own
right. For many applications, these two techniques
are a complementary pair.
</bodyText>
<sectionHeader confidence="0.975181" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9999198">
Sincere thanks are due to all three anonymous re-
viewers of ACL/EACL&apos;97, who provided valuable
comments and constructive suggestions. I would like
to thank Graeme Hirst as well, who carefully read
the final version of this paper.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990904">
Chester, Daniel. 1976. The translation of formal
proofs into English. Artificial Intelligence, 7:178-
216.
Dale, Robert. 1992. Generating Referring Expres-
sions. ACL-MIT PressSeries in Natural Language
Processing. MIT Press.
Dale, Robert and Nicholas Haddock. 1991. Con-
tent determination in the generation of referring
expressions. Computational Intelligence, 7(4).
Edgar, Andrew and Francis Jeffry Pelletier. 1993.
Natural language explanation of natural deduc-
tion proofs. In Proc. of the first Conference of the
Pacific Association for Computational Linguistics,
Vancouver, Canada. Centre for Systems Science,
Simon Fraser University.
Gentzen, Gerhard. 1935. Untersuchungen iiber das
logische SchlieBen I. Math. Zeitschrift, 39:176-210.
Grosz, Barbara J. and Candace L. Sidner. 1986. At-
tention, intentions, and the structure of discourse.
Computational Linguistics, 12(3):175-204.
Hovy, Eduard H. 1988. Generating Natural Lan-
guage under Pragmatic Constraints. Lawrence
Erlbaum Associates, Hillsdale, New Jersey.
Huang, Xiaorong. 1990. Reference choices in math-
ematical proofs. In L. C. Aiello, editor, Proc. of
</reference>
<page confidence="0.987404">
196
</page>
<reference confidence="0.994631384615384">
9th European Conference on Artificial Intelligence,
pages 720-725. Pitman Publishing.
Huang, Xiaorong. 1994. Planning argumentative
texts. In Proc. of COLING-94, pages 329-333,
Kyoto, Japan.
Huang, Xiaorong. 1996. Human Oriented Proof
Presentation: A Reconstructive Approach. Infix,
Sankt Augustin.
Huang, Xiaorong and Armin Fiedler 1997. Proof
Verbalization as an Application of NLG. In Proc.
of IJCAI-97, Nagoya, Japan, forthcoming.
Kilger, Anne and Wolfgang Finkler. 1995. Incre-
mental generation for real—time applications. Re-
search Report RR-95-11, DFKI, Saarbriicken, Ger-
many.
McDonald, David D. 1983. Natural language gen-
eration as a computational problem. In Brady
and Berwick: Computational Models of Discourse.
MIT Press.
McKeown, Kathleen. 1985. Text Generation. Cam-
bridge University Press, Cambridge, UK.
Moore, Johanna and Cecile Paris. 1989. Plan-
ning text for advisory dialogues. In Proc. 27th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 203-211, Vancouver,
British Columbia.
Ochs, Elinor. 1979. Planned and unplanned dis-
course. Syntax and Semantics, 12:51-80.
Paris, Cecile. 1988. Tailoring object descriptions to
a user&apos;s level of expertise. Computational Linguis-
tics, 14:64-78.
Pattabhiraman, T. and Nick Cercone. 1993.
Decision-theoretic salience interactions in lan-
guage generation. In Ruzena Bajcsy, editor,
Proc. of IJCA I-93, volume 2, pages 1246-1252,
Chambery, France. Morgan Kaufmann.
Reichman, Rachel. 1985. Getting Computers to Talk
Like You and Me. Discourse Context, Focus, and
Semantics. MIT Press.
Reiter, Ehud and Robert Dale. 1992. A fast algo-
rithm for the generation of referring expressions.
In Proc. of COLING-92, volume 1, pages 232-238.
Reithinger, Norbert. 1991. Eine parallele Architek-
tur zur inkrementellen Generierung multimodaler
Dialogbeitrage. Ph.D. thesis, Universitat des Saar-
landes. Also available as book, Infix, Sankt Au-
gustin, 1991.
Sibun, Penelope. 1990. The local organization of
text. In K. McKeown, J. Moore, and S. Niren-
burg, editors, Proc. of the fifth international nat-
ural language generation workshop, pages 120-127,
Dawson, Pennsylvania.
</reference>
<page confidence="0.998169">
197
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.417407">
<title confidence="0.999971">Planning Reference Choices for Argumentative Texts</title>
<author confidence="0.732128">Xiaorong Huang</author>
<affiliation confidence="0.6759295">Techne Knowledge Systems 439 University Avenue</affiliation>
<address confidence="0.992059">Toronto, Ontario M5S 3G4 Canada</address>
<email confidence="0.997866">xhOFormalSystems.ca</email>
<abstract confidence="0.995389689655172">This paper deals with the reference choices involved in the generation of argumentative text. Since a natual segmentation of discourse into attentional spaces is needed to carry out this task, this paper first proposes an architecture for natural language generation that combines planning navigawork in its own right. While hierarchical planning spans out an attentional hierarchy of the discourse produced, local navigation fills details into the primitive discourse spaces. The usefulness of this architecture actually goes beyond the particular domain of application for which it is developed. A piece of argumentative text such as the proof of a mathematical theorem conveys a sequence of derivations. For each step of derivation, the premises derived in the previous context and the inference method (such as the application of a particular theorem or definition) must be made clear. Although not restricted to nominal phrases, our reference decisions are similar to those concerning nominal subsequent referring expressions. Based on the work of Reichmann, this paper presents a discourse theory that handles reference choices by taking into account both textual distance as well as the attentional hierarchy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Chester</author>
</authors>
<title>The translation of formal proofs into English.</title>
<date>1976</date>
<journal>Artificial Intelligence,</journal>
<pages>7--178</pages>
<contexts>
<context position="4708" citStr="Chester, 1976" startWordPosition="749" endWordPosition="750">ecture that models text generation as a combination of hierarchical planning and focus-guided navigation. Following (Grosz and Sidner, 1986) we further assume that every posting of a new task by the hierarchical planning mechanism creates new attentional spaces. Based on this segmentation, PROVERB makes reference choices according to a discourse theory adapted from Reichman (Reichman, 1985; Huang, 1990). 2 The System PROVERB PROVERB is a text planner that verbalizes natural deduction (ND) style proofs (Gentzen, 1935). Several similar attempts can be found in previous work. The system EXPOUND (Chester, 1976) is an example of direct translation: Although a sophisticated linearization is applied on the input ND proofs, the steps are then translated locally in a template-driven way. ND proofs were tested as inputs to an early version of MUMBLE (McDonald, 1983); the main aim, however, was to show the feasibility of the architecture. A more recent attempt can be found in THINKER (Edgar and Pelletier, 1993), which implements several interesting but isolated proof presentation strategies. PROVERB however can be seen as the first serious attempt for a comprehensive system that produces adequate argumenta</context>
</contexts>
<marker>Chester, 1976</marker>
<rawString>Chester, Daniel. 1976. The translation of formal proofs into English. Artificial Intelligence, 7:178-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating Referring Expressions.</title>
<date>1992</date>
<booktitle>ACL-MIT PressSeries in Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3546" citStr="Dale, 1992" startWordPosition="569" endWordPosition="570">are two of the possible verbalizations: 1. (inference method omitted): &amp;quot;Since l is the unit element of U, and u is an element of U, u * lu =u.&amp;quot; 2. (reasons omitted): &amp;quot;According to the definition of unit element, u * lu u.&amp;quot; An explicit reference to a premise or an inference method is not restricted to a nominal phrase, as opposed to many of the treatments of subsequent references found in the literature. Despite this difference, the choices to be made here have much in common with the choices of subsequent references discussed in more general frameworks (Reichman, 1985; Grosz and Sidner, 1986; Dale, 1992): they depend on the availability of the object to be referred to in the context and are sensitive to the segmentation of a context into an attentional hierarchy. Therefore, we have first to devise an architecture for natural language generation that facilitates a natural and effective segmentation of discourse. The 190 basic idea is to distinguish between language production activities that effect the global shift of attention, and language production activities that involve only local attentional movement. Concretely, PROVERB uses an architecture that models text generation as a combination </context>
<context position="6031" citStr="Dale, 1992" startWordPosition="956" endWordPosition="957">e macroplanner produces a sequence of PCAs, the DRCC (Derive Reference Choices Component) module of the microplanner enriches the PCAs with reference choices. The TSG (Text Structure Generator) module subsequently produces the text structures as the output of the microplanner. Finally, text structures are realized by TAG-GEN (Kilger and Finkler, 1995), our realization component. In this paper, we concentrate only on the macroplanner and the DRCC component. 2.1 Architecture of the Macroplanner Most current text planners adopt a hierarchical planning approach (Hovy, 1988; Moore and Paris, 1989; Dale, 1992; Reithinger, 1991). Nevertheless there is psychological evidence that language has an unplanned, spontaneous aspect as well (Ochs, 1979). Based on this observation, Sibun (Sibun, 1990) implemented a system for generating descriptions of objects with a strong domain structure, such as houses, chips, and families. Her system produces text using a technique she called local organization. While a hierarchical planner recursively breaks generation tasks into subtasks, local organization navigates the domain-object following the local focus of Natural Deduction Proof Figure 1: Architecture of PROVE</context>
<context position="19153" citStr="Dale, 1992" startWordPosition="3198" endWordPosition="3199">given either in the verbalization of the reasons, or in that of the conclusion. 5 Reference Choices in PROVERB 5.1 Referring to Reasons Because reasons are intermediate conclusions proved previously in context, their reference choices have much in common with the problem of choosing anaphoric referring expressions in general. To account for this phenomenon, concepts like activat194 edness, foregroundness and consciousness have been introduced. More recently, the shift of focus has been further investigated in the light of a structured flow of discourse (Reichman, 1985; Grosz and Sidner, 1986; Dale, 1992). The issue of salience is also studied in a broader framework in (Pattabhiraman and Cercone, 1993). Apart from salience, it is also shown that referring expressions are strongly influenced by other aspects of human preference. For example, easily perceivable attributes and basic-level attributes values are preferred (Dale and Haddock, 1991; Dale, 1992; Reiter and Dale, 1992). In all discourse-based theories, the update of the focus status is tightly coupled to the factoring of the flux of text into segments. With the segmentation problem settled in section 3, the DRCC module makes reference c</context>
</contexts>
<marker>Dale, 1992</marker>
<rawString>Dale, Robert. 1992. Generating Referring Expressions. ACL-MIT PressSeries in Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Nicholas Haddock</author>
</authors>
<title>Content determination in the generation of referring expressions.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="19495" citStr="Dale and Haddock, 1991" startWordPosition="3248" endWordPosition="3251">account for this phenomenon, concepts like activat194 edness, foregroundness and consciousness have been introduced. More recently, the shift of focus has been further investigated in the light of a structured flow of discourse (Reichman, 1985; Grosz and Sidner, 1986; Dale, 1992). The issue of salience is also studied in a broader framework in (Pattabhiraman and Cercone, 1993). Apart from salience, it is also shown that referring expressions are strongly influenced by other aspects of human preference. For example, easily perceivable attributes and basic-level attributes values are preferred (Dale and Haddock, 1991; Dale, 1992; Reiter and Dale, 1992). In all discourse-based theories, the update of the focus status is tightly coupled to the factoring of the flux of text into segments. With the segmentation problem settled in section 3, the DRCC module makes reference choices following a discourse theory adapted from Reichman (Reichman, 1985). Based on empirical data, Reichman argues that the choice of referring expressions is constrained both by the status of the discourse space and by the object&apos;s level of focus within this space. In her theory, there are seven status assignments a discourse space may h</context>
</contexts>
<marker>Dale, Haddock, 1991</marker>
<rawString>Dale, Robert and Nicholas Haddock. 1991. Content determination in the generation of referring expressions. Computational Intelligence, 7(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Edgar</author>
<author>Francis Jeffry Pelletier</author>
</authors>
<title>Natural language explanation of natural deduction proofs.</title>
<date>1993</date>
<booktitle>In Proc. of the first Conference of the Pacific Association for Computational Linguistics,</booktitle>
<institution>Canada. Centre for Systems Science, Simon Fraser University.</institution>
<location>Vancouver,</location>
<contexts>
<context position="5109" citStr="Edgar and Pelletier, 1993" startWordPosition="815" endWordPosition="818">985; Huang, 1990). 2 The System PROVERB PROVERB is a text planner that verbalizes natural deduction (ND) style proofs (Gentzen, 1935). Several similar attempts can be found in previous work. The system EXPOUND (Chester, 1976) is an example of direct translation: Although a sophisticated linearization is applied on the input ND proofs, the steps are then translated locally in a template-driven way. ND proofs were tested as inputs to an early version of MUMBLE (McDonald, 1983); the main aim, however, was to show the feasibility of the architecture. A more recent attempt can be found in THINKER (Edgar and Pelletier, 1993), which implements several interesting but isolated proof presentation strategies. PROVERB however can be seen as the first serious attempt for a comprehensive system that produces adequate argumentative texts from ND style proofs. Figure 1 shows the architecture of PROVERB(Huang, 1994a; HuangFiedler, 1997): the macroplanner produces a sequence of PCAs, the DRCC (Derive Reference Choices Component) module of the microplanner enriches the PCAs with reference choices. The TSG (Text Structure Generator) module subsequently produces the text structures as the output of the microplanner. Finally, t</context>
</contexts>
<marker>Edgar, Pelletier, 1993</marker>
<rawString>Edgar, Andrew and Francis Jeffry Pelletier. 1993. Natural language explanation of natural deduction proofs. In Proc. of the first Conference of the Pacific Association for Computational Linguistics, Vancouver, Canada. Centre for Systems Science, Simon Fraser University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Gentzen</author>
</authors>
<title>Untersuchungen iiber das logische SchlieBen I.</title>
<date>1935</date>
<journal>Math. Zeitschrift,</journal>
<pages>39--176</pages>
<contexts>
<context position="4616" citStr="Gentzen, 1935" startWordPosition="734" endWordPosition="735"> activities that involve only local attentional movement. Concretely, PROVERB uses an architecture that models text generation as a combination of hierarchical planning and focus-guided navigation. Following (Grosz and Sidner, 1986) we further assume that every posting of a new task by the hierarchical planning mechanism creates new attentional spaces. Based on this segmentation, PROVERB makes reference choices according to a discourse theory adapted from Reichman (Reichman, 1985; Huang, 1990). 2 The System PROVERB PROVERB is a text planner that verbalizes natural deduction (ND) style proofs (Gentzen, 1935). Several similar attempts can be found in previous work. The system EXPOUND (Chester, 1976) is an example of direct translation: Although a sophisticated linearization is applied on the input ND proofs, the steps are then translated locally in a template-driven way. ND proofs were tested as inputs to an early version of MUMBLE (McDonald, 1983); the main aim, however, was to show the feasibility of the architecture. A more recent attempt can be found in THINKER (Edgar and Pelletier, 1993), which implements several interesting but isolated proof presentation strategies. PROVERB however can be s</context>
</contexts>
<marker>Gentzen, 1935</marker>
<rawString>Gentzen, Gerhard. 1935. Untersuchungen iiber das logische SchlieBen I. Math. Zeitschrift, 39:176-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<contexts>
<context position="3533" citStr="Grosz and Sidner, 1986" startWordPosition="565" endWordPosition="568"> history, the following are two of the possible verbalizations: 1. (inference method omitted): &amp;quot;Since l is the unit element of U, and u is an element of U, u * lu =u.&amp;quot; 2. (reasons omitted): &amp;quot;According to the definition of unit element, u * lu u.&amp;quot; An explicit reference to a premise or an inference method is not restricted to a nominal phrase, as opposed to many of the treatments of subsequent references found in the literature. Despite this difference, the choices to be made here have much in common with the choices of subsequent references discussed in more general frameworks (Reichman, 1985; Grosz and Sidner, 1986; Dale, 1992): they depend on the availability of the object to be referred to in the context and are sensitive to the segmentation of a context into an attentional hierarchy. Therefore, we have first to devise an architecture for natural language generation that facilitates a natural and effective segmentation of discourse. The 190 basic idea is to distinguish between language production activities that effect the global shift of attention, and language production activities that involve only local attentional movement. Concretely, PROVERB uses an architecture that models text generation as a</context>
<context position="13167" citStr="Grosz and Sidner, 1986" startWordPosition="2163" endWordPosition="2166"> u E U u* lu =u u E F lu E F semigroup(F, *) solution(u, u, lu, F,*) u*1 = u 1 E F solution(u, u, 1, F,*) 1 = lu 1 = lu group(F,*) A subgroup(U, F,*) A unit(F,1,*) A unit(U, lu,*) = 1 = 1u Reason (113/13) (Def-subgroup 7) (Def-unit 7) (3 9) (}13&apos;13) (Def-unit 7 11) (Def-subset 8 11) (Def-subset 8 9) (Def-group 7) (Def-solution 12 13 14 15) (Def-unit 7 13) (Def-unit 7) (Def-solution 13 17 18 15) (Th-solution 17 16 19) (Choice 10 20) (Ded 7 21) Figure 3: Abstracted Proof about Unit Element of Subgroups of a discourse into an attentional hierarchy, since following the theory of Grosz and Sidner (Grosz and Sidner, 1986), there is a one-to-one correspondence between the intentional hierarchy and the attentional hierarchy. In this section, we illustrate the attentional hierarchy with the help of an example, which will be used to discuss reference choices later. The input proof in Figure 3 is an ND style proof for the following theorem2: Theorem: Let F be a group and U a subgroup of F. If 1 and lu are unit elements of F and U respectively, then 1 = lu. The definitions of semigroup, group, and unit are obvious. solution (a , b, c, F, *) stands for &amp;quot;c is a solution of the equation a * x = b in F.&amp;quot; Each line in th</context>
<context position="19140" citStr="Grosz and Sidner, 1986" startWordPosition="3193" endWordPosition="3197">inference method can be given either in the verbalization of the reasons, or in that of the conclusion. 5 Reference Choices in PROVERB 5.1 Referring to Reasons Because reasons are intermediate conclusions proved previously in context, their reference choices have much in common with the problem of choosing anaphoric referring expressions in general. To account for this phenomenon, concepts like activat194 edness, foregroundness and consciousness have been introduced. More recently, the shift of focus has been further investigated in the light of a structured flow of discourse (Reichman, 1985; Grosz and Sidner, 1986; Dale, 1992). The issue of salience is also studied in a broader framework in (Pattabhiraman and Cercone, 1993). Apart from salience, it is also shown that referring expressions are strongly influenced by other aspects of human preference. For example, easily perceivable attributes and basic-level attributes values are preferred (Dale and Haddock, 1991; Dale, 1992; Reiter and Dale, 1992). In all discourse-based theories, the update of the focus status is tightly coupled to the factoring of the flux of text into segments. With the segmentation problem settled in section 3, the DRCC module make</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, Barbara J. and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Generating Natural Language under Pragmatic Constraints. Lawrence Erlbaum Associates,</title>
<date>1988</date>
<location>Hillsdale, New Jersey.</location>
<contexts>
<context position="5996" citStr="Hovy, 1988" startWordPosition="950" endWordPosition="951">ang, 1994a; HuangFiedler, 1997): the macroplanner produces a sequence of PCAs, the DRCC (Derive Reference Choices Component) module of the microplanner enriches the PCAs with reference choices. The TSG (Text Structure Generator) module subsequently produces the text structures as the output of the microplanner. Finally, text structures are realized by TAG-GEN (Kilger and Finkler, 1995), our realization component. In this paper, we concentrate only on the macroplanner and the DRCC component. 2.1 Architecture of the Macroplanner Most current text planners adopt a hierarchical planning approach (Hovy, 1988; Moore and Paris, 1989; Dale, 1992; Reithinger, 1991). Nevertheless there is psychological evidence that language has an unplanned, spontaneous aspect as well (Ochs, 1979). Based on this observation, Sibun (Sibun, 1990) implemented a system for generating descriptions of objects with a strong domain structure, such as houses, chips, and families. Her system produces text using a technique she called local organization. While a hierarchical planner recursively breaks generation tasks into subtasks, local organization navigates the domain-object following the local focus of Natural Deduction Pr</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Hovy, Eduard H. 1988. Generating Natural Language under Pragmatic Constraints. Lawrence Erlbaum Associates, Hillsdale, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaorong Huang</author>
</authors>
<title>Reference choices in mathematical proofs. In</title>
<date>1990</date>
<booktitle>Proc. of 9th European Conference on Artificial Intelligence,</booktitle>
<pages>720--725</pages>
<editor>L. C. Aiello, editor,</editor>
<publisher>Pitman Publishing.</publisher>
<contexts>
<context position="4500" citStr="Huang, 1990" startWordPosition="716" endWordPosition="717">tinguish between language production activities that effect the global shift of attention, and language production activities that involve only local attentional movement. Concretely, PROVERB uses an architecture that models text generation as a combination of hierarchical planning and focus-guided navigation. Following (Grosz and Sidner, 1986) we further assume that every posting of a new task by the hierarchical planning mechanism creates new attentional spaces. Based on this segmentation, PROVERB makes reference choices according to a discourse theory adapted from Reichman (Reichman, 1985; Huang, 1990). 2 The System PROVERB PROVERB is a text planner that verbalizes natural deduction (ND) style proofs (Gentzen, 1935). Several similar attempts can be found in previous work. The system EXPOUND (Chester, 1976) is an example of direct translation: Although a sophisticated linearization is applied on the input ND proofs, the steps are then translated locally in a template-driven way. ND proofs were tested as inputs to an early version of MUMBLE (McDonald, 1983); the main aim, however, was to show the feasibility of the architecture. A more recent attempt can be found in THINKER (Edgar and Pelleti</context>
<context position="17009" citStr="Huang, 1990" startWordPosition="2833" endWordPosition="2834">4: Proof Tree as Discourse Model OA 175 a6 00 0 a oo 0 0 0 a Oa F) 16 a 173 1.12 171 • Closed spaces are attentional spaces without open goals. 4 A Classification of Reference Forms A referring expression should help a reader to identify an object from a pool of candidates. This section presents a classification of the possible forms with which mathematicians refer to conclusions previously proved (called reasons) or to methods of inference available in a domain. 4.1 Reference Forms for Reasons Three reference forms have been identified by the author for reasons in naturally occurring proofs (Huang, 1990): 1. The omit form: where a reason is not mentioned at all. 2. The explicit form: where a reason is literally repeated. 3. The implicit form: By an implicit form we mean that although a reason is not verbalized directly, a hint is given in the verbalization of either the inference method, or of the conclusion. For instance, in the verbalization below &amp;quot;Since u is an element in U, u * lu = u by the definition of unit.&amp;quot; the first reason of the PCA in Section 1, &amp;quot;since 1u is the unit element of U&amp;quot; is hinted at by the inference method which reads &amp;quot;by the definition of unit&amp;quot;. Although omit and impli</context>
</contexts>
<marker>Huang, 1990</marker>
<rawString>Huang, Xiaorong. 1990. Reference choices in mathematical proofs. In L. C. Aiello, editor, Proc. of 9th European Conference on Artificial Intelligence, pages 720-725. Pitman Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaorong Huang</author>
</authors>
<title>Planning argumentative texts.</title>
<date>1994</date>
<booktitle>In Proc. of COLING-94,</booktitle>
<pages>329--333</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="5395" citStr="Huang, 1994" startWordPosition="859" endWordPosition="860"> is applied on the input ND proofs, the steps are then translated locally in a template-driven way. ND proofs were tested as inputs to an early version of MUMBLE (McDonald, 1983); the main aim, however, was to show the feasibility of the architecture. A more recent attempt can be found in THINKER (Edgar and Pelletier, 1993), which implements several interesting but isolated proof presentation strategies. PROVERB however can be seen as the first serious attempt for a comprehensive system that produces adequate argumentative texts from ND style proofs. Figure 1 shows the architecture of PROVERB(Huang, 1994a; HuangFiedler, 1997): the macroplanner produces a sequence of PCAs, the DRCC (Derive Reference Choices Component) module of the microplanner enriches the PCAs with reference choices. The TSG (Text Structure Generator) module subsequently produces the text structures as the output of the microplanner. Finally, text structures are realized by TAG-GEN (Kilger and Finkler, 1995), our realization component. In this paper, we concentrate only on the macroplanner and the DRCC component. 2.1 Architecture of the Macroplanner Most current text planners adopt a hierarchical planning approach (Hovy, 198</context>
<context position="6731" citStr="Huang, 1994" startWordPosition="1057" endWordPosition="1058">planned, spontaneous aspect as well (Ochs, 1979). Based on this observation, Sibun (Sibun, 1990) implemented a system for generating descriptions of objects with a strong domain structure, such as houses, chips, and families. Her system produces text using a technique she called local organization. While a hierarchical planner recursively breaks generation tasks into subtasks, local organization navigates the domain-object following the local focus of Natural Deduction Proof Figure 1: Architecture of PROVERB attention. PROVERB combines both of these approaches in a uniform planning framework (Huang, 1994a). The hierarchical planning splits the task of presenting a particular proof into subtasks of presenting subproofs. While the overall planning mechanism follows the RST-based planning approach (Hovy, 1988; Moore and Paris, 1989; Reithinger, 1991), the planning operators more resemble the schemata in schema-based planning (McKeown, 1985; Paris, 1988) since presentation patterns associated with specific proof patterns normally contain multiple RST-relations. PROVERB&apos;s hierarchical planning is driven by proof patterns that entail or suggest established ways of presentation. For trivial proofs t</context>
</contexts>
<marker>Huang, 1994</marker>
<rawString>Huang, Xiaorong. 1994. Planning argumentative texts. In Proc. of COLING-94, pages 329-333, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaorong Huang</author>
</authors>
<title>Human Oriented Proof Presentation: A Reconstructive Approach. Infix, Sankt Augustin.</title>
<date>1996</date>
<contexts>
<context position="24117" citStr="Huang, 1996" startWordPosition="4009" endWordPosition="4010">stant, first try to find an implicit form; if impossible, use an explicit form. 3. If a reason is structurally distant but textually close, first try to find an implicit form; if impossible, omit it. 4. An explicit form will be used for reasons that are both structurally and textually far. Note that the result of applying rule 2 and rule 3 depends on the availability of an implicit form, which often interacts with the verbalization of the rest of a PCA, in particular with that of the inference method. Since the reference choice for methods is handled independent of the discourse segmentation (Huang, 1996), however, it is not discussed in this paper. Fourteen PCAs are generated by the macroplanner of PROVERB for our example in Figure 3. The 195 microplanner and the realizer of PROVERB finally produces: Proof: Let F be a group, U be a subgroup of F, 1 and 1u be unit elements of F and U, respectively. According to the definition of unit element, lu E U. Therefore there is an X, X E U. Now suppose that u is such an X. According to the definition of unit element, u * lu = u. Since U is a subgroup of F, U C F. Therefore lu E F. Similarly u E F, since u E U. Since F is a group, F is a semigroup. Beca</context>
</contexts>
<marker>Huang, 1996</marker>
<rawString>Huang, Xiaorong. 1996. Human Oriented Proof Presentation: A Reconstructive Approach. Infix, Sankt Augustin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaorong Huang</author>
<author>Armin Fiedler</author>
</authors>
<title>Proof Verbalization as an Application of NLG.</title>
<date>1997</date>
<booktitle>In Proc. of IJCAI-97,</booktitle>
<location>Nagoya, Japan, forthcoming.</location>
<marker>Huang, Fiedler, 1997</marker>
<rawString>Huang, Xiaorong and Armin Fiedler 1997. Proof Verbalization as an Application of NLG. In Proc. of IJCAI-97, Nagoya, Japan, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Kilger</author>
<author>Wolfgang Finkler</author>
</authors>
<title>Incremental generation for real—time applications.</title>
<date>1995</date>
<tech>Research Report RR-95-11,</tech>
<location>DFKI, Saarbriicken, Germany.</location>
<contexts>
<context position="5774" citStr="Kilger and Finkler, 1995" startWordPosition="914" endWordPosition="917">ut isolated proof presentation strategies. PROVERB however can be seen as the first serious attempt for a comprehensive system that produces adequate argumentative texts from ND style proofs. Figure 1 shows the architecture of PROVERB(Huang, 1994a; HuangFiedler, 1997): the macroplanner produces a sequence of PCAs, the DRCC (Derive Reference Choices Component) module of the microplanner enriches the PCAs with reference choices. The TSG (Text Structure Generator) module subsequently produces the text structures as the output of the microplanner. Finally, text structures are realized by TAG-GEN (Kilger and Finkler, 1995), our realization component. In this paper, we concentrate only on the macroplanner and the DRCC component. 2.1 Architecture of the Macroplanner Most current text planners adopt a hierarchical planning approach (Hovy, 1988; Moore and Paris, 1989; Dale, 1992; Reithinger, 1991). Nevertheless there is psychological evidence that language has an unplanned, spontaneous aspect as well (Ochs, 1979). Based on this observation, Sibun (Sibun, 1990) implemented a system for generating descriptions of objects with a strong domain structure, such as houses, chips, and families. Her system produces text usi</context>
</contexts>
<marker>Kilger, Finkler, 1995</marker>
<rawString>Kilger, Anne and Wolfgang Finkler. 1995. Incremental generation for real—time applications. Research Report RR-95-11, DFKI, Saarbriicken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
</authors>
<title>Natural language generation as a computational problem.</title>
<date>1983</date>
<booktitle>In Brady and Berwick: Computational Models of Discourse.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4962" citStr="McDonald, 1983" startWordPosition="791" endWordPosition="792">l spaces. Based on this segmentation, PROVERB makes reference choices according to a discourse theory adapted from Reichman (Reichman, 1985; Huang, 1990). 2 The System PROVERB PROVERB is a text planner that verbalizes natural deduction (ND) style proofs (Gentzen, 1935). Several similar attempts can be found in previous work. The system EXPOUND (Chester, 1976) is an example of direct translation: Although a sophisticated linearization is applied on the input ND proofs, the steps are then translated locally in a template-driven way. ND proofs were tested as inputs to an early version of MUMBLE (McDonald, 1983); the main aim, however, was to show the feasibility of the architecture. A more recent attempt can be found in THINKER (Edgar and Pelletier, 1993), which implements several interesting but isolated proof presentation strategies. PROVERB however can be seen as the first serious attempt for a comprehensive system that produces adequate argumentative texts from ND style proofs. Figure 1 shows the architecture of PROVERB(Huang, 1994a; HuangFiedler, 1997): the macroplanner produces a sequence of PCAs, the DRCC (Derive Reference Choices Component) module of the microplanner enriches the PCAs with r</context>
</contexts>
<marker>McDonald, 1983</marker>
<rawString>McDonald, David D. 1983. Natural language generation as a computational problem. In Brady and Berwick: Computational Models of Discourse. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
</authors>
<title>Text Generation.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="7070" citStr="McKeown, 1985" startWordPosition="1104" endWordPosition="1105">ely breaks generation tasks into subtasks, local organization navigates the domain-object following the local focus of Natural Deduction Proof Figure 1: Architecture of PROVERB attention. PROVERB combines both of these approaches in a uniform planning framework (Huang, 1994a). The hierarchical planning splits the task of presenting a particular proof into subtasks of presenting subproofs. While the overall planning mechanism follows the RST-based planning approach (Hovy, 1988; Moore and Paris, 1989; Reithinger, 1991), the planning operators more resemble the schemata in schema-based planning (McKeown, 1985; Paris, 1988) since presentation patterns associated with specific proof patterns normally contain multiple RST-relations. PROVERB&apos;s hierarchical planning is driven by proof patterns that entail or suggest established ways of presentation. For trivial proofs that demonstrate no characteristic patterns, however, this technology will fail. PROVERB navigates such relatively small parts of a proof and chooses the next conclusion to be presented under the guidance of a local focus mechanism. While most existing systems follow one of the two approaches exclusively, PROVERB uses them as complementar</context>
<context position="12227" citStr="McKeown, 1985" startWordPosition="1973" endWordPosition="1975">e situation when the proof below is awaiting presentation. 1] : P(a, b) 1 • P a b , [3 2] : Q(a,b)&apos; 4: R b, [5] : Q(a,b) A R(b, c) Assume that node [1] is the local focus, {a, b} is the set of focal centers, [3] is a previously presented node and node [5] is the root of the proof to be presented. [2] is chosen as the next node to be presented, since it does not introduce any new semantic object and its overlap with the focal centers a a, bp is larger than the overlap of [4] with the focal centers ({b}). For local focus mechanisms used in another domain of application, readers are referred to (McKeown, 1985). 3 The Attentional Hierarchy The distinction between hierarchical planning and local navigation leads to a very natural segmentation 192 NNo S;D 7; I7; I7; F 7; I;11 F 7;11 I7;11 I7;11 F 7;11 F 7;11 I7;11 F7;11 I7;11 I7;11 F 7; IFormula group(F, *) A subgroup(U, F, *) A unit(F,1,*) A unit(U, lu, *) UCF lu E U 3sx U u E U u* lu =u u E F lu E F semigroup(F, *) solution(u, u, lu, F,*) u*1 = u 1 E F solution(u, u, 1, F,*) 1 = lu 1 = lu group(F,*) A subgroup(U, F,*) A unit(F,1,*) A unit(U, lu,*) = 1 = 1u Reason (113/13) (Def-subgroup 7) (Def-unit 7) (3 9) (}13&apos;13) (Def-unit 7 11) (Def-subset 8 11)</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown, Kathleen. 1985. Text Generation. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Moore</author>
<author>Cecile Paris</author>
</authors>
<title>Planning text for advisory dialogues.</title>
<date>1989</date>
<booktitle>In Proc. 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>203--211</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="6019" citStr="Moore and Paris, 1989" startWordPosition="952" endWordPosition="955">HuangFiedler, 1997): the macroplanner produces a sequence of PCAs, the DRCC (Derive Reference Choices Component) module of the microplanner enriches the PCAs with reference choices. The TSG (Text Structure Generator) module subsequently produces the text structures as the output of the microplanner. Finally, text structures are realized by TAG-GEN (Kilger and Finkler, 1995), our realization component. In this paper, we concentrate only on the macroplanner and the DRCC component. 2.1 Architecture of the Macroplanner Most current text planners adopt a hierarchical planning approach (Hovy, 1988; Moore and Paris, 1989; Dale, 1992; Reithinger, 1991). Nevertheless there is psychological evidence that language has an unplanned, spontaneous aspect as well (Ochs, 1979). Based on this observation, Sibun (Sibun, 1990) implemented a system for generating descriptions of objects with a strong domain structure, such as houses, chips, and families. Her system produces text using a technique she called local organization. While a hierarchical planner recursively breaks generation tasks into subtasks, local organization navigates the domain-object following the local focus of Natural Deduction Proof Figure 1: Architect</context>
</contexts>
<marker>Moore, Paris, 1989</marker>
<rawString>Moore, Johanna and Cecile Paris. 1989. Planning text for advisory dialogues. In Proc. 27th Annual Meeting of the Association for Computational Linguistics, pages 203-211, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elinor Ochs</author>
</authors>
<title>Planned and unplanned discourse. Syntax and Semantics,</title>
<date>1979</date>
<pages>12--51</pages>
<contexts>
<context position="6168" citStr="Ochs, 1979" startWordPosition="975" endWordPosition="976">ith reference choices. The TSG (Text Structure Generator) module subsequently produces the text structures as the output of the microplanner. Finally, text structures are realized by TAG-GEN (Kilger and Finkler, 1995), our realization component. In this paper, we concentrate only on the macroplanner and the DRCC component. 2.1 Architecture of the Macroplanner Most current text planners adopt a hierarchical planning approach (Hovy, 1988; Moore and Paris, 1989; Dale, 1992; Reithinger, 1991). Nevertheless there is psychological evidence that language has an unplanned, spontaneous aspect as well (Ochs, 1979). Based on this observation, Sibun (Sibun, 1990) implemented a system for generating descriptions of objects with a strong domain structure, such as houses, chips, and families. Her system produces text using a technique she called local organization. While a hierarchical planner recursively breaks generation tasks into subtasks, local organization navigates the domain-object following the local focus of Natural Deduction Proof Figure 1: Architecture of PROVERB attention. PROVERB combines both of these approaches in a uniform planning framework (Huang, 1994a). The hierarchical planning splits </context>
</contexts>
<marker>Ochs, 1979</marker>
<rawString>Ochs, Elinor. 1979. Planned and unplanned discourse. Syntax and Semantics, 12:51-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile Paris</author>
</authors>
<title>Tailoring object descriptions to a user&apos;s level of expertise.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<pages>14--64</pages>
<contexts>
<context position="7084" citStr="Paris, 1988" startWordPosition="1106" endWordPosition="1107">ration tasks into subtasks, local organization navigates the domain-object following the local focus of Natural Deduction Proof Figure 1: Architecture of PROVERB attention. PROVERB combines both of these approaches in a uniform planning framework (Huang, 1994a). The hierarchical planning splits the task of presenting a particular proof into subtasks of presenting subproofs. While the overall planning mechanism follows the RST-based planning approach (Hovy, 1988; Moore and Paris, 1989; Reithinger, 1991), the planning operators more resemble the schemata in schema-based planning (McKeown, 1985; Paris, 1988) since presentation patterns associated with specific proof patterns normally contain multiple RST-relations. PROVERB&apos;s hierarchical planning is driven by proof patterns that entail or suggest established ways of presentation. For trivial proofs that demonstrate no characteristic patterns, however, this technology will fail. PROVERB navigates such relatively small parts of a proof and chooses the next conclusion to be presented under the guidance of a local focus mechanism. While most existing systems follow one of the two approaches exclusively, PROVERB uses them as complementary techniques i</context>
</contexts>
<marker>Paris, 1988</marker>
<rawString>Paris, Cecile. 1988. Tailoring object descriptions to a user&apos;s level of expertise. Computational Linguistics, 14:64-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pattabhiraman</author>
<author>Nick Cercone</author>
</authors>
<title>Decision-theoretic salience interactions in language generation.</title>
<date>1993</date>
<booktitle>In Ruzena Bajcsy, editor, Proc. of IJCA I-93,</booktitle>
<volume>2</volume>
<pages>1246--1252</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Chambery, France.</location>
<contexts>
<context position="19252" citStr="Pattabhiraman and Cercone, 1993" startWordPosition="3212" endWordPosition="3215">. 5 Reference Choices in PROVERB 5.1 Referring to Reasons Because reasons are intermediate conclusions proved previously in context, their reference choices have much in common with the problem of choosing anaphoric referring expressions in general. To account for this phenomenon, concepts like activat194 edness, foregroundness and consciousness have been introduced. More recently, the shift of focus has been further investigated in the light of a structured flow of discourse (Reichman, 1985; Grosz and Sidner, 1986; Dale, 1992). The issue of salience is also studied in a broader framework in (Pattabhiraman and Cercone, 1993). Apart from salience, it is also shown that referring expressions are strongly influenced by other aspects of human preference. For example, easily perceivable attributes and basic-level attributes values are preferred (Dale and Haddock, 1991; Dale, 1992; Reiter and Dale, 1992). In all discourse-based theories, the update of the focus status is tightly coupled to the factoring of the flux of text into segments. With the segmentation problem settled in section 3, the DRCC module makes reference choices following a discourse theory adapted from Reichman (Reichman, 1985). Based on empirical data</context>
</contexts>
<marker>Pattabhiraman, Cercone, 1993</marker>
<rawString>Pattabhiraman, T. and Nick Cercone. 1993. Decision-theoretic salience interactions in language generation. In Ruzena Bajcsy, editor, Proc. of IJCA I-93, volume 2, pages 1246-1252, Chambery, France. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Reichman</author>
</authors>
<title>Getting Computers to Talk Like You and Me. Discourse Context, Focus, and Semantics.</title>
<date>1985</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3509" citStr="Reichman, 1985" startWordPosition="563" endWordPosition="564">on the discourse history, the following are two of the possible verbalizations: 1. (inference method omitted): &amp;quot;Since l is the unit element of U, and u is an element of U, u * lu =u.&amp;quot; 2. (reasons omitted): &amp;quot;According to the definition of unit element, u * lu u.&amp;quot; An explicit reference to a premise or an inference method is not restricted to a nominal phrase, as opposed to many of the treatments of subsequent references found in the literature. Despite this difference, the choices to be made here have much in common with the choices of subsequent references discussed in more general frameworks (Reichman, 1985; Grosz and Sidner, 1986; Dale, 1992): they depend on the availability of the object to be referred to in the context and are sensitive to the segmentation of a context into an attentional hierarchy. Therefore, we have first to devise an architecture for natural language generation that facilitates a natural and effective segmentation of discourse. The 190 basic idea is to distinguish between language production activities that effect the global shift of attention, and language production activities that involve only local attentional movement. Concretely, PROVERB uses an architecture that mod</context>
<context position="15923" citStr="Reichman, 1985" startWordPosition="2646" endWordPosition="2647"> (node 10, 3„r E U), and the subproof that proceeds after assuming a new constant u satisfying P (node 11, U E U). When PROVERB opens a new attentional space, the reader will be given information to post an open goal and the corresponding premises. Elementary attentional spaces are often composed of multiple PCAs produced by consecutive navigation steps, such as U5 and U6. It is interesting to note that elementary attentional space cannot contain PCAs that are produced by consecutive planning operators in a pure hierarchical planning framework. Adapting the theory of Reichman for our purpose (Reichman, 1985), we assume that each attentional space may have one of the following status: • an attentional space is said to be open if its root is still an open goal. — The active attentional space is the innermost attentional space that contains the local focus. — The controlling attentional space is the innermost proof unit that contains the active attentional space. — precontrol attentional spaces are attentional spaces that contain the controlling attentional space. 193 Figure 4: Proof Tree as Discourse Model OA 175 a6 00 0 a oo 0 0 0 a Oa F) 16 a 173 1.12 171 • Closed spaces are attentional spaces wi</context>
<context position="19116" citStr="Reichman, 1985" startWordPosition="3191" endWordPosition="3192">domain-specific inference method can be given either in the verbalization of the reasons, or in that of the conclusion. 5 Reference Choices in PROVERB 5.1 Referring to Reasons Because reasons are intermediate conclusions proved previously in context, their reference choices have much in common with the problem of choosing anaphoric referring expressions in general. To account for this phenomenon, concepts like activat194 edness, foregroundness and consciousness have been introduced. More recently, the shift of focus has been further investigated in the light of a structured flow of discourse (Reichman, 1985; Grosz and Sidner, 1986; Dale, 1992). The issue of salience is also studied in a broader framework in (Pattabhiraman and Cercone, 1993). Apart from salience, it is also shown that referring expressions are strongly influenced by other aspects of human preference. For example, easily perceivable attributes and basic-level attributes values are preferred (Dale and Haddock, 1991; Dale, 1992; Reiter and Dale, 1992). In all discourse-based theories, the update of the focus status is tightly coupled to the factoring of the flux of text into segments. With the segmentation problem settled in section</context>
</contexts>
<marker>Reichman, 1985</marker>
<rawString>Reichman, Rachel. 1985. Getting Computers to Talk Like You and Me. Discourse Context, Focus, and Semantics. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>A fast algorithm for the generation of referring expressions.</title>
<date>1992</date>
<booktitle>In Proc. of COLING-92,</booktitle>
<volume>1</volume>
<pages>232--238</pages>
<contexts>
<context position="19531" citStr="Reiter and Dale, 1992" startWordPosition="3254" endWordPosition="3257">s like activat194 edness, foregroundness and consciousness have been introduced. More recently, the shift of focus has been further investigated in the light of a structured flow of discourse (Reichman, 1985; Grosz and Sidner, 1986; Dale, 1992). The issue of salience is also studied in a broader framework in (Pattabhiraman and Cercone, 1993). Apart from salience, it is also shown that referring expressions are strongly influenced by other aspects of human preference. For example, easily perceivable attributes and basic-level attributes values are preferred (Dale and Haddock, 1991; Dale, 1992; Reiter and Dale, 1992). In all discourse-based theories, the update of the focus status is tightly coupled to the factoring of the flux of text into segments. With the segmentation problem settled in section 3, the DRCC module makes reference choices following a discourse theory adapted from Reichman (Reichman, 1985). Based on empirical data, Reichman argues that the choice of referring expressions is constrained both by the status of the discourse space and by the object&apos;s level of focus within this space. In her theory, there are seven status assignments a discourse space may have. Within a discourse space, four </context>
</contexts>
<marker>Reiter, Dale, 1992</marker>
<rawString>Reiter, Ehud and Robert Dale. 1992. A fast algorithm for the generation of referring expressions. In Proc. of COLING-92, volume 1, pages 232-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
</authors>
<title>Eine parallele Architektur zur inkrementellen Generierung multimodaler Dialogbeitrage.</title>
<date>1991</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat</institution>
<contexts>
<context position="6050" citStr="Reithinger, 1991" startWordPosition="958" endWordPosition="959">er produces a sequence of PCAs, the DRCC (Derive Reference Choices Component) module of the microplanner enriches the PCAs with reference choices. The TSG (Text Structure Generator) module subsequently produces the text structures as the output of the microplanner. Finally, text structures are realized by TAG-GEN (Kilger and Finkler, 1995), our realization component. In this paper, we concentrate only on the macroplanner and the DRCC component. 2.1 Architecture of the Macroplanner Most current text planners adopt a hierarchical planning approach (Hovy, 1988; Moore and Paris, 1989; Dale, 1992; Reithinger, 1991). Nevertheless there is psychological evidence that language has an unplanned, spontaneous aspect as well (Ochs, 1979). Based on this observation, Sibun (Sibun, 1990) implemented a system for generating descriptions of objects with a strong domain structure, such as houses, chips, and families. Her system produces text using a technique she called local organization. While a hierarchical planner recursively breaks generation tasks into subtasks, local organization navigates the domain-object following the local focus of Natural Deduction Proof Figure 1: Architecture of PROVERB attention. PROVE</context>
</contexts>
<marker>Reithinger, 1991</marker>
<rawString>Reithinger, Norbert. 1991. Eine parallele Architektur zur inkrementellen Generierung multimodaler Dialogbeitrage. Ph.D. thesis, Universitat des Saarlandes. Also available as book, Infix, Sankt Augustin, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Sibun</author>
</authors>
<title>The local organization of text. In</title>
<date>1990</date>
<booktitle>Proc. of the fifth international natural language generation workshop,</booktitle>
<pages>120--127</pages>
<editor>K. McKeown, J. Moore, and S. Nirenburg, editors,</editor>
<location>Dawson, Pennsylvania.</location>
<contexts>
<context position="6216" citStr="Sibun, 1990" startWordPosition="982" endWordPosition="983">Generator) module subsequently produces the text structures as the output of the microplanner. Finally, text structures are realized by TAG-GEN (Kilger and Finkler, 1995), our realization component. In this paper, we concentrate only on the macroplanner and the DRCC component. 2.1 Architecture of the Macroplanner Most current text planners adopt a hierarchical planning approach (Hovy, 1988; Moore and Paris, 1989; Dale, 1992; Reithinger, 1991). Nevertheless there is psychological evidence that language has an unplanned, spontaneous aspect as well (Ochs, 1979). Based on this observation, Sibun (Sibun, 1990) implemented a system for generating descriptions of objects with a strong domain structure, such as houses, chips, and families. Her system produces text using a technique she called local organization. While a hierarchical planner recursively breaks generation tasks into subtasks, local organization navigates the domain-object following the local focus of Natural Deduction Proof Figure 1: Architecture of PROVERB attention. PROVERB combines both of these approaches in a uniform planning framework (Huang, 1994a). The hierarchical planning splits the task of presenting a particular proof into s</context>
</contexts>
<marker>Sibun, 1990</marker>
<rawString>Sibun, Penelope. 1990. The local organization of text. In K. McKeown, J. Moore, and S. Nirenburg, editors, Proc. of the fifth international natural language generation workshop, pages 120-127, Dawson, Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>